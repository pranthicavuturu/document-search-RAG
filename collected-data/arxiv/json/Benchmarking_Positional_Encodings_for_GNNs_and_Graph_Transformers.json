{
    "title": "Benchmarking Positional Encodings for GNNs and Graph Transformers",
    "abstract": "Recent advances in Graph Neural Networks (GNNs) and Graph Transformers (GTs) have been driven by innovations in architectures and Positional Encodings (PEs), which are critical for augmenting node features and capturing graph topol- ogy. PEs are essential for GTs, where topological information would otherwise be lost without message-passing. However, PEs are often tested alongside novel architectures, making it difficult to isolate their effect on established models. To address this, we present a comprehensive benchmark of PEs in a unified frame- work that includes both message-passing GNNs and GTs. We also establish the- oretical connections between MPNNs and GTs and introduce a sparsified GRIT attention mechanism to examine the influence of global connectivity. Our findings demonstrate that previously untested combinations of GNN architectures and PEs can outperform existing methods and offer a more comprehensive picture of the state-of-the-art. To support future research and experimentation in our framework, we make the code publicly available. 1 INTRODUCTION Graph machine learning has traditionally relied on message-passing neural networks (MPNNs), which work through iterative rounds of neighborhood aggregation (Kipf & Welling, 2016). In each round, nodes update their states by incorporating information from their neighbors along with their own current states. While effective in capturing local graph structures, this approach can strug- gle with modeling long-range dependencies. Graph Transformer (GT) architectures utilize full at- tention mechanisms to circumvent this, but necessitate new methods to integrate graph topology information (Dwivedi & Bresson, 2020). This is similar to how positional encodings (PEs) in Nat- ural Language Processing (NLP) represent token positions within sequences (Vaswani et al., 2017). However, encoding positional information in graphs is more complex than in sequences. Ideally, positional encodings should allow the reconstruction of the graph’s topology from node features and provide useful inductive biases to improve performance (Black et al., 2024). Despite the growing number of new graph transformer architectures and positional encodings, there has been a lack of systematic evaluation comparing these encodings across different GT architectures. This makes it difficult to determine whether observed performance improvements are due to novel encodings or architectural innovations. In this paper, we conduct a comprehensive evaluation of various positional encodings for both message-passing and transformer frameworks. Our goal is to understand the impact of positional encodings on model performance and identify the best combinations of encodings and architectures. By benchmarking state-of-the-art graph transformers with a variety of positional encodings, we provide a clear picture of the current state of the field and offer guidance for future research. Addi- tionally, we further strengthen the theoretical connection between MPNNs and GTs. Although GTs are generally considered fundamentally different due to their use of attention mechanisms, we show that under certain conditions, MPNNs and GTs can be equally expressive, with additional results that extend the scope of previous analyses (Veliˇckovi´c, 2023; M¨uller & Morris, 2024). Specifically, MPNNs can be applied to fully-connected graphs and operate like a GT, while attention mech- anisms can also be adapted for local message-passing. Our theoretical analysis demonstrates that both MPNNs and GTs can have the same expressiveness when the underlying topology of the MPNN 1 arXiv:2411.12732v1  [cs.LG]  19 Nov 2024",
    "body": "Benchmarking Positional Encodings for GNNs and Graph Transformers\nBENCHMARKING\nPOSITIONAL\nENCODINGS\nFOR\nGNNS AND GRAPH TRANSFORMERS\nFlorian Gr¨otschla\nETH Zurich\nZurich, Switzerland\nfgroetschla@ethz.ch\nJiaqing Xie\nETH Zurich\nZurich, Switzerland\njiaxie@ethz.ch\nRoger Wattenhofer\nETH Zurich\nZurich, Switzerland\nwattenhofer@ethz.ch\nABSTRACT\nRecent advances in Graph Neural Networks (GNNs) and Graph Transformers\n(GTs) have been driven by innovations in architectures and Positional Encodings\n(PEs), which are critical for augmenting node features and capturing graph topol-\nogy. PEs are essential for GTs, where topological information would otherwise\nbe lost without message-passing. However, PEs are often tested alongside novel\narchitectures, making it difficult to isolate their effect on established models. To\naddress this, we present a comprehensive benchmark of PEs in a unified frame-\nwork that includes both message-passing GNNs and GTs. We also establish the-\noretical connections between MPNNs and GTs and introduce a sparsified GRIT\nattention mechanism to examine the influence of global connectivity. Our findings\ndemonstrate that previously untested combinations of GNN architectures and PEs\ncan outperform existing methods and offer a more comprehensive picture of the\nstate-of-the-art. To support future research and experimentation in our framework,\nwe make the code publicly available.\n1\nINTRODUCTION\nGraph machine learning has traditionally relied on message-passing neural networks (MPNNs),\nwhich work through iterative rounds of neighborhood aggregation (Kipf & Welling, 2016). In each\nround, nodes update their states by incorporating information from their neighbors along with their\nown current states. While effective in capturing local graph structures, this approach can strug-\ngle with modeling long-range dependencies. Graph Transformer (GT) architectures utilize full at-\ntention mechanisms to circumvent this, but necessitate new methods to integrate graph topology\ninformation (Dwivedi & Bresson, 2020). This is similar to how positional encodings (PEs) in Nat-\nural Language Processing (NLP) represent token positions within sequences (Vaswani et al., 2017).\nHowever, encoding positional information in graphs is more complex than in sequences. Ideally,\npositional encodings should allow the reconstruction of the graph’s topology from node features and\nprovide useful inductive biases to improve performance (Black et al., 2024). Despite the growing\nnumber of new graph transformer architectures and positional encodings, there has been a lack of\nsystematic evaluation comparing these encodings across different GT architectures. This makes it\ndifficult to determine whether observed performance improvements are due to novel encodings or\narchitectural innovations.\nIn this paper, we conduct a comprehensive evaluation of various positional encodings for both\nmessage-passing and transformer frameworks. Our goal is to understand the impact of positional\nencodings on model performance and identify the best combinations of encodings and architectures.\nBy benchmarking state-of-the-art graph transformers with a variety of positional encodings, we\nprovide a clear picture of the current state of the field and offer guidance for future research. Addi-\ntionally, we further strengthen the theoretical connection between MPNNs and GTs. Although GTs\nare generally considered fundamentally different due to their use of attention mechanisms, we show\nthat under certain conditions, MPNNs and GTs can be equally expressive, with additional results\nthat extend the scope of previous analyses (Veliˇckovi´c, 2023; M¨uller & Morris, 2024). Specifically,\nMPNNs can be applied to fully-connected graphs and operate like a GT, while attention mech-\nanisms can also be adapted for local message-passing. Our theoretical analysis demonstrates that\nboth MPNNs and GTs can have the same expressiveness when the underlying topology of the MPNN\n1\narXiv:2411.12732v1  [cs.LG]  19 Nov 2024\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nis fully connected. Based on these insights, we extend our evaluation to include MPNNs with po-\nsitional encodings on fully-connected graphs and modify state-of-the-art attention mechanisms for\nlocalized graph convolutions. Our results indicate that by combining existing positional encodings\nand architectures, state-of-the-art performance can be achieved on several benchmark datasets.\nOur contributions can be summarized as follows:\n1. We conduct an empirical evaluation of various positional encodings across message-\npassing neural networks and Graph Transformers to consolidate the current state-of-the-art\nand find new combinations that surpass the previous best models.\n2. We provide theoretical insights into the relationship between MPNNs and GTs, showing\nconditions where they share similar expressiveness. Based on these observations, we intro-\nduce a sparsified version of GRIT attention for localized graph convolutions, which proves\neffective across multiple datasets.\n3. We provide a unified evaluation framework implementing all used architectures and posi-\ntional encodings in one codebase to facilitate the testing of new positional encodings and\nmodels. The code is made publicly available.1\n2\nRELATED WORK\nMessage-Passing Neural Networks (MPNNs).\nEarlier graph neural networks (GNNs), including\nmodels like GCN (Kipf & Welling, 2016), GAT (Veliˇckovi´c et al., 2017), GraphSAGE (Hamilton\net al., 2017), and GIN (Xu et al., 2018), have paved the way for various advancements. Some con-\nvolutional filtering variants incorporate edge attributes into their architecture. GatedGCN (Bresson\n& Laurent, 2017) employs gates as sparse attention mechanisms, while GINE (Hu et al., 2019)\naugments features with local edges. Recent efforts aim to enhance the expressive power of GNNs,\naddressing the limitations imposed by the 1-WL test. For instance, Principal Neighborhood Ag-\ngregation (PNA) (Corso et al., 2020) combines different aggregators with degree-scalers to tackle\nisomorphism tasks in continuous feature spaces. Higher-order GNNs, like k-GNN (Morris et al.,\n2019; Maron et al., 2019), build on the k-WL algorithm, a more generalized version of the WL test,\noffering increased expressive power. Other approaches, such as GSN (Bouritsas et al., 2022) and\nGIN-AK+ (Zhao et al., 2021), utilize substructures (subgraphs) for message passing, while methods\nlike CIN (Bodnar et al., 2021) operate on regular cell complexes, although they remain less pow-\nerful than the 3-WL test. Importantly, these models serve as baselines in some graph transformers,\ndemonstrating comparable performance with certain GTs, as cited in GraphGPS (Ramp´aˇsek et al.,\n2022), GRIT (Ma et al., 2023), and Exphormer (Shirzad et al., 2023).\nGraph Transformers (GTs).\nGraph Transformers (GT) were popularized in recent years\n(Ramp´aˇsek et al., 2022; Liu et al., 2023; Mao et al., 2024; Zhang et al., 2023). Modules includ-\ning positional or structural encodings, global attention, and local message passing are considered as\nmainstream design components for a standard graph transformer model, which successfully solved\nthe problem of in-scalability (Ramp´aˇsek et al., 2022; Shirzad et al., 2023) in large graphs, lack of\ngraph inductive bias (Ma et al., 2023), and over-smoothing problems (Chen et al., 2022b). Apart\nfrom its maturity in some machine learning fields such as natural language processing, computer\nvision, or bioinformatics that many previous GT papers have mentioned, GTs have also demon-\nstrated their strength by extending their application to scientific domains such as differential equa-\ntions (Bryutkin et al., 2024; Choromanski et al., 2022), quantum physics (Wang et al., 2022a), and\nsymbolic regression (Zhong & Meidani). Some recent works are theoretical analysis in graph trans-\nformers regarding the theoretical expressive power of GT (Zhou et al., 2024), and the analytical\nrelationship between positional encodings in GT (Keriven & Vaiter, 2024; Black et al., 2024). How-\never, there is currently a lack of a practical benchmark that compares different types of positional\nencodings. MPNNs and GTs have been compared extensively in the literature, with early work ob-\nserving that these models can simulate one another (Veliˇckovi´c, 2023). A more rigorous theoretical\nanalysis has demonstrated that GTs can be related to MPNNs when a virtual node is employed (Cai\net al., 2023). Furthermore, it has been established that GTs can simulate MPNNs, provided that the\npositional encodings are sufficiently strong (M¨uller & Morris, 2024). In contrast, our findings show\n1https://github.com/ETH-DISCO/Benchmarking-PEs\n2\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nconditions under which MPNNs operating on fully connected graphs can achieve equal expressive-\nness to that of GTs, without requiring additional positional encodings or architectural modifications.\nGTs traditionally make use of positional encodings to encode the graph topology, especially when\nfull attention is used. We provide an in-depth review of positional encodings and benchmarking in\nSection 3.1 and Appendix A.1.\n3\nTHEORETICAL FOUNDATIONS\n3.1\nPOSITIONAL ENCODINGS\nNumerous positional encodings for graph-based models have been discussed in recent research, but\nthey are often scattered across various ablation studies with no unified framework. In this paper,\nwe categorize and streamline the formal definition of existing graph-based positional encodings into\nthree main categories: Laplacian-based, Random walk-based, and others.\nWe start with some fundamental definitions related to graphs. Let the input graph be G = (V, E, X),\nwhere X ∈R|V| represents the node features. For any graph G, essential properties include the\ndegree matrix D and the adjacency matrix A. The graph Laplacian matrix L is defined as L = D−A.\nA normalized graph Laplacian is given by L = I −D−1\n2 AD−1\n2 = U T ΛU, where the i-th row of U\ncorresponds to the graph’s i-th eigenvector ui, and Λ is a diagonal matrix containing the eigenvalues\nof L. We define a graph neural network model f(·) parameterized by Θ. We denote Xk\nPE as the\npositional encoding for node K.\nLaplacian-based methods utilize functions of the k-th eigenvector Uk,:, Λ, and parameters Θ. Ex-\namples include Laplacian Positional Encoding (LapPE) (Ramp´aˇsek et al., 2022) and Sign-Invariant\nNetworks (SignNet) (Lim et al., 2022).\nXk\nPE = f (Uk,:, Λ, Θ, {·})\nRandom walk-based methods are derived from polynomial function p(·) of D and A. Examples\nare Random-Walk Structural Encoding RWSE (Ramp´aˇsek et al., 2022), Random-Walk Diffusion\n(RWDIFF / LSPE) (Dwivedi et al., 2021), and Relative Random Walk Probability Based (RRWP)\n(Ma et al., 2023).\nXk\nPE = p (D, A, {·})\nOther methods rely on different procedures, such as colors obtained by mapping 1-WL to higher\ndimensions. We thus use this umbrella class for all remaining PEs. Examples include the WL-\nbased Positional Encoding (WLPE) (Dwivedi & Bresson, 2020) and Graph Convolution Kernel\nNetworks (GCKN) (Mialon et al., 2021). We aim to succinctly summarize and unify these positional\nencoding methods for better accessibility and comparison. The Appendix contains more specific\ndetails (including equations) for each positional encoding.\n3.2\nMESSAGE-PASSING NETWORKS\nMPNNs comprise multiple layers that repeatedly apply neighborhood aggregation and combine\nfunctions to learn a representation vector for each node in the graph. For an input graph G =\n(V, E, X), the i-th layer of a MPNN can be written as\nc(i)\nv\n= COMBINE(i) \u0010\nc(i−1)\nv\n, AGGREGATE(i) \u0010nn\nc(i−1)\nw\n: w ∈N(v)\noo\u0011\u0011\n,\nwhere c(i−1)\nv\nrepresents the state of node v after layer (i −1).\n3.3\nGRAPH TRANSFORMERS\nTransformer models have been widely used in modeling sequence-to-sequence data in different do-\nmains (Vaswani et al., 2017). Although the attention mechanism has commonly been used to learn\non graph-structured data (Veliˇckovi´c et al., 2017), the use of transformers is relatively recent. A\nGT layer relies on a self-attention module that lets nodes attend to a set of “neighbors”, effectively\nresulting in a dependency graph G of nodes that can attend to each other. We will refer to the nodes\n3\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nthat a node u can attend to simply as its neighborhood N(u). Many architectures use “full attention”\non the graph (as opposed to “sparse attention”), meaning that all nodes can attend to all other nodes\nin the graph, i.e., the underlying dependency graph for attention is fully connected. Based on the\ngiven neighborhood, the generalized attention mechanism first computes attention scores αu,v for\nevery node u and every v ∈N(u), based on the embeddings c(i−1)\nu\nand c(i−1)\nv\nfrom the previous\niteration, and potentially including labels for the edges (u, v) ∈V (G). The attention coefficients\nare then used to weigh the importance of neighbors and compute a new embedding for u as follows:\nc(i)\nu = Θ\n\nc(i−1)\nu\n+\nX\nv∈N (u)\nαu,v · δ(c(i−1)\nv\n)\n\n,\nwhere Θ and δ are transformations for embeddings. This definition aligns with popular architectures\nsuch as Exphormer (Shirzad et al., 2023) and GraphGPS (Ramp´aˇsek et al., 2022). We use Exphormer\nas a running example to clarify the practical applicability of our proofs, as its attention mechanism\ncan attend to arbitrary neighborhoods. In the case of Exphormer, δ becomes a linear transformation,\nΘ the identity function, and attention coefficients αu,v are computed via dot-product attention that\nintegrates edge labels.\nTo maintain information about the original topology, adding connectivity information back into the\nattention mechanism is essential. This is typically done by using positional encodings. Positional\nencodings can come in the form of node encodings (Ramp´aˇsek et al., 2022), which are essentially\nfeatures added to the nodes before the attention block is applied, or edge features, where every edge\nis endowed with (additional) features, such as the shortest-path-distance between the respective\nnodes (Ying et al., 2021). In our framework, positional encodings are modeled as labels for nodes\nin G, whereas relative positional encodings can be modeled as edge labels.\n3.4\nBRIDGING GTS AND WL\nIn the literature, various attempts have been made to bridge the gap between Graph Transformers\n(GTs) and the WL test (M¨uller & Morris, 2024; Cai et al., 2023). This is usually done by defining\nnew variants of the WL algorithm that apply to the GT of interest (Kim et al., 2022). However, we\nargue that such extensions are not necessary. Instead, we can interpret the execution of a GT on G\nas an MPNN on a new topology G′ = (V, E′) corresponding to the dependency graph, representing\nthe information flow in the attention layer (Veliˇckovi´c, 2023). For example, a GT with full attention\ncan be seen as an MPNN on the fully connected graph, with E′ = V × V . Relative positional\nencodings can be added to the MPNN as edge labels. This means we can use the (edge-augmented)\n1-WL algorithm on G′ to upper bound the expressive power of a GT on G. While it is perhaps not\nsurprising that GT expressivity can be upper bounded in this way, we also show that GTs can attain\nthis upper bound under some reasonable assumptions. To facilitate this proof, we use the same idea\nas Xu et al. (2018) to show the equivalence between the GIN architecture and 1-WL.\nLemma 3.1 (Adapted from Corollary 6 by Xu et al. (2018)). Assume X is a countable set. There\nexists a function f : X →Rn so that for infinitely many choices of ϵ, including all irrational num-\nbers, h(c, X) = f(c) + P\nx∈X f(x) is unique for each pair (c, X), where c ∈X and X ⊆X\nis a multiset of bounded size. Moreover, any function g over such pairs can be decomposed as\ng(c, X) = φ\n\u0000(f(c) + (1 + ϵ) P\nx∈X f(x)\n\u0001\nfor some function φ.\nSee proof on page 16.\nTo complete the proof for GTs, we adapt Corollary 6 by moving the use of the multiplicative factor\nϵ ∈R from f(c) to the aggregation P\nx∈X f(x). This is because the GT can multiply the aggrega-\ntion by ϵ using the attention coefficients while it cannot transform c(i−1)\nu\ndirectly. The ϵ is used in\nthe proof to differentiate between embeddings from neighbors and a node’s own embedding.\nWith the adapted Lemma, we can prove the following:\nTheorem 3.2. Let G = (V, E) be a graph with node embeddings cv for nodes v ∈V . A GT layer\non the dependency graph G′ = (V, E′) can map nodes v1, v2 ∈V to different embeddings only if\nthe 1-WL algorithm using E′ assigns different labels to nodes v1 and v2. For equivalence, we need δ\n(in the definition of GTs) to be injective and αu,v = c for a given constant c ∈R and all (u, v) ∈E′,\nmaking the GT as expressive as the 1-WL algorithm.\n4\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nSee proof on page 17.\nThe result implies that we can bound the expressiveness of a GT by that of the WL algorithm. As\nan example, GTs with full attention, as used by Ramp´aˇsek et al. (2022) and Ma et al. (2023), can be\nbound by the 1-WL algorithm on the fully connected graph. In this case, we can interpret positional\nencodings for node pairs as edge features on the complete graph.\nIn the case of Exphormer, we notice that δ can be parametrized to be injective when using positional\nencodings for nodes. This works if the query and key matrices are 0, leading all attention coefficients\nfor a node u to be\n1\n|N (u)|, while the value matrix can be set to the identity matrix times c. The\nonly part where Exphormer lacks is the power of Θ, which does not fulfill the requirements in the\ntheorem. Other architectures like GRIT make up for this by using MLPs to encode embeddings (Ma\net al., 2023).\nWe further note that a similar statement can be made for rewiring techniques that change the graph’s\ntopology: Applying 1-WL to the rewired topology naturally leads to similar equivalence results.\nMotivated by the fact that MPNNs and GTs can be seen as applying a “convolution” to some neigh-\nborhood, we test how well traditional message-passing convolutions like GatedGCN perform on\nthe fully-connected graph and propose a localized variant of the GRIT attention mechanism that\nconsiders a local neighborhood.\n3.5\nSPARSE GRIT MESSAGE-PASSING CONVOLUTION\nGRIT introduces two main innovations: (1) A new attention mechanism that updates edge labels\non a fully connected graph and (2) RRWP as a positional encoding. While it is relatively easy to\nuse RRWP with both other message-passing and Graph Transformer architectures, we need some\nadaptions to use the GRIT attention mechanism with message-passing GNNs on sparse graphs.\nAs motivated earlier in Section 3.4, a Graph Transformer can be seen as message-passing on a\nfully-connected graph. Therefore, we generalize the GRIT attention mechanism designed for fully-\nconnected graphs to a message-passing convolution that works with any neighborhood. We call the\nresulting convolution Sparse GRIT, as it can attend to local neighborhoods on sparse graphs and does\nnot suffer from the quadratic computational overhead that the original GRIT mechanism has. This\nmakes sparse GRIT more efficient and scalable, as we further underline in our empirical evaluation\nin Section 5.2.\nSparse GRIT utilizes the same updating edge labels ˆei,j as the original, but only for edges that exist\nin the original graph. This further distinguishes the convolution from other popular local attention\nmechanisms like GAT. The main difference to GRIT lies in the update function ˆxi for nodes, which\nnow attend to their local neighborhood instead of all nodes in the graph. It becomes:\nˆxi =\nX\nj∈N (i)\newj·ˆei,j\nP\nk∈N (i) ewk·ˆei,k · (WV xj + WEV ˆei,j)\n(1)\nwhere wj is the attention weight, WV and WEV are weight matrices. In contrast to GRIT, the\nsummation is taken only over a node’s local neighborhood using the implementation of a sparse\nsoftmax. With these changes, sparse GRIT works the same as GRIT on a fully connected graph.\nWe, therefore, effectively transform the GRIT GT into an MPNN, which enables us to isolate and\nanalyze what impact the graph that is used for message-passing (fully connected vs. local) has.\nResults and empirical analysis of the sparse GRIT and GRIT are provided in Section 5.\n4\nBENCHMARKING POSITIONAL ENCODINGS\n4.1\nGENERAL FRAMEWORK\nThe general GNN framework we consider for our evaluation is depicted in Figure 1. More infor-\nmation on the employed datasets can be found in Appendix A.4. We describe the main components\nhere and give an overview over what methods were tested.\nDesign Space 1: Positional Encoding.\nAs specified in Section 3.1, we test three types of graph-\nbased positional encodings, treating them as node feature augmentations. More background for the\ndifferent encodings is given in Appendix 3.1\n5\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nInput Graph\nSparse\nDense\nConnection Type\nPositional Encoding\nGNN layer\nGraphGPS\nGatedGCN\nExphormer\nGRIT\nPrediction Head\nPostGNN\nMLP\nPooling\nMLP\nNode level\nGraph level\nRewired\nLaplacian-\nbased\nRandom-\nwalk based\nOther\nLapPE\nSignNet\nGCKN\nESLapPE\nWLPE\nRWSE\nRWDIFF\nRRWP\nGINE\nSparseGRIT\nPredictor\nLink level\nFigure 1: Overview of our evaluation framework, illustrating the preprocessing steps on the left\nand the GNN model architecture on the right. The framework allows for extensive experimentation\nwith various components, including positional encodings, connection types, and GNN layers. This\nmodular approach facilitates a comprehensive analysis of how different configurations impact model\nperformance. In our experimentation, we mainly focus on the positional encoding and GNN layer,\nwhile we also test different connection types.\nDesign Space 2: Connection Type.\nIn most real-world graph datasets, graphs tend to be sparse.\nThis means that message-passing on the original topology can potentially lead to a lack of global\ninformation exchange that is necessary for the task. To mitigate this issue, GTs usually employ\nfull-attention on the complete, fully-connected graph (as discussed in Section 3.4). To change the\ntopology that is used for the following GNN layer, we apply a classic MPNN to the fully-connected\ngraph to compare it to GTs and adapt the currently best-performing GT to run on the original graph\nwith our SparseGRIT convolution. While Exphormer implicitly applies some degree of rewiring,\nwe do not consider further rewiring approaches in this work to keep the number of comparisons at a\nreasonable level.\nDesign Space 3: GNN Layers.\nBased on the chosen topology, we apply several GNN layers and\nbenchmark their performance. On the MPNN side, we consider GINE, GatedGCN, and SparseGRIT,\nwhile we use GraphGPS, Exphormer, and GRIT as classical GTs. The architectures were chosen\ndue to the fact that they are widely used and currently perform best in leaderboards for the tasks we\nconsider. Other convolutions and transformer layers can easily be tested in our general framework.\nDesign Space 4: Prediction Heads.\nLastly, we need task-specific prediction heads that decode to\neither link level, node level, or graph level tasks for the datasets we consider. We use the same setup\nas popularized by GraphGPS (Ramp´aˇsek et al., 2022) and do not undertake further testing here.\n4.2\nBENCHMARKING FRAMEWORK\nTo enable the evaluation of models and future research for measuring the impact of positional\nencodings, we provide a unified codebase that includes the implementation of all tested models\nand the respective positional encodings. We base the code off GraphGPS Ramp´aˇsek et al. (2022)\nand integrate all missing implementations. This makes for reproducible results and easy exten-\nsibility for new datasets, models, or positional encodings. Our codebase further provides readily\navailable implementations for NodeFormer (Wu et al., 2022), Difformer (Wu et al., 2023), GOAT\n(Kong et al., 2023), GraphTrans (Wu et al., 2021), GraphiT (Mialon et al., 2021), and SAT (Chen\net al., 2022a) that are based on the respective original codebases. The code is publicly available at\nhttps://github.com/ETH-DISCO/Benchmarking-PEs.\nIn our experiments, we use five different random seeds for the BENCHMARKINGGNN (Dwivedi\net al., 2023) datasets and four for the others. The train-test split settings adhere to those estab-\nlished previously, employing a standard split ratio of 8:1:1. All experiments can be executed on\neither a single Nvidia RTX 3090 (24GB) or a single RTX A6000 (40GB). To avoid out-of-memory\n(OOM) issues on LRGB and OGB datasets, we ensure that 100GB of reserved CPU cluster memory\nis available when pre-transforming positional encodings. Configurations that did not fit into this\n6\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\n64\n66\n68\n70\n72\n74\n76\nMean Performance\nCIFAR10\n50\n55\n60\n65\n70\n75\n80\nCLUSTER\nno PE\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\nPATTERN\nno PE\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\n96.75\n97.00\n97.25\n97.50\n97.75\n98.00\n98.25\n98.50\nMNIST\nno PE\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nZINC\nFigure 2: Performance comparison of target metrics across selected datasets from BENCHMARK-\nINGGNN. The boxplots illustrate the performance range for all models included in the study, with\nwhiskers representing the minimum and maximum performance observed. Notably, RRWP consis-\ntently achieves the best results, whereas certain PEs, such as SignNet on CIFAR10, can sometimes\ndecrease performance relative to the baseline without PEs.\ncomputational envelope were not considered. The hyperparameters used for each architecture are\nprovided in the Appendix, as well as running times and memory usage for the PE pre-processing.\n5\nEVALUATION\nBased on the framework we established in Section 4.2, we benchmark the performance of differ-\nent PEs on the BENCHMARKINGGNN (Dwivedi et al., 2023) and LRGB (Dwivedi et al., 2022)\ndatasets. Results for ogbg-molhiv and ogbg-molpcba can be found in Appendix A.7.\n5.1\nBENCHMARKINGGNN DATASETS\nWe benchmark state-of-the-art models with commonly used PEs in-depth to identify the best config-\nurations. This analysis is often overlooked when new PEs are introduced alongside new architectures\nwithout being evaluated with existing models. Our approach decouples the architecture from the PE,\nallowing us to measure the full range of possible combinations. Our experimental evaluation starts\nwith a dataset-centric approach, examining the effect of various PEs on model performance. Fig-\nure 2 illustrates the range of values for the respective target metrics achieved by different PEs. These\nvalues are aggregated over all models in our analysis, while more detailed, unaggregated results are\navailable in Appendix A.7. Notably, while we could reproduce most results of previously tested\nmodel and PE combinations, we consistently observed slightly worse values for GRIT. This was the\ncase even when using the official codebase and the most up-to-date commit at the time of writing,\nwith provided configuration files intended to reproduce the results stated in the original paper.\nOur findings reveal that PEs can significantly influence model performance, with the best choice\nof PE varying depending on the dataset and task. However, PEs can also negatively impact per-\nformance in some cases. For instance, while RRWP performs best on the CIFAR10 dataset and\nZINC, there are not always clear winners. Sometimes, good performance can be achieved even\nwithout any positional encoding (e.g., for PATTERN). This is also evident when examining the\nbest-performing configurations for each model and PE. While the complete results for all runs are\n7\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nno PE\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\n0.50\n0.55\n0.60\n0.65\nMean Performance\nPeptides-func\nno PE\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nPeptides-struct\nno PE\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\n0.34\n0.36\n0.38\n0.40\n0.42\n0.44\nPascalVOC-SP\nFigure 3: Performance comparison of target metrics across selected datasets from the Long-Range\nGraph Benchmark. The boxplots illustrate the performance range of all models included in the\nstudy, with whiskers indicating the minimum and maximum performance observed. Plots for the\nremaining datasets are provided in Appendix A.7.\nprovided in Appendix A.7, we summarize the best-performing configurations for the BENCHMARK-\nINGGNN datasets in Table 1, indicating which PE led to the best performance for each model and\ndataset. This enables a fair comparison of all architectures and helps determine the optimal PE\noverall.\nIn our comparison, we observe that the sparse GRIT convolution emerges as the best graph con-\nvolution for sparse topologies. It competes effectively with the full GRIT attention across most\ndatasets. This suggests that these datasets do not require extensive long-range information exchange\nand can achieve strong performance with sparse message-passing. The GatedGCN convolution on\nthe fully-connected graph does perform better than the original overall, but generally lacks behind\nattention-based layers. Regarding the effectiveness of different PEs, random-walk-based encodings\nsuch as RRWP and RWSE consistently perform well across the tested models. The only notable\nexception is the CLUSTER dataset, where SignNet performs competitively for some architectures,\nalthough the best results are still achieved with RRWP.\n5.2\nLONG-RANGE GRAPH BENCHMARK\nWe extend our evaluation to the LRGB datasets and use hyperparameter configurations based on\nthose by T¨onshoff et al. (2023), with results presented in Table 2. In these datasets, Laplacian-based\nencodings generally outperform others (except for the Peptides variations), likely due to their ability\nto capture more global structure in the slightly larger graphs. This might also be reflected in the fact\nthat transformer-based architectures or models that facilitate global information exchange consis-\ntently perform better. Our findings largely align with previous rankings, except for PCQM-Contact,\nwhere we achieve a new state-of-the-art with Exphormer, which underscores the importance of thor-\nough benchmarking of existing models. Figure 3 further analyzes the performance of the employed\nPEs. It is noteworthy that RRWP could not be utilized for larger datasets due to its significant mem-\nory footprint and computational complexity, similar to models employing full attention mechanisms.\nThe results align with our previous analysis and show that on datasets like Peptides-func, the PE has\nTable 1: Results for the best-performing models and the PE they use for the BENCHMARKINGGNN\ndatasets. All runs except those for EGT and TIGT were done by us. SparseGRIT performs on par\nwith GRIT on most datasets, indicating that full attention might not be necessary for all of them. We\ncolor the best, second best, and third best models.\nModel\nCIFAR10 ↑\nCLUSTER ↑\nMNIST ↑\nPATTERN ↑\nZINC ↓\nEGT (Hussain et al., 2022)\n68.70 ± 0.41\n79.23 ± 0.35\n98.17 ± 0.09\n86.82 ± 0.02\n0.108 ± 0.009\nTIGT (Choi et al., 2024)\n73.96 ± 0.36\n78.03 ± 0.22\n98.23 ± 0.13\n86.68 ± 0.06\n0.057 ± 0.002\nGINE\n66.14 ± 0.31\n(ESLapSE)\n59.66 ± 0.63\n(SignNet)\n97.75 ± 0.10\n(RWDIFF)\n86.69 ± 0.08\n(RWSE)\n0.075 ± 0.006\n(RWDIFF)\nGatedGCN\n69.57 ± 0.79\n(RRWP)\n75.29 ± 0.05\n(SignNet)\n97.91 ± 0.08\n(RRWP)\n86.83 ± 0.03\n(RWSE)\n0.102 ± 0.003\n(RWSE)\nSparseGRIT\n74.95 ± 0.26 (RRWP)\n79.87 ± 0.08 (RRWP)\n98.12 ± 0.05\n(RWSE)\n87.17 ± 0.04 (RRWP) 0.065 ± 0.003 (RRWP)\nExphormer\n75.21 ± 0.10 (LapPE)\n78.28 ± 0.21\n(SignNet) 98.42 ± 0.18 (RRWP)\n86.82 ± 0.04\n(RWSE)\n0.092 ± 0.007\n(SignNet)\nGRIT\n75.66 ± 0.41 (RRWP)\n79.81 ± 0.11 (RRWP)\n98.12 ± 0.14\n(RRWP)\n87.22 ± 0.03 (RRWP) 0.059 ± 0.001 (RRWP)\nGatedGCN (FC)\n71.08 ± 0.60\n(RRWP)\n74.78 ± 0.46\n(SignNet) 98.20 ± 0.15 (GCKN)\n86.85 ± 0.02\n(RWSE)\n0.114 ± 0.003\n(RWSE)\nGraphGPS\n72.31 ± 0.20\n(noPE)\n78.31 ± 0.11\n(SignNet)\n98.18 ± 0.12\n(ESLapSE) 86.87 ± 0.01 (RWSE)\n0.074 ± 0.006\n(RWSE)\n8\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nTable 2: Best-performing models and PEs for the LRGB datasets. We achieve a new state-of-the-art\nfor PCQM-Contact.\nModel\nCOCO-SP ↑\nPCQM-Contact ↑\nPascalVOC-SP ↑\nPeptides-func ↑\nPeptides-struct ↓\nGCN (T¨onshoff et al., 2023)\n13.38 ± 0.07\n45.26 ± 0.06\n0.78 ± 0.31\n68.60 ± 0.50\n24.60 ± 0.07\nGINE (T¨onshoff et al., 2023)\n21.25 ± 0.09\n46.17 ± 0.05\n27.18 ± 0.54\n66.21 ± 0.67\n24.73 ± 0.17\nGatedGCN (T¨onshoff et al., 2023)\n29.22 ± 0.18\n46.70 ± 0.04\n38.80 ± 0.40\n67.65 ± 0.47\n24.77 ± 0.09\nCRaWl (T¨onshoff et al., 2021)\n-\n-\n45.88 ± 0.79\n70.74 ± 0.32\n25.06 ± 0.22\nS2GCN (Geisler et al., 2024)\n-\n-\n-\n73.11 ± 0.66\n24.47 ± 0.32\nDRew (Gutteridge et al., 2023)\n-\n34.42 ± 0.06\n33.14 ± 0.24\n71.50 ± 0.44\n25.36 ± 0.15\nGraph ViT (He et al., 2023)\n-\n-\n-\n68.76 ± 0.59\n24.55 ± 0.27\nGatedGCN-VN (Rosenbluth et al., 2024) 32.44 ± 0.25\n-\n44.77 ± 1.37\n68.23 ± 0.69\n24.75 ± 0.18\nExphormer\n34.85 ± 0.11 (ESLapPE)\n47.37 ± 0.24\n(LapPE)\n42.42 ± 0.44\n(LapPE)\n64.24 ± 0.63\n(LapPE)\n24.96 ± 0.13\n(LapPE)\nGraphGPS\n38.91 ± 0.33 (RWSE)\n46.96 ± 0.17\n(LapPE)\n45.38 ± 0.83\n(ESLapPE)\n66.20 ± 0.73\n(LapPE)\n24.97 ± 0.24\n(LapPE)\nSparseGRIT\n19.76 ± 0.38\n(noPE)\n45.85 ± 0.11\n(LapPE)\n35.19 ± 0.40\n(GCKN)\n67.02 ± 0.80\n(RRWP)\n24.87 ± 0.14\n(LapPE)\nGRIT\n21.28 ± 0.08\n(RWDIFF)\n46.08 ± 0.07\n(SignNet)\n35.56 ± 0.19\n(noPE)\n68.65 ± 0.50\n(RRWP) 24.54 ± 0.10 (RRWP)\na consistent impact on the performance, even when the values are aggregated over different archi-\ntectures. This impact can also be of a negative nature when compared to the baseline that does not\nuse any PE. On other datasets (for example PascalVOC-SP), the PE seems to play a lesser role and\ngood results can be achieved without any PE. The complete results are reported in Appendix A.7.\n5.3\nRUNNING TIME AND MEMORY COMPLEXITY FOR PES\nThe computational cost of positional encodings (PEs) is a critical consideration, particularly for\nlarge graphs where methods with high complexity quickly become infeasible. We evaluated the\nrunning time and memory usage for various PEs, and the full results are presented in Appendix A.8.\nRRWP is the most memory-intensive PE, but maintains reasonable running times.\nRWSE and\nRWDIFF, on the other hand, tend to have significantly longer running times but are relatively more\nmemory-efficient. Laplacian-based methods, such as LapPE and ESLapPE, offer a good balance be-\ntween computational speed and memory usage, making them practical even for larger datasets. PPR\nand GCKN come with high computational demands in both time and memory, making them less\nsuited for large-scale graphs. In contrast, Laplacian-based encodings like ESLapPE strike a better\ntrade-off, making them practical for a broader range of graph sizes while still offering competitive\nperformance.\n5.4\nGUIDELINES FOR PRACTICIONERS\nFor superpixel graph datasets, such as PascalVOC-SP, COCO-SP, MNIST, and CIFAR10, we found\nthat the inclusion of positional encodings generally does not result in substantial performance im-\nprovements. In particular, larger superpixel graphs like PascalVOC-SP and COCO-SP showed mini-\nmal to no gains from adding PEs, while MNIST similarly exhibited negligible benefits. An exception\nto this trend is CIFAR10, where RRWP demonstrated potential for enhancing model performance.\nThis suggests that while superpixel graphs may not typically benefit from positional encodings,\nRRWP could be considered as a candidate for improvement. However, the gains observed may not\nalways justify the increased computational complexity associated with RRWP for such datasets.\nIn contrast, molecular datasets, such as ZINC and the Peptides variations, displayed a strong de-\npendency on the choice of positional encoding, with significant variations in model performance\nbased on the PE used. For instance, ZINC consistently showed the best results with PPR. On the\nother hand, the Peptides datasets revealed task-specific preferences: Peptides-func benefited the\nmost from RRWP, while Peptides-struct achieved optimal performance with WLPE. Interestingly,\ndespite using identical graph structures, the two Peptides tasks favored different PEs, which indi-\ncates that the nature of the prediction target (functional vs. structural) plays a significant role. Thus,\nwhen dealing with molecular datasets, practitioners are advised to experiment with various PEs, as\nthe optimal choice may depend more on the specific task than on the graph structure itself. Still,\nthe optimal PE for a given dataset is generally consistent across different models, which, in com-\nbination with the fact that we test on commonly used datasets provides practitioners with a strong\nstarting point for their experiments. This distinction is further highlighted when comparing random-\nwalk-based encodings with Laplacian encodings, where one typically emerges as the clear winner\ndepending on the dataset and task.\n9\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\n6\nCONCLUSIONS\nThis study underscores the critical role of positional encodings in enhancing the performance of\nGraph Neural Networks (GNNs), particularly within Graph Transformer architectures. We con-\nducted a thorough comparison of various positional encodings across a wide array of state-of-the-art\nmodels and identify the optimal configurations for diverse datasets, as well as offer valuable insights\ninto the relationship between positional encodings and model performance. While we consolidated\nmuch of the current state-of-the-art, we also identified new configurations that surpass the perfor-\nmance of existing best models, such as Exphormer on PCQM-Contact. This underscores the neces-\nsity of in-depth comparisons to provide a fair and accurate ranking. Our theoretical considerations\nhave led to the development of the SparseGRIT model. This model shows competitive performance\nacross multiple benchmarks, while maintaining scalability to larger graphs. It shows that sparse\nmessage-passing together with the right positional encodings is a viable option on many datasets.\nFurthermore, we provide a comprehensive overview of the current state-of-the-art in graph learn-\ning and highlight the importance of selecting appropriate positional encodings to achieve optimal\nresults. Our unified codebase includes implementations of all tested models and encodings, which\nserves as a valuable resource for future research. The framework ensures reproducible results and\nsupports the integration of new datasets, models, and positional encodings, thereby facilitating fur-\nther experimentation.\nLimitations.\nDue to computational constraints, we could not explore all possible hyperparameter\nconfigurations and there might be slightly better performing ones that we did not catch. Additionally,\nalthough we tested a wide range of models and encodings, it is infeasible to test every model and PE.\nThis is why we focused on current state-of-the-art for both. Further, our evaluations are based on a\nspecific set of benchmark datasets, which may not fully represent the diversity of real-world graph\nstructures. Thus, performance on these benchmarks may not generalize to all types of graph data.\nNevertheless, our unified codebase serves as a robust foundation for further testing and development,\nand enables researchers to overcome these limitations by facilitating the inclusion of new datasets,\nmodels, and positional encodings.\n10\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nREFERENCES\nDominique Beaini, Saro Passaro, Vincent L´etourneau, Will Hamilton, Gabriele Corso, and Pietro\nLi`o. Directional graph networks. In International Conference on Machine Learning, pp. 748–\n758. PMLR, 2021.\nMitchell Black, Zhengchao Wan, Gal Mishne, Amir Nayyeri, and Yusu Wang. Comparing graph\ntransformers via positional encodings. arXiv preprint arXiv:2402.14202, 2024.\nDeyu Bo, Chuan Shi, Lele Wang, and Renjie Liao. Specformer: Spectral graph neural networks meet\ntransformers. In The Eleventh International Conference on Learning Representations, 2022.\nCristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and\nMichael Bronstein. Weisfeiler and lehman go cellular: Cw networks. Advances in neural infor-\nmation processing systems, 34:2625–2640, 2021.\nGiorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improving graph\nneural network expressivity via subgraph isomorphism counting. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 45(1):657–668, 2022.\nXavier Bresson and Thomas Laurent.\nResidual gated graph convnets.\narXiv preprint\narXiv:1711.07553, 2017.\nAndrey Bryutkin, Jiahao Huang, Zhongying Deng, Guang Yang, Carola-Bibiane Sch¨onlieb, and An-\ngelica Aviles-Rivero. Hamlet: Graph transformer neural operator for partial differential equations.\narXiv preprint arXiv:2402.03541, 2024.\nChen Cai, Truong Son Hy, Rose Yu, and Yusu Wang. On the connection between mpnn and graph\ntransformer. In International Conference on Machine Learning, pp. 3408–3430. PMLR, 2023.\nDexiong Chen, Leslie O’Bray, and Karsten Borgwardt. Structure-aware transformer for graph rep-\nresentation learning. In International Conference on Machine Learning, pp. 3469–3489. PMLR,\n2022a.\nJinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He. Nagphormer: A tokenized graph transformer\nfor node classification in large graphs. arXiv preprint arXiv:2206.04910, 2022b.\nPeng Chen. Permuteformer: Efficient relative position encoding for long sequences. In Proceed-\nings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10606–\n10618, 2021.\nYun Young Choi, Sun Woo Park, Minho Lee, and Youngho Woo. Topology-informed graph trans-\nformer. arXiv preprint arXiv:2402.02005, 2024.\nKrzysztof Choromanski, Han Lin, Haoxian Chen, Tianyi Zhang, Arijit Sehanobish, Valerii Likhosh-\nerstov, Jack Parker-Holder, Tamas Sarlos, Adrian Weller, and Thomas Weingarten. From block-\ntoeplitz matrices to differential equations on graphs: towards a general theory for scalable masked\ntransformers. In International Conference on Machine Learning, pp. 3962–3983. PMLR, 2022.\nGabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li`o, and Petar Veliˇckovi´c. Principal\nneighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems,\n33:13260–13271, 2020.\nVijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.\narXiv preprint arXiv:2012.09699, 2020.\nVijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.\nGraph neural networks with learnable structural and positional representations. arXiv preprint\narXiv:2110.07875, 2021.\nVijay Prakash Dwivedi, Ladislav Ramp´aˇsek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan\nLuu, and Dominique Beaini. Long range graph benchmark. Advances in Neural Information\nProcessing Systems, 35:22326–22340, 2022.\n11\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nVijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and\nXavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research,\n24(43):1–48, 2023.\nDongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao Wu, Andrey Malevich,\nJingrui He, and Bo Long. Vcr-graphormer: A mini-batch graph transformer via virtual connec-\ntions. arXiv preprint arXiv:2403.16030, 2024.\nJohannes Gasteiger, Aleksandar Bojchevski, and Stephan G¨unnemann.\nPredict then propagate:\nGraph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.\nSimon Geisler, Arthur Kosmala, Daniel Herbst, and Stephan G¨unnemann. Spatio-spectral graph\nneural networks. arXiv preprint arXiv:2405.19121, 2024.\nFlorian Gr¨otschla, Jo¨el Mathys, Robert Veres, and Roger Wattenhofer. Core-gd: A hierarchical\nframework for scalable graph visualization with gnns. arXiv preprint arXiv:2402.06706, 2024.\nBenjamin Gutteridge, Xiaowen Dong, Michael M Bronstein, and Francesco Di Giovanni. Drew: Dy-\nnamically rewired message passing with delay. In International Conference on Machine Learning,\npp. 12252–12267. PMLR, 2023.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\nAdvances in neural information processing systems, 30, 2017.\nXiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A\ngeneralization of vit/mlp-mixer to graphs. In International Conference on Machine Learning, pp.\n12724–12745. PMLR, 2023.\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure\nLeskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265,\n2019.\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,\nand Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances\nin neural information processing systems, 33:22118–22133, 2020.\nYinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, and Pan\nLi. On the stability of expressive positional encodings for graph neural networks. arXiv preprint\narXiv:2310.02579, 2023.\nMd Shamim Hussain, Mohammed J Zaki, and Dharmashankar Subramanian.\nEdge-augmented\ngraph transformers: Global self-attention is enough for graphs. arXiv preprint arXiv:2108.03348,\n3, 2021.\nMd Shamim Hussain, Mohammed J Zaki, and Dharmashankar Subramanian. Global self-attention\nas a replacement for graph convolution. In Proceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pp. 655–665, 2022.\nWei Ju, Siyu Yi, Yifan Wang, Zhiping Xiao, Zhengyang Mao, Hourun Li, Yiyang Gu, Yifang Qin,\nNan Yin, Senzhang Wang, et al. A survey of graph neural networks in real world: Imbalance,\nnoise, privacy and ood challenges. arXiv preprint arXiv:2403.04468, 2024.\nGuolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In\nInternational Conference on Learning Representations, 2020.\nNicolas Keriven and Samuel Vaiter. What functions can graph neural networks compute on random\ngraphs? the role of positional encoding. Advances in Neural Information Processing Systems, 36,\n2024.\nJinwoo Kim, Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon\nHong. Pure transformers are powerful graph learners. Advances in Neural Information Processing\nSystems, 35:14582–14595, 2022.\n12\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\nworks. arXiv preprint arXiv:1609.02907, 2016.\nRisi Kondor and Jean-Philippe Vert. Diffusion kernels. kernel methods in computational biology,\npp. 171–192, 2004.\nKezhi Kong, Jiuhai Chen, John Kirchenbauer, Renkun Ni, C Bayan Bruss, and Tom Goldstein. Goat:\nA global transformer on large-scale graphs. In International Conference on Machine Learning,\npp. 17375–17390. PMLR, 2023.\nDevin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L´etourneau, and Prudencio Tossou. Re-\nthinking graph transformers with spectral attention. Advances in Neural Information Processing\nSystems, 34:21618–21629, 2021.\nWeirui Kuang, WANG Zhen, Yaliang Li, Zhewei Wei, and Bolin Ding. Coarformer: Transformer\nfor large graph via graph coarsening. 2021.\nPan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably\nmore powerful neural networks for graph representation learning. Advances in Neural Information\nProcessing Systems, 33:4465–4478, 2020.\nYi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic\ngraphs. arXiv preprint arXiv:2206.11990, 2022.\nDerek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie\nJegelka.\nSign and basis invariant networks for spectral graph representation learning.\narXiv\npreprint arXiv:2202.13013, 2022.\nJiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang,\nLichao Sun, Philip S Yu, et al. Towards graph foundation models: A survey and beyond. arXiv\npreprint arXiv:2310.11829, 2023.\nShengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He.\nOne transformer can understand both 2d & 3d molecular data. In The Eleventh International\nConference on Learning Representations, 2022a.\nShengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. Your transformer\nmay not be as powerful as you expect. Advances in Neural Information Processing Systems, 35:\n4301–4315, 2022b.\nLiheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet K Dokania, Mark Coates,\nPhilip Torr, and Ser-Nam Lim. Graph inductive biases in transformers without message passing.\nIn International Conference on Machine Learning, pp. 23321–23337. PMLR, 2023.\nHaitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Michael\nGalkin, and Jiliang Tang. Graph foundation models. arXiv preprint arXiv:2402.02216, 2024.\nHaggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph\nnetworks. Advances in neural information processing systems, 32, 2019.\nDominic Masters, Josef Dean, Kerstin Klaser, Zhiyi Li, Sam Maddrell-Mander, Adam Sanders,\nHatem Helal, Deniz Beker, Ladislav Ramp´aˇsek, and Dominique Beaini. Gps++: An optimised\nhybrid mpnn/transformer for molecular property prediction. arXiv preprint arXiv:2212.02229,\n2022.\nGr´egoire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph\nstructure in transformers, 2021.\nChristopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav\nRattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.\nIn Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 4602–4609, 2019.\nLuis M¨uller and Christopher Morris. Aligning transformers with weisfeiler-leman. arXiv preprint\narXiv:2406.03148, 2024.\n13\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nWonpyo Park, Woonggi Chang, Donggeon Lee, Juntae Kim, and Seung-won Hwang. Grpe: Relative\npositional encoding for graph transformer. arXiv preprint arXiv:2201.12787, 2022.\nLadislav Ramp´aˇsek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Do-\nminique Beaini. Recipe for a general, powerful, scalable graph transformer. Advances in Neural\nInformation Processing Systems, 35:14501–14515, 2022.\nEran Rosenbluth, Jan T¨onshoff, Martin Ritzert, Berke Kisin, and Martin Grohe. Distinguished in\nuniform: Self attention vs. virtual nodes. arXiv preprint arXiv:2405.11951, 2024.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-\ntions. In Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp.\n464–468, 2018.\nHamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J Sutherland, and Ali Kemal\nSinop. Exphormer: Sparse transformers for graphs. In International Conference on Machine\nLearning, pp. 31613–31632. PMLR, 2023.\nJan T¨onshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe. Walking out of the weisfeiler leman\nhierarchy: Graph learning beyond message passing. arXiv preprint arXiv:2102.08786, 2021.\nJan T¨onshoff, Martin Ritzert, Eran Rosenbluth, and Martin Grohe. Where did the gap go? reassess-\ning the long-range graph benchmark. arXiv preprint arXiv:2309.00367, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nPetar Veliˇckovi´c. Everything is connected: Graph neural networks. Current Opinion in Structural\nBiology, 79:102538, 2023.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\nHanrui Wang, Pengyu Liu, Jinglei Cheng, Zhiding Liang, Jiaqi Gu, Zirui Li, Yongshan Ding, Wei-\nwen Jiang, Yiyu Shi, Xuehai Qian, et al. Graph transformer for quantum circuit reliability predic-\ntion. 2022a.\nHaorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and stable positional encoding\nfor more powerful graph neural networks. arXiv preprint arXiv:2203.00199, 2022b.\nQitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. Nodeformer: A scalable graph\nstructure learning transformer for node classification. Advances in Neural Information Processing\nSystems, 35:27387–27401, 2022.\nQitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi Yan.\nDif-\nformer: Scalable (graph) transformers induced by energy constrained diffusion. arXiv preprint\narXiv:2301.09474, 2023.\nZhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion Stoica.\nRepresenting long-range context for graph neural networks with global attention. Advances in\nNeural Information Processing Systems, 34:13266–13279, 2021.\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A\ncomprehensive survey on graph neural networks. IEEE transactions on neural networks and\nlearning systems, 32(1):4–24, 2020.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\nHow powerful are graph neural\nnetworks? In International Conference on Learning Representations, 2018.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and\nTie-Yan Liu. Do transformers really perform badly for graph representation? Advances in neural\ninformation processing systems, 34:28877–28888, 2021.\n14\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and\nAlexander J Smola. Deep sets. Advances in neural information processing systems, 30, 2017.\nZiwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. Graph meets\nllms: Towards large graph models. In NeurIPS 2023 Workshop: New Frontiers in Graph Learn-\ning, 2023.\nLingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any gnn\nwith local structure awareness. arXiv preprint arXiv:2110.03753, 2021.\nWeiheng Zhong and Hadi Meidani. A graph transformer for symbolic regression.\nCai Zhou, Rose Yu, and Yusu Wang. On the theoretical expressive power and the design space\nof higher-order graph transformers. In International Conference on Artificial Intelligence and\nStatistics, pp. 2179–2187. PMLR, 2024.\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,\nChangcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applica-\ntions. AI open, 1:57–81, 2020.\n15\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nA\nAPPENDIX\nA.1\nEXTENDED RELATED WORK\nPositional Encodings for Graphs.\nPositional encodings are traditionally used in natural language\nprocessing to capture the absolute position of a token within a sentence (Vaswani et al., 2017) or the\nrelative distance between pairs of tokens (Shaw et al., 2018; Ke et al., 2020; Chen, 2021). Similarly,\npositional encoding in graphs aims to learn both local topology and global structural information\nof nodes efficiently. This approach has been successfully implemented with the introduction of\nthe graph transformer (Dwivedi & Bresson, 2020). With the advent of graph transformers in the\nfield of graph representation learning, many traditional graph theory methods have been revitalized.\nGraph signal processing techniques have been employed such as Laplacian decomposition and fi-\nnite hop random walks (Ramp´aˇsek et al., 2022; Dwivedi et al., 2023; Ma et al., 2023; Beaini et al.,\n2021; Dwivedi & Bresson, 2020; Kreuzer et al., 2021; Dwivedi et al., 2021; Lim et al., 2022; Wang\net al., 2022b) as absolute or relative positional encoding. Node properties such as degree central-\nity (Ying et al., 2021) and personalized PageRank (PPR) (Gasteiger et al., 2018; Fu et al., 2024)\ncould be mapped and expanded into higher dimensions for absolute positional encoding, while the\nshortest distance between nodes could be used for relative positional encoding (Li et al., 2020; Ying\net al., 2021). Recent studies have focused on developing learnable positional encodings for graphs\n(Ying et al., 2021) and exploring their expressiveness and stability as well (Wang et al., 2022b; Ma\net al., 2023; Huang et al., 2023). Additionally, graph rewiring combined with layout optimization to\ncoarsen graphs has been proposed as a form of positional encoding (Gr¨otschla et al., 2024).\nGNN Benchmarking.\nOne of the first GNN benchmarking papers compared architectures with\nand without positional encodings (PEs) (Dwivedi et al., 2023), where their PE mainly refers to Lapla-\ncian positional encoding (LapPE). Their study was limited to the GatedGCN model and discussed\nthe expressive power, robustness, and efficiency of state-of-the-art message-passing methods. Addi-\ntionally, several surveys have benchmarked the complexity, specific tasks, unified message-passing\nframeworks (Wu et al., 2020; Zhou et al., 2020), robustness, and privacy (Ju et al., 2024). The LRGB\ndataset (Dwivedi et al., 2022) has been tested in both GNNs and transformers to demonstrate the su-\nperiority of Graph Transformers (GTs) over Message Passing Neural Networks (MPNNs). Many\nstate-of-the-art GTs have included this benchmark in their experiments (Ramp´aˇsek et al., 2022;\nShirzad et al., 2023; Ma et al., 2023). One limitation of the LRGB benchmark is that LRGB only\nconsiders LapPE and random walk structural encodings (RWSE). One notable work benchmarked\nusing LRGB by fine-tuning the architectures of GraphGPS and pre-processing (T¨onshoff et al.,\n2023). We adopt their settings but place greater emphasis on the effect of positional encodings.\nA.2\nPROOFS\nLemma 3.1 (Adapted from Corollary 6 by Xu et al. (2018)). Assume X is a countable set. There\nexists a function f : X →Rn so that for infinitely many choices of ϵ, including all irrational num-\nbers, h(c, X) = f(c) + P\nx∈X f(x) is unique for each pair (c, X), where c ∈X and X ⊆X\nis a multiset of bounded size. Moreover, any function g over such pairs can be decomposed as\ng(c, X) = φ\n\u0000(f(c) + (1 + ϵ) P\nx∈X f(x)\n\u0001\nfor some function φ.\nProof of Lemma 3.1. We slightly tightly follow the proof by Xu et al. (2018) for Corollary 6, but\ndefine h as h(c, X) ≡f(c) + (1 + ϵ) P\nx∈X f(x) (with f defined as in the original proof). We then\nwant to show that for any (c′, X′) ̸= (c, X) with c, c′ ∈X and X, X′ ⊂X, h(c, X) ̸= h(c′, X′)\nholds, if ϵ is an irrational number. We show the same contradiction as Xu et al. (2018): For any\n(c, X), suppose there exists (c′, X′) such that (c′, X′) ̸= (c, X) but h(c, X) = h(c′, X′) holds.\nWe consider the following two cases: (1) c′ = c but X′ ̸= X, and (2) c′ ̸= c. For the first case,\nh(c, X) = h(c, X′) implies P\nx∈X f(x) = P\nx∈X′ f(x). By Lemma 5 from Xu et al. (2018) it\nfollows that equality will not hold. For the second case, we can rewrite h(c, X) = h(c′, X′) as the\nfollowing equation:\nϵ ·\n X\nx∈X\nf(x) −\nX\nx∈X′\nf(x)\n!\n=\n \nf(c′) +\nX\nx∈X′\nf(x)\n!\n−\n \nf(c) +\nX\nx∈X\nf(x)\n!\n16\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nWe assume ϵ to be irrational, and if P\nx∈X f(x) −P\nx∈X′ f(x) ̸= 0, then the left side of the\nequation is irrational, while the right side is rational. If P\nx∈X f(x) −P\nx∈X′ f(x) = 0, then the\nequation reduces to f(c′) = f(c), also a contradiction.\nTheorem 3.2. Let G = (V, E) be a graph with node embeddings cv for nodes v ∈V . A GT layer\non the dependency graph G′ = (V, E′) can map nodes v1, v2 ∈V to different embeddings only if\nthe 1-WL algorithm using E′ assigns different labels to nodes v1 and v2. For equivalence, we need δ\n(in the definition of GTs) to be injective and αu,v = c for a given constant c ∈R and all (u, v) ∈E′,\nmaking the GT as expressive as the 1-WL algorithm.\nProof of Theorem 3.2. First, we show that a GT is bounded by 1-WL on the same topology by\nshowing that 1-WL is at least as powerful as a graph transformer. As 1-WL hashes all neighbor\nstates with an injective function, we can observe states from all nodes in the graph in the aggregated\nmultiset at node v, including possible edge labels. This information is sufficient to compute the\nresult of the attention module at every node.\nFor the other direction, we can make use of Lemma 3.1 by setting c to the desired ϵ and follow the\nsame proof as (Xu et al., 2018). Note that Θ and δ have to be powerful enough such that we can\napply the universal approximation theorem.\nA.3\nTESTED GNN ARCHITECTURES\nMessage Passing Neural Networks (MPNN).\nFor message passing neural networks, we primar-\nily choose GatedGCN (Bresson & Laurent, 2017) and GINE (Hu et al., 2019). The message-passing\nupdate rule for GateGCN is as follows:\nxℓ+1\ni\n= f ℓ\nG-GCNN\n\u0000xℓ\ni, {xℓ\nj : j →i}\n\u0001\n= ReLU\n\nU ℓxℓ\ni +\nX\nj→i\nηij ⊙V ℓxℓ\nj\n\n\n(2)\nwhere xj, j ∈N(i) are node features, and ηij are edge gates which are employed by ηij =\nσ\n\u0000Aℓxℓ\ni + Bℓxℓ\nj\n\u0001\n. The update for GINE is defined as follows:\nx′\ni = hΘ\n\n(1 + ϵ) · xi +\nX\nj∈N (i)\nReLU(xj + ei,j)\n\n\n(3)\nwhere ϵ is a hyper-parameter as specified in GIN paper, edge information ei,j is injected into indi-\nvidual node features, and MLP h(·) is parameterized by Θ.\nOur rationale is as follows:\n1. When observing popular MPNNs such as DGN (Beaini et al., 2021), PNA (Corso et al.,\n2020), and GSN (Bouritsas et al., 2022), they are not consistently scalable or tested on\nmedium-scale datasets as thoroughly as classical MPNNs like GatedGCN and GINE , as\nindicated in GraphGPS (Ramp´aˇsek et al., 2022). This limitation is also evident in current\nstate-of-the-art graph neural networks like CIN (Bodnar et al., 2021) and GIN-AK+ (Zhao\net al., 2021), which lack reported results on large-scale graphs, such as most datasets from\nthe Open Graph Benchmark (Hu et al., 2020; Ramp´aˇsek et al., 2022).\n2. Most graph transformers incorporate edge information, making direct comparisons to\nGNNs without edge information unfair.\nGraph convolution networks (GCN) (Kipf &\nWelling, 2016) and Graph Isomorphism Network (GIN) (Xu et al., 2018) potentially lack\nthese updates. Hence, we use modified convolutional graph filters that include edge at-\ntributes in message passing, specifically GatedGCN and GINE.\n3. We aim to investigate if the results from GatedGCN on fully connected graphs are com-\nparable to those from GT on sparse graphs. In addition to the above points, an improved\nGatedGCN architecture has been found to perform on par with GT on the peptides-func\nand peptides-struct datasets (T¨onshoff et al., 2023). This finding motivates us to explore\nthe effects of positional encodings on the GatedGCN architecture.\n17\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nBy focusing on GatedGCN and GINE, we aim to leverage their established scalability and perfor-\nmance on medium to large-scale datasets, while fairly comparing their edge attribute capabilities to\ngraph transformers.\nGraph Transformers.\nCurrent Graph Transformers can be divided into two families:\n• GTs with only full attention layers. This family includes the following GT(s): EGT (Hus-\nsain et al., 2021), GKAT (Choromanski et al., 2022), GRIT (Ma et al., 2023), NAGphormer\n(Chen et al., 2022b) , Vanilla GT (Dwivedi & Bresson, 2020), GraphiT (Mialon et al.,\n2021), GRPE (Park et al., 2022), SignNet (Lim et al., 2022), SAN (Kreuzer et al., 2021),\nSpecformer (Bo et al., 2022), TokenGT (Kim et al., 2022), and Transformer-M (Luo et al.,\n2022a).\n• GTs with additional message passing layers as an inductive bias. This family includes\nthe following GT(s): Coarformer (Kuang et al., 2021), Equiformer (Liao & Smidt, 2022),\nExphormer (Shirzad et al., 2023), GOAT (Kong et al., 2023), GraphGPS (Ramp´aˇsek et al.,\n2022), Graphormer (Ying et al., 2021), GPS++ (Masters et al., 2022), GraphTrans (Wu\net al., 2021), SAT (Chen et al., 2022a), NodeFormer (Wu et al., 2022), and URPE (Luo\net al., 2022b).\nIn this research, we select GRIT from the first class, and GraphGPS and Exphormer from the second\nclass. Other models can also be classified into one of these two categories.\nGraphGPS Update.\nAs specified in the original paper, the model follows a pattern where the\noutput from global attention layers interacts with the output from a global attention (vanilla trans-\nformer) layer. In this context, X represents the node features, E represents the edge features, and A\nis the adjacency matrix.\nˆXℓ+1\nM , Eℓ+1 = MPNNℓ\ne(Xℓ, Eℓ, A),\nˆXℓ+1\nT\n= GlobalAttnℓ(Xℓ),\nXℓ+1\nM\n= BatchNorm\n\u0010\nDropout\n\u0010\nˆXℓ+1\nM\n\u0011\n+ Xℓ\u0011\n,\nXℓ+1\nT\n= BatchNorm\n\u0010\nDropout\n\u0010\nˆXℓ+1\nT\n\u0011\n+ Xℓ\u0011\n,\nXℓ+1 = MLPℓ\u0000Xℓ+1\nM\n+ Xℓ+1\nT\n\u0001\nExphormer Update.\nAs specified in the Exphormer paper and observed from its implementation,\nthe model follows a training pattern similar to that used in SAN (Kreuzer et al., 2021):\nATTNH(X):,i = xi +\nh\nX\nj=1\nWj\nOWj\nV XNH(i) · σ\n\u0012\u0010\nWj\nEENH(i) ⊙Wj\nKXNH(i)\n\u0011T \u0010\nWj\nQxi\n\u0011\u0013\nwhere X is the node features, and E is the edge features. The most important aspect is that they\ncompute the local sparse attention mechanism using 1) virtual nodes and 2) expander graphs.\nA.4\nDATASETS\nStatistics and prediction tasks are listed in Table 3. Licenses for each datasets are listed in Table 4.\nBenchmarkingGNN include MNIST, CIFAR10, CLUSTER, PATTERN, and ZINC, following the\nprotocols established in GraphGPS (Ramp´aˇsek et al., 2022), Exphormer (Shirzad et al., 2023), and\nGRIT (Ma et al., 2023). These datasets have traditionally been employed for benchmarking Graph\nNeural Networks (GNNs) (Dwivedi et al., 2023), excluding graph transformers. In this paper, we ad-\nhere to these established settings but aim to revisit both message passing neural networks (MPNNs)\nand graph transformers.\nLong-Range Graph Benchmark (LRGB) (Dwivedi et al., 2022) encompasses Peptides-func,\nPeptides-struct, PascalVOC-SP, PCQM-Contact, and COCO. Graph learning in this context is heav-\nily influenced by the interactions between pairs of long-range vertices. Prior research has explored\n18\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nthe potential optimal hyperparameters for both MPNNs and GTs within the LRGB framework\n(T¨onshoff et al., 2023). Our objective is to identify the most effective combination of GNN ar-\nchitectures and positional encoding strategies.\nOpen Graph Benchmark (OGB) (Hu et al., 2020) includes: 1) node-level tasks like OGBN-Arxiv\nand 2) graph-level tasks like OGBG-MOLHIV and OGBG-MOLPCBA. These datasets are consider-\nably larger in scale compared to the aforementioned benchmarks. Our goal is to discover scalable\npositional encoding methods, as conventional graph Laplacian decomposition for positional encod-\ning is not feasible for large graphs.\nTable 3: Statistics for each dataset\nDataset\n# Graphs\nAvg. |N |\nAvg. |E|\nDirected\nPrediction level\nPrediction task\nMetric\nZINC\n12,000\n23.2\n24.9\nNo\ngraph\nregression\nMean Abs. Error\nMNIST\n70,000\n70.0\n564.5\nYes\ngraph\n10-class classif.\nAccuracy\nCIFAR10\n60,000\n117.6\n941.1\nYes\ngraph\n10-class classif.\nAccuracy\nPATTERN\n14,000\n118.9\n2,359.2\nNo\ninductive node\nbinary classif.\nAccuracy\nCLUSTER\n12,000\n117.2\n1,510.9\nNo\ninductive node\n6-class classif.\nAccuracy\nPascalVOC-SP\n11,355\n479.4\n2,710.5\nNo\ninductive node\n21-class classif.\nF1 score\nCOCO-SP\n123,286\n476.4\n2,693.7\nNo\ninductive node\n81-class classif.\nF1 score\nPCQM-Contact\n529,434\n30.1\n69.1\nNo\ninductive link\nlink ranking\nMRR (Fil.)\nPeptides-func\n15,535\n150.9\n307.3\nNo\ngraph\n10-task classif.\nAvg. Precision\nPeptides-struct\n15,535\n150.9\n307.3\nNo\ngraph\n11-task regression\nMean Abs. Error\nogbn-arxiv\n1\n169,343\n1,166,243\nYes\ntransductive node\n40-class classif.\nAccuracy\nogbg-molhiv\n41,127\n25.5\n27.5\nNo\ngraph\nbinary classif.\nAUROC\nogbg-molpcba\n437,929\n26.0\n28.1\nNo\ngraph\n128-task classif.\nAvg. Precision\nIn future work, we hope to add more large-scale inductive datasets such as OGBG-PPA, OGBG-\nCode2 and PCQM4Mv2 (Hu et al., 2020), and transductive datasets such as CS, Physics and Com-\nputer and Photo (Shirzad et al., 2023) into comparison.\nA.5\nPOSITIONAL ENCODINGS\nA.5.1\nLAPLACIAN BASED METHODS\nTable 4: Dataset licenses.\nDataset\nLicense\nZINC\nMIT License\nMNIST\nCC BY-SA 3.0 and MIT License\nCIFAR10\nCC BY-SA 3.0 and MIT License\nPATTERN\nMIT License\nCLUSTER\nMIT License\nPascalVOC-SP\nCustom license and CC BY 4.0 License\nCOCO-SP\nCustom license and CC BY 4.0 License\nPCQM-Contact\nCC BY 4.0 License\nPeptides-func\nCC BY-NC 4.0 License\nPeptides-struct\nCC BY-NC 4.0 License\nogbn-arxiv\nMIT License\nogbg-molhiv\nMIT License\nogbg-molpcba\nMIT License\nWe define L as the Laplacian matrix for our in-\nput graph G = (V, E) . According to graph\ntheory, as it’s positive semidefinite and sym-\nmetric, it could be further decomposed as L =\nP\ni λiuiuT\ni , where λi is the eigenvalue and ui\nis the eigenvector. Under a unified scheme of\npositional encoding for graph neural networks,\nwe define a normalized graph Laplacian L =\nI −D−1\n2 AD−1\n2 = U T ΛU where i-th row of\nU corresponds to the graph’s i-th eigenvector\nui, and Λ is a diagonal matrix containing all\neigenvalues.\nUnder the Laplacian-based set-\ntings, we could express each positional encod-\ning for node k in a similar way by:\nXk\nPE = f (Uk,:, Λ, Θ, {·})\n(4)\nwhere Uk,: represents the i-th row of U, Λ is a diagonal matrix containing all eigenvalues, Θ is the\nfunction parameters which represent the linear or non-linear operations on U and Λ, and {·} is the\nadditional parameters that are utilized by each method individually. We consider three Laplacian-\nbased methods: Laplacian Positional Encoding (LapPE), Sign-Invariant Positional Encoding (Sign-\nNet), and rectified Graph Convolution Kernel Network-based Positional Encoding (GCKN).\nLapPE (Ramp´aˇsek et al., 2022)\nLapPE, or Laplacian Positional Encoding, is a method that lever-\nages the eigenvectors of the graph Laplacian to encode positional information for nodes. The core\nidea is that the eigenvectors corresponding to higher eigenvalues contain more information about\nthe local structure of the graph, especially the relationships between a node and its neighbors. We\ncan further concatenate the eigenvectors with their corresponding eigenvalue. In the actual imple-\nmentation, an additional parameter S is employed to randomly split the sign of this concatenated\n19\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\neigenvector. Subsequently, we apply either a DeepSet (Zaheer et al., 2017) or an MLP Φ parameter-\nized by Θ to this eigenvector.\nXk\nPE = f (Uk,:, Λ, Θ, S)\n(5)\n= ΦΘ (S ⊙(Uk,: ∥Λk))\n(6)\nThe equivariant and stable version of LapPE (ESLapPE) (Wang et al., 2022b) follows the same\nprocedure but omits the post-processing MLP.\nSignNet (Lim et al., 2022)\nSignNet is an advanced version of LapPE, which considers both the\noriginal eigenvector and its inversely signed counterparts.\nAn additional graph neural network\n(GNN) is applied to capture local Laplacian signals before passing them to the MLP. The out-\nputs from two distinct or shared GNNs are then added together. This approach is proven to be\nsign-invariant and capable of approximating any continuous function of eigenvectors with the de-\nsired invariances (Lim et al., 2022). While we did not consider BasisNet in this work, researchers\ncould further explore its inclusion in their studies for comparison with SignNet. The expression for\nSignNet is:\nXk\nPE = f (Ui,:, Λ, Θ, A)\n(7)\n= ΦΘ (GNN(Uk,: ∥Λk, A) + GNN((−1) ⊙(Uk,: ∥Λk), A))\n(8)\nGCKN (Mialon et al., 2021)\nAccording to GraphiT (Mialon et al., 2021), there are two ways to\nconstruct the new graph Laplacian matrix, either by using diffusion kernels (GCKN) or a p-step\nrandom walk kernel (p-RWSE). Here, we introduce the diffusion kernel (p-step is similar) where the\nnew Laplacian is computed by multiplying by inverse β and then placed onto the exponential of e.\nU is the new eigenvector matrix. The method is similar to LapPE, but the additional parameter β is\nused to control the diffusion process (Kondor & Vert, 2004). The expression for GCKN is given by:\nXk\nPE = f (Uk,:, Λ, Θ, {S, β})\n(9)\n= ΦΘ (S ⊙(Uk,: ∥Λk)) , where U T\nk ΛUk = e−βL\n(10)\nA.5.2\nRANDOM WALK BASED METHODS\nWe denote p as a polynomial function. From the following settings, we can see that this class of\nmethods takes a polynomial of D−1A, which is:\nXk\nPE = p(D, A, {·})\n(11)\nRWSE (Ramp´aˇsek et al., 2022)\nRandom Walk Structural Encoding (RWSE) encodes the graph\nstructure by computing the frequencies of random walks starting from each node. Specifically,\nthe RWSE method calculates the probabilities of nodes being visited at each step of the random\nwalk. This approach utilizes the polynomial (D−1A)k, where D is the degree matrix and A is the\nadjacency matrix, to represent the result of a k-step random walk. For the biased version, weights\nθk are used to weight the results of each step. The formula is as follows:\nXk\nPE = p(D, A, K)\n(12)\n=\nK\nX\nk=1\n(D−1A)k, or =\nK\nX\nk=1\nθk(D−1A)k if biased\n(13)\nRWDIFF (LSPE) (Dwivedi et al., 2021)\nLearnable positional encoding, on the other hand, can\nencode positions through random walk diffusion and decouples structural and positional encoding\n(Dwivedi et al., 2021). Unlike RWSE, RWDIFF concatenates the random walk diffusion features\nfrom each time step (new dimension), while RWSE directly adds those k-step random walk matrices.\nThe initial condition where no random walk is performed is also considered, with the additional\nparameter I, which is the identity matrix. The formula is as follows:\nXk\nPE = p(D, A, {K, I})\n(14)\n= [I, D−1A, (D−1A)2, (D−1A)3, ..., (D−1A)K−1]k,k\n(15)\n= Ik,k ∥(D−1A)k,k ∥(D−1A)2\nk,k ∥(D−1A)3\nk,k ∥... ∥(D−1A)K−1\nk,k\n(16)\n20\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nRRWP (Ma et al., 2023)\nOne improvement on RWDIFF in GRIT (Ma et al., 2023) is that the\nsparse graph is connected to a fully connected graph for each graph in the batch, as well as an\naddition update in the edge features where it has shown a better performance on LRGB benchmark.\nIt is stated that it is at least as expressive as a biased RWSE (Ma et al., 2023). The form of Xk\nPE\nis not changed here, instead we mention the edge attributes (as the structural encoding). The edge\nfeature is also indicated by random walk diffusion, however, the off-diagonal entry indicates the\nedge features, which is represented by a form of probability from node i to j:\nPi,j = [I, D−1A, (D−1A)2, (D−1A)3, ..., (D−1A)K−1]i,j\n(17)\n= Ii,j ∥(D−1A)i,j ∥(D−1A)2\ni,j ∥(D−1A)3\ni,j ∥... ∥(D−1A)K−1\ni,j\n(18)\nPPR (Gasteiger et al., 2018)\nPersonalized PageRank (PPR) propagation is an approximate and\nfaster propagation scheme under message passing (Gasteiger et al., 2018). For each node, its PageR-\nank is given in its analytical form as:\nXk\nPE = p(D, A, {α, |V|})\n(19)\n= α\n\u0000I|V| −(1 −α)D−1A\n\u0001−1 ik\n(20)\nwhere ik is the indicator function, and α controls the distance from the root node. It is considered\none of the positional encodings in GRIT (Ma et al., 2023), which is strictly less powerful than\nRRWP. From its analytical solution, it is also classified under random walk-based methods since the\nfunction is inversely related to D−1A.\nA.5.3\nOTHER METHODS\nWLPE (Dwivedi & Bresson, 2020)\nThe Weisfeiler-Lehman Positional Encoding (WLPE)\nmethod, as introduced by Dwivedi & Bresson (2020), leverages the Weisfeiler-Lehman (WL) graph\nisomorphism test to generate positional encodings for nodes in a graph. Firstly, the hashed node\nfeature for node k X′\nk is updated by using a hash function that combines the node’s own feature Xk\nwith the features of its neighbors:\nX′\nk = hash (Xk, {Xu : u ∈N(v), v ∈V})\n(21)\nHere, N(v) denotes the neighborhood of node v, and V is the set of all nodes in the graph. Secondly,\nthe positional encoding Xk\nPE is generated by applying a function f to X′\nk and the hidden dimension\ndh:\nXk\nPE = f(X′\nk, dh)\nThe function f typically involves sinusoidal transformations to embed the positional information\ninto a continuous vector space. This transformation is detailed as follows:\nXk\nPE =\n\u0014\nsin\n\u0012\nX′\nk\n10000\n2l\ndh\n\u0013\n, cos\n\u0012\nX′\nk\n10000\n2l+1\ndh\n\u0013\u0015\nj dh\n2\nk\nl=0\nIn this expression:\n• X′\nk is the hashed feature of node k.\n• dh is the hidden dimension, controlling the size of the positional encoding.\n• l ranges from 0 to\n\u0004 dh\n2\n\u0005\n, ensuring that the resulting vector has dh dimensions.\nA.6\nMODEL CONFIGURATIONS\nWe provide the model configuration here to ensure reproducibility.\nBENCHMARKINGGNN\nFor BENCHMARKINGGNN, we adhere to established settings from rel-\nevant literature for each model. Specifically, for the GatedGCN and GraphGPS models, we follow\nthe configurations detailed in the GraphGPS paper (Ramp´aˇsek et al., 2022). For the Exphormer\nmodel, we utilize the settings from the Exphormer paper (Shirzad et al., 2023). For the GINE,\nSparse GRIT, and Global GRIT models, we adopt the configurations from the GRIT paper (Ma\net al., 2023). We provide five tables, one for each dataset, to ensure comprehensive coverage of the\nBENCHMARKINGGNN. Unlike the GraphGPS paper, which fixed the positional encoding, we will\nreport the statistics of the computation of positional encoding in separate tables. Configurations are\nlisted from table 5 to table 9.\n21\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nTable 5: Model Configurations for MNIST\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.0\n-\n4\n52\nmean\n16\n100\n-\n8\nGINE\n0.001\n0.0\n-\n3\n52\nmean\n16\n150\nBatchNorm\n18\nGraphGPS\n0.001\n0.0\n4\n3\n52\nmean\n16\n100\nBatchNorm\n18\nExphormer\n0.001\n0.1\n4\n5\n40\nmean\n16\n150\nBatchNorm\n8\nGRITSparseConv\n0.001\n0.0\n-\n3\n52\nmean\n16\n150\nBatchNorm\n18\nGRIT\n0.001\n0.0\n4\n3\n52\nmean\n16\n150\nBatchNorm\n18\nTable 6: Model Configurations for CIFAR10\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.0\n-\n4\n52\nmean\n16\n100\n-\n8\nGINE\n0.001\n0.0\n-\n4\n52\nmean\n16\n150\nBatchNorm\n18\nGraphGPS\n0.001\n0.0\n4\n3\n52\nmean\n16\n150\nBatchNorm\n8\nExphormer\n0.001\n0.1\n4\n5\n40\nmean\n16\n150\nBatchNorm\n8\nGRITSparseConv\n0.001\n0.0\n-\n3\n52\nmean\n16\n150\nBatchNorm\n18\nGRIT\n0.001\n0.0\n4\n3\n52\nmean\n16\n150\nBatchNorm\n18\nTable 7: Model Configurations for PATTERN\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.0\n-\n4\n64\n-\n32\n100\nBatchNorm\n10\nGINE\n0.0005\n0.0\n-\n10\n64\n-\n32\n100\nBatchNorm\n21\nGraphGPS\n0.001\n0.0\n4\n6\n64\n-\n32\n100\nBatchNorm\n10\nExphormer\n0.0002\n0.0\n4\n4\n40\n-\n32\n100\nBatchNorm\n10\nGRITSparseConv\n0.0005\n0.0\n-\n8\n64\n-\n32\n100\nBatchNorm\n21\nGRIT\n0.0005\n0.0\n8\n10\n64\n-\n32\n100\nBatchNorm\n21\nTable 8: Model Configurations for CLUSTER\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.0\n-\n4\n48\n-\n16\n100\nBatchNorm\n16\nGINE\n0.001\n0.01\n-\n2\n64\n-\n16\n100\nBatchNorm\n16\nGraphGPS\n0.001\n0.01\n8\n16\n48\n-\n16\n100\nBatchNorm\n16\nExphormer\n0.0002\n0.1\n8\n20\n32\n-\n16\n200\nBatchNorm\n8\nSparseGRIT\n0.001\n0.01\n-\n16\n48\n-\n16\n100\nBatchNorm\n32\nGRIT\n0.0005\n0.0\n8\n10\n64\n-\n32\n100\nBatchNorm\n21\nTable 9: Model Configurations for ZINC\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.0\n-\n4\n64\nadd\n32\n1700\nBatchNorm\n8\nGINE\n0.001\n0.0\n-\n10\n64\nadd\n32\n1500\nBatchNorm\n21\nGraphGPS\n0.001\n0.0\n4\n10\n64\nadd\n32\n1500\nBatchNorm\n21\nExphormer\n0.001\n0.0\n4\n4\n64\nadd\n32\n1500\nBatchNorm\n8\nSparseGRIT\n0.001\n0.0\n-\n10\n64\nadd\n32\n1500\nBatchNorm\n21\nGRIT\n0.001\n0.0\n8\n10\n64\nadd\n32\n1500\nBatchNorm\n18\nLong Range Graph Benchmark\nWe mainly consider four models: GatedGCN, GraphGPS, Ex-\nphormer, and Sparse GRIT. For GatedGCN and GraphGPS, we primarily follow the fine-tuned con-\nfigurations as described by Tonshoff et al. (2023) (T¨onshoff et al., 2023). For Sparse GRIT, we\nadopt the hyperparameters used for the peptides-func and peptides-struct datasets and transfer these\nsettings to the COCO-SP, Pascal-VOC, and PCQM-Contact datasets, as detailed by Dwivedi et al.\n(2022) (Dwivedi et al., 2022). For Exphormer, we follow the configurations proposed by Shirzad et\nal. (2023) (Shirzad et al., 2023). Configurations are listed from table 10 to table 14.\nOpen Graph Benchmark\nWe mainly consider three models: GraphGPS, Exphormer, and Sparse\nGRIT. Due to scalability issues, we do not include configurations for the GPS model for ogbn-arxiv.\nGraphGPS (Ramp´aˇsek et al., 2022), Exphormer (Shirzad et al., 2023) and Sparse GRIT have shared\nthe same settings for the ogbg-molpcba dataset. Configurations are listed from table 15 to table 17.\n22\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nTable 10: Model Configurations for Peptides-func\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.1\n-\n10\n95\nmean\n200\n250\nBatchNorm\n16\nGraphGPS\n0.001\n0.1\n4\n6\n76\nmean\n200\n250\nBatchNorm\n16\nExphormer\n0.0003\n0.12\n4\n8\n64\nmean\n128\n200\nBatchNorm\n16\nGRITSparseConv\n0.0003\n0.0\n-\n4\n96\nmean\n16\n300\nBatchNorm\n16\nGRIT\n0.0003\n0.0\n4\n4\n96\nmean\n16\n200\nBatchNorm\n17\nTable 11: Model Configurations for Peptides-struct\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.1\n-\n8\n100\nmean\n128\n250\nBatchNorm\n16\nGraphGPS\n0.001\n0.1\n4\n8\n64\nmean\n200\n250\nBatchNorm\n16\nExphormer\n0.0003\n0.12\n4\n4\n88\nmean\n128\n200\nBatchNorm\n16\nGRITSparseConv\n0.0003\n0.05\n-\n4\n96\nmean\n16\n300\nBatchNorm\n16\nGRIT\n0.0003\n0.0\n8\n4\n96\nmean\n32\n200\nBatchNorm\n24\nTable 12: Model Configurations for PCQM-Contact\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.1\n-\n8\n215\n-\n500\n100\n-\n16\nGraphGPS\n0.001\n0.0\n4\n6\n76\n-\n500\n150\nBatchNorm\n16\nExphormer\n0.0003\n0.0\n4\n7\n64\n-\n128\n200\nBatchNorm\n16\nGRITSparseConv\n0.001\n0.0\n-\n10\n64\n-\n500\n100\nBatchNorm\n16\nTable 13: Model Configurations for Pascal-VOC\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.2\n-\n10\n95\n-\n50\n200\n-\n16\nGraphGPS\n0.001\n0.1\n4\n8\n68\n-\n50\n200\nBatchNorm\n16\nExphormer\n0.0005\n0.15\n8\n4\n96\n-\n32\n300\nBatchNorm\n16\nGRITSparseConv\n0.001\n0.0\n-\n10\n64\n-\n50\n250\nBatchNorm\n16\nTable 14: Model Configurations for COCO-SP\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGatedGCN\n0.001\n0.1\n-\n6\n120\n-\n16\n200\n-\n16\nGraphGPS\n0.001\n0.1\n4\n8\n68\n-\n50\n200\nBatchNorm\n16\nExphormer\n0.0005\n0.0\n4\n7\n72\n-\n32\n200\nBatchNorm\n16\nGRITSparseConv\n0.0005\n0.0\n-\n4\n64\n-\n32\n200\nBatchNorm\n16\nTable 15: Model Configurations for OGBN-Arxiv\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nExphormer\n0.001\n0.3\n2\n4\n80\nadd\n1\n600\nBatchNorm\n16\nGRITSparseConv\n0.001\n0.1\n-\n4\n64\nadd\n1\n600\nBatchNorm\n8\nTable 16: Model Configurations for OGBG-Molhiv\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGraphGPS\n0.0001\n0.05\n4\n10\n64\nmean\n32\n100\nBatchNorm\n8\nExphormer\n0.0001\n0.05\n4\n8\n64\nmean\n32\n100\nBatchNorm\n16\nGRITSparseConv\n0.0001\n0.0\n-\n8\n64\nmean\n32\n100\nBatchNorm\n16\nTable 17: Model Configurations for OGBG-Molpcba\nModel\nlr\ndropout\nheads\nlayers\nhidden dim\npooling\nbatch size\nepochs\nnorm\nPE dim\nGraphGPS\n0.0005\n0.2\n4\n5\n384\nmean\n512\n100\nBatchNorm\n20\nExphormer\n0.0005\n0.2\n4\n5\n384\nmean\n512\n100\nBatchNorm\n20\nGRITSparseConv\n0.0005\n0.2\n-\n5\n384\nmean\n512\n100\nBatchNorm\n20\n23\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nA.7\nCOMPLETE RESULTS\nThis section provides the results of all runs that we conducted in the paper. This also includes the\nnon-aggregated results that show the performance of every positional encoding on any model and\ndataset.\nA.7.1\nBENCHMARKINGGNN\nTable 18 has listed all results of GNN models with different positional encodings on BENCHMARK-\nINGGNN datasets. Figure 4 shows a percentage of improvement compared to GNN models without\nany positional encoding.\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\nImprovement (%)\nGatedGCN\n4\n3\n2\n1\n0\n1\nGINE\n1.5\n1.0\n0.5\n0.0\nExphormer\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.0\nGraphGPS\n2\n1\n0\n1\n2\n3\nSparseGRIT\n2\n1\n0\n1\nGRIT\n2\n1\n0\n1\ndataset = CIFAR10\nGatedGCN (Fully-connected)\n0\n5\n10\n15\n20\nImprovement (%)\n0\n5\n10\n15\n20\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.0\n0.5\n0.0\n0.5\n1.0\n1\n0\n1\n2\n3\n0.5\n0.0\n0.5\n1.0\n1.5\n0\n2\n4\n6\n8\ndataset = CLUSTER\n0.5\n0.0\n0.5\n1.0\n1.5\nImprovement (%)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n0.4\n0.3\n0.2\n0.1\n0.0\n0.1\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\ndataset = PATTERN\n0.25\n0.20\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\nImprovement (%)\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n0.35\n0.30\n0.25\n0.20\n0.15\n0.10\n0.05\n0.00\n0.125\n0.100\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\n0.00\n0.05\n0.10\n0.15\n0.3\n0.2\n0.1\n0.0\n0.00\n0.05\n0.10\n0.15\n0.20\ndataset = MNIST\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\nPE\n60\n40\n20\n0\n20\n40\nImprovement (%)\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\nPE\n0\n50\n100\n150\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\nPE\n40\n20\n0\n20\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\nPE\n50\n40\n30\n20\n10\n0\n10\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\nPE\n50\n25\n0\n25\n50\n75\n100\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\nPE\n50\n40\n30\n20\n10\n0\n10\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nPPR\nGCKN\nWLPE\nPE\n60\n40\n20\n0\n20\ndataset = ZINC\nFigure 4: Percentage of improvement compared to GNN models without any positional encoding\n(BENCHMARKINGGNN)\nA.7.2\nLONG RANGE GRAPH BENCHMARK\nTable 19 has listed all results of GNN models with different positional encodings on BENCHMARK-\nINGGNN dataset. Figure 5 shows a percentage of improvement compared to GNN models without\nany positional encoding. Figure 6 shows the mean average of improvement on the Long Range\nGraph Benchmark for each positional encoding individually.\nA.7.3\nOPEN GRAPH BENCHMARK\nTable 20 has listed all results of GNN models with different positional encodings on Open Graph\nBenchmark dataset.\nA.8\nSTATISTICS FOR POSITIONAL ENCODINGS\nWe measure both the time that is taken to measure pre-computing positional enoodings (PEs), as\nwell as the space that CPU is taken to precompute it, which are presented from Table 21 to Table 26.\nFigure 7 shows the comparison between time and memory for each dataset, which is log-scaled.\n24\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\n2\n0\n2\n4\n6\n8\n10\nImprovement (%)\nGatedGCN\n12\n10\n8\n6\n4\n2\n0\n2\nGraphGPS\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\n2.5\nExphormer\n0\n10\n20\n30\nSparseGRIT\n0\n10\n20\n30\n40\ndataset = Peptides-func\nGRIT\n25\n20\n15\n10\n5\n0\nImprovement (%)\n40\n30\n20\n10\n0\n10\n20\n3\n2\n1\n0\n1\n2\n3\n2\n1\n0\n1\n2\n3\n0\n2\n4\n6\ndataset = Peptides-struct\n0.5\n0.0\n0.5\n1.0\n50\n40\n30\n20\n10\n0\n0.5\n0.0\n0.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\ndataset = PCQM-Contact\n5\n4\n3\n2\n1\n0\n4\n3\n2\n1\n0\n1\n2\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n5\n4\n3\n2\n1\n0\ndataset = PascalVOC-SP\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\nPE\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\nPE\n0.5\n0.0\n0.5\n1.0\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\nPE\n0.8\n0.6\n0.4\n0.2\n0.0\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\nPE\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ndataset = COCO-SP\nFigure 5: Percentage of improvement compared to GNN models without any positional encoding\n(LRGb)\n25\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\n0.50\n0.55\n0.60\n0.65\nMean Performance\nPeptides-func\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nPeptides-struct\nno PE\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nPCQM-Contact\nno PE\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\n0.34\n0.36\n0.38\n0.40\n0.42\n0.44\nPascalVOC-SP\nno PE\nLapPE\nESLapPE\nSignNet\nRWSE\nRWDIFF\nRRWP\nGCKN\nWLPE\n0.200\n0.225\n0.250\n0.275\n0.300\n0.325\n0.350\n0.375\nCOCO-SP\nFigure 6: Mean performance of different positional encodings on Long Range Graph Benchmark\n10\n2\n10\n3\nElapsed Time (seconds)\n10\n3\n10\n4\nMemory Usage (MB)\nMNIST\n10\n3\nElapsed Time (seconds)\n10\n3\nMemory Usage (MB)\nCIF\nAR10\n10\n2\nElapsed Time (seconds)\n10\n3\n10\n4\nMemory Usage (MB)\nPATTERN\n10\n2\nElapsed Time (seconds)\n10\n3\n10\n4\nMemory Usage (MB)\nCLUSTER\n10\n1\n10\n2\nElapsed Time (seconds)\n10\n2\n10\n3\nMemory Usage (MB)\nZINC\n10\n2\nElapsed Time (seconds)\n4\n×\n10\n2\n5\n×\n10\n2\n6\n×\n10\n2\n7\n×\n10\n2\nMemory Usage (MB)\nPeptides-func\n10\n2\nElapsed Time (seconds)\n4\n×\n10\n2\n5\n×\n10\n2\n6\n×\n10\n2\n7\n×\n10\n2\nMemory Usage (MB)\nPeptides-struc\n10\n3\nElapsed Time (seconds)\n4\n×\n10\n3\n5\n×\n10\n3\n6\n×\n10\n3\nMemory Usage (MB)\nPCQM-Contact\n10\n2\n10\n3\nElapsed Time (seconds)\n10\n3\nMemory Usage (MB)\nPascalVOC-SP\n3\n×\n10\n3\n4\n×\n10\n3\n6\n×\n10\n3\nElapsed Time (seconds)\n8\n×\n10\n3\n9\n×\n10\n3\nMemory Usage (MB)\nCOCO -SP\n10\n2\nElapsed Time (seconds)\n10\n3\nMemory Usage (MB)\nogbg-molhiv\n10\n3\n3\n×\n10\n2\n4\n×\n10\n2\n6\n×\n10\n2\nElapsed Time (seconds)\n3\n×\n10\n3\n4\n×\n10\n3\n5\n×\n10\n3\nMemory Usage (MB)\nogbg-molpcba\nMethod\nLapPE\nESLapPE\nRWSE\nSignNet\nPPR\nGCKN\nWLPE\nRWDIFF\nRRWP\nFigure 7: Temporal complexity vs. spatial complexity for different positional encodings\n26\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nTable 18: Different positional encodings with GNNs on BENCHMARKINGGNN including ZINC,\nMNIST, CIFAR10, PATTERN, and CLUSTER. Experiments are run on a NVIDIA RTX 3090 and\nRTX A6000. Five random seeds are: 0, 7, 42, 100, and 2024 (although it should be noted that the\nexecution of PyG on the cuda backend is non-deterministic). Note that the batched graphs are sparse\nas default. Batched graphs are only fully-connected when it comes to RRWP.\nSparse Graph\nMNIST ↑\nCIFAR10 ↑\nPATTERN ↑\nCLUSTER ↑\nZINC ↓\nGatedGCN + noPE\n97.800±0.138\n69.303±0.318\n85.397±0.040\n61.695±0.261\n0.2398±0.0094\nGatedGCN + ESLapPE\n97.870±0.090\n69.438±0.297\n85.422±0.161\n61.953±0.082\n0.2409±0.0131\nGatedGCN + LapPE\n97.575±0.025\n69.285±0.205\n86.700±0.000\n65.130±0.405\n0.1718±0.0024\nGatedGCN + RWSE\n97.840±0.171\n69.038±0.152\n86.833±0.030\n65.675±0.296\n0.1016±0.0030\nGatedGCN + SignNet\n97.553±0.167\n68.570±0.240\n86.763±0.027\n75.293±0.047\n0.1060±0.0021\nGatedGCN + PPR\n97.797±0.045\n69.224±0.546\n86.522±0.093\n74.175±0.122\n0.3678±0.0198\nGatedGCN + GCKN\n97.745±0.069\n69.408±0.222\n86.758±0.049\n62.478±0.156\n0.1446±0.0048\nGatedGCN + WLPE\n97.693±0.235\n69.418±0.165\n84.980±0.160\n62.738±0.291\n0.1779±0.0059\nGatedGCN + RWDIFF\n97.823±0.119\n69.528±0.494\n86.760±0.043\n65.653±0.470\n0.1346±0.0074\nGatedGCN + RRWP\n97.908±0.076\n69.572±0.787\n85.465±0.148\n61.728±0.174\n0.2451±0.0131\nGINE + noPE\n97.712±0.120\n65.554±0.225\n85.482±0.272\n48.783±0.060\n0.1210±0.0107\nGINE + ESLapPE\n97.596±0.071\n66.140±0.310\n85.546±0.114\n48.708±0.061\n0.1209±0.0066\nGINE + LapPE\n97.555±0.045\n65.325±0.195\n85.835±0.195\n48.685±0.035\n0.1144±0.0028\nGINE + RWSE\n97.686±0.073\n65.238±0.283\n86.688±0.084\n50.642±0.694\n0.0795±0.0034\nGINE + SignNet\n97.692±0.165\n64.538±0.314\n86.538±0.044\n59.660±0.630\n0.0993±0.0069\nGINE + PPR\n97.650±0.088\n65.082±0.434\n85.658±0.048\n47.440±2.290\n0.3019±0.0122\nGINE + GCKN\n97.708±0.105\n65.976±0.308\n85.844±0.157\n48.780±0.149\n0.1169±0.0029\nGINE + WLPE\n97.716±0.118\n66.132±0.225\n85.676±0.084\n48.997±0.068\n0.1205±0.0062\nGINE + RWDIFF\n97.750±0.097\n65.632±0.553\n85.764±0.209\n49.148±0.168\n0.0750±0.0058\nGINE + RRWP\n96.742±0.277\n62.790±1.501\n86.526±0.036\n48.736±0.108\n0.0857±0.0009\nExphormer + noPE\n98.414±0.047\n74.962±0.631\n85.676±0.049\n77.500±0.151\n0.1825±0.0209\nExphormer + ESLapPE\n98.354±0.108\n74.880±0.322\n86.734±0.024\n78.218±0.267\n0.2023±0.0140\nExphormer + LapPE\n98.270±0.070\n75.205±0.095\n86.565±0.075\n77.175±0.165\n0.1503±0.0117\nExphormer + RWSE\n98.254±0.084\n74.434±0.205\n86.820±0.040\n77.690±0.147\n0.0933±0.0050\nExphormer + SignNet\n98.136±0.094\n73.842±0.317\n86.752±0.088\n78.280±0.211\n0.0924±0.0072\nExphormer + PPR\n98.076±0.126\n74.076±0.104\n86.712±0.047\n78.098±0.211\n0.2414±0.0123\nExphormer + GCKN\n98.402±0.067\n74.926±0.288\n86.730±0.040\n77.470±0.067\n0.1690±0.0056\nExphormer + WLPE\n98.398±0.162\n74.794±0.358\n85.454±0.033\n77.402±0.120\n0.1465±0.0095\nExphormer + RWDIFF\n98.416±0.055\n74.886±0.810\n86.792±0.023\n77.550±0.057\n0.1360±0.0082\nExphormer + RRWP\n98.418±0.179\n74.504±0.369\n85.652±0.001\n77.434±0.056\n0.1914±0.0153\nGraphGPS + noPE\n98.136±0.085\n72.310±0.198\n84.182±0.276\n77.590±0.158\n0.1610±0.0045\nGraphGPS + ESLapPE\n98.180±0.117\n72.122±0.511\n86.700±0.055\n77.800±0.107\n0.1795±0.0110\nGraphGPS + LapPE\n98.065±0.075\n72.310±0.530\n86.550±0.150\n77.355±0.115\n0.1086±0.0062\nGraphGPS + RWSE\n98.116±0.102\n72.034±0.756\n86.866±0.010\n77.550±0.195\n0.0744±0.0060\nGraphGPS + SignNet\n98.012±0.091\n72.152±0.323\n86.734±0.069\n78.308±0.111\n0.0945±0.0019\nGraphGPS + PPR\n98.010±0.097\n71.842±0.325\n86.124±0.214\n76.828±0.250\n0.1349±0.0054\nGraphGPS + GCKN\n98.180±0.117\n72.194±0.515\n86.786±0.043\n77.514±0.182\n0.1460±0.0078\nGraphGPS + WLPE\n98.038±0.134\n72.258±0.661\n84.916±0.195\n76.866±0.171\n0.1204±0.0055\nGraphGPS + RWDIFF\n98.026±0.101\n71.800±0.363\n86.820±0.063\n77.478±0.150\n0.0924±0.0212\nGraphGPS + RRWP\n98.146±0.105\n72.084±0.466\n84.436±0.224\n77.420±0.080\n0.1690±0.0084\nSparseGRIT + noPE\n97.940±0.071\n72.778±0.627\n85.948±0.148\n77.274±0.170\n0.1255±0.0062\nSparseGRIT + ESLapPE\n97.970±0.110\n72.494±0.501\n86.018±0.319\n77.238±0.066\n0.1280±0.0077\nSparseGRIT + LapPE\n97.915±0.065\n72.640±0.040\n86.555±0.025\n76.100±0.085\n0.1070±0.0017\nSparseGRIT + RWSE\n98.122±0.054\n72.330±0.600\n86.914±0.031\n77.148±0.174\n0.0676±0.0060\nSparseGRIT + SignNet\n97.946±0.122\n71.003±0.301\n86.794±0.055\n78.882±0.146\n0.0821±0.0043\nSparseGRIT + PPR\n98.020±0.194\n71.926±0.833\n86.650±0.033\n78.732±0.202\n0.2536±0.0193\nSparseGRIT + GCKN\n97.958±0.127\n72.598±0.535\n86.650±0.033\n76.746±0.187\n0.1233±0.0071\nSparseGRIT + WLPE\n97.946±0.125\n72.096±0.835\n85.712±0.027\n77.170±0.143\n0.1262±0.0059\nSparseGRIT + RWDIFF\n98.022±0.083\n72.366±0.388\n86.938±0.045\n77.214±0.065\n0.0690±0.0039\nSparseGRIT + RRWP\n98.088±0.048\n74.954±0.256\n87.168±0.041\n79.872±0.079\n0.0651±0.0027\nGRIT + noPE\n98.108±0.190\n74.402±0.135\n87.126±0.033\n78.616±0.178\n0.1237±0.0057\nGRIT + ESLapPE\n98.010±0.141\n74.558±0.682\n87.140±0.064\n78.588±0.111\n0.1241±0.0031\nGRIT + LapPE\n97.875±0.001\n73.325±0.505\n86.985±0.015\n77.960±0.310\n0.1039±0.0035\nGRIT + RWSE\n98.068±0.182\n73.652±0.623\n87.116±0.046\n78.880±0.057\n0.0671±0.0037\nGRIT + SignNet\n97.766±0.220\n72.812±0.482\n87.085±0.064\n79.770±0.150\n0.0945±0.0098\nGRIT + PPR\n97.986±0.082\n73.568±0.451\n86.780±0.001\n78.958±0.175\n0.1390±0.0076\nGRIT + GCKN\n98.084±0.139\n73.946±0.910\n87.194±0.044\n78.542±0.149\n0.1306±0.0141\nGRIT + WLPE\n98.022±0.173\n74.206±0.684\n86.863±0.033\n78.500±0.091\n0.1218±0.0035\nGRIT + RWDIFF\n98.024±0.148\n73.956±0.202\n87.152±0.045\n78.778±0.090\n0.0671±0.0060\nGRIT + RRWP\n98.124±0.141\n75.662±0.410\n87.217±0.034\n79.812±0.109\n0.0590±0.0010\n27\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nTable 19:\nFive Long Range Graph Benchamrk Datasets which include Peptides func,\nPepteide struct, PCQM Contact, PascalVOC-SuperPixels and COCO-SuperPixels.\nThe hyper-\nparameters for Peptides func and Pepteide struct follow the original GraphGPS settings.\nFor\nGraphGPS, we follow the settings of a special study into LRGB (T¨onshoff et al., 2023) where it\nfound a state-of-the-art settings for GraphGPS on those five datasets.\nSparse Graph\nPeptides-func\nPeptides-struct\nPCQM-Contact\nPascalVOC-SP\nCOCO-SP\nGatedGCN + noPE\n0.6523±0.0074\n0.2470±0.0005\n0.4730±0.0003\n0.3923±0.0020\n0.2619±0.0045\nGatedGCN + LapPE\n0.6581±0.0068\n0.2472±0.0003\n0.4764±0.0004\n0.3920±0.0033\n0.2671±0.0006\nGatedGCN + ESLapPE\n0.6484±0.0037\n0.2490±0.0020\n0.4736±0.0006\n0.3930±0.0041\n0.2628±0.0004\nGatedGCN + RWSE\n0.6696±0.0022\n0.2485±0.0022\n0.4749±0.0005\n0.3882±0.0041\n0.2657±0.0007\nGatedGCN + SignNet\n0.5327±0.0137\n0.2688±0.0016\n0.4672±0.0001\n0.3814±0.0005\n-\nGatedGCN + GCKN\n0.6544±0.0040\n0.2483±0.0009\n0.4687±0.0002\n0.3933±0.0044\n-\nGatedGCN + WLPE\n0.6562±0.0053\n0.2473±0.0012\n0.4671±0.0003\n0.3805±0.0018\n-\nGatedGCN + RWDIFF\n0.6527±0.0053\n0.2474±0.0003\n0.4740±0.0003\n0.3919±0.0019\n0.2674±0.0031\nGatedGCN + RRWP\n0.6516±0.0072\n0.2514±0.0001\n-\n-\n-\nGraphGPS + noPE\n0.6514±0.0123\n0.4243±0.0305\n0.4649±0.0025\n0.4517±0.0112\n0.3799±0.0056\nGraphGPS + LapPE\n0.6620±0.0073\n0.2497±0.0024\n0.4696±0.0017\n0.4505±0.0062\n0.3859±0.0016\nGraphGPS + ESLapPE\n0.6516±0.0062\n0.2568±0.0013\n0.4639±0.0031\n0.4538±0.0083\n0.3866±0.0017\nGraphGPS + RWSE\n0.6510±0.0071\n0.2549±0.0033\n0.4685±0.0009\n0.4531±0.0073\n0.3891±0.0033\nGraphGPS + SignNet\n0.5719±0.0055\n0.2657±0.0021\n0.4624±0.0020\n0.4291±0.0056\n-\nGraphGPS + GCKN\n0.6502±0.0101\n0.2519±0.0005\n0.4609±0.0007\n0.4515±0.0053\n-\nGraphGPS + WLPE\n0.5851±0.0441\n0.5203±0.0504\n0.4622±0.0012\n0.4501±0.0057\n-\nGraphGPS + RWDIFF\n0.6519±0.0077\n0.4769±0.0360\n0.4669±0.0006\n0.4488±0.0097\n0.3873±0.0024\nGraphGPS + RRWP\n0.6505±0.0058\n0.3734±0.0157\n-\n-\n-\nExphormer + noPE\n0.6200±0.0052\n0.2584±0.0019\n0.4661±0.0021\n0.4149±0.0047\n0.3445±0.0052\nExphormer + LapPE\n0.6424±0.0063\n0.2496±0.0013\n0.4737±0.0024\n0.4242±0.0044\n0.3471±0.0028\nExphormer + ESLapPE\n0.6281±0.0085\n0.2513±0.0022\n0.4676±0.0018\n0.4141±0.0054\n0.3485±0.0011\nExphormer + RWSE\n0.6240±0.0069\n0.2579±0.0010\n0.4642±0.0039\n0.4218±0.0063\n0.3485±0.0011\nExphormer + SignNet\n0.5458±0.0097\n0.2667±0.0037\n0.4615±0.0066\n0.3966±0.0020\n-\nExphormer + GCKN\n0.6422±0.0080\n0.2514±0.0012\n0.4604±0.0038\n0.4196±0.0049\n-\nExphormer + WLPE\n0.6216±0.0069\n0.2558±0.0011\n0.2051±0.0080\n0.4104±0.0071\n-\nExphormer + RWDIFF\n0.6275±0.0031\n0.2556±0.0021\n0.4642±0.0032\n0.4165±0.0059\n0.3417±0.0006\nExphormer + RRWP\n0.6208±0.0074\n0.2586±0.0014\n-\n-\n-\nSparseGRIT + noPE\n0.4885±0.0036\n0.2550±0.0006\n0.4527±0.0006\n0.3471±0.0030\n0.1976±0.0038\nSparseGRIT + LapPE\n0.5884±0.0059\n0.2487±0.0014\n0.4585±0.0011\n0.3514±0.0026\n0.1974±0.0008\nSparseGRIT + ESLapPE\n0.5161±0.0069\n0.2537±0.0005\n0.4532±0.0005\n0.3462±0.0035\n0.1958±0.0001\nSparseGRIT + RWSE\n0.5570±0.0079\n0.2537±0.0012\n0.4553±0.0014\n0.3460±0.0071\n0.1969±0.0010\nSparseGRIT + SignNet\n0.5115±0.0064\n0.2640±0.0018\n0.4573±0.0003\n0.3419±0.0074\n-\nSparseGRIT + GCKN\n0.5871±0.0042\n0.2492±0.0010\n0.4500±0.0004\n0.3519±0.0040\n-\nSparseGRIT + WLPE\n0.4808±0.0016\n0.2547±0.0005\n0.4489±0.0012\n0.3439±0.0027\n-\nSparseGRIT + RWDIFF\n0.5521±0.0072\n0.2550±0.0008\n0.4551±0.0005\n0.3447±0.0046\n0.1965±0.0011\nSparseGRIT + RRWP\n0.6702±0.0080\n0.2504±0.0025\n-\n-\n-\nGRIT + noPE\n0.4861±0.0053\n0.2489±0.0008\n0.4525 ± 0.0001\n0.3556 ± 0.0019\n0.2105 ± 0.0004\nGRIT + LapPE\n0.5834±0.0105\n0.2474±0.0005\n0.4580 ± 0.0020\n0.3551 ± 0.0032\n0.2112 ± 0.0005\nGRIT + ESLapPE\n0.4831±0.0023\n0.2584±0.0002\n0.4486 ± 0.0014\n0.3485 ± 0.0028\n0.2100 ± 0.0008\nGRIT + RWSE\n0.5432±0.0034\n0.2612±0.0008\n0.4524 ± 0.0001\n0.3461 ± 0.0058\n0.2114 ± 0.0009\nGRIT + SignNet\n0.5307±0.0085\n0.2600±0.0018\n0.4608 ± 0.0007\n0.3385 ± 0.0045\n-\nGRIT + GCKN\n0.5868±0.0051\n0.2477±0.0006\n0.4521 ± 0.0002\n0.3516 ± 0.0003\n-\nGRIT + WLPE\n0.4798±0.0012\n0.2578±0.0011\n0.4515 ± 0.0004\n0.3441 ± 0.0011\n-\nGRIT + RWDIFF\n0.5801±0.0036\n0.2639±0.0010\n0.4563 ± 0.0003\n0.3521 ± 0.0079\n0.2128 ± 0.0008\nGRIT + RRWP\n0.6865±0.0050\n0.2454±0.0010\n-\n-\n-\n28\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nTable 20: Results for three OGB datasets.\nSparse Graph\nogbn-arxiv\nogbg-molhiv\nogbg-molpcba\nGPS + noPE\n-\n77.885±2.641\n28.573±0.215\nGPS + ESLapPE\n-\n78.295±0.925\n28.373±0.477\nGPS + LapPE\n-\n77.256±0.806\n29.325±0.300\nGPS + GCKN\n-\n77.652±1.326\n-\nGPS + WLPE\n-\n75.835±0.857\n27.968±0.154\nGPS + RWSE\n-\n77.890±1.045\n28.563±0.283\nGPS + RRWP\n-\n76.383±1.189\n28.765±0.268\nExphormer + noPE\n70.782±0.029\n78.347±0.440\n28.355±0.224\nExphormer + ESLapPE\n-\n77.578±1.595\n28.123±0.281\nExphormer + LapPE\n-\n76.818±0.744\n27.858±0.082\nExphormer + GCKN\n-\n78.045±1.146\n-\nExphormer + WLPE\n70.738±0.095\n75.587±1.172\n27.283±0.312\nExphormer + RWSE\n70.693±0.132\n77.053±0.295\n28.490±0.257\nExphormer + RRWP\n-\n77.305±1.250\n-\nSparseGRIT + noPE\n70.955±0.119\n77.752±1.331\n20.950±0.076\nSparseGRIT + ESLapPE\n-\n77.670±1.870\n20.953±0.086\nSparseGRIT + LapPE\n-\n75.393±1.358\n22.748±0.455\nSparseGRIT + GCKN\n-\n75.453±0.893\n-\nSparseGRIT + WLPE\n70.877±0.045\n76.060±1.066\n20.670±0.211\nSparseGRIT + RWSE\n-\n76.973±0.242\n23.628±0.205\nSparseGRIT + RRWP\n-\n78.353±0.546\n-\nTable 21: Running Time for Pretransforming PEs (measured in seconds) on BENCHMARKINGGNN\nMNIST\nCIFAR10\nPATTERN\nCLUSTER\nZINC\nLapPE\n93\n123\n28\n23\n8\nESLapPE\n90\n122\n28\n21\n7\nRWSE\n225\n252\n55\n53\n31\nSignNet\n90\n122\n28\n20\n7\nPPR\n585\n600\n90\n71\n313\nGCKN\n1180\n1705\n552\n448\n89\nWLPE\n166\n217\n166\n108\n8\nRWDIFF\n210\n209\n226\n158\n15\nRRWP\n159\n-\n47\n53\n22\nTable 22: Memory Usage for Pretransforming PEs (measured in MB) on BENCHMARKINGGNN\nMNIST\nCIFAR10\nPATTERN\nCLUSTER\nZINC\nLapPE\n795.55\n1021.10\n290.78\n251.45\n67.68\nESLapPE\n652.55\n809.55\n293.79\n377.49\n148.95\nRWSE\n772.05\n994.92\n286.80\n243.91\n60.12\nSignNet\n803.17\n1024.11\n293.41\n377.97\n58.90\nPPR\n2430.63\n4142.62\n1849.49\n1701.08\n1041.20\nGCKN\n355.12\n824.58\n292.69\n170.61\n68.00\nWLPE\n1268.05\n1709.76\n1506.82\n809.11\n104.82\nRWDIFF\n264.00\n312.49\n145.37\n191.87\n28.38\nRRWP\n43327.52\n-\n29823.30\n36577.61\n2414.64\nTable 23: Running Time for Pretransforming PEs (measured in seconds) on LRGB\nPeptides-func\nPeptides-struct\nPCQM-Contact\nPascalVOC-SP\nCOCO-SP\nLapPE\n44\n44\n358\n203\n2889\nESLapPE\n44\n45\n384\n203\n3045\nRWSE\n92\n90\n1500\n418\n8373\nSignNet\n44\n44\n386\n206\n-\nGCKN\n530\n527\n5072\n2377\n-\nWLPE\n34\n33\n451\n97\n-\nRWDIFF\n47\n46\n1075\n764\n8976\n29\n\nBenchmarking Positional Encodings for GNNs and Graph Transformers\nTable 24: Memory Usage for Pretransforming PEs (measured in MB) on LRGB\nPeptides-func\nPeptides-struct\nPCQM-Contact\nPascalVOC-SP\nCOCO-SP\nLapPE\n438.33\n651.95\n4625.45\n875.47\n9149.31\nESLapPE\n438.84\n654.52\n4617.98\n877.84\n9153.73\nRWSE\n424.32\n421.45\n4410.43\n875.87\n9112.50\nSignNet\n647.73\n646.92\n6091.65\n871.27\n-\nGCKN\n662.06\n661.18\n6016.68\n909.13\n-\nWLPE\n710.55\n709.84\n6525.88\n1639.22\n-\nRWDIFF\n334.18\n334.54\n3484.15\n677.29\n7248.34\nTable 25: Running Time for Pretransforming PEs (measured in seconds) on OGB\nogbn-arxiv\nogbg-molhiv\nogbg-molpcba\nESLapPE\n-\n28\n296\nLapPE\n-\n26\n314\nGCKN\n-\n325\n-\nWLPE\n14\n31\n334\nRWSE\n35\n101\n1034\nRRWP\n-\n54\n-\nTable 26: Memory Usage for Pretransforming PEs (measured in MB) on OGB\nogbn-arxiv\nogbg-molhiv\nogbg-molpcba\nESLapPE\n-\n296.21\n4374.86\nLapPE\n-\n328.23\n4514.98\nGCKN\n-\n238.57\n-\nWLPE\n57.76\n382.05\n5080.64\nRWSE\n-\n301.18\n2799.23\nRRWP\n-\n3220.03\n-\n30",
    "pdf_filename": "Benchmarking_Positional_Encodings_for_GNNs_and_Graph_Transformers.pdf"
}