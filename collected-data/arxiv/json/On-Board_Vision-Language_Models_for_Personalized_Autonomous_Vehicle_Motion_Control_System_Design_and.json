{
    "title": "On-Board Vision-Language Models for Personalized Autonomous Vehicle",
    "abstract": "just technically self-driving, but also the ability to provide Personalized driving refers to an autonomous vehicle’s human-like driving experiences that align with individual ability to adapt its driving behavior or control strategies preferencesandexpectations[11,12]. to match individual users’ preferences and driving styles Previous work in personalized autonomous driving has whilemaintainingsafetyandcomfortstandards. However, followedtwomainapproaches. Thefirstusesclusteringal- existing works either fail to capture every individual pref- gorithmstoclassifydriversintobroadcategories(e.g., ag- erence precisely or become computationally inefficient as gressive or conservative), but this fails to capture individ- the user base expands. Vision-Language Models (VLMs) ual nuances and preferences, forcing users into predefined offer promising solutions to this front through their natu- groupsthatmaynotmatchtheiractualdrivingstyle[48,50]. rallanguageunderstandingandscenereasoningcapabili- The second approach develops individual models for each ties. Inthiswork,weproposealightweightyeteffectiveon- user through learning-based methods [14, 49], but this re- board VLM framework that provides low-latency person- quiresextensivetrainingdataperuserandbecomescompu- alized driving performance while maintaining strong rea- tationally inefficient as the user base grows. Furthermore, soningcapabilities. OursolutionincorporatesaRetrieval- thesemethodslacktheabilitytoreasonaboutreal-timehu- Augmented Generation (RAG)-based memory module that maninstructionsoradapttochangingenvironments. enables continuous learning of individual driving prefer- ences through human feedback. Through comprehensive Recent advances in Vision-Language Models (VLMs) real-world vehicle deployment and experiments, our sys- havedemonstratedpromisingcapabilitiesinunderstanding tem has demonstrated the ability to provide safe, comfort- complexdrivingscenariosandnaturallanguageinstructions able, and personalized driving experiences across various through their integration of computer vision and language scenarios and significantly reduce takeover rates by up to processing[7,10,30,37]. ThedevelopmentinVLMshas 76.9%. Tothebestofourknowledge, thisworkrepresents ledresearcherstoleverageVLMs’multimodalunderstand- the first end-to-end VLM-based motion control system in ing capabilities to enhance both perception and decision- real-worldautonomousvehicles. making in autonomous systems [36, 46]. However, there remainsaresearchgapinleveragingVLMstoenhancecon- trolpoliciesoradaptthemtoindividualdrivingpreferences 1.Introduction andstyles. Thisgapisparticularlyevidentinthechallenge of translating a high-level understanding of human prefer- The autonomous driving industry is experiencing an evo- ences and scenario information into actionable low-level lution towards human-centric systems [6, 28], where vehi- control policies. Additionally, the computational demands cleautomationextendsbeyondonlyconsideringtraditional of previously adopted large-scale models make on-board safetyandefficiencymetricsbutalsoconsidersunderstand- deployment infeasible, forcing reliance on cloud-based in- ing users’ implicit instructions and providing personalized ferencing. Thissolutiondependsonstableinternetconnec- driving experiences [11, 12]. Personalized driving experi- tivityandcanintroducesignificantlatencyissuesinthein- encesarecrucialforuseracceptanceandtrust,astheyhelp ference process, with response times reaching up to 3 or bridgethegapbetweenautonomoustechnologyandhuman 4 seconds [26]. This is incompatible with the reliable and expectations. Thistrendreflectsagrowingrecognitionthat nearreal-timerequirementsofautonomousdriving. 4202 voN 71 ]IA.sc[ 1v31911.1142:viXra",
    "body": "On-Board Vision-Language Models for Personalized Autonomous Vehicle\nMotion Control: System Design and Real-World Validation\nCanCui1 ,ZichongYang1 ,YupengZhou1 ,JuntongPeng1,\nSung-YeonPark1,CongZhang1,YunshengMa1,XuCao2,WenqianYe3,\nYihengFeng1,JiteshPanchal1,LingxiLi1,YaobinChen1,ZiranWang1\n1PurdueUniversity 2UniversityofIllinoisUrbana-Champaign 3UniversityofVirginia\n{cancui, ziran}@purdue.edu\nAbstract successfuladoptionoftheautonomousvehiclerequiresnot\njust technically self-driving, but also the ability to provide\nPersonalized driving refers to an autonomous vehicle’s human-like driving experiences that align with individual\nability to adapt its driving behavior or control strategies preferencesandexpectations[11,12].\nto match individual users’ preferences and driving styles\nPrevious work in personalized autonomous driving has\nwhilemaintainingsafetyandcomfortstandards. However,\nfollowedtwomainapproaches. Thefirstusesclusteringal-\nexisting works either fail to capture every individual pref-\ngorithmstoclassifydriversintobroadcategories(e.g., ag-\nerence precisely or become computationally inefficient as\ngressive or conservative), but this fails to capture individ-\nthe user base expands. Vision-Language Models (VLMs)\nual nuances and preferences, forcing users into predefined\noffer promising solutions to this front through their natu-\ngroupsthatmaynotmatchtheiractualdrivingstyle[48,50].\nrallanguageunderstandingandscenereasoningcapabili-\nThe second approach develops individual models for each\nties. Inthiswork,weproposealightweightyeteffectiveon-\nuser through learning-based methods [14, 49], but this re-\nboard VLM framework that provides low-latency person-\nquiresextensivetrainingdataperuserandbecomescompu-\nalized driving performance while maintaining strong rea-\ntationally inefficient as the user base grows. Furthermore,\nsoningcapabilities. OursolutionincorporatesaRetrieval-\nthesemethodslacktheabilitytoreasonaboutreal-timehu-\nAugmented Generation (RAG)-based memory module that\nmaninstructionsoradapttochangingenvironments.\nenables continuous learning of individual driving prefer-\nences through human feedback. Through comprehensive Recent advances in Vision-Language Models (VLMs)\nreal-world vehicle deployment and experiments, our sys- havedemonstratedpromisingcapabilitiesinunderstanding\ntem has demonstrated the ability to provide safe, comfort- complexdrivingscenariosandnaturallanguageinstructions\nable, and personalized driving experiences across various through their integration of computer vision and language\nscenarios and significantly reduce takeover rates by up to processing[7,10,30,37]. ThedevelopmentinVLMshas\n76.9%. Tothebestofourknowledge, thisworkrepresents ledresearcherstoleverageVLMs’multimodalunderstand-\nthe first end-to-end VLM-based motion control system in ing capabilities to enhance both perception and decision-\nreal-worldautonomousvehicles. making in autonomous systems [36, 46]. However, there\nremainsaresearchgapinleveragingVLMstoenhancecon-\ntrolpoliciesoradaptthemtoindividualdrivingpreferences\n1.Introduction andstyles. Thisgapisparticularlyevidentinthechallenge\nof translating a high-level understanding of human prefer-\nThe autonomous driving industry is experiencing an evo- ences and scenario information into actionable low-level\nlution towards human-centric systems [6, 28], where vehi- control policies. Additionally, the computational demands\ncleautomationextendsbeyondonlyconsideringtraditional of previously adopted large-scale models make on-board\nsafetyandefficiencymetricsbutalsoconsidersunderstand- deployment infeasible, forcing reliance on cloud-based in-\ning users’ implicit instructions and providing personalized ferencing. Thissolutiondependsonstableinternetconnec-\ndriving experiences [11, 12]. Personalized driving experi- tivityandcanintroducesignificantlatencyissuesinthein-\nencesarecrucialforuseracceptanceandtrust,astheyhelp ference process, with response times reaching up to 3 or\nbridgethegapbetweenautonomoustechnologyandhuman 4 seconds [26]. This is incompatible with the reliable and\nexpectations. Thistrendreflectsagrowingrecognitionthat nearreal-timerequirementsofautonomousdriving.\n4202\nvoN\n71\n]IA.sc[\n1v31911.1142:viXra\nToaddresstheselimitations, weproposeanovelVLM- prompts with detailed road agent information for more ef-\nbased framework for real-time personalized autonomous fective planning [35]. Moreover, some VQA datasets in-\nvehicle control. Our system enables efficient on-board cludelocalizedobjectdatatobetteranticipatethefuturebe-\ndeployment while maintaining strong instruction under- havior of potential risk objects [34, 38]. Unlike existing\nstanding and scene reasoning capabilities. We present the works that primarily focus on VLMs operating in simula-\nfirst-of-its-kindreal-worldimplementationofanon-board tion environments, our approach addresses generating ac-\nVLM-basedpersonalizedmotioncontrolsystem. Themain tionpoliciesthatcanbeadaptedtoreal-worldvehicle-level\ncontributionsofthisworkare: applicationsusingVLMs.\n• We develop an efficient on-board VLM that achieves 2.2. Personalization/Human-AI Alignment in Au-\ncomparable reasoning capabilities to cloud-based solu- tonomousDriving\ntions while operating independently of internet connec-\nThere has been an increased focus on the personalization\ntivity. Ourlightweightsolutionaddressescriticalcompu-\nof autonomous driving, aiming to follow the driving expe-\ntationalconstraintsforreal-worldautonomousvehicles.\nriencetoindividualpreferencesandneeds[2,16]. Inrecent\n• Weproposeanovelapproachtotranslatediversehuman\ndevelopments, various studies have explored personalized\ninstructionsandvisualinputsintoactionablecontrolpoli-\nadaptive cruise control systems, focusing on steady-state\ncies, handling both explicit commands (‘go faster’) and\noperation [57], Gaussian process-based methods [48, 49],\nimplicitfeedback(‘Ifeeluncomfortable’)anddiverseen-\nTransformer and RNN integration [40] for enhanced user\nvironmentconditions.\nexperience and driving preferences. Schrum et al. in-\n• WeintroduceaRAG-basedmemorymodulethatincorpo-\ntroduced MAVERIC, a novel framework for personaliz-\nrates human feedback for continuous learning and adap-\ning driving styles, which demonstrated the ability to adapt\ntation,enablingpersonalizeddrivingexperiencesthrough\nto humans’ driving preferences influenced by personality\niterativerefinementofcontrolstrategies.\ntraits and perceived similarity [41]. Buzdugan et al. fo-\n• Through extensive real-world deployment and experi-\ncused on safety-critical scenarios, using driver-in-the-loop\nments,wedemonstratesafe,comfortable,andreliableau-\nsimulations to detect and adapt to personalized driving\ntonomousdrivingperformanceacrossvariousinstruction\nstyles, thus bridging the gap between human driving be-\ntypes and successful personalization capabilities, reduc-\nhavior and autonomous vehicle predictability [5]. Ma et\ningtakeoverratesbyupto76.9%.\nal.[31]leveragedRAGwhichisanapproachthatenhances\nmodelcapabilitiesbyretrievingrelevanthistoricalinforma-\n2.RelatedWorks\ntion to augment the generation process[20] to learn from\n2.1.Vision-LanguageModelsforAutonomousDriv- human feedback. There are also studies on systems of-\ning fering personalized experiences to predict human actions\nand increase human trust instead of directly altering vehi-\nEarlyapplicationsoflanguagemodelsinautonomousdriv-\ncle control [14, 23, 45, 51, 55]. However, such personal-\ning involved human-guided planning, integrating natural\nizationframeworksoftenencounterlimitationssuchasdy-\nlanguage commands or advice, and generating language-\nnamically adapting to human preferences or unseen traffic\nbased interpretations or control signals for vehicle opera-\nscenarios. This is where VLMs could potentially comple-\ntions [18, 19, 27]. With the advent of VLMs, initial ef-\nmentthesesystemsbyofferingmorecomplexandcontext-\nforts focused on image-based models [3, 13, 21, 25] that\naware adaptations, leveraging their advanced language un-\nutilizedimageencodersandbridgingmodulesconnectedto\nderstandingandgenerativecapabilities.\nLLMs. More recently, video-based VLMs have emerged\n[22, 29, 56], enhancing their applicability in autonomous 3. On-Board Vision-Language Models for Per-\ndrivingbyprocessingimagesequencescrucialforreal-time\nsonalizedMotionControl\ndecision-making. Fine-tuned with instruction-following\ndatasets specific to driving scenes, these VLMs address Inthissection,wepresentouron-boardVLMforpersonal-\ntasks such as visual question answering (VQA) to achieve izedmotioncontrolinautonomousdriving,designedtoac-\nacomprehensiveunderstandingoftrafficscenariosandthe commodate individual driving styles. Our approach lever-\nbehavioroftheego-vehicle[30,37]. Additionally,predict- agesacompact9B-parameterVLM,fine-tunedfromQwen-\ning future waypoints has become a prominent task within VL[3],whichprocessesbothvisualinformation(including\nthis domain, often employing Chain-of-Thought mecha- weather conditions, road types, and traffic situations) and\nnisms to improve planning by generating text sequences verbal commands to generate personalized control strate-\nfor perception, prediction, and planning [43, 47]. Certain giesforeachuser. ThereducedscaleofthisVLMenables\nmodelsdonotsolelyrelyonimageinputs;instead,theyin- edge deployment while maintaining command interpreta-\ncorporate perception and prediction submodules to enrich tion and reasoning capabilities, allowing the system to ef-\nFigure1.Anoverviewoftheproposedframeworkforpersonalizedautonomousvehiclemotioncontrol.Thesystemworkflowbeginswith\nprocessingfourinputstreams(SystemMessageS, HumanInstructionI, CameraImageV, andHistoricalMemoryH)throughanon-\nboardVLM,whichgeneratespersonalizedactionpoliciesP containingMPCandPIDcontrolparametermatrices.Thesepoliciesarethen\nexecutedthroughthevehicle’sdrive-by-wiresystem.Ifthehumanevaluates,humanfeedbackF iscollectedandstoredintheRAG-based\nmemorymoduleforcontinuouslearningandadaptationofthesystem’sbehaviortoindividualpreferences.\nfectivelyunderstandandrespondtoimplicithumaninstruc- evant historical scenarios H are retrieved and provided to\ntions. TheoverallframeworkisshowninFig. 1. the VLM as reference. After each trip, users can provide\nfeedbackF onthegeneratedcontrolpolicyP forthecur-\n3.1.ProblemStatement\nrentsituations(includinginstructionsIandvisualinputV),\nIn this paper, we adopt the traditional module-based au- which helps refine the VLM’s reasoning process. Subse-\ntonomousdrivingframeworkthatincludesthefullpipeline quently,theinstructionsI,scenedescriptionD,policyP,\nfromperceptiontomotioncontrol,andourfocusisspecifi- andfeedbackF arepackagedasahistoricaldataentryand\ncallyonenhancingthedecision-makingprocessatthemo- storedintheRAGdatabase. Therefore,therearethreepro-\ntion control level, adapting the control performance to ac- ceduresinoursystem:\ncommodatedifferenthumandrivingstyles. Thegoalofthe VLMExecution: P ←V −L −M −−f(I,S,V,H);\nproposedsystemistotranslatebothverbalcommandsIand\nHuman (1)\nvisual inputs V into executable control sequences for the HumanFeedback: F ←−−−−[I,V,P];\nmotioncontrolprocess. TheonboardVLMactsasatrans- MemoryUpdate: H ←[I,D,P,F]\nlationmechanismf : (I,V) → P thatgeneratesapolicy\n3.2.SystemInput\nP,whichisthenfedintopredefinedmaneuverprograms.\nAdditionally, system messages S are sent to our VLM AsillustratedinFig. 1,ourfine-tunedon-boardVLMpro-\ntospecifyboththetasksandadjustmentstrategies. Inprac- cesses four critical inputs for reasoning. The primary in-\ntice, S is generated through a predefined language gener- putsconsistofvisualdataV fromtheonboardcameraand\nator. These system messages define the VLM’s role and natural language commands I which are converted from\nobjectives, and provide explicit instructions for the tuning verbal human instructions I using the open-source local\nstrategy. speech recognition model ‘Whisper [39].’ Notably, due to\nSimultaneously, to further enhance personalization, we the advanced reasoning and understanding ability of our\nimplement a RAG system called the memory module to fine-tuned VLM, our system can interpret implicit expres-\nbuild a database storing historical human-vehicle interac- sions from humans such as “It is nice weather, I want to\ntions. Whenever a human activates this system, only rel- enjoytheview.”Thisabilitytounderstandimplicitinstruc-\ntionsiscrucial,asuserstypicallycommunicatethroughnat- 3.4.RAG-EnhancedMemoryModule\nuralconversationalphrasesratherthanexplicitvalue-based\nGiventhatour8B-parameterVLMlackstheextensiverea-\ncommands containing exact parameters. Furthermore, our\nsoning capabilities of larger, 100B-200B parameter mod-\nsystemleveragescontextualandenvironmentalinformation\nels, we employ a RAG-based approach [20] and integrate\ncaptured in the visual inputs V, including weather condi-\namemorymoduletoenhancereasoningandenablehuman\ntions, traffic situations, and road characteristics. For in-\nfeedback learning. This system is built upon the Chroma\nstance, the system automatically adopts a more conserva-\nvectordatabase[8],enablingefficientstoragehistoryinter-\ntive driving policy during nighttime operations or adverse\nactionsandretrievalofsimilardrivingscenarios.\nweatherconditions.\nThe memory module is uniquely created for each user,\nAdditionally, a pre-defined system message generator ensuringapersonalizeddrivingexperiencefollowsindivid-\nis employed to produce a customized system message S, ualpreferencesandpatterns. Itstoresdriving-relatedinfor-\nwhich is then simultaneously sent to the VLM. This mes- mationinastructuredformatcomprisingcommandspaired\nsage includes essential information about the system, in- withcorrespondingcontexttuples:\ncludingtheuser’sidentity,specificobjectives,andkeyprin-\nciples guiding the system’s behavior, particularly how to {I−(I,D,P,F)} (3)\nutilizethecontrollerortuneparameters.\nWhen processing a new driving scenario, the instruction I\nFurthermore,theVLMincorporatesrelevantinteraction\nisusedforsimilaritymatchingtoretrievethetop-ksimilar\nhistoryHextractedfromourmemorymoduleascontextual\nprior situations. The associated data values are then sent\ninput,whichincludesprevioushumaninstructionsI,scene\ntotheVLM,enhancingdecision-makingwithrelevantcon-\ndescriptionsD, executedactionsA, anduserfeedbackF.\ntext and supporting personalization. This RAG-enhanced\nThis historical context enables the VLM to generate more\nmemoryenablestheVLMtohandlesimilarsituationswith\nappropriate responses by considering past interactions and\ngreater accuracy and consistency, improving the vehicle’s\nhumanfeedback. Forexample,ifauserhaspreviouslyex-\nresponsiveness to unique user preferences and enhancing\npressedapreferenceforcautiousdrivingincertainscenar-\ntheoveralldrivingexperience.\nios, the system can capture this preference into its current\ndecision-making process, ensuring more personalized and 3.5.Multi-ControllerJointMotionControl\ncontextually appropriate responses. A detailed discussion\nAsshowninFig.1,weimplementadecoupledcontrolstrat-\nofhowourmemorymoduleworkswillbepresentedinsub-\negy that separates lateral and longitudinal vehicle motion\nsection3.4.\ncontrol. ThelateralcontrolishandledbyMPCcalculating\nthe longitudinal acceleration α, while longitudinal control\n3.3.ReasoningandActionGeneration\nis managed through a PID controller calculating the front\nsteeringangleδ .Themotionplanningmoduleintheupper\nInourapproach,reasoningwithintheVLMframeworken- f\nlayer provides trajectories consisting of waypoints, which\nablestheinterpretationofdiversedrivingscenariosanduser\nour motion control system tracks. The calculated α and\ninstructionstogenerateactionableoutputs.Traditionalcon-\nδ are then transmitted to the drive-by-wire system devel-\ntrollers in motion control typically rely on a default set of f\nopedbyAutonomousStuff[1]forprecisecontrolofthrottle,\nparameters; however, following the approach in [42], our\nbraking,andsteering.\nVLM will generate two distinct action matrices to sepa-\nFor the longitudinal control, the PID controller calcu-\nrately manage the PID controller for longitudinal move-\nlates the α for each time step ∆t to minimize the velocity\nment and the MPC for lateral movement. These matrices\nerrore ,whichisthedifferencebetweenthecurrentveloc-\ntranslatethemodel’sunderstandingoftheenvironmentand v\nityV andthedesiredvelocityV .\nuserpreferencesintoprecisecontrolactions,guidingtheau- current ref\ntonomousvehicle’sbehavior. Specifically,theyareusedby t\nthecontrollerstogenerateaccelerationaandsteeringangle α(t)=K e (t)+K\n(cid:88)\ne (i)∆t+K\n∆e v(t)\n(4)\np v i v d ∆t\nδ f,whichareexecutedbythevehicle’sECU.TheECUthen i=0\nsendslow-levelcontrolsignalstothedrive-by-wiresystem\nwhereK ,K ,andK aretheproportionalterms,integra-\ndeveloped by AutonomousStuff [1], enabling smooth and p i d\ntionterms,andderivativetermsthatwillbecontainedinthe\nresponsive vehicle operation. The general process of this\nactionmatrixgeneratedbyourVLM.\nsubsectioncanbeshownbelow:\nForthelateralcontrol,ourMPCapproachutilizesalin-\n(cid:20) (cid:21) earvehicledynamicsmodel[44]topredictfuturestatesand\nK K K\nOutputActionP := p i d optimizethefrontsteeringangle,δ ,overafiniteprediction\nW W W f\nl h s (2) horizon. With the prediction model[44], the control incre-\nActionExecutionP −C −−on −t −ro −l −le −r →s [δ ,a]→ECU mentisthenobtainedbysolvingaQuadraticProgram(QP)\nf\n[4]tominimizethecostfunctionJ intheMPC: trained model weights and introducing trainable, low-rank\ndecompositionmatricesintoeachlayeroftheTransformer\nJ =ETQE+∆TR∆ (5)\nf f architecture. Thisapproachsignificantly reducesthenum-\nwhereE isthepredictedstatecalculatedbytheprediction ber of trainable parameters required, making fine-tuning\nmodel,and∆ isthefuturecontrolinput. TheQandRare moreefficient.\nf\nweighting matrices that penalize tracking state deviations Thefine-tuningprocessforourVLMisconductedona\nand control effort. Our VLM generates the action matrix cluster of four NVIDIA A100 GPUs, each equipped with\nthat primarily considers three key components: the weight 40GB of memory. The model is trained over five epochs\nfor lateral error (W ∈ Q), the weight for heading error withaper-devicebatchsizeoftwofortrainingandonefor\nl\n(W ∈ Q), and the weight for the squared terms of speed validation, using a learning rate of 1e-5. Additionally, we\nh\nandsteeringinputs(W ∈ R). Theseweightingfactorsare implementedgradientaccumulationwitheightsteps,allow-\ns\nselectedastheydemonstratethemostsignificantimpacton ingforeffectivelargerbatchprocessing. Thissetupenables\nlateralcontrolperformance. theentiretrainingprocesstobecompletedinapproximately\nfivehours, ensuringbothaccuracyandefficiencyinmodel\n3.6.EfficientOn-BoardVLMModule\nperformance.\nWe generate a dataset of 10,000 image-instruction pairs,\neachlabeledwiththedesiredaction,tocreateacomprehen-\nCompression and On-Board Deployment of VLM\nsive training set for fine-tuning our on-board VLM. This\nAWQ [24] is a hardware-friendly technique for low-bit,\nVLM is based on the Qwen-VL architecture [3], which\nweight-only quantization, specifically designed for VLM.\nwe fine-tune using the Low-Rank Adaptation (LoRA)\nAWQ minimizes quantization error by identifying the 1%\nmethod [15] (a type of Parameter Efficient Fine-Tuning\nsalient weights, which are then scaled using an equivalent\n(PEFT)[53]),enablingsignificantcustomizationwhilepre-\ntransformationtopreservetheirprecision. WeapplyAWQ\nserving computational efficiency. To optimize for on-\nto quantize our model to INT4, achieving improved quan-\nboarddeployment,weapply4-bitActivation-AwareWeight\ntization performance suited for on-board deployment. Ad-\nQuantization (AWQ) [24], compressing the VLM to in-\nditionally,weutilizetheLMDeploytoolkit[9]tooptimize\ncrease inference speed without sacrificing too much accu-\ninferencetime. Thisenhancementismadepossiblethrough\nracy. Thiscombinationoftechniquesensuresaresponsive,\nfeaturessuchaspersistentbatching,blockedKVcache,ten-\non-boardVLMsuitedtoreal-timeresponse.\nsorparallelism,andoptimizedCUDAkernels,allofwhich\ncontributetohigh-performance,low-latencyoperation.\nDataset Collection We develop a specialized training\ndataset to fine-tune the Qwen-VL model [3], consisting of 4.Real-WorldExperiment\n1,200 semi-human-annotated image-text pairs. Each im-\nage,representingatrafficscenesourcedfromtheNuScenes Tocomprehensivelyevaluateoursystem’sperformance,we\ndataset, whichincludesnumerousdiversetrafficscenarios, conductaseriesofexperimentsassessingitsabilitytopro-\nis paired with a human-provided instruction and a corre- vide safe, comfortable, reliable, and personalized driving\nspondingactionlabelintheformofacontrolleractionma- experiences. We employ multiple evaluation metrics: a\ntrix,guidingthemodel’sresponseindifferenttrafficscenar- driving score to measure driving performance, including\nios. safety, comfort, and alignment with environmental condi-\nThe human instructions are also very diverse, ranging tionsandhumaninstructions; takeoverfrequencytoassess\nfrom explicit commands like ‘speed up’ to more implicit personalization capabilities; and evaluator-based assess-\ncues such as ‘I am in an urgent situation.’ This diversity ments to investigate trustworthiness, reliability, and user\nallows the VLM to interpret both clear and vague inputs, satisfaction. Additionally,weperformanablationstudyto\nimproving its ability to understand complex human inten- examinetheeffectivenessofthememorymodule.\ntions. To enhance the model’s responsiveness to different\n4.1.ExperimentSetup\ndriving styles, we annotate each image with three differ-\nentinstructiontypes—aggressive,moderate,andconserva- AutonomousVehicleSetup: AsshowninFig. 2,weuse\ntive—eachpairedwithacorrespondingaction. Thismulti- anautonomousvehicletoconductreal-worldexperiments,\nfacetedapproachensuresthattheVLMcanadaptitsbehav- whichisadrive-by-wire-enabled2019LexusRX450h. We\nior to match various driving styles, enabling it to respond deploytheopen-sourceautonomousdrivingsoftwareAuto-\nflexiblyandcontextuallyacrossdiversetrafficconditions. ware.AI [17] with ROS Melodic in Ubuntu 18.04. We use\n3D-NDT [32, 33] for mapping and localization. An Ap-\nLoRAFinetune WeapplytheLoRAmethodtofine-tune tiv ESR 2.5 radar, a Velodyne VLP-32C LiDAR, and two\nour Qwen-VL model. LoRA works by freezing the pre- Mako-G319Ccamerasaredeployedonthevehicletoenable\nTable1.DrivingPerformanceValidation.↓:LowerValuesareBetter.↑:HigherValuesareBetter.\nSafetyMetrics ComfortMetrics TimeEfficiency Alignment\nSD cr ei nv ain rig o Model CoT lli im sioe nt (o s)↑ (mS 2/V sx 2)↓ (10−2S mV 2y /s2)↓ (m|α /¯ sx 2| )↓ (m|J /¯ sx 3| )↓ (10−| 1α m¯y /| s2)↓ (m| /J¯ sy 3| )↓ LateL nL cM y(s)↓ AC lo igm nm ma en nd t↑ AS lic ge nn mar ei no t↑ DrivingScore↑\nBaseline 2.44 28.8 0.36 0.78 3.27 0.46 0.44 - 92.0 60.0 75.6\nAcceleration GPT-4o 2.52 30.0 0.39 0.83 3.40 0.52 0.52 5.82 92.9 71.3 76.4\nOurs 2.46 30.8 0.39 0.81 3.07 0.53 0.81 1.98 96.3 60.9 76.5\nBaseline 2.44 3.91 1.65 0.37 3.14 0.88 1.01 - 88.5 60.0 74.5\nLane\nGPT-4o 2.71 3.88 2.23 0.53 4.38 1.13 1.39 4.84 90.4 88.6 78.4\nChange\nOurs 2.15 4.07 2.15 0.41 3.35 0.98 1.02 1.83 92.2 71.9 77.5\nBaseline - 1.12 7.52 0.22 1.81 1.29 1.36 - 88.0 60.0 70.4\nLeft\nGPT-4o - 0.93 11.5 0.29 2.75 2.11 2.40 5.23 91.3 85.0 71.4\nTurn\nOurs - 0.94 6.74 0.19 1.67 1.33 1.32 1.64 90.2 67.8 74.4\nInstructionDirectness Inthefieldoflinguistics,instruc-\ntions can range from simple to complex in terms of how\ndirectly they convey intent [54]. To evaluate our model’s\nnaturallanguageunderstandingcapabilities,weclassifyin-\nstructionsintothreelevelsofincreasingcomplexity: Level\n1consistsofstraightforwardcommandsthatexplicitlystate\nthedesiredaction;Level2includesmoderatelycomplexin-\nstructionsthatmayrequiresomecontextualunderstanding;\nLevel3representssophisticatedcommandsthatinvolveim-\nFigure2.Overviewoftheexperimentsetup.\nplicitmeaningsorcomplexconditions.\ntheperceptioncapabilities. Theon-boardcomputerhasan\n4.2.SystemDrivingPerformance\nInteli9-99009thGen3.10/5.0GHzhexa-core65Wproces-\nsorwitheightcoresand16threads,64GBRAM,NVIDIA To showcase our system’s driving performance, we con-\nQuadroRTX-A400016GBGPU,and512GBNVMesolid ductcomparativeexperimentsagainsttwosystems: abase-\nstatedrive. linesystemusingpre-definedcontrollerparametersforgen-\neralsafetyandcomfortandasystemutilizingGPT-4owith\nfew-shotlearning. Weevaluateacrossthreeprimarymeta-\nTestTrackandParticipants Thefieldexperiments1aim drivingscenarios(acceleration,lanechanging,andturning),\natvalidatingthereal-worldperformanceofourpersonalized witheachscenariotestedundertendifferentcommandsand\nmotion control system. We include three types of driving five weather conditions (sunny, rain, fog, snow, and night)\nbehaviors—accelerating, lane changing, and turning—to totestthemodel’svisionunderstandingasshowninFig.3.\ncomprehensivelyvalidatecontroloversteering,throttle,and\nbraking.Anoverviewofthetesttrackanddrivingbehaviors\nEvaluate Metrics The models are then assessed based\nis shown in Fig. 2. For both acceleration and lane change\non four key aspects—safety, comfort, time efficiency, and\nscenarios,aleadvehicleispositioned30maheadoftheego\nalignment,asshowninTab.1. AnoveralldrivingscoreSis\nvehicle,acceleratingfromstaticto45km/hwithanaccel-\nthencalculatedasaweightedsumoftheindividualmetric\nerationof1.26m/s2. Intheaccelerationscenario,theego\nscores,denotedas:\nvehicleacceleratesfromacompletestoptoreach50km/h.\nIn the lane change scenario, the ego vehicle maintains 50 (cid:88)\nS = w ·S (6)\nkm/h while overtaking the lead vehicle. For the intersec- k k\ntionturningscenario,theegovehiclenavigatesacurvewith\nwherekincludesalltenmetrics:TimetoCollision(τ),lon-\naradiusof23.89mataconstantspeedof30km/h.\ngitudinalandlateralspeedvariance(SV andSV ),lateral\nx y\nOurstudyincludessevenparticipantswithdiversedemo-\nandlongitudinalmeanabsoluteacceleration(|α¯ |and|α¯ |),\nx y\ngraphiccharacteristics.Theparticipantsconsistedof61.4% lateralandlongitudinalmeanabsolutejerk(|J¯|and|J¯|),\nx y\nmaleand28.6%femaledrivers,withagesrangingfrom23\nLLM Latency, Command Alignment and Scenario Align-\nto 30 years (mean = 26.42, std = 3.24). Their driving ex-\nment. All the scores S range from 0 to 100, while the\nk\nperiencevariesconsiderably(mean=6.42years,std=4.27\nweightsofscoresw areempiricallytunedforeachdriving\nk\nyears). AllparticipantsholdvalidU.S.drivinglicenses.\nscenario. Forinstance,longitudinalparametershavehigher\nweights in acceleration scenarios, while lateral parameters\n1Theexperimentsconductedinthisstudysatisfyalllocaltrafficguide-\nareweightedmoreheavilyinturningscenarios. Forsafety\nlinesandguaranteethesafetyoftheparticipants. Ahumanalwayssitsin\nmetrics, we set a critical Time to Collision threshold τ =\nthedriver’sseatoftheautonomousvehicletomonitoritsstatusandget\nreadytotakeover. 1.5topreventpotentialcollisions. Othermetricslikespeed\nFigure3.Samplevisioninputsfromdifferentweatherscenarios.\nvariance, acceleration, and jerk are scored relative to the\nbaselinemodel.\nThealignmentevaluationconsistsoftwoaspects. Com-\nmandAlignmentquantifiesthedeviationbetweenthemodel\noutput and the expected parameter range, calculated as a\nweighted average across six control parameters. For each\nparameter, we establish three ranges based on past experi-\nments,correspondingtoaggressive,conservative,ormoder-\natedrivingstyles. TakingthePIDcontroller’sproportional\nFigure4. Takeoverratecomparisonbetweenthebaselineandour\nparameterK pasanexample,thescoreiscalculatedas: method.\nvehicle controller with default settings, where two uni-\n100(Kp−Kp,min),\nK ∈[K ,K ),\n1K 00p, ,lower−Kp,min\nK\npp\n∈[K\npp ,, lm owin er,Kp p,l ,o uw ppe er\nr),\nfi\na pn\nre odd\nal\ncc\na\nho ten art gar ao\nl\nil\nr\nnl ee ssr tps oem\nuc\nrta\ni\nVvn ea Llg Mye\n.\n-v\nW\nbe ah\ne\nsi ecc dole\nm\nado\np\nap\na\npe\nr\ntr\ne\nia vt\nt\nei ho min\ns\nos\nc\ntoo ionn nvl\ne\nco\nn\non\nt\nnig\no\nti rnt ou\na\nld\nl\nsi an ypa\ns-\n-l\nS∗ = (7)\nKp 01\nK\n,00 p( ,mK ap x,m −a Kx− p,uK ppp er), eK\nlsp\ne.∈[K p,upper,K p,max], t\nv\nwe im\nid te\nh.\ne\nvT\nx\nah\np\nryr lo\ni ic\nnu\ni\ngg th\ni\ndno esu gtt\nr ru\neth\nc\nete\nsio\noe nx fsp de\no\nir rri em\ncim\nte nn\np\net\nl\nss\ni\nsc, ,ip\nt\npa\np\nrr orti\ne\nmc fi\ne\npp\nr\nta\ne\nin nnt gcs\ne\ntsc h/o\nf\neu eel sd\nyd\nsbp\nta\ner co mk-\ntomakecorrespondingadjustments. Theinstructionswere\nwhere K p,min and K p,max are the minimum and max- categorizedintothreelevelsofdirectness,asdefinedinSub-\nimum overall parameter range obtained through experi- section4.1.\nments, while K p,lower and K p,upper are determined by the Everyparticipantissupposedtoprovideatleastfivein-\ncommandintentionlabeledbyhumanexperts.Thescenario structions for each scenario. For each instruction-scenario\nalignment score computes whether the model can capture pair,participantscompletedtwotrips-onewiththebaseline\ndetails of the scene through vision input and act more ag- systemandonewithourVLMsolution.Toensureunbiased\ngressivelyorconservativelybasedonthecurrentcondition. evaluation,participantsarenotinformedwhichsystemthey\nItiscalculatedbytallyingthepercentageofinstanceswhere areusingduringeachtrip. Weusethetakeoverrateasour\nthemodelgivesmoreconservativeparametersetsinadverse primary metric to evaluate the system’s ability to provide\nweatherconditionscomparedtothesunnyclearscenarios. personalizeddrivingexperiences.\nThe results demonstrate that our VLM-based method\nResult AsshowninTab.1,thecommandalignmentscore consistently outperforms the baseline system across all\nof our VLM model is similar to or greater than GPT-4o, three levels of instruction directness. With Level 1 (di-\nshowingourmodelhashighreasoningcapabilitiesregard- rect) instructions, our method achieves a 5.56% takeover\ning the personalization of control parameters. As for the rateversusthebaseline’s19.44%,representinga71.4%re-\nscenario alignment, our model shows significantly better duction. For Level 2 (moderately direct) instructions, the\nperformancethanbaselineconditionsinlanechangingand rates are 6.06% versus 12.12%, showing a 50% improve-\nleft turn scenarios but a score very similar to the baseline ment. Most notably, for Level 3 (indirect) instructions,\nscenario. We think this is mostly due to the model con- our method achieves 8.33% versus the baseline’s 36.11%,\nsidering acceleration on a clear straight road in less dan- marking a 76.9% reduction in takeover rate. The result\ngerous situations and thus does not act too conservatively. demonstrates our system’s better capability in interpreting\nIn terms of the overall Driving Score, our model also sur- andactingbasedonuserpreferences,regardlessofhowex-\npasses the performance of baseline models and even GPT- plicitlytheyarecommunicated.\n4o in some scenarios, indicating our model provides the\n4.4.Evaluator-basedAssessment\nmostwell-roundeddrivingexperience.\nToassesstheimpactofoursystemontrust,reliability,per-\n4.3.Human-in-the-LoopValidation\nsonalization, and understandability, we conduct a survey\nThis subsection evaluates the effectiveness of our method tocaptureparticipants’attitudesfromvariousperspectives,\nin reducing takeover rates compared to the baseline sys- rated on a scale from 1 (low) to 5 (high). Participants rate\ntem. The baseline consists of the traditional autonomous thematchofpersonalizeddrivingperformance,systemreli-\nPreference Match Reliability Trust Understandability\n5.0 p = 0.0330 p = 0.0087 p = 0.0003 5.0 p = 0.0166 p = 0.0257 p = 0.0003 5.0 p = 0.0670 p = 0.0373 p = 0.0005 5.0 p = 0.0642 p = 0.0086 p = 0.0006\n4.5 4.5 4.5 4.5\n4.0 4.0 4.0 4.0\n3.5 3.5 3.5 3.5\n3.0 3.0 3.0 3.0\n2.5 2.5 2.5 2.5\n2.0 2.0 2.0 2.0\n1.5 1.5 1.5 1.5\n1.0 1.0 1.0 1.0\nAcceleration Lane Change Left Turn Acceleration Lane Change Left Turn Acceleration Lane Change Left Turn Acceleration Lane Change Left Turn\nFigure5. MediancomparisonbetweenVLM-basedmodelandthebaselineundervariousdrivingscenarios. Orangerepresentsbaseline\nwhileBluerepresentsourmethod.\ntheaveragetakeoverrateincreasessubstantiallyto24.44%.\nThebaselinesystemperformsworstwitha44.44%average\ntakeover rate. These results indicate a 72.7% reduction in\ntakeoverratewhenaddingthememorymoduletoourbase\narchitectureandan85%overallreductioncomparedtothe\nbaseline. This significant improvement suggests that the\nRAG-based memory module plays a crucial role in main-\ntainingpersonalizedvehiclecontrolbyeffectivelyutilizing\nFigure6.Effectivenessofmemorymodulesintakeoverrates historicalinteractionsanduserpreferences.\nability,leveloftrust,andunderstandabilityofthesystem’s\n5.Conclusion\nuser interface after each trip. Similar to the previous sec-\ntion, participants are unaware of which system they were\nInthispaper,wepresentedanon-boardVLM-basedframe-\nusing. TheresultsaredisplayedinFig. 5.\nwork designed to enhance motion control tasks in au-\nAfter collecting data, we conduct a Wilcoxon signed-\ntonomous driving, offering a more human-centric and re-\nranktest[52],arobustnon-parametricapproachforpaired\nsponsive user experience. Our personalized motion con-\ndata, to investigate the effectiveness of the VLM-based\ntrolsystemrepresentsanovelintegrationofVLMsintoau-\nmethod. Thenullhypothesis(H )positsthatthemedianof\n0 tonomousdriving,offeringthreesignificantcontributionsto\nthe baseline is greater than or equal to that of our method,\nhuman-centricdrivingexperiences. First,throughitsRAG-\nwhilethealternativehypothesis(H )suggeststhattheme-\n1 based memory module, the system demonstrates advanced\ndian of the baseline is less than that of our method. Us-\npersonalizationcapabilities,effectivelylearningandadapt-\ning a significance level of p < .05 to reject H , our anal-\n0 ing to individual driving preferences while maintaining\nysis shows that the VLM-based method significantly out-\nsafetyandcomfortstandardsacrossvariousscenarios. Sec-\nperforms the baseline across multiple metrics, including\nond, the fine-tuned VLM-based framework leverages mul-\nmatchingpersonalizeddrivingbehaviorsanddemonstrating\ntimodalreasoningtounderstandbothcomplexvisualscene\ngreaterreliability. Inmorechallengingscenarios(e.g.,lane\ninformation and implicit natural language instructions, en-\nchanges, left turns), our VLM-based method’s advantages\nabling human-like human-vehicle interaction. Finally, the\nareparticularlymoreevident,leadingtoincreasedtrustand\nsystem achieves remarkable computational efficiency with\nimprovedunderstandabilitycomparedtothebaseline.\nanoptimized9BVLMimplementation,processingwithan\naverage1.6-secondlatencyandlessthan16GBGPUmem-\n4.5.AblationStudyonMemoryModule\nory consumption on standard vehicle hardware, making it\nTo further validate the effectiveness of our RAG-based feasibleforcommercialdeployment.Throughexperiments,\nMemoryModule(MM),weconductanablationstudywith we demonstrated that our VLM-based approach enhanced\nthree human participants, comparing three configurations: drivingreliability, trustworthiness, andpersonalization, re-\nour complete system with MM, the system without MM ducingtheneedforhumantakeoverbyupto76.8%while\n(W/O MM), and the baseline. The results demonstrate the maintainingnearreal-timeresponse. Thisframeworkcon-\nsignificantimpactoftheMMonreducingtakeoverrates. tributed to enhancing personalized autonomous driving by\nAs shown in Fig. 6, With three participants, our com- aligning vehicle behavior with individual user preferences\nplete system with the memory module achieves the low- andconsideringenvironmentalinformation,markingasig-\nest average takeover rate of 6.67%. When we remove the nificantsteptowardamoreadaptable,user-centeredhuman-\nmemorymodulewhilekeepingothercomponentsthesame, autonomyteamingsolution.\nerocS erocS erocS erocS\nReferences [13] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\n[1] AutonomousStuff. “autonomousstuff”,2023. 4\nFung, and Steven Hoi. Instructblip: Towards general-\n[2] Il Bae, Jaeyoung Moon, Junekyo Jhung, Ho Suk, Taewoo\npurpose vision-language models with instruction tuning,\nKim,HyungbinPark,JaekwangCha,JinhyukKim,Dohyun\n2023. 2\nKim, and Shiho Kim. Self-driving like a human driver in-\n[14] RunjiaDu,KyungtaeHan,RohitGupta,SikaiChen,Samuel\nsteadofarobocar:Personalizedcomfortabledrivingexperi-\nLabi,andZiranWang.Drivermonitoring-basedlane-change\nenceforautonomousvehicles,2022. 2\nprediction:Apersonalizedfederatedlearningframework.In\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\n2023IEEEIntelligentVehiclesSymposium(IV),pages1–7.\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nIEEE,2023. 1,2\nZhou. Qwen-vl: Aversatilevision-languagemodelforun-\n[15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nderstanding,localization,textreading,andbeyond,2023. 2,\nZhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.\n5\nLora: Low-rankadaptationoflargelanguagemodels,2021.\n[4] AlbertoBemporad,ManfredMorari,VivekDua,andEfstra-\n5\ntiosNPistikopoulos. Theexplicitsolutionofmodelpredic-\n[16] Chao Huang, Hailong Huang, Peng Hang, Hongbo Gao,\ntivecontrolviamultiparametricquadraticprogramming. In\nJingda Wu, Zhiyu Huang, and Chen Lv. Personalized tra-\nProceedingsofthe2000Americancontrolconference.ACC\njectory planning and control of lane-change maneuvers for\n(IEEECat.No.00CH36334),pages872–876.IEEE,2000.5\nautonomousdriving. IEEETransactionsonVehicularTech-\n[5] Ioana-Diana Buzdugan, Silviu Butnariu, Ioana-Alexandra\nnology,70(6):5511–5523,2021. 2\nRosu, Andrei-Cristian Pridie, and Csaba Antonya. Per-\n, [17] ShinpeiKato,EijiroTakeuchi,YoshioIshiguro,YoshikiNi-\nsonalized driving styles in safety-critical scenarios for au-\nnomiya, Kazuya Takeda, and Tsuyoshi Hamada. An open\ntonomous vehicles: An approach using driver-in-the-loop\napproach to autonomous vehicles. IEEE Micro, 35(6):60–\nsimulations. Vehicles,5(3):1149–1166,2023. 2\n68,2015. 5\n[6] SimeonCCalvert,Danie¨lDHeikoop,GiulioMecacci,and\n[18] Jinkyu Kim, Teruhisa Misu, Yi-Ting Chen, Ashish Tawari,\nBartVanArem.Ahumancentricframeworkfortheanalysis\nand John Canny. Grounding human-to-vehicle advice for\nof automated driving systems based on meaningful human\nself-drivingvehicles,2019. 2\ncontrol. TheoreTical issues in ergonomics science, 21(4):\n[19] JinkyuKim,SuhongMoon,AnnaRohrbach,TrevorDarrell,\n478–506,2020. 1\nandJohnCanny.Advisablelearningforself-drivingvehicles\n[7] LongChen,OlegSinavski,JanHu¨nermann,AliceKarnsund,\nbyinternalizingobservation-to-actionrules. InProceedings\nAndrewJamesWillmott, DannyBirch, DanielMaund, and\noftheIEEE/CVFConferenceonComputerVisionandPat-\nJamieShotton. Drivingwithllms: Fusingobject-levelvec-\nternRecognition,pages9661–9670,2020. 2\ntor modality for explainable autonomous driving. In 2024\n[20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nIEEEInternationalConferenceonRoboticsandAutomation\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich\n(ICRA),pages14093–14100.IEEE,2024. 1\nKu¨ttler, Mike Lewis, Wen-tau Yih, Tim Rockta¨schel, et al.\n[8] Chroma. The AI-native open-source embedding database,\nRetrieval-augmentedgenerationforknowledge-intensivenlp\n2023. 4\ntasks. AdvancesinNeuralInformationProcessingSystems,\n[9] LMDeploy Contributors. Lmdeploy: A toolkit for com-\n33:9459–9474,2020. 2,4\npressing,deploying,andservingllm. https://github.\ncom/InternLM/lmdeploy,2023. 5 [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\n[10] CanCui, YunshengMa, XuCao, WenqianYe, YangZhou,\nfrozenimageencodersandlargelanguagemodels,2023. 2\nKaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang,\n[22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nKuei-DaLiao,TianrenGao,ErlongLi,KunTang,Zhipeng\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nCao, Tong Zhou, Ao Liu, Xinrui Yan, Shuqi Mei, Jianguo\nVideochat:Chat-centricvideounderstanding,2024. 2\nCao,ZiranWang,andChaoZheng.Asurveyonmultimodal\nlargelanguagemodelsforautonomousdriving. InProceed- [23] XishunLiao,XuanpengZhao,ZiranWang,ZhouqiaoZhao,\ningsoftheIEEE/CVFWinterConferenceonApplicationsof KyungtaeHan,RohitGupta,MatthewJBarth,andGuoyuan\nComputerVision(WACV)Workshops,pages958–979,2024. Wu. Driver digital twin for online prediction of personal-\n1 izedlanechangebehavior. IEEEInternetofThingsJournal,\n[11] CanCui,YunshengMa,ZichongYang,YupengZhou,Peiran 2023. 2\nLiu, Juanwu Lu, Lingxi Li, Yaobin Chen, Jitesh H. Pan- [24] JiLin,JiamingTang,HaotianTang,ShangYang,Wei-Ming\nchal,AmrAbdelraouf,RohitGupta,KyungtaeHan,andZi- Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang,\nranWang. Largelanguagemodelsforautonomousdriving ChuangGan,andSongHan. Awq:Activation-awareweight\n(llm4ad): Concept,benchmark,simulation,andreal-vehicle quantizationforllmcompressionandacceleration,2024. 5\nexperiment,2024. 1 [25] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.\n[12] Can Cui, Zichong Yang, Yupeng Zhou, Yunsheng Ma, Visualinstructiontuning,2023. 2\nJuanwuLu,LingxiLi,YaobinChen,JiteshPanchal,andZi- [26] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang\nranWang. Personalizedautonomousdrivingwithlargelan- Yuan,ZhaoSong,AnshumaliShrivastava,CeZhang,Yuan-\nguagemodels:Fieldexperiments,2024. 1 dongTian,ChristopherRe,andBeidiChen. Dejavu: Con-\ntextualsparsityforefficientLLMsatinferencetime. InPro- [40] NoveenSachdeva,ZiranWang,KyungtaeHan,RohitGupta,\nceedings of the 40th International Conference on Machine andJulianMcAuley. Gapformer: Fastautoregressivetrans-\nLearning,pages22137–22176.PMLR,2023. 1 formersmeetrnnsforpersonalizedadaptivecruisecontrol.\n[27] KekeLong,HaotianShi,JiaxiLiu,andXiaopengLi. Vlm- In 2022 IEEE 25th International Conference on Intelligent\nmpc:Visionlanguagefoundationmodel(vlm)-guidedmodel Transportation Systems (ITSC), pages 2528–2535. IEEE,\npredictive controller (mpc) for autonomous driving. arXiv 2022. 2\npreprintarXiv:2408.04821,2024. 2 [41] Mariah L. Schrum, Emily Sumner, Matthew C. Gombolay,\nandAndrewBest. Maveric: Adata-drivenapproachtoper-\n[28] Shanhe Lou, Zhongxu Hu, Yiran Zhang, Yixiong Feng,\nsonalizedautonomousdriving,2023. 2\nMengchuZhou,andChenLv.Human-cyber-physicalsystem\n[42] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu,\nforindustry5.0:Areviewfromahuman-centricperspective.\nPing Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei\nIEEETrans.Autom.Sci.Eng,pages1–18,2024. 1\nZhan, and Mingyu Ding. Languagempc: Large language\n[29] RuipuLuo,ZiwangZhao,MinYang,JunweiDong,DaLi,\nmodels as decision makers for autonomous driving, 2023.\nPengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and\n4\nZhongyuWei. Valley: Videoassistantwithlargelanguage\n[43] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen,\nmodelenhancedability,2023. 2\nHanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo,\n[30] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and\nAndreasGeiger, andHongyangLi. Drivelm: Drivingwith\nChaowei Xiao. Dolphins: Multimodal language model for\ngraphvisualquestionanswering,2024. 2\ndriving,2023. 1,2\n[44] JarrodMSnideretal. Automaticsteeringmethodsforau-\n[31] Yunsheng Ma, Can Cui, Xu Cao, Wenqian Ye, Peiran Liu, tonomousautomobilepathtracking.RoboticsInstitute,Pitts-\nJuanwuLu, AmrAbdelraouf, RohitGupta, KyungtaeHan, burgh,PA,Tech.Rep.CMU-RITR-09-08,2009. 4\nAniket Bera, et al. Lampilot: An open benchmark dataset [45] XuSun,JingpengLi,PinyanTang,SiyuanZhou,Xiangjun\nforautonomousdrivingwithlanguagemodelprograms. In Peng,HaoNanLi,andQingfengWang. Exploringperson-\nProceedingsoftheIEEE/CVFConferenceonComputerVi- alisedautonomousvehiclestoinfluenceusertrust.Cognitive\nsionandPatternRecognition,pages15141–15151,2024. 2 Computation,12:1170–1186,2020. 2\n[32] Martin Magnusson. The three-dimensional normal- [46] XiaoyuTian,JunruGu,BailinLi,YichengLiu,ChenxuHu,\ndistributions transform: an efficient representation for reg- YangWang,KunZhan,PengJia,XianpengLang,andHang\nistration, surfaceanalysis, andloopdetection. PhDthesis, Zhao.DriveVLM:TheConvergenceofAutonomousDriving\nO¨rebrouniversitet,2009. 5 andLargeVision-LanguageModels. arXiv,2024. 1\n[33] Martin Magnusson, Achim Lilienthal, and Tom Duckett. [47] XiaoyuTian,JunruGu,BailinLi,YichengLiu,YangWang,\nScanregistrationforautonomousminingvehiclesusing3d- Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and\nndt. JournalofFieldRobotics,24(10):803–827,2007. 5 Hang Zhao. Drivevlm: The convergence of autonomous\n[34] Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, drivingandlargevision-languagemodels,2024. 2\nJianhua Han, Hang Xu, and Li Zhang. Reason2drive: [48] YanbingWang,ZiranWang,KyungtaeHan,PrashantTiwari,\nTowards interpretable and chain-based reasoning for au- and Daniel B Work. Personalized adaptive cruise control\ntonomousdriving,2024. 2 viagaussianprocessregression. In2021IEEEInternational\nIntelligentTransportationSystemsConference(ITSC),pages\n[35] ChenbinPan,BurhaneddinYaman,TommasoNesti,Abhirup\n1496–1502.IEEE,2021. 1,2\nMallik, Alessandro G Allievi, Senem Velipasalar, and Liu\n[49] YanbingWang,ZiranWang,KyungtaeHan,PrashantTiwari,\nRen.Vlp:Visionlanguageplanningforautonomousdriving,\nand Daniel B Work. Gaussian process-based personalized\n2024. 2\nadaptive cruise control. IEEE Transactions on Intelligent\n[36] ChenbinPan,BurhaneddinYaman,TommasoNesti,Abhirup\nTransportationSystems,23(11):21178–21189,2022. 1,2\nMallik, Alessandro G. Allievi, Senem Velipasalar, and Liu\n[50] Ziran Wang, Xishun Liao, Chao Wang, David Oswald,\nRen.VLP:VisionLanguagePlanningforAutonomousDriv-\nGuoyuan Wu, Kanok Boriboonsomsin, Matthew J Barth,\ning. InCVPR,2024. 1\nKyungtaeHan,BaekGyuKim,andPrashantTiwari. Driver\n[37] SungYeonPark,MinJaeLee,JiHyukKang,HahyeonChoi,\nbehavior modeling using game engine and real vehicle: A\nYoonahPark,JuhwanCho,AdamLee,andDongKyuKim. learning-basedapproach. IEEETransactionsonIntelligent\nVlaad: Visionandlanguageassistantforautonomousdriv- Vehicles,5(4):738–749,2020. 1\ning. InProceedingsoftheIEEE/CVFWinterConferenceon\n[51] ZiranWang,RohitGupta,KyungtaeHan,HaoxinWang,Ak-\nApplicationsofComputerVision(WACV)Workshops,pages\nila Ganlath, Nejib Ammar, and Prashant Tiwari. Mobility\n980–987,2024. 1,2\ndigital twin: Concept, architecture, case study, and future\n[38] TianwenQian,JingjingChen,LinhaiZhuo,YangJiao,and challenges. IEEEInternetofThingsJournal,9(18):17452–\nYu-Gang Jiang. Nuscenes-qa: A multi-modal visual ques- 17467,2022. 2\ntionansweringbenchmarkforautonomousdrivingscenario, [52] FrankWilcoxon. Individualcomparisonsbyrankingmeth-\n2024. 2 ods. InBreakthroughsinstatistics:Methodologyanddistri-\n[39] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, bution,pages196–202.Springer,1992. 8\nChristine McLeavey, and Ilya Sutskever. Robust speech [53] LinglingXu,HaoranXie,Si-ZhaoJoeQin,XiaohuiTao,and\nrecognitionvialarge-scaleweaksupervision,2022. 3 Fu Lee Wang. Parameter-efficient fine-tuning methods for\npretrained language models: A critical review and assess-\nment,2023. 5\n[54] GeorgeYule. Thestudyoflanguage. Cambridgeuniversity\npress,2022. 6\n[55] Cong Zhang, Chi Tian, Tianfang Han, Hang Li, Yiheng\nFeng, Yunfeng Chen, Robert W. Proctor, and Jiansong\nZhang. Evaluation of infrastructure-based warning system\nondrivingbehaviors-aroundaboutstudy,2023. 2\n[56] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tunedaudio-visuallanguagemodelforvideoun-\nderstanding,2023. 2\n[57] Zhouqiao Zhao, Ziran Wang, Rohit Gupta, Kyungtae Han,\nand Prashant Tiwari. Personalized adaptive cruise control\nbased on steady-state operation, 2023. US Patent App.\n17/578,330. 2",
    "pdf_filename": "On-Board_Vision-Language_Models_for_Personalized_Autonomous_Vehicle_Motion_Control_System_Design_and.pdf"
}