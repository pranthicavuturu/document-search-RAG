{
    "title": "On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control System Design and",
    "abstract": "Personalized driving refers to an autonomous vehicle’s ability to adapt its driving behavior or control strategies to match individual users’ preferences and driving styles while maintaining safety and comfort standards. However, existing works either fail to capture every individual pref- erence precisely or become computationally inefficient as the user base expands. Vision-Language Models (VLMs) offer promising solutions to this front through their natu- ral language understanding and scene reasoning capabili- ties. In this work, we propose a lightweight yet effective on- board VLM framework that provides low-latency person- alized driving performance while maintaining strong rea- soning capabilities. Our solution incorporates a Retrieval- Augmented Generation (RAG)-based memory module that enables continuous learning of individual driving prefer- ences through human feedback. Through comprehensive real-world vehicle deployment and experiments, our sys- tem has demonstrated the ability to provide safe, comfort- able, and personalized driving experiences across various scenarios and significantly reduce takeover rates by up to 76.9%. To the best of our knowledge, this work represents the first end-to-end VLM-based motion control system in real-world autonomous vehicles.",
    "body": "On-Board Vision-Language Models for Personalized Autonomous Vehicle\nMotion Control: System Design and Real-World Validation\nCan Cui1 , Zichong Yang1 , Yupeng Zhou1 , Juntong Peng1,\nSung-Yeon Park1, Cong Zhang1, Yunsheng Ma1, Xu Cao2, Wenqian Ye3,\nYiheng Feng1, Jitesh Panchal1, Lingxi Li1, Yaobin Chen1, Ziran Wang1\n1Purdue University\n2University of Illinois Urbana-Champaign\n3University of Virginia\n{cancui, ziran}@purdue.edu\nAbstract\nPersonalized driving refers to an autonomous vehicle’s\nability to adapt its driving behavior or control strategies\nto match individual users’ preferences and driving styles\nwhile maintaining safety and comfort standards. However,\nexisting works either fail to capture every individual pref-\nerence precisely or become computationally inefficient as\nthe user base expands. Vision-Language Models (VLMs)\noffer promising solutions to this front through their natu-\nral language understanding and scene reasoning capabili-\nties. In this work, we propose a lightweight yet effective on-\nboard VLM framework that provides low-latency person-\nalized driving performance while maintaining strong rea-\nsoning capabilities. Our solution incorporates a Retrieval-\nAugmented Generation (RAG)-based memory module that\nenables continuous learning of individual driving prefer-\nences through human feedback.\nThrough comprehensive\nreal-world vehicle deployment and experiments, our sys-\ntem has demonstrated the ability to provide safe, comfort-\nable, and personalized driving experiences across various\nscenarios and significantly reduce takeover rates by up to\n76.9%. To the best of our knowledge, this work represents\nthe first end-to-end VLM-based motion control system in\nreal-world autonomous vehicles.\n1. Introduction\nThe autonomous driving industry is experiencing an evo-\nlution towards human-centric systems [6, 28], where vehi-\ncle automation extends beyond only considering traditional\nsafety and efficiency metrics but also considers understand-\ning users’ implicit instructions and providing personalized\ndriving experiences [11, 12]. Personalized driving experi-\nences are crucial for user acceptance and trust, as they help\nbridge the gap between autonomous technology and human\nexpectations. This trend reflects a growing recognition that\nsuccessful adoption of the autonomous vehicle requires not\njust technically self-driving, but also the ability to provide\nhuman-like driving experiences that align with individual\npreferences and expectations [11, 12].\nPrevious work in personalized autonomous driving has\nfollowed two main approaches. The first uses clustering al-\ngorithms to classify drivers into broad categories (e.g., ag-\ngressive or conservative), but this fails to capture individ-\nual nuances and preferences, forcing users into predefined\ngroups that may not match their actual driving style [48, 50].\nThe second approach develops individual models for each\nuser through learning-based methods [14, 49], but this re-\nquires extensive training data per user and becomes compu-\ntationally inefficient as the user base grows. Furthermore,\nthese methods lack the ability to reason about real-time hu-\nman instructions or adapt to changing environments.\nRecent advances in Vision-Language Models (VLMs)\nhave demonstrated promising capabilities in understanding\ncomplex driving scenarios and natural language instructions\nthrough their integration of computer vision and language\nprocessing [7, 10, 30, 37]. The development in VLMs has\nled researchers to leverage VLMs’ multimodal understand-\ning capabilities to enhance both perception and decision-\nmaking in autonomous systems [36, 46]. However, there\nremains a research gap in leveraging VLMs to enhance con-\ntrol policies or adapt them to individual driving preferences\nand styles. This gap is particularly evident in the challenge\nof translating a high-level understanding of human prefer-\nences and scenario information into actionable low-level\ncontrol policies. Additionally, the computational demands\nof previously adopted large-scale models make on-board\ndeployment infeasible, forcing reliance on cloud-based in-\nferencing. This solution depends on stable internet connec-\ntivity and can introduce significant latency issues in the in-\nference process, with response times reaching up to 3 or\n4 seconds [26]. This is incompatible with the reliable and\nnear real-time requirements of autonomous driving.\narXiv:2411.11913v1  [cs.AI]  17 Nov 2024\n\nTo address these limitations, we propose a novel VLM-\nbased framework for real-time personalized autonomous\nvehicle control.\nOur system enables efficient on-board\ndeployment while maintaining strong instruction under-\nstanding and scene reasoning capabilities. We present the\nfirst-of-its-kind real-world implementation of an on-board\nVLM-based personalized motion control system. The main\ncontributions of this work are:\n• We develop an efficient on-board VLM that achieves\ncomparable reasoning capabilities to cloud-based solu-\ntions while operating independently of internet connec-\ntivity. Our lightweight solution addresses critical compu-\ntational constraints for real-world autonomous vehicles.\n• We propose a novel approach to translate diverse human\ninstructions and visual inputs into actionable control poli-\ncies, handling both explicit commands (‘go faster’) and\nimplicit feedback (‘I feel uncomfortable’) and diverse en-\nvironment conditions.\n• We introduce a RAG-based memory module that incorpo-\nrates human feedback for continuous learning and adap-\ntation, enabling personalized driving experiences through\niterative refinement of control strategies.\n• Through extensive real-world deployment and experi-\nments, we demonstrate safe, comfortable, and reliable au-\ntonomous driving performance across various instruction\ntypes and successful personalization capabilities, reduc-\ning takeover rates by up to 76.9%.\n2. Related Works\n2.1. Vision-Language Models for Autonomous Driv-\ning\nEarly applications of language models in autonomous driv-\ning involved human-guided planning, integrating natural\nlanguage commands or advice, and generating language-\nbased interpretations or control signals for vehicle opera-\ntions [18, 19, 27]. With the advent of VLMs, initial ef-\nforts focused on image-based models [3, 13, 21, 25] that\nutilized image encoders and bridging modules connected to\nLLMs. More recently, video-based VLMs have emerged\n[22, 29, 56], enhancing their applicability in autonomous\ndriving by processing image sequences crucial for real-time\ndecision-making.\nFine-tuned with instruction-following\ndatasets specific to driving scenes, these VLMs address\ntasks such as visual question answering (VQA) to achieve\na comprehensive understanding of traffic scenarios and the\nbehavior of the ego-vehicle [30, 37]. Additionally, predict-\ning future waypoints has become a prominent task within\nthis domain, often employing Chain-of-Thought mecha-\nnisms to improve planning by generating text sequences\nfor perception, prediction, and planning [43, 47]. Certain\nmodels do not solely rely on image inputs; instead, they in-\ncorporate perception and prediction submodules to enrich\nprompts with detailed road agent information for more ef-\nfective planning [35]. Moreover, some VQA datasets in-\nclude localized object data to better anticipate the future be-\nhavior of potential risk objects [34, 38]. Unlike existing\nworks that primarily focus on VLMs operating in simula-\ntion environments, our approach addresses generating ac-\ntion policies that can be adapted to real-world vehicle-level\napplications using VLMs.\n2.2. Personalization/Human-AI Alignment in Au-\ntonomous Driving\nThere has been an increased focus on the personalization\nof autonomous driving, aiming to follow the driving expe-\nrience to individual preferences and needs [2, 16]. In recent\ndevelopments, various studies have explored personalized\nadaptive cruise control systems, focusing on steady-state\noperation [57], Gaussian process-based methods [48, 49],\nTransformer and RNN integration [40] for enhanced user\nexperience and driving preferences.\nSchrum et al.\nin-\ntroduced MAVERIC, a novel framework for personaliz-\ning driving styles, which demonstrated the ability to adapt\nto humans’ driving preferences influenced by personality\ntraits and perceived similarity [41]. Buzdugan et al. fo-\ncused on safety-critical scenarios, using driver-in-the-loop\nsimulations to detect and adapt to personalized driving\nstyles, thus bridging the gap between human driving be-\nhavior and autonomous vehicle predictability [5]. Ma et\nal. [31] leveraged RAG which is an approach that enhances\nmodel capabilities by retrieving relevant historical informa-\ntion to augment the generation process[20] to learn from\nhuman feedback.\nThere are also studies on systems of-\nfering personalized experiences to predict human actions\nand increase human trust instead of directly altering vehi-\ncle control [14, 23, 45, 51, 55]. However, such personal-\nization frameworks often encounter limitations such as dy-\nnamically adapting to human preferences or unseen traffic\nscenarios. This is where VLMs could potentially comple-\nment these systems by offering more complex and context-\naware adaptations, leveraging their advanced language un-\nderstanding and generative capabilities.\n3. On-Board Vision-Language Models for Per-\nsonalized Motion Control\nIn this section, we present our on-board VLM for personal-\nized motion control in autonomous driving, designed to ac-\ncommodate individual driving styles. Our approach lever-\nages a compact 9B-parameter VLM, fine-tuned from Qwen-\nVL [3], which processes both visual information (including\nweather conditions, road types, and traffic situations) and\nverbal commands to generate personalized control strate-\ngies for each user. The reduced scale of this VLM enables\nedge deployment while maintaining command interpreta-\ntion and reasoning capabilities, allowing the system to ef-\n\nFigure 1. An overview of the proposed framework for personalized autonomous vehicle motion control. The system workflow begins with\nprocessing four input streams (System Message S, Human Instruction I, Camera Image V , and Historical Memory H) through an on-\nboard VLM, which generates personalized action policies P containing MPC and PID control parameter matrices. These policies are then\nexecuted through the vehicle’s drive-by-wire system. If the human evaluates, human feedback F is collected and stored in the RAG-based\nmemory module for continuous learning and adaptation of the system’s behavior to individual preferences.\nfectively understand and respond to implicit human instruc-\ntions. The overall framework is shown in Fig. 1.\n3.1. Problem Statement\nIn this paper, we adopt the traditional module-based au-\ntonomous driving framework that includes the full pipeline\nfrom perception to motion control, and our focus is specifi-\ncally on enhancing the decision-making process at the mo-\ntion control level, adapting the control performance to ac-\ncommodate different human driving styles. The goal of the\nproposed system is to translate both verbal commands I and\nvisual inputs V into executable control sequences for the\nmotion control process. The onboard VLM acts as a trans-\nlation mechanism f : (I, V ) →P that generates a policy\nP , which is then fed into predefined maneuver programs.\nAdditionally, system messages S are sent to our VLM\nto specify both the tasks and adjustment strategies. In prac-\ntice, S is generated through a predefined language gener-\nator. These system messages define the VLM’s role and\nobjectives, and provide explicit instructions for the tuning\nstrategy.\nSimultaneously, to further enhance personalization, we\nimplement a RAG system called the memory module to\nbuild a database storing historical human-vehicle interac-\ntions. Whenever a human activates this system, only rel-\nevant historical scenarios H are retrieved and provided to\nthe VLM as reference. After each trip, users can provide\nfeedback F on the generated control policy P for the cur-\nrent situations (including instructions I and visual input V ),\nwhich helps refine the VLM’s reasoning process. Subse-\nquently, the instructions I, scene description D, policy P ,\nand feedback F are packaged as a historical data entry and\nstored in the RAG database. Therefore, there are three pro-\ncedures in our system:\nVLM Execution :\nP\nV LM\n←−−−−f(I, S, V, H);\nHuman Feedback :\nF\nHuman\n←−−−−[I, V, P];\nMemory Update :\nH ←[I, D, P, F]\n(1)\n3.2. System Input\nAs illustrated in Fig. 1, our fine-tuned on-board VLM pro-\ncesses four critical inputs for reasoning. The primary in-\nputs consist of visual data V from the onboard camera and\nnatural language commands I which are converted from\nverbal human instructions I using the open-source local\nspeech recognition model ‘Whisper [39].’ Notably, due to\nthe advanced reasoning and understanding ability of our\nfine-tuned VLM, our system can interpret implicit expres-\nsions from humans such as “It is nice weather, I want to\nenjoy the view.” This ability to understand implicit instruc-\n\ntions is crucial, as users typically communicate through nat-\nural conversational phrases rather than explicit value-based\ncommands containing exact parameters. Furthermore, our\nsystem leverages contextual and environmental information\ncaptured in the visual inputs V , including weather condi-\ntions, traffic situations, and road characteristics.\nFor in-\nstance, the system automatically adopts a more conserva-\ntive driving policy during nighttime operations or adverse\nweather conditions.\nAdditionally, a pre-defined system message generator\nis employed to produce a customized system message S,\nwhich is then simultaneously sent to the VLM. This mes-\nsage includes essential information about the system, in-\ncluding the user’s identity, specific objectives, and key prin-\nciples guiding the system’s behavior, particularly how to\nutilize the controller or tune parameters.\nFurthermore, the VLM incorporates relevant interaction\nhistory H extracted from our memory module as contextual\ninput, which includes previous human instructions I, scene\ndescriptions D, executed actions A, and user feedback F .\nThis historical context enables the VLM to generate more\nappropriate responses by considering past interactions and\nhuman feedback. For example, if a user has previously ex-\npressed a preference for cautious driving in certain scenar-\nios, the system can capture this preference into its current\ndecision-making process, ensuring more personalized and\ncontextually appropriate responses. A detailed discussion\nof how our memory module works will be presented in sub-\nsection 3.4.\n3.3. Reasoning and Action Generation\nIn our approach, reasoning within the VLM framework en-\nables the interpretation of diverse driving scenarios and user\ninstructions to generate actionable outputs. Traditional con-\ntrollers in motion control typically rely on a default set of\nparameters; however, following the approach in [42], our\nVLM will generate two distinct action matrices to sepa-\nrately manage the PID controller for longitudinal move-\nment and the MPC for lateral movement. These matrices\ntranslate the model’s understanding of the environment and\nuser preferences into precise control actions, guiding the au-\ntonomous vehicle’s behavior. Specifically, they are used by\nthe controllers to generate acceleration a and steering angle\nδf, which are executed by the vehicle’s ECU. The ECU then\nsends low-level control signals to the drive-by-wire system\ndeveloped by AutonomousStuff [1], enabling smooth and\nresponsive vehicle operation. The general process of this\nsubsection can be shown below:\nOutput Action P :=\n\u0014Kp\nKi\nKd\nWl\nWh\nWs\n\u0015\nAction Execution P\nControllers\n−−−−−−−−→[δf, a] →ECU\n(2)\n3.4. RAG-Enhanced Memory Module\nGiven that our 8B-parameter VLM lacks the extensive rea-\nsoning capabilities of larger, 100B-200B parameter mod-\nels, we employ a RAG-based approach [20] and integrate\na memory module to enhance reasoning and enable human\nfeedback learning. This system is built upon the Chroma\nvector database [8], enabling efficient storage history inter-\nactions and retrieval of similar driving scenarios.\nThe memory module is uniquely created for each user,\nensuring a personalized driving experience follows individ-\nual preferences and patterns. It stores driving-related infor-\nmation in a structured format comprising commands paired\nwith corresponding context tuples:\n{I −(I, D, P, F)}\n(3)\nWhen processing a new driving scenario, the instruction I\nis used for similarity matching to retrieve the top-k similar\nprior situations. The associated data values are then sent\nto the VLM, enhancing decision-making with relevant con-\ntext and supporting personalization. This RAG-enhanced\nmemory enables the VLM to handle similar situations with\ngreater accuracy and consistency, improving the vehicle’s\nresponsiveness to unique user preferences and enhancing\nthe overall driving experience.\n3.5. Multi-Controller Joint Motion Control\nAs shown in Fig. 1, we implement a decoupled control strat-\negy that separates lateral and longitudinal vehicle motion\ncontrol. The lateral control is handled by MPC calculating\nthe longitudinal acceleration α, while longitudinal control\nis managed through a PID controller calculating the front\nsteering angle δf. The motion planning module in the upper\nlayer provides trajectories consisting of waypoints, which\nour motion control system tracks. The calculated α and\nδf are then transmitted to the drive-by-wire system devel-\noped by AutonomousStuff [1] for precise control of throttle,\nbraking, and steering.\nFor the longitudinal control, the PID controller calcu-\nlates the α for each time step ∆t to minimize the velocity\nerror ev, which is the difference between the current veloc-\nity Vcurrent and the desired velocity Vref.\nα(t) = Kpev(t) + Ki\nt\nX\ni=0\nev(i)∆t + Kd\n∆ev(t)\n∆t\n(4)\nwhere Kp, Ki, and Kd are the proportional terms, integra-\ntion terms, and derivative terms that will be contained in the\naction matrix generated by our VLM.\nFor the lateral control, our MPC approach utilizes a lin-\near vehicle dynamics model [44] to predict future states and\noptimize the front steering angle, δf, over a finite prediction\nhorizon. With the prediction model[44], the control incre-\nment is then obtained by solving a Quadratic Program (QP)\n\n[4] to minimize the cost function J in the MPC:\nJ = ET QE + ∆T\nf R∆f\n(5)\nwhere E is the predicted state calculated by the prediction\nmodel, and ∆f is the future control input. The Q and R are\nweighting matrices that penalize tracking state deviations\nand control effort. Our VLM generates the action matrix\nthat primarily considers three key components: the weight\nfor lateral error (Wl ∈Q), the weight for heading error\n(Wh ∈Q), and the weight for the squared terms of speed\nand steering inputs (Ws ∈R). These weighting factors are\nselected as they demonstrate the most significant impact on\nlateral control performance.\n3.6. Efficient On-Board VLM Module\nWe generate a dataset of 10,000 image-instruction pairs,\neach labeled with the desired action, to create a comprehen-\nsive training set for fine-tuning our on-board VLM. This\nVLM is based on the Qwen-VL architecture [3], which\nwe fine-tune using the Low-Rank Adaptation (LoRA)\nmethod [15] (a type of Parameter Efficient Fine-Tuning\n(PEFT) [53]), enabling significant customization while pre-\nserving computational efficiency.\nTo optimize for on-\nboard deployment, we apply 4-bit Activation-Aware Weight\nQuantization (AWQ) [24], compressing the VLM to in-\ncrease inference speed without sacrificing too much accu-\nracy. This combination of techniques ensures a responsive,\non-board VLM suited to real-time response.\nDataset Collection\nWe develop a specialized training\ndataset to fine-tune the Qwen-VL model [3], consisting of\n1,200 semi-human-annotated image-text pairs. Each im-\nage, representing a traffic scene sourced from the NuScenes\ndataset, which includes numerous diverse traffic scenarios,\nis paired with a human-provided instruction and a corre-\nsponding action label in the form of a controller action ma-\ntrix, guiding the model’s response in different traffic scenar-\nios.\nThe human instructions are also very diverse, ranging\nfrom explicit commands like ‘speed up’ to more implicit\ncues such as ‘I am in an urgent situation.’ This diversity\nallows the VLM to interpret both clear and vague inputs,\nimproving its ability to understand complex human inten-\ntions. To enhance the model’s responsiveness to different\ndriving styles, we annotate each image with three differ-\nent instruction types—aggressive, moderate, and conserva-\ntive—each paired with a corresponding action. This multi-\nfaceted approach ensures that the VLM can adapt its behav-\nior to match various driving styles, enabling it to respond\nflexibly and contextually across diverse traffic conditions.\nLoRA Finetune\nWe apply the LoRA method to fine-tune\nour Qwen-VL model. LoRA works by freezing the pre-\ntrained model weights and introducing trainable, low-rank\ndecomposition matrices into each layer of the Transformer\narchitecture. This approach significantly reduces the num-\nber of trainable parameters required, making fine-tuning\nmore efficient.\nThe fine-tuning process for our VLM is conducted on a\ncluster of four NVIDIA A100 GPUs, each equipped with\n40GB of memory. The model is trained over five epochs\nwith a per-device batch size of two for training and one for\nvalidation, using a learning rate of 1e-5. Additionally, we\nimplemented gradient accumulation with eight steps, allow-\ning for effective larger batch processing. This setup enables\nthe entire training process to be completed in approximately\nfive hours, ensuring both accuracy and efficiency in model\nperformance.\nCompression and On-Board Deployment of VLM\nAWQ [24] is a hardware-friendly technique for low-bit,\nweight-only quantization, specifically designed for VLM.\nAWQ minimizes quantization error by identifying the 1%\nsalient weights, which are then scaled using an equivalent\ntransformation to preserve their precision. We apply AWQ\nto quantize our model to INT4, achieving improved quan-\ntization performance suited for on-board deployment. Ad-\nditionally, we utilize the LMDeploy toolkit [9] to optimize\ninference time. This enhancement is made possible through\nfeatures such as persistent batching, blocked KV cache, ten-\nsor parallelism, and optimized CUDA kernels, all of which\ncontribute to high-performance, low-latency operation.\n4. Real-World Experiment\nTo comprehensively evaluate our system’s performance, we\nconduct a series of experiments assessing its ability to pro-\nvide safe, comfortable, reliable, and personalized driving\nexperiences.\nWe employ multiple evaluation metrics: a\ndriving score to measure driving performance, including\nsafety, comfort, and alignment with environmental condi-\ntions and human instructions; takeover frequency to assess\npersonalization capabilities; and evaluator-based assess-\nments to investigate trustworthiness, reliability, and user\nsatisfaction. Additionally, we perform an ablation study to\nexamine the effectiveness of the memory module.\n4.1. Experiment Setup\nAutonomous Vehicle Setup:\nAs shown in Fig. 2, we use\nan autonomous vehicle to conduct real-world experiments,\nwhich is a drive-by-wire-enabled 2019 Lexus RX450h. We\ndeploy the open-source autonomous driving software Auto-\nware.AI [17] with ROS Melodic in Ubuntu 18.04. We use\n3D-NDT [32, 33] for mapping and localization. An Ap-\ntiv ESR 2.5 radar, a Velodyne VLP-32C LiDAR, and two\nMako-G319C cameras are deployed on the vehicle to enable\n\nTable 1. Driving Performance Validation. ↓: Lower Values are Better. ↑: Higher Values are Better.\nDriving\nScenario\nModel\nSafety Metrics\nComfort Metrics\nTime Efficiency\nAlignment\nDriving Score↑\nTime to\nCollision(s)↑\nSVx\n(m2/s2)↓\nSVy\n(10−2m2/s2)↓\n| ¯\nαx|\n(m/s2)↓\n| ¯Jx|\n(m/s3)↓\n| ¯\nαy|\n(10−1m/s2)↓\n| ¯Jy|\n(m/s3)↓\nLLM\nLatency(s)↓\nCommand\nAlignment↑\nScenario\nAlignment↑\nAcceleration\nBaseline\n2.44\n28.8\n0.36\n0.78\n3.27\n0.46\n0.44\n-\n92.0\n60.0\n75.6\nGPT-4o\n2.52\n30.0\n0.39\n0.83\n3.40\n0.52\n0.52\n5.82\n92.9\n71.3\n76.4\nOurs\n2.46\n30.8\n0.39\n0.81\n3.07\n0.53\n0.81\n1.98\n96.3\n60.9\n76.5\nLane\nChange\nBaseline\n2.44\n3.91\n1.65\n0.37\n3.14\n0.88\n1.01\n-\n88.5\n60.0\n74.5\nGPT-4o\n2.71\n3.88\n2.23\n0.53\n4.38\n1.13\n1.39\n4.84\n90.4\n88.6\n78.4\nOurs\n2.15\n4.07\n2.15\n0.41\n3.35\n0.98\n1.02\n1.83\n92.2\n71.9\n77.5\nLeft\nTurn\nBaseline\n-\n1.12\n7.52\n0.22\n1.81\n1.29\n1.36\n-\n88.0\n60.0\n70.4\nGPT-4o\n-\n0.93\n11.5\n0.29\n2.75\n2.11\n2.40\n5.23\n91.3\n85.0\n71.4\nOurs\n-\n0.94\n6.74\n0.19\n1.67\n1.33\n1.32\n1.64\n90.2\n67.8\n74.4\nFigure 2. Overview of the experiment setup.\nthe perception capabilities. The on-board computer has an\nIntel i9-9900 9th Gen 3.10/5.0GHz hexa-core 65W proces-\nsor with eight cores and 16 threads, 64GB RAM, NVIDIA\nQuadro RTX-A4000 16GB GPU, and 512GB NVMe solid\nstate drive.\nTest Track and Participants\nThe field experiments1 aim\nat validating the real-world performance of our personalized\nmotion control system. We include three types of driving\nbehaviors—accelerating, lane changing, and turning—to\ncomprehensively validate control over steering, throttle, and\nbraking. An overview of the test track and driving behaviors\nis shown in Fig. 2. For both acceleration and lane change\nscenarios, a lead vehicle is positioned 30 m ahead of the ego\nvehicle, accelerating from static to 45 km/h with an accel-\neration of 1.26 m/s2. In the acceleration scenario, the ego\nvehicle accelerates from a complete stop to reach 50 km/h.\nIn the lane change scenario, the ego vehicle maintains 50\nkm/h while overtaking the lead vehicle. For the intersec-\ntion turning scenario, the ego vehicle navigates a curve with\na radius of 23.89 m at a constant speed of 30 km/h.\nOur study includes seven participants with diverse demo-\ngraphic characteristics. The participants consisted of 61.4%\nmale and 28.6% female drivers, with ages ranging from 23\nto 30 years (mean = 26.42, std = 3.24). Their driving ex-\nperience varies considerably (mean = 6.42 years, std = 4.27\nyears). All participants hold valid U.S. driving licenses.\n1The experiments conducted in this study satisfy all local traffic guide-\nlines and guarantee the safety of the participants. A human always sits in\nthe driver’s seat of the autonomous vehicle to monitor its status and get\nready to take over.\nInstruction Directness\nIn the field of linguistics, instruc-\ntions can range from simple to complex in terms of how\ndirectly they convey intent [54]. To evaluate our model’s\nnatural language understanding capabilities, we classify in-\nstructions into three levels of increasing complexity: Level\n1 consists of straightforward commands that explicitly state\nthe desired action; Level 2 includes moderately complex in-\nstructions that may require some contextual understanding;\nLevel 3 represents sophisticated commands that involve im-\nplicit meanings or complex conditions.\n4.2. System Driving Performance\nTo showcase our system’s driving performance, we con-\nduct comparative experiments against two systems: a base-\nline system using pre-defined controller parameters for gen-\neral safety and comfort and a system utilizing GPT-4o with\nfew-shot learning. We evaluate across three primary meta-\ndriving scenarios (acceleration, lane changing, and turning),\nwith each scenario tested under ten different commands and\nfive weather conditions (sunny, rain, fog, snow, and night)\nto test the model’s vision understanding as shown in Fig. 3.\nEvaluate Metrics\nThe models are then assessed based\non four key aspects—safety, comfort, time efficiency, and\nalignment, as shown in Tab. 1. An overall driving score S is\nthen calculated as a weighted sum of the individual metric\nscores, denoted as:\nS =\nX\nwk · Sk\n(6)\nwhere k includes all ten metrics: Time to Collision (τ), lon-\ngitudinal and lateral speed variance (SVx and SVy), lateral\nand longitudinal mean absolute acceleration (| ¯\nαx| and | ¯\nαy|),\nlateral and longitudinal mean absolute jerk (| ¯Jx| and | ¯Jy|),\nLLM Latency, Command Alignment and Scenario Align-\nment. All the scores Sk range from 0 to 100, while the\nweights of scores wk are empirically tuned for each driving\nscenario. For instance, longitudinal parameters have higher\nweights in acceleration scenarios, while lateral parameters\nare weighted more heavily in turning scenarios. For safety\nmetrics, we set a critical Time to Collision threshold τ =\n1.5 to prevent potential collisions. Other metrics like speed\n\nFigure 3. Sample vision inputs from different weather scenarios.\nvariance, acceleration, and jerk are scored relative to the\nbaseline model.\nThe alignment evaluation consists of two aspects. Com-\nmand Alignment quantifies the deviation between the model\noutput and the expected parameter range, calculated as a\nweighted average across six control parameters. For each\nparameter, we establish three ranges based on past experi-\nments, corresponding to aggressive, conservative, or moder-\nate driving styles. Taking the PID controller’s proportional\nparameter Kp as an example, the score is calculated as:\nS∗\nKp =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100(Kp−Kp,min)\nKp,lower−Kp,min ,\nKp ∈[Kp,min, Kp,lower),\n100,\nKp ∈[Kp,lower, Kp,upper),\n100(Kp,max−Kp)\nKp,max−Kp,upper ,\nKp ∈[Kp,upper, Kp,max],\n0,\nelse.\n(7)\nwhere Kp,min and Kp,max are the minimum and max-\nimum overall parameter range obtained through experi-\nments, while Kp,lower and Kp,upper are determined by the\ncommand intention labeled by human experts. The scenario\nalignment score computes whether the model can capture\ndetails of the scene through vision input and act more ag-\ngressively or conservatively based on the current condition.\nIt is calculated by tallying the percentage of instances where\nthe model gives more conservative parameter sets in adverse\nweather conditions compared to the sunny clear scenarios.\nResult\nAs shown in Tab. 1, the command alignment score\nof our VLM model is similar to or greater than GPT-4o,\nshowing our model has high reasoning capabilities regard-\ning the personalization of control parameters. As for the\nscenario alignment, our model shows significantly better\nperformance than baseline conditions in lane changing and\nleft turn scenarios but a score very similar to the baseline\nscenario. We think this is mostly due to the model con-\nsidering acceleration on a clear straight road in less dan-\ngerous situations and thus does not act too conservatively.\nIn terms of the overall Driving Score, our model also sur-\npasses the performance of baseline models and even GPT-\n4o in some scenarios, indicating our model provides the\nmost well-rounded driving experience.\n4.3. Human-in-the-Loop Validation\nThis subsection evaluates the effectiveness of our method\nin reducing takeover rates compared to the baseline sys-\ntem. The baseline consists of the traditional autonomous\nFigure 4. Takeover rate comparison between the baseline and our\nmethod.\nvehicle controller with default settings, where two uni-\nfied controllers manage vehicle operations on longitudinal\nand lateral respectively. We compare this conventional ap-\nproach against our VLM-based adaptive motion control sys-\ntem. Throughout the experiments, participants could pro-\nvide explicit instructions or implicit preferences/feedback\nwith varying degrees of directness, prompting the system\nto make corresponding adjustments. The instructions were\ncategorized into three levels of directness, as defined in Sub-\nsection 4.1.\nEvery participant is supposed to provide at least five in-\nstructions for each scenario. For each instruction-scenario\npair, participants completed two trips - one with the baseline\nsystem and one with our VLM solution. To ensure unbiased\nevaluation, participants are not informed which system they\nare using during each trip. We use the takeover rate as our\nprimary metric to evaluate the system’s ability to provide\npersonalized driving experiences.\nThe results demonstrate that our VLM-based method\nconsistently outperforms the baseline system across all\nthree levels of instruction directness.\nWith Level 1 (di-\nrect) instructions, our method achieves a 5.56% takeover\nrate versus the baseline’s 19.44%, representing a 71.4% re-\nduction. For Level 2 (moderately direct) instructions, the\nrates are 6.06% versus 12.12%, showing a 50% improve-\nment.\nMost notably, for Level 3 (indirect) instructions,\nour method achieves 8.33% versus the baseline’s 36.11%,\nmarking a 76.9% reduction in takeover rate.\nThe result\ndemonstrates our system’s better capability in interpreting\nand acting based on user preferences, regardless of how ex-\nplicitly they are communicated.\n4.4. Evaluator-based Assessment\nTo assess the impact of our system on trust, reliability, per-\nsonalization, and understandability, we conduct a survey\nto capture participants’ attitudes from various perspectives,\nrated on a scale from 1 (low) to 5 (high). Participants rate\nthe match of personalized driving performance, system reli-\n\nAcceleration\nLane Change\nLeft Turn\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nScore\np = 0.0330\np = 0.0087\np = 0.0003\nPreference Match\nAcceleration\nLane Change\nLeft Turn\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nScore\np = 0.0166\np = 0.0257\np = 0.0003\nReliability\nAcceleration\nLane Change\nLeft Turn\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nScore\np = 0.0670\np = 0.0373\np = 0.0005\nTrust\nAcceleration\nLane Change\nLeft Turn\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nScore\np = 0.0642\np = 0.0086\np = 0.0006\nUnderstandability\nFigure 5. Median comparison between VLM-based model and the baseline under various driving scenarios. Orange represents baseline\nwhile Blue represents our method.\nFigure 6. Effectiveness of memory modules in takeover rates\nability, level of trust, and understandability of the system’s\nuser interface after each trip. Similar to the previous sec-\ntion, participants are unaware of which system they were\nusing. The results are displayed in Fig. 5.\nAfter collecting data, we conduct a Wilcoxon signed-\nrank test [52], a robust non-parametric approach for paired\ndata, to investigate the effectiveness of the VLM-based\nmethod. The null hypothesis (H0) posits that the median of\nthe baseline is greater than or equal to that of our method,\nwhile the alternative hypothesis (H1) suggests that the me-\ndian of the baseline is less than that of our method. Us-\ning a significance level of p < .05 to reject H0, our anal-\nysis shows that the VLM-based method significantly out-\nperforms the baseline across multiple metrics, including\nmatching personalized driving behaviors and demonstrating\ngreater reliability. In more challenging scenarios (e.g., lane\nchanges, left turns), our VLM-based method’s advantages\nare particularly more evident, leading to increased trust and\nimproved understandability compared to the baseline.\n4.5. Ablation Study on Memory Module\nTo further validate the effectiveness of our RAG-based\nMemory Module (MM), we conduct an ablation study with\nthree human participants, comparing three configurations:\nour complete system with MM, the system without MM\n(W/O MM), and the baseline. The results demonstrate the\nsignificant impact of the MM on reducing takeover rates.\nAs shown in Fig. 6, With three participants, our com-\nplete system with the memory module achieves the low-\nest average takeover rate of 6.67%. When we remove the\nmemory module while keeping other components the same,\nthe average takeover rate increases substantially to 24.44%.\nThe baseline system performs worst with a 44.44% average\ntakeover rate. These results indicate a 72.7% reduction in\ntakeover rate when adding the memory module to our base\narchitecture and an 85% overall reduction compared to the\nbaseline. This significant improvement suggests that the\nRAG-based memory module plays a crucial role in main-\ntaining personalized vehicle control by effectively utilizing\nhistorical interactions and user preferences.\n5. Conclusion\nIn this paper, we presented an on-board VLM-based frame-\nwork designed to enhance motion control tasks in au-\ntonomous driving, offering a more human-centric and re-\nsponsive user experience. Our personalized motion con-\ntrol system represents a novel integration of VLMs into au-\ntonomous driving, offering three significant contributions to\nhuman-centric driving experiences. First, through its RAG-\nbased memory module, the system demonstrates advanced\npersonalization capabilities, effectively learning and adapt-\ning to individual driving preferences while maintaining\nsafety and comfort standards across various scenarios. Sec-\nond, the fine-tuned VLM-based framework leverages mul-\ntimodal reasoning to understand both complex visual scene\ninformation and implicit natural language instructions, en-\nabling human-like human-vehicle interaction. Finally, the\nsystem achieves remarkable computational efficiency with\nan optimized 9B VLM implementation, processing with an\naverage 1.6-second latency and less than 16 GB GPU mem-\nory consumption on standard vehicle hardware, making it\nfeasible for commercial deployment. Through experiments,\nwe demonstrated that our VLM-based approach enhanced\ndriving reliability, trustworthiness, and personalization, re-\nducing the need for human takeover by up to 76.8% while\nmaintaining near real-time response. This framework con-\ntributed to enhancing personalized autonomous driving by\naligning vehicle behavior with individual user preferences\nand considering environmental information, marking a sig-\nnificant step toward a more adaptable, user-centered human-\nautonomy teaming solution.\n\nReferences\n[1] AutonomousStuff. “autonomousstuff”, 2023. 4\n[2] Il Bae, Jaeyoung Moon, Junekyo Jhung, Ho Suk, Taewoo\nKim, Hyungbin Park, Jaekwang Cha, Jinhyuk Kim, Dohyun\nKim, and Shiho Kim. Self-driving like a human driver in-\nstead of a robocar: Personalized comfortable driving experi-\nence for autonomous vehicles, 2022. 2\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond, 2023. 2,\n5\n[4] Alberto Bemporad, Manfred Morari, Vivek Dua, and Efstra-\ntios N Pistikopoulos. The explicit solution of model predic-\ntive control via multiparametric quadratic programming. In\nProceedings of the 2000 American control conference. ACC\n(IEEE Cat. No. 00CH36334), pages 872–876. IEEE, 2000. 5\n[5] Ioana-Diana Buzdugan, Silviu Butnariu, Ioana-Alexandra\nRos,u, Andrei-Cristian Pridie, and Csaba Antonya.\nPer-\nsonalized driving styles in safety-critical scenarios for au-\ntonomous vehicles: An approach using driver-in-the-loop\nsimulations. Vehicles, 5(3):1149–1166, 2023. 2\n[6] Simeon C Calvert, Dani¨el D Heikoop, Giulio Mecacci, and\nBart Van Arem. A human centric framework for the analysis\nof automated driving systems based on meaningful human\ncontrol.\nTheoreTical issues in ergonomics science, 21(4):\n478–506, 2020. 1\n[7] Long Chen, Oleg Sinavski, Jan H¨unermann, Alice Karnsund,\nAndrew James Willmott, Danny Birch, Daniel Maund, and\nJamie Shotton. Driving with llms: Fusing object-level vec-\ntor modality for explainable autonomous driving. In 2024\nIEEE International Conference on Robotics and Automation\n(ICRA), pages 14093–14100. IEEE, 2024. 1\n[8] Chroma. The AI-native open-source embedding database,\n2023. 4\n[9] LMDeploy Contributors.\nLmdeploy: A toolkit for com-\npressing, deploying, and serving llm. https://github.\ncom/InternLM/lmdeploy, 2023. 5\n[10] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou,\nKaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang,\nKuei-Da Liao, Tianren Gao, Erlong Li, Kun Tang, Zhipeng\nCao, Tong Zhou, Ao Liu, Xinrui Yan, Shuqi Mei, Jianguo\nCao, Ziran Wang, and Chao Zheng. A survey on multimodal\nlarge language models for autonomous driving. In Proceed-\nings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision (WACV) Workshops, pages 958–979, 2024.\n1\n[11] Can Cui, Yunsheng Ma, Zichong Yang, Yupeng Zhou, Peiran\nLiu, Juanwu Lu, Lingxi Li, Yaobin Chen, Jitesh H. Pan-\nchal, Amr Abdelraouf, Rohit Gupta, Kyungtae Han, and Zi-\nran Wang. Large language models for autonomous driving\n(llm4ad): Concept, benchmark, simulation, and real-vehicle\nexperiment, 2024. 1\n[12] Can Cui, Zichong Yang, Yupeng Zhou, Yunsheng Ma,\nJuanwu Lu, Lingxi Li, Yaobin Chen, Jitesh Panchal, and Zi-\nran Wang. Personalized autonomous driving with large lan-\nguage models: Field experiments, 2024. 1\n[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning,\n2023. 2\n[14] Runjia Du, Kyungtae Han, Rohit Gupta, Sikai Chen, Samuel\nLabi, and Ziran Wang. Driver monitoring-based lane-change\nprediction: A personalized federated learning framework. In\n2023 IEEE Intelligent Vehicles Symposium (IV), pages 1–7.\nIEEE, 2023. 1, 2\n[15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models, 2021.\n5\n[16] Chao Huang, Hailong Huang, Peng Hang, Hongbo Gao,\nJingda Wu, Zhiyu Huang, and Chen Lv. Personalized tra-\njectory planning and control of lane-change maneuvers for\nautonomous driving. IEEE Transactions on Vehicular Tech-\nnology, 70(6):5511–5523, 2021. 2\n[17] Shinpei Kato, Eijiro Takeuchi, Yoshio Ishiguro, Yoshiki Ni-\nnomiya, Kazuya Takeda, and Tsuyoshi Hamada. An open\napproach to autonomous vehicles. IEEE Micro, 35(6):60–\n68, 2015. 5\n[18] Jinkyu Kim, Teruhisa Misu, Yi-Ting Chen, Ashish Tawari,\nand John Canny.\nGrounding human-to-vehicle advice for\nself-driving vehicles, 2019. 2\n[19] Jinkyu Kim, Suhong Moon, Anna Rohrbach, Trevor Darrell,\nand John Canny. Advisable learning for self-driving vehicles\nby internalizing observation-to-action rules. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 9661–9670, 2020. 2\n[20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni,\nVladimir Karpukhin, Naman Goyal, Heinrich\nK¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al.\nRetrieval-augmented generation for knowledge-intensive nlp\ntasks. Advances in Neural Information Processing Systems,\n33:9459–9474, 2020. 2, 4\n[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models, 2023. 2\n[22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding, 2024. 2\n[23] Xishun Liao, Xuanpeng Zhao, Ziran Wang, Zhouqiao Zhao,\nKyungtae Han, Rohit Gupta, Matthew J Barth, and Guoyuan\nWu. Driver digital twin for online prediction of personal-\nized lane change behavior. IEEE Internet of Things Journal,\n2023. 2\n[24] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming\nChen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang,\nChuang Gan, and Song Han. Awq: Activation-aware weight\nquantization for llm compression and acceleration, 2024. 5\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning, 2023. 2\n[26] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang\nYuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuan-\ndong Tian, Christopher Re, and Beidi Chen. Deja vu: Con-\n\ntextual sparsity for efficient LLMs at inference time. In Pro-\nceedings of the 40th International Conference on Machine\nLearning, pages 22137–22176. PMLR, 2023. 1\n[27] Keke Long, Haotian Shi, Jiaxi Liu, and Xiaopeng Li. Vlm-\nmpc: Vision language foundation model (vlm)-guided model\npredictive controller (mpc) for autonomous driving. arXiv\npreprint arXiv:2408.04821, 2024. 2\n[28] Shanhe Lou, Zhongxu Hu, Yiran Zhang, Yixiong Feng,\nMengchu Zhou, and Chen Lv. Human-cyber-physical system\nfor industry 5.0: A review from a human-centric perspective.\nIEEE Trans. Autom. Sci. Eng, pages 1–18, 2024. 1\n[29] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li,\nPengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and\nZhongyu Wei. Valley: Video assistant with large language\nmodel enhanced ability, 2023. 2\n[30] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and\nChaowei Xiao. Dolphins: Multimodal language model for\ndriving, 2023. 1, 2\n[31] Yunsheng Ma, Can Cui, Xu Cao, Wenqian Ye, Peiran Liu,\nJuanwu Lu, Amr Abdelraouf, Rohit Gupta, Kyungtae Han,\nAniket Bera, et al. Lampilot: An open benchmark dataset\nfor autonomous driving with language model programs. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 15141–15151, 2024. 2\n[32] Martin\nMagnusson.\nThe\nthree-dimensional\nnormal-\ndistributions transform: an efficient representation for reg-\nistration, surface analysis, and loop detection. PhD thesis,\n¨Orebro universitet, 2009. 5\n[33] Martin Magnusson, Achim Lilienthal, and Tom Duckett.\nScan registration for autonomous mining vehicles using 3d-\nndt. Journal of Field Robotics, 24(10):803–827, 2007. 5\n[34] Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai,\nJianhua Han, Hang Xu, and Li Zhang.\nReason2drive:\nTowards interpretable and chain-based reasoning for au-\ntonomous driving, 2024. 2\n[35] Chenbin Pan, Burhaneddin Yaman, Tommaso Nesti, Abhirup\nMallik, Alessandro G Allievi, Senem Velipasalar, and Liu\nRen. Vlp: Vision language planning for autonomous driving,\n2024. 2\n[36] Chenbin Pan, Burhaneddin Yaman, Tommaso Nesti, Abhirup\nMallik, Alessandro G. Allievi, Senem Velipasalar, and Liu\nRen. VLP: Vision Language Planning for Autonomous Driv-\ning. In CVPR, 2024. 1\n[37] SungYeon Park, MinJae Lee, JiHyuk Kang, Hahyeon Choi,\nYoonah Park, Juhwan Cho, Adam Lee, and DongKyu Kim.\nVlaad: Vision and language assistant for autonomous driv-\ning. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision (WACV) Workshops, pages\n980–987, 2024. 1, 2\n[38] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and\nYu-Gang Jiang. Nuscenes-qa: A multi-modal visual ques-\ntion answering benchmark for autonomous driving scenario,\n2024. 2\n[39] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,\nChristine McLeavey, and Ilya Sutskever.\nRobust speech\nrecognition via large-scale weak supervision, 2022. 3\n[40] Noveen Sachdeva, Ziran Wang, Kyungtae Han, Rohit Gupta,\nand Julian McAuley. Gapformer: Fast autoregressive trans-\nformers meet rnns for personalized adaptive cruise control.\nIn 2022 IEEE 25th International Conference on Intelligent\nTransportation Systems (ITSC), pages 2528–2535. IEEE,\n2022. 2\n[41] Mariah L. Schrum, Emily Sumner, Matthew C. Gombolay,\nand Andrew Best. Maveric: A data-driven approach to per-\nsonalized autonomous driving, 2023. 2\n[42] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu,\nPing Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei\nZhan, and Mingyu Ding.\nLanguagempc: Large language\nmodels as decision makers for autonomous driving, 2023.\n4\n[43] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen,\nHanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo,\nAndreas Geiger, and Hongyang Li. Drivelm: Driving with\ngraph visual question answering, 2024. 2\n[44] Jarrod M Snider et al. Automatic steering methods for au-\ntonomous automobile path tracking. Robotics Institute, Pitts-\nburgh, PA, Tech. Rep. CMU-RITR-09-08, 2009. 4\n[45] Xu Sun, Jingpeng Li, Pinyan Tang, Siyuan Zhou, Xiangjun\nPeng, Hao Nan Li, and Qingfeng Wang. Exploring person-\nalised autonomous vehicles to influence user trust. Cognitive\nComputation, 12:1170–1186, 2020. 2\n[46] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu,\nYang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang\nZhao. DriveVLM: The Convergence of Autonomous Driving\nand Large Vision-Language Models. arXiv, 2024. 1\n[47] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang,\nZhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and\nHang Zhao.\nDrivevlm: The convergence of autonomous\ndriving and large vision-language models, 2024. 2\n[48] Yanbing Wang, Ziran Wang, Kyungtae Han, Prashant Tiwari,\nand Daniel B Work.\nPersonalized adaptive cruise control\nvia gaussian process regression. In 2021 IEEE International\nIntelligent Transportation Systems Conference (ITSC), pages\n1496–1502. IEEE, 2021. 1, 2\n[49] Yanbing Wang, Ziran Wang, Kyungtae Han, Prashant Tiwari,\nand Daniel B Work. Gaussian process-based personalized\nadaptive cruise control.\nIEEE Transactions on Intelligent\nTransportation Systems, 23(11):21178–21189, 2022. 1, 2\n[50] Ziran Wang, Xishun Liao, Chao Wang, David Oswald,\nGuoyuan Wu, Kanok Boriboonsomsin, Matthew J Barth,\nKyungtae Han, BaekGyu Kim, and Prashant Tiwari. Driver\nbehavior modeling using game engine and real vehicle: A\nlearning-based approach. IEEE Transactions on Intelligent\nVehicles, 5(4):738–749, 2020. 1\n[51] Ziran Wang, Rohit Gupta, Kyungtae Han, Haoxin Wang, Ak-\nila Ganlath, Nejib Ammar, and Prashant Tiwari. Mobility\ndigital twin: Concept, architecture, case study, and future\nchallenges. IEEE Internet of Things Journal, 9(18):17452–\n17467, 2022. 2\n[52] Frank Wilcoxon. Individual comparisons by ranking meth-\nods. In Breakthroughs in statistics: Methodology and distri-\nbution, pages 196–202. Springer, 1992. 8\n[53] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and\nFu Lee Wang. Parameter-efficient fine-tuning methods for\n\npretrained language models: A critical review and assess-\nment, 2023. 5\n[54] George Yule. The study of language. Cambridge university\npress, 2022. 6\n[55] Cong Zhang, Chi Tian, Tianfang Han, Hang Li, Yiheng\nFeng, Yunfeng Chen, Robert W. Proctor, and Jiansong\nZhang. Evaluation of infrastructure-based warning system\non driving behaviors-a roundabout study, 2023. 2\n[56] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An\ninstruction-tuned audio-visual language model for video un-\nderstanding, 2023. 2\n[57] Zhouqiao Zhao, Ziran Wang, Rohit Gupta, Kyungtae Han,\nand Prashant Tiwari. Personalized adaptive cruise control\nbased on steady-state operation, 2023.\nUS Patent App.\n17/578,330. 2",
    "pdf_filename": "On-Board_Vision-Language_Models_for_Personalized_Autonomous_Vehicle_Motion_Control_System_Design_and.pdf"
}