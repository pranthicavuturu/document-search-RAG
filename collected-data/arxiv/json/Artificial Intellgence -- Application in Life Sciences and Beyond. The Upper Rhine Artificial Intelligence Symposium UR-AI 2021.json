{
    "title": "Artificial Intellgence -- Application in Life Sciences and Beyond. The Upper Rhine Artificial Intelligence Symposium UR-AI 2021",
    "context": "",
    "body": "ARTIFICIAL INTELLIGENCE\nAPPLICATION IN LIFE SCIENCES AND BEYOND\nEDITED BY\nKARL-HERBERT SCHÄFER\nFRANZ QUINT\nUR-AI 2021\nTHE UPPER-RHINE ARTIFICIAL INTELLIGENCE SYMPOSIUM\nCOLLECTION OF ACCEPTED PAPERS OF THE SYMPOSIUM\nKAISERSLAUTERN, 27th OCTOBER 2021\n\nCopyright: This volume was published under the license \"Creative Commons Attribution 4.0\u0003\nInternational\" (CC BY 4.0). The legally binding license agreement can be found at\nhttps://creativecommons.org/licenses/by/4.0/deed.en\nPublished by: Hochschule Kaiserslautern, University of Applied Sciences\nCover illustration by: vs148/shutterstock\n\n \n \nThe Upper-Rhine Artificial Intelligence Symposium \nUR-AI 2021 \n \nARTIFICIAL INTELLIGENCE - APPLICATION IN LIFE SCIENCES AND BEYOND \n \n \n \nKarl-Herbert Schäfer, Franz Quint (eds.) \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nKaiserslautern, 27th October 2021 \n \n \n\n \n \nThe Upper-Rhine Artificial Intelligence Symposium \nUR-AI 2021 \n \nARTIFICIAL INTELLIGENCE - APPLICATION IN LIFE SCIENCES AND BEYOND \n \n \n \n \n \n \n \n \n \n \n \n \nConference Chairs \nKarl-Herbert Schäfer, Kaiserslautern University of Applied Sciences \nFranz Quint, Karlsruhe University of Applied Sciences \nProgram Committee \nAndreas Christ, Offenburg University of Applied Sciences \nThomas Lampert, Télécom Physique Strasbourg \nJörg Lohscheller, Trier University of Applied Sciences \nEnkelejda Miho, Universities of Applied Sciences and Arts Northwestern Switzerland \nUlrich Mescheder, Furtwangen University of Applied Sciences \nChristoph Reich,  Furtwangen University of Applied Sciences \nKarl-Herbert Schäfer, Kaiserslautern University of Applied Sciences \nFranz Quint, Karlsruhe University of Applied Sciences \nOrganising Committee \nMatthias Bächle, Kaiserslautern University of Applied Sciences \nAnna Dister, TriRhenaTech \nSusanne Schohl, Kaiserslautern University of Applied Sciences \nJessica Weyer, Kaiserslautern University of Applied Sciences  \n \n\nTable of contents \nForeword ............................................................................................................\u0011\u0003v \nKarl-Herbert-Schäfer and Franz Quint \nChallenges in Live Monitoring of Machine Learning Systems .......................... 1 \nPatrick Baier and Stanimir Dragiev \nPrediction of Activators for Pathogen Sensing Receptors using Machine \nLearning ..................................................................................................... 11 \nPyaree Mohan Dash, Pratiti Bhadra, Volkhard Helms and Bernd Bufe \nDeep Learning associated with Computational Fluid Dynamics to predict \npollution concentration fields in urban areas ............................................. 13 \nXavier Jurado, Nicolas Reiminger, Marouane Benmoussa, José Vazquez and Cédric \nWemmert \nImproving COVID-19 CXR Detection with Synthetic Data Augmentation .... 21 \nDaniel Schaudt, Christopher Kloth, Christian Späte, Andreas Hinteregger, Meinrad Beer \nand Reinhold von Schwerin \nDetection of driver drowsiness by calculation the speed of eye blinking ........ 28 \nMuhammad Fawwaz Yusri, Patrick Mangat and Oliver Wasenmüller \nMFmap: A semi-supervised generative model matching cell lines to cancer \nsubtypes ..................................................................................................... 38 \nXiaoxiao Zhang and Maik Kschischo \nSystematic investigation of Basic Data Augmentation Strategies on \nHistopathology Images .............................................................................. 39 \nJonas Annuscheit, Benjamin Voigt, Oliver Fischer, Patrick Baumann, Sebastian \nLohmann, Christian Krumnow and Christian Herta \nOnline extraction of functional data from video recordings of gut movements \nusing AI features ........................................................................................ 49 \nPervaiz Khan, Manuela Gries, Steven Schulte, Anne Christmann, Ahmed Sheraz, Marko \nBaller, Karl-Herbert Schäfer and Andreas Dengel \nAn artificial neural network-based toolbox for the morphological analysis of \nred blood cells in flow ............................................................................... 52 \nMarcelle Lopes and Stephan Quint \nComparing a deterministic and a Bayesian classification neural network for \nchest diseases in radiological images......................................................... 53 \nJonas Nolde and Ruxandra Lasowski \ni\n\nGaussian Process Inspired Neural Networks for Spectral Unmixing Dataset \nAugmentation ............................................................................................. 61 \nJohannes Anastasiadis and Michael Heizmann \nR&D of a Multisensory System for Excavation Machines for the Real-Time \nGeneration of AI/ML Classified, Georeferenced and BIM compliant Voxel \nModels of Soil (ZIM Project HOBA) ........................................................ 71 \nReiner Jäger, Mohamed Almagboul, Guru Prashanth Sridhar and Anantha Praveen \nChaitanya \nPredictive prognostic for Li-Ion batteries in electric vehicles .......................... 73 \nInès Jorge, Ahmed Samet, Tedjani Mesbahi and Romuald Bone \nAn Architecture to Quantify the Risk of AI-Models ........................................ 84 \nAlexander Melde, Astrid Laubenheimer, Norbert Link and Christoph Schauer \nExercises in Human-Centered AI: On Shneiderman’s Second Copernican \nRevolution .................................................................................................. 94 \nDieter Wallach, Lukas Flohr, Annika Kaltenhauser and Sven Fackert \nInterpretable Machine Learning for Quality Engineering in Manufacturing - \nImportance measures that reveal insights on errors ................................... 96 \nHolger Ziekow, Ulf Schreier, Alexander Gerling and Alaa Saleh \nApplication of Artificial Intelligence/Machine Learning Methods for the \nDevelopment of Internal Combustion Engines – An Overview .............. 106 \nYoussef Beltaifa, Shahida Faisal and Maurice Kettner \nModeling for Explainability: Ethical Decision-Making in Automated Resource \nAllocation ................................................................................................. 119 \nChristina Cociancig, Christoph Lüth and Rolf Drechsler \nCondition Monitoring of Electric Motor with Convolutional Neural Network\n .................................................................................................................. 128 \nTanju Gofran, Maurice Kettner and Dieter Schramm \nClassification and Prediction of Bicycle-Road-Quality using IMU Data ....... 138 \nJohannes Heidt and Klaus Dorer \nModeling natural convection in porous media using convolutional neural \nnetworks ................................................................................................... 150 \nMohammad Reza Hajizadeh Javaran, Amadou-Oury Bah, Mohammad Mahdi Rajabi, \nGabriel Frey, Florence Le Ber and Marwan Fahs \nPoint Cloud Capturing and AI-based Classification for as-built BIM using \nAugmented Reality .................................................................................. 158 \nThomas Klauer and Bastian Plaß \nii\n\nA Reference Architecture for Dialog Management in Conversational Agents in \nHigh-Engagement Use Cases .................................................................. 167 \nStephan Kurpjuweit and Nima Samsami \nVerify Embedded Systems faster and more efficiently with AI ..................... 175 \nBjörn Morgenthaler, Alexander Schwarz and Manuel Duque-Anton \nSuitability analysis of machine learning algorithms: Processing 3D spatial data \nfor automated robot control ..................................................................... 184 \nBenjamin Peric and Michael Engler \nVizNN: Visual Data Augmentation with Convolutional Neural Networks for \nCybersecurity Investigation ..................................................................... 194 \nAmélie Raymond, Baptiste Brument and Pierre Parrend \nTowards generating complex programs represented asnode-trees with \nreinforcement learning ............................................................................. 205 \nAndreas Reich and Ruxandra Lasowski \nUse of AI and Image Segmentation for 3D Modelling ................................... 212 \nMichael Weber, Tobias Weiß, Dr. Franck Gechter and Dr. Reiner Kriesten \nImproving Temporal Consistency in Aerial Based Crowd Monitoring Using \nBayes Filters ............................................................................................ 224 \nJan Calvin Kramer, Thomas Golda, Jonas Hansert and Thomas Schlegel1 \nPotentials of Semantic Image Segmentation Using Visual Attention Networks \nfor People with Dementia ........................................................................ 234 \nLiane Meßmer and Christoph Reich \niii\n\n$%\n\nForeword \nWhen people talk about breakthrough technologies today, artificial intelligence (AI) is at the \nforefront. More and more fields are being supported by it in their scientific, economic and social \nchallenges. The year 2021 undoubtedly highlights two areas in particular where AI is not only \nurgently needed, but its development is being driven forward rapidly while being viewed with \nsome scepticism in society: healthcare and data protection.  \nThe COVID-19 pandemic shows that our society, despite its high level of development, remains \nvulnerable to ancient threats. However, it also shows that humanity is capable of finding a \nresponse, albeit imperfect, in a very short time. To this, artificial intelligence provides significant \ncontributions. It would be unthinkable to manage the analysis and processing of the huge \namounts of data needed, especially in the emerging field of medical image and data analysis, to \nfind a solution without AI. The pandemic has also made us realize that big data is not just \ngenerated as measurement data by sensor systems. Home schooling, home office, online \nbanking, online shopping, etc. also generate a huge amount of data, which in its nature is \npersonal. Artificial intelligence can be used to draw automated conclusions from this data that \ntouch on privacy. This is not desirable. As such, data protection and the legal and ethical \nframework for using this data are becoming increasingly important.  \nThe TriRhenaTech alliance of universities of applied sciences from the Upper Rhine region has \nbeen addressing the topic of artificial intelligence for many years. After the focus in previous \nyears at the conferences in Offenburg and Karlsruhe was on application-oriented AI research in \nthe industrial sector, the focus at this year's conference in Kaiserslautern is on the aforementioned \ncurrent topics from health, Life sciences, data protection and beyond. The at hand conference \nproceeding contains the articles on the oral and selected poster presentations. We hope you enjoy \nreading them and we would be pleased if you find interesting approaches, worth to be considered. \nWe encourage you to contact the authors, jointly develop the ideas presented there further and \npossibly incorporate them into new products. \nKaiserslautern, October 2021 \nKarl-Herbert Schäfer \nFranz Quint \n%\n\nChallenges in Live Monitoring of\nMachine Learning Systems\nPatrick Baier1 and Stanimir Dragiev2\n1 Hochschule Karlsruhe – University of Applied Sciences\npatrick.baier@h-ka.de\n2 Zalando Payments\nstanimir.dragiev@zalando.de\nAbstract. A machine learning (ML) system involves multiple layers of software\nand therefore needs monitoring to ensure a reliable operation. As opposed to\ntraditional software services, the quality of its predictions can only be guaranteed\nif the data that ﬂows into the system follows a similar distribution as the data\nthe ML model was trained on. This poses additional requirements on monitoring.\nIn this paper we outline a scheme for monitoring ML services based on feature\ndistribution comparison between the data used for training and for live prediction.\nTo showcase this we introduce payment risk prediction as an application scenario.\nIts long feedback delays and real time requirements motivate monitoring and at\nthe same time holds speciﬁc challenges which we address. In this context we discuss\ntrade-oﬀs for the practical implementation of the monitoring scheme and share\nour best practices.\nKeywords: reliable machine learning, monitoring, production systems, feature\ndistribution, non-stationarity\n1\nIntroduction\nLive monitoring of software systems is an important and well studied ﬁeld [1] that helps to\nprevent unexpected service interruption and ensures stability and reliability. Monitoring\ntypically involves collecting metrics about a system and checking if these values lie within\na range of expected values. If this is not the case, alerts are triggered to warn a system\noperator who checks for the healthiness of the system. Example metrics are the size of\nthe free heap memory of a software process or the CPU utilization of the machine that\nit is running on. While monitoring of software systems has a long history and is widely\napplied, the proliferation of systems that rely on machine learning (ML) models brings\na new challenge to this ﬁeld.\nA typical ML system consumes input data and maps it to a prediction, which is used\nin a downstream decision engine. For instance, a fraud detection system uses payment\ntransaction data to predict if a transaction is fraud or eligible. This prediction is then\nused to decide if a warning to the card holder should be triggered or even to cancel the\ntransactions. To ensure that the predictions of the ML system are accurate, the model\nis tested after training and before live deployment on a held-out test set with respect to\ntypical quality metrics such as accuracy and area under the ROC curve. In general, the\nprediction quality of a ML model on unseen input data is within expectation only if the\ninput data is similar to the training data [2]. Technically, similarity means that unseen\ndata points are drawn from the same distribution as the data used for training the model.\nThe assessed quality measure on the test set – e.g. accuracy – cannot be promised in\n1\n\nlive operation if the live distribution and the training distribution diﬀer. Hence, a live\nmonitoring system is needed that periodically checks that the data which is served to\nthe ML model in the live system is close enough to the training data distribution.\nThere are two main reasons why the input data distribution in a live system may\ndiﬀer from the training data distribution: The calculation of an input feature in some\npreceding system is ﬂawed (e.g. money values are sent in euros instead of cents) or\nthere is a natural data drift triggered by, for instance, phenomena like inﬂation. Without\nproper monitoring such data shifts can stay unnoticed. While the technical monitoring\nof the software stack can look perfectly ﬁne, the quality of the predictions and hence the\ndecisions based upon them may already suﬀer substantially. This happens long before\nthe impact is measurable, resulting in big monetary damages. Thus, ML systems need an\nadditional layer of monitoring that is concerned with checking for sane data distributions\nto meet the expected quality of service.\nWhile several deployment related concerns of ML models are already tackled [3,4]\nthe problems that arise with live monitoring of such systems are not well studied yet.\nIn this paper we propose a ML monitoring system which is based on the experiences of\nrunning and monitoring ML systems in production environments for almost ten years.\nBesides giving some basic overview of the nuts and bolts of such a monitoring system,\nwe highlight the technical challenges and trade-oﬀs that arise with it, e.g. windowing,\nseasonality and computational trade-oﬀs.\n2\nBasic Monitoring System\nBefore we discuss the technical challenges of ML monitoring, we start with a basic\noverview of the monitoring system we propose. To make this more tangible, we ﬁrst\nintroduce an example application scenario which will be used to lay out the subsequent\nconcepts.\n2.1\nApplication Scenario\nGiven is an online payment provider that handles the complexity of payment transactions\non behalf of an online merchant. The provider strives for the best customer experience\nwhich includes seamless transaction processing and convenient payment options, e.g.\ncredit/debit card, cash-on-delivery, etc., and most notably deferred payment. For each\ntransaction, the provider needs to decide if a consumer can be oﬀered a deferred pay-\nment option. For instance, if a person wants to buy shoes online, will the person have\nthe option to pay only after receiving the shoes? In general, deferred payment options\nmake the online buying experience closer to the oﬄine shopping and thus increase con-\nversion rates of online shops, but also bear the risk of not receiving the money from the\ncustomer (also known as payment default). To make sure that deferred payment options\nare oﬀered only to the right customers, the payment provider needs to predict the risk\nof a payment default for every customer. Moreover, this prediction has to be ﬁnished\nbefore the customer arrives at the payment selection page of its shopping session. To\ntackle this problem payment providers typically employ ML systems that use features\nabout the customer and the current shopping session. The output is a prediction of the\npayment default likelihood of a customer for this purchase. Such a model can be trained\non historic payment data and is widely employed among online payment providers.\nThis scenario has two special characteristics which make monitoring especially chal-\nlenging: (1) The model has to provide its prediction in real-time, i.e. a payment default\n2\n\nFig. 1: Model Life Cycle\nprobability has to be provided in a matter of a few hundred milliseconds. Models with\nsuch a requirement are typically running encapsulated in web services that are deployed\nin the cloud. (2) The scenario has a delayed feedback loop. This means that we can only\nevaluate the decision of the model after a long delay. In the above scenario, if the model\npredicts that deferred payment should be granted to a customer it can take weeks until\nthe money comes in. Only then a label for the data point (customer paid or defaulted)\nis available and we know if the model’s prediction was right. Other examples for sys-\ntems with such a delayed feedback loop are order return prediction or customer churn\nprediction.\nEspecially the combination of these two properties highlights the importance of mon-\nitoring: We need to ensure that the model is working as expected, otherwise we may\nonly ﬁnd about it weeks later when the quality of the system’s decisions can ﬁnally be\nassessed. In worst case, several weeks of wrong predictions may result in a complete\nﬁnancial ﬁasco for a company.\n2.2\nModel Life Cycle\nTo make the contextual dependencies of model monitoring visible, we shortly sketch the\ntypical ML workﬂow that is preceding the monitoring (see Figure 1). What is left aside\nhere, are all the phases that precede model training (e.g. label deﬁnition, data acquisition,\nfeature engineering, etc).\nThe model life cycle starts by training the model on the training data. To check the\nmodels performance it is evaluated on a hold-out test data set and performance metrics\nlike accuracy or area under the curve are calculated. If everything looks ﬁne, the model is\ndeployed to the live system where it receives requests from a client application. A request\ncontains a data point to predict on, i.e. it contains all features that the model needs for a\nprediction. The model returns its output probability to the client within a few hundred\nmilliseconds. Typically only a few requests are routed at this stage to the newly deployed\nmodel in order to do a ﬁnal check on its technical readiness. After that more and more\nrequests are gradually routed to the new model until the full traﬃc arrives there. That\nis the point when model monitoring kicks in to make sure that the model runs reliably\nand that performance observed on the test data can be expected under real conditions.\n3\n\n2.3\nMonitoring Metrics\nThe most important question in monitoring is what exactly to monitor to ensure that\nthe system is working as expected. In systems with a delayed feedback loop we cannot\njust monitor a metric like accuracy, since a feedback about the model’s decision is not\nimmediately observable. However, as a proxy we can monitor the sanity of the data that\nis ﬂowing into the model, which are the input feature values.\nIt is widely understood that ML models only perform as expected if the data fed to\nthe live system is similar to the data used to train the model [2]. As a result, the primary\ncandidate metric to monitor is the diﬀerence between the data distributions of the train-\ning data and the live data that is currently ﬂowing into the model. Both are empirical\ndata distributions that can be compared by statistical metrics like the Wasserstein dis-\ntance (also known as Earth mover’s distance) or the Kullback–Leibler divergence. Such\na distance score quantiﬁes the similarity of the two distributions. Figure 2 shows an ex-\nample with diﬀerent empirical distributions and the corresponding Wasserstein distance.\nNote that for the rest of this paper, we assume that the Wasserstein distance is used for\ncomparing distributions.\n(a) Similar distributions, WD=0.9\n(b) Diﬀerent distributions, WD=15\nFig. 2: Wasserstein distances (WD) when comparing two empirical distributions.\n2.4\nMonitoring System\nThe proposed monitoring system works as follows: Every t seconds the system computes\nfor every input feature of the model its current live distribution considering the last n\nrequests that were sent to the model. For every feature, this distribution is compared to\nthe training data distribution using the Wasserstein distance. As a result, the monitoring\noutputs every t seconds the Wasserstein distance for every feature of the model. If one\nof the computed scores lies above a preselected alerting threshold the monitoring system\ntriggers an alert to a system operator to investigates the cause for the distribution shift.\nWe will quickly discuss possible reasons for this in the following subsection. The whole\nmonitoring process in summarized in Figure 3 that shows an example ﬂow for one feature.\nFinding the right alerting threshold is crucial: If the chosen threshold is too low, false\npositive alerts are triggered, which means that the monitoring alerts even though the\nfeature distributions did not signiﬁcantly change. This can for instance happen due to\n4\n\nFig. 3: Overview of monitoring process for one feature.\nsome recent outlier data points in the live system. On the other hand, if the alerting\nthreshold is too high, real distribution changes can stay unnoticed since no alerts are\ntriggered. Problems may then not be detected fast enough and the whole purpose of\nmonitoring becomes obsolete. A typical way to ﬁnd a good threshold is too start with a\nrather low value and then slightly increase the threshold if the resulting alerts can safely\nbe identiﬁed as false positives.\nThe description of the system above contains two parameters that must be set before\nthe monitoring system can operate: (1) Time t that determines how often a distribution\ncomparison is triggered. (2) Window size n which determines how many recent live re-\nquests are considered for calculating the live data distribution. Finding good parameter\nsettings is again not straightforward but crucial. Both are discussed in more detail in the\ntechnical challenges in Section 3.\n2.5\nSources of Errors\nAn alarm triggered by the monitoring system means that there is a change in the data\ndistribution for at least one of the features. The cause of such a shift typically comes\nfrom one of the following sources: (1) There is a technical problem with delivering the\ncorrect feature value to the model, i.e. the client sends the wrong data. (2) The input\nfeature data suﬀers from a natural distribution shift.\nIn the ﬁrst case, the model receives wrong data from the preceding system. This\ncan have several causes. For instance, a downtime in a database or a unstable network\nconnection may lead to missing feature values which over time lead to a changed data\ndistribution. Another problem could be a newly introduced bug in the preceding system\nthat alters a feature value. For instance, money values are sent in cents instead of euros\nafter a new software deployment. It is crucial to detect such cases since such a tiny bug\ncan have immense eﬀects on the output of a ML model.\nIn contrast, a natural shift in the input data does not stem from a technical problem\nbut some underlying phenomena in the data itself. This can typically be observed by a\ngradual shift in the live data over time. The resolution to such a problem could be to\nanalyse the data shift and remove any trend before the data goes into the model. Another\nalternative is to re-train the model in short time intervals to always include the freshest\navailable data.\n5\n\n3\nTechnical Challenges\nIn this section we look into the technical challenges that are inherent to the presented\nmonitoring system. To solve them, one has to decide for certain trade-oﬀs that are speciﬁc\nto the available data and the application scenario.\n3.1\nAggregation Window Size\nOne important parameter for the system is the size of the aggregation window which is\nused to determine the distribution of the live data. In the previous section this window\nsize was denoted as parameter n. The choice of this parameter has trade-oﬀs in both\ndirections: If n is chosen rather small, there is only very recent data in the aggregation\nwindow. This is on the one hand desirable since we prefer to build the live data distri-\nbution from relatively fresh data. For instance, consider the extreme case in which the\naggregation window contains all live data seen until time t. If there is a change in a\nfeature at time t + 1, it would take a long time to be visible in the live data distribution\nsince the aggregation window is dominated by the old data.\nOn the other hand, choosing a small n could detect such distribution shifts very\nquickly but comes with problems regarding the representation of the distribution. Build-\ning the empirical distribution from only a small aggregation window suﬀers from uncer-\ntainty due to the small sample size. Only if we build the distribution from a large enough\nnumber of empirical data points, we can be sure to approximate the underlying distribu-\ntion. Hence, if we choose n too small we will derive a wrong empirical distribution and\nthe alerting system will kick in since the distance to the training data is too big.\nTo show this trade-oﬀwe conducted a small experiment. For a sample feature, we\ncreated 10k training data points drawn from a Gaussian distribution with μ = 100 and\nσ2 = 10. Going back to the sample scenario, this feature could for instance represent the\nsummed price of items in a customer order. To simulate live data we created a new data\npoint at every time step which was drawn from the same distribution. In the two plots\nin Figure 4 at every 50 time steps the Wasserstein distance between the training and live\ndistribution with window size n ∈{10, 100, 1000} is plotted.\n(a) No data shift\n(b) Data shift at t = 2000\nFig. 4: Eﬀect of diﬀerent window sizes on the Wasserstein distance\nFrom Figure 4a we see that a small window size results in rather high Wasserstein\ndistances even though the data comes from the same distribution. As discussed before,\n6\n\nif the window size is chosen too small, the live distribution is not representative for the\nunderlying distribution. Only if we increase n to 1000, the Wasserstein distance is small\nenough such that the two distributions can be considered equal. Hence, we have to choose\nn big enough to avoid false-positive alerts. In Figure 4b, we changed the distribution of\nthe live data at time t = 2000 by reducing the mean of the Gaussian to 90. Here we can\nsee that choosing a big n can lead to a rather late detection of a distribution shift, which\nmay be very costly for the business.\nTo ﬁnd a good window size in practice, we recommend to choose n as small as possible\nbut at the same time to make sure that the Wasserstein distance stays low for data from\nthe same distribution.\n3.2\nSeasonality\nThe second problem that can arise when monitoring distribution shifts is seasonality in\ninput features. To illustrate this we assume that the amount of payment transactions in\nthe last hour is a feature in the ML model of the aforementioned payment provider. The\ndistribution of such a feature naturally ﬂuctuates across one day. For an experiment we\nassumed the feature to ﬂuctuate within one day as shown in Figure 5a. The curve in this\nﬁgure shows at every time of the day the mean of all data points within the last ﬁve\nminutes. If we compare the live distribution of this feature against the distribution of the\ntraining data, we face the problem that the current live distribution contains a seasonal\nshift, while the training data is aggregated over all training data points and is static with\nμ = 1000. As a result, the Wasserstein distance can increase signiﬁcantly during the day\neven when using a big window size of n = 1000 (see the upper curve in Figure 5b).\nTo overcome this problem, we can limit the comparison of live data to only the set\nof training data that comes from the same time window. For instance, if we want to\ncompare live vs. train distribution at 2pm, we compare the current live distributions\nonly with training data points for which the data was also collected in that time frame,\ni.e. every day in the corresponding window before 2pm. In this way, the seasonality can\nbe factored out and the distribution comparison results in a low Wasserstein scores (see\nthe lower curve in Figure 5b).\n(a) Feature with seasonality\n(b) Distribution Monitoring\nFig. 5: Eﬀect of seasonality on distribution monitoring\n7\n\n3.3\nComputational Aspects\nOne last consideration is about the computational eﬀort that is required for model mon-\nitoring. The Wasserstein distance is computationally complex but has linear approxima-\ntions [5] that are usually good enough for model monitoring.\nHere we face again a trade-oﬀ: If the distribution diﬀerence is calculated frequently,\ncomputational costs increase and, hence, also increase monetary eﬀorts for running the\nmonitoring system. On the other hand, if the distribution diﬀerence is calculated only\noccasionally there can be a signiﬁcant delay in detecting problems in the ML system.\nTo ﬁnd a good trade-oﬀone has to consider the amount of traﬃc in the live system\nand determine how much computation is actually spend for calculating the Wasserstein\ndistance. Based on this number a meaningful trade-oﬀbetween computation costs and\nmonitoring delay can be found.\n4\nRelated Work\nWhile monitoring in software systems is a ﬁeld that is studied quite well, the additional\ncomplexity that ML systems introduce to monitoring is only addressed so far in few\npublications. Breck et al. [3] were among the ﬁrst that summarized the challenges that\narise when running ML models in live systems. While they list model monitoring as\none important aspect, they do not go into technical details on how to implement this.\nKlaise et al. [6] discuss in their work aspects of monitoring and explainability of deployed\nmodels. In this context, they discuss statistics measure for detecting a drift in the live\ndistribution but do not cover the comparison between live and training data distribution.\nThe authors of this articles also provide an open source system that helps to automatically\ndetect distribution drifts in the live system [7]. Finally, Paleyes et al. [8] review reports\nof deployed ML solutions and also shortly discuss the aspect of monitoring. However,\nnone of the reported system has a delayed feedback loop and hence lacks a concept of\ndistribution comparison which is necessary in this case.\n5\nDiscussion\n5.1\nMonitoring adaptive systems\nA question that may arise in Section 2.3 is why, in the ﬁrst place, we should moni-\ntor changing feature distributions instead of building a model that can adapt to these\nchanges? Indeed, some models can accommodate drifts. If a drift is predictable, it can be\nmodeled explicitly. Unexpected drifts, on the other hand, can be accounted for by updat-\ning the model by online training on the new data points. This is only possible by limiting\nthe ”memory” in the training process. Training data which does not look far in the past\nmakes a model more sensible to recent changes. The smaller the training data horizon\nback in time, the faster the new models will pick up the new situation. However, a short\ntraining data period poses the risk that the new model ”forgets” patterns available in\nthe old data but not in the recent data. This introduces another trade-oﬀfor how to deal\nwith drifts and is rather an argument for monitoring than an alternative: each setting\nfavours a particular case and imposes a risk which is better discovered sooner than later.\n8\n\n5.2\nLimitations\nThe approach described in this paper is based on monitoring individual features to catch\na change in order to maintain a promised quality of the ML system. Examples can be\nconstructed, however, that show a distribution change in a higher-dimensional feature\nspace while the univariate distributions of the individual features remain stable. The\nML system may not generalize as well as expected to some examples from the changed\ndistribution and the performance can degrade. With the current approach, such a change\nmay stay unnoticed. A remedy would be to extend the distribution comparisons to pairs,\ntriples or higher tuples of features or even to the whole feature space. However, enumerat-\ning all such tuples introduces a huge overhead for a questionable beneﬁt: knowing that,\nsay, 20 features collectively deviate is not very actionable in general. In special cases,\nthough, the collective behaviour of subsets of features may well be of interest.\nWhile monitoring distributions does a good job to uncover changes, there are a class\nof problems which are rooted in the opposite. A popular write-up with practical advices\nfor ML engineers [9] features ”stale tables” as a particular pitfall which occurs more\nfor ML systems than for others. Let us assume a table owned by another team has not\nbeen updated for a while. If a feature aggregates some counts from the table and these\ncounts are missing for the recent past, the feature will slightly drift towards zero. For\nexample, consider a feature that counts how often a customer visited a page in the past\nweek. If a table with the daily visits of customers per page freezes, the aggregations\nwill decrease. Sooner or later this will appear in the distribution comparison. However,\nif a feature does not slide a window, the distribution will not change. Here should be\nnoted that monitoring feature distributions may help discover bugs opaque to traditional\nmonitoring, however it mainly aims at detecting shifts innate to the environment.\n6\nConclusion\nThe monitoring of an ML service is a prerequisite for the reliable operation and for main-\ntaining service levels promised at the time of development of the ML models. Especially\nin a complex environment like payment risk prediction, without ML monitoring the en-\nterprise is put at risk. Against the background of delayed feedback, not fully observable\ndecision eﬀects and non-stationary features, we outline the main traits of a monitoring\nsystem based on comparing the observed feature distributions. We discuss ways to aggre-\ngate and compare the distributions avoiding misalignment by making trade-oﬀs needed\nin the practical implementation.\nWhile monitoring ML services can discover a range of internal and external hazards,\nit is not a panacea, in particular it is not a substitute to the traditional software service\nmonitoring; it is rather an augmentation to the existing well maintained monitoring\npractices.\nReferences\n1. Gao, L., Lu, M., Li, L., Pan, C.: A survey of software runtime monitoring. (2017)\n2. Koh, P.W., Sagawa, S., Marklund, H., Xie, S.M., Zhang, M., Balsubramani, A., Hu, W.,\nYasunaga, M., Phillips, R.L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W., Earnshaw,\nB., Haque, I., Beery, S.M., Leskovec, J., Kundaje, A., Pierson, E., Levine, S., Finn, C., Liang,\nP.: Wilds: A benchmark of in-the-wild distribution shifts. (2021)\n3. Breck, E., Cai, S., Nielsen, E., Salib, M., Sculley, D.: What’s your ML test score? A rubric\nfor ML production systems. (2016)\n9\n\n4. Murphy, C., Kaiser, G., Arias, M.:\nAn approach to software testing of machine learning\napplications. (2007)\n5. Atasu, K., Mittelholzer, T.: Linear-complexity data-parallel earth mover’s distance approxi-\nmations. (2019)\n6. Klaise, J., Looveren, A.V., Cox, C., Vacanti, G., Coca, A.: Monitoring and explainability of\nmodels in production (2020)\n7. Van Looveren, A., Vacanti, G., Klaise, J., Coca, A., Cobb, O.: Alibi detect: Algorithms for\noutlier, adversarial and drift detection (2019)\n8. Paleyes, A., Urma, R.G., Lawrence, N.D.: Challenges in deploying machine learning: a survey\nof case studies (2021)\n9. Zinkevich,\nM.:\nRules\nof\nmachine\nlearning:\nBest\npractices\nfor\nML\nengineering.\nhttps://developers.google.com/machine-learning/guides/rules-of-ml (2017)\n10\n\nPrediction of Activators for Pathogen Sensing Receptors using \nMachine Learning   \nPyaree Mohan Dash1,2, Pratiti Bhadra1, Volkhard Helms1 , Bernd Bufe2 \n \n1 Center for Bioinformatics, Saarland Informatics Campus, Saarland University, D-66041 Saarbrücken, \nGermany.  \n2 Department of Informatics and Microsystems Technology, University of Applied Sciences \nKaiserslautern, Germany. \nbernd.bufe@hs-kl.de   \n \nAbstract. Formyl  peptide  receptors  (FPRs)  are  G  protein-coupled  receptors  (GPCRs)  \nthat  are predominantly  expressed  in  the  immune  system,  where  they  play  a  critical  \nrole  in  detecting bacterial  invasion  and  inflammatory  responses [1] through  detection  \nof  pathogen-derived formylated peptides [2]. Recent studies highlighted an involvement \nof FPRs in various diseases [1, 3],  such  as  bacterial  and  viral  infections,  Alzheimer's  \nand  prion  diseases,  immunodeficiency, diabetes, and cancer. Given the sheer importance \nof FPRs, there is an immediate need for a better  understanding  of  the  mode  of  action  of  \nthese  receptors.  A  current  challenge  in  FPR research is their well-documented capability \nto intact with an extremely vast number of structurally diverse  ligands  such  as  bacterial  \nand  virus-derived  peptides,  various  small  non-peptide molecules, and even some lipid-\nderivatives, that lack any obvious common structural motifs [1]. Because of the high \npotential of FPRs as a therapeutic target, we developed a computational method to predict \nFPR ligands using machine learning. Moreover, we can provide experimental evidence that \nour computation models are promising data mining tools that are useful tools to identify \nFPR activators from a vast amount of bacterial amino-acid sequence information that is \ncontained in public databases.  \nThe human genome encodes the three FPR genes FPR1, FPR2, and FPR3. In this study, we \nfocused  on  FPR1  and  FPR2.  The  proposed  agonist  prediction  classifiers  utilize  amino-\nacid composition  and  physicochemical  properties  as  features.  Our  optimized  prediction  \nmodels showed high test accuracy (FPR1: 82% and FPR2: 90%), Matthew’s correlation \ncoefficient (MCC) of 0.5 (FPR1) and 0.6 (FPR2), and area under the receiver operating \ncharacteristic curve (AUC-ROC) score of 0.76 (FPR1) and 0.90 (FPR2). To demonstrate \nthe performance of the proposed prediction models in the real world, we screened the \nEscherichia coli K12 proteome and selected 30  novel  peptides  (20  predicted  as  activators  \nand  10  as  non-activators)  for  experimental validation. Human embryonic kidney (HEK-\n293T) cells were used to perform a cell-based calcium flux  assay  using  Molecular  \nDevices’  Flex  station.  The  experimental  validation  showed  a  true negative rate of 90% \n(9/10 non-activators) and a true positive rate of 80% (18/20 activators). Furthermore, our \nstudy also sheds light on the physio-chemical properties of FPR agonists and antagonists. \nA feature descriptor analysis revealed that FPR1 is activated by peptides with higher \naromaticity,  low  hydrophobicity,  low  volume,  and  high  density  when  compared  to  \nthe  peptide activators of FPR2. Moreover, the gene set annotation analysis of the predicted \nFPR agonists indicated  that  FPR1  and  FPR2  activators  are  involved  in  different  \nmetabolic  processes  and transport systems related to bacterial stress responses. This \nindicates that our models can be used to mine novel information on the biological function \nof FPRs, which is potentially helpful for the rational design of therapeutic approaches.   \nKeywords: Formyl peptide receptors, pathogen sensing, machine learning, gene ontology \n11\n\nReferences \n1. The sensing of bacteria: emerging principles for the detection of signal sequences by \nformyl peptide receptors. Bufe B, Zufall F. Biomol Concepts. 2016 Jun 1;7(3):205-14. \ndoi: 10.1515/bmc-2016-0013.   \n2. Recognition of bacterial signal peptides by mammalian formyl peptide receptors: a new \nmechanism for sensing pathogens. Bufe B, Schumann T, Kappl R, Bogeski I, Kummerow \nC, Podgórska M, Smola S, Hoth M, Zufall F. J Biol Chem. 2015 Mar 20;290(12):7369-\n87. doi: 10.1074/jbc.M114.626747.  \n3. Bacterial MgrB peptide activates chemoreceptor Fpr3 in mouse accessory olfactory \nsystem and drives avoidance behavior. Bufe B, Teuchert Y, Schmid A, Pyrski M, Pérez-\nGómez A, Eisenbeis J, Timm T, Ishii T, Lochnit G, Bischoff M, Mombaerts P, Leinders-\nZufall T, Zufall F. Nat Commun. 2019 Oct 25;10(1):4889. doi: 10.1038/s41467-019-\n12842-x. \n12\n\nDeep Learning associated with Computational Fluid\nDynamics to predict pollution concentration ﬁelds in\nurban areas\nXavier Jurado1,2, Nicolas Reiminger1, Marouane Benmoussa1, Jos´e Vazquez1,2, and\nC´edric Wemmert2\n1 AIR&D, Strasbourg, France\n2 University of Strasbourg, ICube Laboratory, France\nAbstract. air quality is a worldwide major health issue, as an increasing number\nof people are living in densiﬁed cities. Several methods exist to monitor pollution\nlevels in a city, either physical models or sensors. Computational Fluid Dynamics\n(CFD) is a popular and reliable approach to resolve locally pollutant dispersion\nin urban context for its capacity to consider complex phenomenon at local scale.\nNevertheless, this method is computationally expensive and is not suitable for real\ntime monitoring over large areas and city shape that evolves permanently. To over-\ncome this issue, a deep learning model based on the MultiResUNetarchitecture\nhave been trained to learn pollutant dispersion from precalculated computational\nﬂuid dynamics. This model has been used in situ on an area spanning 1km² with\nreal values from traﬃc and meteorological sensors in the surroundings of Stras-\nbourg (France) and compared against the equivalent CFD results. Classic air\nquality metrics shows that the Deep Learning model manages to have satisfying\nresults against the CFD model. The similarity index used in the study shows a\n62% similarity for a result obtained in minutes against the CFD result obtained\nin tenth of hours.\nKeywords: Computational Fluid Dynamics ; Air pollution ; Machine Learning\n; Deep Learning ; Real Time Assessment\n1\nIntroduction\nAir pollution is a critical worldwide health issue with about 8 million death related to\nit yearly, according to the World Health Organization (WHO) [1,2]. To tackle this issue,\nWHO provided pollution concentration values that should not be exceeded. In European\nUnion, regulation has been enforced on the main air pollutant such as particulate matter\nor nitrogen dioxide [3]. To check if these values are respected, several measures have been\nimplemented in France:\n– New real estate project near pollutant sources such as heavy traﬃc roads, plants, or\ncentral heating system must study thoroughly air quality in the wanted area. How-\never, these regulations are only applied at some particular timestamps and speciﬁc\nplaces.\n– Sensor monitoring. But reliable sensors are expensive to acquire and maintain. For\nthe entirety of Strasbourg city (around 80km²), only 4 sensors are deployed to date.\n– Simulation of the annual pollution dispersion on the entire city. However, models\nthat allow large area to be simulated may not be adapted for urban areas because of\nbuildings not taken into account.\n13\n\nAmong the possible models of the third point, a popular approach in the scientiﬁc com-\nmunity is to create airborne pollutant dispersion maps in urban areas is to use Compu-\ntational Fluid Dynamics (CFD) [4,5]. It allows to accurately consider a lot of diﬀerent\nphysical phenomena from building impact on the ﬂow to solar radiation or chemical re-\naction. Indeed, pollutant dispersion concentration ﬁeld error can reach less than 10%\nwhen compared to experimental data [6] and about 30% when compared to real life in\nsitu experiments [7]. Nevertheless, the counterbalance of this method is that it is compu-\ntationally expensive. For instance, to cover 1km², the method roughly needs around 30\nmillion cells and can require a week of computation to converge on 96 CPUs. Further-\nmore, each time the building layout changes, it would require starting new simulations\nagain. CFD is therefore not adapted for real time simulation, despite its great accuracy\nand detailed description of physical phenomena.\nTo accelerate the computation, an innovative solution based on deep learning was devel-\noped. The idea consists in training a neural network with pre-calculated CFD simulations,\nto create a new air quality model that can determine pollutant dispersion in a matter\nof minutes over a large area. Indeed, recent advances in deep learning for spatial infor-\nmation treatment with convolutional based architectures have proved to be able to solve\nissues, notably in semantic segmentation that was impossible before. A popular model,\nthe MultiResUNet[8], heir of UNet[9], has proved to be particularly capable at han-\ndling spatial information. This model has been trained with about 5,000 examples of\nCFD results of pollutant dispersion from diﬀerent urban areas. The input of the model\nis the 3D shape of the buildings, the wind force and direction, and the position of the\nroads, considered as the sources of pollution.\nThis deep learning model is then included in a wider system that uses real time meteo-\nrological, traﬃc and sensor data to map the concentration ﬁeld in real time on an entire\nurban district.\n2\nMaterial and method\n2.1\nCFD air quality modeling\nTo train the Deep Learning architecture examples of pollutant dispersion were obtained\nusing Computational Fluid Dynamics (CFD). The software to compute the simulation\nis OpenFoam 5.0 which is an open source software for numerical simulations of diﬀerent\nkind such as ﬂuid mechanics or radiation. The approach elected here to solve the air ﬂow\nis a Reynold Averaged Navier Stokes (RANS) with a k-epsilon renormalization group\n(RNG) [10] performing unsteady simulation. For the pollutant dispersion a transport\nequation coupled with the air ﬂow is used.\nThe boundary conditions for the upper and lateral boundaries are symmetry condi-\ntions, the ground as a wall with a rugosity of z0 = 0.1m, the building as a wall condition,\nthe outlet as a freestream, the inlet as a logarithmic wind proﬁle law as proposed by [11].\nFor the meshing, the guidelines from [12] are respected with the top and lateral\nboundaries situated at 5H from the closest building including with H the height the\nhighest building. The mesh is insensitive with cells of 0.5m nearest to the buildings.\nThe model, equations and validation have been detailed in previous published paper [13]\nwhere the same approach has been described and properly validated.\n2.2\nDeep learning network\nThe Deep Learning network used to learn the CFD is the MultiResUNet from [8].\nThis network is ﬁrst designed to be applied for segmentation. In this work, it has been\n14\n\nconverted to solve pollutant dispersion from ﬂuid mechanics. The input are the distance\nfrom the pollutant source and the height of the buildings in the area and the output\nis the pollutant dispersion ﬁeld. The ﬁnal results covers an area of 100 × 100m2 by AI\npredictions as showed in Figure 1. The details of the MultiResUNet architecture are\npresented in Figure 2.\nFig. 1: Input/output images for the Deep Learning model\nFig. 2: Architecture details of the MultiResUNet\nThe loss function used is a custom loss called J3D and deﬁned as followed:\nJ3D = 1 −Vpred\n\u0002 Vtrue\nVpred\n\u0003 Vtrue\n≃1 −min(yi, ˆyi)\nmax(yi, ˆyi)\n(1)\nwhere Vpred and Vtrue is the volume represented by the grayscale value of respectively\nthe ground truth and the predicted result, yi and ˆyi are respectively the ground truth\nimage and the predict deep learning result.\nThe dataset for the training and validation are made of around 5,000 examples of\ndiﬀerent CFD simulations with varying building layouts and pollution sources. 20% are\nused for the validation and 80% for the training. For the test to check on the AI capability\nof predicting pollutant dispersion ﬁeld on unseen neighborhood, it will be compared with\na real neighborhood presented in Section 2.3 that will be modelled in CFD. The training\nwas made on 25 epochs with a patience of 5 epochs on the validation data.\n15\n\n2.3\nCase study\nThe site is located in the surrounding of Strasbourg (GPS coordinates: 48.603468, 7.743355).\nThe building layouts of the case study is obtained thanks to the open data of the city of\nStrasbourg which provide digital model of the whole city (https://data.strasbourg.eu).\nFor the test case, a real life situation is used, the ﬁrst of April of 2021 at the traﬃc\npeak which happens around 08:30 AM (to have the highest concentration related to\nroad traﬃc in the area). The wind speed and directions were obtained using the API\nopenWeatherMap with a wind speed of 2m/s and a wind direction 200°N.\nFig. 3: Map of the Schiltigheim district with the 3 main roads used in this study\nThere are 27 diﬀerent roads in the area. The data on traﬃc were obtained through the\nopen data of the city of Strasbourg for the 4 available roads (https://data.strasbourg.eu):\n– Road Bischwiller (part 1): 560 vehicles in 30 min (18.7 veh/min) with a mean velocity\nof 37.9km/h,\n– Road Bischwiller (part 2): 784 vehicles in 30 min (26.1 veh/min) with a mean velocity\nof 15.5km/h,\n– Street Mairie: 488 vehicles in 30 min (16.3 veh/min) with a mean velocity of 17.8km/h,\n– Street General de Gaulle: 654 vehicles in 30 min (21.8 veh/min) with a mean velocity\nof 16.3km/h.\nFor other roads in the area, traﬃc information is lacking, thus they have been classiﬁed\nas secondary that will have 30% of the traﬃc of closest main road and tertiary that will\nhave 5% of the closest main road. Figure 5 shows the map of the district of the study,\nwith the three main roads and the secondary and tertiary roads. The choice of 30% and\n5% is arbitrary for the sake of the example since there is no study on this traﬃc either\nwith sensors or models.\nEmissions are calculated based on methods proposed by the European Environment\nAgency (EEA) in their ”EMEP/EEA Air pollutant emission inventory guidebook 2016”,\nTier 3 method for engine-related NOX, PM10 and PM2.5 emissions (hot and cold emis-\nsions); 2017 metropolitan ﬂeet data found in the ”OMINEA” databases provided by the\nCentre Interprofessionnel Technique d’´Etudes de la Pollution Atmosph´erique (share of\ndiﬀerent vehicle types, fuels and EURO standards in France).\n16\n\nThe whole neighborhood have been modeled at once with CFD spanning an area of\n1 km2 made of 28 million cells. The buildings as well as the velocity magnitude ﬁeld at\nan height of 1.5m is shown on Fig. 4.\nFig. 4: Building layouts and ﬂow ﬁeld at an height of 1.5m\n2.4\nEvaluation\nSeven metrics will be used, 4 from the air quality domain and three others from the\ncomputer vision. The air quality criteria have been chosen according to [14] in which\nthe authors present several metrics with some overlapping since they evaluate the same\naspect of the model. They also provides empirical threshold to consider a model as making\ngood predictions:\n– Fraction of predictions within a factor of two of observation, noted FAC2, a good\nmodel should respect ≃> 0.5,\nFAC2 = fraction of data that satisfy 0.5 < Cpred\nCref\n< 2\n(2)\n– Normalised Mean Squared Error, noted NMSE, a good model should respect NMSE\n≃< 1.5,\nNMSE = (Cref −Cpred)2\nCpredCref\n,\n(3)\n– Fraction Bias noted FB, |FB| < 0.3,\nFB =\n(Cref −Cpred)\n0.5(Cpred + Cref),\n(4)\n– Correlation coeﬃcient, noted R (no threshold is given for this parameter),\nR = (Cref −Cref)(Cpred −Cpred)\nσCpredσCref\n,\n(5)\nThe three other metrics are:\n17\n\n– J3D\nJ3D ≃min(Cref, Cpred)\nmax(Cref, Cpred)\n(6)\n– Relative mean absolute error MAErel\nMAErel = |Cref −Cpred|\nCpred\n(7)\n– Structural similarity SSIM\nSSIM(A, B) =\n(2μAμB + c1)(2σAB + c2)\n(μ2\nA + μ2\nB + c1)(σ2\nA + σ2\nB + c2)\n(8)\nc1 = (k1L)2\nc2 = (k2L)2\n(9)\nwith Cpred the model prediction concentration, Cref the reference concentration (ground\ntruth), μA and μB are the respective average of A and B, σ2\nA and σ2\nB are the respective\nvariances of A and B, σAB is the covariance of A and B, L is the dynamic range of the\npixel values and k1 and k2 are two constants respectively 0.01 and 0.03 (by default).\n3\nResults\nTo evaluate the deep learning capabilities to be applied in real life situation, a comparison\nhas been made with real world data at the traﬃc at 08:30AM in the south of Schiltigheim,\nFrance the ﬁrst of April 2021 between results from a CFD simulation and our deep\nlearning approach on the NOx dispersion from traﬃc emissions. The results proposed\nrespectively by the CFD and MultiResUNet for the whole neighborhood are shown on\nFig.5\n(a) CFD result\n(b) MultiResUNet result\nFig. 5: Maps of the studied district and comparison of the two results)\nIt can be tedious to compare the results between the CFD and the deep learning\nnetwork since the CFD determines the dispersion in 3D while the deep learning approach\n18\n\nworks in 2D only at a given height. Nonetheless, the CFD needed one week of computation\non 96 CPU while the deep learning network needed around 3 minutes on a GTX 1080Ti\nGPU, representing a speed up by x3000. To evaluate the accuracy of the predictions, the\nmetrics presented above were computed between the prediction and the CFD considered\nas the ground truth and are presented below on Table 1.\nMetrics\nFAC2 NMSE\nFB\nR\nMAErel\nJ3D\nSSIM\nScore\n0.818\n1.565\n0.176 0.851\n0.431\n0.620 0.768\nExpected values\n> 0.5\n< 1.5\n< 0.3\n1\n0\n1\n1\nTable 1: Evaluation of the quality of the dispersion model given by the deep learning\napproach.\n4\nConclusion\nAs demonstrated by our work, deep learning has proved to be able to predict results\nclose to CFD for air pollutant dispersion. Moreover, the MultiResUNet architecture\nwas able to compute the dispersion in a matter of minutes over a wide area against several\ndays for the CFD. This makes the Deep Learning approach a potential model to predict\nin real time over large scale the pollutant dispersion from traﬃc related pollution.\nReferences\n1. WHO: Mortality and burden of disease from ambient air pollution, Global Health Obser-\nvatory data (2016)\n2. WHO: Mortality from household air pollution, Global Health Observatory data (2016)\n3. EU: Directive 2008/50/EC of the european parliament and of the council of 21 May 2008\non ambient air quality and cleaner air for Europe. European Union. (2008)\n4. Reiminger, N., Jurado, X., Vazquez, J., Wemmert, C., Blond, N., Dufresne, M., Wertel, J.:\nEﬀects of wind speed and atmospheric stability on the air pollution reduction rate induced\nby noise barriers. Journal of Wind Engineering and Industrial Aerodynamics 200 (May\n2020) 104160\n5. Santiago, J.L., Martilli, A., Martin, F.: On dry deposition modelling of atmospheric pollu-\ntants on vegetation at the microscale : application to the impact of street vegetation on air\nquality. Boundary-Layer Meteorology 162 (2017) 451–474\n6. Reiminger, N., Vazquez, J., Blond, N., Dufresne, M., Wertel, J.: How pollutant concen-\ntrations evolve in step-down street canyons as a function of buildings geometric properties.\n(2019)\n7. Rivas, E., Santiago, J.L., Lech´on, Y., Mart´ın, F., Ari˜no, A., Pons, J.J., Santamar´ıa, J.M.:\nCFD modelling of air quality in Pamplona City (Spain): Assessment, stations spatial rep-\nresentativeness and health impacts valuation.\nScience of the Total Environment (2019)\n19\n8. Ibtehaz, N., Rahman, M.S.: MultiResUNet : Rethinking the U-Net Architecture for Multi-\nmodal Biomedical Image Segmentation. Neural Networks 121 (January 2020) 74–87 arXiv:\n1902.04049.\n9. Ronneberger, O., Fischer, P., Brox, T.:\nU-Net: Convolutional Networks for Biomedical\nImage Segmentation. arXiv:1505.04597 [cs] (May 2015) arXiv: 1505.04597.\n19\n\n10. Yakhot, V., Orszag, S.A., Thangam, S., Gatski, T.B., Speziale, C.G.: Development of tur-\nbulence models for shear ﬂows by a double expansion technique. Physics of Fluids A: Fluid\nDynamics 4(7) (July 1992) 1510–1520\n11. Richards, P.J., Hoxey, R.P.: Appropriate boundary conditions for computational wind en-\ngineering models using the k-E turbulence model. (1993) 9\n12. Franke, J., Hellsten, A., Schl¨unzen, H., Carissimo, B.: Best practice guideline for the CFD\nsimulation of ﬂows in the urban environment. COST Action 732 (2007)\n13. Reiminger, N., Vazquez, J., Blond, N., Dufresne, M., Wertel, J.: CFD evaluation of mean\npollutant concentration variations in step-down street canyons. Journal of Wind Engineering\nand Industrial Aerodynamics 196 (January 2020) 104032\n14. Chang, J.C., Hanna, S.R.: Air quality model performance evaluation. Meteorology and\nAtmospheric Physics 87(1-3) (September 2004)\n20\n\nImproving COVID-19 CXR Detection with Synthetic\nData Augmentation\nDaniel Schaudt1, Christopher Kloth2, Christian Späte1, Andreas Hinteregger2, Meinrad\nBeer2, and Reinhold von Schwerin1\n1 Technische Hochschule Ulm - Ulm University of Applied Sciences\ndaniel.schaudt@thu.de, spaete@mail.hs-ulm.de, reinhold.vonschwerin@thu.de\n2 Universitätsklinikum Ulm - Ulm University Medical Center\nchristopher.kloth@uniklinik-ulm.de, andreas.hinteregger@uni-ulm.de,\nmeinrad.beer@uniklinik-ulm.de\nAbstract. Since the beginning of the COVID-19 pandemic, researchers have de-\nveloped deep learning models to classify COVID-19 induced pneumonia. As with\nmany medical imaging tasks, the quality and quantity of the available data is\noften limited. In this work we train a deep learning model on publicly available\nCOVID-19 image data and evaluate the model on local hospital chest X-ray data.\nThe data has been reviewed and labeled by two radiologists to ensure a high-\nquality estimation of the generalization capabilities of the model. Furthermore,\nwe are using a Generative Adversarial Network to generate synthetic X-ray im-\nages based on this data. Our results show that using those synthetic images for\ndata augmentation can improve the model's performance signi\u001ccantly. This can\nbe a promising approach for many sparse data domains.\nKeywords: Deep Learning, Medical Imaging, GANs, Data Augmentation\n1\nIntroduction\nThe ongoing COVID-19 pandemic brings many challenges for societies all around the\nglobe. For the healthcare sector, it is important to screen infected patients in an e\u001bective\nand reliable manner. This is especially true in an emergency setting, where patients\nalready experience advanced symptoms. The prevalent test used for COVID-19 detection\nis the reverse transcription polymerase chain reaction (RT-PCR) [1\u00153]. This method has\na high false negative rate and the processing requires dedicated personnel and can take\nhours to days [4].\nSince chest X-ray (CXR) images of COVID-19 patients show typical \u001cndings including\nperipheral opacities and ground class patterns in the absence of pleural e\u001busion [4, 5],\nthey can be used as a \u001crst-line triage tool [6]. This could speed up the identi\u001ccation\nprocess, as CXR images are easy to obtain and rather inexpensive with a lower radiation\ndose than computed tomography (CT) images. Using deep learning models for detection\nof COVID-19 prevalence in CXR images is promising, because it eliminates the need for\nspecialized medical sta\u001b in an emergency setting. This can further help to alleviate the\nchallenges to the healthcare systems around the world and has the potential to save lives.\nIn this retrospective study, we are training a deep convolutional neural network (CNN)\non the openly available COVIDx V8b dataset [7] and evaluate the model on local hospital\nCXR data. We speci\u001ccally choose this learning framework to assess the generalization\nabilities of a CNN in the medical imaging context. Since high quality CXR image data\nis sparse, we see this as the most common use case for models in production.\n21\n\nFurthermore, we are using a modi\u001ced version of the StyleGAN architecture [8] to\ngenerate synthetic COVID-19 positive and COVID-19 negative CXR images for data\naugmentation. This is done to o\u001bset some negative side e\u001bects encountered by a distri-\nbutional shift between the training and testing data.\n2\nRelated Work\nThere has been a lot of previous work on applying deep learning to CXR images to\ndetect a COVID-19 pulmonary disease [7, 9\u001512]. However, most of the existing work\nis using publicly available CXR and COVID-19 image data. Most of those images are\ncollected from heterogeneous sources with varying image and label quality, which raises\nconcerns about the quality and valid evaluation of deep learning models [13, 14].\nGenerative Adversarial Networks (GANs) [15] have been used for many applications\nin the medical imaging domain [16\u001518]. Some studies show promising results speci\u001ccally\nfor the CXR and COVID-19 domain [19, 20]. In contrast to existing work, we integrate\ndi\u001berentiable augmentation [21] into our GAN architecture. This enables us to train on\na very small dataset and still get meaningful results.\n3\nMaterials and Methods\nOur goal for this work is to correctly detect a COVID-19 pulmonary disease in chest\nX-ray images on local university hospital study data. Therefore, we train a deep learning\nmodel on publicly available COVID-19 image data and evaluate the model based on\nour study data. We further enhance the amount of available training data by generating\nsynthetic X-ray images. In this section we explain the origin and distribution of the data,\nas well as the deep learning model and training process.\n3.1\nData\nIn this work we analyze chest X-ray images in posteroanterior (PA) and anteroposterior\n(AP) front view. Typically the AP view is encountered for cases where the patient is\nbedridden. Figure 1 shows two male patient example CXR images from our study data.\nTraining data We use two di\u001berent training datasets, see Table 1. As a \u001crst step, we\nuse the COVIDx V8b dataset [7] to train our model. This dataset is one of the biggest\ncurated and publicly available COVID-19 CXR datasets. We use the training split of the\ndataset, which contains 13.794 COVID-19 negative and 2.158 COVID-19 positive frontal\nview X-ray images of 14.978 unique patients.\nIn a second step we enhance this training data by using 20.000 additional synthetic\nCXR images that we generated based on our study data. With that, we can add 10.000\nCOVID-19 positive and 10.000 COVID-19 negative images to our existing COVIDx V8b\ntraining data. This synthetic data is used to further augment the training of the classi-\n\u001ccation model and increase image diversity. A sample of the generated images has been\nreviewed by a radiologist to ensure that the model produces meaningful data.\nValidation data We validate the model by calculating loss metrics on the so called test\nsplit of the COVIDx V8B dataset. This dataset contains 200 COVID-19 positive and 200\nCOVID-19 negative images. We used this dataset to tune model parameters. This is to\navoid over\u001ctting our model to the testing data.\n22\n\nFig. 1. Chest X-ray images with lungs marked in red. (Left) COVID-19 positive image, typical\nGround-glass opaci\u001ccation marked in blue. (Right) COVID-19 negative image.\nTesting data The central data in this work comes from a single center retrospective\nstudy of the Universitätsklinikum Ulm. For this study 566 patients (average age 51.12y\n+/- 18.73y; range 23-82y, 315 women) of a single institution (11/2019-05/2020) were\nincluded. The data has been carefully reviewed and labeled by two radiologists after ded-\nicated training into COVID-19 positive and COVID-19 negative. The senior radiologist\n(CK) has 8 years of experience in thoracic imaging. This resulted in 110 positive images\nand 223 negative images, as seen in Table 1.\nThis testing data is used as a holdout set for \u001cnal model evaluation. With this method\nwe make sure to avoid any patient overlap between the training and testing data. Fur-\nthermore, we get a high-quality estimation of the generalization capabilities of the model\nwhen evaluating on the testing data. This is because the testing images come from a\ndi\u001berent data source, which leads to a distributional shift.\nTable 1. Distribution of images for all datasets\nDataset\nSplit\nCOVID-19 positive COVID-19 negative\nCOVIDx V8B\nTraining\n2.158\n13.794\nCOVIDx V8B + Synthetic Training\n12.158\n23.794\nCOVIDx V8B\nValidation\n200\n200\nUniklinik Ulm Study\nTest\n110\n223\n3.2\nNetwork Architecture\nFor classi\u001ccation we use the ResNet50 architecture [22]. The network has been pretrained\non the ImageNet [23] database. We replace the \u001cnal fully connected layer with a linear\n23\n\nβ 1 =\n0.9\nβ2 = 0.999\n0.001\n0.006\n224 × 224\n±5◦\n24\n\nhyperparameters and training procedures as described. This is to evaluate the e\u001bect of\nusing the synthetic data and make the results comparable.\nFor the StyleGAN generator, we train two di\u001berent models: one for each class of\nCOVID-19 positive and negative images. This is a simple method to ensure that we can\ngenerate a speci\u001cc image class. For further details regarding the training process of the\nStyleGAN generator see Späte 2021 [24].\n4\nResults\nTo investigate our models in a quantitative manner, we computed the accuracy, as well\nas F1-score, precision and recall for each class on the validation and testing data. The\nmetrics for the validation data are shown in Table 2. Both models perform quite well on\nthe validation data with an accuracy of 96 % and 95.5 % respectively. The results are\nin line with Wang et al. 2020 [7] and their COVIDNet-CXR-2 model. Interestingly, the\nCovidx+Synth model falls behind the other models, despite having a lot more training\ndata. This could be another indication of a distributional shift between the COVIDx\ndataset and the study data of the Universitätsklinikum Ulm.\nThe results for the testing data are also shown in Table 2. The table shows that a\nmodel trained on the COVIDx dataset can adapt quite well to the testing data, with\nan accuracy of 89.49 %. The model achieves a decent precision for COVID-19 cases\n(90.32 %), which is good since too many false positives would increase the burden for the\nhealthcare system due to the need for additional PCR testing. With a rather low recall\nof 76.36 % the model does miss quite a lot of COVID-19 cases. This can be especially\nproblematic in this sensitive medical setting, since false negatives lead to undetected\ncases of COVID-19.\nThis drawback can be controlled by using additional synthetic data to train the model.\nTable 2 shows an increase in accuracy (92.49 %) and most evaluation metrics. Especially\nthe improved recall of 95.45 % is very desirable. This comes with the cost of a slight\nreduction in precision (-6.32 %). Based on those results, it can be seen that our models\nperform quite well, especially when incorporating the synthetic data, but there are still\nseveral areas for improvement.\nTable 2. Evaluation metrics for models COVIDx and COVIDx+Synth on validation data (with\nreported metrics from Wang et al. [7] for comparison) and on testing data.\nAccuracy\nF1-Score\nPrecision\nRecall\nModel\nC19 pos. C19 neg. C19 pos. C19 neg. C19 pos. C19 neg.\nValidation Data\nCOVIDx\n0.9600\n0.9583\n0.9615\n1.0000\n0.9259\n0.9200\n1.0000\nCOVIDx+Synth\n0.9550\n0.9548\n0.9552\n0.9596\n0.9505\n0.9500\n0.9600\nCOVIDNet-CXR-2 [7]\n-\n-\n-\n0.9700\n0.9560\n0.9550\n0.9700\nTesting Data\nCOVIDx\n0.8949\n0.8276\n0.9244\n0.9032\n0.8917\n0.7636\n0.9596\nCOVIDx+Synth\n0.9249\n0.8936\n0.9420\n0.8400\n0.9760\n0.9545\n0.9103\n25\n\n5\nLimitations and Discussion\nIn this work we showed that a deep learning model trained with a comparatively large\nvolume of publicly available data for COVID-19 detection is able to generalize well to\nsingle source, local hospital data with patient demographics and technical parameters\nindependent of the training data. This is not without limitations, since the distributional\nshift between the training and testing data can lead to some undesirable results, especially\nfor important metrics like low recall values.\nWe show that this can be improved by using synthetically generated data to aug-\nment the training data. Although this works quite well, one of the reasons could be a\nrebalancing e\u001bect, that could have been achieved with various resampling methods as\nwell. Another reason could be a light form of data leakage, since the synthetic data was\ngenerated based on the testing data. This is not fully clear, since the StyleGAN generator\nhas no direct access to the ground truth data and just learns based on the feedback of a\ndiscriminator. Despite these concerns, the model shows promising \u001crst results and using\nsuch a model in an emergency setting could give a fast estimation for the prevalence of\npulmonary in\u001cltrates and therefore improve clinical decision-making and resource allo-\ncation.\nReferences\n1. Vogels, C.B., Brito, A.F., Wyllie, A.L., Fauver, J.R., Ott, I.M., Kalinich, C.C., Petrone,\nM.E., Casanovas-Massana, A., Muenker, M.C., Moore, A.J., Klein, J., Lu, P., Lu-Culligan,\nA., Jiang, X., Kim, D.J., Kudo, E., Mao, T., Moriyama, M., Oh, J.E., Park, A., Silva, J.,\nSong, E., Takahashi, T., Taura, M., Tokuyama, M., Venkataraman, A., Weizman, O.E.,\nWong, P., Yang, Y., Cheemarla, N.R., White, E.B., Lapidus, S., Earnest, R., Geng, B.,\nVijayakumar, P., Odio, C., Fournier, J., Bermejo, S., Farhadian, S., Cruz, C.S.D., Iwasaki,\nA., Ko, A.I., Landry, M.L., Foxman, E.F., Grubaugh, N.D.:\nAnalytical sensitivity and\ne\u001eciency comparisons of SARS-COV-2 qRT-PCR primer-probe sets. (apr 2020)\n2. Udugama, B., Kadhiresan, P., Kozlowski, H.N., Malekjahani, A., Osborne, M., Li, V.Y.C.,\nChen, H., Mubareka, S., Gubbay, J.B., Chan, W.C.W.: Diagnosing COVID-19: The disease\nand tools for detection. ACS Nano 14(4) (mar 2020) 3822\u00153835\n3. Yang, Y., Yang, M., Shen, C., Wang, F., Yuan, J., Li, J., Zhang, M., Wang, Z., Xing, L.,\nWei, J., Peng, L., Wong, G., Zheng, H., Wu, W., Liao, M., Feng, K., Li, J., Yang, Q., Zhao,\nJ., Zhang, Z., Liu, L., Liu, Y.: Evaluating the accuracy of di\u001berent respiratory specimens\nin the laboratory diagnosis and monitoring the viral shedding of 2019-nCoV infections. (feb\n2020)\n4. Arevalo-Rodriguez, I., Buitrago-Garcia, D., Simancas-Racines, D., Zambrano-Achig, P.,\nCampo, R.D., Ciapponi, A., Sued, O., Martínez-García, L., Rutjes, A., Low, N., Bossuyt,\nP.M., Perez-Molina, J.A., Zamora, J.: FALSE-NEGATIVE RESULTS OF INITIAL RT-\nPCR ASSAYS FOR COVID-19: A SYSTEMATIC REVIEW. (apr 2020)\n5. Kong, W., Agarwal, P.P.: Chest imaging appearance of COVID-19 infection. Radiology:\nCardiothoracic Imaging 2(1) (feb 2020) e200028\n6. Rubin, G.D., Ryerson, C.J., Haramati, L.B., Sverzellati, N., Kanne, J.P., Raoof, S., Schluger,\nN.W., Volpi, A., Yim, J.J., Martin, I.B.K., Anderson, D.J., Kong, C., Altes, T., Bush, A., De-\nsai, S.R., onathan Goldin, Goo, J.M., Humbert, M., Inoue, Y., Kauczor, H.U., Luo, F., Maz-\nzone, P.J., Prokop, M., Remy-Jardin, M., Richeldi, L., Schaefer-Prokop, C.M., Tomiyama,\nN., Wells, A.U., Leung, A.N.: The role of chest imaging in patient management during\nthe COVID-19 pandemic: A multinational consensus statement from the \u001deischner society.\nRadiology 296(1) (July 2020) 172\u0015180\n7. Wang, L., Lin, Z.Q., Wong, A.: COVID-net: a tailored deep convolutional neural network\ndesign for detection of COVID-19 cases from chest x-ray images. Scienti\u001cc Reports 10(1)\n(November 2020)\n26\n\n8. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial\nnetworks. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), IEEE (June 2019)\n9. Khan, A.I., Shah, J.L., Bhat, M.M.: CoroNet: A deep neural network for detection and\ndiagnosis of COVID-19 from chest x-ray images.\nComputer Methods and Programs in\nBiomedicine 196 (November 2020) 105581\n10. Ucar, F., Korkmaz, D.: COVIDiagnosis-net: Deep bayes-SqueezeNet based diagnosis of the\ncoronavirus disease 2019 (COVID-19) from x-ray images. Medical Hypotheses 140 (July\n2020) 109761\n11. Keidar, D., Yaron, D., Goldstein, E., Shachar, Y., Blass, A., Charbinsky, L., Aharony, I.,\nLifshitz, L., Lumelsky, D., Neeman, Z., Mizrachi, M., Hajouj, M., Eizenbach, N., Sela, E.,\nWeiss, C.S., Levin, P., Benjaminov, O., Bachar, G.N., Tamir, S., Rapson, Y., Suhami, D.,\nAtar, E., Dror, A.A., Bogot, N.R., Grubstein, A., Shabshin, N., Elyada, Y.M., Eldar, Y.C.:\nCOVID-19 classi\u001ccation of x-ray images using deep neural networks. European Radiology\n(may 2021)\n12. Shamout, F.E., Shen, Y., Wu, N., Kaku, A., Park, J., Makino, T., Jastrz¦bski, S., Witowski,\nJ., Wang, D., Zhang, B., Dogra, S., Cao, M., Razavian, N., Kudlowitz, D., Azour, L., Moore,\nW., Lui, Y.W., Aphinyanaphongs, Y., Fernandez-Granda, C., Geras, K.J.:\nAn arti\u001ccial\nintelligence system for predicting the deterioration of COVID-19 patients in the emergency\ndepartment. npj Digital Medicine 4(1) (may 2021)\n13. Tartaglione, E., Barbano, C.A., Berzovini, C., Calandri, M., Grangetto, M.:\nUnveiling\nCOVID-19 from CHEST x-ray with deep learning: A hurdles race with small data. Inter-\nnational Journal of Environmental Research and Public Health 17(18) (September 2020)\n6933\n14. Oakden-Rayner, L.: Exploring the chestxray14 dataset: problems (Dec 2017)\n15. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville,\nA., Bengio, Y.: Generative adversarial nets. In Ghahramani, Z., Welling, M., Cortes, C.,\nLawrence, N., Weinberger, K.Q., eds.: Advances in Neural Information Processing Systems.\nVolume 27., Curran Associates, Inc. (2014)\n16. Frid-Adar, M., Diamant, I., Klang, E., Amitai, M., Goldberger, J., Greenspan, H.: GAN-\nbased synthetic medical image augmentation for increased CNN performance in liver lesion\nclassi\u001ccation. Neurocomputing 321 (dec 2018) 321\u0015331\n17. Yi, X., Walia, E., Babyn, P.: Generative adversarial network in medical imaging: A review.\nMedical Image Analysis 58 (dec 2019) 101552\n18. Kazeminia, S., Baur, C., Kuijper, A., van Ginneken, B., Navab, N., Albarqouni, S.,\nMukhopadhyay, A.: GANs for medical image analysis. Arti\u001ccial Intelligence in Medicine\n109 (sep 2020) 101938\n19. Karbhari, Y., Basu, A., Geem, Z.W., Han, G.T., Sarkar, R.: Generation of synthetic chest\nx-ray images and detection of COVID-19: A deep learning based approach. Diagnostics\n11(5) (may 2021) 895\n20. Motamed, S., Rogalla, P., Khalvati, F.: RANDGAN: Randomized generative adversarial\nnetwork for detection of COVID-19 in chest x-ray. Scienti\u001cc Reports 11(1) (apr 2021)\n21. Zhao, S., Liu, Z., Lin, J., Zhu, J.Y., Han, S.: Di\u001berentiable augmentation for data-e\u001ecient\ngan training. In: Conference on Neural Information Processing Systems (NeurIPS). (2020)\n22. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE (June 2016)\n23. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A large-scale hi-\nerarchical image database.\nIn: 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, IEEE (jun 2009)\n24. Spaete, C.: Synthetic generation of medical images (unpublished master's thesis). Master's\nthesis, Technische Hochschule Ulm, Ulm (2021)\n25. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. (2014)\n26. Smith, L.N., Topin, N.: Super-convergence: Very fast training of neural networks using large\nlearning rates. (2017)\n27\n\nDetection of Driver Drowsiness by Calculating the\nSpeed of Eye Blinking\nMuhammad Fawwaz Yusri, Patrick Mangat, and Oliver Wasenm¨uller\nMannheim University of Applied Science, Germany\nmuhammadfawwaz.yusri@stud.hs-mannheim.de\np.mangat@hs-mannheim.de\no.wasenmueller@hs-mannheim.de\nAbstract. Many road accidents are caused by drowsiness of the driver. While\nthere are methods to detect closed eyes, it is a non-trivial task to detect the gradual\nprocess of a driver becoming drowsy. We consider a simple real-time detection\nsystem for drowsiness merely based on the eye blinking rate derived from the eye\naspect ratio. For the eye detection we use HOG and a linear SVM. If the speed of\nthe eye blinking drops below some empirically determined threshold, the system\ntriggers an alarm, hence preventing the driver from falling into microsleep. In\nthis paper, we extensively evaluate the minimal requirements for the proposed\nsystem. We ﬁnd that this system works well if the face is directed to the camera,\nbut it becomes less reliable once the head is tilted signiﬁcantly. The results of\nour evaluations provide the foundation for further developments of our drowsiness\ndetection system.\nKeywords: Computer vision, driver drowsiness detection, eye detection, eye blink-\ning rate\n1\nIntroduction\nAround 74% of European road users mostly agree that tired driving or microsleep is\na frequent crash cause. The statistics were gained in 2018 by E-Survey of road users’\nattitudes from more than 35,000 respondents across 32 countries [1].\nThus, driver monitoring becomes of increased importance [2], since the consequence\nof drowsiness can be recognized distinctively during driving. This behavior can be seen\nas the driver slowly starts losing consciousness. Furthermore, one of the important char-\nacteristics of drowsiness is slow eye movement [3,4]. In this paper, the movement of the\neyes will be the key criterion to distinguish between wakeful and drowsy drivers. We\nimplement and evaluate a practical and simple drowsiness detection algorithm that can\nbe easily integrated into driver-assistance systems. The system is merely based on the\neye aspect ratio and eye blinking rate, where we combine Histograms of Oriented Gradi-\nents (HOG) and linear support vector machines for reliable and accurate eye detection.\nUpon extensive experiments, we determine a threshold for the eye blinking rate, below\nwhich our algorithm triggers an alarm. We conducted extensive evaluations based var-\nious test cases, which challenge our system. While our drowsiness detection algorithm\nworks in principle, we identify circumstances, in which our system is less accurate. In this\nway we systematically elaborate the next steps to further improve our simple drowsiness\ndetection system.\n28\n\n2\nRelated Work\nThere are several methods to detect the features of the eyes as well as drowsiness. For\ninstance, some of the researches apply Viola-Jones cascade classiﬁer to diﬀerentiate the\neyes from other facial parts [5,6]. By determining the number of pixels on the iris, cornea\nand eyelid, the number of blinking and duration of the closed eyes can be calculated.\nWhile comparing the number of blinking with the blink rate set (normal = 8-10 blinks\nper minute, sleepy = 4-6 blinks per minute), drowsiness can be identiﬁed [5]. Islam et\nal. [6] calculate the eye aspect ratio to determine the eye closure time and total blink\nper minute. These values can then be compared to appropriate thresholds and an alarm\nis activated if the value exceeds or falls below the corresponding thresholds (depending\non the critical variable to consider).\nPicot et al. [7] developed a more advanced method to determine the drowsiness state\nof a driver. Their idea is analogous to the use of electro-oculograms (EOG) [8], where\nelectrodes are placed near the eyes and the voltage signals are measured. They record\nvisual signs from 60 hours of driving from diﬀerent drivers. Based on data-mining tech-\nniques their algorithm then identiﬁes patterns of drowsiness. Moreover, another similar\napproach uses a head gear to record the pupils on a driving simulation [9]. The algorithm\ncomputes the vertical length of dark pupils and is able to detect drowsiness from this\nvariable.\nA development in drowsiness detection uses binarization in combination with image\nﬁlters. The system proposed by Ueno et al. [10] is able to detect the vertical position of\nthe eyes. Therein the algorithm takes into consideration the size of the eyes to calculate\nthe ratio of opened and closed eyes.\nThe drowsiness of the driver is addressed by detecting the state of the eyes when\nthey are closed for a certain period of time [11,12,13]. Therein, the relevant parts of the\neyes are detected using Haar-Cascade classiﬁers [11,12]. This approach seems particularly\nsuitable to be easily integrated into a driver-assistance system, since it is merely based\non eye detection. However, it only detects whether the driver has already fallen into the\nmicrosleep state, which may be too late for the successful prevention of road accidents.\nThe goal of our work is to set up a comparably simple real-time drowsiness detection\nsystem with minimal requirements and to challenge it in an extensive evaluation by\nexecuting various test cases. In our work we follow Haq et al. [5] and use the eye blinking\nrate to decide if a driver becomes drowsy. However, we determine the eye blinking rate\ndiﬀerently. We measure the eye aspect ratio (used by Islam et al. [6]) and derive the eye\nblinking speed from it. In order to set the threshold for the eye blinking rate below which\nthe driver is considered to be drowsy, we follow Picot et al. [7] and Hayami et al. [9]\nby simulating a scenario that imitates sleepiness of a driver. The threshold will also be\ndependent on the individual size of the eyes [10]. After ﬁxing the threshold experimentally,\nwe challenge our drowsiness detection method in a series of test cases (partly inspired\nby Suhaiman et al. [13]). While the proposed method works in principle, our extensive\nevaluation reveals a reduction of the reliability in certain scenarios (e.g. tilting the head\nby larger angles). In this way we can systematically elaborate the next research steps to\nincrease the accuracy of our simple drowsiness detection system in a broader range of\nscenarios.\n3\nMethods\nThe drowsiness of a driver can be anticipated by analyzing the movement of the eyelids.\nThe eyelids move slower than a normal blink. In this paper, we implement an algorithm\n29\n\n(a) 6 points eye landmarks\n(b) EAR over time for an eye blinking around\nframe 110\nFig. 1: Comparison between opened and closed eye [18]\nthat allows us to determine the speed of the blinking eye. Moreover, we use Histogram\nof Oriented Gradients (HOG) and a Linear Support Vector Machine (SVM) method to\nimprove eye detection (4.89% higher accuracy compared to Haar-Cascade, see Rahmad\net al. [14]).\n3.1\nEyes Detection\nThe algorithm is trained to detect the landmarks of the facial features in the dlib library\nby using an ensemble of regression tress [15]. HOG image descriptor and SVM are the\nmethod for the process training of an object [16]. There are many datasets available to\ndetect these landmarks and we are using the dataset from IBUG which has 68 points of\nfacial landmarks [17].\n3.2\nEye Blinking Speed\nHaving detected the eyes of the driver, the next step is to determine the eye blinking\nspeed. Firstly, we have to detect whether the state of the eye is opened or closed. A\nsuitable measure to derive the state of the eye is the eye aspect ratio (EAR). We follow\nthe deﬁnition of Soukupov´a et al. [18]:\nEAR = |p2 −p6| + |p3 −p5|\n2|p1 −p4|\n,\n(1)\nwhere p1 to p6 are the facial landmarks as depicted in Figure 1a. When the eye is opened,\nthe EAR is above 0.35, but when it is closed, the value rapidly dropped below 0.15 (see\nFigure 1b and Section 4 for the corresponding experiments).\nBased on the ﬂow diagram in Figure 2, ﬁrstly, the algorithm will ﬁnd the average eye\nsize (AES) of the driver deﬁned by\nAverage Eye Size (AES) = Max EAR1 + Max EAR2 + Max EAR3\n3\n,\n(2)\ni.e. we take the arithmetic mean of three measured maximum EARs. (Notice that the\naccuracy of the AES can be improved by measuring more maximum EARs, but this is\nat the expense of a higher computational eﬀort.)\n30\n\nFig. 2: Flow diagram to determine the eye blinking speed\nAfterwards, Max and Min Threshold will be calculated based on the average eye’s\nsize value. Max and Min Threshold are deﬁned by\nMax Threshold = 2\n3AES + 0.0467 ,\n(3)\nMin Threshold = Max Threshold −0.05 .\n(4)\nThe numerical values in these equations were found empirically. After Max and Min\nThreshold have been determined, the algorithm will search for the maximum value of\nthe EAR (denoted by Max EAR) while capturing the images frame by frame. When\nthe current EAR is less than the current maximum value, it will start the timer and at\nthe same time ﬁnd the minimum value of the EAR (denoted by Min EAR). The ﬁnal\nminimum value is determined, when the current EAR eventually becomes larger than\nthe minimum value, thus the timer will stop. The blinking speed for each blink can be\ncalculated by\nBlinking Speed = Max EAR −Min EAR\nStart Time −Stop Time .\n(5)\nIf the blinking speed becomes suﬃciently low, the algorithm will activate an alarm system.\nFor this purpose we introduce an empirically determined drowsiness threshold. Whenever\nthe eye blinking speed is below this drowsiness threshold, the algorithm identiﬁes the\ndriver as being in a drowsy state.\n4\nEvaluation\nFor the evaluation of our drowsiness detection system we proceed as follows. We ﬁrst\nshow experimentally that changing the distance between eyes and camera leaves the EAR\ninvariant. Then, we determine the speed of the eye blinking in the wakeful and sleepy\nstate, respectively. Finally, we evaluate the impact of changes in the head positions on\nour system.\n31\n\n(a) The distance between p2 and p6\n(b) EAR from both images\nFig. 3: Result for constant EAR’s evaluation\n4.1\nEvaluation of the Impact of Distance Variations on the EAR\nWhen alternating the distance between eyes and camera, the apparent size of the eyes\nwill change. However, based on eq. (1) we expect the EAR to remain invariant.\nThe following evaluation shows experimentally that the EAR is indeed invariant under\nmodiﬁcation of the distance of camera and eyes. For this purpose, two similar images\nwith diﬀerent sizes are used instead of a live stream video. The reason is to have a ﬁxed\nEAR reference from a static image, enabling a comparison of the EAR from both images\nthat have diﬀerent eye’s sizes. The sizes of the eyes in both images are relatively diﬀerent\nbecause one image is close to the camera and the other is far from it. The size can be\nmeasured by calculating the diﬀerence distance between two points of facial landmarks\n(e.g. p2 and p6 in Figure 3a, which are the upper and lower eyelid) in both images.\nThe distance between these points in Image 1 (Near, blue line in Figure 3a and\nFigure 3b) is two times bigger than Image 2 (Far, orange line in Figure 3a and Figure 3b)\nwhich is approximately 8 and 4 pixels (see Figure 3a), respectively. It shows that the\nposition of the eyes in Image 1 is nearer to the camera than Image 2.\nThe ratio between both distances is approximately 1:2. The result in Figure 3b shows\nthat the measured EARs for Images 1 and 2 are approximately 0.3052 and 0.3006, re-\nspectively. The deviation is only 1.5%, which is acceptable for our purposes.\n4.2\nEvaluation for Normal Blinking\nThe purpose of this evaluation is to check whether Max EAR, Min EAR, and hence, the\nblinking speed in eq. (5) are calculated correctly. Moreover, the average blinking speed\nin the wakeful state can be determined from this test.\nThe participants were asked to blink normally for 8 times. The ﬁrst three blinks were\nanalyzed by the algorithm to obtain the average size of the eyes. The other ﬁve blinks\nwere necessary to evaluate the EAR and the speed of the blinking. Three tests from three\nparticipants were conducted thoroughly and the results are as follows.\nParticipant 1: Figure 4a shows that the maximum of the EAR, which is the highest\nvalue of the EAR before the eyes start to close, is calculated correctly above the Max\nThreshold (deﬁned in (3)). Moreover, we see that there is only one data point below the\nminimum threshold (orange line). The minimum threshold can be determined using (4).\nWe experimentally obtain Max EAR = 0.4572 and Min EAR = 0.1098. Figure 4b shows\n32\n\n(a) Speed of normal blinking\n(b) Speed values from ﬁve blinks\nFig. 4: Result from Participant 1 (normal blink)\n(a) Normal blinking’s speed\n(b) Speed values from ﬁve blinks\nFig. 5: Result from Participant 2 (normal blink)\nthat the values of the blinking speed are above 1 pixel/second and the average speed is\n2.0418 pixel/second.\nParticipant 2: Figure 5a shows that Max EAR = 0.4522. However, there are two data\npoints below the minimum threshold, whose diﬀerence in the EAR is 0.01 pixels. The\nalgorithm needs to determine which value to choose as a minimum. The ﬁnal minimum\nvalue is Min EAR = 0.1401 which is the correct value because this is the value when\nthe eyes are completely closed. If the diﬀerences between the values below minimum\nthreshold is less than 0.01, then the ﬁnal minimum will be the ﬁrst value because this is\nthe point where the eye is completely shut.\nWe can see that Figure 5b is similar to Figure 4b where the speed values are above 1\npixel/second and the average speed is 1.5783 pixel/second.\nParticipant 3: In this case the EAR reaches a rather ﬂat maximum at Max EAR =\n0.3558 before dropping sharply below the minimum threshold (see Figure 6a). The mini-\nmum EAR is measured to be Min EAR = 0.1049. Again, the blinking speed never drops\nbelow 1 pixel/second (see Figure 6b). The average eye blinking rate is approximately at\n2 pixels/second.\n33\n\n(a) Normal blinking’s speed\n(b) Speed values from ﬁve blinks\nFig. 6: Result from Participant 3 (normal blink)\n(a) Sleepy blinking’s speed\n(b) Speed values from ﬁve blinks\nFig. 7: Result from Participant 1 (sleepy blink)\nTo summarize the results of the three tests, the average speed for normal blinking\nhas a threshold value of 1 pixel/second. However, there are certain cases where the eye\nblinking speed can slightly drop below this threshold.\n4.3\nEvaluation for Sleepy Blinking\nFor the measurement of the EARs and the eye blinking rate in the drowsy state, we\nessentially repeat the previous experiments. The evaluation was conducted by testing\nthree participants with 5 trials. They were asked to imitate the behavior of a sleepy\ndriver in front of the camera by closing their eyes slowly. The corresponding data are\nshown and discussed below.\nParticipant 1: In Figure 7a it can be seen that the number of frames from Max EAR =\n0.3053 down to Min EAR = 0.1406 is larger than in the wakeful state, indicating a lower\neye blinking rate. Indeed, Figure 7b shows the smaller speed values for all ﬁve blinks.\nThe average speed is 0.4421 pixel/second.\nParticipant 2: Figure 8a has a slightly diﬀerent result from the ﬁrst test (see Figure 7a),\nspeciﬁcally regarding the values below the minimum threshold. It can be seen that the\nEAR remains below the minimum threshold after Min EAR = 0.1056 has been reached.\n34\n\n(a) Sleepy blinking’s speed\n(b) Speed values from ﬁve blinks\nFig. 8: Result from Participant 2 (sleepy blink)\n(a) Sleepy blinking’s speed\n(b) Speed values from ﬁve blinks\nFig. 9: Result from Participant 3 (sleepy blink)\nThe participant was closing his eyes longer than usual which mimics one of the main\nbehaviors of a sleepy person. The values are in the range between 0.5167 pixel/second\nand 0.2172 pixel/second. The average speed value is 0.3557 pixel/second.\nParticipant 3: The graph in Figure 9a has a similar pattern as in the second test\n(Figure 8a), where there are many values below the minimum threshold. The results\nfrom the third test (Figure 9b) came out as expected and the speed values are in the\nrange between 0.5176 pixel/second and 0.2292 pixel/second. The average speed of the\neye blinking is 0.35586 pixel/second.\nAs a result of these tests, it can be concluded that the average speed value for sleepy\neye blink is below 0.5 pixel/second and hence the drowsiness threshold to activate the\nalarm will be set to 0.55 pixel/second (taking into account a safety buﬀer). Thus, this is\nthe threshold value which will activate the alarm if the speed drops below it.\n4.4\nEvaluation for Diﬀerent Head Positions\nIn this last evaluation, the participant went through diﬀerent situations to test the re-\nliability of the system. These test cases, inspired by Suhaiman et al. [13], simulate the\nscenario when the driver moves his head in diﬀerent directions.\n35\n\nIn the ﬁrst test case, the participant was instructed to move his head upwards, down-\nwards, left and right while looking in front. Eventually, in the second test case, the\nparticipant also move his head in the same direction as in the ﬁrst situation but the\neyes also follow the direction of the head. For example, if the participant tilts his head\nupwards, his eyes should look upwards. Table 1 shows the diﬀerent situations of the head\nmovement and the results.\nTable 1: Evaluation of diﬀerent head positions\nEyes look in front\nTest case\nResult\nHead\nfaces\nupward\nand downward\n• Able to detect eyes up to a certain degree\n• Able to detect eye blinking\n• EAR becomes smaller\n• Smaller EAR aﬀects ﬁnding the correct Max EAR and Min EAR\nHead turns to the left\nand right\n• Able to detect eyes as long both eyes are visible to the camera\n• Able to detect eye blinking but the speed is inaccurate\n• Some speeds cannot be calculated for certain head poses\nEyes follow head’s movement\nTest case\nResult\nHead\nfaces\nupward\nand downward\n• Able to detect eyes up to a certain degree\n• EAR becomes smaller (bigger than when eyes look in front)\n• Able to detect eye blinking\n• Speed is less accurate\nHead turns to the left\nand right\n• Able to detect eyes as long both eyes are visible to the camera\n• Able to detect eye blinking but the speed is inaccurate\n• Some speeds cannot be calculated for certain head poses\n5\nConclusion\nIn this paper, we have shown that by calculating the speed of the eye blinking, we are\nable to distinguish between a wakeful and a drowsy blink of a driver in real-time. In\nparticular, we can also detect the gradual process of a driver becoming drowsy. Such a\nreal-time drowsiness detection system plays a key role in preventing car accidents due to\nmicrosleep.\nHowever, our extensive evaluations also revealed some deﬁcits, which should be ad-\ndressed in future developments of our simple algorithm. Firstly, the problem where the\nalgorithm cannot detect the eyes in a certain angle when the head is in a certain position\n(such as tilted upwards or downwards) can be improved either by identifying the rota-\ntion of the head and give conditions in the program or by including additional cameras\npositioned in diﬀerent angles [19]. Secondly, facial expression such as smiling has an im-\npact on the measured EAR. Therefore an additional algorithm is needed to detect facial\nexpression, which can then be used to adapt the maximum and minimum thresholds\ndeﬁned in (3) and (4). Another improvement of our algorithm regards the inclusion of\noptical eﬀects that can occur due to eye glasses.\n36\n\nReferences\n1. Goldenbeld, C., Nikolaou, D.: E-survey of road users’ attitude (2019) SWOV Institute for\nRoad Safety Research SWOV.\n2. Cruz, S.D.D., Wasenm¨uller, O., Beise, H.P., Stifter, T., Stricker, D.: SVIRO: Synthetic vehi-\ncle interior rear seat occupancy dataset and benchmark. In: IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV). (2020)\n3. Ebrahim, P.: Driver drowsiness monitoring using eye movement features derived from elec-\ntrooculography. PhD thesis, University of Stuttgart (2016)\n4. Soares, S., Monteiro, T., Lobo, A., Couto, A., Cunha, L., Ferreira, S.: Analyzing driver\ndrowsiness: From causes to eﬀects. MDPI Sustainability 12(5) (2020)\n5. Haq, Z.A., Hasan, Z.: Eye-blink rate detection for fatigue determination. In: India Interna-\ntional Conference on Information Processing (IICIP). (2016)\n6. Islam, A., Rahaman, N., Ahad, M.A.R.: A study on tiredness assessment by using eye blink\ndetection. In: Jurnal Kejuruteraan. (2019) 209–214\n7. Picot, A., Charbonnier, S., Caplier, A.: Drowsiness detection based on visual signs: blink-\ning analysis based on high frame rate video. In: IEEE International Instrumentation and\nMeasurement Technology Conference (I2MTC). (2010)\n8. Galley, N., Schleicher, R., Galley, L.: Blink parameter as indicators of driver’s sleepiness\n– possibilities and limitations (2003) University of Cologne, Institute for Clin. Psychology\nand Psychotherapy).\n9. Hayami, T., Matsunaga, K., Shidoji, K., Matsuki, Y.: Detecting drowsiness while driving\nby measuring eye movement-a pilot study. In: IEEE International Conference on Intelligent\nTransportation Systems (ITSC). (2002)\n10. Ueno, H., Kaneda, M., Tsukino, M.:\nDevelopment of drowsiness detection system.\nIn:\nVehicle Navigation and Information Systems Conference (VNIS). (1994) 15–20\n11. Adochiei, I.R., Oana-Isabela, Adochiei, N.I., Gabriel, M.P., Larco, C.M., Mustata, S.M.,\nCostin, D.: Drivers’ drowsiness detection and warning systems for critical infrastructures.\nIn: IEEE International Conference on E-Health and Bioengineering (EHB). (2020)\n12. Kolpe, P., Kadam, P., Mashayak, U.:\nDrowsiness detection and warning system using\npython. In: International Conference on Communication and Information Processing (IC-\nCIP). (2020)\n13. Suhaiman, A.A., May, Z., Rahman, N.A.A.: Development of an intelligent drowsiness de-\ntection system for drivers using image processing technique. In: IEEE Student Conference\non Research and Development. (2020)\n14. Rahmad, C., Asmara, R., Putra, D., Dharma, I., Darmono, H., Muhiqqin, I.: Comparison\nof viola-jones haar cascade classiﬁer and histogram of oriented gradients (hog) for face\ndetection. In: IOP Conference Series: Materials Science and Engineering. (2020)\n15. Sagonas, C., Antonakos, E., Tzimiropoulos, G., Zafeiriou, S., Pantica, M.: 300 faces in-the-\nwild challenge: database and results. Image and Vision Computing (2016) 3–18\n16. Dalal, N., Triggs, B.:\nHistograms of oriented gradients for human detection.\nIn: IEEE\nConference on Computer Vision and Pattern Recognition (CVPR). (2005)\n17. Kazemi, V., Sullivan, J.: One millisecond face alignment with an ensemble of regression\ntrees. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (2014)\n18. Soukupov´a, T., Cech, J.: Real-time eye blink detection using facial landmarks. In: 21st\nComputer Vision Winter Workshop. (2016)\n19. Feld, H., Mirbach, B., Katrolia, J., Selim, M., Wasenm¨uller, O., Stricker, D.: DFKI cabin\nsimulator: A test platform for visual in-cabin monitoring functions. In: Commercial Vehicle\nTechnology, Springer (2021) 417–430\n37\n\nMFmap: A semi-supervised generative model matching cell \nlines to cancer subtypes \nXiaoxiao Zhang1,2, Maik Kschischo1\n1  Department of Mathematics and Technology, RheinAhrCampus, University of Applied Sciences \nKoblenz, 53424 Remagen, Germany \nkschischo@rheinahrcampus.de \n2 Department of Informatics, Technical University of Munich, 81675 Munich, Germany \nAbstract. Cell lines are widely used experimental models in cancer research. However, \nWUDQVODWLQJ\u0003 SUHFOLQLFDO\u0003 ¿QGLQJVLQWR\u0003 FOLQLFDO\u0003 DSSOLFDWLRQV\u0003 is limited by the discordance \nEHWZHHQ\u0003FHOO\u0003OLQHV\u0003XVHG\u0003DQG\u0003WXPRXUV\u0011\u0003:H\u0003GHYHORSHGWKH\u0003PRGHO\u0003¿GHOLW\\\u0003PDS\u0003\u000b0)PDS\f\u000f\u0003D\u0003\nsemi-supervised generative model to integrate high-dimensional geneexpression, copy \nnumber variation and somatic mutation data of both tumours and cell lines into a small set \nof features that are highly associated with cancer subtypes, and predict the cell line subtypes \nsimultaneously. These low-dimensional features are biologically interpretable and can be \nused for matching a given cell line to individual tumours. This enables cancer researchers \nWR\u0003VHOHFW\u0003WKH\u0003EHVW\u0003FHOO\u0003OLQH\u0003PRGHO\u0003IRU\u0003WKHLU\u0003H[SHULPHQWV\u00117KH\u0003KLJK\u0003DFFXUDF\\\u0003\u000bWHVW\u0003VHW\u0003)\u0003\u0014\u0003VFRUH\u0003\n! \u001c\u0013\b\f\u0003RI\u00030)PDS\u0003FDQFHU\u0003VXEW\\SH\u0003SUHGLFWLRQ\u0003LV\u0003YDOLGDWHG\u0003LQ\u0003WHQ\u0003GL൵HUHQW\u0003FDQFHU\u0003GDWDVHWV\u0011\nFrom an application perspective, we demonstrate how the predicted cancer subtype for cell\nOLQHV\u0003 FDQ\u0003 EH\u0003 H[SORLWHG\u0003 IRU\u0003 GLVFRYHULQJ\u0003 GUXJ\u0003 VHQVLWLYLW\\\u0003 GL൵HUHQFHV\u0003 DPRQJ\u0003 VXEW\\SHV\u0003 LQ\nglioblastoma and breast cancer. This is helpful for guiding personalised treatment decisions \nand could facilitate drug repurposing for cancer treatments. Thanks to its generative nature, \nMFmap enables the analysis of cellular status transitions during cancer progression. In\nSDUWLFXODU\u000f\u0003ZH\u0003VKRZ\u0003WKDW\u0003DUWL¿FLDOO\\\u0003SHUWXUELQJ\u0003FDQFHU\u0003VDPSOHV\u0003IURP\u0003D\u0003EDVHOLQH\u0003VXEW\\pe to\nan aggressive subtype indeed acquires marker features unique to the targeted subtype in\nglioblastoma. From a methodological perspective, the newly derived loss function of our\n0)PDS\u0003 DOORZV\u0003 WR\u0003 MRLQWO\\\u0003 WUDLQ\u0003 D\u0003 FODVVL¿FDWLRQ\u0003 PRGHO\u0003 DQG\u0003 D\u0003 JHQHUDWLYH\u0003 PRGHO on both\nODEHOOHG\u0003\u000bWXPRXUV\f\u0003DQG\u0003XQODEHOOHG\u0003\u000bFHOO\u0003OLQHV\f\u0003GDWD\u0003LQ\u0003DQ\u0003RQH-step-optimisation manner. We\nIXUWKHU\u0003HPSLULFDOO\\\u0003VKRZ\u0003WKDW\u0003WKH\u00030)PDS\u0003FDQ\u0003DFKLHYH\u0003H[FHOOHQW\u0003FODVVL¿FDWLRQ\u0003DFFXUDF\\\u0003DQG\ngood generative performance simultaneously. These results show that the MFmap will be\nuseful for many semi-supervised prediction tasks in the biomedical sciences and beyond.\nThis work was supported by the FOR2800 research unit funded by the Deutsche \nForschungsgemeinschaft. \n38\n\nSystematic investigation of Basic Data Augmentation\nStrategies on Histopathology Images\nJonas Annuscheit, Benjamin Voigt, Oliver Fischer, Patrick Baumann,\nSebastian Lohmann, Christian Krumnow and Christian Herta\nUniversity of Applied Sciences (HTW) Berlin, Centrum f¨ur Biomedizinische Bild- und\nInformationsverarbeitung (CBMI), Ostendstraße 25, 12459 Berlin, Germany\n{first name}.{last name}@htw-berlin.de\nAbstract. Recent years have witnessed the rapid progress of deep neural net-\nworks. However, in supervised learning, the success of the models hinges on a\nlarge amount of training data. Therefore, data augmentation techniques were de-\nveloped to increase the eﬀective size of the training data. Using such techniques is\nespecially important for domains where the amount of available data is limited. In\ndigital pathology, data augmentation is therefore often applied to improve the per-\nformance of classiﬁcations. This work systematically investigates single data aug-\nmentation techniques on diﬀerent datasets using multiple network architectures.\nFurthermore, it proposes guidelines on using data augmentation when training\ndeep neural networks on histopathological data.\nKeywords: Convolutional Neural Network, Data Augmentation, Digital Pathol-\nogy\n1\nIntroduction\nThe prediction quality of supervised learning models relies on the available data’s quan-\ntity, quality, and heterogeneity. In training a deep neural network, these factors are\nessential to create a robust and generalizing model. Diﬀerent transformation techniques\ncan be utilized on the available data to synthesize new samples if a dataset lacks some\nof these aspects. Such techniques are summarized under the term data augmentation.\nNowadays, there are a variety of diﬀerent augmentation methods to synthesize new\ndata. These range from classic image manipulation approaches to more contemporary\nmethods like training with adversarial examples [1] or generating entirely new datasets\nusing generative adversarial networks [2]. Several studies investigated the eﬀect of such\ndata transformations on traditional machine learning datasets and proved their beneﬁt [3,\n4]. We applied and reviewed some of these transformations to the domain of histopatho-\nlogical datasets.\nIn recent years, the medical ﬁeld of pathology has been subject to digital change.\nPart of this change is to aid the traditional diagnostics, i.e., inspecting extracted tissue\nsections under a light microscope with computer algorithms [5, 6]. A promising option is\nto let machine learning or deep learning support pathologists’ diagnostic work. Therefore,\nnumerous research studies attempt to answer speciﬁc pathological questions using neural\nnetworks. Since these questions are usually image classiﬁcation problems, the approaches\nuse the supervised learning regime, utilizing convolutional neural networks (CNN) [7–9].\nAlthough there are examples of publicly available digitized tissue samples [10], there\nis a lack of well-curated datasets useful for the supervised learning approach. In addition,\nhighlighting the need for data augmentation methods in this domain, most public datasets\n39\n\nare relatively small. Collecting suitable images for a given medical problem is challenging\ndue to the non-uniformity of manifestations and the need to consider patient rights.\nLabeling these images requires the highly specialized expertise of a pathologist, adding\nto an already busy workload.\nIn this work, we build a pipeline to systematically investigate basic data augmen-\ntation techniques on diﬀerent classiﬁcation datasets and network architectures. For this\npurpose, we selected two public histopathological datasets for diﬀerent medical prob-\nlems: classiﬁcation of mitosis candidates and tissue type classiﬁcation. We trained three\ncontemporary CNN architectures for all of these data sets, examining diﬀerent types of\naugmentation methods. This paper describes the experimental setup to measure the in-\nﬂuence of a single data augmentation technique on the model’s performance. In addition,\nit proposes guidelines for using data transformations in the supervised learning setting\nregarding diﬀerent types of histopathological data. Finally, it discusses under which cir-\ncumstances data augmentation has a reliable beneﬁt for a model’s training process.\n2\nRelated Work\nDue to its regularization eﬀect, data augmentation is a popular method used in deep\nlearning pipelines to reduce overﬁtting and increase the robustness of a model, especially\nconcerning an image classiﬁcation problem. In fact, the method is so established that\nseveral tools exist to make standard techniques more accessible [11–13] or even automate\nthe augmentation process [14, 15].\nSeveral studies examined the actual inﬂuence of diﬀerent data augmentation tech-\nniques and showed its beneﬁcial eﬀects in the context of natural images [3, 16]. A widely\nused taxonomy to divide the common techniques into categories is basic image manipula-\ntions, e.g., geometric transformations, cropping, occlusion, noise injection, ﬁltering, color\ntransformations, and deep learning approaches, e.g., adversarial training, style transfer,\nsynthetic image generation via generative adversarial networks [3, 17].\nUnlike in the natural image domain, where datasets can provide millions of images, far\nfewer qualitatively annotated samples are available in the ﬁeld of histopathology. Hence,\ndata augmentation has established itself as an integral part of the training pipelines in\nthis area as well. Interestingly, it is almost exclusively the use of newer augmentation\ntechniques from the deep learning approaches that have been broadly reviewed thus far.\nGenerative adversarial networks (GANs) were investigated to solve the stain normal-\nization problem using style transfer methods. Color diﬀerences and disturbances are a\nconsiderable challenge through various tissue staining protocols and the varying digi-\ntization processes. Style transfer can homogenize the color distribution in a data set\nand thus the distributional shift in a dataset [18–20]. Some studies even explored the\ntransfer of staining protocols utilizing GANs; e.g., Mercan et al. trained a model that\nconverts images obtained from H&E stained tissue into virtual PHH3 staining [21]. In\naddition, GANs are used to synthesize completely artiﬁcial samples to enrich small data\nsets [22–24].\nConcerning basic image manipulation, many approaches use several techniques to aug-\nment their datasets but do not evaluate the inﬂuence of augmentations; see, e.g. [25, 26].\nPrimarily, basic manipulation techniques are used intensively in conjunction with semi-\nsupervised learning methods, which are becoming increasingly popular in this domain\n[27–29]. Color transformations, in particular, are one of the most widely used techniques\ndue to the nature of histopathological images [26]. Tellez et al.[30] and Karimi et al.[31]\n40\n\nexamined the stain normalization problem more closely and developed custom augmen-\ntation techniques for it.\nHowever, in-depth studies which comprehensively evaluate the eﬀect of basic image\nmanipulation techniques can only be found for radiology images in the medical domain\n[17, 32–34].\n3\nMethod\nWe have developed a pipeline to measure the eﬀect of data augmentation techniques\nin supervised learning on histopathology datasets.1 We can conﬁgure experiments as a\ntriple (d, m, t) ∈D ×M ×T where D is the set of possible datasets, M denotes the set of\nconsidered deep neural network architectures, and T corresponds to the set of diﬀerent\ndata augmentations. A speciﬁc transformation t manipulates online the batches drawn\nfrom the dataset d to create transformed batches, that are used to train a deep neural\nnetwork m. Keeping the pair (d, m) constant, the inﬂuence of t on the trained model can\nbe measured by comparing its parameters and performance. This section describes the\nsets D, M, T in more detail and contains information about the training and evaluation\nprotocol used.\n3.1\nDatasets D\nSet D consists of the publicly available datasets MIDOG2 and BACH3. Both sets were\npre-processed to ﬁt a classiﬁcation problem.\nThe task of the BACH dataset is to distinguish between four diﬀerent tissue types.\nIt is a tiny dataset with images of 2048x1536 pixels and 100 samples per class, i.e., 400\nsamples overall. Therefore, we cropped patches from the original images using a 512x512\nwindow with a 256-pixel step to increase the dataset size. In addition, we discarded\npatches not containing any H&E-stained tissue during the process by removing tiles with\nless than 3% tissue. Finally, we split the dataset into three subsets using random sampling\nfor training (4801 samples), validation (1655 samples), and test (1647 samples). Patches\nwith overlapping pixels in the subsets were removed. The classes of the dataset are nearly\nbalanced.\nThe MItosis DOmain Generalization Challenge [35] published a dataset of human\nbreast cancer tissue samples. However, we note that up to this point, only the training\nset is publicly available, consisting of 1721 mitotic ﬁgures and 2714 non-mitotic examples.\nThe samples were acquired using three diﬀerent whole slide image scanners and annotated\nby trained pathologists with a multi-expert blind annotation pipeline. We pre-processed\nthe dataset by cropping a 250x250 patch around each annotation center. We sampled\nthree distinct subsets keeping the class balance: training (2219), validation (1071), and\ntest (1145).\n3.2\nModels M\nThe model set M consists of the networks VGG[36], Inception[37] and Densenet[38]. These\nnetworks form a cross-section over the development of CNN architecture and, therefore,\n1 The\nsource\ncode\nis\navailable\nvia\nGithub:\nhttps://github.com/CBMI-HTW/Data-\nAugmentation-Histology\n2 https://imi.thi.de/midog/the-dataset/\n3 https://iciar2018-challenge.grand-challenge.org/Dataset/\n41\n\nhave distinct structural elements. We intend to investigate whether these structures react\ndiﬀerently to data augmentation. We use a pre-trained PyTorch model with reinitialized\nclassiﬁcation head, i.e. vgg11 bn, inception v3 and densenet121, as baseline for each net-\nwork type. These models require as input square images of a model-dependent size nm,\ni.e. the input images have shape nm × nm.\n3.3\nTransformations T\nThe transformations in T fall in the categories: color-based, geometric-based, ﬁlter-based\ntransformations, and erasing. All transformations are realized by using the implementa-\ntion of the torchvision library.\nIn our setup, transformations are applied with a certain probability p, with p = 1 if not\nstated diﬀerently. For most of these transformations t an additional parameter s controls\nthe strength of the transformation on the input x, i.e. the output of the transformation is\nts(x). For each corresponding transformation, s is sampled from a certain interval, where\nthe size of this interval is a hyperparameter in our setting.\nThe hyperparameters of the geometric-based transformations are determined by nm\nand the maximal distortion without getting blacked borders. For all other transformations\nwe perform a hyperparameter optimization on the BACH dataset and VGG model to\nidentify the best parameter ranges. We trained for each conﬁguration of parameters a\nminimum of 7 models and choose the setting with the highest mean validation accuracy as\nthe best performing one. These models were only used to determine the hyperparamters\nand not for the ﬁnal test results.\nColor-Based Transformation As color-based transformations we use the standard\ntransformation for brightness, contrast, gamma value, hue angle and saturation. The\nstrength s is sampled randomly from the interval [s0 −s1, s0 + s1] where, the center s0\nis deﬁned by ts0 = identity. Here, we sample s indirectly by drawing r ∈[0, 1] according\nto the beta distribution Beta(α = 8, β = 8) and computing s = s0 + (2r −1)s1. The\nhalf-width s1 was determined in a hyperparameter optimization. The parameters for the\nhyperparameter optimization are summarized in Tab. 1, where the intervals for choosing\ns1 where found iteratively by hand over multiple trials, ensuring that the chosen value\ndoes not lie on the boundary.\nGeometric-Based Transformation As geometric transformations, we investigate ﬂips,\nrotations, random cropping to size nm, shearing and scaling. For scaling, rotation, and\nshearing, we sample s uniformly from an interval [s0 −s1, s0 +s1] as summarized in Tab.\n1, where we consider two scaling scenarios. In the case of the ﬂip transformation, we\napply horizontal and/or vertical ﬂips to the input, each with a probability of 0.5. Both\nscaling transformations, as well as shearing, is done with p = 0.9.\nFilter- and Erasing-Based Transformation As ﬁlter-based transformations, we\nstudy Gaussian blurring as well as a sharpness adaption.\nFor Gaussian blurring, we pick uniformly an odd kernel size between 3 and 15 and use\na minimum and maximum sigma (these are direct inputs to the trochvision implemen-\ntation) of 0.001 and 0.5, respectively and set p = 0.5. We did a hyperparameter search\nfor the maximal kernel-size in {7, 11, 15}, the maximum sigma value in [0.5, 8] and p in\n{0.5, 0.75}.\n42\n\nTable 1. Parameters used for the color- and geometric-based transformations. The center s0 is\nﬁxed where the interval half-width s1 was either optimized with a hyperparameter optimization\nwithin the given intervals or chosen as stated.\ntransformation s0\ns1\nchoice of s1\ntransformation s0\ns1\nchoice of s1\nbrightness\n1.0 0.0175\n[0.005, 0.3]\nrotation\n0 180◦ﬁxed geometrically\ncontrast\n1.0\n0.1\n[0.025, 0.6]\nscale I\n1 0.15\nchosen by hand\nhue\n0.0 0.00625 [0.00025, 0.6]\nscale II\n1 0.29 ﬁxed geometrically\nsaturation\n1.0\n0.025\n[0.01, 0.6]\nshear\n0 22◦ﬁxed geometrically\ngamma\n1.0\n0.05\n[0.00625, 0.6]\nThe sharpness adaption depends on a parameter s ≥0 where for s < 1, the image\nis blurred and for s > 1 sharpened and s = 1 corresponds to the identity. To deﬁne s\nwe sample r ∈[0, 1] according to Beta(α = 8, β = 8) and set s = 2r if r ≤0.5 and\ns = 4.5(2r −1) + 1 if r > 0.5. The factor 4.5 determines the maximal sharpening factor\nand was chosen from a hyperparameter search in the interval [1.0, 8.0].\nFor erasing-based transformations, we select 3 potentially overlapping erasing rect-\nangles with a size ranging from 0.01nm to 0.2nm and aspect ratio ranging from 0.5 to 2.\nThe trochvision implementation selects the corresponding parameters uniformly within\nthese ranges. Each rectangle is then applied with p = 0.75. The applied rectangles are\nthen ﬁlled with zeros (erasing (black)) or with white noise (erasing (random)). All three\nparameters, the number of rectangles, the maximal size and the appliance probability\nwere hyperparameter optimized for the erasing (black) scenario over the sets {1, 3},\n[0.025nm, 0.4nm] and {0.5, 0.75}, respectively.\n3.4\nTraining Protocol\nFor the data augmentation experiments we calculated all triplet combinations (d, m, t).\nWe implemented equal training settings to ensure maximum comparability with the\nbaseline models, i.e., optimizer, scheduler, learning rates, weight initialization, and ﬁxed\ndata loading. The only diﬀerence in the pipeline was using the investigated transformation\nt in the default transformation sequence.\nTo implement one training cycle, we followed Chollet’s recommendations [39] on ﬁne-\ntuning. First, we trained reinitialized layers for a warm-up period before updating all\nnetwork parameters’ to prevent the negative eﬀect of a possible large error signal on\npreviously learned features. We implemented this by training 10 epochs with a 1-cycle\nlearning rate scheduler [40], a LAMB optimizer [41] and a L2 loss regularization. Then,\nfollowed by another training over 50 epochs, all weights are updated in the network\nusing the same optimizer and scheduler policies. We always selected the last model of\nthe training process for the evaluation to ensure that the data was seen equally.\nWhen loading a sample x, we apply a ﬁxed transformation sequence. We perform this\nsequence to avoid artiﬁcial padded black borders in arbitrary rotation transformations.\nHowever, to maintain comparability, the sequence is always used. (1) Resize the x to\n√\n2∗nm with nm being the network input size. (2) Apply the investigated transformation\nt to the resized sample with probability p. (3) Center crop the transformed sample with\nthe size nm. (4) Normalize the ﬁnal input by the mean and standard deviation of each\ncolor channel. Exceptions to this sequence are erasing-based transformations, which are\napplied at the end of the sequence to not corrupt the normalization process.\n43\n\nWe performed a hyperparameter optimization for the training settings of each baseline\nmodel and dataset (d, m) using grid search. These ﬁnal settings are reported in table 2\nand used for the training of each triplet (d, m, t).\nTable 2. Final parameter used for the training of each triplet (d, m, t) where the parameters\nare optimized for the baseline model of every pair (d, m) ∈D × M.\nBACH\nMIDOG\nParameter\nVGG\nInception\nDensenet\nVGG\nInception\nDensenet\nEpochs (total)\n60\n60\n60\n60\n60\n60\nEpochs (warm-up)\n10\n10\n10\n10\n10\n10\nLearning Rates\n0.0016\n0.0016\n0.0016\n0.0064\n0.0032\n0.0032\nWeight Decay\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\nBatch Size\n64\n64\n64\n64\n64\n64\n3.5\nEvaluation Protocol\nTo account for the randomness during training given due to weight initialization of the\nclassiﬁcation head, dropout layer and random application of augmentations, that results\nin a spread of experiment outcomes, we conducted 10 experiment runs per conﬁguration\n(d, m, t). The 10 experiments only diﬀer by a random seed given at the start of training.\nThis allows gathering a statistic, which makes a more accurate statement about the\neﬀectiveness of an augmentation and also enables an expressive comparison between\nthem. As metric, we use the accuracy value on the plain test set for testing the model.\n4\nResults and Discussion\nWe summarize our ﬁndings for each triplet in a boxplot.4 We interpret an augmentation\ntechnique as eﬀective if its median is above the mean of the baseline model (continuous\nhorizontal line) and their overlap in the interquartile range (IQR = Q3 - Q1) is minimal.\nWe show the results for each dataset in Figure 1 and Figure 2 grouped by the augmenta-\ntion strategies and the network architecture. For a clean plot appearance, a few extreme\noutliers were discarded from the visualization.\nFor the BACH dataset, we observe an expected behavior of the baseline models. With\nthe rising complexity of the network architecture, the accuracy increases slightly. How-\never, comparing the baseline with models trained using data augmentations shows that\nthe geometric transformations stand out. Since the histopathological data is rotation\ninvariant and the importance of morphological structures, we anticipated such results.\nFurthermore, erasing-based augmentations also provide a beneﬁcial eﬀect. On the other\nhand, ﬁlter-based and color-based transformations do not seem to have a positive in-\nﬂuence, nor do they harm the model’s performance for the chosen hyperparameters.\nEspecially regarding the color-based augmentations methods, that ﬁnding was surpris-\ning since we presumed the manipulation of the color space to be a critical factor in the\ncontext of histopathological data. Our hypothesis that the color distortions caused by\n4 Interactive versions of the charts and more details of the results can be viewed at https://cbmi-\nhtw.github.io/Data-Augmentation-Histology-Website/\n44\n\nFig. 1. Results for the BACH dataset. We show the individual mean accuracies on the test set\nas well as resulting boxplots for the 10 runs for the baseline conﬁguration and the individual\ntransformations for all three network architectures.\nFig. 2. Results for the MIDOG dataset. We show the individual mean accuracies on the test\nset as well as resulting boxplots for the 10 runs for the baseline conﬁguration and the individual\ntransformations for all three network architectures.\n45\n\nthe hyper-optimized transformations were too close to the identity could not be veriﬁed.\nAdditional experiments with artiﬁcial parameter values for the transformations to change\nthe input drastically lead to similar results. The results for the MIDOG dataset are essen-\ntially alike. Geometric-based augmentations raise the performance, whereas color-based\nand ﬁlter-based transformations have no signiﬁcant eﬀect. Indeed, the scatter and the\noverlap with the baseline is so tremendous that no positive eﬀect can be ascribed to these\ntransformations. Erasing-based augmentation harms the model performance signiﬁcantly.\nWe assume that the augmentation occasionally covers the cell nucleus, which is essential\nfor distinguishing between mitosis and non-mitosis. However, we did not investigate this\nfurther and leave this for future work.\nAdditionally, we have to mention that it took much computational eﬀort to get the\nresults reliable and robust due to intensive hyperparameter optimization. We observed a\nstrong sensitivity towards the hyperparameters of the learning process. Tiny changes in\nthe training settings, e.g., learning rate or batch size, let the beneﬁts of data augmentation\nvanish in noise. Therefore, we advise using augmentation techniques in combination with\nwell-optimized training hyperparameters to proﬁt from the method. We even suggest\ntuning the transformation parameters for an optimal result.\n5\nConclusion\nWe examined basic data augmentation techniques frequently used in deep learning classi-\nﬁer training pipelines on two histopathological data. Overall geometric-based techniques\nincrease the model performance on such datasets. However, surprisingly, color-based aug-\nmentations do not have the expected impact and are costly due to the required parameter\noptimization. Next to supervised learning settings, we expect this work to improve con-\ntemporary semi-supervised learning methods, e.g., contrastive learning, and assume such\nmethods will considerably impact the training of deep learning models in the histopatho-\nlogical domain.\nAcknowledgments\nThe authors thank Dr. Tim-Rasmus Kiehl, M.D. (Charit´e, Institute of Pathology) for\nhelpful comments and acknowledge the ﬁnancial support by the Federal Ministry of Edu-\ncation and Research of Germany (BMBF) in the project deep.HEALTH (13FH770IX6).\nReferences\n1. Miyato, T., Maeda, S., Koyama, M., Nakae, K., Ishii, S.: Distributional smoothing by virtual\nadversarial examples. In: 4th International Conference on Learning Representations, ICLR\n2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. (2016)\n2. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep convo-\nlutional generative adversarial networks. arXiv preprint arXiv:1511.06434 (2015)\n3. Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation for deep learning.\nJournal of Big Data 6(1) (2019) 1–48\n4. O’Gara, S., McGuinness, K.: Comparing data augmentation strategies for deep image clas-\nsiﬁcation. In: Irish Machine Vision and Image Processing Conference (IMVIP). (2019)\n5. Madabhushi, A., Lee, G.: Image analysis and machine learning in digital pathology: Chal-\nlenges and opportunities. Medical image analysis 33 (2016) 170–175\n6. Niazi, M.K.K., Parwani, A.V., Gurcan, M.N.: Digital pathology and artiﬁcial intelligence.\nThe lancet oncology 20(5) (2019) e253–e261\n46\n\n7. Ertosun, M.G., Rubin, D.L.: Automated grading of gliomas using deep learning in digital\npathology images: A modular approach with ensemble of convolutional neural networks.\nIn: AMIA Annual Symposium Proceedings. Volume 2015., American Medical Informatics\nAssociation (2015) 1899\n8. Li, C., Wang, X., Liu, W., Latecki, L.J.: Deepmitosis: Mitosis detection via deep detection,\nveriﬁcation and segmentation networks. Medical image analysis 45 (2018) 121–133\n9. Rouhi, R., Jafari, M., Kasaei, S., Keshavarzian, P.: Benign and malignant breast tumors\nclassiﬁcation based on region growing and cnn segmentation. Expert Systems with Appli-\ncations 42(3) (2015) 990–1002\n10. Tomczak, K., Czerwi´nska, P., Wiznerowicz, M.: The cancer genome atlas (tcga): an immea-\nsurable source of knowledge. Contemporary oncology 19(1A) (2015) A68\n11. Buslaev, A., Iglovikov, V.I., Khvedchenya, E., Parinov, A., Druzhinin, M., Kalinin, A.A.:\nAlbumentations: Fast and ﬂexible image augmentations. Information 11(2) (2020)\n12. Casado-Garc´ıa, A., Dom´ınguez, C., Garc´ıa-Dom´ınguez, M., Heras, J., In´es, A., Mata, E.,\nPascual, V.: Clodsa: a tool for augmentation in classiﬁcation, localization, detection, se-\nmantic segmentation and instance segmentation tasks. BMC bioinformatics 20(1) (June\n2019) 323\n13. Bloice, M.D., Roth, P.M., Holzinger, A.: Biomedical image augmentation using augmentor.\nBioinformatics 35(21) (2019) 4522–4524\n14. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning aug-\nmentation strategies from data. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. (2019) 113–123\n15. Lim, S., Kim, I., Kim, T., Kim, C., Kim, S.:\nFast autoaugment.\nAdvances in Neural\nInformation Processing Systems 32 (2019) 6665–6675\n16. Perez, L., Wang, J.: The eﬀectiveness of data augmentation in image classiﬁcation using\ndeep learning. arXiv preprint arXiv:1712.04621 (2017)\n17. Chlap, P., Min, H., Vandenberg, N., Dowling, J., Holloway, L., Haworth, A.:\nA review\nof medical image data augmentation techniques for deep learning applications. Journal of\nMedical Imaging and Radiation Oncology (2021)\n18. Cho, H., Lim, S., Choi, G., Min, H.:\nNeural stain-style transfer learning using gan for\nhistopathological images. arXiv preprint arXiv:1710.08543 (2017)\n19. Zanjani, F.G., Zinger, S., Bejnordi, B.E., van der Laak, J.A., de With, P.H.: Stain normal-\nization of histopathology images using generative adversarial networks. In: 2018 IEEE 15th\nInternational Symposium on Biomedical Imaging (ISBI 2018), IEEE (2018) 573–577\n20. Mahapatra, D., Bozorgtabar, B., Thiran, J.P., Shao, L.: Structure preserving stain normal-\nization of histopathology images using self supervised semantic guidance. In: International\nConference on Medical Image Computing and Computer-Assisted Intervention, Springer\n(2020) 309–319\n21. Mercan, C., Mooij, G., Tellez, D., Lotz, J., Weiss, N., van Gerven, M., Ciompi, F.: Virtual\nstaining for mitosis detection in breast histopathology. In: 2020 IEEE 17th International\nSymposium on Biomedical Imaging (ISBI), IEEE (2020) 1770–1774\n22. Xue, Y., Zhou, Q., Ye, J., Long, L.R., Antani, S., Cornwell, C., Xue, Z., Huang, X.: Synthetic\naugmentation and feature-based ﬁltering for improved cervical histopathology image clas-\nsiﬁcation. In: International conference on medical image computing and computer-assisted\nintervention, Springer (2019) 387–396\n23. Wei, J., Suriawinata, A., Vaickus, L., Ren, B., Liu, X., Wei, J., Hassanpour, S.: Generative\nimage translation for data augmentation in colorectal histopathology images. Proceedings\nof machine learning research 116 (2019) 10\n24. Quiros, A.C., Murray-Smith, R., Yuan, K.: Pathologygan: Learning deep representations\nof cancer tissue. In: Proceedings of the Third Conference on Medical Imaging with Deep\nLearning. (2020)\n25. Roy, K., Banik, D., Bhattacharjee, D., Nasipuri, M.: Patch-based system for classiﬁcation of\nbreast histology images using deep learning. Computerized Medical Imaging and Graphics\n71 (2019) 90–103\n47\n\n26. Lafarge, M.W., Pluim, J.P., Eppenhof, K.A., Moeskops, P., Veta, M.: Domain-adversarial\nneural networks to address the appearance variability of histopathology images. In: Deep\nlearning in medical image analysis and multimodal learning for clinical decision support.\nSpringer (2017) 83–91\n27. Ciga, O., Martel, A.L., Xu, T.: Self supervised contrastive learning for digital histopathology.\narXiv preprint arXiv:2011.13971 (2020)\n28. Azizi, S., Mustafa, B., Ryan, F., Beaver, Z., Freyberg, J., Deaton, J., Loh, A., Karthike-\nsalingam, A., Kornblith, S., Chen, T., et al.: Big self-supervised models advance medical\nimage classiﬁcation. arXiv preprint arXiv:2101.05224 (2021)\n29. Liu, Q., Louis, P.C., Lu, Y., Jha, A., Zhao, M., Deng, R., Yao, T., Roland, J.T., Yang, H.,\nZhao, S., et al.: Simtriplet: Simple triplet representation learning with a single gpu. arXiv\npreprint arXiv:2103.05585 (2021)\n30. Tellez, D., Litjens, G., B´andi, P., Bulten, W., Bokhorst, J.M., Ciompi, F., van der Laak, J.:\nQuantifying the eﬀects of data augmentation and stain color normalization in convolutional\nneural networks for computational pathology. Medical image analysis 58 (2019) 101544\n31. Karimi, D., Nir, G., Fazli, L., Black, P.C., Goldenberg, L., Salcudean, S.E.: Deep learning-\nbased gleason grading of prostate cancer from histopathology images—role of multiscale\ndecision aggregation and data augmentation. IEEE journal of biomedical and health infor-\nmatics 24(5) (2019) 1413–1426\n32. Fabian, Z., Heckel, R., Soltanolkotabi, M.:\nData augmentation for deep learning based\naccelerated mri reconstruction with limited data. In: International Conference on Machine\nLearning, PMLR (2021) 3057–3067\n33. Tran, N.T., Tran, V.H., Nguyen, N.B., Nguyen, T.K., Cheung, N.M.: On data augmentation\nfor gan training. IEEE Transactions on Image Processing 30 (2021) 1882–1897\n34. Nalepa, J., Marcinkiewicz, M., Kawulok, M.: Data augmentation for brain-tumor segmen-\ntation: A review. Front Comput Neurosci 13 (12 2019) 83\n35. Aubreville, M., Bertram, C., Veta, M., Klopﬂeisch, R., Stathonikos, N., Breininger, K., ter\nHoeve, N., Ciompi, F., Maier, A.: Mitosis domain generalization challenge (March 2021)\n36. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recog-\nnition. In Bengio, Y., LeCun, Y., eds.: 3rd International Conference on Learning Represen-\ntations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\n(2015)\n37. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the inception ar-\nchitecture for computer vision. In: Proceedings of the IEEE conference on computer vision\nand pattern recognition. (2016) 2818–2826\n38. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolu-\ntional networks. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition. (2017) 4700–4708\n39. Chollet, F.: Deep Learning with Python. Manning (November 2017)\n40. Smith, L.N., Topin, N.: Super-convergence: Very fast training of neural networks using large\nlearning rates. In: Artiﬁcial Intelligence and Machine Learning for Multi-Domain Operations\nApplications. Volume 11006., International Society for Optics and Photonics (2019) 1100612\n41. You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J.,\nKeutzer, K., Hsieh, C.J.: Large batch optimization for deep learning: Training bert in 76\nminutes. In: International Conference on Learning Representations. (2020)\n48\n\nOnline extraction of functional data from video recordings of \ngut movements using AI features\nPervaiz Khan1, Manuela Gries2, Ahmed Sheraz1, Steven Schulte2, Anne Christmann2, Marko \nBaller2, Karl-Herbert Schäfer2, Andreas Dengel1\n1 DFKI (German Research Centre for Artificial Intelligence, Kaiserslautern Germany),  \n2 University of Applied Sciences Kaiserslautern, Campus Zweibrücken, Germany\nAbstract. The  gut  is  an  often  underestimated  organ  which  contributes  significantly \nto  our  health condition 1 .  It  is  also  one  of  the  main  entry  gates  for  pathogens,  toxins \nor  drugs,  thus influencing the whole body. The gut harbors an intrinsic and autonomeously \nworking nervous system,  the  socalled  enteric  nervous  system  (ENS)  that  regulates \nblood  flow,  resorption, mucosal barrier function and gastrointestinal motility. The \ngastrointestinal motility is an appropriate readout to evaluate the health status of the whole \norgan, since all kind of compromising or challenging agents, either orally or systemically \nadministered  will  affect  the  ENS 2,3,4 .  Moreover  the  gut  can  be  compromised  in  \nvarious diseases.  During  the  last  years,  the  role  of  the  gut  in  neurodegenerative  \nprocesses  and diseases came more and more into focus and it could i.e. be demonstrated \nthat gut motility changes  also  in  models  of  Alzheimer  and  Parkinsons  disease  (PD) \n5,6 .  In  PD  patients, gastrointestinal problems often appear long before the disease is \ndiagnosed. In a recent study it could nicely be demonstrated that in very young (2 month) \nmice that overexpress the alpha synuclein  pathogenic  peptide,  the  alteration  of  colonic  \nmotility  was  related  to  molecular alterations of the ENS 7 . While the use of muscle strips \nwith the included ENS to investigate isometric contractions is a rather artificial approach, \nusing intact gut segments with intact mucous layers and even  if necessary, mesenterial \nperfusion, can deliver much more in vivo equivalent data to analyse gut activity under the \ninfluence of external or systemic factors.  \nFigure 1 Heat maps that represent the movement of the colon in a PD model (right) compred to the wild \ntype mouse colon. While the WT shows a continuous series of propagating contractions, the PD gut is rather \nslow.\nGut segments from the  colon of adult mice were fixed in a tubing system and placed in an \norgan bath under continuous luminal perfusion. The perfusion was performed using an \nefflux resistance of 3cm H 2 O to induce gut movements. The organ bath chamber was \nequipped with a frontal and bottom glass plate, so that a continuous video recording from \n49\n\nfront and bottom could  be  realized.  The  gut  was  perfused  and  superfused  with  a  37°C \nwarm  Tyrode  buffer solution at an pH of 7,4. The buffer was oxygenized prior to perfusion \nto obtain a sufficient oxygen saturation. The gut was allowed to equilibrate for 10 min after \nfixaton in the organ bath  under  physiological  conditions.  Then  the  experiment  was  \nstarted.  Two  cameras, positioned either in front or at the bottom window of the organ bath \nchamber, were started at the same time  and the spontaneous activity ot the gut recorde for \n10 min. Then the gut was challenged with individual drug compounds to stimulate its \nactivity for another 10 min. At the end of the experiment, a maximal stimulation was \nachieved using Acetylecholine. To analyze the movement of the gut, initially 33 virtual dots \npositions are selected on the boundary of the gut manually.  Then, in each frame of the \nvideo, vertical movement of the dots is tracked using distance transforms while considering \nthe fixed horizontal position. Then, the distance of every dot is measured to the \ncorresponding dot in the previous frame. In this way,  gut  movement  is  tracked  in  the \ncomplete  video.  The  heat  map  of  the  movement  is presented in the Figure below:\nFigure 2 Heat map presents the vertical movement of guts in a video.\nThe approach demonstrates the use of AI algorithms to extract valuable and quantifiable \ndata from gut movements online in real time. This allows to investigate the impact of either \ndrugs, toxins, nutritional components or even diseases upon gastrointestinal motility. The \nmethod delivers  timely  and  spatial  resolution  of  gut  movements,  so  that  a  detailed \nanalysis  of functional distinct entities of the gut (i.e. proximal and distal colon) can be \nevaluated and compared.\nReferences\n1. Disorders Niesler B, Kuerten S, Demir IE, Schäfer KH.of the enteric nervous system - a\nholistic view. Nat Rev Gastroenterol Hepatol. 2021 Jan 29. doi: 10.1038/s41575-020-\n00385-2.\n2. Schreiber D, Klotz M, Laures K, Clasohm J, Bischof M, Schäfer KH.The mesenterially\nperfused rat small intestine: A versatile approach for pharmacological testings. Ann Anat.\n2014 May;196(2-3):158-66\n3. Schreiber D, Jost V, Bischof M, Seebach K, Lammers WJ, Douglas R, Schäfer KH.\nMotility patterns of ex vivo intestine segments depend on perfusion mode. World J\nGastroenterol. 2014 Dec 28;20(48):18216-27. doi: 10.3748/wjg.v20.i48.18216.\n50\n\n4. Subramanya SB, Stephen B, Nair SS, Schäfer KH, Lammers WJ.) Effect of Ethanol\nExposure on Slow Wave Activity and Smooth Muscle, Contraction in the Rat Small\nIntestine. Dig Dis Sci. 2015 Dec;60(12):3579-89. doi: 10.1007/s10620-015-3813-7\n5. Semar S, Klotz M, Letiembre M, Van Ginneken C, Braun A, Jost V, Bischof M, Lammers\nWJ, Liu Y, Fassbender K, Wyss-Coray T, Kirchhoff F, Schäfer KH. Changes of the\nenteric nervous system in Amyloid Precursor Protein transgenic mice correlate with\ndisease progression. Journal of Alzheimers Disease, 2013, 36(1):7-20. doi: 10.3233/JAD-\n1205116\n6. Schäfer KH, Christmann A, Gries M. Can we trust the gut? The role of the intestine in\nneurodegeneration. J Physiol. 2020 Oct;598(19):4141-4142. doi: 10.1113/JP280336.\nEpub 2020 Aug 20.PMID: 32706398\n7. Gries M, Christmann A,Schulte S, Weyland M, Rommel S, Martin M,  Baller M, Röth R,\nSchmitteckert S, Unger M, Liu Y,  Sommer F, Mühlhaus T,  Schroda M, Timmermans\nJp, Pintelon I, Rappold Ga, Britschgi M,  Lashuel H, Menger Md, Laschke Mw, Niesler\nB, Schäfer Kh. Parkinson mice show functional and molecular changes in the gut long\nbefore motoric disease onset. Molecular Neurodegeneration Juni 2021\n51\n\nAn artificial neural network-based toolbox for the orphological \nanalysis of red blood cells in flow \nMarcelle Lopes, Stephan Quint \nCysmic GmbH, Germany \nmarcelle.lopes@cysmic.de, stephan.quint@cysmic.de \nAbstract. We present a toolbox that combines image processing techniques with artificial \nintelligence to enable single-cell the detection and characterization of red blood cells \nobserved in microfluidic flow. In healthy subjects, red blood cells show a smooth transition \nbetween an axis-symmetric (“croissant”) and non-axis-symmetric (“slipper”) shape \ndepending on their flow velocity. However, in subjects with blood diseases this shape \ndynamics is disturbed and results in deviating blood flow properties. Current diagnostic \nmethods rely on the identification of genetic mutations in addition to functional tests, \nincluding the manual evaluation of red blood cells in stasis. Although the latter technique \nis considered a clinical standard, it is not sufficient to discriminate between blood diseases \nand their severities. The automation of the characterization of images of single red blood \ncells in flow is an unbiased technique that could set new standards in blood disease clinical \ndiagnostics. Considering the large variety of red blood cells shape deformations, we \ndeveloped a semi-supervised neural network for a reliable and reproducible cell shape \nevaluation. By arranging ideal shapes as cornerstones of the training data set, cell shape \ntransitions are self-learned during the training process of the neural network. This highly \nreduces the amount of required training data as well as the need for a manual pre-\nclassification. Our approach , in addition to avoiding errors due to manually selected \ntraining data (supervised training), also enables the definition of custom thresholds and \nmetrics for further discrimination and statistical analysis. The technique will be tested on \nblood of patients with inherited rare anemias, e.g., sickle-cell disease, as well as on \ntransfusion blood and chronic and infectious diseases, such as COVID-19. \nKeywords: Artificial Neural Networks, Variational Autoencoders, Red Blood cells, Blood \ndisease clinical diagnostics \n52\n\nComparing a deterministic and a Bayesian\nclassiﬁcation neural network for chest diseases in\nradiological images\nJonas Nolde and Ruxandra Lasowski\nHochschule Furtwangen University\njonas.nolde@hs-furtwangen.de,\nruxandra.lasowski@hs-furtwangen.de\nAbstract. A common mantra for automated decision systems is that a system\nshould know when it doesn’t know. Bayesian neural networks are designed to\ncapture uncertainties over the network weights and in theory, they perform better\npredictions and output uncertainties. To this end, we compare in this paper a\ndeterministic neural network and a Bayesian neural network for the classiﬁcation\nof chest diseases in radiological images. We use the ChestX-ray14 data set [1]\ninvolving 14 respiratory diseases like pneumonia and atelectasis. We found that the\ndeterministic network similar to CheXNet [2] outperformed the Bayesian version\nin this task, whereas, employed on the more simplistic MNIST dataset it did not.\nOur experiments suggest that there is a gap between theory and practical use of\nBNNs for very deep networks and real clinical data.\nKeywords: medicine, radiology, classiﬁcation, deterministic neural networks,\nBayesian neural networks\n1\nIntroduction\nIn 2016 almost 2.38 million people died from lower respiratory infections worldwide.\nThis was the sixth leading cause of mortality for all ages that year [3]. For the last three\ndecades, pneumonia was the most common cause of death for children under 5 years\n[4]. While the treatment of a diagnosed pneumonia patient can be done eﬃciently with\nlow-cost, low-tech medication and care [5], the detection of lower respiratory infections\nleaves room for improvement. The diagnosis of such diseases with the help of chest X-rays\nis very eﬀective and currently the best available method [6]. Unfortunately, this task is\nchallenging and requires expert radiologists which are rare in impoverished regions.\nPast work about the detection and classiﬁcation of lung diseases on chest X-rays with\nconvolutional neural networks (CNNs) could achieve promising results that match or\nexceed the performance of radiologists [2]. These deterministic neural networks, albeit\nbeing right most of the time, won’t tell you how certain they are about their decisions.\nIn critical applications, where the cost of error is high, an indication of conﬁdence can\nbe extremely valuable, especially in uncertain edge cases.\nWhile deterministic neural networks contain a speciﬁc set of weights, Bayesian neural\nnetworks (BNNs) assign probabilities to all possible sets of weights, allowing for un-\ncertainty quantiﬁcation. In this work, we will assess the applicability of Bayesian deep\nlearning in the ﬁeld of medical diagnosis. We, therefore, implement a version of the de-\nterministic neural network CheXNet [2] and a Bayesian version of CheXNet using recent\nadvances in Bayesian deep learning. We then evaluate and compare their performance\non the ChestX-ray14 data set [1]. Furthermore, we eﬃciently measure aleatoric and epis-\ntemic uncertainties in the Bayesian model’s predictions.\n53\n\n2\nRelated Work\n2.1\nBayes’ Theorem\nBayesian deep learning utilizes Bayes’ Theorem to calculate conditional probabilities. In\nthe context of deep learning, Bayes’ Theorem can be rewritten as\np(w|D) = p(D|w) p(w)\np(D)\n,\n(1)\nwith a neural network’s parameters w (the hypothesis) and data D (the evidence).\nBayesian deep learning aims to calculate the posterior distribution p(w|D), which ”cap-\ntures the set of plausible model parameters, given the data” [7]. This is done by mul-\ntiplying the likelihood p(D|w) of data D occurring given parameters w with the prior\ndistribution p(w) and normalizing it by the data distribution p(D).\nWe can implement a Bayesian neural network by replacing a ”deterministic network’s\nweight parameters with distributions over these parameters, and instead of optimising\nthe network weights directly we average over all possible weights (referred to as marginal-\nisation)” [7]. With Bayesian inference we can calculate the posterior distribution p(w|D)\nand predict probabilities y∗given new data x∗:\np(y∗|x∗, D) =\n\u0004\np(y∗|x∗, w) p(w|D) dw .\n(2)\n2.2\nVariational Inference\nHinton and Van Camp initially proposed variational inference for neural networks in\n1993 [8] as an alternative to methods involving expensive Monte Carlo sampling. In\n2011, Graves [9] published an improved approach, suitable for more complex neural net-\nworks. Variational inference solves the intractability of the integral over the true posterior\ndistribution p in eq. 2 by integrating over a simpliﬁed posterior distribution q with varia-\ntional parameters θ instead. This variational posterior qθ is an approximation of the true\nposterior p but is easier to sample from. In most cases, a Gaussian distribution N(μ, σ2)\nwith parameters θ = (μ, σ2) for the mean and the variance respectively is used. Graves’\napproach optimizes the posterior approximation qθ(w|D) ≈p(w|D) by minimizing the\nvariational free energy F, also referred to as the negative variational lower bound or\nnegative evidence lower bound (ELBO). For deep learning, F can be reinterpreted as\nminimum description length cost function [9]:\nF(D, θ) = KL[qθ(w)||p(w)] −Eqθ(w)[log p(D|w)],\n(3)\nwhere KL[qθ(w)||p(w)] is the Kullback-Leibler (KL) divergence between both distri-\nbutions. It consists of a data-dependent part (the likelihood cost or error loss) and a\nprior-dependent part (the complexity loss). The function embodies a trade-oﬀbetween\nsatisfying the complexity of the data D and the simplicity of the prior p(w) [10].\nGraves’ approach for a tractable approximation of the Bayesian neural network’s pos-\nterior distribution was the cornerstone for more eﬃcient and stable estimation methods\nlike Stochastic Gradient Variational Bayes (SGVB) [11], the local reparameterization\ntrick [12] and the ﬂipout estimator [13].\n54\n\n2.3\nUncertainty Estimation in Bayesian Deep Learning\nKendall and Gal [7] describe the two types of uncertainty in the context of Bayesian deep\nlearning as follows:\nEpistemic uncertainty is often referred to as model uncertainty, as it ”captures our\nignorance about which model generated our collected data” [7].\nAleatoric uncertainty, on the other hand, ”captures noise inherent in the observations”\n[7] and thus is often referred to as data uncertainty. This type of uncertainty can further\nbe categorized into homoscedastic uncertainty and heteroscedastic uncertainty. While\nhomoscedastic uncertainty stays constant for diﬀerent inputs, heteroscedastic uncertainty\nvaries from input to input, with some potentially having more noisy outputs than others.\nThey note that ”heteroscedastic uncertainty is especially important for computer vision\napplications” [7].\nThe paper concludes, that measuring both types of uncertainty is crucial for the safety\nand reliability of models. Aleatoric uncertainty is important for ”large data situations,\nwhere epistemic uncertainty is explained away” and ”real-time applications, because\nwe can form aleatoric models without expensive Monte Carlo samples” [7]. Epistemic\nuncertainty is important for ”safety-critical applications, because epistemic uncertainty\nis required to understand examples which are diﬀerent from training data” and ”small\ndatasets where the training data is sparse” [7].\nKendall and Gal [7] proposed a method to estimate both the aleatoric and the epis-\ntemic uncertainty, which was later reﬁned for classiﬁcation by Kwon et al. [14]:\n1\nT\nT\n\u0005\nt=1\ndiag(ˆpt) −ˆpt ˆpT\nt\n\u0006\n\u0007\b\n\t\naleatoric\n+ 1\nT\nT\n\u0005\nt=1\n(ˆpt −pt)(ˆpt −pt)T\n\u0006\n\u0007\b\n\t\nepistemic\n,\n(4)\nwith the predicted probability vector ˆpt = p( ˆwt) = Softmax{f ˆ\nwt(x∗)}, that is sampled\nT times, the diagonal matrix diag(ˆpt) with elements of vector ˆpt, and the mean predicted\nprobability p = \nT\nt=1 ˆpt/T. We later use the formula of 4 in our experiment to measure\nour model’s uncertainties.\n3\nMethods\n3.1\nFirst Tests with TensorFlow Probability and MNIST\nTo gain our ﬁrst practical experience in implementing a Bayesian convolutional neural\nnetwork for disease classiﬁcation on X-rays with TensorFlow 2.0 and its probabilistic\nprogramming library TensorFlow Probability, we ﬁrst looked at a simpler image clas-\nsiﬁcation task. We implemented the small LeNet-5 network [15]. It consists of only 7\nlayers (3 convolutional layers, 2 subsampling layers, 2 fully connected layers) making it\na relatively simple and small network in today’s time. The original LeNet-5 architecture\nis a deterministic neural network and does not use probabilistic methods. To make it\nBayesian we replaced the deterministic convolutional and fully connected layers from\nTensorFlow with the probabilistic layers from TensorFlow Probability. The model’s task\nis to classify the images of the MNIST data set [15] and tell which digit is depicted.\nThe data set contains 28×28 pixel small, grayscale images of a handwritten digit (0-9),\nnormalized in size and centered in the image and is split into 60,000 training samples and\n10,000 test samples. We trained the model in mini-batches with 128 normalized images\neach, where the Adam optimizer [16] minimizes the categorical cross-entropy loss.\n55\n\nResults and Conclusion After training for 75 epochs, the Bayesian model achieved a\nvalidation accuracy of 0.984, which was good enough for us to stop the training. We then\nperformed Monte Carlo sampling by asking the trained model to predict the labels of the\nsame unseen images 50 times. The resulting outputs were diﬀerent at each prediction as\nﬁgure 1(b) shows. For comparison, we trained a non-Bayesian, deterministic version of\nthe network. 10 epochs of training already yielded a training accuracy of 0.997. Figure\n1(a) shows the resulting test prediction plots. Note that a deterministic model always\noutputs the same values for the same input, requiring only one prediction per sample\nat inference. To show how the models perform on unusual data, we tested with ”fake”\nMNIST images from the notMNIST database, which contains images that look similar\nto those in the MNIST database but show letters instead of numbers. The resulting\nprediction plots can be seen in the last samples of ﬁgure 1.\n(a) Deterministic model\n(b) Bayesian model\nFig. 1: Predictions of the deterministic and the Bayesian model on the MNIST data set.\nConcluding, the ﬁrst thing to note is that training the Bayesian neural network takes\nconsiderably longer than training the deterministic one. Figure 1 shows that both models\ncorrectly classiﬁed the ﬁrst sample. However, the Bayesian model predicted wrong classes\nseveral times during the Monte Carlo inference on the second sample. Looking at the input\nimage, we can argue that the ”5” looks like a ”6” or an ”8” to some degree. Thus, having\nthe Bayesian model predict those digits a few times is a justiﬁable and possibly desirable\nsign of uncertainty. The third sample depicts the letter ”F” in a circle and is also inverted.\nHence, the sample neither belongs to a class that can be predicted by the model nor did\nthe model see comparable images during training. While the deterministic model was sure\nit saw the digit ”2” (ﬁgure 1(a)), the Bayesian model was highly uncertain in its decision\n(ﬁgure 1(b)). The plot shows that during Monte Carlo sampling the model predicted\ndiﬀerent classes every time, causing the average prediction to have a low, almost similar\nprobability for every class. This can be interpreted as an indication of high uncertainty\nand the model saying ”I don’t know”. In critical applications like medical diagnosis, this\nbehavior is highly desirable.\n56\n\n3.2\nCheXNet and Bayesian CheXNet\nThe main goal of this work is to show the advantages of Bayesian deep learning over\ntraditional, deterministic deep learning for medical image classiﬁcation. We compare\nperformances of the deterministic CheXNet [2] and our Bayesian neural network with a\nsimilar architecture.\nCheXNet achieves state-of-the-art results on all 14 diseases of the publicly available\nChestX-ray14 data set [1]. The data consists of 112,120 single grayscale image ﬁles and\nCSV ﬁles with metadata like the images’ disease labels and bounding boxes indicating the\nlocation of the disease. We pre-processed the images by down-scaling them to 224×224\npixels and normalizing the 8-bit pixel values (0 - 255) to ﬂoat values between −1 and 1.\nFinally, we split the data into train, validation, and test set containing 98,656 (93.5%),\n6,336 (6%), and 432 (0.5%) data points respectively.\nFig. 2: DenseNet121 architecture with Dense Blocks D and Transition Blocks T;\nSource: [17]\nTo replicate CheXNets model architecture, we used a copy of the DenseNet121 model\n(depicted in ﬁgure 2) coming with TensorFlow’s Keras API. We changed the input shape\nof the model to match the monochromatic 224×224 image matrices. Furthermore, we\nchanged the output layer to return 14 values and used the sigmoid non-linearity activation\nfunction instead of softmax, as we want the model to predict independent continuous\nvalues to indicate each disease. For our Bayesian version of CheXNet, we replace the\ndeterministic convolutional and fully connected layers with probabilistic layers from the\nTensorFlow Probability library.\nTraining Results We trained both models with mini-batches by minimizing the binary\ncross-entropy loss with the Adam optimizer. After 70 epochs, the deterministic model\nachieved a training AUC of 0.9006 and a validation AUC of 0.8582. We outperform the\noriginal CheXNet with an average per-class AUC score of 0.8414 by a small margin. Our\nBayesian model failed to achieve good performance just by introducing the probabilistic\nlayers. We stopped the training after 17 epochs as the validation loss started to increase\nwhile the AUC decreased to 0.5 which is the random baseline.\n3.3\nModel Improvement Approaches\nAs our ﬁrst results show, we couldn’t achieve good model performance in this task simply\nby using Bayesian layers. Although we could get additional beneﬁt from implementing\nuncertainty measures, a bad performing model isn’t practically useful in any real-world\napplication, let alone in disease detection. In the following, we will discuss and test several\napproaches we took to increase the model’s performance.\n57\n\nChoosing Hyperparameters In our ﬁrst approach, we searched for better hyper-\nparameters by training the neural network with diﬀerent handpicked sets of learning\nrates, mini-batch sizes, and optimization algorithms. The model that performed best was\ntrained with the Adagrad optimizer, a learning rate of 0.1, and a mini-batch size of 64.\nAn automated search for parameters with Hyperopt [18] led to similar results.\nDealing with Imbalanced Data As our data set is extremely imbalanced (with signiﬁ-\ncantly more negatives than positives), we introduce a weighted loss function that penalizes\nmisclassiﬁed positive samples more. This way we can nudge the model towards looking\nat positive training samples more carefully. This approach, which was also proposed in\nthe CheXNet paper, signiﬁcantly improved our model’s learning and performance on the\nF1-score and the AUC value.\nInitialization with Pre-trained Weights The deterministic CheXNet is initialized\nwith weight parameters pre-trained on the ImageNet data set. In an attempt to make use\nof parameter transfer learning in our Bayesian model, we initialize the model’s priors with\nnormal distributions with variance 1, mean-shifted towards the single-point parameter\nvalues from the pre-trained weights. After training for a while, we concluded that the\ninitialization with weights pre-trained on ImageNet, neither improved model performance\nnor sped up the training.\n4\nResults and Conclusion\nTest set\nAUC F1-score F2-score Epistemic Aleatoric\nDeterministic\nFull\n0.8339 0.1444\n0.1019\n-\n-\n1\n0.7552 0.2858\n0.2000\n-\n-\n2\n0.6940 0.2858\n0.2000\n-\n-\n3\n0.9502 0.0000\n0.0000\n-\n-\n4\n0.8523 0.3333\n0.2778\n-\n-\n5\n0.9091 0.4000\n0.2941\n-\n-\n6\n0.7614 0.0000\n0.0000\n-\n-\nBayesian\nFull\n0.6579 0.1298\n0.2551\n-\n-\n1\n0.5378 0.1176\n0.1923\n0.0111\n0.2098\n2\n0.4544 0.1176\n0.1923\n0.0143\n0.2088\n3\n0.4851 0.0571\n0.1135\n0.0033\n0.2149\n4\n0.6098 0.0851\n0.1695\n0.0076\n0.2129\n5\n0.5739 0.0976\n0.1888\n0.0109\n0.2080\n6\n0.6686 0.1499\n0.2884\n0.0037\n0.2166\nTable 1: Test results of the deterministic and the Bayesian model on each test set.\nWe assessed the deterministic and the Bayesian models’ performance on the test set\nand additional samples with Gaussian noise. Our deterministic model achieved an AUC\n58\n\nof 0.8339, similar to the 0.8414 stated in the original CheXNet paper. This veriﬁes that\nour implementation of the model and the rest of our deep learning pipeline work as\nexpected. The Bayesian model achieved a lower AUC of 0.6579, as well as lower F1-,\nand F2-scores. We measured mean epistemic uncertainties (”model uncertainty” [7]) of\n0, 0085 and mean aleatoric uncertainties (”data uncertainty” [7]) of 0, 2118.\n(a) Deterministic model\n(b) Bayesian model\nFig. 3: The models’ predictions for a test sample.\n(a) Ground Truth\n(b) Deterministic model\n(c) Bayesian model\nFig. 4: The models’ CAMs for class 3 of a test sample and the actual location of the\ndisease.\nFigure 3 shows an example of the models’ predicted probabilities for a test sample\nwith true labels 2 and 3. The deterministic model predicted classes 2 and 3 with the\nthreshold of 0.5, while the Bayesian model’s averaged predictions of the 50 predictions\nsampled show all classes to be true. The generated class activation maps (CAMs) of\nﬁgure 4 show that both models didn’t look at the right location for their prediction of\nclass 3.\nWe interpret the much higher aleatoric uncertainty as a result of the nature of ra-\ndiological images, which can also contain pacemakers and/or other patient-speciﬁc aids\nand the low resolution of the images that were fed into the network. The epistemic\nuncertainty suggests that the prior for the model should be adjusted. The sampled pos-\nterior probabilities range between 0.6 and 0.8 for each class on most of the test samples,\nwhich could imply that the model’s weight distributions were initialized with a too high\nvariance that couldn’t be reduced during training. So far, state-of-the-art results with\n59\n\nBayesian neural networks were achieved on simplistic and carefully curated data sets like\nMNIST and CIFAR-10 moderate deep networks [19]. Our experiments with radiological\nimages and very deep networks didn’t achieve state-of-the-art results. This suggests that\nthe complexity of the data, the model size, and/or the initialized variance are the most\nimportant factors that can be further analyzed.\nReferences\n1. Summers, R.:\nNih chest x-ray dataset of 14 common thorax disease categories.\nhttps://nihcc.app.box.com/v/ChestXray-NIHCC/ﬁle/220660789610 (2017) Accessed: 2020-\n07-27.\n2. Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D.Y., Bagul, A., Lan-\nglotz, C., Shpanskaya, K.S., Lungren, M.P., Ng, A.Y.: Chexnet: Radiologist-level pneumonia\ndetection on chest x-rays with deep learning. CoRR abs/1711.05225 (2017)\n3. Troeger, C., Blacker, B., Khalil, I.A., Rao, P.C., Cao, J., Zimsen, S.R., Albertson, S.B.,\nDeshpande, A., Farag, T., Abebe, Z., et al.: Estimates of the global, regional, and national\nmorbidity, mortality, and aetiologies of lower respiratory infections in 195 countries, 1990–\n2016: a systematic analysis for the global burden of disease study 2016. The Lancet infectious\ndiseases 18(11) (2018) 1191–1210\n4. Max Roser, H.R., Dadonaite, B.: Child and infant mortality. Our World in Data (2013)\n5. World Health Organization (WHO):\nPneumonia.\nhttps://www.who.int/news-room/fact-\nsheets/detail/pneumonia (2019) Accessed: 2020-07-29.\n6. Kesselman, A., Soroosh, G., Mollura, D.J., Abbey-Mensah, G., Borgstede, J., Bulas, D.,\nCarberry, G., Canter, D., Ebrahim, F., Escalon, J., et al.:\n2015 rad-aid conference on\ninternational radiology for developing countries: the evolving global radiology landscape.\nJournal of the American College of Radiology 13(9) (2016) 1139–1144\n7. Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for computer\nvision? In: Advances in neural information processing systems. (2017) 5574–5584\n8. Hinton, G.E., Van Camp, D.: Keeping the neural networks simple by minimizing the descrip-\ntion length of the weights. In: Proceedings of the sixth annual conference on Computational\nlearning theory. (1993) 5–13\n9. Graves, A.:\nPractical variational inference for neural networks.\nIn: Advances in neural\ninformation processing systems. (2011) 2348–2356\n10. Blundell, C., Cornebise, J., Kavukcuoglu, K., Wierstra, D.: Weight uncertainty in neural\nnetworks (2015)\n11. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114\n(2013)\n12. Kingma, D.P., Salimans, T., Welling, M.: Variational dropout and the local reparameteri-\nzation trick. In: Advances in neural information processing systems. (2015) 2575–2583\n13. Wen, Y., Vicol, P., Ba, J., Tran, D., Grosse, R.:\nFlipout: Eﬃcient pseudo-independent\nweight perturbations on mini-batches. arXiv preprint arXiv:1803.04386 (2018)\n14. Kwon, Y., Won, J.H., Kim, B.J., Paik, M.C.: Uncertainty quantiﬁcation using bayesian\nneural networks in classiﬁcation: Application to ischemic stroke lesion segmentation. (2018)\n15. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE 86(11) (1998) 2278–2324\n16. Kingma, D.P., Ba, J.:\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980 (2014)\n17. Ruiz,\nP.:\nUnderstanding\nand\nvisualizing\ndensenets.\nhttps://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a\n(2018) Accessed: 2020-08-18.\n18. Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. The Journal of\nMachine Learning Research 13(1) (2012) 281–305\n19. Shridhar, K., Laumann, F., Liwicki, M.: A comprehensive guide to bayesian convolutional\nneural network with variational inference. CoRR abs/1901.02731 (2019)\n60\n\nGaussian Process Inspired Neural Networks for\nSpectral Unmixing Dataset Augmentation\nJohannes Anastasiadis and Michael Heizmann\nInstitute of Industrial Information Technology (IIIT),\nKarlsruhe Institute of Technology (KIT),\nHertzstr. 16, 76187 Karlsruhe, Germany\nanastasiadis@kit.edu\nAbstract. Hyperspectral imaging is increasingly used for product monitoring in\nindustrial processes. Spectral unmixing is an important task in this context. As\nin many other areas of signal processing, neural networks also provide promising\nresults for spectral unmixing. Unfortunately, it is very time-consuming to prepare\nlabelled training data for the neural networks. To address this problem, this pa-\nper presents a method where small training datasets are augmented to improve\nspectral unmixing performance. Inspired by Gaussian processes, simple neural net-\nworks are trained which are capable of generating additional training data. These\nare similar to the original training data but cover areas in the continuous label\nspace that are not covered by the original data.\nKeywords: Spectral unmixing, spectral variability, data augmentation, neural\nnetwork, Gaussian process\n1\nIntroduction\nSince they are non-contact and non-destructive, optical measurement methods are often\nused for monitoring industrial processes. This also includes checking for the correct prod-\nuct composition. For this task hyperspectral images are often used because they have a\nﬁnely sampled spectrum in each pixel characterizing the materials involved [1]. In con-\ntrast, conventional colour images are usually not able to solve this problem suﬃciently\nbecause these only contain three colour channels and diﬀerent spectra can result in the\nsame colour channel values. Spectral unmixing is needed if more than one material is\ncontained in a pixel and therefore only a mixed material spectrum is available. The aim\nis to get the relative proportions, the abundances, of the pure materials covered by the\npixel [2]. This is often done using mixing models, such as the linear mixing model (LMM),\nwhich has proven to be a good approximation. However, depending on the problem, more\ncomplex mixing models can provide better results but are also more diﬃcult to use [3].\nIn addition, there is spectral variability, which can be taken into account by the models\nusing additional parameters.\nInstead of a model-based, a data-based approach is also feasible. Artiﬁcial neural\nnetworks in particular have achieved great success in recent years. This is also true for\nspectral unmixing and comes with additional advantages [4]. One of them is that the non-\nnegativity and the sum-to-one constraints can be enforced by output layer design. The\nother advantage is that spectral variability can be taken into account if it is contained in\nthe training data [5]. Ideally, the trained neural networks are robust to spectral variability.\nTo achieve this and a good spectral unmixing performance, lots of signiﬁcant training\ndata are needed, which are often not available in this domain.\n61\n\nMainly for classiﬁcation problems, augmentation strategies are widely used to in-\ncrease the size of training datasets synthetically and improve performance [6, 7]. Data\naugmentation can also be used with spectral unmixing, which can be considered as a\nregression problem. Here it appears useful to generate spectra based on abundances that\ndo not occur in the original training dataset. Ideally, spectral variability is also taken\ninto account by generating many spectra for each abundance set. We have shown in a\nprevious paper that this improves spectral unmixing performance of convolutional neural\nnetworks (CNNs) [8]. There we used a generative convolutional neural network with addi-\ntional random inputs for spectral variability to learn the relationship between abundances\nand mixed spectra.\nIn this paper we model the spectra as Gaussian processes with the wavelength as the\nindex and the abundances as parameters. Gaussian processes are deﬁned by the mean and\ncovariance functions. Here we are dealing with functions which depend on the wavelength\nand are parametrised by the abundances. To learn these functions, we use simple neural\nnetworks. To generate the additional training data, further abundances are given to the\nneural networks. Spectral variability is taken into account by the generation of multiple\nspectra.\nIn a previous paper we have already modelled the spectra as Gaussian random vec-\ntors [9]. However, that paper was not about dataset augmentation, but about model-based\ndata generation taking spectral variability into account. For the model-based approach,\nonly a set of spectra of each pure material is needed, whereas here we need additional sets\nof spectra of material mixtures. The additional information should lead to better spectral\nunmixing performance. Another approach exists where spectral unmixing is achieved by\ndirect application of Gaussian process regression [10], however, not for the augmentation\nof training data.\nThe rest of the paper is organized as follows: Section 2 summarises the necessary\nbasics regarding spectral unmixing. Afterwards in Section 3 the proposed approach is\ndescribed in detail. The evaluation of the approach is given in Section 4. The paper is\nsummarized and a conclusion is drawn in Section 5.\n2\nSpectral Unmixing\nThis paper deals with supervised spectral unmixing, which assumes that the spectra of\nthe pure substances involved are known [2]. Common spectral unmixing methods are\nmodel-based, with the LMM, representing a good approximation in many cases, being\nthe most commonly used [2, 11–14]. There also exist non-linear mixing models [3], which\nare not considered in this paper. The objective, the estimation of abundances ˆa ∈RP , is\nachieved using the LMM by\nˆa = arg min\na\n∥y −M a∥2\n2 .\n(1)\nHere y ∈RΛ is a measured spectrum, i.e. a pixel of a hyperspectral image, sampled at Λ\nwavelength channels, M = [m1, ..., mP ] ∈RΛ×P are the spectra of the up to P involved\npure materials, and a = [a1, ..., aP ]T ∈RP are the corresponding abundances. The\noptimisation can be done by calculating the pseudo-inverse. However, constraints must\nbe fulﬁlled for the abundances in order to remain physically plausible. Those constraints\nare the non-negativity constraint (2) and the sum-to-one constraint (3).\nap ≥0\n∀p\n(2)\n62\n\nP\n\u0005\np=1\nap = 1\n(3)\nThe consideration of these constraints counteracts model errors caused by the assump-\ntion of a linear mixing behaviour. A well established approach that optimises the LMM\nconsidering (2) and (3) is the Fully Constrained Least Squares (FCLS) algorithm [15].\nInstead of (1), the Lagrangian L : RP +1 →R with the Lagrange multiplier l ∈R is\noptimized:\nL(a, l) = ∥y −M a∥2\n2 −l\n\u000b P\n\u0005\np=1\nap −1\n\f\n.\n(4)\nThe second part of (4) forces constraint (3). Additionally, negative ˆap and the corre-\nsponding spectra in M are removed in an iterative procedure to ensure (2) as well.\nUntil now, the assumption has been made that the pure substances involved can be\nrepresented by a single spectrum. However, there is so-called spectral variability. It is\ncaused, among other things, by changing surface conditions and the resulting variation\nin the angle of illumination [5]. Extended mixing models are available that take spectral\nvariability into account by using additional parameters, such as the extended linear mix-\ning model (ELMM) [16] or the generalized linear mixing model [17]. The ELMM uses\nthe diagonal matrix B ∈RP ×P to extend the LMM optimization problem to\nˆa = arg min\na,B\n∥y −M B a∥2\n2 .\n(5)\nAfter presenting the basics, the next section describes the approach used to augment\ntraining datasets. The aim is to improve the performance of spectral unmixing for data-\nbased methods.\n3\nProposed Approach\nThe prerequisites for this approach are a set of available spectra for diﬀerent abundance\nvectors a. This is quite reasonable in an industrial environment, e.g. in a calibration\ndataset. The measured spectra are available as vectors ya ∈RΛ in which each entry\ncorresponds to the reﬂectance of light for a speciﬁc wavelength. For each abundance\nvector a there are diﬀerent measured spectra, which diﬀer due to spectral variability.\nThese are now to be modelled as one Gaussian process Y (λ|a) with the wavelength index\nλ ∈N as the index and parametrised with the abundance vector a.\nGaussian processes are completely deﬁned by a mean function and an (auto-)covariance\nfunction [18, p. 13]. In this case, the mean value function is\nmY (λ|a)\n(6)\nand the covariance function with the second wavelength index λ∗∈N\nkY (λ, λ∗|a) .\n(7)\n3.1\nData Preparation\nIn order to be able to represent this model with neural networks, the data are prepared.\nFirst, for all abundance vectors a, the mean vector (8) and auto-covariance matrix (9)\nare calculated.\nma = 1\nNa\nNa\n\u0005\nn=1\nyan\n(8)\n63\n\nKa =\n1\nNa −1\nNa\n\u0005\nn=1\n(yan −ma) (yan −ma)T\n(9)\nHere Na ∈N denotes the number of measured spectra for a given abundance vector a.\nThe elements of ma and Ka for all available a can now be used as training data for the\nneural networks Nm and Nk that are supposed to learn (6) and (7).\nThe neural network Nm has the abundance vector a and the wavelength index λ as\ninput variables and as the output variable the corresponding value of ma. The neural\nnetwork Nk has the abundance vector a and the transformed indices λ′\n1 = λ + λ∗and\nλ′\n2 = max(λ) −|λ −λ∗| as input variables and as the output variable the corresponding\nvalue of Ka. The indices λ′\n1 and λ′\n2 are used because of two properties that the covariance\nmatrices have. Firstly, there are higher values on the main diagonal, and secondly, they\nare symmetrical. This results in the neural network being kept quite simple later on, as\nit has to learn fewer changes in monotonicity (see Fig. 1).\nFig. 1. Illustration of the values of the indices in the auto-covariance matrix (from left to right):\nλ, λ∗, λ′\n1 and λ′\n2. Dark blue denotes a low value, yellow a high value\n3.2\nNeural Networks for Data Augmentation\nThe neural networks can now be trained with the prepared training data as described\nabove. The neural networks each have P +1 (a and λ) or P +2 (a, λ′\n1, and λ′\n2) inputs and\nonly one output. To learn the desired relation a quite simple neural network is suﬃcient.\nThe networks consist of fully connected layers, i.e. layers in which all neurons of\none layer are connected to all neurons of the neighbouring layers. The rectiﬁed linear\nunit (ReLU) frelu(z) = max(0, z) is used as the activation function in all layers, with the\nexception of the last layer, where the logistic function\nflog(z) =\n1\n1 + e−z\n(10)\nis used. Batch normalisation is carried out prior to the rectiﬁed linear units [19]. The\nnetworks Nm and Nk have the same structure, which is shown in Fig. 2. The logistic loss\nfunction is used as objective function:\n−1\nB\nB\n\u0005\nb=1\nob · log(ˆob) + (1 −ob) · log(1 −ˆob) .\n(11)\nIt is evaluated for each output value ˆob ∈(0, 1) and corresponding label ob ∈(0, 1) of\na training batch of size B ∈N. The logistic loss function is often used for a two-class\n64\n\nAMTmi\n9×\n6mHHv +QMM2+i2/ H\u001cv2`\n\"\u001ci+? MQ`K\u001cHBb\u001ciBQM- _2Gl\n6mHHv +QMM2+i2/ H\u001cv2`\nGQ;BbiB+ 7mM+iBQM\nPmiTmi\nFig. 2. Schematic representation of the neural network: There are four blocks consisting of a\nfully connected layer, batch normalisation and a ReLU activation function. This is followed by\na fully connected layer with the logistic activation function\nclassiﬁcation problem (cross-entropy loss). However, it also works with continuous labels\nand is suitable here because the values of the spectra range between 0 and 1.\nUsing the trained neural networks Nm and Nk, an augmentation of the original train-\ning dataset can now be performed.\n3.3\nAugmentation Strategy\nFor the augmentation, additional mean value vectors and covariance matrices can now\nbe generated by specifying abundance vectors for Nm and Nk that do not occur in the\noriginal training dataset. This allows pseudo-random generators to be used to produce\nspectra that complement the original training dataset. The spectra generated in this way\nalso show spectral variability.\nIn order to augment the datasets at a lower eﬀort a second strategy is used, where\nthe original training datasets are only augmented by the mean value spectra. The neural\nnetwork, which is later used for spectral unmixing (see Section 4), then has to learn the\nspectral variability on basis of the already existing training data.\nThe spectral unmixing performance of the augmented datasets is compared with that\nof the non-augmented datasets.\n4\nExperimental Results\nPreceding the evaluation, the parameters used for Nm and Nk and the evaluation datasets\nare presented. The number of neurons was determined to be 32 for all layers and in both\n65\n\ncases (Nm and Nk). The neural networks were trained with the Adam optimizer [20]. The\nparameters from [20] were used, except for the learning rate, which was set to 0.01. The\nnumber of epochs was set to 2000 (both networks) for the datasets containing mixtures of\nquartz sand (see below) and to 3000 (Nm) and 4000 (Nk) for the colour powder dataset.\n4.1\nDatasets\nThree datasets are used, which were recorded in our image processing laboratory. This\nensures that we know the abundances as accurately as possible. All datasets consist of\nﬁne powders. These were mixed according to the speciﬁed abundances until the mixtures\nwere homogeneous. A white balance with a reﬂectance standard was carried out after\nthe recordings of the hyperspectral images, which compensates both spatial and spectral\ninhomogeneities of illumination and measurement setup. All datasets were acquired in\n91 wavelength channels, ranging from 450 nm to 810 nm. For each mixture, 400 samples\nwere acquired.\nTwo of the datasets contain mixtures of coloured quartz sand. The ﬁrst of them\n(quartz-3) contains 45 mixtures of at maximum 3 components varying in abundance steps\nof 0.125 . The other one (quartz-4) includes 56 mixtures of at most 4 components, varying\nin abundance steps of 0.2 . The quartz sand datasets have a lower spectral variability and\nthe non-linearity in the mixing behaviour is less signiﬁcant compared to the following\ndataset. The third dataset consists of 56 mixtures of colour powders (colour-4), which\nalso have up to 4 components. Again, the components are varied in abundance steps\nof 0.2 . The colour-4 dataset shows a high non-linearity between mixed spectra and the\nspectra of the pure substances and a high spectral variability. Hence, its spectra are more\ndiﬃcult to unmix.\nAll three datasets are divided into a test and a training dataset according to the\nabundances. For the datasets with four components, the samples with no abundance of\nvalue 0.2 or 0.8 are included in the training dataset. All other samples are included in\nthe test dataset. This yields 16 abundance vectors in the training and 40 in the test\ndataset. The quartz-3 test dataset includes those where at least one abundance has the\nvalue 0.125, 0.375, 0.625 or 0.875. In consequence, there are 30 abundance vectors in the\ntest dataset and 15 in the training dataset.\n4.2\nEvaluation of Generated Data\nUsing the neural networks from Section 3, new data are generated. The abundance vec-\ntors used as inputs are exactly the same as those in the test dataset. For each given\nabundance vector 400 spectra are generated, which correspond to the number of spectra\nper abundance vector in the test dataset.\nAs a measure of performance, the average minimum norm ΔAMN is used between I\nmeasured spectra yi and H generated spectra ˆyh corresponding to an abundance vector:\nΔAMN = 1\nI\nI\n\u0005\ni=1\nmin\nh ∥yi, ˆyh∥2 .\n(12)\nThis performance measure was chosen because it tests whether a spectrum was generated\nas similar as possible to each spectrum in the test dataset. The calculation is done\nseparately for each abundance vector and the corresponding spectra. The mean value of\nall ΔAMN over all abundance vectors in the test dataset is called global average minimum\nnorm ΔGAMN (see Table 1). The results of both proposed augmentation strategies are\n66\n\ncompared with the performance of the generative convolutional neural network (Gen.\nCNN) with and without covariance matrix regularisation (-CovR) we presented in [8].\nTable 1. Comparison of ΔGAMN for all test datasets. In the ﬁrst two columns the results from\n[8] are listed for comparison. The third column presents the values for the proposed method and\nthe last column uses only the generated mean vectors ˆma as generated spectra\nΔGAMN\nGen. CNN [8]\nGen. CNN-CovR [8]\nProposed (normal)\nProposed (mean only)\nquartz-3\n0.1219\n0.0812\n0.0993\n0.1371\nquartz-4\n0.1113\n0.0787\n0.0903\n0.1298\ncolour-4\n0.1242\n0.0967\n0.1016\n0.1470\nTable 1 shows that the inclusion of the covariance matrices results in lower ΔGAMN\nvalues for all datasets compared to only using the mean vectors. This is because spectral\nvariability is taken into account. The results of the proposed method are better than\nthose of the unmodiﬁed generative CNN. However, the best results were achieved with\nthe generative CNN with covariance matrix regularisation. It is also noticeable that\nwithin a method, the order of the datasets regarding ΔGAMN always remains the same,\nwhich is due to the diﬃculty of the datasets.\nIn the next subsection, it will be investigated whether these results are consistent\nwith those of spectral unmixing using augmented training datasets.\n4.3\nSpectral Unmixing Performance\nFor evaluation of the spectral unmixing performance, we use the same CNN as in [8],\nof which we have already presented the three-dimensional version in [4]. The CNN is\ntrained with the original training dataset as well as with diﬀerent augmented training\ndatasets. The performance with respect to the test dataset is compared below. The CNN\nfor spectral unmixing consists of three convolutional layers with a convolutional kernel\nlength of 3. Then two fully connected layers follow. The numbers of feature maps from the\ninput layer to the output layer are 1, 16, 32, 64, 64 and 1. We use the root-mean-square\nerror over all N samples of a test dataset\nΔRMSE =\n\r\n\u000e\n\u000e\n\u000f 1\nN\nN\n\u0005\nn=1\n1\nP\nP\n\u0005\np=1\n(ˆapn −apn)2\n(13)\nas a performance measure. For the methods that are not based on neural networks (see\nSection 2), the results are shown in Table 2 for the sake of clarity. For the remaining\nmethods ΔRMSE is displayed in Figure 3.\nTo obtain the results below, the network was trained with diﬀerent numbers of epochs\ndepending on the dataset and method. The quartz-3 dataset was trained for 251, the\nquartz-4 dataset for 41 and the colour-4 dataset for 31 epochs for the proposed method.\nWhen only mean vectors are used for augmentation, the number of epochs reduces to\n31 (quartz-4) and 21 (colour-4). As a reference, we use the non-augmented training\ndataset, that was trained for 81 (quartz-3), 21 (quartz-4) and 21 (colour-4) epochs. The\ndiﬀerent numbers of epochs are chosen to avoid overﬁtting.\nThe original training datasets were augmented with a diﬀerent number of spectra.\nFigure 3 shows the step size s ∈[0, 1] in which the additional abundance vectors were\n67\n\nTable 2. Comparison of ΔRMSE for all test datasets for FCLS and ELMM based spectral\nunmixing\nΔRMSE\nquartz-3\nquartz-4\ncolour-4\nFCLS\n0.1608\n0.1115\n0.2987\nELMM\n0.1555\n0.1056\n0.2990\nvaried to generate the new data. All possible abundance vectors corresponding to the step\nsize s are used in spectra generation, except for those already contained in the original\ntraining dataset.\n0.00\n0.02\n0.04\ns = 1\n5\ns = 1\n8\ns =\n1\n10\ns =\n1\n20\n0.00\n0.03\n0.06\nΔRMSE\ns = 1\n4\ns = 1\n5\ns = 1\n8\ns =\n1\n16\nLQi\n\u001cm;K2Mi2/\n:2MX *LL\n:2MX *LL@*Qp_\nS`QTQb2/\nUMQ`K\u001cHV\nS`QTQb2/\nUK2\u001cM QMHvV\n0.00\n0.05\n0.10\ns = 1\n4\ns = 1\n5\ns = 1\n8\ns =\n1\n16\nFig. 3. Comparison of ΔRMSE for the test datasets of the quartz-3, quartz-4, and colour-4\ndatasets (top to bottom) for CNN-based spectral unmixing using diﬀerent augmentation strate-\ngies. The dashed lines are used for a better visual comparability with the non-augmented case\nIt is shown that the data-based spectral unmixing methods (Figure 3) perform better\nthan the model-based methods (Table 2). Gaussian process based augmentation leads to\nan improvement compared to the non-augmented training dataset for all datasets and\nall step sizes s. The size of s does not have a major inﬂuence, unless it is chosen too\nlarge, in which case the performance deteriorates. If only the mean spectra are used\nfor augmentation, the results are comparable. This is probably due to the fact that the\nspectral variability does not depend too much on the abundances and is already well\nrepresented by the spectra available in the original training dataset. For the colour-4\n68\n\ndataset, the result is worsened by adding the information from the covariance matrices.\nIn this case, the assumption of a Gaussian process is likely to be an oversimpliﬁcation.\nThe results from [8] cannot be reached with this approach. However, the training\ntime of the neural networks for augmentation is approximately 9 times1 shorter. On the\none hand, this is due to the lower dimensional data points and therefore simpler neural\nnetworks. On the other hand, the size of the training dataset for augmentation is reduced\nif only the described moments are used as training data. The latter is especially true if\nonly the mean spectra are used. In this case it is only one spectrum per abundance vector\ninstead of Na. This leads to an approximately 120 times1 shorter training time compared\nto [8].\n5\nConclusion\nIn this work, an approach to augment training datasets for spectral unmixing was pre-\nsented. For this purpose, inspired by Gaussian processes, a mean and a covariance func-\ntion are learned by two neural networks. These networks are then used to generate\nadditional training data.\nIt was shown that the performance of spectral unmixing with a CNN can be improved\nby the additional training data generated by these neural networks. It depends on the\ndataset how signiﬁcant the improvement is. The improvement is slightly lower as with\nan existing method that uses a generative CNN for augmentation. However, the training\ntime is an order of magnitude shorter. If only the neural network for the mean value\nfunction is used, where a similar increase in performance was observed depending on the\ndataset, the training time decreases by another order of magnitude.\nIn the future, something in between the two approaches presented would also be\nfeasible. There, only the more relevant parts of the covariance functions would be used.\nReferences\n1. Gowen, A., O’Donnell, C., Cullen, P., Downey, G., Frias, J.: Hyperspectral imaging – an\nemerging process analytical tool for food quality and safety control. Trends in Food Science\n& Technology 18(12) (2007) 590–598\n2. Keshava, N., Mustard, J.F.: Spectral unmixing. IEEE signal processing magazine 19(1)\n(2002) 44–57\n3. Dobigeon, N., Altmann, Y., Brun, N., Moussaoui, S.: Linear and nonlinear unmixing in\nhyperspectral imaging. In Ruckebusch, C., ed.: Data Handling in Science and Technology.\nVolume 30. Elsevier (2016) 185–224\n4. Anastasiadis, J., Puente Le´on, F.: Spatially resolved spectral unmixing using convolutional\nneural networks (German paper). tm – Technisches Messen 86(s1) (2019) 122–126\n5. Borsoi, R.A., Imbiriba, T., Bermudez, J.C.M., Richard, C., Chanussot, J., Drumetz, L.,\nTourneret, J.Y., Zare, A., Jutten, C.: Spectral variability in hyperspectral data unmixing:\nA comprehensive review. arXiv preprint arXiv:2001.07307 (2020)\n6. Simard, P.Y., Steinkraus, D., Platt, J.C., et al.:\nBest practices for convolutional neural\nnetworks applied to visual document analysis. In: Icdar. Volume 3. (2003)\n7. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep convolutional\nneural networks. In: Advances in neural information processing systems. (2012) 1097–1105\n8. Anastasiadis, J., Heizmann, M.: CNN-based augmentation strategy for spectral unmixing\ndatasets considering spectral variability. In Bruzzone, L., ed.: SPIE Remote Sensing – Image\nand Signal Processing for Remote Sensing XXVI. Volume 11533 of Proceedings of SPIE.,\nSPIE (2020) 188–199\n1 Training performed on NVIDIA Quadro P5000.\n69\n\n9. Anastasiadis, J., Heizmann, M.: Generation of artiﬁcial training data for spectral unmixing\nby modelling spectral variability using gaussian random variables. In: OCM 2021 – Op-\ntical Characterization of Materials : Conference Proceedings. Ed.: Beyerer, J., L¨angle, T.,\nKarlsruher Institut f¨ur Technologie (KIT) (2021) 129–139\n10. Altmann, Y., Dobigeon, N., McLaughlin, S., Tourneret, J.: Nonlinear spectral unmixing of\nhyperspectral images using Gaussian processes. IEEE Transactions on Signal Processing\n61(10) (2013) 2442–2453\n11. Bauer, S., Stefan, J., Puente Le´on, F.:\nHyperspectral image unmixing involving spatial\ninformation by extending the alternating least-squares algorithm. tm – Technisches Messen\n82(4) (2015) 174–186\n12. Krippner, W., Bauer, S., Puente Le´on, F.: Optical determination of material abundances\nin mixtures (German paper). tm – Technisches Messen 84(3) (2017) 207–215\n13. Krippner, W., Bauer, S., Puente Le´on, F.: Considering spectral variability for optical ma-\nterial abundance estimation. tm – Technisches Messen 85(3) (2018) 149–158\n14. Krippner, W., Puente Le´on, F.: Band selection and estimation of material abundances using\nspectral ﬁlters (German paper). tm – Technisches Messen 85(6) (2018) 454–467\n15. Heinz, D., Chang, C.I., Althouse, M.L.: Fully constrained least-squares based linear unmix-\ning. In: IEEE 1999 International Geoscience and Remote Sensing Symposium. Volume 2.,\nIEEE (1999) 1401–1403\n16. Veganzones, M.A., Drumetz, L., Tochon, G., Dalla Mura, M., Plaza, A., Bioucas-Dias, J.,\nChanussot, J.: A new extended linear mixing model to address spectral variability. In: 2014\n6th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing\n(WHISPERS), IEEE (2014) 1–4\n17. Imbiriba, T., Borsoi, R.A., Bermudez, J.C.M.: Generalized linear mixing model accounting\nfor endmember variability. In: 2018 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), IEEE (2018) 1862–1866\n18. Rasmussen, C.E., Williams, C.K.I.:\nGaussian processes for machine learning.\nAdaptive\ncomputation and machine learning. MIT Press, Cambridge, Mass. [u.a.] (2006)\n19. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. CoRR abs/1502.03167 (2015)\n20. Kingma, D., Ba, J.:\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980 (2014)\n70\n\nR&D of a Multisensory System for Excavation Machines for \nthe Real-time Generation of AI/ML classified, Georeferenced \nand BIM compliant Voxel Models of Soil (ZIM Project \nHOBA)\nAlmagboul, M., Anantha, P. C., Jäger, R. and G. P. Sridhar \nKarlsruhe University of Applied Sciences, Center of Applied Research (CAR) \nreiner.jaeger@web.de\nAbstract. The R&D project “Homogeneous soils assistant for the automatic, construction \nsite-specific recording of soil classes according to the new VOB 2016”, shortly HOBA, \ndeals with the development of automatic classification, detection & segmentation of \nconstruction \nsite \nspecific \nsoil \ntypes \n(http://www.navka.de/index.php/de/weitere-\nprojekte/hoba-project-overview-1). HOBA is financed as a so-called ZIM (Central \nInnovation Program for SMEs) research and development project by the Federal Ministry \nfor Economic Affairs and Energy (BMWI). Research and development are carried out in \nthe GNSS & Navigation  Laboratory  (http://goca.info/Labor.GNSS.und.Navigation/index  \n.php)  in collaboration with the main industry partners MTS Schrode AG (www.mts-\nonline.de) and VEMCON GmbH (www.vemcon.de). \nThe aim of the R&D at the HKA is the development of the hardware and software of a \ncompact sensor- and computing system unit, mounted on the excavator, briefly called HKA \nHOBA-Box (fig. 1). The hardware and software development of the HKA HOBA-Box is \nan innovative contribution to the BIM-compliant digital real-time documentation of \nexcavation work in civil engineering.\nFigure 1 Excavator with distributed sensors and HKA HOBA-Box located in the area of the shovel\nThe HKA HOBA-Box enables a multi-sensory 3D geo-referencing of the excavation in the \nETRF89 / ITRF, based on GNSS/MEMS/Optics (RGB/ToF-camera) sensors. The complete \ngeo-referencing  steps  of  the  box  are  based  on  a  Bayesian  sensor-fusion algorithmic \nfusion in the general NAVKA (www.navka.de) multisensory-multiplatform lever arm \ndesign, leading to the full navigation state vector  \n71\n\nݕ(ݐ) = (ݔ௘ݕ௘ݖ௘|ݔሶ௘ݕሶ௘ݖሶ௘|xሷ௘yሷ௘zሷ௘|ݎ௘݌௘ݕ௘|߱௘௕,௫\n௕\n߱௘௕,௬\n௕\n߱௘௕,௭\n௕\n|߱ሶ௘௕,௫\n௕\n߱ሶ௘௕,௬\n௕\n߱ሶ௘௕,௭\n௕\n)் \nThe Bayesian SLAM (Simultaneous Localization and Mapping) means an extension of \nݕ (ݐ),, which is based - in the case of HOBA - on the Gauß-Markov-model’s information \nof the optical sensor data of the ToF and the RGB camera. The extension of the parameter \nspace for SALM is on the 3D map ݉ (ݐ), and so it leads to  ݕ(ݐ)ௌ௅஺ெ= (ݕ(ݐ), ݉ (ݐ)).  \nThe SLAM parameters ݕ(ݐ)ௌ௅஺ெ= (ݕ(ݐ), ݉ (ݐ)) are used for the computation of a so-\ncalled ETRF89 / ITRF \"voxel\"-based 3D model of the excavation volume, based on an \nOctree-based representation of the environment, using the ToF-camera data.   \nFurthermore, the HKA HOBA-box will allow the classification of the soil types on the site \nusing image-based AI/ML algorithms, and finally the re-calculation of the classified and \ngeo-referenced 2D images onto the geo-referenced 3D voxel model according to the soil \ntypes. Here pixels to pixels in the image have to be assigned to the 3D surface on the model \nE\\\u0003FDOFXODWLQJ\u0003FRUUHVSRQGHQW\u0003SRLQWV\u0003\u000b3\u000f\u00033\u0003ƍ\u0003\f\u0003EHWZHHQ\u0003WKH\u0003WZR\u0003UHSUHVHQWDWLRQV\u000f\u0003L\u0011H\u0011\u0003\u0015'\u0003DQG\u0003\n3D. The part of Machine Learning (ML) introduces a real-time soil texture classification \nand segmentation on the construction site. The R&D target of the abovementioned BIM-\ncompliant soils assistant, according to the new VOB 2016 - realized as a physical system \nby HKA HOBA-box - enables civil engineers an insight into the geo-technical properties \nof the excavated soil, as well as to the borders between these soil types, on the construction \nsite. The mathematical and algorithmic approach for soil classification  is  the  Fully \nConvolutional  Network-Based  Semantic  Segmentation  (FCN). This is a pixel-level \nclassification, which achieves a pixel fine-grained inference. Therefore, it is then the best \npractice for such application of soil texture labelling on the site.  \nThe improved FCN-based approaches e.g., SegNet & DeepLap, are used as encoders for \nthe training of the soil texture-model via Transfer Learning (TL). The TL is utilized, as it \nis a well-known method and regarded to shorten the processing time of model training, and \ntherefore decrease the computational efforts considerably. Furthermore, the HKA HOBA-\nbox trains the model on a specific dataset already, as provided by  the HKA industry partner \ni.e. MTS Schrode AG. The dataset i.e., images of the construction site with different present \nsoil types, provides - through a high-precision 2D annotation process -  i.e. instance-aware\nsegmentation and detailed precise masks for a corresponding 15 classes of the soil types,\nwhich are used for training, testing, and validation.\nThe respective AI/KL developments are based on Python & C/C++, using both machine \nlearning frameworks, namely TensorFlow 1.x and PyTorch. The re-trained model is \nconverted into an ONNX (Open Neural Network Exchange), which is an open format for \nML models allowing interchanging between different ML frameworks. \n72\n\nPredictive prognostic for Li-Ion batteries in electric\nvehicles\nIn`es Jorge, Ahmed Samet, Tedjani Mesbahi, and Romuald Bon´e\nICube, CNRS (UMR 7357)\nINSA Strasbourg – University of Strasbourg\nines.jorge@insa-strasbourg.fr\nAbstract. The development of clean vehicles and more speciﬁcally electric and\nhybrid vehicles relies on the performances of Lithium Ion batteries. More eﬃcient\nthan all the other battery chemistry in terms of energy density and output power,\nthese batteries bring hybrid and electric vehicle in line with thermal vehicles.\nHowever, they still suﬀer from a limited driving range and lifespan, and their\nperformances can be aﬀected by numerous factors, one of the most important one\nbeing the driving proﬁle imposed by a user.\nPrognostics and health management strategies make use of operating data in\norder to better understand the ageing mechanisms of Lithium Ion batteries and\nto forecast their future degradation trend. In this article, we introduce our method\nto predict the Remaining Useful Life of Lithium Ion batteries based on the dataset\npublished by the Massachusetts Institute of Technology, through the use of low\ncomputational cost machine learning algorithms. Our artiﬁcial neural networks\ntake both historical data and time series representing the driving proﬁle of a\nbattery as input, and predict with accuracy the Remaining Useful Life of a battery.\nCompared to previous approaches in the literature, we obtain reliable and accurate\npredictions of the Remaining Useful Life of any battery at any moment in its life\nfrom the observation of only charge and discharge cycle. The importance of driving\ndata in prognostics and health management strategies of Lithium Ion batteries is\nshown throughout this article.\nKeywords: Lithium Ion batteries, Prognostics and Health Management, Ma-\nchine Learning, Artiﬁcial Neural Networks, Feature extraction, Remaining Useful\nLife\n1\nIntroduction\nIn the case of hybrid vehicles (HEV), and even more so in the case of all-electric power-\ntrains, the on-board energy storage system remains the weak link: very expensive, limited\nin driving range, slow to recharge, main cause of over-costs... The challenge for any car\nmanufacturer wishing to develop a HEV or an Electric Vehicle (EV) is therefore not only\nto optimise the electric power-train, both in terms of cost and range, but also to bring the\nbattery into line with the life of the vehicle. Battery lifetime is therefore a crucial element\nfor the development of EVs under acceptable cost conditions. Indeed, the battery is the\nkey component and the most expensive one in a HEV or EV. In this context, the failure\nof battery could lead to serious inconvenience, performance deterioration, accelerated\nageing and costly maintenance.\nTherefore, the prognostics and health management (PHM) of on-board energy storage\nsystems, which aims to monitor their health and to predict their degradation trend,\nappears to be a crucial element in the development of new battery powered vehicles.\n73\n\nPHM strategies make it possible to forecast the evolution of the storage capacity of\na battery and to predict its Remaining Useful Life (RUL), which correspond to the\nnumber of charge and discharge cycles it can withstand before reaching its end of life.\nThat allows to perform maintenance service in advance if necessary, using the past and\ncurrent information about battery usage and capacity degradation trend.\nThe aim of this article is to present a method based on machine learning for the predictive\nprognostics of Li-Ion batteries in EV applications. The challenge is to use ageing data\nof Li-Ion batteries in order to extract knowledge on the state of health (SOH) of the\nbatteries. In this paper, we focus speciﬁcally on the dataset published in [1] as it is the\nlargest available and contains extremely valuable data that apply very well to machine\nlearning techniques. The key contributions are (i) the development of low computational\nmodels based on Artiﬁcial Neural Networks (ANN), (ii) an online forecasting of the RUL\nof batteries, (iii) the use of driving data in the predictive model.\nThe remainder of this article is structured as follows : section 2 is a brief introduction\nto related work in the ﬁelds of predictive prognostics of Li-Ion batteries, section 3 provides\na detailed presentation of the dataset on which we based our work, section 4 focuses on the\ndeveloped architecture for RUL prediction based on ANN and feature extraction, section\n5 presents all experimental results with a comparison with other approaches found in the\nliterature and section 6 oﬀers a brief conclusion with a presentation of future works.\n2\nBackground\nConcerning Li-Ion batteries, prognostics and health management strategies (PHM) aim\nat determining how and when a failure will occur and to give a long term image of the\nstate of health (SOH) of the battery [2]. This can be done either by observing previous\ndata acquired through various sensors or by simulating the behaviour of a battery in its\nenvironment thanks to physical models.\nIn a great majority of papers, PHM of Li-Ion batteries consists in determining their\nRUL, which is the number of charge and discharge cycles it can go through before reaching\nthe End of Life (EoL) criteria. A battery is considered out of use for an electric vehicle\nwhen it has reached a SOH of 80%. The SOH of a battery represents the storage capacity\nat a given time compared to its initial storage capacity :\nSOH =\nQactual\nQnominal\nand\nSOH% =\nQactual\nQnominal\n∗100\n(1)\nMost approaches deal with the prediction of the RUL in terms of cycles. This can be\ndone either by designing a complete physical model and simulating the behaviour of a\nbattery, or by focusing on real data taken as input of machine learning models. This latter\ntype of models makes it possible to forecast the temporal evolution of the battery SOH\nusing a sliding window approach, or to predict the RUL directly from the observation of\nageing features.\n2.1\nModel based approaches\nModel-based techniques were the ﬁrst ones to be developed, before massive data acqui-\nsition and challenges linked to big data appeared. A model-based approach for the PHM\nof a system relies on the establishment of a simulation model according to physical rules\nand functioning equations. The aim is to understand and reproduce the behaviour of\na system in order to obtain simulated data that could be exploited, in particular with\n74\n\nthe introduction of disturbances. It implies a complete understanding of the system and\ngives a global representation of the diﬀerent answers to solicitations. Downey et al. in\n[3] have modeled the degradation phenomena of active materiel loss in Li-Ion batteries\nin order to estimate the battery capacity. The battery was represented by an electro-\nchemical model that takes into account heat generation equations in [4]. Zhang et al.\n[5] elaborated a comprehensive lead-acid battery model made of seven sub-models each\nmodeling a physical phenomena. The model estimates internal resistance, terminal volt-\nage, internal temperature, SOC and battery capacity using the load current and ambient\ntemperature.\n2.2\nData driven approaches\nData driven approaches for PHM have emerged with the development of industry 4.0 and\nmassive data acquisition strategies. Real operating data is collected and given as input to\na black box model, that uses past data to forecast the evolution of a system. Operating\ndata most of the time consists in physical features observed according to time through\ndiﬀerent sensors linked to a battery or a battery cell during ageing tests (observation of\ncurrent, voltage, internal resistance, temperature...). This results in very large sequences\nof data as the cycle life of a Li-Ion battery can reach more than 2000 cycles. All data driven\nstrategies require a data prepossessing step in order to make operating data compatible\nwith data driven models. However, there can be a great variety of approaches when\nbuilding a data driven predictive model, mainly due to the development of machine\nlearning algorithms that apply very well to large amounts of data and PHM problematic.\nThis quick state of the art of data driven approaches separates the diﬀerent models found\nin the literature into two categories : window-based models and early cycle models.\nWindow approach As explained earlier, operating data of Li-Ion batteries can result\nin very larges sequences of data due to their very high lifespan. A common approach for\nsimplifying the problem is to use a window approach. There are two types of data se-\nquences in Li-Ion battery ageing data. The ﬁrst one are historical data sequences, which\nare represented as a function of the number of cycles. For example, SOH is computed at\neach cycle, just as internal resistance or charging time. For each of these data sequences,\nthere is one value for each cycle, and a sequence window is therefore composed of several\nconsecutive cycles. The second type of data sequence are temporal data, which are rep-\nresented as a function of time. Here, operating data is directly acquired though sensors,\nand for each cycle, the temporal evolution of several features such as current, voltage\nor temperature can be observed. Temporal data represent the real use of the battery. A\nwindow of time sequences can then either be a sample of time series from one cycle, or a\nsuccession of time series that corresponds to several consecutive cycles. Most approaches\ndeal only with the observation of window of historical data, and especially the SOH.\nThe evolution of SOH contained in one window makes it possible to forecast the future\ndegradation trend and therefore to predict the RUL according to the predicted SOH\nfade. This method has proved very eﬀective and can be applied to a great variety of ML\ntechniques [6–10]. However, the main drawback is that the accuracy of the prediction\ndepends on the size of the window. The larger the amount of historical data, the better\nthe accuracy. Moreover, very few approaches take advantage of time series.\nEarly cycle prediction Some approaches mention other RUL prediction techniques\nbased on features calculated from early cycles data. Severson et al. have computed several\n75\n\nfeatures from cycle 1 to cycle 100 and applied a linear regression as a supervised learning\ntechnique to predict the cycle life of a given cell. This method also removes the problem of\ndealing with temporal or sequential values but requires to use only brand-new batteries,\nafter cycling them 100 times.\n3\nBattery ageing data\nThroughout the literature, several datasets are often cited and used for data driven\napproaches concerning PHM of Li-Ion batteries. The NASA Prognostics Center of Excel-\nlence (PCoE) published a massively used dataset for SOH prediction [11]. It consists in\n34 batteries tested under diﬀerent charge and discharge conditions until the EoL criteria\nis reached. More batteries were tested in this dataset without reaching end of life though,\nwhich limits the applications.\nThe NASA PCoE also published a dataset that consists in testing 4 batteries with\nrandom charge and discharge currents during 1500 cycles [12]. After 1500 cycles, charac-\nterisation cycles are performed in order to evaluate the evolution of the batteries’ SOH.\nAn other dataset published by the Sandia National laboratories aims at studying the\neﬀect of Depth of Discharge (DoD), load current and temperature on battery degradation.\n86 cells of three diﬀerent chemistries (LFP, NMC and NCA) are tested in this study.\nSome papers also mention custom battery ageing datasets [13]. The main drawback\nwhen testing batteries for health prognostics is that EoL criteria need to be reached.\nConsidering the performances of Li-Ion batteries, this may take a long time and require\na lot of resources for testing a representative number of cells. To overcome the resource\nand time problem, a paper also shows the use of training data generated with a physic-\nbased model of Li-Ion battery [14].\nEven though all this data is of great interest, we decided to base our approach on\na new dataset described in [1] (with supplementary information in [15]). This dataset\ngathers more information than all earlier mentioned datasets, to our knowledge, as it\noﬀers complete operating data of 124 cells tested from beginning to EoL.\n3.1\nMIT dataset\nIn [1], the department of chemical engineering of the Massachusetts Institute of Tech-\nnology, in collaboration with Toyota engineering and with the Department of Materials\nScience and engineering of Stanford university, have built the largest available dataset\nregarding Li-Ion battery ageing. This dataset is a highly valuable source of information\nas very few public data can provide that much resource. The cells that were used for\ntesting are LFP/graphite cells from A123 manufacturer, model APR18650M1A. These\ncells have a 3.3V nominal voltage and a 1.1Ah nominal capacity. They can provide dis-\ncharge currents up to 30A.\nThe cells were tested in a 30°C chamber and cycled with an battery tester from Arbin\nmanufacturer. The batteries are always discharged at a constant current of 4.4A. The\nmost important factor in the tests is the charging policy. Batteries are charged following\na multi-steps constant-current/constant-voltage (CC-CV) policy which makes it possible\nto reduce the charging time. By applying a fast charging policy, batteries are tested under\nconditions that are close to the real use of batteries in an EV. Indeed, one of the main\nchallenges that EV are facing is the charging time, which should be as short as possible\nwithout damaging the cells.\n76\n\nAs explained in section 2.2, there are two diﬀerent kind of data sequences in this\ndataset : historical data sequences and times series.\nFigures 1, 2 and 3 are representations of time series for one given cycle (the charging\npattern, evolution of external temperature during one cycle, discharge voltage...) and\nﬁgures 4, 5 and 6 show the global evolution of a given historical data sequence over the\nfull life cycle of a battery.\nFig. 1. Charging pattern\nFig. 2. Cell T°\nFig. 3. Discharge V and I\nFig. 4. Capacity fade\nFig. 5. Average cell T°\nFig. 6. Internal resistance\nThe available dataset oﬀers a considerable amount of ageing data from the ﬁrst cycle\nof each cell to the EoL. Every cycle gives information about ageing signs and SOH that\nshould be taken into account. Both historical data and time series contain information\nabout the RUL and SOH of the battery at a given time, but they need to be pre-processed\nand combined in order to highlight the factors that most represent the degradation trend\nof a battery.\n3.2\nExploitation of driving data\nAs detailed in section 2.2, most approaches are based on the exploitation of SOH historical\ncurves only to forecast the future SOH degradation trend [7, 16, 13]. We see two major\ndrawbacks in designing a SOH forecasting model based on previous SOH data only. First,\nas all available data consists of experimental data built from laboratory cell tests, the\ndegradation trend is quite steady. Indeed, in the MIT dataset, cells are discharged at a\nconstant current rate, identically throughout their whole cycle life. Similarly, the charging\nprotocol does not vary from beginning to end of life. This results in SOH degradation\npatterns that are very similar from one battery to another, as can be seen in ﬁg 4.\nTherefore, forecasting future SOH degradation trend from past SOH data is simpliﬁed\nand can be implemented with most machine learning algorithms.\nSecondly, studying the global trend of SOH can give a good idea of the long term\ndegradation but could not make it possible to catch local variations due to a speciﬁc\nuse of the battery. Current (I), voltage (V) and temperature (T°) time series reﬂect the\nreal use of the battery : I and V curves represent the driver solicitations (acceleration,\nspeed, breaks...) and T°brings information about the environment in which the battery\nis used (cold or warm weather, night or day etc . . . ). Therefore, we believe that using\n77\n\ndriving data as input to our PHM model is crucial in understanding all possible causes\nof deterioration.\n3.3\nTraining dataset\nThe RUL of a battery decreases at each cycle. Our target is to predict the RUL of a\nbattery at any given cycle, focusing on only one cycle. That means that in this paper,\na window based approach is used, where the window size is of one cycle. RUL can be\ncalculated for each cycle following equation 2 :\nRUL = ki −n\n(2)\nwhere ki is the cycle life of cell number i and n is the observed current cycle of the\ncell.\nOur approach mixes the use of historical data and temporal series. These two types\nof data can’t be used directly together as historical data have one scalar value per cycle\nand time series have one vector of varying length per cycle. Therefore, before using time\nseries in our model, a feature extraction technique is used to condense the information\ncontained in each vector into one scalar value. For example, several features are computed\nfrom each time series such as the root mean square value, area under the curve or average\nvalue. Computed features from time series can then be exploited as input to any given\nmodel in the same way as historical data. The training dataset as used in our approach\nis represented in ﬁgure 7.\nFig. 7. Training dataset composed of historical data and time series features\nIn order to compare the performances of diﬀerent combinations of features, two models\nwere developed : one using only historical data, and another one using a selection of\nhistorical data and time series features.\n78\n\n4\nProposed architecture\nAs the dataset is quite recent, only little work has been based on it. In this paper,\nthe architecture described consists in extracting features from temporal series and use\nthem along with historical data to predict the RUL of a battery. Our predictive model\nis based on a well known machine learning regression tool : Artiﬁcial Neural Networks\n(ANN). Our goal here is not to investigate new predicting approaches but to prove that\nthe available data combined with low computational models can lead to very eﬃcient\nprediction performances. We only used ANN in this work because they can adapt to a\ngreat variety of data types and size. Moreover, we based our approach on a prediction of\nRUL as a scalar value. No sequential prediction of SOH or ageing features is made.\nTwo diﬀerent ANN were built according to the number of features they take as input.\nThe ﬁrst one takes as input features extracted from time series and SOH, and will be\nreferred to as TSF ANN (Time Series Features ANN). The second one takes as input\nonly historical features and will be referred to as HF ANN (Historical Features ANN).\nAs explained is the previous section, this article also studies the impact of input\nfeatures on the prediction performances. In the dataset described in section 3.3, each\ncycle is considered as one training sample, and the target is the RUL of the battery. As\neach cycle of each cell is considered, the dataset results in more than 99000 samples and a\nvarying number of features according to the model. As there is a great amount of available\ndata, the structure of the ANN can extend to several layers, each with a great number\nof neurons. Dropout was added after each layer in order to avoid over ﬁtting during\ntraining. In order to ﬁnd the best combination of hyper parameters (number of layers,\nnumber of units per layers, activation function, dropout rate...), several conﬁgurations\nwhere tested following a Bayesian optimisation procedure. In all cases, the output layer\nof our model only contains one unit and no activation function as a regression is made\non the RUL, directly in terms of number of cycles.\n5\nExperiments\nThe following section describes how each model was tested on the diﬀerent feature se-\nlections and compares the performances of our diﬀerent models between them and in\ncomparison with other RUL prediction approaches on the same dataset as us.\n5.1\nTraining process\nThe ﬁrst step of the training process is to perform the optimisation of the hyper pa-\nrameters as explained earlier. After having completed the setup of hyper-parameters for\ndiﬀerent models, optimised models are completely re-trained. The dataset described in\nsection 3.3 is randomly separated into three distinct ensembles : a training, a validation\nand a test set. In order to obtain reliable results, the process is repeated several times.\nThe error measure is computed as the mean of all obtained measures during successive\ntraining.\n5.2\nError metrics\nDuring training, the back propagation process for weight optimisation is carried out with\nthe Adam optimiser. The loss is calculated with Mean Square Error and performance is\njudged with the Mean Absolute Error metrics. We used mini-batch gradient descent in\n79\n\norder to obtain an eﬃcient and relatively short training time combined with an accurate\nconvergence towards the minimum loss.\nIn order to compare the performances of our models between them and with other\napproaches in the literature, several scoring measures are used. In a vast majority of\nworks, the evaluation of models is based on the Root Mean Square Error (RMSE) and\nMean Absolute Error (MAE). We also add the Normalised Mean Square Error (NMSE)\nin order to compare the performances of our models with future works, and the Standard\nDeviation of the MAE (σMAE) in order to evaluate the reliability of the model. These\nquality measures are expressed as follows:\nRMSE =\n\r\n\u000e\n\u000e\n\u000f 1\nN\nN\n\u0005\ni=1\n(ypred,i −yi)2\n(3)\nMAE = 1\nN\nN\n\u0005\ni=1\n|ypred,i −yi|\n(4)\nσMAE =\n\r\n\u000e\n\u000e\n\u000f 1\nN\nN\n\u0005\ni=1\n(ai −MAE)2\n(5)\nNMSE =\n\nN\ni=1(ypred,i −yi)2\nN ∗V\n(6)\nIn all these formulas, ypred,i is the RUL predicted by the model, yi is the real RUL\nand N is the number of samples on which error is calculated.\nIn the equation of standard deviation, a is the absolute error of sample i.\nV is the variance of y. For example, the use of the mean of y as the predicted values\nwould give an NMSE of 1.\n5.3\nPrediction performances\nIn this section the predicting results of our ANNs will be compared between them. Our\ntwo models are built to predict one single value of RUL. A 2D dataset is fed to the\nnetworks and a 1D output is given, which corresponds to the predicted RUL in terms\nof cycle. The output can take any possible positive value. Figures 8 and 9 represent the\npredicting performances of the diﬀerent networks. The predicted RUL is plotted as a\nfunction of the real RUL.\nTable 1 details the predicting performances of the two developed models. The best\nprediction performances are obtained with the TSF ANN, which proves that the infor-\nmation contained in time series is highly valuable when designing a PHM strategy for\nLi-Ion batteries. Not only is the MAE lower with the TSF ANN (11.44 cycle compared\nto 15.08 with the HF ANN), but the predictions are more reliable. Indeed, the standard\ndeviation of absolute error is lower with the TSF ANN, which means that there are less\naberrant predictions and that more prediction errors are closer to the MAE. Histograms\nof the absolute error are represented in ﬁgures 10 and 11 show that a greater number of\nprediction with the TSF ANN have an error between 0 and the MAE.\n80\n\nFig. 8. TSF ANN predicting performances\nFig. 9. HF ANN predicting performances\nFig. 10. TSF ANN absolute error histogram\nFig. 11. HF ANN absolute error histogram\nTable 1. Performance of the cycle-window based ANN according to the type of features\nMAE\nσMAE\nRMSE\nNMSE\nHistorical features\n15.08\n31.45\n34.88\n8.4*10−3\nTime series features\n11.44\n26.78\n29.16\n5.9*10−3\n5.4\nComparison with other approaches\nAlthough many papers in the literature mention their performances in the prediction of\nRUL, we can only compare our results with others that were obtained using the same\ndataset. For now, very few papers have based their approach on this dataset. The original\npaper [1] proposed a feature-based approach using a linear combination of the selected\nfeatures. The only other approach we have found using this dataset was proposed by a\nresearch group in an online application designed to predict the RUL and current cycle of\nany battery [10]. They have based their approach on a CNN.\nTable 2. Comparison of diﬀerent approaches in the literature\nRMSE\nMAE\nHistorical Cycle based ANN\n34.88\n15.08\nTSF Cycle based ANN\n29.16\n11.44\nLR from [15]\n173\nN/A\nCNN from [10]\nN/A\n115\nTable 2 compares the results obtained by all existing approaches with our best per-\nforming model. Although not all the same scoring measures were used in the two com-\nparative works, the available scores show that our approach outperforms the prediction\nperformances of the linear model developed by [1] and CNN developed by [10]. These\n81\n\nresults illustrate the fact that accurate prediction through machine learning needs a great\nnumber of training samples and a good feature extraction strategy. Designing a window\nbased approach at the scale of one cycle, and extracting features from driving curves is\nmore eﬃcient than building features from early cycles or from a temporal window over\nseveral consecutive cycles for a use in ANN.\n6\nConclusions\nThis paper is a description of our work on an innovative dataset published by the MIT,\ndealing with the ageing of Li-Ion batteries. Building a performing data-driven model re-\nlies essentially on the quality of data. With this work, we have proved that the dataset\nthat had triggered our attention contains highly valuable information, with features rep-\nresenting the ageing phenomenon both in the historical domain and time series domain\n(driving data). We propose a low computational cost technique with well-known machine\nlearning models such as artiﬁcial neural networks combined with features extraction tech-\nniques based on the exploitation of driving curves.\nOur results show that a basic approach can outperform more complex models such as\nCNN. With our one cycle window based approach, we take advantage of all the infor-\nmation contained in the dataset. The prediction of RUL can be made at any cycle when\ntesting a battery, and can above all be applied to cells whose current cycle is not known.\nFor future work, we plan to dig further in the same direction. The use of driving data\nappears to be crucial, and we believe that employing Recurrent Neural Networks that\nare particularly adapted to the study of temporal series and forecasting problems could\nimprove the performances of our models.\nReferences\n1. Severson, K.A., Attia, P.M., Jin, N., Perkins, N., Jiang, B., Yang, Z., Chen, M.H., Aykol,\nM., Herring, P.K., Fraggedakis, D., Bazant, M.Z., Harris, S.J., Chueh, W.C., Braatz, R.D.:\nData-driven prediction of battery cycle life before capacity degradation. Nature Energy 4(5)\n(2019) 383–391\n2. Wang, J., Wen, G., Yang, S., Liu, Y.: Remaining Useful Life Estimation in Prognostics Using\nDeep Bidirectional LSTM Neural Network.\nProceedings - 2018 Prognostics and System\nHealth Management Conference, PHM-Chongqing 2018 (2019) 1037–1042\n3. Downey, A., Lui, Y.H., Hu, C., Laﬂamme, S., Hu, S.: Physics-based prognostics of lithium-\nion battery using non-linear least squares with dynamic bounds. Reliability Engineering\nand System Safety 182(October 2018) (2019) 1–12\n4. Kozlowski, J.D., Byington, C.S., Garga, A.K., Watson, M.J., Hay, T.A.: Model-based predic-\ntive diagnostics for electrochemical energy sources. IEEE Aerospace Conference Proceedings\n6 (2001) 63149–63164\n5. Zhang, Y., Xiong, R., He, H., Liu, Z.: A LSTM-RNN method for the lithuim-ion battery\nremaining useful life prediction. 2017 Prognostics and System Health Management Confer-\nence, PHM-Harbin 2017 - Proceedings (20150098) (2017)\n6. Chen, C., Pecht, M.:\nPrognostics of lithium-ion batteries using model-based and data-\ndriven methods. Proceedings of IEEE 2012 Prognostics and System Health Management\nConference, PHM-2012 (2012) 1–6\n7. Li, X., Zhang, L., Wang, Z., Dong, P.:\nRemaining useful life prediction for lithium-ion\nbatteries based on a hybrid model combining the long short-term memory and Elman neural\nnetworks. Journal of Energy Storage 21(December 2018) (2019) 510–518\n8. Zhang, L., Mu, Z., Sun, C.: Remaining Useful Life Prediction for Lithium-Ion Batteries\nBased on Exponential Model and Particle Filter. IEEE Access 6 (mar 2018) 17729–17740\n82\n\n9. Zheng, S., Ristovski, K., Farahat, A., Gupta, C.: Long Short-Term Memory Network for\nRemaining Useful Life estimation. In: 2017 IEEE International Conference on Prognostics\nand Health Management, ICPHM 2017, Institute of Electrical and Electronics Engineers\nInc. (jul 2017) 88–95\n10. H. Knobloch, A. Frenk, W.C.: Predicting Battery Lifetime with CNNs (2019)\n11. Saha, B., Goebel, K., Christophersen, J.: Comparison of prognostic algorithms for estimating\nremaining useful life of batteries. Transactions of the Institute of Measurement and Control\n31(3-4) (2009) 293–308\n12. Bole, B., Kulkarni, C.S., Daigle, M.: Adaptation of an electrochemistry-based Li-ion battery\nmodel to account for deterioration observed under randomized use. PHM 2014 - Proceedings\nof the Annual Conference of the Prognostics and Health Management Society 2014 (2014)\n502–510\n13. Zhang, Y., Xiong, R., He, H., Pecht, M.G.:\nLong short-term memory recurrent neural\nnetwork for remaining useful life prediction of lithium-ion batteries. IEEE Transactions on\nVehicular Technology 67(7) (2018)\n14. Veeraraghavan, A., Adithya, V., Bhave, A., Akella, S.: Battery aging estimation with deep\nlearning.\n2017 IEEE Transportation Electriﬁcation Conference, ITEC-India 2017 2018-\nJanua (2018) 1–4\n15. Severson, K.A., Attia, P.M., Jin, N., Yang, Z., Perkins, N., Chen, M.H., Aykol, M., Herring,\nP., Fraggedakis, D., Bazant, M.Z., Harris, S.J., Chueh, W.C., Braatz, R.D.: (Supplementary\ninformation)Data-driven prediction of battery cycle life before capacity degradation\n16. Qu, J., Liu, F., Ma, Y., Fan, J.: A Neural-Network-Based Method for RUL Prediction and\nSOH Monitoring of Lithium-Ion Battery. IEEE Access 7 (2019) 87178–87191\n83\n\nAn Architecture to Quantify the Risk of AI-Models\nAlexander Melde1, Astrid Laubenheimer1, Norbert Link2, and Christoph Schauer2\n1 Karlsruhe University of Applied Sciences\nalexander.melde@h-ka.de astrid.laubenheimer@h-ka.de\n2 Inferics GmbH\nnorbert.links@inferics.com christoph.schauer@inferics.com\nAbstract. In this paper we propose a multi-step approach to quantify the risk of\nAI-models. To evaluate the quality of a learned AI-model for image classiﬁcation,\na previously unseen part of a dataset is classiﬁed and the predictions are compared\nwith their groundtruth to measure the accuracy of a model. In contrary, we ﬁrst\nsplit the test dataset into two parts based on how unambiguous each sample can\nbe assigned to a class. Samples that are close to the class decision boundary of\nmultiple learned models are considered particularly diﬃcult to classify. Second, we\ncreate a quantiﬁcation of the model’s ability to extrapolate on hard-to-classify or\nunseen data by training the model on “easy” data and evaluating it on the “diﬃ-\ncult” split. Inside our models, we project the data into a 3-dimensional space using\na neural network. We analyze this projection using the histogram of mutual dis-\ntances, the silhouette measure [1] and the entropy of it to assess the extrapolation\nquality and thus robustness of the model. Subsequently, we apply our approach to\nthe MNIST dataset [2] to prove its eﬀectiveness. We see that models trained only\non “easy” data are less robust than models trained on mixed data, which includes\n“diﬃcult” data that lies in-between classes. This behavior is evident in both our\nquantitative measurements and qualitative evaluation In this paper, after an in-\ntroduction to the topic and scope, related work is presented and the approach is\nexplained in general terms. Subsequently, the application of the approach to the\nMNIST dataset is described and the results of these experiments are presented.\nFinally, a conclusion is drawn and options for future work are given.\nKeywords: quality assurance, explainable AI, explainability, artiﬁcial intelli-\ngence, machine learning, explainable artiﬁcial intelligence, human activity recog-\nnition, action recognition, evaluation of AI systems, applications of AI in life\nsciences\n1\nIntroduction\nIn recent years, there has been increasing research on artiﬁcial intelligence (AI) methods\nin order to make our everyday lives easier and safer. For instance, assisted living in com-\nbination with outpatient care services has become increasingly popular as an alternative\nto nursing homes. In addition to the established emergency call systems, more and more\nsensor-based AI systems are entering the market. These systems can inform nursing staﬀ\nor trigger an alarm when they detect dangerous situations or unusual activities involving\nresidents. Further increases in popularity are to be expected for these systems as new\nAI-based technologies support safety and security for self-determined living in familiar\nsurroundings. In general, these technologies are based on machine learning (ML) mod-\nels trained on datasets whose quality of being representative for real world scenarios is\nunknown.\n84\n\nFor providers of such systems the introduction of new ML models into their products\nis of a high risk. The set of data on which the model can be evaluated in advance\nof the product launch commonly is not numerous enough and often generated under\nlaboratory conditions which do not represent the true conditions for the product in real\nuse. Furthermore, the performance of the models (from sight of economic eﬃciency) can\nnot securely be predicted in advance because well introduced measures such as precision\nand recall [3, 415] do not lead to a reliable estimation of the risk of failures such as false\nalarms or missed detections and the associated ﬁnancial cost.\nIn the context of systems that rely on activity recognition in domestic environments,\nthis leads to problems, when closely related activities have to be distinguished. As one\nexample “drinking a glass of water” and “brushing your teeth” are hard to distinguish,\nespecially when the decision has to be made in the absence of a semantic context. In\npractice, classiﬁers usually are trained and tested on data that represents both activities\nin a clearly distinguishable way, which often leads to models with high accuracy.\nIn the wild however, one has to expect situations that are positioned fuzzy between\nthe two classes and therefore hard to recognize. If such situations are not only underrep-\nresented in the training data but also in the test data, they are not suﬃciently considered\nin the quality assessment as well. Nevertheless, the behavior of the model on exactly these\nsituations deﬁnes the risk of the use of the model in a real-life system.\nIn this paper, we propose a framework to evaluate how models which are trained on\n“easy” data only perform on “diﬃcult” data which lies in-between classes. We demon-\nstrate its eﬀectiveness on the MNIST dataset [2].\n2\nRelated Work\nApproaches to assess model quality regardless of the distribution of the test data are\nprovided by the ﬁeld of Explainable Artiﬁcial Intelligence (XAI), which is a term to de-\nscribe methods that explain decisions made by AI algorithms. Many deep neural network\narchitectures are fundamentally black boxes whose decision-making is not comprehensi-\nble. Explainable AI attempts to make individual model executions or the entire model\ndecision strategy more transparent. By understanding the decisions of a model, its qual-\nity can be assessed more independently from the particular test data and misconceptions\narising from a limited scope of test data can be avoided. In recent years, numerous meth-\nods for XAI have been published. So far, these usually follow one of the following three\napproaches: First, the model can be built in a way that it is explainable naturally [4]\n(or by design, e.g. by using decision trees [5]), second original models can be replaced\nby ﬁtted surrogate models that allow local or global interpretations [6, 7] and third, ex-\nplanations can be generated using a direct process for either local or global explanations\nby putting a model into a more explainable state during training or by explaining single\npredictions, e.g. by determining the most important features for a certain decision [8, 9].\nAn extensive survey of XAI methods is given by Burkart and Huber in 2021 [9]. XAI\ntries to identify the risk of an AI model’s decision being wrong and questions decisions\nmade by the model. However, our approach does not explain the model itself, but instead\nattempts to quantify this risk. It describes the set of training data and how well it can\nbe used to describe a real-world problem, and therefore does not fall into any of these\ncategories.\nAnother related ﬁeld of research is coverage testing, a technique determining whether\nthe test cases used are actually covering the application area. Mani et. al. [10] explain\nthe necessity to measure the quality of a dataset beyond the standard accuracy measure\n85\n\n(proposing a set of four metrics to measure the coverage quality of a test dataset in the\nfeature space of a model) and propose and demonstrate the eﬀectiveness of a systematic\ntest case generation system (samples additional test cases from the feature space) [10, 1].\nThey propose four diﬀerent test quality dimensions that (1) measure the distribution of\ntest data across individual classes, (2) measure the percentage of test data for each class\nthat lies close to the centroid of the trained cluster or (3) near the boundary with respect\nto every other class of trained class clusters and (4) measure for each pair of classes the\npercentage of the boundary-condition (3) [10, 2]. This proposed system is evaluated on\nthe MNIST dataset as well. This approach is closely related to active learning, a term to\ndescribe methods that ﬁnd areas of the feature space that are not suﬃciently sampled\nand ask the user to add test data for this speciﬁc areas [11]. This might also be done\nby evaluating the density function in the feature space. In contrary to these approaches,\nwe do not measure the quality of coverage in the feature space but the quality of data\ndistribution in the latent space.\nA related approach that can be applied in both the latent and feature space is outlier\ndetection, which looks for data points outside the distribution of the dataset [12]. In\ncontrast, we evaluate quality using only the data points that lie between classes.\n3\nApproach\nOur proposed approach starts by splitting the test split of a dataset into two parts:\none of them representing data samples that can clearly be assigned to one class (“easy\ndata”) and the other one representing data samples that cannot be assigned to a class\nunambiguously due to its proximity to the class decision border in the latent space of\nthe network (“diﬃcult data”). The split can be found by means of a majority voting\napproach across several proven model architectures.\nIn the second step, we quantify the ability of arbitrary models to extrapolate from\n“easy” to “diﬃcult” data, by training the model on “easy” data and evaluating it on the\n“diﬃcult” split, which then leads to a quantiﬁcation of the models ability to extrapolate\nonto hard to classify or unseen data. For our approach we assume that the model consists\nof an embedder backbone network which projects the data into the latent space and\na classiﬁer head evaluating the projections. The extrapolation quality of the model is\nassessed by analyzing the projections in the latent space, where the histogram of mutual\ndistances is analyzed. The silhouette measure [1] and the entropy are used to compare\nthe model performance on the “easy” and “diﬃcult” datasets as well as to measure the\nextrapolation power of the model.\nThe steps are visually summarized in Fig. 1 and described in detail below.\n3D Plots\nSilhouette\nEntropy\nsorting method\n(sort data by\ndifficulty)\neasy\ndifficult\ndataset\nexperiments\nAcurracy\nFig. 1: Simple Overview of our architecture\n86\n\nTo split the dataset by diﬃculty, we train seven diﬀerent models using the same\noriginal training split but each time a diﬀerent network architecture. We are using con-\nvolutional neural networks (CNNs) [13] of various complexity.\nFor each learned model, the test set is predicted and compared with the ground truth.\nWe count the number of incorrect predictions per sample image of the test set to quantify\nthe diﬃculty of each sample. The higher this number is, the more diﬃcult this sample was\nto predict. We then group all of these samples into the two datasets which we consider\n“easy” and “diﬃcult”.\nWe train and test diﬀerent combinations of dataset splits for test and training, loss\nfunctions, network architectures and number of epochs to measure the accuracy [3, 101]\nof diﬀerent combinations to ﬁnally be able to ﬁnd the “best working” combination in the\ntraditional sense.\nIn order to measure robustness, we ﬁrst create a visual representation of the model.\nFor this we train multiple deep neural networks (with diﬀerent dataset diﬃculties). We\nanalyze the latent space of a model, which represents the training quality of a model\nand will serve as an important indicator for its robustness. The samples of each class\nare expected to form clusters in this space. We use both this visual representation of the\n3-dimensional latent space and the silhouette histogram [1] and its entropy to measure\nthe separability of these clusters.\nThe silhouette value of an object ranges between −1 and 1. Higher values mean the\nobject is positioned better in the clustered space. The distribution of these values for\na certain set of objects can serve as an indicator of the quality of the clustering. The\nsilhouettes are calculated as\ns(⃗x) =\nb(⃗x) −a(⃗x)\nmax{a (⃗x) , b (⃗x)}\n(1)\nwith a(⃗x) being the average dissimilarity (distance) of an object ⃗x to all other objects of\nits own cluster X and b(⃗x) being the minimum average dissimilarity of ⃗x to all objects of\nall other clusters Cx [1, 55]. To quantize this primarily visual measurement, we calculate\nthe entropy of the frequency distribution of the calculated silhouette using the following\nformula:\nS =\n\u0005\ni\n(pi ∗log2(pi))\n(2)\nwith each pi being a bin of the histogram representing the silhouette values. In our\nexamples we used 500 bins because it provided well distinguishable visual results, neither\nbeing too abstract nor too detailed to visually assess separation quality.\nA well separated cluster will lead to a more robust implementation and a lower risk\nwhen applied to a real-world use case. These “easy” situations are characterized by a\nsilhouette diagram where most values are concentrated in a peak on the right hand\nside (see Fig. 2a), leading to a low entropy value. In contrast, for “diﬃcult” situations\nthe silhouettes values are spread across the histogram (see Fig. 2b) leading to a higher\nentropy.\n4\nApplication to MNIST\nIn this paper our method is applied on MNIST, a dataset of 70.000 ﬁxed-size images of\nsize-normalized and centered handwritten digits. It was created in 1998 by LeCun et.\nal. [2] based on a subset of the NIST handprinted forms and characters dataset [14].\n87\n\n(a) The model is trained with both “easy” and\n“diﬃcult” data, the silhouette has a peak on\nthe right side.\n(b) The model is trained with “easy” data, the\nsilhouette values are spreaded across the cosine\nsimilarity spectrum.\nFig. 2: Visual distinction of training dataset diﬃculty based on the silhouette. The same\nnetwork was trained using diﬀerently diﬃcult dataset splits and each time tested with\nthe same previously unseen “diﬃcult” data.\nThe complete process of our approach applied to the MNIST dataset is shown in the\narchitecture diagram (see Fig. 3).\nStep1-ML-Classifier\nStep1-Evaluation\nStep2-Evaluation\n(very easy)\n(very difficult)\ntest\ntest\nMeasurement for the models decision making reliability\nStep1-ML-Classifier\n(with different network architecture)\ntest\nMNIST-Test-sorted\nTraining\nTest\n...\n3D Plots\nSilhouette\nHistograms\nEntropy\nStep2-ML-Classifier\nStep2-ML-Classifier\nStep2-Evaluation\ntraining\nimages\nlabels\nTest - Split\nAggregator\n(counts number of incorrect predictions per sample)\nincorrect\npredictions\nSplit by Difficulty/Separability (Treshhold-based)\nStep1-Evaluation\nTraining\nPreprocessing\nNeural Network\n(n, 28, 28)[0, 1]\nlearned\nmodel\npredicted\nlabels\n(n)[0-9]\nModel inference\n(classification /\nprediction)\nComparison of\nprediction and\nground truth\nimages\nlabels\nStep2-ML-Classifier\nStep2-ML-Classifier\nimages\nlabels\nTest - Split\nTraining\nimages\nlabels\nTrain - Split\npredictions (n)[0-9]\nModel\ninference\n(classification)\nlearned model\nLoss (Distance) Function \n(e.g. ArcFace, SoftMax)\nAccuracy\nMNIST-Test\n...\nTrain - Split\nPreprocessing\nMT-di\nMT-ez\nNeural Network\nInput Layer\nHidden Layers\nOutput Layer\n(n, 28, 28) [0-255]\nrepresentation of\nlatent feature space\nduring inference\n(n) [0-1]3\nStep2-Evaluation\nStep2-Evaluation\nSorting and Splitting a Dataset by Difficulty\nMeasuring Robustness (in latent space)\nMNIST-Train\nFig. 3: Detailed overview of our architecture applied to the MNIST dataset\nWe use the original MNIST training and test split from the ‘keras.datasets‘ library\nand apply common preprocessing steps to reshape the list of pixels to a multidimensional\narray of shape (28,28,1), then rescale all values between 0 and 1 and ﬁnally one-hot encode\nthe class labels.\nFor splitting the dataset by diﬃculty we use various architectures taken from popular\npublications and blog posts for image classiﬁcation [15–17]. An exemplary selection of\nthese is shown in Fig. 4 to visualize the diﬀerences in their approaches and complexity\n(diﬀerent amount, selection, arrangement and sizes of layers and ﬁlters).\nThe trained deep neural networks for determining robustness are based on a version of\nthe established VGG architecture [17]. We use the VGG8 architecture shown in Fig. 4c,\nwhich, unlike VGG16, allows us to process images smaller than 32x32px [17], which\nis important when applying this method to the MNIST dataset, of which the images\nhave a size of 28x28px [2]. The architecture is retrieved from a repository containing an\nimplementation of the ArcFace loss function [18], which we will use to visualize what\nthe model has learned. By removing the output layer of the tested deep neural network\narchitecture (SoftMax/ArcFace Layer in Fig. 4c), we can directly access the last dense\nlayer representing the latent space of the model.\n88\n\nMax Pooling 2D (2,2)\nDroupout (0.5)\nDense\nConvolution 2D (5,5)\nMax Pooling 2D (2,2)\nConvolution 2D (5,5)\n28\n28\n1\n24\n24\n32\n10\nFlatten\n1024\nReLu\nReLu\nReLu\n64\n4\n4\n64\n8\n8\n32\n12\n12\n(a) CNN1 [15]\nDense\nConvolution 2D (3,3)\nConvolution 2D (3,3)\n28\n28\n1\n26\n26\n32\n32\n24\n24\n10\nFlatten\nSoftMax\nBatch Normalization\nReLu\nConvolution 2D (3,3)\nBatch Normalization\nReLu\nBatch Normalization\nReLu\nDropout (0.4)\nConvolution 2D (3,3)\nConvolution 2D (3,3)\nBatch Normalization\nReLu\n1024\nConvolution 2D (3,3)\nBatch Normalization\nReLu\nBatch Normalization\nReLu\nDropout (0.4)\nBatch Normalization\nDropout (0.4)\nReLu\nDense\n32\n12\n12\n64\n10\n10\n64\n8\n8\n64\n4\n4\n128\n(b) CNN2 [16]\nMax Pooling 2D (2,2)\nDroupout (0.5)\nFlatten\nDense\nBatch Normalization\nReLu\nConvolution 2D (3,3)\nMax Pooling 2D (2,2)\nBatch Normalization\nReLu\nConvolution 2D (3,3)\nMax Pooling 2D (2,2)\nBatch Normalization\nReLu\nConvolution 2D (3,3)\n28\n28\n1\n28\n28\n16\n16\n16\n14\n14\n32\n14\n14\n3\n3\n64\n576\n× 2\n× 2\n× 2\n3\nSoftMax / ArcFace\nDense\n10\nBatch Normalization\n64\n7\n7\n32\n7\n7\n(c) VGG8 (VGG16: [17])\nFig. 4: Selected network architectures used in our experiments. Network layers are colored\ngreen, the dimension of data between layers is visualized by yellow boxes. The size of the\nconvolutional ﬁlters is given in parentheses after the layer name (width, height), similar\nto the percentage of the drop-out in the corresponding layers.\nIn our experiments, we are measuring the dissimilarity of two objects d(⃗x, ⃗y) using\nthe cosine function where ⃗x · ⃗y is the dot product of ⃗x and ⃗y:\nd(⃗x, ⃗y) = 1 −\n⃗x · ⃗y\n||⃗x||2 ||⃗y||2\n.\n(3)\nConsisting of very basic mathematical functions, it is computationally simple and in\nour experiments provided well distinguishable results in the silhouette graphs.\n5\nResults\nWe apply the proposed method on the well-known MNIST dataset of handwritten digits\n[2] to demonstrate the eﬀectiveness of our proposed approach.\nTo determine the diﬃculty of the image samples, seven diﬀerent CNN-based classiﬁ-\ncators were trained and used to classify the test dataset. We consider every element that\nwas incorrectly classiﬁed at least once as part of the “diﬃcult” data split, leading to a\nﬁnal split of 9632 “easy” and 368 “diﬃcult” images (colored green and orange in Fig. 3).\nNext, the classiﬁers shown in Fig. 4 are trained with the “easy” data and then were\nvalidated using the “diﬃcult” data split and the standard accuracy measurement. The\nresults in Table 1 indicate that the diﬀerent network architectures have diﬀerent learning\ncurves and in general perform diﬀerently well, pointing out they diﬀer enough to ensure\na reliable selection of network architectures to measure dataset sample diﬃculty.\n89\n\nTable 1: Testing accuracy when training with “easy” and testing with “diﬃcult” data.\nValidation Accuracy after\nTraining Accuracy after\nNetwork\n7 Epochs\n100 Epochs\n7 Epochs\n100 Epochs\nCNN1\n0.56739\n0.78190\n0.99686\n0.99956\nCNN2\n0.37174\n0.84733\n0.99476\n0.99967\nVGG8\n0.45435\n0.78408\n0.97935\n0.99978\nTo visualize the classiﬁcation confusion, the predicted classes of the VGG8 classiﬁer\nare grouped by their groundtruth classes in Fig. 5a) in the form of a confusion matrix.\nA confusion matrix representing a classiﬁer with high accuracy shows high values on the\nmain diagonal and small values outside of it. We consider training with the “easy” data\nand testing the “diﬃcult” data as a diﬃcult task, which explains why in this case high\nnumbers appear outside the diagonal. The highest number of confusions is given for the\ncase that a handwritten digit 9 is classiﬁed as 5, but we also see that the number 9 is\ngenerally over-represented in the “diﬃcult” dataset. Example images of these confused\nMNIST digits can be seen in the other images of Fig. 5, labeled with the groundtruth\nfollowed by predictions of two classiﬁcators used in our experiments.\n(a) Confusion matrix of ﬁnal VGG8 classiﬁer\n(b) 4: 4/9\n(c) 7: 7/4\n(d) 7: 7/9\n(e) 9: 9/7\nFig. 5: Confusion matrix of MNIST images and sample images [2], their groundtruths\nand predictions made by one (a) or two (b – e) classiﬁcators. The label of images b – e\nshow the ground truth followed by the diﬀerent predictions of the two classiﬁcators.\nFor all following experiments, we will use both the traditional SoftMax loss [3, 181]\nas well as the ArcFace loss [18].\nTo visualize what the model has learned, the latent space is plotted in a 3D space.\nAs expected, the diﬀerent MNIST classes form clusters in this space. Datasets varying in\ndiﬃculty will lead to diﬀerently well separated clusters. The better the clusters are sepa-\nrated, the more robust the model should be. This 3D visualization of the learned clusters\nis shown in Fig. 6. For both losses, diﬀerent combinations of training and test datasets\nare shown. This overlapped view is useful to detect outliers arising from diﬃcult data.\nFrom this visualization alone, we can see that when training with not only the “easy”\nimages, but the complete train dataset (which also includes more diﬃcult elements), a\nfar better separability could be achieved. We can also notice that in our experiments,\n90\n\nusing the ArcFace loss [18] resulted in better separated clusters than using the SoftMax\nloss. This pattern is observed independently of the number of training epochs.\n(a) ArcFace [18] loss function\n(b) SoftMax [3, 181] loss function\nFig. 6: Visualization of learned features in the latent space when trained for 100 epochs\nusing diﬀerent loss functions and diﬀerent combinations of train and test datasets. Dif-\nferent colors represent diﬀerent classes of the MNIST [2] dataset, diﬀerent symbols are\nused to represent the diﬀerent dataset combination.\nDiﬀerent Splits of diﬃculty lead to the distributions of silhouettes shown in Fig. 7. A\nhigh silhouette means an item is located near the center of the learned cluster.\nThe quantitative results of our experiments (shown in Table 2) and the visualized\nsilhouette values (see Fig. 7) show the generally better performance of ArcFace over\nSoftMax and the beneﬁts of training more epochs.\nFig. 7: Histograms of the silhouette values after training for 200 epochs with the SoftMax\nand ArcFace loss. Using ArcFace leads to more high silhouette values (especially in the\nrightmost bin) representing a better separation for all combinations of test and training\ndatasets.\nIn general, if an image sample has a high silhouette value, it is well positioned in\nthe clustered feature space. We can see that combinations that are deemed “diﬃcult”\nlead to a wider distribution, e.g. training with easy images and testing diﬃcult images,\nwhile “easy” combinations lead to a peak near the high end of the histogram. A wide\ndistribution shows that there is no clear tendency to how good the elements are clustered,\nbut a peak on the right side shows that most elements are clustered well. This is also\n91\n\nTable 2: Entropy values measured in our experiments. A low entropy is an indicator of\ngood cluster separation in the latent space.\nDataset for\nSoftMax Entropy after\nArcFace Entropy after\ntraining\ntesting\n7\n100\n200\nEpochs\n7\n100\n200\nEpochs\noriginal train\neasy test\n2.09\n1.65\n1.57\n2.13\n0.39\n0.30\noriginal train\noriginal test\n2.27\n1.80\n1.71\n2.37\n0.57\n0.40\noriginal train\ndiﬃcult test\n4.89\n4.45\n4.21\n4.91\n3.24\n3.38\neasy train\ndiﬃcult test\n5.39\n4.96\n5.09\n5.52\n4.99\n4.81\nrepresented in the entropy value of each graph, shown in Table 2. The wider the graph,\nthe higher the entropy. The smallest value for the “easy” train and “diﬃcult” test scenario\nis given when training with ArcFace for 200 epochs. Overall, we see that in most cases\ntraining using the SoftMax Loss is less eﬀective than training with the ArcFace Loss. This\nis also shown in the histogram in Fig. 7, as with SoftMax the classic “blue” combination\nof training and testing using the oﬃcial dataset splits shows a wider peak at the right side\nthan with ArcFace, visualizing that more elements are worse positioned in the feature\nspace.\n6\nConclusion\nWe proposed a framework to separate a dataset into diﬀerent levels of diﬃculty using\na majority voting approach and evaluated how models which are trained on the “easy”\ndata split only perform when the “diﬃcult” data split is used for testing. We ensure a\nboth qualitative and quantitative measurement by evaluating plots of the latent space\nand silhouette histograms as well as the entropy value of the silhouette histogram.\nThis theoretical approach has been applied to the MNIST dataset [2] to prove its eﬃ-\nciency. The results of the experiments are as expected: training with a dataset consisting\nof only “easy” data leads to less robust models than training with the full dataset that\nalso contains “diﬃcult” samples. We have also proven that the entropy of the silhouette\nmeasure histogram and both the visualizations are useful to determine the robustness of\nan AI-model. During these experiments, we also measured that using the ArcFace loss\ninstead of SoftMax leads to a better clustering and therefore more robust models in most\ncases.\n7\nFuture Work\nIn future work, this approach will be transferred to more complex applications using\naction recognition to support the use cases in the area of life sciences. For this we will\nuse an activity recognition video dataset and matching machine learning models for video\naction classiﬁcation.\nIndependently, in future projects, the majority voting approach can be replaced by\nusing the silhouette coeﬃcient to separate “easy” and “diﬃcult” samples. The assumption\nthat a high silhouette value implies that the data sample is easy to classify will result in\nshorter training time, as the number of required classiﬁcators is reduced.\n92\n\nReferences\n1. Rousseeuw, P.J.:\nSilhouettes: A Graphical Aid to the Interpretation and Validation of\nCluster Analysis. Journal of computational and applied mathematics 20 (1987) 53–65\n2. LeCun,\nY.:\nThe\nMNIST\ndatabase\nof\nhandwritten\ndigits.\nhttp://yann.\nlecun.\ncom/exdb/mnist/ (1998)\n3. Goodfellow,\nI.,\nBengio,\nY.,\nCourville,\nA.:\nDeep\nLearning.\nMIT\nPress\n(2016)\nhttp://www.deeplearningbook.org.\n4. Loh, W.Y.: Classiﬁcation and regression trees. Wiley Interdisciplinary Reviews: Data Mining\nand Knowledge Discovery 1(1) (2011) 14–23\n5. Yang, Y., Morillo, I.G., Hospedales, T.M.:\nDeep neural decision trees.\narXiv preprint\narXiv:1806.06988 (2018)\n6. Ribeiro, M.T., Singh, S., Guestrin, C.: ”Why should i trust you?” Explaining the predictions\nof any classiﬁer. In: Proceedings of the 22nd ACM SIGKDD international conference on\nknowledge discovery and data mining. (2016) 1135–1144\n7. Lundberg, S.M., Lee, S.I.:\nA Uniﬁed Approach to Interpreting Model Predictions.\nIn:\nProceedings of the 31st international conference on neural information processing systems.\n(2017) 4768–4777\n8. ˇStrumbelj, E., Kononenko, I.: Explaining prediction models and individual predictions with\nfeature contributions. Knowledge and information systems 41(3) (2014) 647–665\n9. Burkart, N., Huber, M.F.: A Survey on the Explainability of Supervised Machine Learning.\nJournal of Artiﬁcial Intelligence Research 70 (2021) 245–317\n10. Mani, S., Sankaran, A., Tamilselvam, S., Sethi, A.:\nCoverage Testing of Deep Learning\nModels using Dataset Characterization. arXiv preprint arXiv:1911.07309 (2019)\n11. Settles, B.: Active Learning Literature Survey, University of Wisconsin-Madison Depart-\nment of Computer Sciences (2009)\n12. Wang, H., Bah, M.J., Hammad, M.: Progress in Outlier Detection Techniques: A Survey.\nIeee Access 7 (2019) 107964–108000\n13. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,\nL.D.: Backpropagation Applied to Handwritten Zip Code Recognition. Neural computation\n1(4) (1989) 541–551\n14. Grother, P.J.: NIST special database 19-Hand-printed forms and characters database. Tech-\nnical Report, National Institute of Standards and Technology (1995)\n15. Leban,\nJ.:\nImage\nrecognition\nwith\nMachine\nLearning\non\nPython,\nCon-\nvolutional\nNeural\nNetwork.\nTowards\nData\nScience\n(2020)\nretrieved\nfrom\nhttps://towardsdatascience.com/363073020588 on 03.08.2021.\n16. Deotte, C.:\nWhat is the best CNN architecture for MNIST?\nKaggle notebook (2018)\nretrieved from https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist on\n03.08.2021.\n17. Simonyan, K., Zisserman, A.: Very Deep Convolutional Networks for Large-Scale Image\nRecognition. arXiv preprint arXiv:1409.1556 (2014)\n18. Deng, J., Guo, J., Xue, N., Zafeiriou, S.: ArcFace: Additive Angular Margin Loss for Deep\nFace Recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. (2019) 4690–4699\nAcknowledgements\nWe thank J. Wetzel and T. Iraki for helpful comments and discussion. This work was\nfunded by the Ministry of Science, Research and Arts of Baden-W¨urttemberg (MWK) as\npart of the project Q-AMeLiA (Quality Assurance of Machine Learning Applications).\n93\n\nExercises in Human-Centered AI: On Shneiderman’s Second \nCopernican Revolution \nDieter Wallach1, Lukas Flohr2, Annika Kaltenhauser2, Sven Fackert1\n1 Hochschule Kaiserslautern \ndieter.wallach@hs-kl.de, sven.fackert@hs-kl.de \n2 Ergosign GmbH \nannika.kaltenhauser@ergosign.de, lukas.flohr@ergosign.de \nAbstract. In his stimulating paper “Human-centered Artificial Intelligence” Shneiderman \n(2020) reframed AI research and application development by putting human users at the \ncenter of system design. Shneiderman requests a reunification of the view where humans \nare in the loop around algorithms and AI and suggests putting AI in the loop around humans, \nwith a dedicated focus on the needs of users. From a UX designer’s point of view, we \ndiscuss the idea of putting humans at the center and illuminate the implications of \nShneiderman’s arguments by referring to projects from our lab. By this, we emphasize the \nrole of empirical research, (collaborative) UX Design, and evaluation in the development \nof human-centered AI systems. Our case studies exemplify a human-centered design \napproach of AI-injected systems in different domains and carve out the core learnings we \ngathered. The first case study, IMEDALytics, is taken from a project targeted at the \ndevelopment of a clinical decision-support system (CDSS) for individualized medical risk \nassessment, monitoring, and therapy management in intensive care medicine. We select this \nproject to illustrate the indispensable nature of ethnographic user research to arrive at a \nholistic understanding of user needs. By visualizing the results of contextual observations \nand interviews in comprehensive user journeys, we shift the focus from problem solving \nthrough technology to the design of experience potentials (see Hassenzahl, 2010). We argue \nthat it is paramount to present information to physicians in an unambiguous and \nunderstandable way, which classifies the task as an Explainable AI example in which \nanswers to the following questions need to be derived: \n•\nHow can we combine human abilities of healthcare professionals – such as their\ngeneral understanding, previous experiences, flexibility and creativity in the decision-\nmaking process  – with the powerful possibilities of an AI-based system?\n•\nHow can we make diagnosis and therapy suggestions provided by the system\naccessible to healthcare professionals without depriving their self-efficacy?\n•\nWhich design processes are needed to design an interactive interface that leads to a\nlong-term positive UX?\n•\nWhich influence has (the type of) presented information  – e.g., in the form of\ninformation visualizations  – on the perceived transparency or even trust in a CDSS?\nOur second example focuses on the development of real-world autonomous mobility-on-\ndemand (AMoD) public buses and required services for their operation. In autonomous \nmobility-on-demand systems, passengers are transported by self-driving cars, i.e., by \nvehicles with high or full driving automation capabilities. Comparable to taking a ride in a \n(shared) taxi, journeys in AMoD systems are temporally and spatially flexible. This means \nthat there are neither fixed timetables nor fixed pick-up or drop-off locations required. \nGiven that there is neither a driver nor an accompanying assistant available to answer \n94\n\ntraveler queries, AMoD rides vary greatly from current mobility-on-demand or taxi \nservices. This new situation of riding in a driverless vehicle might feel awkward to \npassengers who are exposed to the decisions and actions of an autonomous system. \nConsequently, user interfaces capable of compensating the absence of a human driver are \nneeded to establish a trustful AV-passenger communication. To counteract these \nchallenges, we had to find answers to questions that include the following: \nx\nHow can and how should AI-infused AMoD systems communicate with\npassengers?\nx\nHow can we design and evaluate user interfaces while taking the complete user\nexperience  – before, during and after a ride  – into account?\nIn our talk, we illustrate the applied formative design approach that is used for an iterative \nrefinement of mobile and in-vehicle passenger UI prototypes (GUI- and Chatbot-based) and \ntheir subsequent empirical evaluation in simulators with increasing fidelity — ranging from \nvideo-based lab setups to real-word (Wizard-of-Oz) on-demand drives. In our third case \nstudy, AI science and AI engineering (Shneiderman, 2020) — i.e., emulating human \nbehavior and developing useful applications — are combined in the creation of an AI-based \n(predictive) prototyping tool. The resulting tool allows the creation of complex interactive \nprototypes for which quantitative performance predictions are derived by running cognitive \nmodels. Such models are automatically generated by monitoring a designer’s interaction \nwhile completing a task scenario using a prototype. The underlying models are based on \nthe ACT-R cognitive architecture (Anderson & Lebiere, 2020) comprising hybrid (i.e., \nsymbolic and sub-symbolic) structures and processes. ACT-R is a prominent example of a \nUnified Theory of Cognition integrating empirically supported assumptions about the \ninterplay between human memory, learning, attention, perception and motor behavior \nwhich has been successfully applied to a broad range of tasks. The generated models can \ndirectly interact with a prototype, perceive its interface elements, learn task interactions and \nbe able to manipulate the state of controls. In our talk we demonstrate that — by generating \nthe behavior of synthetic participants — we can successfully predict human performance \nin real-world tasks ranging from mobile phone applications to the operation of commercial \nriveting machines in aviation industry. By using the very same tool, interface designers can \ncreate a prototype and receive almost instant evidence about its interactional efficiency by \nrunning artificial user models. Predictive prototyping opens the potential to significantly \nshorten iteration cycles by providing quantitative performance KPIs without the need to \nconduct effortful user studies. In the light of the case studies presented, we argue that the \nmethodological apparatus of UX research and collaborative design practices contributes to \nthe development of “human-centered” AI-based systems that result in positive user \nexperiences — and thus increase the likelihood of adoption of AI-based systems in practice.  \nThis work has been funded by the German Federal Ministry of Education and Research \n(BMBF) under the grant numbers 13GW0280B and 02L15A212 as well as by the German \nFederal Ministry of Transport and Digital Infrastructure (BMVI) under the grant number \n16AVF2134G. \nKeywords: User Experience Design, User Research, Prototyping, Autonomous driving, \nClinic decision support systems, Evaluation, Human-Computer Interaction \n95\n\n,QWHUSUHWDEOH\u00030DFKLQH\u0003/HDUQLQJ\u0003IRU\u00034XDOLW\\\u0003(QJLQHHULQJ\u0003LQ\u0003\n0DQXIDFWXULQJ\u0003\u0010\u0003,PSRUWDQFH\u00030HDVXUHV\u0003WKDW\u00035HYHDO\u0003,QVLJKWV\u0003RQ\u0003\n(UURUV\u0003\n+ROJHU\u0003=LHNRZ\u000f\u00038OI\u00036FKUHLHU\u000f\u0003$OH[DQGHU\u0003*HUOLQJ\u000f\u0003$ODD\u00036DOHK\u0003\n%XVLQHVV\u0003,QIRUPDWLRQ\u00036\\VWHPV\u000f\u0003)XUWZDQJHQ\u00038QLYHUVLW\\\u0003RI\u0003$SSOLHG\u00036FLHQFH\u000f\u0003\n\u001a\u001b\u0014\u0015\u0013\u0003)XUWZDQJHQ\u000f\u0003*HUPDQ\\\u0003\n^KROJHU\u0011]LHNRZ\u000fXOI\u0011VFKUHLHU\u000fDOH[DQGHU\u0011JHUOLQJ\u000fDODD\u0011VDOHK`\u0003\n#KV\u0010IXUWZDQJHQ\u0011GH\u0003\n$EVWUDFW\u0011\u00037KLV\u0003SDSHU\u0003DGGUHVVHV\u0003WKH\u0003XVH\u0003RI\u0003PDFKLQH\u0003OHDUQLQJ\u0003DQG\u0003WHFKQLTXHV\u0003RI\u0003LQWHUSUHWDEOH\u0003\nPDFKLQH\u0003 OHDUQLQJ\u0003 WR\u0003 LPSURYH\u0003 TXDOLW\\\u0003 LQ\u0003 PDQXIDFWXULQJ\u0003 SURFHVVHV\u0011\u0003 ,W\u0003 SURSRVHV\u0003 DQDO\\VLV\u0003\nPHWKRGV\u0003RQ\u0003WRS\u0003RI\u00036+$3\u0003YDOXHV\u0003WR\u0003HOLFLW\u0003XVHIXO\u0003LQVLJKWV\u0003IURP\u0003PDFKLQH\u0003OHDUQLQJ\u0003PRGHOV\u0011\u0003\n7KHVH\u0003PHWKRGV\u0003FRQVWLWXWH\u0003QRYHO\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003WKDW\u0003VXSSRUW\u0003TXDOLW\\\u0003HQJLQHHUV\u0003LQ\u0003WKH\u0003\nDQDO\\VLV\u0003RI\u0003SURGXFWLRQ\u0003HUURUV\u0011\u0003:H\u0003LOOXVWUDWH\u0003DQG\u0003WHVW\u0003WKH\u0003SURSRVHG\u0003PHWKRGV\u0003RQ\u0003V\\QWKHWLF\u0003DV\u0003\nZHOO\u0003DV\u0003RQ\u0003UHDO\u0010ZRUOG\u0003GDWD\u0003IURP\u0003D\u0003*HUPDQ\u0003PDQXIDFWXUHU\u0011\u0003\n.H\\ZRUGV\u001d\u0003,QWHUSUHWDEOH\u00030DFKLQH\u0003/HDUQLQJ\u001e\u0003,PSRUWDQFH\u0003PHDVXUHV\u001e\u00030DQXIDFWXULQJ\u0011\u0003\n\u0014\u0003 ,QWURGXFWLRQ\u0003\n5HFHQW\u0003 DGYDQFHPHQWV\u0003 LQ\u0003 PDFKLQH\u0003 OHDUQLQJ\u0003 KDYH\u0003 FUHDWHG\u0003 LQFUHDVHG\u0003 LQWHUHVW\u0003 LQ\u0003 OHYHUDJLQJ\u0003 WKH\u0003\nSRWHQWLDO\u0003RI\u0003WKLV\u0003WHFKQRORJ\\\u0003LQ\u0003D\u0003UDQJH\u0003RI\u0003DSSOLFDWLRQ\u0003GRPDLQV\u0011\u0003:LWKLQ\u0003WKLV\u0003SDSHU\u0003ZH\u0003DGGUHVV\u0003WKH\u0003\nDSSOLFDWLRQ\u0003RI\u0003PDFKLQH\u0003OHDUQLQJ\u0003WR\u0003TXDOLW\\\u0003HQJLQHHULQJ\u0003LQ\u0003PDQXIDFWXULQJ\u0011\u0003+HUH\u000f\u0003WKH\u0003JRDO\u0003LV\u0003WR\u0003\nOHYHUDJH\u0003PDFKLQH\u0003OHDUQLQJ\u0003IRU\u0003LGHQWLI\\LQJ\u0003DQG\u0003UHGXFLQJ\u0003FDXVHV\u0003RI\u0003SURGXFWLRQ\u0003HUURUV\u0011\u0003/LNH\u0003LQ\u0003RWKHU\u0003\nDSSOLFDWLRQV\u000f\u0003WKH\u0003EODFN\u0010ER[\u0003QDWXUH\u0003RI\u0003PDQ\\\u0003PDFKLQH\u0003OHDUQLQJ\u0003DSSURDFKHV\u0003SRVHV\u0003FKDOOHQJHV\u0003WR\u0003WKH\u0003\nDSSOLFDELOLW\\\u0011\u0003 7KLV\u0003 FUHDWHV\u0003 WKH\u0003 QHHG\u0003 IRU\u0003 PHWKRGV\u0003 WR\u0003 H[SODLQ\u0003 PDFKLQH\u0003 OHDUQLQJ\u0003 PRGHOV\u0011\u0003\n6SHFLILFDOO\\\u000f\u0003WKHUH\u0003DUH\u0003WZR\u0003SUHYDOHQW\u0003UHDVRQV\u0003IRU\u0003H[SODLQLQJ\u0003PDFKLQH\u0003OHDUQLQJ\u0003PRGHOV\u0003LQ\u0003TXDOLW\\\u0003\nPDQDJHPHQW\u0003IRU\u0003PDQXIDFWXULQJ\u0011\u00032QH\u0003LV\u0003WR\u0003JDLQ\u0003WUXVW\u0003LQ\u0003WKH\u0003PRGHO\u0003GHFLVLRQV\u0011\u00037KH\u0003RWKHU\u0003LV\u0003WR\u0003\nOHYHUDJH\u0003LQVLJKWV\u0003RI\u0003WKH\u0003PRGHO\u0003WR\u0003GULYH\u0003KXPDQ\u0003GDWD\u0003DQDO\\VLV\u0011\u00037KDW\u0003LV\u000f\u0003TXDOLW\\\u0003HQJLQHHUV\u0003ZDQW\u0003WR\u0003\nEH\u0003SRLQWHG\u0003WR\u0003IDFWRUV\u0003RU\u0003FRPELQDWLRQ\u0003RI\u0003IDFWRUV\u0003WKDW\u0003KHOS\u0003WR\u0003XQGHUVWDQG\u0003DQG\u0003UHGXFH\u0003SURGXFWLRQ\u0003\nHUURUV\u0011\u0003,Q\u0003WKLV\u0003SDSHU\u0003ZH\u0003H[SORUH\u0003H[LVWLQJ\u0003DQG\u0003QHZ\u0003PHWKRGV\u0003RI\u0003H[SODLQDEOH\u0003PDFKLQH\u0003OHDUQLQJ\u0003WR\u0003\nDGGUHVV\u0003WKLV\u0003QHHG\u0011\u0003\n5HFHQWO\\\u000f\u0003D\u0003UDQJH\u0003RI\u0003PHWKRGV\u0003KDYH\u0003EHHQ\u0003SURSRVHG\u0003WKDW\u0003DUH\u0003LQWHQGHG\u0003WR\u0003PDNH\u0003EODFN\u0010ER[\u0003PDFKLQH\u0003\nOHDUQLQJ\u0003PRGHOV\u0003H[SODLQDEOH\u0003\u000bVHH\u0003H\u0011J\u0011\u0003*LOSLQ\u0003HW\u0003DO\u0011\u0003>\u0014@\u0003RU\u00030ROQDU\u0003>\u0015@\u0003IRU\u0003DQ\u0003RYHUYLHZ\f\u0011\u0003$PRQJ\u0003\nWKH\u0003H[LVWLQJ\u0003PHWKRGV\u0003DUH\u0003VHYHUDO\u0003PHDQV\u0003WR\u0003DVVHVV\u0003IHDWXUH\u0003LPSRUWDQFH\u0011\u0003)HDWXUH\u0003LPSRUWDQFH\u0003FDQ\u0003EH\u0003\nXVHG\u0003DV\u0003JXLGDQFH\u0003IRU\u0003KXPDQV\u0003DERXW\u0003ZKHUH\u0003WKH\\\u0003VKRXOG\u0003IRFXV\u0003WKHLU\u0003DQDO\\VLV\u0003RI\u0003WKH\u0003GDWD\u0011\u0003+RZHYHU\u000f\u0003\nWKH\u0003TXHVWLRQ\u0003LV\u000f\u0003ZKDW\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003SURYLGH\u0003XVHIXO\u0003JXLGDQFH\u0011\u0003,Q\u0003WKLV\u0003SDSHU\u0003ZH\u0003DUJXH\u0003WKDW\u0003\nH[LVWLQJ\u0003 LPSRUWDQFH\u0003 PHDVXUHV\u0003 DUH\u0003 QRW\u0003 LGHDO\u0003 IRU\u0003 WKH\u0003 DSSOLFDWLRQ\u0003 LQ\u0003 PDQXIDFWXULQJ\u0003 TXDOLW\\\u0003\nPDQDJHPHQW\u0011\u0003:H\u0003DOVR\u0003SURSRVH\u0003D\u0003UDQJH\u0003RI\u0003QHZ\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003DQG\u0003WHVW\u0003WKHP\u0003RQ\u0003V\\QWKHWLF\u0003\nGDWD\u0003DV\u0003ZHOO\u0003DV\u0003RQ\u0003UHDO\u0010ZRUOG\u0003GDWD\u0003IURP\u0003D\u0003*HUPDQ\u0003PDQXIDFWXUHU\u0011\u0003\n7KLV\u0003SDSHU\u0003LV\u0003D\u0003PLQRU\u0003UHYLVLRQ\u0003RI\u0003RXU\u0003HDUOLHU\u0003WHFKQLFDO\u0003UHSRUW\u0003RQ\u0003WKH\u0003WRSLF\u0003>\u0014\u0013@\u0011\u00036HFWLRQ\u0003\u0015\u0003SUHVHQWV\u0003\nWKH\u0003XVH\u0003FDVH\u0003WKDW\u0003PRWLYDWHV\u0003RXU\u0003ZRUN\u0011\u00036HFWLRQ\u0003\u0016\u0003LQWURGXFHV\u0003RXU\u0003FRQFHSW\u0003IRU\u0003LGHQWLI\\LQJ\u0003LPSRUWDQW\u0003\nIHDWXUHV\u0003DQG\u0003GHILQHV\u0003FRUUHVSRQGLQJ\u0003PHDVXUHV\u0011\u0003,Q\u00036HFWLRQ\u0003\u0017\u0003ZH\u0003HYDOXDWH\u0003WKH\u0003SURSRVHG\u0003LPSRUWDQFH\u0003\nPHDVXUH\u0003RQ\u0003UHDO\u0010ZRUOG\u0003GDWD\u0003IRUP\u0003D\u0003*HUPDQ\u0003PDQXIDFWXUHU\u0003\u000b6,&.\u0003$*\f\u0011\u0003,Q\u00036HFWLRQ\u0003\u0018\u0003ZH\u0003UHYLHZ\u0003\nUHODWHG\u0003ZRUN\u0011\u00036HFWLRQ\u0003\u0019\u0003VXPPDUL]H\u0003WKH\u0003FRQWHQW\u0003RI\u0003WKLV\u0003SDSHU\u0011\u0003\n96\n\n\u0015\u0003 0RWLYDWLQJ\u0003XVH\u0003FDVH\u0003\n7KH\u0003PHWKRGV\u0003SURSRVHG\u0003LQ\u0003WKLV\u0003SDSHU\u0003PD\\\u0003EH\u0003RI\u0003JHQHUDO\u0003DSSOLFDELOLW\\\u000f\u0003EXW\u0003DUH\u0003PDLQO\\\u0003PRWLYDWHG\u0003\nE\\\u0003WKH\u0003XVH\u0003FDVH\u0003RI\u0003TXDOLW\\\u0003HQJLQHHULQJ\u0003LQ\u0003PDQXIDFWXULQJ\u0011\u00036SHFLILFDOO\\\u000f\u0003ZH\u0003GUDZ\u0003WKH\u0003UHTXLUHPHQWV\u0003\nIURP\u0003WKH\u000335()(50/\u0003UHVHDUFK\u0003SURMHFW\u0003>\u0016@\u0003DQG\u0003WKH\u0003SDUWLFLSDWLQJ\u0003PDQXIDFWXULQJ\u0003FRPSDQ\\\u00036,&.\u0003\n$*\u0011\u00037KH\u0003WDVN\u0003RI\u0003TXDOLW\\\u0003HQJLQHHUV\u0003LV\u0003HQVXULQJ\u0003KLJK\u0003SURGXFW\u0003TXDOLW\\\u0003ZKLOH\u0003NHHSLQJ\u0003WKH\u0003SURGXFWLRQ\u0003\nFRVW\u0003ORZ\u0011\u00037KHUHIRUH\u000f\u0003WKH\\\u0003VWULYH\u0003WR\u0003LGHQWLI\\\u0003DQG\u0003HOLPLQDWH\u0003URRW\u0003FDXVHV\u0003RI\u0003SURGXFWLRQ\u0003HUURUV\u0011\u0003$OO\u0003\nSURGXFWV\u0003XQGHUJR\u0003ULJLG\u0003WHVWV\u000f\u0003QRW\u0003RQO\\\u0003ZKHQ\u0003WKH\\\u0003DUH\u0003ILQLVKHG\u0011\u00037KH\u0003ILQDO\u0003FKHFNV\u0003HQVXUH\u0003WKDW\u0003RQO\\\u0003\nKLJK\u0010TXDOLW\\\u0003SURGXFWV\u0003DUH\u0003VKLSSHG\u0003WR\u0003WKH\u0003FRVWXPHUV\u0011\u0003+RZHYHU\u000f\u0003LW\u0003LV\u0003GHVLUDEOH\u0003IRU\u0003WKH\u0003PDQXIDFWXUHU\u0003\nWR\u0003VRUW\u0003RXW\u0003IDXOW\\\u0003SURGXFWV\u0003HDUO\\\u0003LQ\u0003WKH\u0003SURGXFWLRQ\u0003SURFHVV\u0003DQG\u0003WR\u0003DYRLG\u0003DOORFDWLRQ\u0003RI\u0003UHVRXUFHV\u0003WR\u0003\nODWHU\u0003 GLVFDUGHG\u0003 SURGXFWV\u0011\u0003 7KHUHIRUH\u000f\u0003 DGGLWLRQDO\u0003 TXDOLW\\\u0003 FKHFNV\u0003 DUH\u0003 FRQGXFWHG\u0003 DIWHU\u0003 HDFK\u0003\nSURGXFWLRQ\u0003 VWHS\u0003 WKURXJKRXW\u0003 WKH\u0003 SURGXFWLRQ\u0003 OLQH\u0011\u0003 (DFK\u0003 FKHFNSRLQW\u0003 UHFRUGV\u0003 D\u0003 UDQJH\u0003 RI\u0003\nPHDVXUHPHQWV\u0003DQG\u0003VRUWV\u0003RXW\u0003SURGXFWV\u0003WKDW\u0003GR\u0003QRW\u0003VDWLVI\\\u0003SUHGHILQHG\u0003FULWHULD\u0011\u00037KH\u0003UHFRUGHG\u0003GDWD\u0003\nSRLQWV\u0003DUH\u0003DOVR\u0003WKH\u0003VRXUFH\u0003IRU\u0003LQYHVWLJDWLQJ\u0003HUURU\u0003FDXVHV\u0011\u0003)RU\u0003LQVWDQFH\u000f\u0003D\u0003TXDOLW\\\u0003HQJLQHHU\u0003PLJKW\u0003\nUHDOL]H\u0003WKDW\u0003SURGXFWV\u0003WKDW\u0003VKRZ\u0003D\u0003YDOXH\u0003JUHDWHU\u0003WKDQ\u0003;\u0003IRU\u0003SURSHUW\\\u0003<\u0003LQ\u0003SURGXFWLRQ\u0003VWHS\u0003$\u000f\u0003KDYH\u0003\nDQ\u0003LQFUHDVHG\u0003FKDQFH\u0003RI\u0003IDLOLQJ\u0003ZKHQ\u0003FKHFNHG\u0003LQ\u0003SURGXFWLRQ\u0003VWHS\u0003(\u0011\u00037KLV\u0003LQVLJKW\u0003PLJKW\u0003UHVXOW\u0003LQ\u0003\nDGMXVWPHQWV\u0003RI\u0003FKHFN\u0003LQ\u0003VWHS\u0003$\u0003WR\u0003ILOWHU\u0003RXW\u0003VXFK\u0003SURGXFWV\u0003HDUO\\\u0011\u0003\u0003\n+RZHYHU\u000f\u0003 WKH\u0003GDWD\u0003 SRRO\u0003 IRU\u0003 LQYHVWLJDWLRQ\u0003 LV\u0003 YHU\\\u0003 ODUJH\u0011\u0003 7KH\u0003 PDQXIDFWXUHU\u0003 LQ\u0003 WKH\u0003 3()(50/\u0003\nUHVHDUFK\u0003SURMHFW\u0003WDNHV\u0003VHYHUDO\u0003WKRXVDQGV\u0003RI\u0003PHDVXUHPHQWV\u0003IRU\u0003HDFK\u0003SURGXFW\u0011\u0003)RU\u0003KXPDQV\u0003LW\u0003LV\u0003D\u0003\nYHU\\\u0003FKDOOHQJLQJ\u0003WDVN\u0003WR\u0003ILQG\u0003UHOHYDQW\u0003UHODWLRQV\u0003LQ\u0003VXFK\u0003D\u0003ODUJH\u0003SRRO\u0003RI\u0003GDWD\u0011\u00037KH\u0003PHWKRGV\u0003WKDW\u0003ZH\u0003\nSURSRVH\u0003LQ\u0003WKLV\u0003SDSHU\u0003SURYLGH\u0003PHDQV\u0003WR\u0003JXLGH\u0003TXDOLW\\\u0003HQJLQHHUV\u0003WR\u0003UHOHYDQW\u0003PHDVXUPHQWV\u0011\u0003\n\u0016\u0003 )LQGLQJ\u0003LPSRUWDQW\u0003)HDWXUHV\u0003IRU\u00034XDOLW\\\u0003(QJLQHHUV\u0003ZLWK\u00036+$3\u0003\n7KH\u0003JRDO\u0003LV\u0003WR\u0003SRLQW\u0003TXDOLW\\\u0003HQJLQHHUV\u0003WR\u0003LQWHUHVWLQJ\u0003PHDVXUHPHQWV\u0003\u000bIHDWXUHV\f\u0003LQ\u0003WKH\u0003TXDOLW\\\u0003WHVW\u0003\nGDWD\u0011\u0003:H\u0003FRQVLGHU\u0003D\u0003IHDWXUH\u0003LQWHUHVWLQJ\u0003LQ\u0003WKH\u0003WDUJHW\u0003FRQWH[W\u000f\u0003LI\u0003LW\u0003SURYLGHV\u0003DFWLRQDEOH\u0003LQVLJKWV\u0003IRU\u0003\nTXDOLW\\\u0003HQJLQHHUV\u0003WR\u0003DGDSW\u0003WKUHVKROGV\u0003LQ\u0003WKH\u0003FXUUHQW\u0003TXDOLW\\\u0003FKHFNV\u0011\u0003+HQFH\u0003D\u0003IHDWXUH\u0003LV\u0003LQWHUHVWLQJ\u0003\nLI\u0003LW\u0003\u000bD\f\u0003HQDEOHV\u0003D\u0003VLPSOH\u0003UXOH\u0003IRU\u0003SUHGLFWLQJ\u0003HUURUV\u0003DQG\u0003\u000bE\f\u0003WKH\u0003SUHGLFWLRQ\u0003ZLWK\u0003WKDW\u0003UXOH\u0003LV\u0003RI\u0003\nVXIILFLHQW\u0003TXDOLW\\\u0003WR\u0003UHGXFH\u0003SURGXFWLRQ\u0003FRVW\u0011\u0003\n\u0016\u0011\u0014\u00036KDSOH\\\u0003$GGLWLYH\u0003([SODQDWLRQV\u0003\n,Q\u0003RXU\u0003ZRUN\u0003ZH\u0003OHYHUDJH\u0003ORFDO\u0003H[SODQDWLRQV\u0003DQG\u0003VSHFLILFDOO\\\u00036+$3\u0003YDOXHV\u0003>\u0017@\u0003WR\u0003UHDVRQ\u0003DERXW\u0003\nIHDWXUH\u0003LPSRUWDQFH\u0011\u0003/RFDO\u0003H[SODQDWLRQV\u0003DGGUHVV\u0003IHDWXUH\u0003LPSRUWDQFH\u0003IRU\u0003LQGLYLGXDO\u0003GDWD\u0003SRLQWV\u0003\nZKHUHDV\u0003JOREDO\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003FDSWXUH\u0003WKH\u0003JHQHUDO\u0003\u000bH\u0011J\u0011\u0003DYHUDJH\f\u0003LPSRUWDQFH\u0003RI\u0003D\u0003IHDWXUH\u0011\u0003\n/RFDO\u0003H[SODQDWLRQV\u0003KDYH\u0003WKH\u0003DGYDQWDJH\u0003WKDW\u0003WKH\\\u0003FDQ\u0003UHYHDO\u0003LI\u0003IHDWXUHV\u0003DUH\u0003VRPHWLPHV\u0003\u000bSRVVLEO\\\u0003\nRQO\\\u0003D\u0003IHZ\u0003WLPHV\f\u0003RI\u0003JUHDW\u0003LPSRUWDQFH\u0011\u00037KH\u0003FRQWULEXWLRQ\u0003RI\u0003VXFK\u0003IHDWXUHV\u0003PD\\\u0003EH\u0003KLGGHQ\u0003LQ\u0003JOREDO\u0003\nLPSRUWDQFH\u0003PHDVXUHV\u0011\u0003+RZHYHU\u000f\u0003D\u0003IHDWXUH\u0003WKDW\u0003LV\u0003YHU\\\u0003LPSRUWDQW\u0003D\u0003IHZ\u0003WLPHV\u0003FDQ\u0003EH\u0003RI\u0003PDMRU\u0003\nLQWHUHVW\u0003IRU\u0003TXDOLW\\\u0003PDQDJHPHQW\u0011\u00037KLV\u0003LV\u0003EHFDXVH\u0003WKH\u0003LQWHUHVWLQJ\u0003VLWXDWLRQV\u0003\u000bSUHGLFWDEOH\u0003HUURUV\f\u0003\nDUH\u0003 UDUH\u0003 LQ\u0003 D\u0003 SURGXFWLRQ\u0003 SURFHVV\u0011\u0003 :LWK\u0003 RXU\u0003 PHWKRGV\u0003 ZH\u0003 DLP\u0003 WR\u0003 LGHQWLI\\\u0003 VXFK\u0003 IHDWXUHV\u0011\u0003 7KH\u0003\nSURSRVHG\u0003IHDWXUH\u0003LPSRUWDQFH\u0003PHWULFV\u0003DUH\u0003QRW\u0003JOREDO\u0003RU\u0003ORFDO\u0003LQ\u0003WKH\u0003FODVVLFDO\u0003VHQVH\u0011\u00037KH\u0003LGHQWLILHG\u0003\nIHDWXUHV\u0003DUH\u0003LPSRUWDQW\u0003LQ\u0003WKH\u0003VHQVH\u0003WKDW\u0003WKH\\\u0003DOORZ\u0003IRU\u0003JRRG\u0003DQG\u0003VLPSOH\u0003SUHGLFWLRQV\u0003IRU\u0003VRPH\u0003\nUHJLRQV\u0011\u0003\u0003\n\u0016\u0011\u0015\u0003,OOXVWUDWLYH\u0003([DPSOH\u0003IRU\u0003XVLQJ\u00036+$3\u0003YDOXHV\u0003IRU\u00034XDOLW\\\u00030DQDJHPHQW\u0003\n6+$3\u0003YDOXHV\u0003SURYLGH\u0003LQVLJKWV\u0003LQWR\u0003SUHGLFWLRQV\u0003IRU\u0003LQGLYLGXDO\u0003GDWD\u0003SRLQWV\u0011\u0003)RU\u0003D\u0003JLYHQ\u0003GDWD\u0003VHW\u0003\nWKLV\u0003 UHVXOWV\u0003 LQ\u0003 D\u0003 VHW\u0003 RI\u0003 H[SODQDWLRQV\u0011\u0003 $QDO\\]LQJ\u0003 WKLV\u0003 VHW\u0003 RI\u0003 H[SODQDWLRQV\u0003 FDQ\u0003 \\LHOG\u0003 DGGLWLRQDO\u0003\nLQVLJKWV\u0003LQWR\u0003WKH\u0003ZRUNLQJV\u0003RI\u0003D\u0003PRGHO\u0003DQG\u0003WKH\u0003PRGHOHG\u0003SKHQRPHQRQ\u0011\u0003/XQGEHUJ\u0003HW\u0003DO\u0011\u0003>\u0018@\u0003SURSRVHG\u0003\n97\n\nDQG\u0003LPSOHPHQWHG\u0003VRPH\u0003DQDO\\VLV\u0003RQ\u0003WRS\u0003RI\u00036+$3\u0003YDOXHV\u0003DQG\u00036+$3\u0003LQWHUDFWLRQ\u0003YDOXHV\u0011\u00037KHVH\u0003\nLQFOXGH\u0003FOXVWHULQJ\u0003RI\u0003H[SODQDWLRQV\u0003DQG\u0003YLVXDOL]DWLRQ\u0003RI\u0003LQWHUDFWLRQV\u0011\u0003\n7KLV\u0003FKDSWHU\u0003LQWURGXFHV\u0003DGGLWLRQDO\u0003PHWKRGV\u0003IRU\u0003DQDO\\]LQJ\u00036+$3\u0003YDOXHV\u0003DQG\u00036+$3\u0003LQWHUDFWLRQ\u0003\nYDOXHV\u0011\u0003,Q\u0003FRQWUDVW\u0003WR\u0003H[LVWLQJ\u0003PHWKRGV\u0003IRU\u0003DQDO\\VLV\u000f\u0003WKH\u0003SURSRVHG\u0003PHWKRGV\u0003GR\u0003QRW\u0003DLP\u0003DW\u0003SURYLGLQJ\u0003\nD\u0003KROLVWLF\u0003XQGHUVWDQGLQJ\u0003RI\u0003WKH\u0003DQDO\\]HG\u0003PRGHO\u0003RU\u0003SKHQRPHQRQ\u0011\u0003,QVWHDG\u000f\u0003WKH\\\u0003DLP\u0003DW\u0003ILQGLQJ\u0003\nVLPSOH\u0003 DQG\u0003 JRRG\u0003 H[SODQDWLRQV\u0003 IRU\u0003 SDUW\u0003 RI\u0003 WKH\u0003 DQDO\\]HG\u0003 SKHQRPHQRQ\u0003 DQG\u0003 WKHUHE\\\u0003 JXLGH\u0003 WKH\u0003\nDQDO\\VLV\u0003RI\u0003TXDOLW\\\u0003HQJLQHHUV\u0011\u0003:H\u0003DUJXH\u0003WKDW\u0003D\u0003IHDWXUH\u0003RU\u0003UHODWLRQ\u0003EHWZHHQ\u0003IHDWXUHV\u0003LV\u0003RI\u0003SRWHQWLDO\u0003\nLQWHUHVW\u000f\u0003LI\u0003LW\u0003HQDEOHV\u0003\u000bD\f\u0003SUHGLFWLRQ\u0003UXOHV\u0003ZLWK\u0003KLJK\u0003SUHGLFWLRQ\u0003TXDOLW\\\u0003DQG\u0003\u000bE\f\u0003WKH\u0003UHVSHFWLYH\u0003UXOHV\u0003\nDUH\u0003VLPSOH\u0003HQRXJK\u0003IRU\u0003KXPDQ\u0003FRPSUHKHQVLRQ\u0003DV\u0003ZHOO\u0003DV\u0003IRU\u0003WDNLQJ\u0003FRUUHVSRQGLQJ\u0003DFWLRQ\u0003\u000bH\u0011J\u0011\u0003\nDGMXVWLQJ\u0003D\u0003WKUHVKROG\u0003LQ\u0003D\u0003TXDOLW\\\u0003FKHFN\f\u0011\u0003,Q\u0003WKH\u0003PRWLYDWLQJ\u0003XVH\u0003FDVH\u000f\u0003SUHGLFWLRQ\u0003TXDOLW\\\u0003LV\u0003KLJK\u0003\nHQRXJK\u000f\u0003LI\u0003WKH\u0003UDWLR\u0003EHWZHHQ\u0003WUXH\u0003SRVLWLYHV\u000373\u0003DQG\u0003IDOVH\u0003SRVLWLYHV\u0003)3\u0003LV\u0003KLJK\u0003HQRXJK\u0011\u00037KDW\u0003LV\u000f\u0003\n73\u0012)3\u0003PXVW\u0003EH\u0003ELJJHU\u0003WKDQ\u0003WKH\u0003FRVW\u0003RI\u0003D\u0003IDOVH\u0003SRVLWLYH\u0003GLYLGHG\u0003E\\\u0003WKH\u0003VDYLQJV\u0003IRU\u0003D\u0003WUXH\u0003SRVLWLYH\u0011\u0003\n\u000b)DOVH\u0003DQG\u0003WUXH\u0003QHJDWLYHV\u0003DUH\u0003LUUHOHYDQW\u0003DV\u0003WKH\\\u0003GR\u0003QRW\u0003FKDQFH\u0003WKH\u0003DV\u0010LV\u0003SURFHVV\u0011\f\u0003\n:H\u0003IXUWKHU\u0003DUJXH\u0003WKDW\u0003WKH\u0003VXSSRUW\u0003IRU\u0003WKH\u0003UXOH\u0003LV\u0003OHVV\u0003LPSRUWDQW\u0011\u0003(UURUV\u0003DUH\u0003UDUH\u0003LQ\u0003KLJKO\\\u0003RSWLPL]HG\u0003\nSURGXFWLRQ\u0003OLQHV\u0003DQG\u0003DQ\\\u0003SUHGLFWLRQ\u0003UXOH\u0003ZLOO\u0003OLNHO\\\u0003KDYH\u0003ORZ\u0003VXSSRUW\u0011\u0003,W\u0003LV\u0003PRUH\u0003LPSRUWDQW\u0003WR\u0003\nLGHQWLI\\\u0003VWURQJ\u0003HIIHFWV\u0003EHFDXVH\u0003WKH\\\u0003HQDEOH\u0003FOHDU\u0003DFWLRQV\u0011\u0003$\u0003ZHDN\u0003EXW\u0003PRUH\u0003FRPPRQ\u0003HIIHFW\u0003FDQ\u0003\n\\LHOG\u0003D\u0003UXOH\u0003ZLWK\u0003JRRG\u0003VXSSRUW\u0003EXW\u0003LV\u0003PRUH\u0003GLIILFXOW\u0003WR\u0003H[SORLW\u0003LQ\u0003WKH\u0003SURGXFWLRQ\u0003SURFHVV\u0011\u00037KH\u0003FRUH\u0003\nLGHDV\u0003 EHKLQG\u0003 WKH\u0003 SURSRVHG\u0003 DQDO\\VLV\u0003 PHWKRGV\u0003 IROORZ\u0003 IURP\u0003 WKH\u0003 DUJXPHQWV\u0003 DERYH\u0011\u0003 7KDW\u0003 LV\u000f\u0003 WR\u0003\nVDFULILFH\u0003VXSSRUW\u0003LQ\u0003IDYRU\u0003RI\u0003FRQILGHQFH\u0003DQG\u0003VLPSOLFLW\\\u0011\u00037KLV\u0003LV\u0003LQ\u0003FRQWUDVW\u0003WR\u0003HVWDEOLVKHG\u0003IHDWXUH\u0003\nLPSRUWDQFH\u0003 PHDVXUHV\u000f\u0003 ZKLFK\u0003 DLP\u0003 DW\u0003 ILQGLQJ\u0003 IHDWXUHV\u0003 WKDW\u0003 DUH\u0003 RYHUDOO\u0003 PRVW\u0003 LPSRUWDQW\u0003 \u000bH\u0011J\u0011\u0003\nLPSRUWDQW\u0003RQ\u0003DYHUDJH\f\u0011\u0003:H\u0003DUH\u0003ORRNLQJ\u0003IRU\u0003IHDWXUHV\u0003DQG\u0003UHODWLRQV\u0003WKDW\u0003DUH\u0003YHU\\\u0003LPSRUWDQW\u000f\u0003EXW\u0003\nSRVVLEO\\\u0003RQO\\\u0003LQ\u0003D\u0003IHZ\u0003FDVHV\u0011\u0003)RU\u0003WKH\u0003VDNH\u0003RI\u0003LOOXVWUDWLRQ\u0003FRQVLGHU\u0003WKH\u0003VXEVHTXHQW\u0003VLPSOLILHG\u0003FDVH\u001d\u0003\n$VVXPH\u0003D\u0003ILFWLRQDO\u0003PDQXIDFWXUHU\u0003WKDW\u0003KDV\u0003SURGXFHG\u0003\u0014\u0013\u0013\u0013\u0013\u0003SURGXFW\u0003LWHPV\u0011\u0003$Q\u0003LQWHUPHGLDWH\u0003TXDOLW\\\u0003\nWHVW\u0003PHDVXUHV\u0003WKH\u0003IHDWXUHV\u0003$\u0003DQG\u0003%\u0003ZLWK\u0003XQLIRUPO\\\u0003GLVWULEXWHG\u0003YDOXHV\u0003\u000bUDQJH\u0003\u0013\u0003WR\u0003\u0014\f\u0003DQG\u0003KDYH\u0003WKH\u0003\nIROORZLQJ\u0003UHODWLRQ\u0003ZLWK\u0003SURGXFWLRQ\u0003HUURUV\u001d\u0003\u000b\u0014\f\u00033URGXFWV\u0003ZLWK\u0003IHDWXUH\u0003$\u001f\u0013\u0011\u001b\u0003KDYH\u0003URXJKO\\\u0003D\u0003\u0015\u0013\b\u0003\nFKDQFH\u0003RI\u0003IDLOLQJ\u0003WKH\u0003ILQDO\u0003TXDOLW\\\u0003FKHFN\u0011\u0003\u000b\u0015\f\u00033URGXFWV\u0003ZLWK\u0003IHDWXUH\u0003%!\u0013\u0011\u001c\u001c\u0003KDYH\u0003URXJKO\\\u0003D\u0003\u001c\u001b\b\u0003\nFKDQFH\u0003RI\u0003IDLOLQJ\u0011\u00032YHUDOO\u000f\u0003DERXW\u0003\u0014\u0019\b\u0003RI\u0003WKH\u0003VDPSOH\u0003GDWD\u0003UHIOHFW\u0003IDXOW\\\u0003SURGXFWV\u0011\u0003\n7R\u0003VXSSRUW\u0003WKH\u0003TXDOLW\\\u0003HQJLQHHU\u000f\u0003ZH\u0003DLP\u0003WR\u0003LGHQWLI\\\u0003WKH\u0003IHDWXUH\u0003WKDW\u0003\\LHOGV\u0003WKH\u0003PRVW\u0003XVHIXO\u0003LQVLJKWV\u0011\u0003\n:H\u0003WKHUHIRUH\u0003WUDLQ\u0003D\u0003PDFKLQH\u0003OHDUQLQJ\u0003PRGHO\u0003RQ\u0003WKH\u0003TXDOLW\\\u0003WHVW\u0003GDWD\u0003DQG\u0003DQDO\\]H\u0003WKH\u0003IHDWXUH\u0003\nLPSRUWDQFH\u0011\u00037KH\u0003IHDWXUH\u0003LPSRUWDQFH\u0003VFRUH\u0003VKRXOG\u0003SRLQW\u0003WKH\u0003TXDOLW\\\u0003HQJLQHHU\u0003WR\u0003WKH\u0003PRVW\u0003XVHIXO\u0003\nIHDWXUH\u0011\u0003)RU\u0003LOOXVWUDWLRQ\u000f\u0003ZH\u0003EXLOG\u0003D\u0003WUHH\u0003PRGHO\u0003\u000bVHH\u0003)LJXUH\u0003\u0014\f\u0003DQG\u0003FRPSXWH\u0003WKH\u0003W\\SLFDO\u0003IHDWXUH\u0003\nLPSRUWDQFH\u0003VFRUHV\u0003\u000bVHH\u00037DEOH\u0003\u0014\f\u0011\u0003:H\u0003XVHG\u0003WKH\u00033\\WKRQ\u0003;*%RRVW\u0003OLEUDU\\\u0003WR\u0003EXLOG\u0003WKH\u0003WUHH\u000f\u0003XVLQJ\u0003D\u0003\nVLQJOH\u0003WUHH\u0003IRU\u0003VLPSOLFLW\\\u0011\u0003)LJXUH\u0003\u0014\u0003LOOXVWUDWHV\u0003WKH\u0003PRGHO\u0014\u0011\u0003\n7DEOH\u0003\u0014\u0003VKRZV\u0003WKH\u0003YDOXHV\u0003RI\u0003HVWDEOLVKHG\u0003IHDWXUH\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003IRU\u0003WKH\u0003SUHVHQWHG\u0003PRGHO\u0011\u0003\n1RWH\u0003WKDW\u0003WKH\u0003W\\SLFDO\u0003IHDWXUH\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003DOO\u0003FRQVLGHU\u0003IHDWXUH\u0003$\u0003DV\u0003PRVW\u0003LPSRUWDQW\u0011\u0003\n+RZHYHU\u000f\u0003IHDWXUH\u0003$\u0003LV\u0003RI\u0003OLWWOH\u0003XVH\u0003LQ\u0003WKH\u0003WDUJHWHG\u0003DSSOLFDWLRQ\u0003GRPDLQ\u0011\u00037KURXJK\u0003LQYHVWLJDWLQJ\u0003\nIHDWXUH\u0003$\u000f\u0003WKH\u0003TXDOLW\\\u0003HQJLQHHU\u0003FDQ\u0003EXLOG\u0003D\u0003UXOH\u0003RI\u0003WKH\u0003IRUP\u0003³,)\u0003IHDWXUH\u0003$\u0003\u001f\u0003\u0013\u0011\u001b\u00037+(1\u0003(5525´\u0011\u0003\n+RZHYHU\u000f\u0003WKLV\u0003UXOH\u0003LV\u0003QRW\u0003RI\u0003SUDFWLFDO\u0003XVH\u0011\u00036RUWLQJ\u0003RXW\u0003SURGXFWV\u0003ZLWK\u0003D\u0003SUHGLFWHG\u0003HUURU\u0003ZRXOG\u0003\nUHPRYH\u0003\u001b\u0013\b\u0003RI\u0003WKH\u0003SURGXFWV\u0003DQG\u0003DV\u0003PDQ\\\u0003IDOVH\u0003SRVLWLYHV\u0011\u0003,Q\u0003FRQWUDVW\u000f\u0003DQ\u0003DQDO\\VLV\u0003RI\u0003IHDWXUH\u0003%\u0003\nZRXOG\u0003UHVXOW\u0003LQ\u0003D\u0003SUHGLFWLRQ\u0003UXOH\u0003RI\u0003WKH\u0003IRUP\u0003³,)\u0003IHDWXUH\u0003%\u0003!\u0003\u0013\u0011\u001c\u001c\u00037+(1\u0003(5525´\u0011\u00037KLV\u0003UXOH\u0003\nDIIHFWV\u0003D\u0003UHDVRQDEOH\u0003QXPEHU\u0003RI\u0003SURGXFWV\u0003DQG\u0003LV\u0003DOPRVW\u0003DOZD\\V\u0003FRUUHFW\u0011\u0003,W\u0003WKHUHIRUH\u0003HQDEOHV\u0003DQ\u0003\nDGMXVWPHQW\u0003RI\u0003WKH\u0003SURGXFWLRQ\u0003SURFHVV\u0011\u00031RWH\u0003WKDW\u0003WKH\u0003W\\SLFDO\u0003IHDWXUH\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003IDLO\u0003WR\u0003\nUDQN\u0003WKH\u0003IHDWXUHV\u0003DV\u0003GHVLUHG\u0011\u00037DEOH\u0003\u0015\u0003VKRZV\u0003WKH\u0003UHVXOWV\u0003IURP\u0003WKH\u0003PHDVXUHV\u0003WKDW\u0003ZH\u0003LQWURGXFH\u0003LQ\u0003\nWKLV\u0003SDSHU\u0011\u00037KH\\\u0003DOO\u0003FRUUHFWO\\\u0003UDQN\u0003IHDWXUH\u0003%\u0003ILUVW\u0011\u00037KH\u0003VSHFLILFV\u0003RI\u0003HDFK\u0003LPSRUWDQFH\u0003PHDVXUH\u0003\nIROORZ\u0003LQ\u0003WKH\u0003FRUUHVSRQGLQJ\u0003VXEVHFWLRQV\u0003EHORZ\u0011\u0003\n\u0014\u00033RVLWLYH\u0003YDOXHV\u0003LQ\u0003WKH\u0003OHDYH\u0003QRGHV\u0003FRUUHVSRQG\u0003WR\u0003WKH\u0003SUHGLFWLRQ\u0003RI\u0003DQ\u0003HUURU\u0003DQ\u0003QHJDWLYH\u0003YDOXHV\u0003\nWR\u0003WKH\u0003SUHGLFWLRQ\u0003RI\u0003QR\u0003HUURU\u0011\u0003\n98\n\n)LJXUH\u0003\u0014\u0011\u00037KH\u0003ILUVW\u0003WUHH\u0003LQ\u0003WKH\u0003VDPSOH\u0003FDVH\u0003PRGHO\u0003\u000bXVLQJ\u0003;*%RRVW\f\u0003\n7DEOH\u0003\u0014\u0011\u0003(VWDEOLVKHG\u0003IHDWXUH\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003IRU\u0003WKH\u0003PRGHO\u0003LQ\u0003ILJ\u0011\u0014\u0003\n,PSRUWDQFH\u00030HDVXUH\u0003\n6FRUH\u0003IRU\u0003IHDWXUH\u0003$\u0003\n6FRUH\u0003IRU\u0003IHDWXUH\u0003%\u0003\n*DLQ\u0003\n\u0014\u0019\u0011\u0014\u0014\u001c\u0019\u0013\u001b\u0003\n\u001a\u0011\u0018\u0019\u0017\u0019\u0014\u001b\u0003\n&RYHU\u0003\n\u001c\u0017\u0019\u0011\u0016\u001c\u0013\u0019\u001a\u0019\u0003\n\u0019\u001a\u0019\u0011\u0017\u0018\u0014\u0014\u001c\u0019\u0003\n:HLJKW\u0003\n\u0015\u0019\u0016\u0003\n\u0015\u0018\u001b\u0003\n7RWDO\u0003*DLQ\u0003\n\u0017\u0015\u0016\u001c\u0011\u0017\u0018\u0019\u001a\u001a\u0018\u0003\n\u0014\u001c\u0018\u0014\u0011\u0019\u001a\u0014\u0017\u0014\u001a\u0003\n7RWDO\u0003&RYHU\u0003\n\u0015\u0017\u001b\u001c\u0013\u0013\u0011\u001a\u0017\u001a\u001b\u0016\u0018\u0003\n\u0014\u001a\u0017\u0018\u0015\u0017\u0011\u0017\u0013\u001b\u0017\u0018\u001b\u0003\n*LQL\u0010LPSRUWDQFH\u0003\n\u0013\u0011\u0019\u001b\u0013\u0019\u0013\u0018\u0003\n\u0013\u0011\u0016\u0014\u001c\u0016\u001c\u0018\u0003\n$YHUDJH\u00036+$3\u0003YDOXHV\u0003\n\u0013\u0011\u001b\u0019\u0019\u0016\u001a\u0017\u0003\n\u0013\u0011\u0013\u0015\u001c\u001b\u0013\u0019\u0003\n7DEOH\u0003\u0015\u0011\u00033URSRVHG\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003IRU\u0003WKH\u0003PRGHO\u0003LQ\u0003ILJ\u0011\u0014\u0003\n,PSRUWDQFH\u00030HDVXUH\u0003\n6FRUH\u0003IRU\u0003IHDWXUH\u0003$\u0003\n6FRUH\u0003IRU\u0003IHDWXUH\u0003%\u0003\n0D[\u00036+$3\u0003\n\u0013\u0011\u001a\u0013\u0018\u0018\u0015\u0018\u0003\n\u001a\u0011\u0018\u001c\u0014\u0013\u0013\u001c\u0003\n5DQJH\u00036+$3\u0003\n\u0019\u0011\u0017\u0015\u0013\u0017\u0018\u0015\u0003\n\u001b\u0011\u001a\u0015\u0017\u0016\u0015\u0015\u0003\n6PRRWKHG\u00035DQJH\u00036+$3\u0003\n\u0019\u0014\u0011\u0013\u0018\u0016\u0013\u0013\u0013\u0003\n\u0019\u0018\u0011\u001c\u0015\u0013\u0015\u001b\u001c\u0003\n7RS\u0010.\u00036+$3\u0003\u000bZLWK\u0003N \u0014\u0013\f\u0003\n\u0010\u0014\u001c\u0011\u001c\u001b\u0017\u0019\u001b\u0017\u0003\n\u001a\u0018\u0011\u001b\u0019\u0013\u001c\u0016\u001c\u0003\n0D[\u00030DLQ\u0003(IIHFW\u0003\n\u0013\u0011\u0019\u0019\u001a\u0016\u001b\u0014\u0003\n\u0017\u0011\u0017\u001b\u0015\u0017\u001b\u0019\u0003\n\u0016\u0011\u0016\u0003&RQFHSW\u0003IRU\u0003)HDWXUH\u0003,PSRUWDQFH\u0003PHDVXUHV\u0003RQ\u0003WRS\u0003RI\u00036+$3\u0003YDOXHV\u0003\n,Q\u0003WKLV\u0003VHFWLRQ\u0003ZH\u0003LQWURGXFH\u0003D\u0003UDQJH\u0003RI\u0003LPSRUWDQFH\u0003PHDVXUH\u0003RQ\u0003WRS\u0003RI\u00036+$3\u0003YDOXHV\u0011\u0003$OO\u0003WKHVH\u0003\nLPSRUWDQFH\u0003 PHDVXUHV\u0003 OHYHUDJH\u0003 WKH\u0003 ORFDO\u0003 LPSRUWDQFH\u0003 PHDVXUHV\u0003 SURYLGHG\u0003 E\\\u0003 6+$3\u0003 YDOXHV\u000f\u0003 WR\u0003\nLGHQWLI\\\u0003³ORFDOO\\\u0003LQWHUHVWLQJ´\u0003IHDWXUHV\u0011\u0003%\\\u0003³ORFDOO\\\u0003LQWHUHVWLQJ´\u0003ZH\u0003UHIHU\u0003WR\u0003IHDWXUHV\u0003WKDW\u0003\\LHOG\u0003\nJRRG\u0003H[SODQDWLRQV\u0003IRU\u0003HUURUV\u0003LQ\u0003DW\u0003OHDVW\u0003VRPH\u0003FDVHV\u0011\u00037KLV\u0003LV\u0003LQ\u0003FRQWUDVW\u0003WR\u0003PHDVXUHV\u0003WKDW\u0003LGHQWLI\\\u0003\nIHDWXUHV\u0003WKDW\u0003DUH\u0003IUHTXHQWO\\\u0003RU\u0003RQ\u0003DYHUDJH\u0003LPSRUWDQW\u0011\u0003\n7RS\u0003,PSRUWDQFH\u0003IRU\u00037RS\u0010.\u00033UHGLFWLRQV\u0003\n7KH\u0003LGHD\u0003EHKLQG\u0003WKH\u0003DQDO\\VLV\u0003RI\u0003WRS\u0010N\u0003SUHGLFWLRQV\u0003LV\u0003WR\u0003]RRP\u0003LQ\u0003RQ\u0003WKH\u0003PRVW\u0003UHOHYDQW\u0003FDVHV\u0011\u0003:H\u0003\nGHILQH\u0003WRS\u0010N\u0003E\\\u0003UDQNLQJ\u0003WKH\u0003SUHGLFWLRQV\u0003DFFRUGLQJ\u0003WR\u0003WKHLU\u0003FRQILGHQFH\u0011\u0003,Q\u0003WKH\u0003WDUJHW\u0003DSSOLFDWLRQ\u0003\nZH\u0003 RQO\\\u0003 FDUH\u0003 DERXW\u0003 SUHGLFWLQJ\u0003 HUURUV\u0011\u0003 +HQFH\u000f\u0003 ZH\u0003 RQO\\\u0003 ORRN\u0003 DW\u0003 HUURU\u0003 SUHGLFWLRQV\u0003 ZLWK\u0003 KLJK\u0003\nFRQILGHQFH\u0011\u0003+HUH\u0003ZH\u0003XVH\u0003D\u0003QRWLRQ\u0003RI\u0003FRQILGHQFH\u0003WKDW\u0003GRHV\u0003QRW\u0003LPSO\\\u0003TXDQWLILDEOH\u0003FRQILGHQFH\u0003\nLQWHUYDOV\u0011\u00037KDW\u0003LV\u000f\u0003ZH\u0003XVH\u0003WKH\u00036+$3\u0003YDOXHV\u0003LQ\u0003RXU\u0003DQDO\\VLV\u0003WR\u0003GHWHUPLQH\u0003WKH\u0003WRS\u0010N\u0003LQVWDQFHV\u0011\u0003:H\u0003\nWKHQ\u0003DYHUDJH\u0003WKH\u0003ORFDO\u0003IHDWXUH\u0003VSHFLILF\u00036+$3\u0003YDOXHV\u0003IRU\u0003HDFK\u0003RI\u0003WKH\u0003WRS\u0010N\u0003SUHGLFWLRQV\u0011\u0003/HDQLQJ\u0003\nRQ\u0003WKH\u0003QRWDWLRQ\u0003IURP\u0003/XQGEHUJ\u0003DQG\u0003/HH\u0003>\u0017@\u0003DQG\u0003DVVXPLQJ\u0003WKDW\u0003D\u0003KLJK\u00036+$3\u0003YDOXH\u0003UHIHUV\u0003WR\u0003WKH\u0003\nHUURU\u0003FODVV\u000f\u0003WKH\u0003LPSRUWDQFH\u0003PHDVXUH\u0003LV\u0003GHILQHG\u0003DV\u0003IROORZV\u0015\u001d\u0003\n\u0017Ǧ\u000e௜ሺ݂ǡ ܦሻൌͳ\n݇෍\n߶௜ሺ݂ǡ ݔሻ\n௫אሼௗȁ\u0003௞வȁሼௗᇲא஽ȁ௙ሺௗሻழ௙ሺௗᇲሻሽȁሽ\n\u0015\u0003)RU\u0003WKH\u0003VDNH\u0003RI\u0003VLPSOLFLW\\\u0003ZH\u0003DVVXPH\u0003D\u0003WRWDO\u0003RUGHULQJ\u0003RI\u0003SUHGLFWLRQ\u0003VFRUHV\u0011\u0003\n99\n\n\u0003\n\u0003\n+HUH\u000f\u0003I\u0003LV\u0003WKH\u0003PRGHO\u0003IXQFWLRQ\u0003DQG\u0003'\u0003WKH\u0003GDWD\u0003VHWV\u0003IRU\u0003HYDOXDWLQJ\u0003WKH\u0003IHDWXUH\u0003LPSRUWDQFH\u000f\u0003L\u0003WKH\u0003\nIHDWXUH\u0003WR\u0003VFRUH\u000f\u0003DQG\u0003ࢥBL\u0003\u000bI\u000f[\f\u0003WKH\u00036+$3\u0003YDOXH\u0003RI\u0003IHDWXUH\u0003L\u0003IRU\u0003PRGHO\u0003I\u0003DQG\u0003GDWD\u0003SRLQW\u0003[\u0011\u00037KH\u0003\nLQWXLWLRQ\u0003EHKLQG\u0003WKLV\u0003PHDVXUH\u0003LV\u0003WKH\u0003IROORZLQJ\u001d\u0003,Q\u0003WKH\u0003WDUJHW\u0003DSSOLFDWLRQ\u0003ZH\u0003ORRN\u0003IRU\u0003SUHGLFWLRQV\u0003\nZLWK\u0003KLJK\u0003FRQILGHQFH\u0003RQO\\\u0011\u0003+HQFH\u000f\u0003ZH\u0003RQO\\\u0003FDUH\u0003DERXW\u0003IHDWXUHV\u0003WKDW\u0003SOD\\\u0003D\u0003UROH\u0003LQ\u0003WKH\u0003SUHGLFWLRQV\u0003\nZLWK\u0003 KLJK\u0003 FRQILGHQFH\u0011\u0003 :H\u0003 WKHQ\u0003 DJJUHJDWH\u0003 WKH\u0003 ORFDO\u0003 6+$3\u0003 YDOXHV\u0003 IRU\u0003 WKH\u0003 PRVW\u0003 UHOHYDQW\u0003\nSUHGLFWLRQV\u0011\u0003$Q\u0003DVVXPSWLRQ\u0003EHKLQG\u0003WKLV\u0003DSSURDFK\u0003LV\u0003WKDW\u0003WKH\u0003WRS\u0010N\u0003SUHGLFWLRQV\u0003DUH\u0003VLPLODU\u0003IURP\u0003\nWKH\u0003SHUVSHFWLYH\u0003RI\u0003WKH\u0003PRGHO\u0011\u00037KDW\u0003LV\u000f\u0003WKH\\\u0003\\LHOG\u0003SUHGLFWLRQV\u0003ZLWK\u0003KLJK\u0003FRQILGHQFH\u0003IRU\u0003VLPLODU\u0003\nUHDVRQV\u0011\u00037KLV\u0003PD\\\u0003QRW\u0003DOZD\\V\u0003KROG\u0003WUXH\u000f\u0003HVSHFLDOO\\\u0003IRU\u0003ODUJHU\u0003N\u0011\u0003+HQFH\u000f\u0003D\u0003QDWXUDO\u0003H[WHQVLRQ\u0003RI\u0003WKLV\u0003\nPHDVXUH\u0003LV\u0003WR\u0003FRQVLGHU\u0003FOXVWHUV\u0003ZLWKLQ\u0003WKH\u0003WRS\u0003SUHGLFWLRQV\u0011\u0003\n\u0003\n0D[\u00036+$3\u0003\n7KH\u0003LGHD\u0003EHKLQG\u00030D[\u00036+$3\u0003LV\u0003WR\u0003ORRN\u0003IRU\u0003IHDWXUHV\u0003WKDW\u0003FDQ\u0003KDYH\u0003D\u0003KLJK\u0003FRQWULEXWLRQ\u0003WR\u0003WKH\u0003\nRXWFRPH\u0011\u0003,W\u0003LV\u0003VLPSO\\\u0003GHILQHG\u0003DV\u0003WKH\u0003PD[LPXP\u00036+$3\u0003YDOXH\u0003IRU\u0003D\u0003JLYHQ\u0003IHDWXUH\u0003LQ\u0003GDWD\u0003VHW\u001d\u0003\n\u0010\u0003\u0016\u000b\u0004\u0013௜ሺ݂ǡ ܦሻൌሼ߶௜ሺ݂ǡ ݔሻȁݔא ܦሽ\u0003\n\u0003\n+HUH\u000f\u0003ZH\u0003DJDLQ\u0003DVVXPH\u0003WKDW\u0003WKH\u0003RXWFRPH\u0003RI\u0003LQWHUHVW\u0003LV\u0003DVVRFLDWHG\u0003ZLWK\u0003KLJK\u00036+$3\u0003YDOXHV\u0011\u00032QH\u0003\nPD\\\u0003FKDQJH\u0003WKH\u0003VLJQ\u0003RI\u0003WKH\u0003PHDVXUH\u0003RU\u0003VXEVWLWXWH\u0003WKH\u0003PD[LPXP\u0003ZLWK\u0003D\u0003PLQLPXP\u0003IXQFWLRQ\u000f\u0003LI\u0003WKH\u0003\nRSSRVLWH\u0003LV\u0003WKH\u0003FDVH\u0011\u0003\n7KH\u0003LQWXLWLRQ\u0003EHKLQG\u0003WKLV\u0003PHDVXUH\u0003LV\u0003WKDW\u0003LW\u0003FDSWXUHV\u0003WKH\u0003KLJKHVW\u0003HIIHFW\u0003WKDW\u0003D\u0003IHDWXUH\u0003FDQ\u0003KDYH\u0011\u0003\n7KH\u0003UDWLRQDOH\u0003LV\u0003WKDW\u0003D\u0003IHDWXUH\u0003ZLWK\u0003D\u0003KLJK\u0003PD[LPDO\u0003FRQWULEXWLRQ\u0003LV\u0003LQ\u0003VRPH\u0003FDVHV\u0003RI\u0003KLJK\u0003LQWHUHVW\u0011\u0003\n$\u0003ZHDNQHVV\u0003RI\u0003WKLV\u0003PHDVXUH\u0003LV\u0003WKDW\u0003LW\u0003LV\u0003VHQVLWLYH\u0003WR\u0003RXWOLQHUV\u0011\u0003:H\u0003DUJXH\u0003WKDW\u0003FRQILGHQFH\u0003LV\u0003PXFK\u0003\nPRUH\u0003LPSRUWDQW\u0003LQ\u0003WKH\u0003WDUJHWHG\u0003DSSOLFDWLRQ\u0003GRPDLQ\u0003WKDQ\u0003VXSSRUW\u0011\u0003+RZHYHU\u000f\u0003LVRODWHG\u0003RXWOLHUV\u0003FDQ\u0003\nOHDG\u0003WR\u0003XQGHVLUDEOH\u0003UHVXOWV\u0011\u00032QH\u0003PD\\\u0003WKHUHIRUH\u0003DOWHU\u0003WKLV\u0003LPSRUWDQFH\u0003PHDVXUH\u0003E\\\u0003WDNLQJ\u0003D\u0003FHUWDLQ\u0003\nTXDQWLOH\u0003LQVWHDG\u0003RI\u0003WKH\u0003PD[LPXP\u0003YDOXH\u0011\u0003\n\u0003\n0D[\u00030DLQ\u0003(IIHFW\u0003\n7KH\u0003LGHD\u0003EHKLQG\u00030D[\u00030DLQ\u0003(IIHFW\u0003LV\u0003WR\u0003ORRN\u0003IRU\u0003IHDWXUHV\u0003WKDW\u0003KDYH\u0003WKH\u0003KLJKHVW\u0003HIIHFWV\u0003RQ\u0003WKHLU\u0003\nRZQ\u0011\u00037KDW\u0003LV\u000f\u0003ZH\u0003H[SOLFLWO\\\u0003LJQRUH\u0003WKH\u0003HIIHFW\u0003RI\u0003IHDWXUH\u0003LQWHUDFWLRQV\u000f\u0003ZKLFK\u0003DUH\u0003RWKHUZLVH\u0003LQFOXGHG\u0003\nLQ\u0003WKH\u00036+$3\u0003YDOXHV\u0011\u0003/HDQLQJ\u0003RQ\u0003/XQGEHUJ\u0003HW\u0003DO\u0011\u0003>\u0017@\u000f\u0003ZH\u0003GHILQH\u0003WKH\u0003PHDVXUH\u0003DV\u001d\u0003\n\u0010\u0003\u0010\u0003\b௜ሺ݂ǡ ܦሻൌ൛߶௜ሺ݂ǡ ݔሻെσ\n߶௜ǡ௝ሺ݂ǡ ݔሻ\n௝ஷ௜\nหݔא ܦൟ\u0003\u0003\n\u0003\n+HUH\u000f\u0003ࢥB\u000bL\u000fM\f\u0003LV\u0003WKH\u00036+$3\u0003IHDWXUH\u0003LQWHUDFWLRQ\u0003DV\u0003GHILQHG\u0003LQ\u0003>\u0017@\u000f\u0003DQG\u0003ZLWK\u0003ࢥBL\u0003\u000bI\u000f[\f\u0003ZH\u0003GHQRWH\u0003WKH\u0003\nGHSHQGHQF\\\u0003RQ\u0003WKH\u0003PRGHO\u0003I\u0003DQG\u0003WKH\u0003GDWD\u0003VHW\u0003'\u0011\u0003\u0003\n7KH\u0003LQWXLWLRQ\u0003EHKLQG\u0003WKLV\u0003PHDVXUH\u0003LV\u0003DQ\u0003HQKDQFHPHQW\u0003RI\u0003WKH\u00030D[\u00036+$3\u0003PHDVXUH\u0011\u0003$JDLQ\u000f\u0003ZH\u0003\nDVVXPH\u0003WKDW\u0003WKH\u0003RXWFRPH\u0003RI\u0003LQWHUHVW\u0003LV\u0003DVVRFLDWHG\u0003ZLWK\u0003KLJK\u00036+$3\u0003YDOXHV\u0011\u00032QH\u0003PD\\\u0003FKDQJH\u0003WKH\u0003\nIRUHVLJQ\u0003RI\u0003WKH\u0003PHDVXUH\u0003RU\u0003VXEVWLWXWH\u0003WKH\u0003PD[LPXP\u0003ZLWK\u0003D\u0003PLQLPXP\u0003IXQFWLRQ\u000f\u0003LI\u0003WKH\u0003RSSRVLWH\u0003LV\u0003\nWKH\u0003FDVH\u0011\u00037KH\u0003PD[LPDO\u00036+$3\u0003YDOXH\u0003RI\u0003D\u0003IHDWXUH\u0003PD\\\u0003EH\u0003KHDYLO\\\u0003GHSHQGHQW\u0003RQ\u0003WKH\u0003LQWHUDFWLRQ\u0003RI\u0003\nIHDWXUHV\u0011\u0003,Q\u0003WKLV\u0003FDVH\u000f\u0003WKH\u0003KLJK\u0003FRQWULEXWLRQ\u0003RI\u0003WKH\u0003IHDWXUH\u0003FDQQRW\u0003EH\u0003DVVHVVHG\u0003LQ\u0003LVRODWLRQ\u0011\u0003:LWK\u0003\nWKH\u00030D[\u00030DLQ\u0003(IIHFW\u0003PHDVXUH\u0003ZH\u0003DLP\u0003DW\u0003LGHQWLI\\LQJ\u0003IHDWXUHV\u000f\u0003ZKLFK\u0003KDYH\u0003D\u0003KLJK\u0003FRQWULEXWLRQ\u0003\nUHJDUGOHVV\u0003RI\u0003WKH\u0003RWKHU\u0003IHDWXUHV\u0011\u00037KLV\u0003LV\u0003DSSHDOLQJ\u0003LQ\u0003WKH\u0003WDUJHWHG\u0003XVH\u0003FDVH\u000f\u0003EHFDXVH\u0003VXFK\u0003IHDWXUHV\u0003\nVXSSRUW\u0003VLPSOH\u0003GHFLVLRQ\u0003UXOHV\u0003DQG\u0003YLVXDO\u0003DQDO\\VLV\u0003ZLWK\u0003RQO\\\u0003RQH\u0003GLPHQVLRQ\u0011\u0003\n5DQJH\u00036+$3\u0003\n7KH\u0003LGHD\u0003EHKLQG\u00035DQJH\u00036+$3\u0003LV\u0003WR\u0003ORRN\u0003IRU\u0003IHDWXUHV\u0003WKDW\u0003KDYH\u0003D\u0003VWURQJ\u0003LPSDFW\u0003RQ\u0003WKH\u0003RXWFRPH\u0003\nRYHU\u0003WKHLU\u0003YDOXH\u0003UDQJH\u0011\u00038QOLNH\u00030D[\u00036+$3\u000f\u0003LW\u0003FRQVLGHUV\u0003DOVR\u0003QHJDWLYH\u00036+$3\u0003YDOXHV\u0011\u00037KDW\u0003LW\u0003WDNHV\u0003\nLQWR\u0003DFFRXQW\u000f\u0003LI\u0003FHUWDLQ\u0003YDOXH\u0003UDQJHV\u0003RI\u0003WKH\u0003IHDWXUH\u0003DUH\u0003DQ\u0003LQGLFDWLRQ\u0003IRU\u0003QR\u0003HUURU\u0011\u0003\u000b$JDLQ\u000f\u0003ZH\u0003\nDVVXPH\u0003WKDW\u0003WKH\u0003RXWFRPH\u0003RI\u0003LQWHUHVW\u0003LV\u0003DVVRFLDWHG\u0003ZLWK\u0003KLJK\u00036+$3\u0003YDOXHV\u0011\f\u0003:H\u0003GHILQH\u0003WKH\u0003PHDVXUH\u0003\nDV\u0003IROORZV\u001d\u0003\n100\n\n\u0015\u0003\u0016\u000b\u0004\u0013௜ሺ݂ǡ ܦሻൌሼ߶௜ሺ݂ǡ ݔሻȁݔא ܦሽെሼ߶௜ሺ݂ǡ ݔሻȁݔא ܦሽ\n7KH\u0003LQWXLWLRQ\u0003EHKLQG\u0003WKLV\u0003PHDVXUH\u0003LV\u0003WR\u0003ORRN\u0003IRU\u0003IHDWXUHV\u0003WKDW\u0003FDQ\u0003KDYH\u0003D\u0003VWURQJ\u0003LPSDFW\u0003RQ\u0003WKH\u0003\nPRGHO\u0003RXWSXW\u0003LQ\u0003HLWKHU\u0003GLUHFWLRQ\u0011\u0003%\\\u0003ORRNLQJ\u0003DW\u0003WKH\u0003UDQJH\u000f\u0003ZH\u0003FDSWXUH\u0003WKH\u0003VWURQJHVW\u0003ORFDO\u0003HIIHFWV\u0011\u0003\n7KDW\u0003LV\u000f\u0003WKH\u0003VFRUH\u0003LV\u0003KLJK\u0003LI\u0003WKH\u0003IHDWXUH\u0003FRQWULEXWLRQ\u0003YDULHV\u0003VWURQJO\\\u0003EHWZHHQ\u0003VRPH\u0003FDVHV\u0011\u0003\n6PRRWKHG\u00035DQJH\u00036+$3\u0003\n7KH\u0003LGHD\u0003EHKLQG\u00036PRRWKHG\u00035DQJH\u00036+$3\u0003LV\u0003WR\u0003VXSSUHVV\u0003YDULDQFH\u0003IRU\u0003GDWD\u0003SRLQWV\u0003ZLWK\u0003WKH\u0003VDPH\u0003\nIHDWXUH\u0003YDOXH\u0011\u0003'XH\u0003WR\u0003IHDWXUH\u0003LQWHUDFWLRQ\u000f\u0003WKH\u0003VDPH\u0003IHDWXUH\u0003YDOXH\u0003PD\\\u0003FRUUHVSRQG\u0003WR\u0003GLIIHUHQW\u0003\n6+$3\u0003YDOXHV\u0011\u00036PRRWKHG\u00035DQJH\u00036+$3\u0003DYHUDJHV\u0003RYHU\u0003D\u0003VOLGLQJ\u0003ZLQGRZ\u0003WR\u0003VXSSUHVV\u0003YDULDWLRQ\u0003IRU\u0003\nWKH\u0003VLPLODU\u0003IHDWXUH\u0003YDOXHV\u0011\u00038VLQJ\u0003WKH\u0003IRUPXOD\u0003IRU\u0003PRYLQJ\u0003DYHUDJH\u0003WKH\u0003PHDVXUH\u0003LV\u0003GHILQHG\u0003DV\u0003\nIROORZV\u001d\u0003\n\u0016\u0003\u0015\u0003\u0016\u000b\u0004\u0013௜ሺ݂ǡ ܦሻ\nൌ݉ܽݔቆቊͳ\nܹσ\n߶௜ሺ݂ǡ ݔ௡ሻ\n௠ାௐିଵ\nଶ\n௡ୀ௠ିௐିଵ\nଶ\nቤܹെͳ\nʹ\n൑݉\u0003 ൑ȁܦȁ െܹെͳ\nʹ\nሽቋቇ\nെ݉݅݊ቆቊͳ\nܹσ\n߶௜ሺ݂ǡ ݔ௡ሻ\n௠ାௐିଵ\nଶ\n௡ୀ௠ିௐିଵ\nଶ\nቤܹെͳ\nʹ\n൑݉\u0003 ൑ȁܦȁ െܹെͳ\nʹ\nቋቇ\n+HUH\u0003ZH\u0003DVVXPH\u0003WKDW\u0003WKH\u0003GDWD\u0003VHW\u0003'\u0003LV\u0003VRUWHG\u0003E\\\u0003[\u0003DQG\u0003[L\u0003LV\u0003WKH\u0003L\u0010WK\u0003SRVLWLRQ\u0011\u0003\n7KH\u0003LQWXLWLRQ\u0003EHKLQG\u0003WKLV\u0003PHDVXUH\u0003LV\u0003DQ\u0003HQKDQFHPHQW\u0003RI\u00035DQJH\u00036+$3\u0011\u0003,I\u0003D\u0003IHDWXUH\u0003LQWHUDFWV\u0003\nVWURQJO\\\u0003ZLWK\u0003RWKHU\u0003IHDWXUHV\u000f\u0003WKH\u00036+$3\u0003YDOXHV\u0003PD\\\u0003KDYH\u0003D\u0003KLJK\u0003YDULDQFH\u0003IRU\u0003WKH\u0003VDPH\u0003RU\u0003VLPLODU\u0003\nIHDWXUH\u0003YDOXHV\u0011\u0003+RZHYHU\u000f\u0003LW\u0003LV\u0003PRUH\u0003LQWHUHVWLQJ\u0003IRU\u0003WKH\u0003WDUJHW\u0003DSSOLFDWLRQ\u0003WR\u0003ILQG\u0003FKDQJHV\u0003LQ\u0003WKH\u0003\nPRGHO\u0003RXWSXW\u000f\u0003ZKLFK\u0003FRUUHVSRQG\u0003WR\u0003GLIIHUHQW\u0003IHDWXUH\u0003YDOXHV\u0011\u00036XFK\u0003FKDQJHV\u0003DUH\u0003PRUH\u0003KHOSIXO\u0003IRU\u0003\nLGHQWLI\\LQJ\u0003VLPSOH\u0003SUHGLFWLRQ\u0003UXOHV\u0003DQG\u0003DUH\u0003WKHUHIRUH\u0003HPSKDVL]HG\u0003E\\\u0003WKLV\u0003LPSRUWDQFH\u0003PHDVXUH\u0011\u0003\n\u0017\u0003 (YDOXDWLRQ\u0003\n7R\u0003HYDOXDWH\u0003WKH\u0003SURSRVHG\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003ZLWK\u0003UHDO\u0010ZRUOG\u0003GDWD\u000f\u0003ZH\u0003DQDO\\]HG\u0003TXDOLW\\\u0003ORJV\u0003\nIURP\u0003WKH\u0003*HUPDQ\u0003PDQXIDFWXUHU\u00036,&.\u0003$*\u0011\u00031RWH\u0003WKDW\u0003ZH\u0003RPLW\u0003VRPH\u0003GHWDLOV\u0003LQ\u0003WKH\u0003GDWD\u0003GHVFULSWLRQ\u0003\nWKDW\u0003DUH\u0003RI\u0003PLQRU\u0003LPSRUWDQFH\u0003IRU\u0003WKH\u0003HYDOXDWLRQ\u0011\u00037KLV\u0003LV\u0003WR\u0003SURWHFW\u0003LQWHUQDO\u0003LQIRUPDWLRQ\u0003RI\u0003WKH\u0003\nPDQXIDFWXUHU\u0011\u00037KH\u0003DQDO\\]HG\u0003GDWD\u0003FRYHU\u0003D\u0003WLPH\u0003VSDQ\u0003RI\u0003URXJKO\\\u0003RQH\u0003\\HDU\u0003DQG\u0003FRQWDLQ\u0003UHFRUGV\u0003DERXW\u0003\nVHYHUDO\u0003WHQ\u0003WKRXVDQG\u0003SURGXFWV\u0003RI\u0003D\u0003VSHFLILF\u0003W\\SH\u0011\u0003\n6SHFLILFDOO\\\u000f\u0003ZH\u0003DQDO\\]HG\u0003WKH\u0003WHVW\u0003GDWD\u0003IURP\u0003D\u0003SURGXFWLRQ\u0003VWHS\u0003$\u0003DQG\u0003WKH\u0003RXWFRPH\u0003RI\u0003WHVWV\u0003IURP\u0003\nWKH\u0003VXEVHTXHQW\u0003VWHS\u0003%\u0011\u0003:H\u0003WUDLQHG\u0003D\u0003PRGHO\u0003\u000bL\u0011H\u0011\u0003;*%RRVW\u0003FODVVLILHU\f\u0003RQ\u0003\u0016\u0017\b\u0003RI\u0003WKH\u0003TXDOLW\\\u0003WHVW\u0003\nGDWD\u0003IURP\u0003VWHS\u0003$\u000f\u0003ZLWK\u0003WKH\u0003DLP\u0003WR\u0003SUHGLFW\u0003HUURUV\u0003LQ\u0003VWHS\u0003%\u0011\u0003)RU\u0003PRGHO\u0003WUDLQLQJ\u0003ZH\u0003XVHG\u0003URXJKO\\\u0003\n\u0014\u0013\u0013\u0003 WHVW\u0003 SDUDPHWHUV\u0003 IURP\u0003 VWHS\u0003 $\u0003 DV\u0003 IHDWXUHV\u0003 DQG\u0003 WKH\u0003 WHVW\u0003 UHVXOWV\u0003 IURP\u0003 VWHS\u0003 %\u0003 DV\u0003 ODEHO\u0011\u0003 )RU\u0003\nVLPSOLFLW\\\u000f\u0003ZH\u0003UHGXFHG\u0003GLIIHUHQW\u0003HUURU\u0003ODEHOV\u0003WR\u0003WKH\u0003ELQDU\\\u0003RXWFRPH\u0003³SDVVHG´\u0003RU\u0003³IDLOHG´\u0011\u0003\n:LWKLQ\u0003WKLV\u0003VHWXS\u0003ZH\u0003DQDO\\]HG\u0003WKH\u0003WUDLQHG\u0003PRGHO\u0003ZLWK\u0003UHJDUGV\u0003WR\u0003IHDWXUH\u0003LPSRUWDQFH\u0011\u0003:H\u0003WHVWHG\u0003\nWKH\u0003 HVWDEOLVKHG\u0003 DQG\u0003 QHZ\u0003 LQWURGXFHG\u0003 PHDVXUHV\u0003 OLVWHG\u0003 LQ\u0003 6HFWLRQ\u0003 \u0016\u0011\u0003 7KH\u0003 LQWURGXFHG\u0003 PHDVXUHV\u0003\nUHTXLUH\u0003GDWD\u0003LQVWDQFHV\u0003DQG\u0003WKH\u0003PRGHO\u0003DV\u0003LQSXW\u0011\u0003+HUH\u0003ZH\u0003XVHG\u0003WKH\u0003WUDLQLQJ\u0003GDWD\u0011\u0003:H\u0003XVHG\u0003WKH\u0003\n;*%RRVW\u0003OLEUDU\\\u0003IRU\u0003WKH\u0003LPSOHPHQWDWLRQ\u0003RI\u0003WKH\u0003PRGHO\u0016\u0003DQG\u0003WKH\u0003HVWDEOLVKHG\u0003LPSRUWDQFH\u0003PHDVXUHV\u0011\u0003\n7R\u0003LPSOHPHQW\u0003WKH\u0003QHZ\u0003SURSRVHG\u0003PHDVXUHV\u000f\u0003ZH\u0003VHW\u0003XS\u0003RQ\u0003WRS\u0003RI\u0003WKH\u00036+$3\u0003OLEUDU\\\u0003IRU\u0003FRPSXWLQJ\u0003\n6+$3\u0003YDOXHV\u0003DQG\u00036+$3\u0003LQWHUDFWLRQ\u0003YDOXHV\u0011\u00037KH\u0003SDUDPHWHUV\u0003\u000e\u0003DQG\u0003\u001a\u0003IRU\u0003RXU\u0003PHDVXUHV\u0003ZHUH\u0003VHW\u0003\nWR\u0003\u000eεͷͶ\u0003\u000bL\u0011H\u0011\u0003WKH\u0003WRS\u0010\u0014\u0013\u0003SUHGLFWLRQV\f\u0003DQG\u0003\u001aεͻͶ\u0003\u000bL\u0011H\u0011\u0003D\u0003VPRRWKLQJ\u0003ZLQGRZ\u0003RI\u0003\u0018\u0013\u0003YDOXHV\f\u0011\u0003\n\u0016\u0003:H\u0003XVHG\u0003GHIDXOW\u0003SDUDPHWHUV\u000f\u0003ZLWK\u0003H[FHSWLRQ\u0003RI\u0003VFDOHBSRVBZHLJKW\u0011\u00037KLV\u0003SDUDPHWHU\u0003ZDV\u0003DGMXVWHG\u0003\nWR\u0003UHIOHFW\u0003WKH\u0003FRVW\u0003IRU\u0003IDOVH\u0003DQG\u0003WUXH\u0003SRVLWLYHV\u0003LQ\u0003WKH\u0003PDQXIDFWXULQJ\u0003OLQH\u0011\u0003\n101\n\n\u0003\n\u0003\n7KH\u0003DQDO\\VLV\u0003UHVXOWV\u0003LQ\u0003D\u0003UDQNHG\u0003OLVW\u0003RI\u0003IHDWXUHV\u0003IRU\u0003HDFK\u0003WHVWHG\u0003LPSRUWDQFH\u0003PHDVXUH\u0011\u00037DEOH\u0003\u0016\u0003\nVKRZV\u0003WKH\u0003WRS\u0003\u0014\u0013\u0003UHVXOWV\u0003IRU\u0003HVWDEOLVKHG\u0003PHDVXUHV\u0003DQG\u00037DEOH\u0003\u0017\u0003GHSLFWV\u0003WKH\u0003WRS\u0003\u0014\u0013\u0003UHVXOWV\u0003IRU\u0003WKH\u0003\nQHZ\u0003LQWURGXFHG\u0003PHWKRGV\u0011\u0003)HDWXUH\u0003QDPHV\u0003DUH\u0003REIXVFDWHG\u0003WR\u0003SURWHFW\u0003LQWHUQDO\u0003LQIRUPDWLRQ\u0003RI\u0003WKH\u0003\nPDQXIDFWXUHU\u0011\u0003,Q\u0003ERWK\u0003WDEOHV\u0003ZH\u0003KLJKOLJKW\u0003IHDWXUH\u0003'IOHIWBFROB\u0014\u0018\u001c\u0016\u0003DQG\u0003'IOHIWBFROB\u0016\u001a\u001c\u0011\u0003:H\u0003DUJXH\u0003\nWKDW\u0003 \u0010\u0003 DPRQJVW\u0003 WKH\u0003 OLVWHG\u0003 IHDWXUHV\u0003 \u0010\u0003 WKHVH\u0003 WZR\u0003 IHDWXUHV\u0003 DUH\u0003 RI\u0003 PRVW\u0003 LQWHUHVW\u0003 LQ\u0003 WKH\u0003 WDUJHWHG\u0003\nDSSOLFDWLRQ\u0011\u0003+HQFH\u000f\u0003ZH\u0003H[SHFW\u0003WKDW\u0003DQ\u0003HIIHFWLYH\u0003LPSRUWDQFH\u0003PHDVXUH\u0003UDQNV\u0003WKHVH\u0003IHDWXUHV\u0003KLJK\u0011\u0003\n)LJXUH\u0003 \u0015\u0003 DQG\u0003 )LJXUH\u0003 \u0017\u0003 SURYLGH\u0003 WKH\u0003 GHWDLOV\u0003 IRU\u0003 RXU\u0003 DUJXPHQW\u0003 DERXW\u0003 WKH\u0003 LPSRUWDQFH\u0003 RI\u0003\n'IOHIWBFROB\u0014\u0018\u001c\u0016\u0003 DQG\u0003 'IOHIWBFROB\u0016\u001a\u001c\u0011\u0003 7KH\u0003 ILJXUHV\u0003 VKRZ\u0003 WKH\u0003 GLVWULEXWLRQ\u0003 RI\u0003 IHDWXUH\u0003 YDOXHV\u0003 DV\u0003\nKLVWRJUDP\u0011\u00037KH\u0003<\u0010D[HV\u0003GLVSOD\\V\u0003WKH\u0003IUHTXHQF\\\u0003RI\u0003IHDWXUH\u0003YDOXHV\u0003DQG\u0003KDYH\u0003ORJDULWKPLF\u0003VFDOH\u0011\u00037KH\u0003\n;\u0010D[LV\u0003 LV\u0003 VFDOHG\u0003 WR\u0003 FRYHU\u0003 WKH\u0003 ZKROH\u0003 YDOXH\u0003 UDQJH\u0011\u0003 :H\u0003 RPLWWHG\u0003 D[LV\u0003 ODEHOV\u0003 WR\u0003 SURWHFW\u0003 LQWHUQDO\u0003\nLQIRUPDWLRQ\u0003RI\u0003WKH\u0003PDQXIDFWXUHU\u0011\u00037KH\u0003VKDGLQJ\u0003RI\u0003WKH\u0003EDUV\u0003HQFRGH\u0003WKH\u0003SHUFHQWDJH\u0003RI\u0003HUURUV\u0003\u000bIDXOW\\\u0003\nSURGXFW\f\u0003LQ\u0003WKH\u0003UHVSHFWLYH\u0003EDU\u0003\u000bZKLWH\u0003\u0013\b\u0003DQG\u0003EODFN\u0003\u0014\u0013\u0013\b\u0003HUURUV\f\u0011\u0003:H\u0003VKRZ\u0003DOO\u0003IHDWXUHV\u0003WKDW\u0003DUH\u0003\nDPRQJ\u0003WKH\u0003WRS\u0003WKUHH\u0003IHDWXUHV\u0003RI\u0003DQ\\\u0003WHVWHG\u0003LPSRUWDQFH\u0003PHDVXUH\u0011\u0003\n\u0003\n7DEOH\u0003\u0016\u001d\u0003)HDWXUHV\u0003UDQNHG\u0003E\\\u0003HVWDEOLVKHG\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003\n\u0004ǀĞƌĂŐĞ\u0003^,\u0004W\u0003\n\u0012ŽǀĞƌ\u0003\n'ĂŝŶ\u0003\ndŽƚĂů\u0003'ĂŝŶ\u0003\ntĞŝŐŚƚ\u0003\n\u0018ĨůĞĨƚͺŝĚ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϱ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϱ\u0003\n\u0018ĨůĞĨƚͺŝĚ\u0003\n\u0018ĨůĞĨƚͺŝĚ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϴϯϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϴϯϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϴϯϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϵϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϱϳ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϬϱ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϬϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϭϭ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϵϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϵϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϬϱ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϲϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϱ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϳϵ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϭϭ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϵϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϴϯϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϯϮϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϵϯϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϲϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϱϳ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϱϭ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϰϱ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϭϰ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϭϰ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϲϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϮϱ\u0003\n\u0018ĨůĞĨƚͺŝĚ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϭϭ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϲϳ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϱϳ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϳϵ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϲϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϭϭ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϲϲ\u0003\n\u0003\n7DEOH\u0003\u0017\u001d\u0003)HDWXUHV\u0003UDQNHG\u0003E\\\u0003SURSRVHG\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003\nDĂǆ\u0003DĂŝŶ\u0003\u001cĨĨĞĐƚ\u0003\nDĂǆ\u0003^,\u0004W\u0003\nZĂŶŐĞ\u0003^,\u0004W\u0003\n^ŵŽŽƚŚĞĚ\u0003ZĂŶŐĞ\u0003\n^,\u0004W\u0003\ndŽƉͲ<\u0003^,\u0004W\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϵϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϳϵ\u0003\n\u0018ĨůĞĨƚͺŝĚ\u0003\n\u0018ĨůĞĨƚͺŝĚ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϵϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϳϵ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϵϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϵϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϱϵϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺŝĚ\u0003\n\u0018ĨůĞĨƚͺŝĚ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϳϵ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϳϵ\u0003\n\u0018ĨůĞĨƚͺŝĚ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϯϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϮϬ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϬϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϵϯϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϴϯϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϴϯϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϱ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϬϴ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϯϮϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϯϮϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϱ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϱϳ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϱ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϱϳ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϯϱϳ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϮϬ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϭϰ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϴϭϭ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϭϰ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϬϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϭϬ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϰϱ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϯϮϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϮϬ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϭϰ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϲϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϭϮϲ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϭϰ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϬϯ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϵϯϮ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϭϮϲϳ\u0003\n\u0018ĨůĞĨƚͺĐŽůͺϳϭϭ\u0003\n\u0003\n$V\u0003)LJXUH\u0003\u0015\u0003VKRZV\u000f\u0003WKH\u0003IHDWXUHV\u0003'IOHIWBFROB\u0014\u0018\u001c\u0016\u0003DQG\u0003'IOHIWBFROB\u0016\u001a\u001c\u0003KDYH\u0003GHVLUDEOH\u0003SURSHUWLHV\u0003\nIRU\u0003 WKH\u0003 WDUJHWHG\u0003 XVH\u0003 FDVH\u0003 \u000bKRZHYHU\u0003 WKH\u0003 HUURU\u0003 QXPEHUV\u0003 WKDW\u0003 VXSSRUW\u0003 WKLV\u0003 REVHUYDWLRQ\u0003 IRU\u0003\n'IOHIWBFROB\u0016\u001a\u001c\u0003DUH\u0003UDWKHU\u0003VPDOO\f\u0011\u0003)RU\u0003ERWK\u0003IHDWXUHV\u0003RQH\u0003FDQ\u0003REVHUYH\u0003DQ\u0003LQWHUYDO\u0003ZLWK\u0003D\u0003ORZ\u0003HUURU\u0003\nUDWH\u0011\u00032XWVLGH\u0003WKLV\u0003LQWHUYDO\u000f\u0003WKH\u0003HUURU\u0003UDWH\u0003LV\u0003YHU\\\u0003KLJK\u0011\u00037KLV\u0003DOORZV\u0003TXDOLW\\\u0003HQJLQHHUV\u0003WR\u0003GHULYH\u0003\nVLPSOH\u0003UXOHV\u0003RI\u0003WKH\u0003IRUP\u0003³,)\u0003YDOXH\u0003!\u0003;\u0003RU\u0003YDOXH\u0003\u001f\u0003<\u00037+(1\u0003(5525´\u0011\u0003)RU\u0003'IHIWBFROB\u0014\u0018\u001c\u0016\u0003DQG\u0003\n'IOHIWBFROB\u0016\u001a\u001c\u0003WKLV\u0003UXOH\u0003ZRXOG\u0003DSSO\\\u0003LQ\u0003IHZ\u0003FDVHV\u0003DQG\u0003±\u0003DFFRUGLQJ\u0003WR\u0003WKH\u0003WUDLQLQJ\u0003GDWD\u0003±\u0003KDV\u0003KLJK\u0003\nSUHGLFWLRQ\u0003 TXDOLW\\\u0011\u0003 6HYHUDO\u0003 RI\u0003 WKH\u0003 RWKHU\u0003 LGHQWLILHG\u0003 IHDWXUHV\u0003 VKDUH\u0003 WKLV\u0003 SURSHUW\\\u0003 RI\u0003 IHDWXUH\u0003\n'IOHIWBFROB\u0014\u0018\u001c\u0016\u0003DQG\u0003'IOHIWBFROB\u0016\u001a\u001c\u0011\u0003+RZHYHU\u000f\u0003WKH\u0003FRUUHVSRQGLQJ\u0003UHODWLRQV\u0003DUH\u0003ZHDNHU\u0011\u00037KDW\u0003LV\u000f\u0003\nWKH\u0003HUURU\u0003UDWHV\u0003RXWVLGH\u0003WKH\u0003UHVSHFWLYH\u0003LQWHUYDOV\u0003DUH\u0003ORZHU\u0003DQG\u0012RU\u0003WKH\u0003UXOHV\u0003ZRXOG\u0003DSSO\\\u0003WR\u0003IHZHU\u0003\nLQVWDQFHV\u0011\u0003 :H\u0003 WKHUHIRUH\u0003 DUJXH\u0003 WKDW\u0003 WKH\u0003 IHDWXUH\u0003 'IHIWBFROB\u0014\u0018\u001c\u0016\u0003 DQG\u0003 'IOHIWBFROB\u0016\u001a\u001c\u0003 DUH\u0003 PRVW\u0003\nLPSRUWDQW\u0003DQG\u0003VKRXOG\u0003EH\u0003UDQNHG\u0003KLJK\u0003E\\\u0003WKH\u0003LPSRUWDQFH\u0003PHDVXUHV\u0011\u00031RWH\u0003WKDW\u0003WKLV\u0003LV\u0003WKH\u0003FDVH\u0003IRU\u0003\nWKH\u0003QHZ\u0003LQWURGXFHG\u0003PHDVXUHV\u0003DQG\u0003LQ\u0003SDUWLFXODU\u0003IRU\u00030D[\u00030DLQ\u0003(IIHFW\u0003DQG\u00030D[\u00036+$3\u0011\u0003\n102\n\n)LJXUH\u0003\u0015\u001d\u0003'LVWULEXWLRQ\u0003RI\u0003IHDWXUH\u0003YDOXHV\u0003IRU\u0003WUDLQLQJ\u0003GDWD\u0011\u0003\u000b<\u0010D[HV\u0003ZLWK\u0003ORJDULWKPLF\u0003VFDOH\u000f\u0003FRORU\u0003FRGHG\u0003HUURU\u0003\nSHUFHQWDJH\u000f\u0003ZKLWH \u0013\b\u000f\u0003EODFN \u0014\u0013\u0013\b\f\u0011\u0003\n)XUWKHUPRUH\u000f\u0003WKH\u0003IHDWXUHV\u0003'IOHIWBLG\u0003DQG\u0003'IOHIWBFROB\u0016\u0018\u001a\u0003VWLFN\u0003RXW\u0011\u00037KHVH\u0003IHDWXUHV\u0003GR\u0003QRW\u0003VKRZ\u0003D\u0003\nFOHDU\u0003UHODWLRQ\u0003ZLWK\u0003HUURUV\u0003LQ\u0003)LJXUH\u0003\u0015\u0011\u0003)LJXUH\u0003\u0016\u0003SURYLGHV\u0003PRUH\u0003GHWDLOV\u0003RQ\u0003WKHVH\u0003IHDWXUHV\u0011\u0003)RU\u0003\n'IOHIWBLG\u0003DQG\u0003'IOHIWBFROB\u0016\u0018\u001a\u0003LW\u0003VKRZV\u0003WKH\u00036+$3\u0003YDOXHV\u0003DQG\u0003IHDWXUH\u0003YDOXHV\u0003IRU\u0003HDFK\u0003GDWD\u0003SRLQW\u0003\nLQ\u0003WKH\u0003WUDLQLQJ\u0003GDWD\u0011\u00037KH\u0003SORWV\u0003SURYLGH\u0003LQVLJKWV\u0003RQ\u0003ZK\\\u0003WKHVH\u0003IHDWXUHV\u0003DUH\u0003FRQVLGHUHG\u0003LPSRUWDQW\u0003\nE\\\u0003VRPH\u0003PHDVXUHV\u0011\u0003)RU\u0003VRPH\u0003GDWD\u0003SRLQWV\u0003WKH\u0003IHDWXUHV\u0003KDYH\u0003VWURQJ\u0003FRQWULEXWLRQV\u0003WR\u0003WKH\u0003PRGHO\u0003\nRXWSXW\u0011\u0003+RZHYHU\u000f\u0003WKH\u0003YDULDQFH\u0003RI\u0003WKH\u0003FRQWULEXWLRQ\u0003LV\u0003KLJK\u000f\u0003LQ\u0003SDUWLFXODU\u0003LQ\u0003UHJLRQV\u0003ZLWK\u0003SRWHQWLDO\u0003\nKLJK\u0003LPSDFW\u0011\u0003\u000b7KLV\u0003FDQ\u0003EH\u0003VHHQ\u0003LQ\u0003WKH\u0003YHUWLFDO\u0003DUUDQJHPHQW\u0003RI\u0003SRLQWV\u0003EHORZ\u0003RU\u0003DERYH\u0003SRLQW\u0003ZLWK\u0003\nKLJK\u0003FRQWULEXWLRQ\f\u0011\u00037KH\u0003ILJXUH\u0003LPSOLHV\u0003WKDW\u0003WKHVH\u0003IHDWXUHV\u0003KDYH\u0003VWURQJ\u0003LQWHUDFWLRQV\u0003ZLWK\u0003RWKHU\u0003\nIHDWXUHV\u0003DQG\u0003GR\u0003QRW\u0003HQDEOH\u0003JRRG\u0003H[SODQDWLRQV\u0003RQ\u0003WKHLU\u0003RZQ\u0011\u00037KXV\u000f\u0003WKH\u0003IHDWXUHV\u0003DUH\u0003QRW\u0003XVHIXO\u0003IRU\u0003\nVLPSOH\u0003SUHGLFWLRQ\u0003UXOHV\u0003WKDW\u0003FRQVLGHU\u0003RQO\\\u0003RQH\u0003YDOXH\u0011\u0003\u0003\n$V\u0003D\u0003VLGH\u0003QRWH\u000f\u0003WKH\u0003IHDWXUH\u0003'IOHIWBLG\u0003LV\u0003DQ\u0003LGHQWLILHU\u0003YDOXH\u0003WKDW\u0003URXJKO\\\u0003UHVHPEOHV\u0003D\u0003FRXQWHU\u0011\u00037KLV\u0003\nIHDWXUH\u0003LV\u0003QRW\u0003XVHIXO\u0003IRU\u0003SUHGLFWLRQV\u0003EXW\u0003FDQ\u0003KHOS\u0003DQDO\\VLV\u0003LQ\u0003UHWURVSHFW\u0011\u0003\n)LJXUH\u0003\u0016\u001d\u00036+$3\u0003YDOXHV\u0003IRU\u0003'IOHIWBLG\u0003DQG\u0003'IOHIWBFROB\u0016\u0018\u001a\u0003\n)RU\u0003RXU\u0003DQDO\\VLV\u0003ZH\u0003VSOLW\u0003WKH\u0003GDWD\u0003LQWR\u0003WUDLQLQJ\u0003DQG\u0003WHVW\u0003GDWD\u0011\u0003+RZHYHU\u000f\u0003WKH\u0003HYDOXDWLRQ\u0003RQ\u0003WKH\u0003\nWUDLQLQJ\u0003GDWD\u0003PD\\\u0003EH\u0003PRUH\u0003PHDQLQJIXO\u0003LQ\u0003WKH\u0003WDUJHWHG\u0003XVH\u0003FDVH\u0011\u00037KLV\u0003LV\u0003EHFDXVH\u0003WKH\u0003PDLQ\u0003JRDO\u0003LV\u0003\nQRW\u0003WR\u0003EXLOG\u0003D\u0003JHQHUDO\u0003SUHGLFWLRQ\u0003PRGHO\u000f\u0003EXW\u0003WR\u0003SRLQW\u0003TXDOLW\\\u0003HQJLQHHUV\u0003WR\u0003LQWHUHVWLQJ\u0003SKHQRPHQD\u0003\nLQ\u0003WKH\u0003GDWD\u0003\u000bSRVVLEO\\\u0003RQO\\\u0003LQ\u0003UHWURVSHFW\f\u0011\u0003,Q\u0003WKLV\u0003FDVH\u0003WKH\u0003KXPDQ\u0003FDQ\u0003MXGJH\u0003WKH\u0003YDOLGLW\\\u0003RI\u0003WKH\u0003\nILQGLQJV\u0003EDVHG\u0003RQ\u0003GRPDLQ\u0003NQRZOHGJH\u0003DQG\u0003YDOLGDWLRQ\u0003WKURXJK\u0003D\u0003VHSDUDWH\u0003WHVW\u0003VHW\u0003PD\\\u0003EH\u0003RI\u0003OHVV\u0003\nLPSRUWDQFH\u0011\u0003 \u000b6HH\u0003 >\u0015@\u0003 IRU\u0003 D\u0003 PRUH\u0003 GHWDLOHG\u0003 GLVFXVVLRQ\u0003 RI\u0003 XVLQJ\u0003 WHVW\u0003 VHWV\u0003 RU\u0003 WUDLQLQJ\u0003 VHWV\u0003 IRU\u0003\nHYDOXDWLQJ\u0003DSSURDFKHV\u0003RI\u0003LQWHUSUHWDEOH\u0003PDFKLQH\u0003OHDUQLQJ\f\u0011\u0003,Q\u0003RXU\u0003WHVW\u0003FDVH\u000f\u0003WKH\u0003LQVLJKW\u0003IURP\u0003WKH\u0003\nPRGHO\u0003ZRXOG\u0003KDYH\u0003EHHQ\u0003DYDLODEOH\u0003DIWHU\u0003\u0016\u0017\b\u0003RI\u0003WKH\u0003DQDO\\]HG\u0003SHULRG\u0003DQG\u0003EHIRUH\u0003WKH\u0003WHVW\u0003GDWD\u0003LV\u0003\nDYDLODEOH\u0011\u0003+RZHYHU\u000f\u0003LW\u0003LV\u0003VWLOO\u0003LQWHUHVWLQJ\u0003WR\u0003VHH\u0003KRZ\u0003GHULYHG\u0003UXOHV\u0003ZRXOG\u0003KDYH\u0003SOD\\HG\u0003RXW\u0003RQ\u0003WKH\u0003\nWHVW\u0003GDWD\u0003\u000bVHH\u0003)LJXUH\u0003\u0017\f\u0011\u00032YHUDOO\u000f\u0003ZH\u0003ILQG\u0003WKDW\u0003LQVLJKWV\u0003IURP\u0003WKH\u0003WUDLQLQJ\u0003VHW\u0003FRQWLQXH\u0003WR\u0003EH\u0003YDOLG\u0003\nLQ\u0003WKH\u0003WHVW\u0003VHW\u0011\u0003<HW\u000f\u0003WKH\u0003WRWDO\u0003IUHTXHQF\\\u0003RI\u0003HUURUV\u0003GHFUHDVHG\u0011\u00037KLV\u0003LV\u0003QRW\u0003VXUSULVLQJ\u000f\u0003VLQFH\u0003WKH\u0003\nREVHUYHG\u0003SURFHVV\u0003LV\u0003VXEMHFW\u0003WR\u0003FRQWLQXRXV\u0003LPSURYHPHQWV\u0003E\\\u0003WKH\u0003TXDOLW\\\u0003HQJLQHHUV\u0011\u0003\n103\n\n\u0003\n\u0003\n\u0003\n)LJXUH\u0003\u0017\u001d\u0003'LVWULEXWLRQ\u0003RI\u0003IHDWXUH\u0003YDOXHV\u0003IRU\u0003WHVW\u0003GDWD\u0011\u0003\u000b<\u0010D[HV\u0003ZLWK\u0003ORJDULWKPLF\u0003VFDOH\u000f\u0003FRORU\u0003FRGHG\u0003HUURU\u0003\nSHUFHQWDJH\u000f\u0003ZKLWH \u0013\b\u000f\u0003EODFN \u0014\u0013\u0013\b\f\u0011\u0003\n\u0018\u0003 5HODWHG\u0003:RUN\u0003\n5HFHQWO\\\u0003WKHUH\u0003KDV\u0003EH\u0003D\u0003VWURQJ\u0003LQFUHDVH\u0003RI\u0003LQWHUHVW\u0003LQ\u0003ZRUNV\u0003RQ\u0003LQWHUSUHWDEOH\u0003PDFKLQH\u0003OHDUQLQJ\u0017\u0011\u0003\n6XFK\u0003ZRUNV\u0003DLP\u0003WR\u0003DGGUHVV\u0003WKH\u0003RSDTXHQHVV\u0003RI\u0003PDQ\\\u0003PDFKLQH\u0003OHDUQLQJ\u0003PRGHOV\u0003DQG\u0003WR\u0003SURYLGH\u0003\nKXPDQ\u0003XQGHUVWDQGDEOH\u0003H[SODQDWLRQV\u0011\u0003\u000b6HH\u0003>\u0014@\u0003RU\u0003>\u0015@\u0003IRU\u0003DQ\u0003RYHUYLHZ\u0011\f\u0003:RUNV\u0003IURP\u0003WKLV\u0003FDWHJRU\\\u0003\nDUH\u0003JHQHUDOO\\\u0003UHODWHG\u0003WR\u0003RXU\u0003ZRUN\u0011\u0003+RZHYHU\u000f\u0003ZH\u0003DUH\u0003QRW\u0003DZDUH\u0003RI\u0003DQ\\\u0003DSSURDFK\u0003WKDW\u0003LV\u0003WDLORUHG\u0003WR\u0003\nWKH\u0003VSHFLILF\u0003QHHGV\u0003RI\u0003TXDOLW\\\u0003HQJLQHHULQJ\u0003LQ\u0003PDQXIDFWXULQJ\u0011\u00037KH\u0003H[LVWLQJ\u0003ZRUNV\u0003W\\SLFDOO\\\u0003DLP\u0003DW\u0003\nSURYLGLQJ\u0003D\u0003KROLVWLF\u0003XQGHUVWDQGLQJ\u0003RI\u0003D\u0003PRGHO\u0011\u0003,Q\u0003FRQWUDVW\u000f\u0003RXU\u0003ZRUN\u0003DLPV\u0003DW\u0003HOHFWLQJ\u0003VSHFLILF\u0003\nLQVLJKWV\u0003IURP\u0003WKH\u0003PRGHO\u0003WKDW\u0003DUH\u0003KHOSIXO\u0003IRU\u0003TXDOLW\\\u0003HQJLQHHUV\u0011\u0003\n0RUH\u0003 VSHFLILFDOO\\\u0003 UHODWHG\u0003 WR\u0003RXU\u0003 ZRUN\u0003 DUH\u0003 ZRUNV\u0003 RQ\u0003 IHDWXUH\u0003 LPSRUWDQFH\u0011\u0003 :H\u0003 FDQ\u0003 GLVWLQJXLVK\u0003\nEHWZHHQ\u0003 JOREDO\u0003 LPSRUWDQFH\u0003 PHDVXUHV\u0003 DQG\u0003 ±\u0003 PRUH\u0003 UHFHQWO\\\u0003 LQWURGXFHG\u0003 ±\u0003 ORFDO\u0003 LPSRUWDQFH\u0003\nPHDVXUHV\u0011\u0003*OREDO\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003OLNH\u0003JDLQ\u0003>\u0019@\u000f\u0003RU\u0003DQG\u0003VLPLODU\u0003PHDVXUHV\u0003IRU\u0003WUHH\u0003PRGHOV\u0003\n\u000bH\u0011J\u0011\u0003DV\u0003LPSOHPHQWHG\u0003LQ\u0003>\u001a@\f\u0003DUH\u0003ZHOO\u0003HVWDEOLVKHG\u0011\u00037KH\\\u0003DUH\u0003VRPH\u0003IRUP\u0003RI\u0003DJJUHJDWH\u0003WKDW\u0003DLP\u0003DW\u0003\nFDSWXULQJ\u0003WKH\u0003W\\SLFDO\u0003\u000bH\u0011J\u0011\u0003DYHUDJH\f\u0003LPSRUWDQFH\u0003RI\u0003D\u0003IHDWXUH\u0011\u00037KLV\u0003W\\SH\u0003RI\u0003DJJUHJDWH\u0003FRQFHDOV\u0003\nIHDWXUHV\u0003WKDW\u0003DUH\u0003RQO\\\u0003LPSRUWDQW\u0003LQ\u0003VRPH\u0003UDUH\u0003FDVHV\u0011\u0003+RZHYHU\u000f\u0003VXFK\u0003UDUH\u0003FDVHV\u0003FDQ\u0003EH\u0003RI\u0003PRVW\u0003\nLQWHUHVW\u0003LQ\u0003TXDOLW\\\u0003PDQDJHPHQW\u0003IRU\u0003PDQXIDFWXULQJ\u0011\u0003\n/RFDO\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003VXFK\u0003DV\u0003/,0(\u0003>\u001b@\u0003DQG\u00036+$3\u0003>\u0017@\u0003H[SODLQ\u0003IHDWXUH\u0003LPSRUWDQFH\u0003IRU\u0003\nLQGLYLGXDO\u0003 GDWD\u0003 SRLQWV\u0011\u0003 7KLV\u0003 LV\u0003 XVHIXO\u0003 IRU\u0003 XQGHUVWDQGLQJ\u0003 VSHFLILF\u0003 SUHGLFWLRQV\u0011\u0003 +RZHYHU\u000f\u0003 WKH\\\u0003\nUHTXLUH\u0003DGGLWLRQDO\u0003DQDO\\VLV\u0003WR\u0003JDLQ\u0003LQVLJKWV\u0003EH\\RQG\u0003WKH\u0003VFRSH\u0003RI\u0003VLQJOH\u0003GDWD\u0003SRLQWV\u0011\u0003&ORVHO\\\u0003UHODWHG\u0003\nWR\u0003RXU\u0003ZRUN\u0003LV\u0003WKH\u0003ZRUN\u0003RI\u0003/XQGEHUJ\u0003HW\u0003DO\u0011\u0003>\u0018@\u000f\u0003ZKR\u0003DGGUHVV\u0003YLVXDOL]DWLRQV\u0003RQ\u0003WRS\u0003RI\u00036+$3\u0003YDOXHV\u0011\u0003\n6XFK\u0003YLVXDOL]DWLRQV\u0003DUH\u0003KHOSIXO\u0003IRU\u0003D\u0003GHWDLOHG\u0003DQDO\\VLV\u0011\u0003<HW\u000f\u0003WKH\\\u0003VKRZ\u0003LQGLYLGXDO\u0003GDWD\u0003SRLQWV\u0003DQG\u0003\nOHDYH\u0003WKH\u0003LQWHUSUHWDWLRQ\u0003WR\u0003WKH\u0003XVHU\u0011\u0003,Q\u0003FRQWUDVW\u000f\u0003RXU\u0003ZRUN\u0003SRLQWV\u0003TXDOLW\\\u0003HQJLQHHUV\u0003WR\u0003IHDWXUHV\u0003RI\u0003\nLQWHUHVW\u0011\u0003/XQGEHUJ\u0003HW\u0003DO\u0011\u0003>\u0018@\u0003DOVR\u0003XVH\u0003PHDQ\u00036+$3\u0003YDOXHV\u0003DV\u0003JOREDO\u0003LPSRUWDQFH\u0003PHDVXUH\u0011\u00037KLV\u0003LV\u0003\nUHODWHG\u0003 WR\u0003 RXU\u0003 ZRUN\u0003 LQ\u0003 WKH\u0003 VHQVH\u0003 WKDW\u0003 LW\u0003 LV\u0003 DQ\u0003 LPSRUWDQFH\u0003 PHDVXUH\u0003 RQ\u0003 WRS\u0003 RI\u0003 6+$3\u0003 YDOXHV\u0011\u0003\n+RZHYHU\u000f\u0003 WKH\u0003 PHDVXUH\u0003 LV\u0003 QRW\u0003 GHVLJQHG\u0003 IRU\u0003 WKH\u0003 VSHFLILF\u0003 QHHGV\u0003 RI\u0003 RXU\u0003 XVH\u0003 FDVH\u0003 DQG\u0003 ±\u0003 DV\u0003 RXU\u0003\nH[SHULPHQWV\u0003VKRZ\u0003±\u0003LW\u0003LV\u0003OHVV\u0003HIIHFWLYH\u0003WKDQ\u0003RXU\u0003PHDVXUHV\u0003LQ\u0003WKLV\u0003FRQWH[W\u0011\u0003\n&RKHQ\u0003 HW\u0003 DO\u0011\u0003 OHYHUDJH\u0003 6KDSOH\\\u0003 YDOXHV\u0003 LQ\u0003 D\u0003 IHDWXUH\u0003 VHOHFWLRQ\u0003 PHFKDQLVP\u0003 >\u001c@\u0011\u0003 7KHUHE\\\u0003 WKH\\\u0003\nLQGLUHFWO\\\u0003GHILQH\u0003IHDWXUH\u0003LPSRUWDQFH\u0003RQ\u0003WRS\u0003RI\u00036KDSOH\\\u0003YDOXHV\u000f\u0003VLPLODU\u0003WR\u0003>\u0018@\u0011\u0003+RZHYHU\u000f\u0003WKH\\\u0003IRFXV\u0003\nRQ\u0003 PD[LPL]LQJ\u0003 WKH\u0003 RYHUDOO\u0003 SHUIRUPDQFH\u0003 RI\u0003 FODVVLILHUV\u0011\u0003 7KDW\u0003 LV\u000f\u0003 OLNH\u0003 RWKHU\u0003 JOREDO\u0003 LPSRUWDQFH\u0003\nPHDVXUHV\u000f\u0003WKH\\\u0003DLP\u0003DW\u0003FDSWXULQJ\u0003 WKH\u0003W\\SLFDO\u0003LPSRUWDQFH\u0003RI\u0003D\u0003IHDWXUH\u0011\u00037KHUHIRUH\u0003±\u0003XQOLNH\u0003RXU\u0003\nPHDVXUHV±\u0003WKHLU\u0003DQDO\\VLV\u0003LV\u0003QRW\u0003WDLORUHG\u0003WR\u0003LGHQWLI\\LQJ\u0003XVHIXO\u0003IHDWXUHV\u0003IRU\u0003TXDOLW\\\u0003HQJLQHHUV\u0011\u0003\n\u0003\n\u0017\u0003:H\u0003XVH\u0003WKH\u0003WHUP\u0003WR\u0003UHIHU\u0003WR\u0003DSSURDFKHV\u0003RI\u0003H[SODLQDEOH\u00030/\u0003DV\u0003ZHOO\u000f\u0003ZKLOH\u0003DFNQRZOHGJLQJ\u0003DQ\u0003\nRQJRLQJ\u0003GHEDWH\u0003RQ\u0003KRZ\u0003WR\u0003VSHFLI\\\u0003WKH\u0003GLIIHUHQFH\u0003EHWZHHQ\u0003WKH\u0003WHUPV\u0011\u0003\n104\n\n\u0019\u0003 &RQFOXVLRQ\u0003DQG\u0003)XWXUH\u0003:RUN\u0003\n,Q\u0003WKLV\u0003SDSHU\u0003ZH\u0003LQWURGXFHG\u0003IHDWXUH\u0003LPSRUWDQFH\u0003PHDVXUHV\u0003WKDW\u0003DUH\u0003WDLORUHG\u0003WR\u0003WKH\u0003QHHGV\u0003RI\u0003TXDOLW\\\u0003\nHQJLQHHUV\u0003LQ\u0003PDQXIDFWXULQJ\u0011\u00037KH\\\u0003OHYHUDJH\u00036+$3\u0003YDOXHV\u0003WR\u0003LGHQWLI\\\u0003ORFDOO\\\u0003LPSRUWDQW\u0003IHDWXUHV\u0011\u0003\n$ORQJ\u0003V\\QWKHWLF\u0003DQG\u0003UHDO\u0010ZRUOG\u0003GDWD\u0003ZH\u0003GHPRQVWUDWHG\u0003WKH\u0003EHQHILWV\u0003RI\u0003WKHVH\u0003PHDVXUHV\u0011\u00032XU\u0003WHVWV\u0003\nLQGLFDWH\u0003WKDW\u0003³0D[\u00030DLQ\u0003(IIHFW´\u0003DQG\u0003³0D[\u00036+$3´\u0003DUH\u0003PRVW\u0003SURPLVLQJ\u0003DPRQJ\u0003WKH\u0003LQWURGXFHG\u0003\nPHDVXUHV\u0011\u0003 ,Q\u0003 IXWXUH\u0003 ZRUN\u0003 ZH\u0003 SODQ\u0003 WR\u0003 IXUWKHU\u0003 LQYHVWLJDWH\u0003 WKH\u0003 SURSRVHG\u0003 PHDVXUHV\u0003 WR\u0003 GHULYH\u0003\nUHFRPPHQGDWLRQV\u0003 DERXW\u0003 WKHLU\u0003 DSSOLFDWLRQ\u0011\u0003 2WKHU\u0003 GLUHFWLRQV\u0003 RI\u0003 IXWXUH\u0003 ZRUN\u0003 DUH\u0003 H[SDQVLRQV\u0003 WR\u0003\nEHWWHU\u0003OHYHUDJH\u0003ORFDO\u0003LQIRUPDWLRQ\u0003RQ\u0003IHDWXUH\u0003LQWHUDFWLRQ\u0003DQG\u0003WR\u0003EXLOG\u0003UHJLRQDO\u0003PRGHOV\u0003RQ\u0003WRS\u0003RI\u0003\nWKH\u0003LGHQWLILHG\u0003UHOHYDQW\u0003IHDWXUHV\u0011\u0003:H\u0003SODQ\u0003RQ\u0003H[SDQGLQJ\u0003RQ\u0003WKH\u0003LGHD\u0003WR\u0003EXLOG\u0003VXUURJDWH\u0003WUHHV\u0003E\\\u0003\u000bD\f\u0003\nEXLOGLQJ\u0003WUHHV\u0003RQO\\\u0003IRU\u0003UHJLRQV\u0003DQG\u0003QRW\u0003DV\u0003VXUURJDWH\u0003IRU\u0003WKH\u0003HQWLUH\u0003PRGHO\u000f\u0003\u000bE\f\u0003EXLOGLQJ\u0003WUHHV\u0003RQ\u0003D\u0003\nVXEVHW\u0003RI\u0003IHDWXUHV\u000f\u0003DQG\u0003\u000bF\f\u0003WUDLQLQJ\u0003RQ\u0003WKH\u0003RULJLQDO\u0003GDWD\u0003DQG\u0003QRW\u0003WKH\u0003PRGHO\u0003SUHGLFWLRQV\u0011\u0003)LUVW\u0003VWHSV\u0003\nLQ\u0003WKLV\u0003GLUHFWLRQ\u0003KDYH\u0003DOUHDG\\\u0003EHHQ\u0003PDGH\u0003LQ\u0003>\u0014\u0014@\u0011\u0003\u0003\n$FNQRZOHGJHPHQWV\u0003\n7KLV\u0003SURMHFW\u0003ZDV\u0003IXQGHG\u0003E\\\u0003WKH\u0003*HUPDQ\u0003)HGHUDO\u00030LQLVWU\\\u0003RI\u0003(GXFDWLRQ\u0003DQG\u00035HVHDUFK\u000f\u0003IXQGLQJ\u0003\nOLQH\u0003 ³)RUVFKXQJ\u0003 DQ\u0003 )DFKKRFKVFKXOHQ\u0003 PLW\u0003 8QWHUQHKPHQ\u0003 \u000b)+3URI8QW\f³\u000f\u0003 FRQWUDFW\u0003 QXPEHU\u0003\n\u0014\u0016)+\u0015\u0017\u001c3;\u0019\u0011\u0003 :H\u0003 DOVR\u0003 OLNH\u0003 WR\u0003 WKDQN\u0003 WKH\u0003 PDQXIDFWXUHU\u0003 6,&.\u0003 $*\u0003 IRU\u0003 WKH\u0003 FRRSHUDWLRQ\u0011\u0003 7KH\u0003\nUHVSRQVLELOLW\\\u0003IRU\u0003WKH\u0003FRQWHQW\u0003RI\u0003WKLV\u0003SXEOLFDWLRQ\u0003OLHV\u0003ZLWK\u0003WKH\u0003DXWKRUV\u0011\u0003\n5HIHUHQFHV\u0003\n>\u0014@ /\u0011\u0003+\u0011\u0003*LOSLQ\u000f\u0003'\u0011\u0003%DX\u000f\u0003%\u0011\u0003=\u0011\u0003<XDQ\u000f\u0003$\u0011\u0003%DMZD\u000f\u00030\u0011\u00036SHFWHU\u000f\u0003DQG\u0003/\u0011\u0003.DJDO\u000f\u0003³([SODLQLQJ\nH[SODQDWLRQV\u001d\u0003$Q\u0003RYHUYLHZ\u0003RI\u0003LQWHUSUHWDELOLW\\\u0003RI\u0003PDFKLQH\u0003OHDUQLQJ\u000f´\u0003LQ\u0003\u0015\u0013\u0014\u001b\u0003,(((\u0003\u0018WK\n,QWHUQDWLRQDO\u0003&RQIHUHQFH\u0003RQ\u0003GDWD\u0003VFLHQFH\u0003DQG\u0003DGYDQFHG\u0003DQDO\\WLFV\u0003\u000b'6$$\f\u000f\u0003\u0015\u0013\u0014\u001b\u0011\n>\u0015@ &\u0011\u00030ROQDU\u000f\u0003³,QWHUSUHWDEOH\u0003PDFKLQH\u0003OHDUQLQJ\u000f´\u0003$\u0003*XLGH\u0003IRU\u00030DNLQJ\u0003%ODFN\u0003%R[\u00030RGHOV\n([SODLQDEOH\u000f\u0003YRO\u0011\u0003\u001a\u000f\u0003\u0015\u0013\u0014\u001b\u0011\n>\u0016@ +\u0011\u0003=LHNRZ\u0003HW\u0003DO\u0011\u000f\u0003³3URDFWLYH\u0003(UURU\u00033UHYHQWLRQ\u0003LQ\u00030DQXIDFWXULQJ\u0003%DVHG\u0003RQ\u0003DQ\u0003$GDSWDEOH\n0DFKLQH\u0003/HDUQLQJ\u0003(QYLURQPHQW\u000f´\u0003)URP\u00035HVHDUFK\u0003WR\u0003$SSOLFDWLRQ\u000f\u0003S\u0011\u0003\u0014\u0014\u0016\u000f\u0003\u0015\u0013\u0014\u001c\u0011\n>\u0017@ 6\u0011\u00030\u0011\u0003/XQGEHUJ\u0003DQG\u00036\u0011\u0010,\u0011\u0003/HH\u000f\u0003³$\u0003XQLILHG\u0003DSSURDFK\u0003WR\u0003LQWHUSUHWLQJ\u0003PRGHO\u0003SUHGLFWLRQV\u000f´\u0003LQ\n$GYDQFHV\u0003LQ\u00031HXUDO\u0003,QIRUPDWLRQ\u00033URFHVVLQJ\u00036\\VWHPV\u000f\u0003\u0015\u0013\u0014\u001a\u000f\u0003SS\u0011\u0003\u0017\u001a\u0019\u0018±\u0017\u001a\u001a\u0017\u0011\n>\u0018@ 6\u0011\u00030\u0011\u0003/XQGEHUJ\u000f\u0003*\u0011\u0003*\u0011\u0003(ULRQ\u000f\u0003DQG\u00036\u0011\u0010,\u0011\u0003/HH\u000f\u0003³&RQVLVWHQW\u0003LQGLYLGXDOL]HG\u0003IHDWXUH\u0003DWWULEXWLRQ\nIRU\u0003WUHH\u0003HQVHPEOHV\u000f´\u0003DU;LY\u0003SUHSULQW\u0003DU;LY\u001d\u0014\u001b\u0013\u0015\u0011\u0013\u0016\u001b\u001b\u001b\u000f\u0003\u0015\u0013\u0014\u001b\u0011\n>\u0019@ /\u0011\u0003%UHLPDQ\u000f\u0003&ODVVLILFDWLRQ\u0003DQG\u0003UHJUHVVLRQ\u0003WUHHV\u001d\u00035RXWOHGJH\u000f\u0003\u0015\u0013\u0014\u001a\u0011\n>\u001a@ [JERRVW\u0003 GHYHORSHUV\u000f\u0003 ;*%RRVW\u0003 'RFXPHQWDWLRQ\u0003 \u0010\u0003 3\\WKRQ\u0003 $3,\u0003 5HIHUHQFH\u0011\u0003 >2QOLQH@\u0011\n$YDLODEOH\u001d\u0003KWWSV\u001d\u0012\u0012[JERRVW\u0011UHDGWKHGRFV\u0011LR\u0012HQ\u0012ODWHVW\u0012S\\WKRQ\u0012S\\WKRQBDSL\u0011KWPO\n>\u001b@ 0\u0011\u0003 7\u0011\u0003 5LEHLUR\u000f\u0003 6\u0011\u0003 6LQJK\u000f\u0003 DQG\u0003 &\u0011\u0003 *XHVWULQ\u000f\u0003 ³:K\\\u0003 VKRXOG\u0003 L\u0003 WUXVW\u0003 \\RX\"\u001d\u0003 ([SODLQLQJ\u0003 WKH\nSUHGLFWLRQV\u0003RI\u0003DQ\\\u0003FODVVLILHU\u000f´\u0003LQ\u00033URFHHGLQJV\u0003RI\u0003WKH\u0003\u0015\u0015QG\u0003$&0\u00036,*.''\u0003LQWHUQDWLRQDO\nFRQIHUHQFH\u0003RQ\u0003NQRZOHGJH\u0003GLVFRYHU\\\u0003DQG\u0003GDWD\u0003PLQLQJ\u000f\u0003\u0015\u0013\u0014\u0019\u000f\u0003SS\u0011\u0003\u0014\u0014\u0016\u0018±\u0014\u0014\u0017\u0017\u0011\n>\u001c@ 6\u0011\u0003&RKHQ\u000f\u0003*\u0011\u0003'URU\u000f\u0003DQG\u0003(\u0011\u00035XSSLQ\u000f\u0003³)HDWXUH\u0003VHOHFWLRQ\u0003YLD\u0003FRDOLWLRQDO\u0003JDPH\u0003WKHRU\\\u000f´\u00031HXUDO\n&RPSXWDWLRQ\u000f\u0003YRO\u0011\u0003\u0014\u001c\u000f\u0003QR\u0011\u0003\u001a\u000f\u0003SS\u0011\u0003\u0014\u001c\u0016\u001c±\u0014\u001c\u0019\u0014\u000f\u0003\u0015\u0013\u0013\u001a\u0011\n>\u0014\u0013@ +\u0011\u0003=LHNRZ\u000f\u00038\u0011\u00036FKUHLHU\u000f\u0003$\u0011\u0003*HUOLQJ\u000f\u0003DQG\u0003$\u0011\u00036DOHK\u000f\u0003³7HFKQLFDO\u00035HSRUW\u001d\u0003,QWHUSUHWDEOH\n0DFKLQH\u0003/HDUQLQJ\u0003IRU\u00034XDOLW\\\u0003(QJLQHHULQJ\u0003LQ\u00030DQXIDFWXULQJ\u0003\u0010\u0003,PSRUWDQFH\u0003PHDVXUHV\u0003WKDW\nUHYHDO\u0003\nLQVLJKWV\u0003\nRQ\u0003\nHUURUV\u000f´\u0003\n\u0015\u0013\u0014\u001c\u0011\u0003\n>2QOLQH@\u0011\u0003\n$YDLODEOH\u001d\u0003\nKWWS\u001d\u0012\u0012QEQ\u0010\nUHVROYLQJ\u0011GH\u0012XUQ\u001dQEQ\u001dGH\u001dEV]\u001dIQ\u0014\u0010RSXV\u0017\u0010\u0018\u0018\u0016\u0016\u0014\n>\u0014\u0014@ 9\u0011\u0003 *|WWLVKHLP\u000f\u0003 \u0015\u0013\u0015\u0014\u000f\u0003 ,QWHUSUHWLHUEDUH\u0003 6XUURJDW\u00100RGHOOH\u0003 LQ\u0003 GHU\u0003 SURGX]LHUHQGHQ\n,QGXVWULH\u000f\u0003\u000bXQYHU|IIHQWOLFKWH\u00030DVWHUDUEHLW\f\u000f\u0003+RFKVFKXOH\u0003)XUWZDQJHQ\n105\n\nApplication of Machine Learning Methods for the \nDevelopment of Internal Combustion Engines \n–\nAn Overview\nYoussef Beltaifa, Shahida Faisal, Maurice Kettner\nKarlsruhe University of Applied Sciences\nGas Engine Laboratory (GenLab)\nYoussef.beltaifa@h-ka.de \nshahida7085@gmail.com \nMaurice.kettner@h-ka.de \nAbstract. Machine Learning (ML) has a strong potential to improve the performance and \neffectiveness of several technologies and processes. In recent years, ML has gained in im-\nportance, primarily due to its matchless success in image recognition and computer games. \nThese ML accomplishments have motivated to transfer and adapt its algorithms and mod-\neling methods to most scientific disciplines. For instance, in mechanical engineering, ML \nis coming to hold a crucial position ranging from value chain optimization (production) to \nsubstitution of complex simulation models (research and development). In the case of tra-\nditional research and development approach, the analysis and optimization of a process are \nimplemented according to the understanding of the governing mechanisms described by \nphysical and mathematical rules. On the contrary, the intelligence of the ML method origi-\nnates from the extraction of trends and laws based on data patterns, which produces sur-\nprisingly good results in many cases. However, it is not entirely evident why it performs so \nwell. One of the most challenging mechanical engineering topics is the improvement of the \nInternal Combustion Engine (ICE) towards higher efficiency and lower negative impact on \nthe environment. ICEs are very complex systems, which involve high-speed reciprocating\nmotions, transient gas flow and combustion chemistry. Thus, the application of ML meth-\nods for ICEs opens new perspectives regarding the modelling, control and maintenance. \nThese topics are addressed in detail in the course of this paper, based on the most relevant \npublished results found in the literature, to provide an overview to the actual research and \ndevelopment of ICE using ML methods.\nKeywords: Machine Learning; Mechanical Engineering; Internal Combustion Engines; \nModelling; Control; Predictive Maintenance\n1 Introduction\nInternal combustion engines will maintain their position as major power source during the com-\ning decades, particularly for heavy-duty applications [1, 2]. Future internal combustion engines \n106\n\nhave to comply with tightening legislative emission-limits, high fuel-energy conversion-effi-\nciency, affordable prices and customer requirements. To reach this target, engine researchers \nworldwide are working on innovative exhaust aftertreatment systems, alternative combustion \nprocesses, bio- and renewable fuels, lightweight materials, modern lubricants and advanced man-\nufacturing processes. Within the development of innovative combustion processes (research fo-\ncus of Gas Engine Laboratory at Karlsruhe University of Applied Sciences) mainly experimental\n(mostly at the engine test bench) and numerical investigations (0D, 1D and 3D-CFD) are per-\nformed. Engine tests are expensive (costly metrology, etc.) and very time-consuming. Moreover, \nnumerical simulations are very dependent on the validation of the implemented physics-based \nmodels and necessitate in many cases a large computational capacity. Considering this facts, \ndifferent alternative approaches that enable saving costs, time and computational power are re-\nquired. One of the possible solutions that has been increasingly used in recent years is machine \nlearning. \nMachine learning is a branch of knowledge dealing with training computers to forecast output \nvalues or to classify things without having been explicitly programmed for such function. Ma-\nchine learning success in many areas like image/speech recognition, effective internet search, \nself-driving cars is mainly lead by the availability of huge datasets. Machine learning methods \ncan be categorized into two main groups: supervised and unsupervised algorithms, as shown in \nFigure 1, which depicts some of the most used machine learning algorithms. \nFigure 1. Classification of the most common machine learning algorithms\nThe methods marked in bold in Figure 1 are the methods that have been used the most in recent \nstudies dealing with internal combustion engines, which are collected and analyzed within this \npaper. These works are classified within this paper depending on the intended use of machine \nlearning into three categories: Prediction of engine operation parameters and emissions, anomaly \ndetection and predictive maintenance, and real-time engine control. These three topics are cov-\nered throughout this paper in detail.\n107\n\n2 Prediction of Engine Operation Parameters and Emissions\nMany studies [3-18] have demonstrated that engine combustion associated parameters and emis-\nsions can be predicted accurately using neural networks over a wide range of operating condi-\ntions, given that the training data provides good knowledge of the system’s behavior. The com-\nbination of fast-computational time and the network’s ability to analyze broad non-linear prob-\nlems can potentially replace expensive exhaust gas sensors (FID, Gas Chromatograph, etc.) and \nphysics-based, computationally intensive engine modeling approaches. Multi-Layer Perceptron \n(MLP) is a conventional artificial neutral network (ANN) structure that is commonly used for \nthe prediction of engine operating parameters and exhaust gas components. MLP consists of \ninput, hidden and output layers, as seen in Figure 2 on the left.\nFigure 2. Structure of MLP with two (as example) hidden layers (left), Structure of Perceptron (right)\nAs shown in Figure 2 on the right, each input is assigned to a weighting factor, representing the \nimportance of the input factor given by the model when predicting the output. Activation func-\ntions are employed in the hidden and output neurons, allowing mapping the non-linear relation-\nships between outputs and inputs. For the model training, MLP uses among others gradient de-\nscent backpropagation algorithm, where the goal is to minimize the modeling error, meanwhile \nthe weights between neurons are gradually adjusted. Usually, the network training takes place \nusing the Levenberg-Marquardt (LM) backpropagation algorithm, known for high computational \nefficiency. LM algorithm is a curve-fitting method for solving nonlinear least-squares problems. \nLM combines the two minimization algorithms gradient descent and Gaussian-Newton to mini-\nmize the sum of the squared errors between the fitted model function and the experimental data\n[19]. MLP with the weighting approach can also be used to have an insight on the dependency \nof the model output on the input parameters. As an example, the authors in [20] analyzed the \nrelative importance of the in-cylinder parameters affecting the NOx and HC model output by \nextracting the saved weights from the trained network. The results yielded that both HC and NOx\nwere commonly dependent on the engine load and IMEP. The model showed a significant de-\npendency of the NOx emissions on the peak pressure in the combustion chamber, which is phys-\nically reasonable. Higher peak pressures in the combustion chamber are associated with high \ncharge temperatures, which result in turn in a high temperature oxidation of the diatomic nitrogen \nin the combustion air and the formation of “thermal” NOx. Further studies demonstrating the \nsuccess of artificial neural networks in predicting and modeling of engine-operation associated \nparameters and emissions are summarized in Table 1. For these studies, the statistical efficiency \nof the models lies between 94% and 99.9%.  It is important to notice, that MLP with backprop-\nagation is the most frequent encountered machine learning approach in the field of the research \nand development of internal combustion engines.\n108\n\nTable 1. Summary of MLP applications for the prediction of engine operation characteristics and \nemissions found in literature\nFurther neural network concepts/architectures have \nbeen used in other studies. Taghavi et al. [21] con-\nsidered in addition to the MLP network the non-lin-\near autoregressive network with exogenous inputs \n(NARX) as well as the radial basis function (RBF) \nnetwork for the prediction of start of combustion \n(SOC) of a HCCI engine. Input parameters were the \nintake mixture characteristics (Air-Fuel-Ratio, \nEGR, intake mixture temperature) as well as the en-\ngine speed. The NARX algorithm has, depending on \nthe usage (training or prediction) two structures: The \nseries-parallel (or open loop) and the parallel (or \nclosed loop) architectures. The two network archi-\ntectures are shown schematically in Figure 3.\nThe series-parallel architecture is used for training: \nthe prediction at time-step ݐ+ 1 is provided based \non real input and output values at the current time \nstep ݐ, as well as those from the previous ݊time steps, as shown in Eq. 1. The pure feedforward \narchitecture of the series-parallel (open-loop) structure is applied during training due to the fast \nstatic backpropagation [22]. By providing the real input-output pairs during training, the model \nFigure 3. NARX networks architectures\n109\n\nis able to make future prediction with excellent accuracy. After training, the model produces a \nfinal set of adjusted weights, which minimizes the error between the predicted and the true output \nvalues. The adjusted weights together with the activation functions approximate the nonlinear \nmapping function F in Eq. 1. During the prediction stage, the open-loop structure is converted to \na closed-loop architecture. Instead of using the real output when making future prediction (time \nstep t+1), the trained model takes the output predicted by itself from the current time step as \ninput, as well as those from the previously n time-steps, as shown in Eq. 2.\nAdditionally, Taghavi et al. [21] applied the radial basis function (RBF) networks also for pre-\ndicting the SOC using the same input parameters as in the case of the NARX network. The RBF \nnetwork typically uses only an input layer, a single hidden layer and an output layer [23], as \nshown in Figure 4 on the left.\nFigure 4. RBF network architecture (left), Schematic description of data dimensionality increase enabling \nlinear separation (from 2D to 3D) (right)\nThe RBF network is shallow and its behavior is strongly influenced by the nature of the special \nhidden layer, which performs a computation based on a comparison with a prototype vector [23]. \nThe structure and computations performed in the hidden layer are the key to the power of the \nRBF network. Here, a hybrid calculation involving two stages takes place. Within the first stage, \nthe linear separability should be ensured: If needed, a projection of the original data points into \na higher dimensionality, so that they become linearly separable, is performed. This is based on \nthe Cover’s theorem on separability of patterns [24]. For a simplified understanding, Figure 4 on \nthe right shows this step schematically. The second stage is the RBF (Radial Basis Functions) \ncomputation, which is based on the comparison of the input units ܺതwith the prototype vectors \nρത௜in the hidden layer units according to the equation (3) [23].\n݄௜= ߮௜(ܺത) = exp ቆെ\nԡܺതെρത௜ԡଶ\n2 . ߪ௜ଶ\nቇ\n݅א {1, … , m }\n(3)\nݕො(ݐ+ 1) = ܨቆ࢟(࢚), ݕ(ݐെ1), … , ݕ൫ݐെ݊௬൯, ݔ(ݐ+ 1),\nݔ(ݐ), ݔ(ݐെ1), … , ݔ(ݐെ݊௫)\nቇ\n(1)\nݕො(ݐ+ 1) = ܨቆ࢟ෝ(࢚), ݕො(ݐെ1), … , ݕො൫ݐെ݊௬൯, ݔ(ݐ+ 1),\nݔ(ݐ), ݔ(ݐെ1), … , ݔ(ݐെ݊௫)\nቇ\n(2)\n110\n\nm is the total number of the hidden units. Each of these m units is created to have a high impact \non a particular cluster of points, which is closest to its prototype vector ρത௜[23]. Therefore, m can\nbe regarded as the number of clusters used for modeling, and it represents an important hyper-\nparameter available to the algorithm [23]. Each unit has a bandwidth ߪ௜, which is often the same\nfor all units with the different prototype vectors [23].  After the RBF calculation in the hidden \nlayer, the outputs from the RBFs are weighted and summed by a simple connection to the output \nlayer. The values of the weights need to be learned in a supervised way, dealing with the specific \nstudied case [23]. On the contrary, the hidden layer is trained in an unsupervised way [25]. This \ninvolves several parameters such as the prototype vectors, the bandwidths and the number of \nhidden neurons m. Elaborate description about the determination methods of these parameters \ncan be found in [23]. In comparison to MLP and RBF, the NARX network featured a better \nprediction accuracy, reaching R = 0.99933 [21]. \nAnother machine learning process used for \nthe prediction of engine-operation related pa-\nrameters is the Ensemble modeling.  For the \nprediction of the performance as well as effi-\nciency of an engine converted from the diesel \nCI to the natural gas SI combustion process, \nLiu et al. [26] applied ensemble methods \n(bagging and boosting) and compared their \nprediction performances. The model output \nwas the indicated mean effective pressure \n(IMEP). Input parameters were spark timing, \nfuel/air-ratio and engine speed with overall \n153 sets of data (122 for training and 31 for \ntesting). “Unity is strength”: This statement \ndescribes in three words the core idea behind \nthe strength of ensemble methods in machine \nlearning. Such methods improve the predic-\ntive performance of a single model by train-\ning multiple models and combining their pre-\ndictions [27]. The base models building the \nensemble model are “weak” learners, which \nfeature either a high bias or much variance. \nThese are combined within the ensemble \nmethod in such a way that they build a strong \nlearner. The combination strategy of the base \nlearners enables to group the ensemble methods in two main categories, depending on how the \nbase learners are generated [28]. The first category is “bagging”. Here the individual learners are \ncreated independently and their generation can be parallelized [28]. The second category, called \n“boosting”, creates individual learners sequentially in a very adaptive way [28]. Both ensemble \nmethods are shown schematically in Figure 5. For the first step of the bagging algorithm, multi-\nple bootstrap samples (data subsets) are created. These subsets are almost independent datasets \ncreated from the original one using random selection [26]. It is important to notice, that the size \nof the original dataset should be large enough compared to the size of the bootstrap samples so \nthat they are “sufficiently” independent. Subsequently, one “weak” learner (usually the same) is \nFigure 5. General structure of bagging (top) and \nboosting (bottom) ensemble algorithms\n111\n\nfitted for each bootstrap sample. The predictions of the base learners are then combined to the \nfinal prediction of the ensemble model in some kind of weighting process [27]. The combination \nof the base learners within the bagging method enables the reduction of the variance compared \nto the variance levels of the single base learners [28]. Therefore, base models with low bias but \nhigh variance are more suitable for bagging. Concerning the boosting algorithm, it is not suitable \nfor parallelized computation. Boosting starts with training a first base “weak” learner and then \nadapt the distribution of the training data according to the output of the base learner such that \nincorrectly classified samples will have increased consideration from subsequent basic learners\n[28]. In other words, each new base learner focuses on the most difficult samples (wrongly pre-\ndicted by the previous learner), so that we get a strong ensemble model with low bias. Hence, \nbase learners with low variance but high bias are suitable to be combined within boosting en-\nsemble methods. Liu et al. [26] found that boosting outperformed bagging, can deal with data \nset with uneven distributed conditions among the operating range, and provided a high accuracy \nprediction (R2 = 0.9623) even for low frequency cases, which are poorly presented in the original \ndata set. \n3 Anomaly Detection and Predictive Maintenance\nWith recent developments, powertrain systems are becoming more complex. Understanding this\ncomplexity and dealing with associated particular problems/failures requires evolved methods.\nNew detection methodologies involving machine learning and predictive diagnostics have be-\ncome the need of the hour [29]. In this frame, Farsodia et al. [30] proposed an approach com-\nbining unsupervised learning and clustering to detect anomalies, which may occur in engines or \nafter-treatment-systems (ATS). To validate their strategy, Farsodia et al. [30] addressed the ex-\nample of the backpressure problem occurring in the diesel particulate filter (DPF) of an automo-\ntive diesel engine. Figure 6 depicts schematically the approach proposed by Farsodia et al. [30].\nFigure 6. Schematic description of the approach proposed by Farsodia et al. [30] for anomaly detection\nAs shown in Figure 6, the pressure values before and after the DPF are measured and clustered\nin a supervised way using the k-means clustering algorithm. Here, the data set will be distributed \ninto “k” clusters. Each cluster has a centroid, which is defined by averaging (taking mean) of the \nassigned data. First, centroids are determined randomly. Then, data points from the dataset are \narranged to the nearest available centroid. The positions of the centroids change within an opti-\nmization process until further movement of centroid is not possible. The algorithm is one of the \nmost commonly used techniques for clustering purposes, as it quickly finds the centers of the \nclusters. Detailed specifications to the k-means clustering algorithm can be found in [31]. In a \nfurther step, a classification MLP is trained with the defined classes (clusters) from the super-\nvised clustering step and the associated data. The trained classification MLP is then used in a \n112\n\nthird step to predict in an unsupervised manner the operating mode of the DPF. For a better \nmonitoring of malfunction cases, Farsodia et al. [28] defined a “severity factor”, which enables \na time dependent tracking of the DPF functionality degradation. The “severity factor” is derived \nbased on the relative density of data (e.g. malfunction, backpressure, etc.) with respect to total \navailable data points [30]. This “severity factor” gives a pre-warning about any component’s \nmalfunction, which will enable the end user to take necessary preventive measures [30]. \nIn a further case, Farsodia et al. [30] presented a methodology involving the weighted k-nearest \nneighbor (w-kNN) algorithm to predict the temperature shoot-up events in a DPF, which are \nharmful for the ATS from thermal aging and safety perspectives. kNN is among the simplest \nstatistical learning tools in density estimation, classification as well as regression and known to \nbe trivial to train and easy to code [32]. The difference between the standard and the weighted \nkNN is that in the weighted approach the prediction of a test point is more depended on the \nnearest observations [30]. In other words, the k points within the neighborhood of the test point \ndo not contribute equally to the final decision of the test point. Indeed, the closer an observation \nis from a test point, the more it contributes to its classification. For deeper insight into the w-\nkNN-methodology, please refer to [33]. After defining the most probably governing parameters \non the temperature shoot-up event (engine speed, torque, airflow, HC injection quantity, etc.), \nFarsodia et al. [30] classified the training dataset, containing temperature shoot-up events, into \nthree different risk categories: “high”, “medium” and “low”, using w-kNN within a supervised \nlearning process. Category “high” risk implied that there are very high chances that there will be \ntemperature shoot-up post DOC. When testing the trained model with test data from the same \nvehicle, the model released a warning signal about 60 seconds before the temperature shoot-up \nevent occurred. The algorithm derived from the data is “smart” enough to detect the difference \nbetween the high-end temperatures and shoot-up events. However, the excellent beforehand pre-\ndiction performance was not precisely explained by the authors. Especially, the relationship be-\ntween the algorithm behind the occurrence of the warning signal and the previous classification \nstep was not discussed.\nFor a 2.4L diesel excavator engine, Jang et al. [34] proposed also an anomaly detection model,\nwhich is depicted schematically in Figure 7. The main idea of the proposed approach is to extract \nabundant features from gathered data using an autoencoder and then to distinguish between nor-\nmal and abnormal operating conditions with help of a one-class support vector machine \n(OCSVM). First, data was collected from 123 different sensors at high frequency (one value \nevery 0.1 s) over 12 days. Due to the large learning dimension, raw collected data cannot be \napplied to the autoencoder. Therefore, the authors used statistical values instead (median, vari-\nance, deciles, etc.). This enabled the reduction of the data amount and the expression of data \ncharacteristics more prominently. In a second data-dimensionality-reduction step, the autoen-\ncoder is applied to the derived statistical indicators. Autoencoders are neural networks that can \nautomatically (unsupervised) learn useful features from data [35]. Autoencoders work by com-\npressing the data into a latent-space representation also known as bottleneck, and then recon-\nstructing the output from this representation. Jang et al. [34] used compressed features from the \nlatent space of the autoencoder network as input for the classification algorithm, which is the \nOCSVM, which is used in the context of pattern classification to discriminate between two clas-\nses [36]. More details to support vector machines can be found in [37]. Ten days of “healthy” \nmeasurement data were used to train the OCSVM model. The anomaly classification perfor-\nmance was evaluated using data from two days, where faulty events were present. The model \n113\n\naccuracy reached up 73%. However, the model achieved an excellent recall score with 83%, \nindicating the model reliability to ignore false alarms.\nFigure 7. Schematic description of the approach proposed by Jang et al. [34] for anomaly detection\nIn a classification task, the positive or negative results are insufficient without explaining the \nclassifier’s decision-making. Therefore, Jang et al. [34] used Local Interpretable Model-agnostic \nExplanation (LIME) to get more in-depth interpretation regarding the most critical factors con-\ntributing to the classification results. LIME is an algorithm for providing interpretable explana-\ntions for the non-interpretable (black box) ML models such as neural networks. An in-depth\nexplanation to the LIME approach can be found in [38].\n4 Real-Time Engine Control\nConventionally, ICEs control is based on map-calibrations tuned by full factorial or design of \nexperiments processes. To reach engine efficiency targets, manufacturers are increasing the \nnumber of actuators [39], leading to an increase in the calibration design space and thus affecting \nthe real-time capability of the control unit, especially for transient operating conditions. Thus,\nnew control techniques, which can better deal with increasing actuators number, are developed.\nIn this context, Egan et al. [40] introduced a hybrid modelling approach involving the non-linear \nmodel predictive control (nMPC) in combination with static and dynamic (time-dependent be-\nhavior) artificial neural networks. nMPC is an advanced control strategy that has the greatest \nacceptance in the industry, because it provides an intuitive approach to the optimal control of \nsystems subject to constraints [41]. Nevertheless, it has its drawbacks, mainly the large amount \nof calculation required, since an optimization problem is being solved at every sampling time \n[41]. Thus, non-linear MPC use for ICE is usually limited due to the short period available be-\ntween engine cycles (~25ms at 5000 rpm) and the limited computational power of automotive \ncontrol units [42]. Alone the evaluation of non-linear engine-models and its linearization take \nabout 60%-75% of the total computational time per nMPC iteration [41]. Taking into consider-\nation that neural networks can computationally efficient capture non-linear behavior and have \nthe ability to be linearized in minimal time [40], Egan et al. [40] proposed to replace traditional \nengine modeling methods by artificial neural networks and use them within the nMPC frame-\nwork, as shown schematically in Figure 8. Their aim was to accelerate the nMPC processing \ntime and thus facilitating its integration into the engine control unit. Egan et al. [40] found that \nthe proposed control system successfully controls the investigated engine with tractable compu-\ntational load, opening doors for the application of their approach for future Engine Control Units. \n114\n\nFigure 8. MPC architecture with ANN as engine modeling approach, approach proposed by Egan et al. \n[40]\n5 Summary and Outlook\nThis paper deals with existing applications of machine learning in the field of internal combus-\ntion engine development. Most of the work found in literature handles empirical model building\nwith the use of artificial neural networks, especially the MLP structure, which is quite suitable \nfor mapping non-linear processes occurring in an IC engine. Other structures for modeling and \npredicting engine-related parameters have also been found in the literature, such as the NARX \nand the RBF networks. Further studies considered ensemble methods, which are well suitable \nfor modeling engine parameters (especially the boosting algorithm).\nIn addition to model building, machine-learning methods in the field of combustion engines are \nalso used for the predictive maintenance and anomaly detection. Especially, Clustering (k-means \nclustering, k-nearest neighbors, support vector machine, etc.) and “deep learning” (autoencoder, \nconvolutional neural network, etc.) methods are used for these purposes. In this process, XAI \n(explainable artificial intelligence) methods (e.g. Local Interpretable Model-agnostic Explana-\ntion) were also employed to get more in-depth explanations and interpretations for the machine \nlearning model decisions. These methods then ultimately allow a more advanced understanding \nof the engine's behavior.\nOne more field of machine learning application in the engine development segment is the real-\ntime engine control. New engine Control systems face the challenge of dealing with growing \nengine complexity and thus increasing computational intensity. In this context, artificial neural \nnetworks offer the possibility to reduce the computational effort without affecting the number of \nactions to be managed with a given time slot. Indeed, the MLP structure have the advantage that \nit can simply map the engine operation (highly non-linear) and be easily linearized (from non-\nlinear to linear), which motivates for its integration into existing control systems involving com-\nputing intensive optimizers.\nHowever, machine learning also has its drawbacks. One cannot expect any magic from machine \nlearning algorithms. Indeed, simple learning programs are unable to learn complex concepts \nfrom few input data. To deal with this fact, more data and “smarter” algorithms are needed. \nTherefore, researchers are increasing the application of deep learning (DL) methods such as con-\nvolutional neural networks (CNN) [43], generative adversarial networks (GAN), and autoencod-\ners (AE), which has proven to enable the automatic detection of most significant features during \nthe training phase and to exceed the prediction accuracy of the simpler ML models with conven-\ntional human-aided feature extraction. Therefore, we expect an increasing use of deep learning \nalgorithms within the future research and development of internal combustion engines. \n115\n\nReferences\n1. Zhao, L., Ameen, M., Pei, Y., Zhang, Y., Kumar, P., Tzanetakis, T. and Traver, M.,\n“Numerical evaluation of gasoline compression ignition at cold conditions in a heavy-\nduty diesel engine”, SAE Technical Paper, No. 2020-01-0778, 2020\n2. Xu, Z., Ji, F., Ding, S., Zhao, Y., Wang, Y., Zhang, Q., Du, F. and Zhou, Y., „Simula-\ntion and experimental investigation of swirl-loop scavenging in two-stroke diesel engine\nwith two poppet valves”, International Journal of Engine Research, 1468087420916083,\n2020.\n3. Danaiah P., Kumar P., Rao Y., “Performance and emission prediction of a tert butyl al-\ncohol gasoline blended spark ignition engine using artificial neural networks”. Int J Am-\nbient Energy 36:37–41, https://doi.org/10.1080/01430750.2013. 820147, 2013\n4. Uslu S., Celik M., “Performance and exhaust emission prediction of a SI engine fueled\nwith I-amyl alcohol gasoline blends: an ANN coupled RSM based optimization”. Fuel\n265:116922, https://doi.org/10.1016/j.fuel.2019.116922, 2020\n5. Gürgen S., Ünver B., AltÕn ø. “Prediction of cyclic variability in a diesel engine fueled\nwith n-butanol and diesel fuel blends using artificial neural network”. Renew Energy\n117:538–544, https://doi.org/10.1016/j.renene.2017.10.101, 2018\n6. Kara Togun N., Baysec S. “Prediction of torque and specifc fuel consumption of a gaso-\nline engine by using artificial neural networks”. Appl Energy 87:349–355,\nhttps://doi.org/10.1016/j. apenergy.2009.08.016, 2010\n7. Tasdemir S., Saritas I., Ciniviz M., Allahverdi N. “Artificial neural network and fuzzy\nexpert system comparison for prediction of performance and emission parameters on a\ngasoline engine”. Expert Syst Appl 38:13912–13923, https://doi.org/10.\n1016/j.eswa.2011.04.198, 2011\n8. Roy S., Banerjee R., Bose P., “Performance and exhaust emissions prediction of a\nCRDI assisted single cylinder diesel engine coupled with EGR using artifcial neural\nnetwork”. Appl Energy 119:330–340, 2014, https://doi.org/10.1016/j.apenergy. 01.044,\n2014\n9. Maurya R., Saxena M., “Characterization of ringing intensity in a hydrogen-fueled\nHCCI engine”. Int J Hydrogen Energy 43:9423–9437,\nhttps://doi.org/10.1016/j.ijhydene.2018. 03.194, 2018\n10. Martínez-Morales J., Quej-Cosgaya H., Lagunas-Jiménez J. et al., “Design optimization\nof multilayer perceptron neural network by ant colony optimization applied to engine\nemissions data”. Sci China Technol Sci 62:1055–1064, https://doi.org/10. 1007/s11431-\n017-9235-y, 2019\n11. Hariharan N., Senthil V., Krishnamoorthi M., Karthic S. “Application of artifcial neural\nnetwork and response surface methodology for predicting and optimizing dual-fuel CI\nengine characteristics using hydrogen and bio fuel with water injection”, Fuel\n270:117576, https://doi.org/10.1016/j.fuel.2020.117576, 2020\n12. Mehra R., Duan H., Luo S. et al., “Experimental and artificial neural network (ANN)\nstudy of hydrogen enriched compressed natural gas (HCNG) engine under various igni-\ntion timings and excess air ratios”. Appl Energy 228:736–754, https://doi.\norg/10.1016/j.apenergy.2018.06.085, 2018\n13. Ghobadian B., Rahimi H., Nikbakht A. et al. „Diesel engine performance and exhaust\nemission analysis using waste cooking biodiesel fuel with an artificial neural network”.\nRenew Energy 34:976–982, https://doi.org/10.1016/j.renene.2008.08.008, 2009\n116\n\n14. Kapusuz M., Ozcan H., Ahmad J., “Research of performance on a spark ignition engine \nfueled by alcohol e gasoline blends using artificial neural networks”. Appl Therm Eng \n91:525–534, https://doi.org/10.1016/j.applthermaleng.2015.08.058, 2015\n15. Aydin M., Uslu S., Bahattin Çelik M., “Performance and emission prediction of a com-\npression ignition engine fueled with biodiesel-diesel blends: a combined application of \nANN and RSM based optimization”, Fuel. https://doi.org/10.1016/j.fuel. 2020.117472,\n2020\n16. Akkouche N., Loubar K., Nepveu F. et al., ,”Micro-combined heat and power using \ndual fuel engine and biogas from discontinuous anaerobic digestion”. Energy Convers \nManag 205:112407, https://doi.org/10.1016/j.enconman.2019.112407, 2020\n17. Oguz H., SarÕtas I., Baydan H., “Prediction of diesel engine performance using biofuels \nwith artificial neural network”. Expert Syst Appl 37:6579–6586,\nhttps://doi.org/10.1016/j.eswa.2010. 02.128, 2010\n18. Cay Y., Korkmaz I., Cicek A., Kara F., “Prediction of engine performance and exhaust \nemissions for gasoline and methanol using artificial neural network”. Energy 50:177–\n186, https://doi. org/10.1016/j.energy.2012.10.052, 2013\n19. Henri P. Gavin, “The Levenberg-Marquardt algorithm for nonlinear least squares curve-\nfitting problems”, Department of Civil and Environmental Engineering, 2020\n20. Janakiraman V., Suryanarayanan, S., Saravanan, S., and Rao, G., \"Analysis of the Effect \nof In-cylinder Parameters on NOx and HC Emissions of a CI Engine Using Artificial \nNeural Networks,\" SAE Technical Paper 2006-01-3313, 2006\n21. Taghavi, M.; Gharehghani, A.; Nejad, F. Bakhtiari; Mirsalim, M., “Developing a model \nto predict the start of combustion in HCCI engine using ANN-GA approach”. In Energy \nConversion and Management 195, pp. 57–69, doi: 10.1016/j.enconman.2019.05.01,\n2020\n22. Huo F., Poo A., “Non-linear autoregressive network with exogenous inputs based con-\ntour error reduction in CNC machines”. In International Journal of Machine Tools and \nManufacture 67, pp. 45–52, doi: 10.1016/j.ijmachtools.2012.12.007, 2013\n23. C. C. Aggarwal, Neural Networks and Deep Learning, Springer, 2018\n24. F. Samuelson and D. G. Brown, \"Application of Cover's theorem to the evaluation of \nthe performance of CI observers,\" The 2011 International Joint Conference on Neural \nNetworks, pp. 1020-1026, doi: 10.1109/IJCNN.2011.6033334.P, 2011\n25. Faris H., Aljarah I., Mirjalili S., Chapter 28 - Evolving Radial Basis Function Networks \nUsing Moth–Flame Optimizer, Handbook of Neural Computation, Academic Press, \nISBN 9780128113189, 2017\n26. Liu, J, Ulishney, C, & Dumitrescu, CE. \"Improving Machine Learning Model Perfor-\nmance in Predicting the Indicated Mean Effective Pressure of a Natural Gas Engine.\" \nProceedings of the ASME 2020 Internal Combustion Engine Division Fall Technical \nConference, 2020\n27. Sagi O., Rokach L., “Ensemble learning: A survey”, WIREs Data Mining and \nKnowledge Discovery, Volume 8, Issue 4, 2018\n28. Zhou Z., “Machine Learning”, Springer, Nanjing, Jiangsu, China, ISBN 978-981-15-\n1967-3 (eBook), 2021\n29. Stephen M., Machine Learning, Second Edition, 2015\n30. Farsodia, M., Pandey, S., and Ganguly, G., “Advance Data Analytics Methodologies to \nSolve Diesel Engine Exhaust Aftertreatment System Challenges,” SAE Technical Paper \n2019-01-5035, doi:10.4271/2019-01-5035, 2019\n31. Aristidis Likas, Nikos Vlassis, Jakob J. Verbeek, The global k-means clustering algo-\nrithm, Pattern Recognition, Volume 36, Issue 2, Pages 451-461, ISSN 0031-3203, 2003\n117\n\n32. K. S. Ni and T. Q. Nguyen, \"An Adaptable k-Nearest Neighbors Algorithm for MMSE\nImage Interpolation,\" in IEEE Transactions on Image Processing, vol. 18, no. 9, pp.\n1976-1987, doi: 10.1109/TIP.2009.2023706, 2009\n33. M. Bicego and M. Loog, \"Weighted K-Nearest Neighbor revisited,\" 2016 23rd Interna-\ntional Conference on Pattern Recognition (ICPR), pp. 1642-1647, doi:\n10.1109/ICPR.2016.7899872, 2016\n34. Jang G-b, Cho S-B. Anomaly Detection of 2.4L Diesel Engine Using One-Class SVM\nwith Variational Autoencoder, ANNUAL CONFERENCE OF THE PROGNOSTICS\nAND HEALTH MANAGEMENT SOCIETY, 2019.\n35. Walter Hugo Lopez Pinaya, Sandra Vieira, Rafael Garcia-Dias, Andrea Mechelli, Chap-\nter 11 - Autoencoders, Machine Learning, Academic Press, Pages 193-208, ISBN\n9780128157398, 2020\n36. G. D. Fraser, A. D. C. Chan, J. R. Green and D. T. MacIsaac, \"Automated Biosignal\nQuality Analysis for Electromyography Using a One-Class Support Vector Machine,\" in\nIEEE Transactions on Instrumentation and Measurement, vol. 63, no. 12, pp. 2919-\n2930, doi: 10.1109/TIM.2014.2317296, 2014\n37. Suthaharan S., “Support Vector Machine. In: Machine Learning Models and Algorithms\nfor Big Data Classification”. Integrated Series in Information Systems, vol 36. Springer,\nBoston, MA, https://doi.org/10.1007/978-1-4899-7641-3_9, , 2016\n38. Peltola T., Local Interpretable Model-agnostic Explanations of Bayesian Predictive\nModels via Kullback-Leibler Projections, Machine Learning, Cornell University, 2019\n39. Atkinson, C., “Fuel Efficiency Optimization Using Rapid Transient Engine Calibra-\ntion,” SAE Technical Paper 2014- 01-2359, doi: 10.4271/2014-01-2359, 2014\n40. Egan, D., Koli, R., Zhu, Q., and Prucka, R., “Use of Machine Learning for Real-Time\nNon-Linear Model Predictive Engine Control,” SAE Technical Paper 2019-01-1289,\ndoi:10.4271/2019-01-1289, 2019\n41. Bordons C., Garcia-Torres F., Ridao M.A. “Model Predictive Control Fundamentals. In:\nModel Predictive Control of Microgrids”. Advances in Industrial Control. Springer,\nCham, https://doi.org/10.1007/978-3-030-24570-2_2, 2020\n42. Zhu, Q., Prucka, R., Prucka, M., and Dourra, H., “A Nonlinear Model Predictive Con-\ntrol Strategy with a Disturbance Observer for Spark Ignition Engines with External\nEGR,” SAE Int. J. Commer. Veh. 10(1):360-372, doi:10.4271/2017-01-0608, 2017\n43. Gofran T., Neugebauer P., Schramm D., „Feature extraction from raw vibration signal\nand classification of bearing faults using convolutional neural networks”, Artificial In-\ntelligence from research to application, The Upper-Rhine Artificial Intelligence Sympo-\nsium UR-AI, 2019\n118\n\nModeling for Explainability: Ethical Decision-Making in \nAutomated Resource Allocation \nChristina Cociancig1, Christoph Lüth2, Rolf Drechsler3\n1,2,3 University of Bremen; German Research Center for Artificial Intelligence (DFKI GmbH), Germany \n1 chrcoc@uni-bremen.de \n2 christoph.lueth@dfki.de \n3 drechsler@uni-bremen.de \nAbstract. Decisions delegated to artificial intelligence face an alignment problem: humans \nexpect the algorithm to make fast and well-informed decisions aligning with human morals. \nIn the design and engineering process of algorithms, ethical principles enter the black box \nexplicitly and implicitly as functional or non-functional properties, much to the detriment \nof explainability and transparency. Previous work has established surrogate modeling to \npromote explainability and transparency of the decision-making process. We extend on this, \nmodel in lower complexity decision trees and as labeled transition systems, which is a \nmethod inherent to bisimulation theory, as well as evaluate on synthetic data with a rule-\nbased algorithm. As a case study, we analyze the triage processes in German and Austrian \nhospitals during the COVID-19 pandemic, based on official guidelines that regulate the \nallocation of intensive care unit beds. We discovered that the decision processes are similar, \nhowever, the systems do not behave in the same manner. The diverging behavior equates \nto a discrepant ratio of patients treated in intensive care in contrast to the general ward. Our \ninsight leads us to the conclusion that our approach ensures ethical decision-making in \nhealthcare and should be considered due to its explainability and transparency.  \nKeywords: explainability; transparency; automated decision-making; surrogate modeling. \n1 Introduction \nMachine learning and artificial intelligence (AI) in general can support virtually any human \ndecision. Whether we assume the decisions made or intervene to revise it, with data supply and \nautomation, we entrust a mostly black box with coming to the best conclusion it possibly can. \nOften, these decisions we delegate require fast conclusions and a high degree of expert \nknowledge. While algorithms can fulfill these criteria of decision-making, a human decision is \ninherently one that draws upon human morals. That we demand of algorithms to emulate this, \ngave rise to a new field of research that is located at the intercept of computer science and \nphilosophy: machine ethics [1]. \n \nMachine ethics, also referred to as AI ethics when defined in a narrower sense, became an \nincreasingly vital factor in AI research, because the decisions we now automate, have direct \nimplications on human lives. Machine ethics places an emphasis on the establishment of values \nthat should steer the development and deployment of artificial intelligences in the form of \nguidelines for “ethical AI” [2]. Ethicists agree with the pressing issue of ethical algorithmic \ndecision-making by advocating particularly for transparency [3] and explainability [4] of the \ndecisions produced by the black box that a machine learning algorithm, or even more so a deep \nlearning algorithm, can represent. \nRelated work in algorithmic explainability and transparency put forward various approaches, \nincluding but not limited to, surrogate modeling and formal verification. Previous research in the \n119\n\narea of surrogate modeling advanced to complex cases of decision tree modeling of a neural net, \nwith a well-founded result of reduced complexity, high fidelity, and comprehensibility [4]. Even \nthough it has not yet been done in full terms, approximate-bisimulation has been employed to \nmodel (dynamic) neural networks and their behavior in terms of their input and output [5]. These \napproaches add to the extensive list of measures to analyze the decision-making process with the \nintention of optimizing for ethical decision-making of the system. However, they fail to consider \nthat some explainability and transparency is better than none, especially for use cases that involve \ncritical decisions in healthcare. \n \nAI represents an evolution of informed decision-making in the medical field [6]. In the \nclinical environment, well informed decisions must be made fast. Not only in Germany this \npotential has been identified, and discussions to implement decision-making software are well \nunder way or already implemented. SmED (short for “structured initial medical assessment in \nGermany”) is an algorithm that assists medical on-call services to decide where a patient’s \nhealthcare needs can be addressed best: a general practitioner or an emergency clinic [7]. While \nboth are not yet applied in the clinical context, OPTINOFA (short for “optimization of \nemergency care through a structured initial assessment using intelligent assistance services”) \naims to provide an algorithmic assessment of the urgency of treatment in clinics [8]. The most \nstriking difference between the softwares: SmED appears to be rule-based and is not openly \naccessible, OPTINOFA is composed of an AI and will be openly accessible. \n \nIn the interest of examining a contemporary decision process in healthcare, as a case study, \nwe compare approaches to the decision-process of triage during the COVID-19 pandemic in two \ncountries: Germany and Austria. The decision processes of triage are based on a practice of \nresource allocation historically attributed to military medicine, which categorizes patients and \ncommonly prioritizes treatment of patients with a high chance of survival [9]. At the beginning \nof the pandemic, the allocation of resources, i.e., particularly intensive care beds that can \naccommodate a ventilator, has been regulated by strict guidelines in Germany [10] and Austria \n[11]. It is exactly this type of situation in which algorithms are capable to provide humans with \nrelief to make well informed, fast decisions. However, it is of the utmost importance that the \ndecisions provided by machines agree with human ethical values.  \n \nWith this paper we provide a recommendation for transparent and explainable algorithmic \ndecision-making in healthcare, that complies with the ethical principles of explainability and \ntransparency in form of non-functional properties of the system. We build on related work in the \narea and propose formal surrogate modeling with decision trees, including associated entropy \nand information gain values indicating the informative strength of a node within the tree, as well \nas modeling as labeled transition systems, a method inherent to bisimulation, which provides a \ncomparative analysis of the behavior of two systems [12]. With an algorithmic data evaluation, \nwe support the findings of our analysis numerically, and demonstrate, that drawing on metrics \ninherent to the models provide reference for a comparative analysis of systems. With this \nrecommendation, we hope to contribute an opportunity for ethical healthcare software to be \ntransparent and explainable for medical professionals and patients alike.  \n2 Methods \nThis section outlines the methodology applied to construct decision trees and a bisimulation \nevaluation of triage processes, as well as a description of the algorithm we deployed to measure \neffects in ratios. Our hybrid approach of two comparative modeling systems and a test with \nsynthetic data was chosen, because it gives a valuable insight into robust tools that can be \n120\n\naccessed for the purpose of investigating strengths and weaknesses of systems such as the triage \ndecision process. We compared and identified differences in the German and Austrian triage \nguidelines first by focusing on their underlying ethical principles and subsequently in terms of \ntheir functional properties.  \n \nBoth guidelines are governed by implicitly or explicitly defined ethical principals. The \nAustrian guideline, “Allocation of intensive care resources due to the Covid-19 pandemic”, lists \nfour ethical principles influencing every decision within the triage process: justice, non-\nmaleficence, doing good, and the observation of autonomy of the patient [11]. The definition of \neach principle is linked to several more, some non-ethical, values that shall be upheld: using \nresources efficiently, allocating fairly, not endangering the supply system, serving the well-being \nof each individual patient, respecting guardians of patients, and respecting individual freedom \n[11]. Although the Austrian guideline does not connect these values and principles to individual \ndecisions, each decision made within the process should be guided by them. The German \nguideline mentions ethical principles predominantly implicitly by connecting them to decisions \nin the assessment process, including the needs of the patient for intensive care unit (ICU) \ntreatment and the patients will that are directly reflected in decisions within the process, whereas \na prohibition of discrimination due to age, social characteristics, and disabilities, as well as \nfairness are implicit and not represented as decisions within the process per se [10]. \nDecision Trees \nTo investigate the sequence of decisions and the associated informative value of decisions, we \nmanually translated the triage guidelines into their respective decision trees and compared the \nmetrics of entropy and information gain, both inherent to information theory. Each decision \noutlined in the triage guidelines translates into a decision node of a tree. The end nodes represent \nthe decision for or against ICU treatment of an individual patient.  \n \nEntropy is measured in bits and represents the average level of information or uncertainty of \npossible outcomes of a variable. Given a variable X, with possible outcomes x1,…, xn, with an \nassociated probability of P(x1),…, P(xn), the entropy of X is defined by Shannon [13] as: \nH(ܺ) = െ෌\nP(ݔ௜) log P(ݔ௜)\n௡\n௜ୀଵ\n \n(1) \nEntropy can be calculated for each node in a decision tree. For a description of the strength of \nthe node, the value of information gain expresses the change in information entropy from one \nnode to the next: \nܫܩ(ܶ, ܽ) = H(ܶ) െH(ܶ|ܽ), \n(2) \nwhere H(ܶ|ܽ) is the conditional entropy of T given the value of ܽ [14]. Information gain can \ntherefore adopt values between zero and one, a higher information gain is associated with a \nstrong decision node, at which an informative decision is made. \nEvaluation \nTo verify the differences in the performance of the two systems, we evaluated benchmark data \nflowing through the process modeled as decision trees. To this end, we implemented a rule-based \nalgorithm that sorted and evaluated synthetic data of 100 patients based on the health data \nrequired for the German triage decision process.  \n121\n\n \nAlthough the triage criteria to receive intensive care are different for German and Austrian \npatients, both guidelines have baselines in common, whose negation can in no circumstance lead \nto treatment in intensive care. For one, a patient must give consent to receive intensive care. \nMoreover, though the German guideline explicitly states that a necessity for intensive care must \nbe assessed, the same can be assumed for the Austrian system. German medical personnel are \nfurthermore urged to assess the prospect of success of intensive care for a patient as one of the \nfirst steps in the triage process, whereas Austria assesses hopelessness and proportionality of \nICU treatment only as a criterion for the abortion of intensive care [11]. For our purpose, we \nequate the assessment of prospect of success in Germany with the assessment of hopelessness \nand proportionality in Austria and quantify this criterion with 96%, i.e., the average reported \nsurvival rate of COVID-19 in Germany and Austria [15]. \n \nBeyond the shared baseline assumptions, the triage systems additionally assess the patients \non health criteria. These criteria have determined our algorithmic implementation and data \ndevelopment and have been summed to a health score. The criteria formulated for the health \nassessment of German patients consist of five points, which quantify scores or represent the \npresence or absence of a criterion: heightened severity of illness, e.g., acute pulmonary \nembolism, acute organ failure assessed on the sepsis-related organ failure assessment score \n(SOFA), a prognostic marker for COVID-19 patients (we assume this marker to be a positive \nCOVID-19 test), comorbidity, e.g., neurological disease, and health status assessed on the \nclinical frailty scale (CFS) [10]. In Austria, health assessment is done in nine points: chance of \nsurvival via SOFA score, comorbidity, presence of cardiac insufficiency or failure, renal \ninsufficiency or failure, presence of immunosuppression, dementia assessed on Activities of \nDaily Living score (ADL), pulmonary disease, other primary disease, and other relevant criteria \n[11]. \n \nAs a first step to data creation, we affirmed the baseline assumptions for ICU treatment and \ncreated five health data points for each patient randomly, modeling the German health \nassessment. We randomly one-hot encoded for the presence or absence of a criterion and appoint \nscores where applicable, i.e., a SOFA score between 0-24, counts of comorbidities between 0-5, \nand overall health status of CFS score between 1-9. This encoding resulted in an overall health \nassessment score sum between 1-40, with a higher score being associated with a more critical \ncondition. For scores under 20, we assumed care at the general ward as sufficient, patients with \nscores over 20 require intensive care. In the corresponding trees, this decision node is represented \nas a score of under 50% or over 50%. \n \nAs a next step we translated the patient data for the Austrian decision process of nine health \nassessment points, which involved the addition of more scores. As the survival chance in Austria \nis also indicated by the SOFA score, we adopted it from the German model patient. We \ntransferred comorbidity counts over zero as the presence of a comorbidity and associate a \nheightened severity in Germany with a primary disease in Austria, as well as translate the CFS \nscore of five and above as the presence of dementia. The remaining criteria, i.e., cardiac or renal \ninsufficiency, immunosuppression and pulmonary disease were again one-hot encoded with a \nrandom distribution. The complete assessment results in an overall summed score between 1-31. \nWe again divided the score in over 50% and under 50%, a score of 15 or lower does not receive \nintensive care. \n122\n\nBisimulation  \nTo further investigate the difference in behavior of the two triage processes, we remodeled the \ndecision trees as a bisimulation evaluation. Our notation for this evaluation was adopted from \nDavide Sangiorgi [12]. Specifically, we explore the states and transitions of the processes \nmodeled as labelled transition systems (LTS), which are formally described with triples, i.e., \n(ܲݎ, ܣܿݐ, ՜) where Pr is a (non-empty) set, also referred to as the domain or the set of the \nprocesses of the LTS, Act is the set of actions or transitions, and ՜ denotes the transition relation \nbetween processes. Bisimulation is a binary relation on the states of two systems P and Q, if for \nall Ɋ we have: \n \n1) for all P’ with ܲ\nఓ՜ ܲԢ, there is Q’ such that ܳ\nఓ՜ ܳ and ܲᇱܴ ܳԢ; \n2) for all Q with ܳ\nఓ՜ ܳԢ, there is P’ such that ܲ\nఓ՜ ܲԢ and ܲԢ ܴ ܳԢ. \n \n \n \n(3) \nIf the bisimulation is complete, meaning for each process in the system P there is an equivalent \nprocess in system Q, the systems are bisimilar, i.e., they behave in the same way. If not all \nprocesses in system P can be mapped to an equivalent process in system Q, the systems might \nhave equal inputs and outputs, but internally do not behave the same way. \n3 Results \nDecision Trees \nBoth triage decision processes were modeled as decision trees. Figure 1 is a comparison of the \nGerman triage system and the Austrian triage system. For reference, we added entropy (H) values \nfor each decision node in the trees. Due to our assumption of the baseline criteria for intensive \ncare treatment as being met, i.e., necessity of treatment (represented as the first decision node \nwith H = 1 as is standard for decision trees) and consent of the patient, the respective entropy \nvalues do not amount to expressive decision nodes.  \nHowever, we were specifically interested in the decision node labeled prospect of success, \nas the location of this decision and the corresponding nodes in the trees is an identifiable \ndifference between the two decision processes. Based on our assumption that the prospect of \nsuccess of treatment, which is assessed early on in Germany, is equivalent to the criterion of \nhopelessness and proportionality, which is assessed after the ICU treatment has already \ncommenced for the Austrian patient, the anticipated entropy values correspond to the same \nentropy of H = .24. Again, these values are identical, because we assume a survival chance of \n96% for both countries, as it is the reported survival rate of COVID-19 in both countries [15]. \nThe information gain, however, of the prospect of success node amounts to IG = .76 in the \nGerman system, compared to the Austrian system of IG = .75.  \n \n \n123\n\nEvaluation \nThe rule-based algorithmic evaluation of our data of 100 patients revealed, that the Austrian \nsystem initially treats more patients in ICU, more specifically 56% in comparison to 39% in the \nGerman system. This ratio is not representative, however, of the mandatory re-evaluation which \nis featured in both triage processes and more visible in the labeled transition systems.  \nBisimulation\nThe LTS of triage in Germany and Austria offer valuable cues as to how the decision process is \nexecuted and can be described and analyzed formally. Figure 2 compares the LTS side by side \nand indicates their domain, actions, and transition relations. For the sake of clarity and brevity, \nwe indicated transitions abbreviated, e.g., from R1 to R6, instead of “No Necessity assessed” as \nit would be formally described, we simply indicated “No Necessity”.   \nNot only does the LTS comparison demonstrate that the Austrian triage ultimately has less states\nuntil it arrives at a final decision over ICU treatment or no ICU treatment, the formal description \nof the LTS is a further indicator for the similarity of the systems. As we examine the binary \nrelation between the two systems, we pair processes with the same transition relations: \nR = { (R1, Q1), (R3, Q2), (R4, Q3), (R6, Q5) }. \n(4) \nFig. 1. German (left) and Austrian (right) triage decision tree, entropy value per decision node\n124\n\nGiven that not all states in the German system have equals in the Austrian system, the bisimilarity \nof the systems cannot be proven and therefore indicate that the systems do not behave in the \nsame way.\n4 Discussion\nThe importance of interpretable models, that promote transparency and explainability cannot be\nemphasized too much. Our method of surrogate modeling as decision trees and labeled transition \nsystems, as well as evaluating on synthetic data, has not only enabled us to identify the different \napproaches between the triage processes in Germany and Austria, but also given us the \npossibility to explain any given final decision by traversing through the models. Therefore, by \nmodeling in comparison, we achieve transparent and explainable decision-making as it is \npromoted by machine ethics [1,3]. \nAt first sight, the systems seem to align in their design, as they assess patients on similar \ncriteria. With the data evaluated based on the decision trees, however, we see that the smallest \ndifferences in the processes have a substantial effect on the number of patients treated and thus \npossibly on the number of lives saved. The decision trees have provided a comprehensible model \nto enable further evaluation, as we expected and was touched on in [4]. The most striking \nobservation to emerge from the evaluation of information gain, is the difference between the \nvalues for the prospect of success decision node. Even though the numeric difference between \nthe German and Austrian node is marginally small, it indicates that the informational value of \nthe node is in fact different, and this difference is due to its position in the decision process, i.e., \nthe rank of the decision.  \nEven though a bisimulation of neural networks has not yet been successful [5], we were able \nto considerably benefit from modeling the rule-based triage systems as labeled transition systems \nto reproduce the finding of differences from the decision trees. As the binary relation is \nincomplete, the evaluation confirms that the decision processes are not equal, and furthermore, \nthat this inequality can be attributed to an inequality in the patient’s health assessment and the \norder of the decisions made in both countries. The model borrowed from bisimulation has \nFig. 2. LTS of triage in Germany (left) and Austria (right)\n125\n\nprovided us with the ability to formally describe with states and transition relations are \nrepresented in both decision processes.  \n \nAlthough we gained considerable insight from modeling the processes, the link to ethical \nprinciples, especially to the overarching principles of explainability and transparency of \n(algorithmic) decisions, were made by us. Neither the German nor Austrian guideline mentions \nthese principles as essential to decisions in triage. Furthermore, despite the insight, we are not \nqualified to make statements regarding fairness of the decision process, even though the German \ntriage guideline acknowledges this ethical value [10]. Synthetic data is simply not suitable to \nevaluate this metric on.   \n5 Conclusion \nWe have outlined a comparative method of evaluating decision-making by modeling the decision \nprocesses in form of labeled transition systems, as well as decision trees and the corresponding \nmetrics of entropy and information gain, to promote the transparency and explainability of \ndecisions within the triage process. For our case study of triage processes in Germany and \nAustria, we found that differences in the non-functional properties the decision process adheres \nto, i.e., in broader terms the ethical implications of the triage system, has consequences for the \nbehavior of the system and these consequences can be measured. In the context of triage, the \nmeasurable difference of courses of action ultimately equates to lives saved or lost.  \n \nFor decision-making in triage, we conclude that a formal modelling allows for the analysis \nand precise comparison of systems. This can be applied to systems of different countries, or two \ncompeting systems within one country. Our findings assert that a careful engineering of the \ndecision process, whether with implicitly or explicitly translating ethical principles into \ndecisions, can lead to more efficient decision-making. Besides efficiency, with modeling of the \ndesign we access insight about comparability and weaknesses of systems, and measurable \nmetrics can lead to a better understanding of the outcome of specific decisions. In combination, \nformal modelling, i.e., decision trees and bisimulation, lead to transparency and explainability \nof the decisions made.   \n \nOur method could advance many other decision processes conducted by AI or machine \nlearning in healthcare. Decision-making softwares, whether they include an AI or rule-based \nalgorithm similar to what we employed and SmED appears to be, behave according to underlying \nethical principles. As we have concluded from the triage guidelines in our use case, the link \nbetween these principles and the properties of the system they are embedded into, are not always \nfunctional, i.e., direct and apparent. Yet, the insight gained from the models and their \ncorresponding metrics provides an excellent resource to ensure ethical decision-making. Future \nwork evaluating its models algorithmically on benchmark data should, however, aim to collect \nor obtain organic data, which was beyond of the scope of this research.  \nReferences \n1. Anderson, M., Anderson, S. L.: Machine ethics: Creating an ethical intelligent agent. AI\nmagazine 28(4), 15-26 (2007). doi.org/10.1609/aimag.v28i4.2065.\n2. Jobin, A., Ienca, M., Vayena, E.: The global landscape of AI ethics guidelines. Nature\nMachine Intelligence 1, 389–399 (2019). https://doi.org/10.1038/s42256-019-0088-2.\n3. Garcia-Gasulla, D., Cortés, A., Alvarez-Napagao, S., Cortés, U.: Signs for Ethical AI: A\nRoute Towards Transparency. arXiv:2009.13871. (2020).\n126\n\n4. Schaaf, N., Huber, M.F., Maucher, J.: Enhancing Decision Tree based Interpretation of \nDeep Neural Networks through L1-Orthogonal Regularization. arXiv:1904.05394. \n(2019). \n5. Donnarumma, F., Aniello, M., Prevete, R.: Dynamic network functional comparison via \napproximate-bisimulation. Control and Cybernetics 44(1): 99-127 (2015). \n6. Jones, L.D., Golan, D., Hanna, S.A., Ramachandran, M.: Artificial intelligence, machine \nlearning and the evolution of healthcare: A bright future or cause for concern? Bone & \nJoint Research. 7, 223–225 (2018).  \n7. Graf von Stillfried, D., Czihal, T., Meer, A.: Sachstandsbericht: Strukturierte \nmedizinische Ersteinschätzung in Deutschland (SmED). Notfall Rettungsmed. 22, 578–\n588 (2019). https://doi.org/10.1007/s10049-019-0627-8. \n8. Abstracts zu Vorträgen und Postern der 14. Jahrestagung der Deutschen Gesellschaft \nInterdisziplinäre Notfall- und Akutmedizin: 14.–16. November 2019, Bremen. Notfall \nRettungsmed. 22, 1–17 (2019). https://doi.org/10.1007/s10049-019-00645-y. \n9. Iserson, K. V., Moskop, J. C. Triage in medicine, Part I: Concept, History, and Types. \nAnnals of Emergency Medicine 49(3): 275-281 (2007).  \n10. Marckmann, G., Neitzke, G., Schildmann, J., Michalsen, A., Dutzmann, J., Hartog, C., \nJöbges, S., Knochel, K., Michels, G., Pin, M., Riessen, R., Rogge, A., Taupitz, J., \nJanssens, U.: Entscheidungen über die Zuteilung intensivmedizinischer Ressourcen im \nKontext der COVID-19-Pandemie: Klinisch-ethische Empfehlungen der DIVI, der \nDGINA, der DGAI, der DGIIN, der DGNI, der DGP, der DGP und der AEM. \nMedizinische Klinik – Intensivmedizin und Notfallmedizin 115(6): 477-485 (2020). \nhttps://doi.org/10.1007/s00063-020-00708-w. \n11. ARGE Ethik ÖGARI, Allokation intensivmedizinischer Ressourcen aus Anlass der \nCovid-19-Pandemie. Klinisch-ethische Empfehlungen für Beginn, Durchführung und \nBeendigung von Intensivtherapie bei Covid-19-PatientInnen. Vienna: ÖGARI, \nhttp://www.oegari.at/web_files/cms_daten/, last accessed: 2021/08/24. \n12. Sangiorgi, D.: Introduction to Bisimulation and Coinduction. Cambridge University \nPress, Cambridge (2011). https://doi.org/10.1017/CBO9780511777110. \n13. Shannon, C.E.: A Mathematical Theory of Communication. The Bell system technical \njournal 27(3): 379-434 (1948). \n14. Kent, J.T.: Information gain and a general measure of correlation. Biometrika 70(1): 163-\n173 (1983). \n15. Dong, E., Du, H., Gardner, L.: An interactive web-based dashboard to track COVID-19 \nin real time. Lancet Inf Dis. 20(5): 533-534. doi: 10.1016/S1473-3099(20)30120-1, \naccessed: 2021/03/07. \n127\n\nCondition Monitoring of Electric Motor with Convolutional \nNeural Network\nTanju Gofran 1, Maurice Kettner 1 and Dieter Schramm2 \n1 IEEM-Institute of Energy Efficient Mobility, Karlsruhe University of Applied Sciences \ngota000\u0014@h-ka.de\u000fmaurice.kettner@h-ka.de  \n2University of Duisburg-Essen \ndieter.schramm@uni-due.de \nAbstract. Safe, efficient and uninterrupted operation of machine requires continuous \nmonitoring of its health and modern autonomous smart factory demands a Condition \nMonitoring (CM) process without direct human involvement. Deep Learning (DL) \nalgorithms have shown great success of learning directly from data in various real life \napplications and recently it become also popular in CM researches but still detail \nclarification of selecting the DL design and its relevance to learn the features from data \nare often missing. This paper shows a DL algorithm - Convolutional Neural Network \n(CNN) to CM of an Electric motor from its external vibration. The output of the deep \nlayers of the learned model is analyzed to explain how the model extract features of raw \nvibration input and do the classification of different conditions. \nKeywords: Condition Monitoring (CM) 1; Convolutional Neural Network (CNNs) 2; \nFeature Map 3; \n1\u0003 Introduction \nIn the age of the fourth Industrial Revolution, application of Artificial Intelligence is not just a \ndemand but a necessity. A smart factory involves numerous machines and sensors requiring \nmachine to machine and machine to human communication without interruption. Condition \nmonitoring and predictive maintenance of the machines to prevent failure in advance or detect \nany anomaly early enough before breakdown is one of the key trends of Industry 4.0. \nApplication of Machine Learning (ML) algorithms in the field of Condition Monitoring \n(CM) of Electric machines (EM) has been investigated and implemented in reality in various\nresearches for the last several years, but this is still relatively new and has a lot of room for\nimprovement. Vibration based CM of EM has been found very effective as the vibration\nfrequency analysis can uncover several electrical, mechanical defaults and as well as running\nconditions of the machines. But for such analysis exact parameters of the machine and its drive\nis required and furthermore in real life impending fault signatures are not as ideal as theoretical\nfault signature. ML algorithms can learn from monitoring sensor data without prior knowledge\nof the EM and traditional ML based CM process involves extraction of useful information\nfrom raw data and use the extracted features as input of the ML and finally classify different\nfaults. This feature extraction rules is often depend on the domain, so the same algorithm may\nnot work for other domain or motor drive. Deep Learning (DL) algorithms which can directly\nlearn the features from data have recently become very popular approach in many fields\nbecause of advancement of computation power, cloud computing, simpler tools or frameworks\nand also for easily accessible large database.\n128\n\nThe presented paper is a continuation of previous work where novel convolutional neural \nnetwork (CMCNN) architecture was shown to detect bearing faults using a public dataset [1]. \nIn this work we used a newly generated vibration dataset for bearing faults to model the \nCMCNN architecture for multi-sensory input. Separately generated test data is used to evaluate \nthe accuracy of the model and the learned model¶s deep layers are analyzed to understand the\nfeature extraction process. \n2\u0003 Related work: \nThe challenge of beginning researching ML and DL algorithms for CM or fault diagnosis is the \naccess of dataset because creating a realistic mechanical fault dataset generating test-bench is \ncomplex and costly. For vibration based rolling bearing fault diagnosis the dataset produced by \nCase Western Reserve University (CWRU) is the most popular and easily accessible dataset \nthat has been considered as standard reference in many publications [3]. Neupane and Seok \nreviewed a large number of publications regarding DL algorithms using CWRU dataset in their \npaper [3]. Smith and Randall have analyzed the entire dataset of CWRU to recommend \nbenchmark for diagnostic technique [4].  CWRU dataset has mainly six classes of data: \nhealthy, inner ring fault, rolling element fault and outer ring fault at three load zone [2]. The \nfaults were implemented in sizes of 0.007 to 0.028 inch with Electric Discharge Machining and \nthe monitoring bearings were either at Drive-side (DE) or Fan-side (FA) of the motor. All the \nvibrations are measured with three sensors located at DE, FE and at base plate and \nmeasurements were taken for four motor speeds.  \nVarious DL algorithms like Deep Belief Networks (DBN), Autoencoder (AE), Generative \nAdversarial Networks (GAN), Recurrent Neural Networks (RNN), Convolutional Neural \nNetworks (CNN) etc. are investigated to detect bearing faults using the CWRU dataset in the \nliteratures. Stacks of AE based deep neural network (DNN) is applied to classify CWRU \ndataset among ten classes considering different fault sizes as different classes by Jia and et al, \nwhere they used the frequency spectrum of the raw data as the input [2]. Shao and et al. \nshowed DBN based bearing fault classification using both simulated vibration data for inner \nand outer ring fault and the CWRU dataset dividing all the dataset into ten classes [6]. Jiang \nand et al proposed a deep recurrent network (DRNN) to automatically extract feature from \ninput spectrum and diagnose rolling bearing fault in their work [7]. They consider frequency \ndomain signal as input believing noisy vibration data may not be robust. The proposed DRNN \nhas stack of recurrent hidden layers of long short-term memory (LSTM) units and classify the \nCWRU dataset into 12 conditions. GAN based fault diagnosis on CWRU dataset is studied by \nJiang and et al [8]. Their idea of implementing GAN algorithm to differentiate faulty vibration \nfrom healthy vibration as anomaly detection, relating with real industrial scenario where faults \nappear in the bearings after millions of cycle hence data collection for faulty bearing is \ndifficult. For Robust feature extraction and fault classification Shaheryar and et al proposed \nhybrid model (MCNN-SDAE) of two layers multi-channel CNN combined with three stacks of \nDenoising Autoencoder (DAE) using the CWRU dataset [9].  \nGua and et al showed a hierarchical adaptive deep convolutional network (ADCNN) using \nCWRU data where the 1D vibration is converted to 2D matrix and they tested their model for \nboth fault classification also fault size predictions [10].Wide first-layer kernels with deep CNN \n(WDCNN) model is proposed by Zhang and et al also using the CWRU data [11]. They used \ndata argumentation technique which is basically dividing the long signal into segments to \ncreate bigger dataset in which the input width is 2048. Some sets of the training data were \n129\n\noverlapped segments and some were not. Their five layer CNN was designed as the first layer \nhas wide kernel size and following layers have very small kernel width and finally the model \nclassified 10 labels. Other works of CNN based bearing fault diagnosis are presented in the \nliteratures [12-15]. \nThe investigated works in the literatures mostly used same dataset to test their model \naccuracy to test their domain adaptively for example the fan-end and drive-end vibration \ninformation should be clearly different and most cases it is not clear if they considered fault \nclasses for both locations learn the domain robustly or not. Another most interesting note is \nmany of the studies considered the faults sizes as separate classes, where the CWRU dataset \nfault sizes are clearly different (0.007 inch, 0.014 inch, 0.021 inch) which should be easily \ndiagnosable. Among many DL based CM approaches, CNN has shown the most suitability of \nusing raw data directly. \nIn our previous work we used the CWRU dataset to train CMCNN model and classified the \nclasses considering both location of the bearing and fault sizes in same class. The aim of the \ncurrent work is to introduce a new dataset to model the CNN model where same design \napproach is considered as CMCNN presented in previous work [1].  \n3\u0003 Dataset: IEEM - CMData \nThe dataset contains external vibrations of a motor having different types of faulty bearings at \nvarious speed and load combinations. External vibration means it should contain more noise or \nadditional vibration from the rotating parts which is ideal for industrial applications. The \nbearing data generating test-bench is developed at the Institute of Energy Efficient Mobility \n(IEEM) of University of Applied Science and Technology Karlsruhe and supported by SEW-\nEurodrive GmbH (SEW). In the Fig. 1 a view of the test-bench (left) and the CAD design (left) \nis shown. \ndĞƐƚͲ\u0011ĞŶĐŚ\n^ǇŶĐŚƌŽŶŽƵƐ\u0003\nDŽƚŽƌ\n\u0004ƐǇŶĐŚƌŽŶŽƵƐ\u0003\nDŽƚŽƌ\n&\u001c\u0003^ĞŶƐŽƌ\n\u0011\u0004\u0003^ĞŶƐŽƌ\n\u0012ŽƵƉůŝŶŐ\n\u0018\u001c\u0003^ĞŶƐŽƌ\nFig. 1. IEEM-CMData test bench \nThe test-bench has test motor of power 0.75 kW and speed of 1440 RPM which is an \nasynchronous gear motor (R47 DRN80M4 by SEW) with 8.01 gear ratio connected thorough a \nhighly flexible coupling with the load motor of output torque 144Nm, speed of 3000 RPM \nwhich is a synchronous gear motor (R47 CMP80M by SEW) with gear ratio 3.83. Artificial \nfaults were implemented on different parts of the deep groove ball bearing (6304-2RSH by \nSKF). Three acceleration sensors (iCS80 by IDS Innomic GmbH) were installed near FE, DE \nand base plate (BA) to measure the vibrations. For training all types of data are generated for \nboth bearings at Fan-End (FE) and Drive-end (DE) at two different sample rates. In this work \n130\n\nthe sample rate of all input data of the model is 12.8kS/s. National Instrument¶s cDAQ-9174 is\nused for data acquisition and data processing is done with MATLAB 2018a with additional \npackage NI-DAQmx.   \nFig. 2. Example of engraved spall in the inner-ring (left) and example of reduced amount of lubrication \nfor measurement (right) \nArtificial faults are implemented on different parts of the bearing to achieve different types of \nfaults and one third of the recommended lubrication is used during measurements. Fig. 2 shows \nan example of a prepared bearing having a small inner-ring spall created by electric engraver \n(left) and the amount of lubrication used for the measurement (right). Table 1 contains the \ndescription of different types data created for the dataset with short names and labels of classes \nfor the model. Among all prepared bearings, ten bearings (Training Bearings) are used to create \ntraining data for the CMCNN model and three bearings (Test Bearings) are kept for testing the \nmodel. Four speeds (Speed-1 to 4) and five loads (Load-0 to 4) were pre-selected for the \nmeasurements, which are called Known Speed-Load data used for training the model and some \ndata are collected at randomly selected Speed and Load combinations which are called \nUnknown Speed-Load data used for testing the model accuracy.  \nTable 1. Fault description and short naming of the data types with labels for the model \nFault Description \nShort Name \nClass Labels \nFault \nDE \nFE \n4 class 8 class \nHealthy(NO) \nNoFault \nDEOK \nFEOK \n0 \n0 20 10 \nInner ring ((IR) spall of 2mm(S1) \nIRSpall \nDEIRS1 FEIRS1 1 \n1 21 11 \nInner ring (IR) spall of 3.5mm(S2) \nIRSpall \nDEIRS2 FEIRS2 1 \n1 21 11 \nOuter ring (OR) spall of 2mm (S1) \nORSpall DEORS1 FEORS1 2 \n2 22 12 \nOuter ring (OR) spall of 3.5mm (S2) \nIRSpall \nDEORS2 FEORS2 2 \n2 22 12 \nRough rolling surface (RR) \nRRSurface \nDERR \nFERR \n3 \n3 23 13 \n4\u0003 IEEM-CMCNN Architecture for Bearing Fault Classification \nThe model is named as IEEM-CMCNN; has input of three channels 1D data, six convolution \nlayers, three Fully-connected layers and four or eight output classes. The detail architecture of \nthe IEEM-CMCNN is described in the Fig. 3.  \n131\n\nThe input is a three-channel 1D vibration data considering three sensors at three positions. The \nfirst channel contains the main-sensor data, second channel belongs to the opposite-sensor data \nand third channel for the base-sensor data. Main-sensor for the FE bearing is the sensor at FE \nand sensor at DE is the opposite-sensor; for DE bearing this is reversed accordingly. During \ntraining the input of IEEM-CMCNN is a fixed-size: 1 x 1000 x 3 vibration data. The one \ndimensional vibration input length is considered as approximately one revolution of the motor \nshaft as described in previous paper [1]. No pre-processing is done on the training dataset. \n\u0012ŽŶǀϭ\n\u0012ůĂƐƐŝĨŝĐĂƚŝŽŶ\n&ĞĂƚƵƌĞ\u0003\u001cǆƚƌĂĐƚŝŽŶ\nKƵƚƉƵƚ\nEŽ\u0003ĨĂƵůƚ\n/Z^ƉĂůů\nKZ^ƉĂůů\nZZ^ƵƌĨĂĐĞ\nWŽŽůϭ\n&\u0012ϭ\n&\u0012Ϯ\n&\u0012ϯ\nϯ\u0003\u0012ŚĂŶŶĞů\u0003ϭ\u0018\u0003\nsŝďƌĂƚŝŽŶ\u0003\n\u0012ŽŶǀϮ\nWŽŽůϮ\nDĂŝŶ\n^ĞŶƐŽƌ\u0003\n\u0011ĂƐĞ\n^ĞŶƐŽƌ\u0003\nKƉƉ͘\n^ĞŶƐŽƌ\u0003\n/ŶƉƵƚ\nZĞ>h\nZĞ>h\nWŽŽůϲ\nFig. 3. IEEM-CMCNN architecture for four fault classes \nThe three sensor vibration input is than passed through stack of six convolution (Conv) \nlayers. At the first layer the filters have very large receptive field i.e. ͳ ൈͷͲ\u0003and gradually\nreduced sized filters towards higher level thus at final layer filter size become\u0003ͳ ൈʹ. The\nconvolution stride is fixed to ͳ ൈͳ\u0003and padding varies from lower layers i.e. ʹͶ ൈʹͶ\u0003 to\nhigher layers i.e. ʹ ൈʹ\u0003(where the stride and padding size is to capture left/right centre). The\nlast Conv Layer padding is\u0003Ͳ ൈͲ. The convolution stride and padding in layers are calculated\nin way to preserve the most of length of the input of each layer. After first Conv layer one \nbatch normalization layer is kept. Each convolution layers are followed Rectified-Linear unit \nLayer (ReLu) to remove the negative value, those followed by Max-Pooling layers (Pool) of \nwindow size 1x2 with stride 2 and zero-padding.  \nThe stack of Conv layers is then followed by three fully connected (FC) layers: first FC \nlayer has 1024 channels with a ReLu layer, second FC layer has 1000 channels also with one \nReLu layer and third has same number of channel as number of class. The final layer is soft-\nmax layer. We compared different architecture of different number of filters after analysing the \nfilter activities at each Conv layer: in this work the developed architecture has similar number \nof filters as VGG16 [16]. \nThe training was stopped when accuracy is not improving after 3 epochs. \n5\u0003 Model Accuracy Analysis \nAs discussed in Section-2, most of the literatures considered to classify all data types where FE \nand DE data should be easily detectable. In this work we compare two models: 1) training the \nmodel for four classes (Model: 4-Class) where location of bearing (DE and FE) is not known to \n132\n\n \n \nthe model and 2) training the model for eight classes (Model: 8-Class) where two bearing \nlocations belonged to different classes.   \n \nThe accuracy of the models are also evaluated by testing Unknown speed-load data from \ntraining bearings and test bearings as well as Known Speed-Load data from test bearing. This \nway, the test data can be divided into three groups: 1) Unknown Speed-Load data from \nTraining Bearings (UnSpLd_TrBr), 2) Known Speed-Load data from Test Bearings \n(KnSpLd_TsBr) and 3) Unknown Speed-Load data from Test Bearings (UnSpLd_TsBr). \n \n \nFig. 4. Test accuracy comparison for 4 Class Model Vs 8 Class Model  \n \nModel: 4-Class and Model: 8-Class both has average training accuracy above 99% and to \nevaluate the performance accuracies are checked per class labels.  In Fig. 4 the performance of \ntwo models are compared for fault sizes and location of the bearings. The labeling of the \nclasses is given in Table 1. \n6\u0003 Feature Map Analysis  \nDL based CM of electric machine has been successfully applied in many researches but in \ngeneral it is still not clear why the fault detections were  made with high accuracy and how the \nnetwork is learning the features from vibration. In a previous work [1], we analyse the first \nConv layer output by converting them to frequency domain and showed that a significant range \nof frequencies were learned by each filters for each classes. In the paper [11] the authors also \nfocused feature visualization with FFT and showed feature distribution for each layer and each \n10 classes using Stochastic Neighbour Embedding (t-SNE). In this work, we focused on \nunderstanding the how in all convolution layers features are learned and thus the classes are \nseparated. \n6SHFLILF\u0003\n'DWD\u00037\\SH\n7UDLQHG\u0003\n&0&11\n)HDWXUH\u0003IRU\u0003DOO\u0003ILWHUV\u0003\nRI\u0003DOO\u0003OD\\HUV\n$QDO\\VLV\u0003\n2XWSXW\u0003RI\u00030$)\n)LOWHU\u0003\n$FWLYLW\\\n0RVW\u0003$FWLYH\u0003\n)LOWHU\n\u000b0$)\f\n0$)\u0003SHU\u0003OD\\HU\u0003\nIRU\u0003DOO\u0003'DWD\u0003W\\SH\u0003\nIRU\u0003DOO\u0003FODVVHV\n \nFig. 5. Tasks involed for analysis the feature map of trained IEEM-CMCNN \n \n \nOne feature visualizing technique in computer vision is to feed the network with large \namount of dataset and keep track of which images highly activate some neurons, which is \nshown by Girscick et al [17]. We followed similar approach, aiming to understand among \n133\n\n \n \nnumber of filters in the convolution layers if certain filters are contributing to learn the features \ncomparing other filters of the same layers.   \n \nThe tasks involved to analyse the feature map of IEEM-CMCNN can be described by Fig 5. \nAs the trained dataset has different motor operating conditions and two bearing locations in \nfirst step we define the Data type, so one specific class (i.e. IRSpall) has 4 Speeds times 5 \nLoads in total 20 types of data type for both bearing location. Each type of data is feed through \nthe trained model and all the feature maps of all the filters of all layers are saved for later \nanalysis. Filter activity is measured by calculating the area under the curve of the Pool output. \nIn Fig. 6 the Conv output or the feature map of all six layers (left) for one input for DE bearing \nat Speed-4, Load-4 and the filter activity pointing the most active filter in red star at all layers \nfor the same input (right) is shown. It is seen that at each layer one filter is highest active than \nothers; for example at layer-1, filter-3 is most active among the 16 filters and at layer-6 filter-\n338 is the most active among 512 filters. \n \n>ĂǇĞƌͲϭ͕\u0003D\u0004&͗\u0003ϯ\n>ĂǇĞƌͲϮ͕\u0003D\u0004&͗\u0003ϯ\n>ĂǇĞƌͲϯ͕\u0003D\u0004&͗\u0003ϭϳ\n>ĂǇĞƌͲϰ͕\u0003D\u0004&͗\u0003ϭϭϱ\n>ĂǇĞƌͲϱ͕\u0003D\u0004&͗\u0003ϰϱ\n>ĂǇĞƌͲϲ͕\u0003D\u0004&͗\u0003ϯϯϴ\nĨŝůƚĞƌƐ\nWŽŽůŽƵƚĂƌĞĂ\nWŽŽůŽƵƚĂƌĞĂ\nWŽŽůŽƵƚĂƌĞĂ\nWŽŽůŽƵƚĂƌĞĂ\nWŽŽůŽƵƚĂƌĞĂ\nWŽŽůŽƵƚĂƌĞĂ\nĨŝůƚĞƌƐ\nĨŝůƚĞƌƐ\nĨŝůƚĞƌƐ\nĨŝůƚĞƌƐ\nĨŝůƚĞƌƐ\n \nFig. 6. Convolution output or Feature Map of all layers (left) and filter activity for the same input at all \nlayers (right) \n \nIn the next step for all data type Filter Activity is checked thus the most active filter (MAF) of \nall data type is known. MAF means for all 128 inputs of a specific data type most of the time \n(i.e. 90% times) one particular filter is always highest active. In Fig.7 MAF for all 20 data of \nfor both bearings are shown for four layers. In the similar figure MAF for one test data \n(TsKnLd) is also plotted and it is shown that almost all time MAF for test data and training \ndata are same. In this way it can be concluded that for one trained model some certain filters \nare contributing to learn the class features and now these features of MAFs can be examined to \nknow if the features are more differentiable for classes and thus fault classification is highly \naccurate. In Fig. 8 extracted feature or Conv output of MAFs for four classes is plotted over \neach other over for 1st, 2nd 4th and 6th layers and it is observed that from to higher layers the \nclasses are becoming more distinguishable and thus easily diagnosable as different class.  \n134\n\n>ĂǇĞƌͲϭ\n>ĂǇĞƌͲϮ\n>ĂǇĞƌͲϱ\n>ĂǇĞƌͲϲ\nϮϬ\u0003\u0018ĂƚĂ\u0003ƚǇƉĞƐ\u0003ĨŽƌ\u0003ĞĂĐŚ\u0003ďĞĂƌŝŶŐ\u0003ůŽĐĂƚŝŽŶ\nϮϬ\u0003\u0018ĂƚĂ\u0003ƚǇƉĞƐ\u0003ĨŽƌ\u0003ĞĂĐŚ\u0003ďĞĂƌŝŶŐ\u0003ůŽĐĂƚŝŽŶ\nϮϬ\u0003\u0018ĂƚĂ\u0003ƚǇƉĞƐ\u0003ĨŽƌ\u0003ĞĂĐŚ\u0003ďĞĂƌŝŶŐ\u0003ůŽĐĂƚŝŽŶ\nϮϬ\u0003\u0018ĂƚĂ\u0003ƚǇƉĞƐ\u0003ĨŽƌ\u0003ĞĂĐŚ\u0003ďĞĂƌŝŶŐ\u0003ůŽĐĂƚŝŽŶ\n&ŝůƚĞƌƐ\n&ŝůƚĞƌƐ\n&ŝůƚĞƌƐ\n&ŝůƚĞƌƐ\ndƐ<Ŷ^ƉͲEK\ndƐ<Ŷ^ƉͲ/Z\ndƐ<Ŷ^ƉͲKZ\ndƐ<Ŷ^ƉͲZZ\nFig. 7. MAF for all data type for four layers (1st, 2nd, 5th and 6th) of trained IEEM-CMCNN for 4 classes \nEŽ&ĂƵůƚ͗\n/Z^ƉĂůů\nKZ^ƉĂůů\nZZ^ƵƌĨĂĐĞ\n>ĂǇĞƌͲϮ\n>ĂǇĞƌͲϭ\n>ĂǇĞƌͲϱ\n>ĂǇĞƌͲϲ\nD\u0004&\u0003ĨŽƌ\u0003EK͗\u0003ϱϮ͕\u0003/Z͗ϯϯϴ͕\u0003KZ͗ϭϰϮ͕\u0003ZZ͗ϭϰϮ\nD\u0004&\u0003ĨŽƌ\u0003EK͗\u0003ϭϰϰ͕\u0003/Z͗ϰϱ͕\u0003KZ͗Ϯϱϰ͕\u0003ZZ͗Ϯϱϰ\nD\u0004&\u0003ĨŽƌ\u0003EK͗\u0003ϵ͕\u0003/Z͗ϯ͕\u0003KZ͗ϳ͕\u0003ZZ͗ϳ\nD\u0004&\u0003ĨŽƌ\u0003EK͗\u0003ϯ͕\u0003/Z͗ϯ͕\u0003KZ͗ϯ͕\u0003ZZ͗ϭϳ\nFig. 8. Extracted features or Conv output by the MAF relevant to the classes at four layers (1st, 2nd, 5th and \n6th) of trained IEEM-CMCNN for 4 classes  \n135\n\n \n \nThe plotted Conv outputs of all classes are of same Speed-Load, that means all the inputs has \nsame noise or vibration infused in their data and the different fault classes should have \ndifferent significant pattern after de-noising.  In Fig. 8 we can see that the infused noise in this \ncase the healthy or NOFault (in bright green colour) is becoming less and less visible in higher \nlayers and the patterns of the fault classes are becoming more visible in higher layers. \n7\u0003  Conclusion \nThis work shows that the design criteria for the CNN architecture in previous work can be \nadapted for different test-bench data. The accuracy comparison in section-5 (Fig. 4) reveals \nthat both models can detect fault classes with high accuracy for both Training Bearings and \nTest Bearings. The feature analysis explains how in deep layers the model learns the features \nfor different classes.  \n \n \nThis work shows the approach of designing the input size for vibration based bearing fault \ndetection applied for CWRU dataset also adaptable for IEEM-CMData. The multi-channel \ninput design can be considered in other applications where multiple sensors are involved. \nFeature analysis shows that how features are learned from noisy data similar like computer \nvision where it is known that lower layers detect the low-level features like edges, dark spots \nand high-level features like shape, object are learnt in the higher layers. It shows in vibration \nbased detection the lower layers are de-noising the data and in higher layers the pattern of the \nvibration curve which is becoming trainable for the classifier or final layer with high accuracy. \nThis analysis approach can be implemented in different vibration based problems and number \nand thus sizes or numbers of filters in each layer can be analyzed to optimize the model \narchitecture. \n \n8\u0003 References \n[1] \nGofran, Tanju; Neugebauer, Peter; Schramm, Dieter. Feature Extraction from Raw \nVibration Signal and Classification of Bearing Faults Using Convolutional Neural \nNetworks (2019). Proceedings of the Upper-Rhine Artificial Intelligence Symposium on \nArtificial Intelligence From Research to Application. 15-21. ISBN 17±21. 978-3-\n9820756-1-7 \n[2]    Case Western Reserve University Bearing Data Center. Accessed: Jul. 27, 2021. \n[Online]. Available:  https://csegroups.case.edu/bearingdatacenter/pages/welcome-case-\nwestern-reserve-university-bearing-data-center-website \n[3] \nNeupane, Dhiraj; Seok, Jongwon (2020). Bearing fault detection and diagnosis using case \nwestern reserve university dataset with deep learning approaches. A review. IEEE \nAccess. 8. 93155±93178 \n[4] \nSmith, Wade A.; Randall, Robert B. (2015). Rolling element bearing diagnostics using \nthe Case Western Reserve University data. A benchmark study. Mechanical systems and \nsignal processing. 64. 100±131 \n136\n\n \n \n[5] \nJia, Feng; Lei, Yaguo; Lin, Jing; Zhou, Xin; Lu, Na (2016). Deep neural networks. A \npromising tool for fault characteristic mining and intelligent diagnosis of rotating \nmachinery with massive data. Mechanical systems and signal processing. 72. 303±315 \n[6] \nShao, Haidong; Jiang, Hongkai; Zhang, Xun; Niu, Maogui (2015). Rolling bearing fault \ndiagnosis using an optimization deep belief network. Measurement Science and \nTechnology. 26. 115002 \n[7] \nJiang, Hongkai; Li, Xingqiu; Shao, Haidong; Zhao, Ke (2018). Intelligent fault diagnosis \nof rolling bearings using an improved deep recurrent neural network. Measurement \nScience and Technology. 29. 65107 \n[8] \nJiang, Wenqian; Cheng, Cheng; Zhou, Beitong; Ma, Guijun; Yuan, Ye (2019). A novel \nGAN-based fault diagnosis approach for imbalanced industrial time series. arXiv preprint \narXiv:1904.00575 \n[9] \nShaheryar, Ahmad; Yin, Xu-Cheng; Yousuf, Waheed (2017). Robust feature extraction \non vibration data under deep-learning framework. An application for fault identification \nin rotary machines. International Journal of Computer Applications. 167. 37±45 \n[10] Guo, Xiaojie; Chen, Liang; Shen, Changqing (2016). Hierarchical adaptive deep \nconvolution neural network and its application to bearing fault diagnosis. Measurement. \n93. 490±502 \n[11] Zhang, Wei; Peng, Gaoliang; Li, Chuanhao; Chen, Yuanhang; Zhang, Zhujun (2017). A \nnew deep learning model for fault diagnosis with good anti-noise and domain adaptation \nability on raw vibration signals. Sensors. 17. 425 \n[12] Hoang, Duy-Tang; Kang, Hee-Jun (2019). Rolling element bearing fault diagnosis using \nconvolutional neural network and vibration image. Cognitive Systems Research. 53. 42±\n50 \n[13] Xia, Min; Li, Teng; Xu, Lin; Liu, Lizhi; Silva, Clarence W. de (2017). Fault diagnosis for \nrotating machinery using multiple sensors and convolutional neural networks. \nIEEE/ASME transactions on mechatronics. 23. 101±110 \n[14] Zhang, Jiangquan; Yi, Sun; Liang, G. U.O.; Hongli, G. A.O.; Xin, HONG; Hongliang, \nSONG (2020). A new bearing fault diagnosis method based on modified convolutional \nneural networks. Chinese Journal of Aeronautics. 33. 439±447 \n[15] Zhang, Wei; Peng, Gaoliang; Li, Chuanhao. Bearings fault diagnosis based on \nconvolutional neural networks with 2-D representation of vibration signals as input \n(2017). MATEC web of conferences. 13001 \n[16] Simonyan, Karen; Zisserman, Andrew (2014). Very deep convolutional networks for \nlarge-scale image recognition. arXiv preprint arXiv:1409.1556 \n[17] Girshick, Ross; Donahue, Jeff; Darrell, Trevor; Malik, Jitendra. Rich feature hierarchies \nfor accurate object detection and semantic segmentation (2014). Proceedings of the IEEE \nconference on computer vision and pattern recognition. 580±587 \n \n \n137\n\nClassiﬁcation and Prediction of Bicycle-Road-Quality\nusing IMU Data\nJohannes Heidt and Klaus Dorer\nInstitute of Machine Learning and Analytics, Oﬀenburg University\njohannes.heidt@hs-offenburg.de\nklaus.dorer@hs-offenburg.de\nAbstract. The present work ties in with the problem of bicycle road assessment\nthat is currently done using expensive special measuring vehicles. Our alternative\napproach for road condition assessment is to mount a sensor device on a bicycle\nwhich sends accelerometer and gyroscope data via WiFi to a classiﬁcation server.\nThere, a prediction model determines road type and condition based on the sensor\ndata. For the classiﬁcation task, we compare diﬀerent machine learning methods\nwith each other, whereby validation accuracies of 99% can be achieved with deep\nresidual networks such as InceptionTime. The main contribution of this work with\nrespect to comparable work is that we achieve excellent accuracies on a realistic\ndataset classifying road conditions into nine distinct classes that are highly relevant\nfor practice.\nKeywords: Machine Learning, Deep Learning, InceptionTime, ResNet, Time-\nseries Classiﬁcation, Road-Quality Prediction\n1\nIntroduction\nIn November 2019 the Bundestag passed the climate package [1]. There it was decided\nthat at least 900 million Euros should be invested in the expansion and renewal of the\nGerman cycling infrastructure. This raises the question of how to obtain an overview of\nthe current state of cycle tracks.\nWhile the Federal Ministry of Transport and Digital Infrastructure (BMVI) is respon-\nsible for recording and assessing the condition of the major highways, responsibility for\ninner-city roads lies with the communes. The BMVI carries out these assessments at\nﬁxed intervals of four years, using special measuring vehicles that use laser technology\nand digital cameras1. At the communal level, it depends on individual requirements\n[2]: Expensive measuring vehicles are used as well, and often random surveys of the\nresidents are conducted on the condition of the roads. With passing the climate package\nin November 2019, this topic is likely to take on a greater role for local authorities in\nGermany in the near future [1]. The government decided to invest at least 900 million\neuros in the expansion and renewal of the cycling infrastructure.\nThe aim of this work is to develop a new approach for the condition monitoring of\ncycling tracks. For this purpose, we attach an acceleration, gyroscope and GPS sensor to\na bicycle. With the help of machine learning methods, the recorded acceleration values\nand angular velocities are used to predict the road type and condition. Here, the classiﬁer\ndiﬀerentiates between the road types asphalt, cobblestone and gravel as well as the\n1 https://www.bmvi.de/SharedDocs/DE/Artikel/StB/zustand-netzqualitaet-der-\nfahrbahnen.html\n138\n\nconditions smooth, rough and bumpy. Finally, the system sends the recorded sensor data\nto a server, which is responsible for the classiﬁcation and visualization of the results. In\nthis way, our setup determines the condition of the roads concurrently, so that it requires\nno further monitoring tasks by the cyclist.\nFor the training and testing of the machine learning algorithms, we collected the\nrequired sensor data manually. The setup consists of a smartphone and a Bosch XDK\nwhich is equipped with an accelerometer and a gyroscope. Furthermore, we developed\nan app which communicates with the XDK via Bluetooth – this enables us to annotate\nthe data. Alternatively, the XDK can persist the sensor data on its memory card. In this\ncase it can be labeled manually afterwards. In this way, we created two data sets with\ndiﬀerent sampling rates and trained, evaluated and compared diﬀerent classiﬁers. The\nmodels use common algorithms such as tree-based procedures or state-of-the-art residual\nnetworks. To test the transferability of the models, we collected a data set with another\nbicycle and used it for testing the previously trained models. Finally, we created a setup\nwhich allows the wireless transmission of the sensor data to a classiﬁcation server.\n2\nRelated Work\n[3] dealt with both the classiﬁcation of road quality and the detection of bumps. In order\nto accomplish this, the authors attached a smartphone to the steering wheel of a bicycle\nand covered a distance of almost 14 km for a total of 16 times [3][p. 40 f.]. During the rides,\nthey used the smartphone to record GPS coordinates at a rate of 1 Hz and accelerometer\ndata at 37 Hz. From this data, the authors extracted ﬁve attributes - inclination, speed,\nas well as mean, variance and standard deviation of acceleration. Finally, they combined\nthe features into segments based on the GPS coordinates and annotated them manually.\nIn this case, Hoﬀmann et al. distinguished three classes: smooth, rough and bumpy.\nFor prediction they chose the classiﬁers k-Nearest Neighbors (kNN) and Naive Bayes.\nFurthermore, Hoﬀmann et al. tested diﬀerent segment lengths (1 m, 2 m, 5 m, 10 m, 15 m,\n20 m) and feature combinations. In the end, they achieved the best results by choosing a\nsegment length of 20 m and relying only on the three acceleration features. Moreover, the\nother two features proved to be unhelpful, because the speed could not contribute to the\nprediction and the slope even confused the classiﬁers. The class distribution was not very\nbalanced with 6772 smooth, 2044 rough and 646 bumpy segments. For evaluation, the\nauthors performed a tenfold cross-validation, with the kNN classiﬁer achieving a slightly\nbetter result than Naive Bayes with a mean accuracy of 77.457% [3][p. 41 f.]. In our work,\nwe achieve signiﬁcantly higher accuracies, mainly due to the higher sampling frequency.\nLitzenberger et al. also deal with the classiﬁcation of bicycle tracks using accelerometers\n[4]. They compared diﬀerent experimental setups [4][p. 1]. They recorded all data on\nthree ﬂat and straight sections, each 100 m long. Furthermore, the authors chose three\ndiﬀerent section types: cobblestone, gravel and asphalt. These also represent the classes to\nbe predicted. For the data set generation, they drove each route at three diﬀerent speeds\n(10/20/30 km/h) and three diﬀerent tire pressures (3/4/5 bar). Consequently, each route\nwas covered nine times.\nFurthermore, the authors used two diﬀerent devices for data recording [4][p. 2]: First,\nLitzenberger et al. attached an accelerometer sensor with a sampling rate of 500 Hz to\nthe fork of a bicycle. On the other hand, the rider had placed a smartphone in his back\npocket, which recorded accelerometer data at a frequency of 100 Hz. Subsequently, they\nformed samples that encompassed time windows of diﬀerent sizes. Values of one and two\nseconds were tested here. In the next pre-processing step, they calculated 16 summary\n139\n\nstatistics for each of the four channels of the accelerometer (X, Y, and Z direction and\ntotal acceleration). These statistics are: average, maximum, and minimum values, average\npeak distance and amplitude, median FFT (Fast Fourier Transform) signal frequency,\nmaximum and minimum FFT signal frequency, maximum and minimum FFT signal\namplitude, average positive and negative slope, maximum and minimum positive slope,\nand maximum and minimum negative slope.\nFor the classiﬁcation task the authors trained and tested diﬀerent tree-based methods\nand SVMs in a ﬁvefold cross-validation. In the end, an SVM with a polynomial kernel was\nable to achieve the highest accuracy in the detection of three classes - both when using\naccelerometer data (99.2%) and smartphone data (97.7%). In another test, Litzenberger\net al. tried to additionally predict speed and tire pressure, resulting in 27 classes. Here\nan ensemble of boosting trees scored best. With the data from the accelerometer, the\nclassiﬁer scored 97.9%, whereas the smartphone data lead to an accuracy of 48.4%. In all\ntrials, the time window of two seconds produced slightly better results.\nHoﬀman et al. only relied on one track that is run a total of 16 times [3]. This is not\noptimal, because it means that similar data is used in the process of training as in the\nvalidation. Furthermore, the validation accuracy can possibly be improved. Litzenberger\net al. drove three selected routes several times as well [4]. Although they used diﬀerent tire\npressures and speeds, the selected courses are only 100 m long. Consequently, the question\narises to what extent the models are suitable for application in practice. In this work,\nwe achieve a similarly high or even higher validation accuracy compared to Litzenberger\net al. At the same time, a larger and more varied route selection was performed while\nattempting not to run the same tracks more than once.\n3\nApproach\nThe goal of this work is to ﬁnd classiﬁers, that predict road types and their condition with\nthe help of a wireless sensor network. The sensor attached to the bicycle is supposed to\ntransmit the data to a classiﬁcation server. Ideally, the model should achieve the highest\npossible prediction accuracy for unknown roads. The pursued approach is described in\nthis chapter.\n3.1\nHardware\nWith regard to the classiﬁcation task, we chose a Bosch XDK 110 which has a BMA 280\naccelerometer and a BMG 160 gyroscope. Both sensors oﬀer a sampling rate of up to\n2000 Hz. The measuring range of the accelerometer is ±16 g, that of the gyroscope ±2000\n /s. The other sensors of the XDK are not needed in this case. There is also a slot for\nmicro SD cards. Three programmable LEDs and two programmable buttons are available\nfor user interaction. Finally, the XDK oﬀers two options for wireless data transmission:\nBluetooth 4.0 Low Energy IEEE 802.15.1 and Wireless LAN IEEE 802.11b/g/n.\n3.2\nMeasurement\nFor data set generation, we mounted a plastic box modiﬁed for this application to the\nbasket of a 28-inch trekking bike (see ﬁgure 1 bottom left). Before that, we attached the\nbicycle basket to a carrier, which is located above the rear wheel (top left). The Bosch\nXDK can be hooked into the mounted device (middle picture). The lockable plastic box\nhas proven to be a reliable rain protection. Both the bicycle basket and the box device\n140\n\nare securely ﬁxed with several cable ties so that no additional bouncing occurs during the\nride. The two components only ”vibrate” together with the carrier.\nFig. 1: Setup for data recording.\nWe created a total of three data sets: Two training sets at diﬀerent sampling rates and\na ﬁnal test set. The data collection took place with as many diﬀerent routes as possible.\nAt the same time, we ensured that the data set had a relatively balanced class ratio, with\neach of the nine classes accounting for about one-ninth of the entire dataset. As a result,\nroutes of rarely occurring classes were sometimes used several times. When creating the\ndata sets, the cyclist tried to vary the driving speed. With regard to the amount of data\nto be collected.\nMoreover, we developed a labelling app to create the ﬁrst training set. It connects via\nBluetooth to the Bosch XDK, which in turn sends the recorded sensor data to the app.\nVia the app the cyclist can assign the data to one of the nine classes. Afterwards the app\npersists the sensor values on the smartphone. However, since the Bluetooth protocol limits\nthe length of a message to 20 bytes, it is not possible to achieve a very high sampling rate.\nIn this way, the system recorded 734,441 values per sensor axis (x-, y- and z-values of the\naccelerometer and the gyroscope) in a period of 376 minutes, which results in a sampling\nrate of 32.5 Hz. We drove 91.34 km with an average speed of about 14.6 km/h. In general,\nthere were no multiple runs on the same route - except for the rarely occurring classes\nof smooth and bumpy cobblestone. Furthermore, we achieved a relatively balanced class\nratio (the distances covered range from 9.2 to 11.3 km per class).\nWith regard to the second data set, we aimed to maximize the sampling rate. Therefore,\nwe omitted the Bluetooth app and stored the data directly on the SD card of the XDK.\nFor the labelling of the collected sensor data we programmed a simple user interface\nusing the buttons and LEDs of the XDK. With this approach, the XDK was able to\nrecord 5,632,000 values per sensor axis over a period of roughly 314 minutes. Compared\nto the ﬁrst data set, the total recording time is about 62 minutes shorter, but we collected\nsigniﬁcantly more data (factor approx. 7.7). This results in an approximate sampling rate\nof 294.1 Hz - more than nine times higher than in the ﬁrst approach. We sampled 76.85\nkm, with the routes largely identical to those of the ﬁrst data set. Moreover we omitted\nsome shorter tracks for practical reasons, so that we drove in total about 15 km less. The\n141\n\naverage speed was about 14,5 km/h and is very close to that of the ﬁrst data set (14.6\nkm/h). Lastly, the class ratio is relatively balanced, similar to the ﬁrst case.\nFinally, we created a last data set that is only used for testing purposes and not for\ntraining models. By using another bike to collect the sensor data, it should be checked to\nwhat extent the trained models are transferable to another bike. Therefore, we chose a\nbike from the local rental system. As shown in ﬁgure 1 (pictures on the right), we placed\nthe plastic box device in a similar way above the rear wheel. For data acquisition again\nthe variant with the memory card was our choice because it oﬀers the higher sampling\nrate. Apart from the new bike, the procedure is the same as for the creation of the\nsecond data set. In about 34 minutes, the XDK recorded 514,500 measurements per\nsensor axis, resulting in a sampling rate of approx. 252 Hz. Thus the size of the test data\nset corresponds to roughly 9.1% of the second data set. For each of the nine classes, we\nrecorded a distance of around one kilometer, totalling 9.32 km. Compared to the previous\ndata sets, the average speed of 16.4 km/h is slightly higher. However, the class ratios are\nnot quite as balanced as with the previous data sets: The number of measured values per\naxis is between 40,000 and 76,000 for the individual classes.\n3.3\nData Preprocessing\nBefore the collected data is used, several preprocessing steps are performed. To reduce\nnoise in the data, idle times are removed from the data set. These can occur when stopping\nthe bike to annotate the data with the smartphone app. Since the timestamps of all\nsensor data are known, including the GPS coordinates, the idle times can be removed in\nan automated manner.\nBefore the data is used as model input, it is normalized. The individual measurements\nof the six sensor channels are transformed so that they have a mean value of 0 and a\nstandard deviation of 1. In addition, the transformation is performed independently for\ntraining and test data. Finally, this is the model input for the deep learning-based methods.\nFor all other methods, the data are statistically summarized before normalization, whereby\nthe following metrics are calculated for the individual features of a sample: arithmetic\nmean, standard deviation, maximum and minimum value, mean width and amplitude of\nsignal peaks, mean peak distance, mean positive and negative slope as well as maximum\npositive and minimum negative slope. A total of 14 new features are calculated for each of\nthe six features (accelerometer and gyroscope each with x-, y-, and z-values). Consequently,\nthe original sample matrix with dimensions si × 6 becomes a sample vector of length\nsj = 84. Here, si is the sample size respectively the number of time steps and thus a\nconﬁgurable hyperparameter.\n4\nResults\nThis chapter presents the results of the classiﬁcation using several machine learning\nmodels that were trained and evaluated using cross-validation.\n4.1\nHyperparameter Selection\nWe used the scikit-learn library to implement the machine learning methods [5]. Only an\nimplementation for XGBoost is missing, so here the XGBoost library is used [6]. Lastly, we\nimplemented the deep-learning based methods using keras [7] and TensorFlow [8]. When\nimplementing our algorithms, we had to choose the parameters carefully. Eventually, we\n142\n\ntested diﬀerent parameter combinations for each classiﬁer in a grid search, whereby each\nparameter combination of this grid was tested using a tenfold stratiﬁed cross-validation.\nThe ﬁnal parameter selection is presented in the following chapters. It should be noted\nbeforehand that all the other standard parameters, which are not explicitly mentioned in\nthe following parameter grids, remain unchanged and can be looked up in the according\ndocumentation [5–7].\nTree Based Methods In general, tree based methods provide good results for diﬀer-\nent types of classiﬁcation problems [6][p. 785 f.]. Furthermore, their decision-making\nprocess is easy to interpret. That is why this evaluation process includes random forest\nand Extreme Gradient Boosting (XGBoost) [9, 10]. In the case of the ﬁrst data set,\nwe used the following parameter grid for the grid search: t ∈{5, 10, 25, 50, 100, 200},\nmax depth ∈{10, 25, 50}, min samples leaf ∈{1, 2, 5}, min samples split ∈{2, 5, 10}\nand n estimators ∈{2, 5, 10}. Please note that t is not a parameter of the scikit-learn or\nXGBoost models which we used. It describes the number of time steps for which we calcu-\nlated summary statistics such as mean values and standard deviations. With the following\nparameters the random forest has achieved the best result: t = 200, max depth = 50,\nmin samples leaf = 1, min samples split = 5 and n estimators = 100, resulting in a\nmean accuracy of 68.546% with a standard deviation of 3.271%. In contrast, the highest\nmean validation accuracy of the XGBoost classiﬁer is slightly better with 68.963%. At\nthe same time, the standard deviation of 1.646% is slightly lower than with the Ran-\ndom Forest. The parameter selection t = 200, max depth = 50, min samples leaf = 1,\nmin samples split = 2 and n estimators = 300 leads to this result.\nWe adjusted the ﬁrst grid slightly for the second data set, regarding its higher sampling\nrate: t ∈{50, 100, 250, 500, 1000, 2000} and n estimators ∈{100, 300, 1000}, while the\npossible values for the other three variables remain the same. The best result of the\nrandom forest is achieved with t = 2000, max depth = 25, min samples leaf = 2,\nmin samples split = 5 and n estimators = 300 - this leads to a validation accu-\nracy of 83.333%. With XGBoost the parameter values t = 2000, max depth = 25,\nmin samples leaf = 1, min samples split = 2 and n estimators = 1000 result in the\nhighest validation accuracy (86,17%).\nSupport Vector Machine The support vector machine (SVM) also proves to be\na reliable model for a wide range of classiﬁcation tasks [11]. Here we use the follow-\ning parameter grid in connection with the ﬁrst data set: t ∈{5, 10, 25, 50, 100, 200},\nkernel ∈{linear, poly, rbf, sigmoid}, C ∈{2−5, 2, 210}, gamma ∈{scale, auto} and\ndegree ∈{2, 3}. Regarding gamma, a value of auto means 1/nfeatures and scale means\n1/(nfeatures × σ2\nx). With the parameter combination t = 200, kernel = linear and C = 2,\nwe achieved a mean accuracy of 66.539% with a standard deviation of 2.377%, which is\nthe best result.\nFor training with the second data set, we adjusted only the time steps t ∈{50, 100, 250,\n500, 1000, 2000}. With the parameter selection t = 2000, kernel = linear and C = 2 we\nachieved the highest validation accuracy of 86.702%.\nDeep Learning Based Methods As deep learning approaches we tested long short-term\nmemory (LSTM) and convolutional neural networks (CNNs) with diﬀerent architectures\n[12, 13]. Hence, we tested a single LSTM layer, which is fully connected to the output\nlayer. When working with CNNs, we also adjusted the sample size and dropout. The\n143\n\nsample size speciﬁes the number of instances which we combine into one input. It can be\nconsidered as the number of time steps, but in comparison to t we calculated no summary\nstatistics. Additionally, we analyzed diﬀerent values for the number of filters, kernel size\nand pooling size. Here, we tested diﬀerent architectures: a simple CNN consisting of two\nconvolutional layers with dropout and max pooling followed by a fully connected hidden\nlayer which is again fully connected to the output layer. Next, we evaluated a CNN-LSTM\nwhich has a similar architecture like the simple CNN, we only replaced the hidden layer\nwith an LSTM layer. Lastly, we tested two residual networks from Fawaz et al.: ResNet\n[14] and InceptionTime [15]. The main part of the architecture are the residual blocks,\nwhich contain three convolutional layers and use batch normalization in between. The\nblocks themselves are also linked via residual connections. ResNet consists of three and\nInceptionTime of two blocks. Moreover, the residual blocks for InceptionTime include an\ninception module, which enables the use of multiple, concatenated ﬁlter types. In this\npaper both model architectures remain unchanged.\nFor the plain LSTM architecture the parameter grid for the ﬁrst data set looked like\nthis: sample size ∈{20, 50, 100, 150, 200}, recurrent dropout ∈{0, 1/4, 1/2} and units ∈\n{32, 64, 128}. With the parameters sample size = 50, units = 128 and recurrent dropout\n= 1/4, we achieved the highest mean validation accuracy - 65.329% with a standard\ndeviation of 1.03%. The tested CNN architecture uses the following grid: sample size ∈\n{20, 50, 100, 150, 200}, filters ∈{32, 64, 128}, dropout ∈{0, 1/4, 1/2}, kernel size ∈\n{3, 5, 7}, pool size ∈{2, 5}. With the parameters sample size = 100, dropout = 1/2,\nfilters = 32, kernel size = 5 and pool size = 5, we achieved a mean validation accuracy\nof 62.912% with a standard deviation of 1.792%. The CNN-LSTM architecture uses the\nsame grid as the CNN and achieved a slightly better result with a validation accuracy of\n70.222% at a standard deviation of 1.699%. The model uses the parameters sample size =\n200, dropout = 0, filters = 64, kernel size = 3 and pool size = 5. In the case of residual\nnets, we examined only diﬀerent values for sample size ∈{20, 50, 100, 150, 200}. With\na value of sample size = 200, the residual nets achieved the highest mean validation\naccuracy: InceptionTime scores 77.645% with a standard deviation of 1.925%, ResNet\nis just below that with 77.484%. But the standard deviation of 1.382% is slightly lower\nthan with InceptionTime.\n(a) First dataset\n(b) Second dataset\nFig. 2: Comparison of classiﬁcation results.\nIn the case of the second data set, the parameter grid for all the deep learning\nbased models remained almost the same. We only adjusted the sample size for the\nhigher sampling rate: sample size ∈{50, 100, 250, 500, 1000, 2000}. The best result of\nthe LSTM is 71.129% which uses the parameters sample size = 100, units = 32 and\nrecurrent dropout = 0. When using a CNN, we achieved the highest accuracy of 77.852%\nwith the parameters sample size = 250, filters = 128, dropout = 1/2, kernel size = 7\n144\n\nand pool size = 2. The CNN-LSTM scored a signiﬁcantly higher validation accuracy of\n91.312%. Already during the tests with the ﬁrst data set we noticed that the accuracy of\nLSTMs drops rapidly with sample size >= 150, which seems to be a common issue [16].\nTo prevent these problems, we adjusted the pooling size (pool size ∈{5, 15, 50}) compared\nto the ﬁrst parameter grid. The parameter selection sample size = 2000, filters = 32,\ndropout = 0, kernel size = 7 and pool size = 50 ﬁnally leads to the best result for this\nmodel. All in all, the residual networks provide the best results. With a sequence length\nof sample size = 2000, InceptionTime achieves 98.404%, the ResNet scores with 98.227%\nmarginally below.\nWe trained all models over 200 epochs, only the residual nets over 1000 epochs as\nit took signiﬁcantly longer for overﬁtting to occur. In all cases, we stopped the training\nprocess as soon as the training accuracy did not improve over 30 consecutive epochs.\nFurthermore, we used a local computer with 64 GB of RAM and an nVidia GeForce GTX\n1080 graphics card.\n4.2\nClassiﬁcation\nNext, we compare the presented models with each other using the hyperparameter selection\nfrom the previous chapter.\nFirst Data Set As seen in ﬁgure 2 (a), the deep-learning-based methods deliver the best\nresults - especially the two deep residiual networks ResNet and InceptionTime. While\nInceptionTime has the highest mean validation accuracy of the examined models with\n77.65%, ResNet with 77.42% has a slightly higher median than InceptionTime (77.26%).\nFurthermore, the variance in the validation accuracy of the ResNet is lower than that of\nInceptionTime because the ResNet has a lower standard deviation (1.382% vs. 1.925%)\nand a smaller interquartile range (1.129% vs. 3.226%). The top two models are followed\nby the CNN-LSTM, which has an average validation accuracy of 70.22% with a standard\ndeviation of 1.699%. In addition, the median lies at 69.84% with an interquartile range of\n2.484%. After that follow the models XGBoost, Random Forest and SVM with median\nvalues of 68.90%, 68.23% and 66.88% respectively. The interquartile ranges are 1.553%,\n2.958% and 2.605% respectively.\nDuring the tests in chapter 4.1 we observed that almost all classiﬁers achieved their\nbest results with longer sequences. Consequently, the question arises why no values t > 200\nor sample size > 200 were examined. In the end, we set this value as the upper limit for\npractical reasons: At an average speed of about 14.6 km/h and a sampling rate of 32.5\nHz, 200 time steps result in a time period of about 6.15 seconds and thus an approximate\ndistance of 25 m. In terms of application of the model in practice, a classiﬁcation of\nshorter sequences seems preferable.\nTo gain a better understanding of the strengths and weaknesses of the single classiﬁers,\nwe examined the precision and recall rates for the individual classes. It became apparent\nthat all models have problems in recognizing the classes rough asphalt, rough cobblestone\nand bumpy gravel. When also looking at the confusion matrices of these models, it is\nnoticeable that the models often confuse the problem classes with each other. The reason\nfor this could be the sampling rate: At an average speed of about 14.6 km/h, roughly 4.05\nm are covered in one second, so that the smartphone app records a sensor measurement\napproximately every 12.5 cm. However, there are cobblestones that have a shorter length\nthan this. Also in the case of asphalt and gravel, bumps can occur at shorter intervals.\nConsequently, the tests with the next data set should show what inﬂuence the sampling\nrate can have.\n145\n\n(a) Rough asphalt and rough cobblestone\n(32.5 Hz)\n(b) Rough asphalt and rough cobblestone\n(294.1 Hz)\nFig. 3: Comparison of selected sequences at diﬀerent sampling rates.\nSecond Data Set As in the test with the ﬁrst data set, the two deep residual networks\nclearly provide the best results (ﬁgure 2 (b)). Between the two, InceptionTime achieves\nslightly better results: The mean validation accuracy is 99.18% and the median is 99.11%.\nFor the ResNet, it is 98.97% and 99.11%. Again, the variance metrics are nearly equal:\nWhile the validation accuracy of InceptionTime has a standard deviation of 0.60% and\nan interquartile range of 0.98%, the corresponding values of the ResNet are 0.60% and\n0.98%. The CNN-LSTM also achieves very good results with a median of 94.14% and an\ninterquartile range of 1.87%. The SVM follows with a median of 87.77% at an interquartile\nrange of 1.54%. Finally, the tree-based methods XGBoost and Random Forest achieve a\nmedian of 86.40% and 82.74%, with an interquartile range of 1.42% and 1.60%.\nAlready during the tests with the ﬁrst data set we observed that most models beneﬁt\nfrom longer sequence lengths. This is also the case with the second data set, but again we\nlimit the maximum sequence length for practical reasons. In case of the ﬁrst data set we\nselected 200 time steps as maximum. Since the sampling rate is almost ten times higher\nfor the second data set, we set a corresponding value of t = 2000. At a sampling rate of\n294.1 Hz, an average sequence of length t = 2000 records a period of about 6.8 seconds.\nThus, at an average speed of 14.45 km/h, the cyclist covers a distance of roughly 27.3\nmeters.\nFigure 3 (a) shows some selected sequences that indicate why a correct classiﬁcation\ncan be diﬃcult with a lower sampling rate. Rough asphalt and rough cobblestone are two\nclasses that were very frequently confused with each other in the case of the ﬁrst data set.\nThe measured value is the acceleration in the x-direction of the XDK (perpendicular to\ngravity) and it is plotted over 200 time steps. It is noticeable that both time series have\nvery similar characteristics: The measurements vary quite regularly between -5 and 15\nm/s2 with just a few outliers. In addition, the number of peaks and the average period\nlengths appear to be similar. Consequently, a simple distinction of the classes based on the\ngraphs is not possible. For comparison with sequences at 294.1 Hz, we slightly adjusted\nthe x-axis in 3 (b): 1800 time steps are mapped here, which corresponds to a duration\nof about 6.12 seconds. This is about the same time period mapped in the left graphs\n(6.15 seconds). In addition, the measured values of the right-hand ﬁgures ﬂuctuate within\napproximately the same range as in ﬁgure 3 (b). Furthermore, the number of peaks is\ncomparable to the left-hand time series. Nevertheless, the higher frequency sequences\nseem to be more easily distinguishable. In the case of the ﬁrst dataset, our setup recorded\na sensor measurement on average about every 12.5 cm. With the second dataset, the\naverage speed is 14.45 km/h, so that the cyclist covers roughly 4.014 m in one second.\n146\n\nConsequently, our second setup performs a sensor data recording approximately every\n1.36 cm. This allows a much more precise ”scanning” of the road surface.\nThird Data Set As already described in chapter 3.2, we chose the same procedure\nfor data acquisition as for the second data set to record a third data set. The setup\nonly included a diﬀerent bicycle. The purpose is to check whether the previously trained\nmodels are transferable to a new bicycle.\nWhen looking closer at the recorded test data, in particular the standard deviations\nin relation to the measured values of the individual sensor axes, it is noticeable that these\nin some cases diﬀer signiﬁcantly from the training data. Compared to the second data set,\nalmost all standard deviations are about 20-40% lower. Even more signiﬁcant diﬀerences\ncan be found with the gyroscope values, which sometimes deviate by a factor of 3 to 4.\nThe reason for this is most likely the choice of a diﬀerent bicycle and the slightly diﬀerent\nmounting of the XDK support (see ﬁgure 1).\nUsing the third data set, we tested the models from chapter 4.2, which we trained\nusing the second data set. The diﬀerences between the training and test data are reﬂected\nin the classiﬁcation results of the models: ResNet achieves the highest accuracy with\n26.459%, InceptionTime reaches 26.07%, and all other models fall below the 20% mark.\nThis results conclude that the data from chapter 4.2 is not suﬃcient to easily transfer the\nmodels to any bicycle. Besides the choice of the bicycle, the mounting of the XDK holder\nand the tire pressure might also inﬂuence the recorded sensor data. To train a robust\nmodel that delivers solid results regardless of all these factors, the training data must\nrepresent these elements.\n4.3\nComparison to Related Work\nChapter 2 presented the work of Hoﬀmann et al. [3], which distinguishes three classes of\nbicycle tracks: smooth, rough and bumpy. They covered one track with a distance of 14\nkm 16 times in total and recorded accelerometer data at 37 Hz. The best classiﬁcation\nresult is a mean validation accuracy of 77.457%, determined by tenfold cross-validation.\nWith respect to the present work this is most comparable to the results of the ﬁrst data\nset, since we used a similar sampling rate with 32.5 Hz. However, in addition to the\naccelerometer data, we also used gyroscope data, which were not available in the case of\nHoﬀmann et al. On the other hand, this work distinguishes nine classes, whereas Hoﬀmann\net al. distinguish three. The data set of this work is smaller by a factor of about 2.5\nand we attempted to avoid multiple runs on the same track. Finally, the highest mean\nvalidation accuracy with respect to the ﬁrst data set is 77.645% (InceptionTime) which is\nslighlty higher than the best result of Hoﬀmann et al. (77.457%). However, when using our\nsecond data set (294 Hz), the validation accuracy of InceptionTime is signiﬁcantly higher:\n99.183%. Itt should also be noted that our training and test data include a greater variety\nof tracks compared to Hoﬀmann et al., while avoiding multiple runs of the same routes as\nfar as possible. This makes our models particularly suitable for real-world applications.\nThe second work discussed is a paper by Litzenberger et al. [4]. They selected three\nroutes of 100 m length each and have run each of the three routes nine times. They\ncombined three diﬀerent tire pressures (3, 4 and 5 bar) with three diﬀerent speeds (10, 20\nand 30 km/h). They also recorded accelerometer data, ﬁrstly with a smartphone (100\nHz) and secondly with a sensor device (500 Hz). Here the authors distinguish the classes\nof asphalt, cobblestone and gravel, with the highest mean validation accuracy at 99.2%\n(500 Hz) and 97.9% (100 Hz) respectively, determined with a ﬁve-fold cross-validation.\n147\n\nThis is comparable to the results of the second data set of this study, since the sampling\nrate of 294 Hz is almost in the middle of the sampling rates used by Litzenberger et\nal. There are also diﬀerences between the two works: The data set used in this paper is\nsigniﬁcantly larger and the tracks to be predicted are much more diverse. In contrast to\nLitzenberger et al., we additionally use gyroscope values. When predicting nine classes,\nInceptionTime achieves the highest average validation accuracy of 99.18% within the\ntenfold cross-validation, which is minimally lower than the best result of Litzenbeger et\nal. (99.2%), but on nine classes making the results more practically relevant.\nIn order to make our results more comparable to the work of Litzenberger et al.,\nwe also tested the classiﬁcation for three classes - asphalt, cobblestone and gravel. For\nthis purpose, we combined the road types of the second data set so that there is no\ndistinction between the road conditions smooth, rough and bumpy. Consequently, the three\ncombined classes of asphalt, cobblestone and gravel now comprise 1,735,500, 1,887,500\nand 2,009,000 measurements per sensor channel. Using this data, we trained the two\nresidual networks InceptionTime and ResNet using the previously determined parameter\nsettings from chapter 4.1. We performed a tenfold stratiﬁed cross-validation, whereby\nthe models trained in each run for 500 epochs and the highest validation accuracy is\nused for evaluation. In this way, the residual nets slightly exceeded the results previously\nachieved with nine classes: The mean validation accuracy of the ResNet is 99.54% with a\nstandard deviation of 0.163%. InceptionTime performs slightly better with 99.72%, only\nthe standard deviation is a bit higher with 0.213%. To sum up, our models (99.72%) have\na slighty higher prediction accuracy compared to Litzenberger et al. (99.2%) At the same\ntime, our route selection for training and testing is signiﬁcantly larger and more varied,\nso that our models should be way more suited for use in real-world applications.\n5\nConclusion and Future Work\nThis paper describes the development of an approach for condition assessment of cycling\ntracks. In total, we collected three diﬀerent data sets: Two training sets with diﬀerent\nsampling rates (32.5 Hz and 294 Hz) as well as a test set using a diﬀerent bicycle. For the\nﬁrst data set, the two residual networks achieved the highest validation accuracy of about\n77%. However, all models show diﬃculties in recognizing certain classes, and mix-ups\noften occur. A closer look at these classes led to the assumption that a higher sampling\nrate could solve these problems. With the second data set, all models were able to achieve\na signiﬁcantly higher validation accuracy with the residual nets achieving 99% accuracy.\nThe evaluation of the transferability of the models to a diﬀerent bicycle did not lead\nto good results. The recorded data of the third set diﬀers too much from the training\nvalues of the second data set, so that none of the models can make reliable predictions.\nOnce again, the two residual networks record the highest test accuracy, but they reach\nonly about 26%. Despite the poor performance of the third data set, the initial results\nsuggest that this is a very promising approach. The models developed provide higher\nvalidation accuracy than existing approaches. At the same time, we use a much larger\nand more varied selection of cycling tracks, which results in robust models that should be\nmore suitable for use in real-world applications.\nIn future work, a variety of routes, bicycles, riders, speeds and tire pressures should\nbe taken into account when creating the training data. Furthermore, the ﬁnal setup\nfor the transmission of sensor data could be adapted so that it can be used for the\nannotation of training data. For this the already developed bluetooth labelling app\nwould have to be adjusted only slightly. In this way, the user experience could be made\n148\n\nmuch more convenient compared to the previous recording with the XDK’s memory\ncard. Lastly, the ﬁnal test setup could be made more user-friendly by removing the\nsmartphone dependencies from the current architecture. At the moment, the GPS sensor\nof a smartphone is still required. To resolve this dependency, the Bosch XDK could be\nextended with a corresponding sensor via its GPIO ports. Finally, the XDK needs a\nWiﬁ-hotspot - for this purpose a mobile router could be mounted on the bicycle. With\nthese changes, the recording and classiﬁcation of the road condition could be completely\nautomated, so that the cyclist can fully concentrate on the riding itself.\nReferences\n1. EMU: Klimaschutzprogramm 2030 der Bundesregierung zur Umsetzung des Klimaschutzplans\n2050. https://www.bundesregierung.de/breg-de/themen/klimaschutz/klimaschutzprogramm-\n2030-1673578 (2019)\n2. Maerschalk, G., Krause, G., Socina, M., K¨ohler, M., St¨ockner, M.: Daten und Methoden f¨ur\nein systematisches Erhaltungsmanagement inner¨ortlicher Straßen. Forschung Straßenbau\nund Straßenverkehrstechnik (1079) (2013)\n3. Hoﬀmann, M., Mock, M., May, M.: Road-quality classiﬁcation and bump detection with\nbicycle-mounted smartphones.\nIn: Proceedings of the 3rd International Conference on\nUbiquitous Data Mining-Volume 1088, CEUR-WS. org (2013) 39–43\n4. Litzenberger, S., Christensen, T., Hofst¨atter, O., Sabo, A.: Prediction of road surface quality\nduring cycling using smartphone accelerometer data. In: Multidisciplinary Digital Publishing\nInstitute Proceedings. Volume 2. (2018) 217\n5. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,\nM., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D.,\nBrucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine learning in Python. Journal\nof Machine Learning Research 12 (2011) 2825–2830\n6. Chen, T., Guestrin, C.: Xgboost: A scalable tree boosting system. In: Proceedings of the\n22nd acm sigkdd international conference on knowledge discovery and data mining. (2016)\n785–794\n7. Chollet, F., et al.: Keras. https://keras.io (2015)\n8. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis,\nA., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia,\nY., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man´e, D., Monga, R., Moore, S.,\nMurray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker,\nP., Vanhoucke, V., Vasudevan, V., Vi´egas, F., Vinyals, O., Warden, P., Wattenberg, M.,\nWicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous\nsystems (2015) Software available from tensorﬂow.org.\n9. Breiman, L.: Random forests. Machine learning 45(1) (2001) 5–32\n10. Friedman, J.H.: Greedy function approximation: a gradient boosting machine. Annals of\nstatistics (2001) 1189–1232\n11. Cortes, C., Vapnik, V.: Support-vector networks. Machine learning 20(3) (1995) 273–297\n12. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9(8) (1997)\n1735–1780\n13. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,\nL.D.: Backpropagation applied to handwritten zip code recognition. Neural computation\n1(4) (1989) 541–551\n14. Fawaz, H.I., Forestier, G., Weber, J., Idoumghar, L., Muller, P.A.: Deep learning for time\nseries classiﬁcation: a review. Data Mining and Knowledge Discovery 33(4) (2019) 917–963\n15. Fawaz, H.I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D.F., Weber, J., Webb, G.I.,\nIdoumghar, L., Muller, P.A., Petitjean, F.: Inceptiontime: Finding alexnet for time series\nclassiﬁcation. arXiv preprint arXiv:1909.04939 (2019)\n16. Bai, S., Kolter, J.Z., Koltun, V.: An empirical evaluation of generic convolutional and\nrecurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271 (2018)\n149\n\nModeling natural convection in porous media using \nconvolutional neural networks\nMohammad Reza Hajizadeh Javaran1, Amadou-oury Bah2, Mohammad Mahdi Rajabi1, Gabriel \nFrey3, Florence Le Ber3, Marwan Fahs2\n1Civil and Environmental Engineering Faculty, Tarbiat Modares University, PO Box 14115-397, Tehran, \nIran.\n2Université de Strasbourg, CNRS, ENGEES, ITES UMR 7063, 67000 Strasbourg, France.\n3Université de Strasbourg, CNRS, ENGEES, ICube UMR 7357, 67000, Strasbourg, France\nmrhj75@gmail.com, amadou-oury.bah@etu.unistra.fr, \nmmrajabi@modares.ac.ir, g.frey@unistra.fr, \nflorence.leber@engees.unistra.fr, fahs@unistra.fr \n \nAbstract. Deep learning has become increasingly prevalent in a wide range of engineering \ncontexts. In this work, we tried to make a connection between the groundwater engineering \ncommunity and the field of deep learning. Natural convection in porous media is usually \nsimulated using common numerical modeling tools with high computational costs. In this \nwork, we aim to use supervised learning in input-output pairs (porous media characteristics-\nheat map distribution) in an image regression task, employing an encoder-decoder \nconvolutional neural network (ED-CNN) to develop a meta-model that is able to predict the \ndistribution of heat map resulting from a natural convection process in porous media or to \nestimate the characteristics of the porous domain when the heat map distribution is given. \nIn order to achieve this objective, a training data set of  samples is prepared using Comsol \nMultiphysics numerical modeling and is trained with the proposed encoder-decoder CNN. \nWe also employed several evaluation metrics such as root mean squared error (RMSE), \ncoefficient of determination (ܴଶ-score) to assess the robustness of the developed network. \nWe observed promising results in both approaches, as well as accuracy and speed, \nindicating the network's relevance in a variety of groundwater engineering applications to \ncome in the future.\nKeywords: natural convection; porous media; convolutional neural network; encoder-\ndecoder.\n1 Introduction\nNatural convection is an important concept in porous media problems [1]. It is encountered in \nseveral applications such as in heat storage in aquifers, CO2 sequestration in geological \nformations, geothermal energy extraction, and geological deposition of nuclear waste. Physics-\nbased numerical models are commonly used for simulating natural convection in porous media. \nDespite the effectiveness of these models in most cases, they encounter some critical challenges. \nOne key challenge is the computational time cost, which is more noticeable at large time and \nspace scales, especially in repetitive runs. In recent years, several meta-models, such as\npolynomial chaos expansions and feed-forward neural networks, have been proposed in order to \nreduce the simulation time of natural convection models. These meta-models have demonstrated \nacceptable performance in low-dimensional domains, but they do not scale well to high-\ndimensional problems [2]. To overcome this challenge, we propose the use of a convolutional \nneural network (CNN) architecture [3]. We apply the proposed ED–CNN in the context of \n150\n\n'image-to-image regression to (a) estimate the entire heat distribution resulting from a specified \npermeability or (b) estimate the permeability from a heat map.\n2 Methodology\nWe first develop a numerical model based upon a hypothetical square porous media example, \ngenerating heat map distribution images as training data. Each image references a unique value \nof a porous domain characteristic, known as the Rayleigh number. The generated data are then \ntrained and validated using an encoder-decoder CNN, and results are analyzed using various \nmethods.\n2.1 Example description and governing equations  \nA hypothetical, two-dimensional saturated square porous media is considered. As demonstrated \nin Fig. 1, Dirichlet temperature boundary conditions are assigned to the side walls. ܶܮand ܴܶ\nhave constant values and ܶܮ> ܴܶ. We also consider Neumann boundary conditions for the \nbottom and the upper boundaries, which emphasizes impermeable and thermally adiabatic \nconditions. The flow is assumed to be steady-state with a Newtonian and incompressible fluid \nfollowing Darcy's law. The test case is a homogeneous media, with equal hydraulic and thermal \nproperties considered as Rayleigh number (Ra). The natural convection in porous media is \nexplained by the heat transfer equation showing the energy balance, the continuity equation for \nmass balance, coupled with a variable fluid density function. The governing equations are [4]: \n߲ݑ \n߲ݔ + ߲ݒ \n߲ݕ = 0\n(1)\nݑ= െ߲௣\n߲௫\n(2)\nݒ= െ߲݌ \n߲ݕ + ܴܽ. ܶ\n(3)\nܴܽ= ݇. ߩ௖. ߚ. ݃. οܶ. ܪ \nߤ. ߙ \n(4)\nݑ߲ܶ\n߲ݔ+ ݒ߲ܶ \n߲ݕ= ߲ଶܶ \n߲ଶݔ+ ߲ଶܶ \n߲ଶݕ\n(5)\nWhere ݑ(\n௠\n௦) and ݒ(\n௠\n௦) are velocity in the ݔand ݕdirections, respectively, ݌is the pressure, and \nܶis temperature. The dimensionless Rayleigh value is defined while ݇(݉ଶ) is hydraulic \nconductivity, ߩ௖ (݇݃/݉3) is the fluid density, ߚ(1/ܭ) is the fluid thermal expansion, ݃(݉/ݏ2) is \nthe acceleration due to gravity, οܶ(K) is the temperature gradient between the left and right \nwalls (i.e., ܶܮ െ ܴܶ), ܪ (݉) is the size of the domain, ߤ(݇݃/݉. ݏ) is the fluid viscosity, and ߙ\n(݉2/ݏ) is the medium equivalent thermal diffusivity coefficient. \n151\n\nFig.1. Schematic of the problem domain\n2.2 Training Data preparation\nIn order to train the CNN models, we generated data using a COMSOL Multiphysics model, \nsolving the above-mentioned equations, which takes about 600 minutes to reach a steady state.\nUsing uniform probability distribution, sampling is done using Latin hypercube sampling, and \nindependent Rayleigh numbers are chosen on the interval [10, 210] to generate 2000 heat map \nimages. To train an image-to-image regression model, we converted the Rayleigh value numbers \nto 32×32 images using Numpy and Matplotlib packages, each representing a specific Rayleigh \nvalue pertaining to a heat map image and pixel values of images are normalized between 0 and \n1 in the preprocessing step of neural network training. All pixels of Rayleigh images have the \nsame values for each image due to homogeneity; this is because we are developing a \nmethodology, and though it might seem counterintuitive, we are using a homogeneous case as a \nfirst step. The input-output pairs are used to train an encoder-decoder CNN.\n2.3 Encoder-Decoder CNN\nWe employ an encoder-decoder architecture for this problem, consisting of two separate \nsubnetworks; encoder is a subnetwork that extracts features through a contracting process, \nfollowed by a decoder, which reconstructs the image [5],[6]. Decoder CNNs usually have the \nsame network architecture as encoders, except that they are oriented in the opposite direction \n[7]. They recover the spatial resolution lost at the encoder by deconvolution and up-sampling \nand construct output maps based on the feature maps from the encoder [8],[9]. After data \npreparation, we trained the model with a maximum number of 2,000 samples, where 50% are \nused for training, 30% for validation, and 20% for testing. We developed two ED-CNNs, one as \na meta-model and the other as an optimizer. The meta-model is trying to estimate the heatmap \ndistribution as an output while the input Rayleigh parameter images are fed to the model. \nFurthermore, a similar methodology has also been employed to develop a model that acts as an \noptimizer to estimate the Rayleigh number from the heat distribution. The ED-CNN models have \nbeen developed using Keras and Tensorflow python machine learning libraries. Fig.2 shows our \nproposed ED-CNN [2], which is constructed using convolutional layers, each of which is \nfollowed by a batch normalization layer, which regularizes the network while enhancing the \n152\n\naccuracy [10] Two times, down-sampling and rebuilding is done using two pooling layers and \ntwo upsampling layers, respectively in the middle of the network. Furthermore, the activation \nfunction is rectified linear unit (Relu), but the sigmoid function is also used in some layers, and \nthe loss function is mean squared error. The model is trained with 300 epochs using batch size \n24 and the learning rate of 0.0001 with Adam optimizer.\nFig 2. The architecture of the proposed encoder-decoder CNN\n3 Result and discussion\n3.1 CNN as meta-model\nDifferent numbers of training input-output images (including 100, 500, 1000, and 2000) \ngenerated from the numerical model are employed to train the proposed networks. Two \nevaluation criteria are used to assess the performance of the developed ED-CNN modes: (1) the \nroot mean squared error (RMSE) [11], and (2) the coefficient of determination (ܴଶ-score), a \nnumber that shows a good prediction as it gets closer to 1 [12].\nܴܯܵܧ= ඨ1\nܰ෍\n൫ݕ௣௥௘ௗെ ݕ ௧௥௨௘൯\nଶ\nே\n௜ୀଵ\n(6)\nܴଶ= 1 െ\nσ\n൫ݕ௣௥௘ௗെ ݕ ௧௥௨௘൯\nଶ\nே\n௜ୀଵ\nσ\n൫ݕ௣௥௘ௗെݕ௠௘௔௡ ௧௥௨௘ ௩௔௟௨௘௦൯\nଶ\nே\n௜ୀଵ\n(7)\nFig3.a illustrates RMSE decay with different numbers of sample sets. It is apparent from the plot \nthat training the network with about 60 epochs could be enough to reach a stable value of errors. \nIncreasing the number of samples to 2000, the RMSE converges to an acceptable value of \n0.0186. Fig3.b shows ܴଶ-score changes with the number of samples. As it is apparent from the \nplot, increasing the number of samples from 100 to 2000 samples slightly improves the accuracy, \nwhich is more than 0.97, conforming the RMSE plot results. As an example of the results, the \nperformance of ED-CNN used as the meta-model is visualized for a specific value of the \nRayleigh number in Fig4 using different numbers of sample sets to assess the effect of the \nnumber of samples. In this figure, we compare the CNN's predicted heat map with the numerical \nmodeling result, which shows a prediction with a decreasing error as we increase the samples \nfrom 100 to 2000. Using only 100 images shows a noticeable spatial error with a total error of \n153\n\n0.05, but increasing samples to 2000 decreases the total error to about 0.01. The spatial \ndistribution of the error, that is, the absolute value of the difference between CNN and numerical \nmodel predictions of temperature, is calculated pixel by pixel. In the meta-model case, we can \nsee that in the middle of the domain, errors are more prominent.\nFig 3. a) RMSE(K) and b) ܴଶ-score ED-for CNN as meta model\nNumber of \nsamples\nReal HeatMap\nPredicted HeatMap\nSpatial distribution of the \nerror\n100\n500\n1000\n154\n\n2000\nFig 4. Real and predicted heat map comparison for different number of samples for CNN as meta-model\n3.2 CNN as an optimizer\nTo evaluate the proposed network for parameter estimation, the same performance metric, \nRMSE and ܴଶ-score are employed. Fig5 a. illustrates the RMSE decay with the number of \nepochs. It can be inferred from the plot that increasing the number of samples from 100 to 500 \nsignificantly decreases the error while using more than 500 does not affect the RMSE noticeably; \nthis fact is also approved by Fig5. b, while using 500 samples instead of 100, enhances the ܴଶ-\nscore from 0.59 to a value of more than 0.98. The other assessment method is shown in Fig. 6\nas a scatter plot, comparing the predicted and actual values using a maximum training data of \n2000. As it is apparent from the plot, the majority of predicted and real cases cluster around the \n45° line, approving the network's effectiveness. Furthermore, an exemplary table of random \npredicted and test values also confirms the robustness of the network showing a deficient relative \nerror.\nFig5. a) RMSE(-) and b) ܴଶ-score for ED-CNN as optimizer\n155\n\nFig 6. Real and predicted Rayleigh value scatter plot\nTable 2. Relative error of real and predicted Rayleigh value for random test cases\nReal Rayleigh number\nPredicted Rayleigh \nnumber\nRelative error (%)\n49.11\n50.16\n2.13\n56.25\n57.30\n1.86\n60.0\n60.8\n1.33\n89.4\n89.8\n0.44\n117.5\n117.7\n0.17\n192.68\n190.16\n1.30\n4. Conclusion\nIn this paper, we have developed an encoder-decoder CNN to achieve a meta-model and \noptimizer for natural convection problems in porous media, considering the time cost and \naccuracy of the model. We initially generated 2000 heat map images; The data is then trained \nusing similar architectures as meta-model and optimizers. It is apparent from the accurate results \nthat the proposed methodology can be employed as a tool for estimating natural convection heat \ndistribution as a meta-model and estimating the properties of the porous cavity in inverse \nmodeling. It is also observed that this network is trained in about 40 minutes while the numerical \nmodeling process takes more than 600 minutes, which means it saves time, more than 93%\ncompared to numerical modeling tools, showing its robustness in solving the time cost problem. \nIn summary, this network can be used as a meta-model and optimizer and should be also useful \nfor uncertainty analysis.\n156\n\nReferences\n1.\nNield, D. A., & Bejan, A. (2017). Convection in Porous Media. Cham: Springer International \nPublishing. https://doi.org/10.1007/978-3-319-49562-0\n2.\nRajabi, M. M., Hajizadeh Javaran, M. R., Bah, A. O., Frey, G., Le Ber, F., Lehmann, F., & \nFahs, M. (2021). Analyzing the Efficiency and Robustness of Deep Convolutional Neural \nNetworks for Modeling Natural Convection in Heterogeneous Porous Media, submitted to \nInternational Journal of Heat and Mass Transfer.\n3.\nJi, X., Yan, Q., Huang, D., Wu, B., Xu, X., Zhang, A., ... & Wu, M. (2021). Filtered selective \nsearch and evenly distributed convolutional neural networks for casting defects recognition. \nJournal of Materials Processing Technology, 292, 117064.\n4.\nFajraoui, N., Fahs, M., Younes, A., & Sudret, B. (2017). Analyzing natural convection in \nporous enclosure with polynomial chaos expansions: Effect of thermal dispersion, \nanisotropic permeability and heterogeneity. International Journal of Heat and Mass \nTransfer, 115, 205-224.\n5.\nMo, S., Zabaras, N., Shi, X., & Wu, J. (2019a). Deep autoregressive neural networks for \nhighဨdimensional inverse problems in groundwater contaminant source identification Water \nResources Research, 55(5), 3856-3881.\n6.\nSelvin, S., Vinayakumar, R., Gopalakrishnan, E. A., Menon, V. K., & Soman, K. P. (2017, \nSeptember). Stock price prediction using LSTM, RNN and CNN-sliding window model. In \n2017 international conference on advances in computing, communications and informatics \n(icacci) (pp. 1643-1647). IEEE\n7.\nRonneberger, O., Fischer, P., & Brox, T. (2015, October). U-net: Convolutional networks for \nbiomedical image segmentation. In International Conference on Medical image computing \nand computer-assisted intervention (pp. 234-241). Springer, Cham.\n8.\nJiang, Z., Tahmasebi, P., & Mao, Z. (2021). Deep residual U-net convolution neural networks\nwith autoregressive strategy for fluid flow predictions in large-scale geosystems. Advances \nin Water Resources, 150, 103878.\n9.\nTahmasebi, P., Kamrava, S., Bai, T., & Sahimi, M. (2020). Machine Learning in Geo-and \nEnvironmental Sciences: From Small to Large Scale. Advances in Water Resources, 103619\n10. Zhu, Y., & Zabaras, N. (2018). Bayesian deep convolutional encoder–decoder networks for\nsurrogate modeling and uncertainty quantification. Journal of Computational Physics, \n366,415-447.\n11. Zhu, Y., Zabaras, N., Koutsourelakis, P. S., & Perdikaris, P. (2019). Physics-constrained \ndeep learning for high-dimensional surrogate modeling and uncertainty quantification \nwithout labeled data. Journal of Computational Physics, 394, 56-81.\n12. Kumar, D., Roshni, T., Singh, A., Jha, M. K., & Samui, P. (2020). Predicting groundwater \ndepth fluctuations using deep learning, extreme learning machine and Gaussian process: a \ncomparative study. Earth Science Informatics, 13(4), 1237-1250.\n157\n\nPoint Cloud Capturing and AI-based Classification for as-built \nBIM using Augmented Reality\nThomas Klauer and Bastian Plaß\ni3mainz, Institute for Spatial Information and Surveying Technology,\nMainz University of Applied Sciences\n(thomas.klauer, bastian.plass)@hs-mainz.de \nAbstract. The benefits of using Building Information Modeling (BIM) have been proven \nin architecture, engineering and construction industry. However, implementing BIM in in \nfacility management has not been achieved yet due to missing complete and accurate as-\nbuilt BIM. Modeling comprehensive information for as-built documentation from 3D point \ncloud data is referred as Scan-to-BIM but lacks automation caused by unstructured data and \nhigh user input. We tackle the main issue of structuring the 3D point cloud data by using \nartificial intelligence while capture. With both, a highly reliable and low-cost technology \nwe achieve less time-consuming point cloud capturing and segmentation contributing to a \nnovel Scan-to-BIM approach with promising initial results.\nKeywords: LiDAR; Point Cloud; Classification; Augmented Reality; Scan-to-BIM.\n1 Introduction\nSimilar to many other sectors, digitalisation is advancing rapidly in the architecture, engineering \nand construction (AEC) industry. An important component here is the concept of the \"digital \ntwin\" of a new building to be constructed or an existing building to be operated on. Building \nInformation Modelling (BIM) has been established as the method for this, in which a 3D building \nmodel represents the core element. There are various ways to create such a model: in the case of \nnew buildings to be planned, the model is generated \"form the scratch\" by the planners with \nspecialised design software, while in the case of existing buildings it is necessary to capture the \nreal building geometry with its relevant component information in reality. In addition to planning \nand construction, the management of existing buildings also benefits from digital BIM solutions, \nso that facility management will be able to exploit the advantages of BIM in the future with the \nongoing development of efficient solutions for digitising existing buildings.\nThere are various approaches to 3D as-built modelling, such as deriving from 2D CAD plans \n(as-planned BIM) or capturing the up-to-date building representation by metrological methods \n(as-built BIM). In the latter, state-of-the-art 3D point cloud data are obtained from laser scanning \nor structure from motion (SfM) methods and serve both registered and manually pre-processed \nas the information basis for modelling a semantically enriched as-built BIM (Scan-to-BIM). The \nas-built modelling process lacks automation yet due to missing, sparse, outdated and complex \ninformation about the captured objects, relationships and attributes as well as customisable uses \nof the BIM [1,3-4]. In addition, the use of professional and therefore expensive metrology \nhardware has been required to perform such scans and specialised experts are needed to carry \nout both the scanning and modelling processes. This paper will show how these processes can \nbe improved in terms of automation and simplification.\nFor an initial simplification of the scanning process, preliminary work [2] has shown that \nsufficient point cloud quality can be achieved with inexpensive consumer products such as the \n158\n\nApple iPad Pro or the Intel RealSense L515 for as-built modelling of indoor scenes. This \nhardware can be handled by non-experts after a brief instruction, resulting in a significant cost \nreduction for capturing the 3D building geometry. In order to automate and thus simplify the \nmodelling process, methods are needed that can provide semantically structured information of \nthe scanned building geometry to identify relevant, constituent objects, such as building \ncomponents, furnishings or building services elements.\nOne way to achieve this is to divide the raw and unstructured 3D point cloud into semantic \nregions by means of segmentation, in which objects are then automatically recognised [3] and \nfinally geometrically approximated by standard geometries. Structuring the 3D data into \nsemantic regions for further understanding thus represents the initial technical step in the BIM \nmodelling process (c.f. Fig. 1). Artificial Intelligence (AI) methods such as machine learning\n(ML) can be used for segmentation and classification of 3D point cloud data showing various \ncharacteristics and dealing with different conditions usually. Published research [1,4-5] confirms \nthe successful use of automated approaches for highly simplified building representations, but \nless so for the representation of complex reality.\nThis paper presents the development of an intelligent 3D data acquisition and processing method \nusing LiDAR-based consumer hardware, designed for Apple’s Pro Series such as the iPhone 12 \nPro and the iPad Pro. The prototyped 3D data application called “Semantic Data Capture” \nprovides a detailed and semantically structured point cloud using AI based on high-resolution \ndepth data acquired by Apple's vision technology without prior calibration and less technical \nknowledge. This is done by capturing a depth map with the mobile device's built-in sensors. In \ncombination with MLCore, extended by a third party ML model, captured geometries can be \nclassified simultaneously into pre-defined building component categories. The prototype also \nuses Apple's augmented reality framework (ARKit), which allows users to visualise the results \nof the captured and classified data in the overlaid AR-image of the integrated RGB-camera \nsynchronously. An application prototype has been developed that is usable also for non-experts, \nable (a) to generate detailed geometric representations of interior scenes, (b) to combine them \nwith semantic attributes in real time and (c) to deliver a structured 3D point cloud for the further \nScan-to-BIM process. As an application example, an interior analysis in the care sector was \nchosen here, which, for example, checks the suitability of living sites for certain diseases or care \nsituations. A key feature of this novel application in academia is the simultaneous geometric \ncapture and AI-based 3D data classification through a low-cost optical technology with direct \nvisualisation for non-expert users, which has the potential to establish a new state-of-the-art \nScan-to-BIM method.\n2 Requirements for Scan-to-BIM and why it lacks automation\nReferred as Scan-to-BIM, the automated reconstruction of existing buildings for BIM modelling \nis based on 3D point clouds, acquired by consolidated techniques such as laser scanning or SfM. \nBoth techniques allow a rapid and up-to-date acquisition of as-built components with high spatial \nresolution but produce a huge amount of data as a consequence, that needs to be processed in a \ntime-consuming and almost entirely manual process chain as presented in Fig. 1. The BIM model \ngeneration from acquired point cloud data can be roughly organised into the four main categories \ndata preprocessing, segmentation, classification and BIM modelling.\nRepresenting indoor scenes full covered by point clouds requires a variety of scan stations and \nviewpoints that need to be registered immediately after acquisition. The scope of the \n159\n\npreprocessed step is to remove outliers, smooth noise effects and for example downsampling or \ntransforming the point cloud data into a usable format for subsequent processes [6]. Following \nthat, the segmentation serves as the first technical step to transform the unstructured point cloud \ninto several subsets according to the semantic property of points with respect to the scene \ncharacteristic. In line with the subsequent demand, the subsets can address rooms, unique \nconstituent elements or specific regions of interest such as interiors or furnishing elements. Aside \nthe segmentation, classification becomes relevant for mapping the segmented but disordered \npoint cloud data by feature extraction and component identification into regular forms that are\ncapable for the final BIM modelling. \nFig. 1. Scan-to-BIM stages according to [6]. The presented approach summarises the first three categories \nin order to achieve automation and simplification. Consequently, it reduces the manifold\nprocess chain for as-built BIM reconstruction.\nDespite the increasing popularity of BIM for years and a quantity of technical solutions for \nautomated Scan-to-BIM, the overall process remains costly and manually. Recent research \nefforts have been made in the development of automated point cloud segmentation and \nclassification methods, mostly using ML approaches based on artificial neural networks and deep \nlearning [7-12]. However, point cloud processing is still in its infancy because of several \nchallenges described below.\nAs a consequence of evolving 3D data acquisition technologies, point clouds represent the state-\nof-the-art in surface reconstruction for engineering and 3D modeling tasks in the last decades \n[13]. Given a number of discrete points that sample a surface, point clouds are used prevalently \nbut their processing still faces many challenges due to data imperfections and a variety of special \ndata structure characteristics. The most challenging properties dealing with point clouds are the \narbitrary sampling density that results in high redundancy on the one hand but still missing, \nsparse or obsolete data on the other hand due to occlusions, clutter and noise [1,4-5]. Apart from \nthat, 3D point clouds have no inherent order such as pixel neighbours. That is why permutation \ninvariance is another main property to take care of. Furthermore, point clouds are often dense \nwith millions of single points that will provoke big data and large computational times raising \ncubicly on the number of point instances. All these properties affect the automatic processing of \npoint clouds. Nevertheless, many efforts have been made to improve the quality of automated \npoint cloud processing and to detach manual work. Even if the methods differ in approaches\naccording to [14,15], they all used to apply after acquisition in postprocessing. Considering the \nlarge amount of data, postprocessing methods require the point cloud to be patched or \nsignificantly downsampled resulting in a loss of information and additional cost. While \nperforming segmentation tasks on 2D image data using DL techniques represents state-of-the-\nart, a transfer to 3D point cloud data lacks due to above-mentioned reasons. Therefore, this paper \nproposes a new approach for point cloud segmentation in real-time during the acquisition process \nusing deep learning techniques for both data paradigms, 3D mesh classification and reprojected \n2D object classification result by a pre-trained model. Further details about our approach that is \nalso using Augmented Reality (AR) for result visualisation are given in the following chapter.\n160\n\n3 Mobile AI-based Augmented Reality Approach\nMobile devices, such as tablets and especially smartphones with integrated LiDAR sensors, are \nin principle well suited for the 3D acquisition of buildings and building components, as they are \nwidely used as personal devices and thus the users are usually familiar with their functionality \nand usability. Compared to professional acquisition hardware, they are also comparatively cheap \nto purchase and operate with and can be used also by non-experts. Just a few apps like1,2,3,4 on\nthe consumer market enable 3D scanning of indoor spaces. However, these apps are not able to \ncarry out the necessary process steps such as segmentation and classification after the pure\ngeometric acquisition. In order to avoid processing steps with a separate software after or \ndetached from the capture, the main scope of the concept presented here was to integrate this \ndirectly into the mobile application, hereinafter app. The “Semantic Data Capture” app \nintroduced here integrates the functionalities pointed out in Tab. 1.\nTab. 1. Functionality of the presented approach based on a mobile application.\nCapturing the 3D geometries of inner building structures and interiors.\nSegmentation, classification and recognition of components and furnishings.\nImmediate visualisation of the capture and recognition with corresponding usability.\nSaving of the results.\nExporting the results in relevant and open 3D formats.\n3.1 Geometry Capturing\nOn a current mobile device from Apple, geometry capturing and also parts of object recognition \nare performed with the ARKit framework [16] using the built-in LiDAR sensor. In an AR session, \nARKit stores all information that belongs to the captured environment. The core functionalities\nare tracking, i.e. the ability to follow objects relative to the position of the device, and scene \nunderstanding, i.e. the ability to collect information about the detected objects. In addition, \nARKit offers simple integration into existing visualisation libraries (e.g. SceneKit and SpriteKit) \nas well as into individual visualisation solutions with Metal, Apple's computer graphics API. \nARKit uses the built-in sensors of the mobile device such as  the namely HD camera, 6D sensor \nfor rotation, position and accelerometer as the basis for so-called Visual Inertial Odometry \n(VIO), for e.g. the determination of position and orientation supported by the RGB camera feed.\nThe captured optical data is superimposed with the other device sensors (e.g. gyroscope and \naccelerometer) and then position and movement of the device in 3D space are calculated. \nA method called raycasting is used to capture the spatial geometry, which allows 3D scenes to \nbe displayed quickly. This is done by using a virtual ray that is projected from a point on the \nscreen into the real world and allows the calculation of the intersection point with real objects\n(c.f. Fig. 2b). During the ongoing capture, ARKit then creates a virtual world of the captured \ngeometries. To ensure that the computer-generated elements remain in their real positions, so-\ncalled virtual anchors are generated. The anchors used in the app presented here contain 3D \ngeometry data that describe the objects from the environment in the form of nodes, polygons and \n1 3d Scanner App: https://apps.apple.com/de/app/3d-scanner-app/id1419913995\n2 Canvas: Pocket 3D Room Scanner: https://apps.apple.com/us/app/canvas-pocket-3d-room-scanner/id1514382369\n3 Trnio 3D Scanner: https://apps.apple.com/de/app/trnio-3d-scanner/id683053382\n4 Capture: 3D Scan Anything: https://apps.apple.com/de/app/capture-3d-scan-anything/id1444183458\n161\n\nnormals, which in turn form a polygonal mesh. In addition, semantic information can be \npredicted and assigned to each polygon from the mesh. Eight classes are currently supported\nwith ARMeshClassification [17], namely ceiling, door, floor, seat, table, wall, window and none,\nwhen ARKit cannot predict the class of the polygon. Since these eight classes are not sufficient \nto recognise interiors with various other components and furnishings, an AI-based extension has \nbeen developed.\n3.2 AI-based Mesh Classification and Object Detection\nIn order to be able to recognise objects in the virtual representation of a captured interior that are \nnot part of the ARMeshClassification classes, i.e. they have been assigned to the class none, an \nAI-based extension was designed and prototyped. Various libraries or models are available here \nthat can detect objects from (moving) images in real time. The YOLO (You Only Look Once) \nmodel was chosen in this work [18]. The approach of YOLO, in contrast to many other systems \nthat work based on Convolutional Neutral Networks (CNN), is to perform object detection in a \nsingle pass – hence “you only look once”. To make this possible, the CNN YOLO was trained \nwith data from Microsoft's Common Objects in Context (COCO) database [19]. This trained \nnetwork can now be applied to images or videos to perform multiple object detection in a fast \nmanner. The model recognises features across the entire image and creates individual bounding \nboxes that assign a class to recognised objects according to the highest probability. Images are \ndivided into a symmetric grid, where frames are suggested from each cell. Class probabilities are \nalso calculated per cell, corresponding to the number of known classes in the training dataset. \nThe class probabilities depend on the probability that an object is present in the cell.\nThe captured images are first preprocessed with Apple's vision framework. Afterwards a \ncollection of the objects found is returned in the form of observations or an empty array if no \nobjects were found by ARMeshClassification. An observation contains the class of the object \nand the normalised coordinates of the origin as well as the width and length of a frame within \nwhich the object should be located. This data is then passed to CoreML, Apple's ML framework, \nwhere it is classified in the app using YOLO, which is one of several models that can be \nintegrated into CoreML. As an example for indoor scenes, five new classes were implemented, \nnamely tvmonitor, laptop, bed, sink and toilet. The integration of YOLO was a particular \nchallenge in terms of software technology, as mesh classification with ARKit and object \ndetection with YOLO could not be processed in the same procedure. Instead, the processes were \nsplit, first the classification with ARKit and then detection with YOLO. The processes could also \nnot be parallelised in the prototype because of the permanently regeneration of the mesh \ngeometry by ARKit. That is why YOLO processes static areas after they are already classified \nas none.\n3.3 AR-based Interactive Visualisation\nTo visualise the detection process on the mobile device, the classified mesh is overlayed with \nthe real camera image, i.e. AR is created. In case of successful object detection with both the \neight ARKit standard classes and the extended five YOLO classes, the affected area of the mesh \nis coloured associated to the object class. The SceneKit framework [20] (among others) was used \nfor visualisation on the mobile device screen in real-time. SceneKit displays a 3D scene, such as\nthe virtual image created while capturing the interior spaces, on the screen. It calculates which \nelements of the generated mesh are visible from the current camera angle and displays them on \nthe screen. Since the colouring of the mesh runs parallel to the classification, the mesh generated \n162\n\nby ARKit is coloured first and then, after the scan has been completed, all grey (i.e. ARKit class \nnone) mesh parts are processed with YOLO and coloured in terms of highest class probability\naccordingly to the object detected. For this purpose, the objects detected in the camera frames \nby YOLO,as explained in the previous section are then spatially assigned and reprojected from \nthe 2D screen to all anchors in the 3D mesh by raycasting operations. The result can be seen in \nthe following Fig. 2, where an object of the class laptop is classified first as none (c.f. Fig. 2a). \nAfter that, the region is detected by YOLO, thus framed in yellow and reprojected to the \ngenerated anchors of the 3D mesh by ARKit (c.f. Fig. 2b).\n(a)\n(b)\nFig. 2. Acquisition process of a specific object that is classified first as none with ARMeshClassification \n(a) and later finalised with YOLO as laptop (b). The yellow bounding box represents the region of \ninterest for YOLO that is projected to the captured mesh surface by ray traces. The video sequence5\naccording to that figure was captured by an Apple iPad Pro in debug mode with 30 fps.\n4 Result and Outlook\nThis paper presented the mobile app “Semantic Data Capture” based on iOS for (a) capturing\n3D building sites with consumer products and (b) recognising interior structures and furnishings \nusing efficiently augmented reality (AR) and machine learning (ML) methods. This serves to \nsupport the AEC industry in general for planning purposes and facility management for e.g. \nautomated space analysis with respect to BIM. It was shown how user-centred working can also \nbe made possible for non-expert users with the help of an integrated LiDAR sensor and \naugmented reality. For this, consumer hardware from Apple was used, which is currently the\nonly manufacturer offering the functionality of active low-cost and real-time 3D scanning. With \na combination of both, in-built sensors and close software libraries using AR and ML techniques, \na new method for Scan-to-BIM was suggested and successfully prototyped. Here, Apple's own \n5 Video sequence of “Semantic Data Capture”: https://video.hs-mainz.de/Panopto/Pages/Viewer.aspx?id=6838959a-\n83af-4e36-918d-ad970123048d\n163\n\nrecognition methodology for furnishings was extended with classes from the freely available \nYOLO model. The results after the data export are shown in Fig. 3.\n(a)\n(b)\nFig. 3. Results of the app “Semantic Data Capture” (b) illustrated by CloudCompare in comparison to an \nRGB coloured point cloud captured by 3D Scanner App1 (a). The first six classes are from the ARKit \nmesh classification while the class laptop originated from the extended YOLO model.\nDespite the successful implementation of the prototype, there are several challenges that need to \nbe considered in future. First, the simultaneous mesh classification by ARKit and object \ndetection by YOLO could not yet be implemented. This is one of the improvement possibilities \nenvisaged for the immediate future. Improving this aspect, the dual data acquisition could be \navoided and working in real-time will be possible. Further additions can be made in the area of \nobject detection with ML models. For example, YOLO or other models can be used to add further \nclasses for additional components or furnishings at free will. As another main improvement, the \nacquired data could be used in combination with existing models to generate more precise ones\nindividually prepared for the situation through training. One of the core disadvantages of the \nsolution presented here is the utilisation of libraries provided by Apple that cannot be viewed \nand changed by their black box character. Considering the geometrical accuracy of the captured \nand classified objects, LiDAR could not keep up terrestrial laserscanning or SfM techniques in \nterms of reliability and precision. As a consequence, LiDAR results serves currently just for \ncoarse BIM modelling.\nNevertheless, in future, the ability of LiDAR will increase with respect to acquisition and \nregistration accuracy. By means of that, LiDAR will be among the most important acquisition \nmethods for 3D point cloud capturing even for fine geometry and BIM valid model fitting. In \ncollaboration with powerful and smart mobile devices, the possibilities to process 3D data are so \nfar not limited as the prototype shows. “Semantic Data Capture” provides a new paradigm of \nbuilding indoor acquisition and processing simultaneously in order to achieve the final scope of \nScan-to-BIM: the automated 3D modelling of buildings with appropriate accuracy by using \nmobile devices.\n5 Acknowledgements\nThe authors would like to thank D. Iordanov for the contribution to the development of the \napplication within his studies at Mainz University of Applied Sciences.\n164\n\nReferences\n1.\nLópez Iglesias, J.; Díaz Severiano, J. A.; Lizcano Amorocho, P.E.; Del Manchado Val, \nC.; Gómez-Jáuregui, V.; Fernández García, O. et al. (2020): Revision of Automation \nMethods for Scan to BIM. In: Advances in Design Engineering. Cham: Springer \nInternational Publishing (Lecture Notes in Mechanical Engineering), pp. 482–490.\n2.\nPlaß, B.; Emrich, J.; Goetz, S.; Kernstock, D.; Klauer, T. (2021): Evaluation of point \ncloud data acquisition techniques for Scan-to-BIM workflows in Healthcare. In: \nProceedings of the FIG e-Working Week 2021. Amsterdam.\n3.\nPlaß, B.; Prudhomme, C.; Ponciano, J.J. (2021): BIM ON ARTIFICIAL \nINTELLIGENCE FOR DECISION SUPPORT IN E-HEALTH. In: Int. Arch. \nPhotogramm. Remote Sens. Spatial Inf. Sci., XLIII-B2-2021, pp. 207–214. DOI: \n10.5194/isprs-archives-XLIII-B2-2021-207-2021.\n4.\nTang, P.; Huber, D.; Akinci, B.; Lipman, R.; Lytle, A. (2010): Automatic reconstruction \nof as-built building information models from laser-scanned point clouds: A review of \nrelated techniques. In: Automation in Construction 19 (7), pp. 829–843. DOI: \n10.1016/j.autcon.2010.06.007.\n5.\nVolk, R.; Stengel, J.; Schultmann, F. (2014): Building Information Modeling (BIM) for \nexisting buildings – Literature review and future needs. In: Automation in Construction \n38, p. 109–127. DOI: 10.1016/j.autcon.2013.10.023.\n6.\nLoges, S.; Blankenbach, J. (2017): As-built Dokumentation für BIM - Ableitung von \nbauteilorientierten Modellen aus Punktwolken. Photogrammetrie - Laserscanning -\noptische 3D-Messtechnik. In: Beiträge der Oldenburger 3D-Tage, pp. 290–298.\n7.\nQi, C.R.; Su, H.; Mo, K.; Guibas, L.J. (2017): PointNet: Deep Learning on Point Sets\nfor 3D Classification and Segmentation. In: IEEE Conference on Computer Vision and \nPattern Recognition (CVPR), pp. 77-85.\n8.\nQi, C.R.; Yi, L.; Su, H.; Guibas, L.J. (2017): PointNet++: Deep Hierarchical Feature \nLearning on Point Sets in a Metric Space. In: Advances in Neural Information \nProcessing Systems (NeurIPS), pp. 5105-5114.\n9.\nLi, Y.; Bu, R.; Sun, M.; Wu, W.; Di, X.; Chen, B. (2018): PointCNN: Convolution on \nx-transformed points. In: Advances in neural information processing systems, pp. 820–\n830.\n10. Hu, Q.; Yang, B.; Xie, L.; Rosa, S.; Guo, Y.; Wang, Z.; Trigoni, N.; Markham, A. \n(2020): RandLA-Net: Efficient semantic segmentation of large-scale point clouds. In:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern \nRecognition, pp. 11108–11117.\n11. Huang, Q.; Wang, W.; Neumann, U. (2018): Recurrent slice networks for 3D\nsegmentation of point clouds. In: Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, pp. 2626–2635.\n12. Phan, A.V.; Le Nguyen, M.; Nguyen, Y.L.H.; Bui, L.T. (2018): DGCNN: A \nconvolutional neural network over largescale labeled graphs. In: Neural Networks\n(108), pp. 533–543.\n13. Berger, M.; Tagliasacchi, A.; Seversky, L.; Alliez, P.; Guennebaud, G., Levine, J.; \nSharf, A.; Silva, C. (2016): A Survey of Surface Reconstruction from Point Clouds. In:\nComputer Graphics 36 (1), pp. 301-329. DOI: 10.1111/cgf.12802. \n14. Xiaoyi, R.; Baolong, L. (2020): Review of 3D Point Cloud Data Segmentation Methods. \nIn: International Journal of Advanced Network, Monitoring and Controls 5 (1), pp. 66–\n71. DOI: 10.21307/ijanmc-2020-010.\n165\n\n15. Ponciano, J.J.; Roetner, M.; Reiterer, A.; Boochs, F. (2021): Object Semantic\nSegmentation in Point Clouds – Comparison of a Deep Learning and a Knowledge-\nBased Method. In: ISPRS Int. J. Geo-Inf. 10 (4). DOI: 10.3390/ijgi10040256.\n16. Apple ARKit Documentation (2021): https://developer.apple.com/documentation/arkit,\naccessed on Sept., 1., 2021.\n17. Apple ARMeshClassification Documentation (2021): https://developer.apple.com/ \ndocumentation/arkit/armeshclassification, accessed on Sept., 1., 2021.\n18. Redmon, J; Divvala, S.; Girshick, R.; Farhadi, A. (2016): You Only Look Once: \nUnified, Real-Time Object Detection. In: Proceedings Conference on Computer Vision \nand Pattern Recognition, arXiv:1506.02640.\n19. Lin, T.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollar, P.; Zitnick, \nC. L. (2014): Microsoft COCO: Common Objects in Context. In: Computer Vision --\nECCV 2014, European Conference on Computer Vision, Springer International \nPublishing 2014, pp.740-755.\n20. Apple SceneKit Documentation (2021): https://developer.apple.com/documentation/ \nscenekit/ , accessed on Sept., 1., 2021.\n166\n\nA Reference Model for Dialog Management in\nConversational Agents in High-Engagement Use Cases\nNima Samsami, Stephan Kurpjuweit\nHochschule Worms – University of Applied Sciences\nsamsami@hs-worms.de\nkurpjuweit@hs-worms.de\nAbstract. The objective of our research is to systematically derive a reﬁned\nreference model for the dialog management component of a conversational agent.\nFirstly, we characterize high-engagement conversational agents and derive solution\nstrategies to address this class of agents. Secondly, we propose a set of conceptual\ncomponents that reﬁnes the dialog management component and addresses the so-\nlution strategies. Thirdly, we survey implementation approaches for the individual\ncomponents of the reference model.\nKeywords: Conversational Agent; Dialog Management; Natural Language Un-\nderstanding; Natural Language Generation\n1\nIntroduction\nWith the general availability of smart speakers since 2017, conversational agents have\ngained increased popularity among consumers [1]. Based on our experience, the use cases\nof conversational agents in the consumer domain can be characterized as either “task-\noriented use cases” or ”high-engagement use cases”.\nThe objective of our research is to systematically derive a reference model for the di-\nalog management component of a conversational agent from the requirements of high-\nengagement use cases. Our research is based on the following approaches: We characterize\nhigh-engagement conversational agents (section 2) and derive solution strategies to ad-\ndress this class of agents (section 3). Then we propose a set of conceptual components\nthat reﬁnes the dialog management component and addresses the solution strategies and\nsurvey implementation approaches for the individual components of the reference model\n(section 5).\n2\nQuality characteristics of high-engagement conversational\nagents\nBased on our experience, the use cases of conversational agents in the consumer domain\ncan be characterized as either “task-oriented” or ”high-engagement”: For task-oriented\nuse cases the objective is to answer the users‘ information needs or to complete a task\nin as few conversational turns as possible. Example domains include banking, customer\nservice or directory services. As the user wants to ’get a job done’, satisfying the following\ntwo quality characteristics is essential:\n(1) Relevance and (2) Focus: By nature, the bandwidth of conversational agents (i.e.\nthe amount of information that can be communicated to the user per time) is small\ncompared to other - esp. screen-based - digital channels like web or mobile apps. Thus,\n167\n\nconversational agents must deliver relevant and focused responses and reduce the number\nof conversational turns users have to take (‘get to the point‘).\nFor high-engagement use cases the objective is to keep the user engaged in the con-\nversation for as long as possible. Example domains include media, news, entertainment\nor conversational commerce. In the context of our research, the level of user engagement\nis characterized by (a) how often the user starts a session with the agent per time [2],\n(b) how much time the user spends per session [3], and (c) for how long the user is active\noverall (customer lifetime).\nWhile focus and relevance is central to all conversational agents, it may not be enough\nto ensure a high level of user engagement. Depending on the concrete nature of the agent,\nother quality characteristics should be taken into consideration, including:\n(3) Variety: Agent should provide a natural, varied language and avoid repetitive phrases\n(’don’t bore me).\n(4) Topicality: Agents should provide pieces of information that are current and new\nto a user, so that users frequently feel the need to engage with the agent (’satisfy my\ncuriosity’).\n(5) Discoverability: Agents should suggest follow-up actions that may be of interest to\nthe user (’show me what else you can do for me’).\n(6) Adaptability: Agents should be personalized and adapt to the user’s needs over time\n(’become my companion’).\n3\nSolution strategies for high-engagement conversational agents\nTo address the quality characteristics of the high-engagement conversational agents out-\nlined above, concrete solution strategies have to be implemented. The following list\ndescribes generic solution strategies which we expect to be beneﬁcial for most high-\nengagement conversational agents:\n(1) Text variation generation: To avoid repetitive phrases, text variations should be gen-\nerated (ideally automatically).\n(2) Personalized content: Personalized and current content should be selected and deliv-\nered to the user.\n(3) Education: Short messages that explain additional features and follow-up actions\nshould be delivered to the user.\n(4) Graceful error handling / disambiguation: Instead of entering error ﬂows, the agent\nshould try to understand the user’s intent, e.g. via disambiguation.\n(5) Context-awareness: The agent should adapt to the usage context. For example, the\nagent may decide to deliver a longer response of the user is driving in a car.\n(6) Modular responses: To deliver varied responses, personalized content, educational\nmessages, etc. the response should be composed of text fragments in a ﬂexible way.\n4\nConversational agent reference architecture\nFigure 1 shows a well-adapted reference architecture for conversational agents, which\ndecomposes the dialog management component into four sub-components. This architec-\nture serves as a basis for the reﬁned reference model in section 5:\n(1) Natural Language Understanding (NLU): Identiﬁes and parses a user’s text input to\nobtain semantic tags that can be understood by computers, such as entites and intents\n[4].\n(2) Conversational State (CS): Maintains the current conversation state based on the\n168\n\nconversation history. The conversation state is the cumulative meaning of the conversa-\ntion history, which is generally expressed as slot-value pairs.\n(3) Conversational Flow (CF): Outputs the next system action based on the current\nconversation state.\n(4) Natural Language Generation (NLG): Converts system actions to natural language\noutput [5].\nWe select this modular architecture over an end-to-end architecture (see [6]). End-\nto-end architectures are based on successes of deep learning approaches in recent years.\nThe system consists of a large neural network that handles all tasks such as NLU, NLG,\nCF, etc. This model is still being explored and is as yet rarely applied in the industry [6].\nAlthough the trend is toward end-to-end systems, these approaches are still limited and\ncannot clearly outperform the traditional methods [7]. In practice, it may not be feasible\nto implement a speciﬁc agent solely based on an end-to-end architecture due to a lack of\ntraining data.\nFig. 1. Modular structure of dialog system\n5\nReference model for high-engagement agents\nBased on the strategies outlined in section 3, we derive a reference model for high-\nengagement agents. The reference model is a based of the reference architecture described\nin section 4. The reference model consists of a set of conceptual software components (see\nﬁgure 2). Some components are optional and not required for all agents.\nFor the decomposition of the dialog management component we apply the following\ncriteria: (1) Each component has a manageable set of responsibilities. (2) The implemen-\ntation approaches for the components can be chosen - to a large degree - independently\nfrom each other. (3) The model can be implemented on the basis of existing conversational\nAI platforms and frameworks like RASA [8] or IBM Watson [9]. Available implementa-\ntion approaches for the components typically range from traditional (e.g., rule-based)\napproaches to more sophisticated machine learning (ML)-based approaches. In practice,\nit may not be feasible to implement a speciﬁc agent solely based on ML approaches due\nto a lack of training data or development budget. Thus, in our opinion the latter criterion\nis important to allow for a selective component-by-component migration of a traditional\nimplementation towards a ML-based implementation.\nFor each component we describe its responsibilities and survey possible implementa-\ntion approaches. Figure 2 lists the conceptual software components. Flows between the\ncomponents are omitted for clarity.\n169\n\n5.1\nComponent: Conversational Memory\nResponsibilities: One limitation of conversational agents is that they cannot go back\nand forth in a conversation. This makes natural and dynamic communication between\nhumans and computers diﬃcult. A conversation can be carried across multiple topics. To\ndo this, the agent must store what it has already talked about.\n(1) Usage History: The entire usage history of a user stored for analytics purposes.\n(2) Session State: The usage history of the current conversational session is stored with\nthe goal to determine the current conversational context, i.e. which pieces of information\na user request may refer to.\nApproaches: The literature contains descriptions of many models of conversational\nmemory. These models mainly seek to reﬂect how the human brain implements memory.\nElvir et al. also describe an Episodic Memory Architecture to address this problem.\n[10] Vinkler et al. present an architecture consisting of two memory types. A short-term\nmemory to understand the context and a long-term memory to allow the conversational\nagent to refer to previous information in the conversation [11].\n5.2\nComponent: Personalization & Context-Awareness\nResponsibilities: The personalization and context-awareness component accesses the\nusage history and calculates context information that may be required to interpret a new\nuser request and determine the response. There are multiple ﬂavours of conversational\ncontext which can be addressed by individual sub components:\n(1) Personal Preferences: The personal preferences capture which intents and entities the\nuser is especially interested in. These may be set explicitly or derived from the usage\nhistory. Personal preferences can be used to prioritize the text fragments selected for a\nuser.\n(2) Usage Context: The usage context captures aspects like the time of day, the usage\nenvironment (at home, in car, etc.), the device type (smartphone, smart speaker, etc.),\nand the interface type (chat, voice, multi model, etc.), which all may impact the response\ndelivered to the user.\n(3) Emotion Detection: The human being is an emotional being. Each person condi-\ntioned by his emotions and type expresses himself diﬀerently. To carry out a pleasant\ncommunication, it is therefore important to address the emotional intelligence aspect of\ncommunication.\nApproaches: Hao et al. present a method for using content-consistent conversation to\nalso engage in emotion-consistent communication. Emotional Chatting Machine (ECM)\naddresses this factor with three new mechanisms that respectively (1) model high-level\nabstraction of emotion expressions by embedding emotion categories, (2) capture the\nchange of implicit internal emotion states, and (3) use explicit emotion expressions with\nan external emotion vocabulary. [12]\n5.3\nComponent: Conversational Flow\nResponsibilities: Check if all pieces of information to answer the user request are avail-\nable (intent, slot values, context information) with suﬃciently high conﬁdent values and\ndecide whether to (1) answer the user request (standard path), (2) ask a disambiguation\n/ clariﬁcation question (esp. if indicated by the entity disambiguation detection com-\nponent), (3) enter an error handling ﬂow or (4) hand over the conversational ﬂow to a\nhuman (optional). Especially check if the input makes sense in the context of the current\n170\n\nconversational state (e.g., if the agent is waiting for a response to a speciﬁc question) (5)\nConversational State: The conversational state captures ”what the conversation has been\nabout” so far, so that the user can refer to entities mentioned in previous conversational\nturns and ask follow-up questions. The conversational state also determines if a speciﬁc\ntype of input is expected in the upcoming conversational turn.\n(1) Intent Ambiguity: aims to clarify the semantics of an Intent in context by ﬁnding the\nmost appropriate meaning from a predeﬁned Intent.\n(2) Entity Ambiguity: Beyond word sense disambiguation, a word can mean something\ndiﬀerent in diﬀerent contexts. E.g. Mars, Galaxy and Bounty are all delicious. It is\ndiﬃcult for an algorithm to ﬁgure out if it is talking about an astronomical structure\nor chocolate tokens.\n(3) Conversational State: Maintains the current Conversational state based on the con-\nversation history. The conversation state is the cumulative meaning of the conversa-\ntion history, which is generally expressed as slot-value pairs.\nApproaches: Decisions can be made rule-based. Decision criteria are the request data,\nthe conversational state, and the conﬁdence levels. The rules can be speciﬁed as part of\nthe language model. Jan-Gerrit Harms et al. deﬁne Dialog Management as a component of\nConversational Agents that processes the dialog context and determines the agent’s next\naction [13]. Yinpei Dai et al. kategorisieren Dialog Management in three Generations. a)\nThe ﬁrst-generation dialog systems were mainly rule-based. b) Second-generation dialog\nsystems driven by statistical data (hereinafter referred to as the statistical dialog systems)\nemerged with the rise of big data technology. At that time, reinforcement learning was\nwidely studied and applied in dialog systems. A representative example is the statistical\ndialog system based on the Partially Observable Markov Decision Process (POMDP)\nproposed by Professor Steve Young of Cambridge University in 2005 c) third-generation\ndialog systems built around deep learning have emerged. These systems still adopt the\nframework of the statistical dialog systems, but apply a neural network model in each\nmodule [6]. In general, third-generation dialog systems are better than second-generation\ndialog systems, but a large amount of tagged data is required for eﬀective training.\nTherefore, improving the cross-domain migration and scalability of the model has become\nan important area of research [6]. To solve the problems of domain dependency in end-\nto-end systems, Lu Chen et al. propose to use a multi-agent system, where the tasks are\npassed from a domain specialized agent to an agent trained on another domain [14] .\nJan-Gerrit Harms et al. show a taxonomy of the approaches for managing dialogs and a\nclassiﬁcation of a selection of tools. [13]\nAmbiguities can be determined by analyzing the entity and synonym lists of the language\nmodel. The problem can also be addressed using entity linking (EL). EL aims to resolve\nsuch ambiguities by establishing an automatic reference between an ambiguous entity\nmention/span in a context and an entity (persons, locations, organization, etc.) in a\nknowledge base. [15] Neural networks are used for this purpose as end2end systems\n[16] or in conjunction with ontologies [17] [18]. Sevgili et al. use graph embeddings as\nan eﬃcient method [15]. Mar´ıa G Buey et al. present a method that work even if the\nontology is not known at training time [19].\n5.4\nComponent: Response Generation\nResponsibilities: Decide which types of text fragments to include in the response and in\nwhich order, request the individual text fragments from the text generation components,\nbuild the response to the user by concatenating the text fragments.\n171\n\n(1) Response Assembly: Decide which types of text fragments to include in the response\nand in which order, request the individual text fragments from the text generation\ncomponents, build the response to the user by concatenating the text fragments\n(2) Text Fragment Generation: Generate the natural language response of a speciﬁc type.\nThe response types are usually speciﬁc for the intent at hand. However, there are\nresponse types that can be used across intents. Examples include: Disambiguation /\nclariﬁcation questions, error messages and educational messages (which suggest ad-\nditional features to the user).\n(3) Text Variation Generation: This module ensures that the texts vary based on the\nsituation and the course of the conversation to enable a dynamic conversation. It\navoids that always the same answers follows to the same questions.\n(4) Education: Helps the user to learn how to use the agent from agent itself and improve\nhis experience with the agent.\n(5) Personal Recommendations: Through entertainment history and usage of the agent,\nthe agent learns more about the user and can include this information in the answer.\nE.g. in the form of interesting facts.\nApproaches: Decision which types of text fragments to include can be made rule-based.\nDecision criteria are the intent and the context information (esp. the conversational\nstate). The component may query the text generation components upfront to ﬁgure out,\nif a new text fragment of a speciﬁc type is available.\nText generation can be done rule-based by ﬁlling in data from a structured data source\ninto text templates for individual sentences and concatenating the sentences. Traditional\nlanguage generation methods are based on pipelines, such as the well-known standard\nArchitecture six Component Pipeline, which was originally proposed by Reiter [20] and\nhas been further developed by others. This includes the following stand-alone compo-\nnents: (1) Content Determination [21] (2) Document Structuring [22] (3) Lexicalization\n[23] (4) Referring expression generation [24] (5) Sentence aggregation [25] (6) Linguis-\ntic realization [20] for this module exists diﬀerent ﬂavors: Hand-coded grammar-based\nsystems, Templates and Statistical Approaches [5] New approaches are based on deep\nlearning. Santhanam et al. divide these into four categories [5] (1) Language Models [26]\n(2) Encoder-Decoder Archiecture [27] (3) Memory Networks [28] (4) Transformer Models\n[29]. Lowe et al. present a system for high engaging dialog generation [30].\n6\nConclusion\nIn this contribution we propose a reference model for the dialog management component\nof a conversational agents which addresses high-engagement use cases.\nThe reference model may serve as a basis for multiple tasks, especially: system design\n(as a starting point to design both individual agents and agent creation platforms),\nsystem evaluation (as a structure to evaluate and compare agent creation platforms),\nand research (as a framework to structure future research projects and to put individual\nresearch contributions in context).\n172\n\nFig. 2. Dialog Management Refernce Model (Conceptual Sub-components)\nReferences\n1. Adamopoulou, E., Moussiades, L.: Artiﬁcial Intelligence Applications and Innovations, 16th\nIFIP WG 12.5 International Conference, AIAI 2020, Neos Marmaras, Greece, June 5–7, 2020,\nProceedings, Part II. IFIP Advances in Information and Communication Technology (2020)\n373–383\n2. Moore, R.J., Arar, R.: Conversational Ux Design: A Practitioner’s Guide to the Natural\nConversation Framework. Illustrated edition edn. ACM Books (5 2019)\n3. Mandryk, R., Hancock, M., Perry, M., Cox, A., Porcheron, M., Fischer, J.E., Reeves, S.,\nSharples, S.: Voice Interfaces in Everyday Life. Proceedings of the 2018 CHI Conference on\nHuman Factors in Computing Systems (2018) 1–12\n4. Peng, B., Li, X., Gao, J., Liu, J., Wong, K.F., Su, S.Y.: Deep Dyna-Q: Integrating Planning\nfor Task-Completion Dialogue Policy Learning. arXiv (2018)\n5. Santhanam, S., Shaikh, S.: A Survey of Natural Language Generation Techniques with a\nFocus on Dialogue Systems - Past, Present and Future Directions. arXiv (2019)\n6. Dai, Y., Yu, H., Jiang, Y., Tang, C., Li, Y., Sun, J.: A Survey on Dialog Management:\nRecent Advances and Challenges. arXiv (2020)\n7. Chernyavskiy, A., Ilvovsky, D., Nakov, P.: Transformers: ”The End of History” for NLP?\narXiv (2021)\n8. Bhattacharyya, S., Ray, S., Dey, M.: Proceedings of the Global AI Congress 2019. Advances\nin Intelligent Systems and Computing (2020) 303–318 URAI21: Context-Aware Conversa-\ntional Agent for a Closed Domain Task.\n9. IBM: Conversational chatbot reference architecture\n10. Elvir, M., Gonzalez, A.J., Walls, C., Wilder, B.: Remembering a Conversation – A Conver-\nsational Memory Architecture for Embodied Conversational Agents. Journal of Intelligent\nSystems 26(1) (2017) 1–21\n11. Vinkler, M.L., Yu, P.: Conversational Chatbots with Memory-based Question and Answer\nGeneration. PhD thesis (11 2020)\n12. Zhou, H., Huang, M., Zhang, T., Zhu, X., Liu, B.: Emotional Chatting Machine: Emotional\nConversation Generation with Internal and External Memory. arXiv (2017)\n173\n\n13. Harms, J.G., Kucherbaev, P., Bozzon, A., Houben, G.J.: Approaches for Dialog Management\nin Conversational Agents. IEEE Internet Computing 23(2) (2018) 13–22\n14. Chen, L., Chen, Z., Tan, B., Long, S., Gasic, M., Yu, K.: AgentGraph: Towards Universal\nDialogue Management with Structured Deep Reinforcement Learning. arXiv (2019)\n15. Sevgili, O., Panchenko, A., Biemann, C.: Improving Neural Entity Disambiguation with\nGraph Embeddings. Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics: Student Research Workshop (2019) 315–322\n16. Kolitsas, N., Ganea, O.E., Hofmann, T.: End-to-End Neural Entity Linking. Proceedings\nof the 22nd Conference on Computational Natural Language Learning (2018) 519–529\n17. Gracia, J., Mena, E.: Multiontology semantic disambiguation in unstructured web contexts.\nIn: Proceedings of the 2009 K-CAP Workshop on Collective Knowledge Capturing and\nRepresentation. 1–9\n18. Distante, D., Faralli, S., Rittinghaus, S., Rosso, P., Samsami, N.: DomainSenticNet: An\nOntology and a Methodology Enabling Domain-Aware Sentic Computing. Cognitive Com-\nputation (2021) 1—16\n19. Buey, M.G., Bobed, C., Gracia, J., Mena, E.: Semantic Relatedness for Keyword Disam-\nbiguation: Exploiting Diﬀerent Embeddings. arXiv (2020)\n20. Reiter, E., Dale, R.: Building Natural Language Generation Systems. (2000) 23–40\n21. Konstas, I., Lapata, M.: Unsupervised Concept-to-text Generation with Hypergraphs. In:\nProceedings of the 2012 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Montr´eal, Canada, Association\nfor Computational Linguistics (6 2012) 752–761\n22. Dimitromanolaki, A., Androutsopoulos, I.: Learning to Order Facts for Discourse Planning\nin Natural Language Generation. arXiv (2003)\n23. Gatt, A., Krahmer, E.: Survey of the State of the Art in Natural Language Generation:\nCore tasks, applications and evaluation. Journal of Artiﬁcial Intelligence Research 61 (2018)\n65–170\n24. Engonopoulos, N., Koller, A.: Generating eﬀective referring expressions using charts. Pro-\nceedings of the INLG and SIGDIAL 2014 Joint Session 162–171\n25. Barzilay, R., Lapata, M.: Aggregation via set partitioning for natural language generation.\nProceedings of the main conference on Human Language Technology Conference of the\nNorth American Chapter of the Association of Computational Linguistics - (2006) 359–366\n26. Ghosh, S., Chollet, M., Laksana, E., Morency, L.P., Scherer, S.:\nAﬀect-LM: A Neural\nLanguage Model for Customizable Aﬀective Text Generation.\nProceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n(2017) 634–642\n27. Ke, P., Guan, J., Huang, M., Zhu, X.: Generating Informative Responses with Controlled\nSentence Function. Proceedings of the 56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) (2018) 1499–1508\n28. Wang, P., Wu, Q., Shen, C., Dick, A., Hengel, A.v.d.: FVQA: Fact-Based Visual Question\nAnswering. IEEE Transactions on Pattern Analysis and Machine Intelligence 40(10) (2016)\n2413–2427\n29. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. arXiv (2018)\n30. Lowe, R., Noseworthy, M., Serban, I.V., Angelard-Gontier, N., Bengio, Y., Pineau, J.: To-\nwards an Automatic Turing Test: Learning to Evaluate Dialogue Responses. arXiv (2017)\n174\n\nVerify Embedded Systems Faster and more Efficiently with \nArtificial Intelligence\nAlexander Schwarz1, Björn Morgenthaler2, Victor Vaquero Martinez3, Miguel Garrido García4,\nManuel Duque-Antón5\n1 comlet Verteilte Systeme GmbH\nalexander.schwarz@comlet.de \n2 comlet Verteilte Systeme GmbH\nbjoern.morgenthaler@comlet.de \n3 comlet Verteilte Systeme GmbH\nvictorvaquero.etereot@gmail.com \n4 comlet Verteilte Systeme GmbH\nmiguel.garrido@comlet.de \n5 comlet Verteilte Systeme GmbH\nmanuel.duque-anton@comlet.de \nAbstract. Embedded systems are the basis for many electronic devices. As a combination \nof hardware and software designed for specific purposes, Embedded Systems ensure the \nfunctionality of Connected Cars, Autonomous Driving, Smart Farming, Industrial Internet \nof Things and Smart Homes. The enormous competitive pressure forces manufacturers to \nsignificantly shorten their time to market and thus reduces the corresponding production \ncycles. This challenge is directed to the same extent to quality assurance. Due to the \nconstantly growing number of (regression) tests, it is no longer practicable to perform all \nverifications in all development phases up to the finished product: each quality feature is \nplanned and configured individually. But this approach is usually carried out manually with \na lot of effort and is rather rarely adapted over time. On the other hand, software changes \nvery quickly: new features are added, new dependencies arise or are resolved. \nCommunication between individual components change. The probability that errors are \nfound by tests (too) late is substantially increased with each change.\nThis paper presents an approach that successfully mitigates this challenge with the help of\nsuitable Artificial Intelligence methods. To reduce lead time, a mechanism is developed \nthat reduces the number of required tests. Based on the data from previous verifications, a \n(significantly) smaller subset of tests, which is sufficient to verify the correctness of the \nchange, is selected. The remaining probability that tests necessary for negative verification \nof the software are not considered, is thereby accepted. Initial results, using data from \nseveral open-source projects as well as the use of a prototype machine learning pipeline, \nshow promising results with respect to their predictive capabilities.\nKeywords: Embedded Systems; Automated Testing; Artificial Intelligence; Reduced \nComplexity of Testing; Machine Learning; Continuous Testing; Industrial Internet of \nThings (IIoT); Agile Software Development\n1 Introduction\nAs a technology partner and solution provider, comlet connects embedded systems of both global \nmanufacturers (OEMs) and medium-sized companies into an overall system and offers intuitive\nusability by Artificial Intelligence (AI) while generating increased value. For example, \npredictive maintenance can be offered in the Industrial Internet of Things and new security risks \n175\n\ncan be detected in digital networking with the help of intelligent anomaly detection. To assure \nquality, comlet can also reduce the complexity of necessary tests with the help of AI while \nmaintaining reliability.\nThe rise of monolithic databases, agile methodologies doing fast integration and multimillion \nsource code line projects have created a practical problem in terms of computing power. What \nprevious methodologies could do with branch manual testing and nightly runs is impossible or \ntoo time consuming to do for Continuous Integration (CI) teams.\n2 Problem\nApproaches have been established that take the introduced challenge into account: one of which \nbeing Continuous Testing (CT) as the integration of automated tests into a CI pipeline (see \nFigure 1), including the requirement of detecting errors as early as possible (\"fail fast\").\nFig. 1. Test automation as quality gate for a CI pipeline\nIn a CI pipeline, software is continuously developed in separate branches and eventually merged \nwith the main development branch (trunk) once it has been completed. To assess the quality level \nbefore the individual branches are merged, various checks are performed at the so-called quality \ngates (QG). \nEach QG is adequately planned at each integration step and configured with respect to the \ntrade-off between duration, the risk of not detecting errors and feedback (see Figure 2).\n176\n\nFig. 2. Trade-Off while planning QGs\nThis process is usually carried out manually and must be adapted according to the changing \nrequirements during development.\nThe developer also receives immediate feedback by the QG whether the changes contain \nerrors and is obliged to eliminate these accordingly before integration.\nAs every step of development gets moved to the trunk, there is a higher need for quality \ncontrol before and after the integration. The problem resides here, as projects with hundreds or \nthousands of commits and thousands or more of test suites cannot bear the computing power \nneeded for running every test on every new code change – even with CT.\n3 State of the art\nTo manage the previously mentioned problems, there have been multiple solutions: Google [1],\nfor example, uses a simplification by accumulation. First, they use a bigger aggregation of tests \nnamed test targets. Secondly, they batch multiple code changes until they reach a specific \nthreshold, e.g., 100 commits. At that moment the new code version, with the complete \nrecollection of code changes, is run as a single unit on every test target (needed) that has not \nbeen run since the latest quality control. \nThis solution is based on a couple of heuristics and approximations but at its core it runs on \nsomething named change-based testing. The main property of this scheme is to only run the tests \nthat somehow (the means change between methods) depend on the code changes, and as such \nare likely to fail, to reduce the resources needed. For this, useful features like historical failure \ndata or code dependencies could be used. It can be done by any manual or automatic heuristic, \nbut any non-automatic means bear a problem of scalability.\nAs of now there have been quite a few ad-hoc attempts at resolving the change-based testing \ntask, and specifically, selecting an optimal subset of tests. Other companies commonly do it with \nspecialized automatic solutions for some business or project - sometimes in conjunction with \nmanual work [2] [3] [4]. There have also been some attempts at solving this problem [5] [6] by\nusing simple heuristics and rules like recent failures, similarity between test and code; and \nspecially code coverage. Some try to develop a set of diversified tests, with coverage measures \nor input analysis to cover the whole code change through search methods like greedy or local \nbeam search [7]. There’s also some more general development in risk prediction through specific \nfeatures like historic failure data, similarity measures, code changes [2], etc.\nTo the author’s knowledge there are only a couple of public papers, from Google [1] and \nFacebook [8], trying a similar approach of using machine learning to resolve the problem of \nselecting an optimal test subset but, as mentioned before, only for their own companies in an ad-\nhoc fashion. Another paper [9] used a reinforcement learning method only with historical failure \ninput data.\nThe tool “Sherlock” by OMICRON [10] [11] tries to blend automatic and manual test \nselection by means of code-test dependency by dynamic analysis but without taking care of the \nremaining information. \nTo the author’s knowledge up until today there is no homogenized approach that analyzes \nthe general problem for every possible available input.\n177\n\n4 Solution\nTo solve the above stated problem more universally, the development of a pipeline capable of \npredicting a minimal test subset through training on historical test failure data is approached (see \nFigure 3).\nMain objective then is to create a data model to input this learning algorithm and identify the \nminimum features required, the cost-effectiveness of the features, the most suitable algorithms \nand finally the actual predictive capability of the learning model.\nFig. 3. Overview of proposed pipeline\nThe pipeline transfers all available data into a common, unified and abstract data model, the \nUnified Data Model (UDM). This data can come from various, very different sources, such as \nissue or bug trackers, source control management, documentation, (static) code analysis and \nautomation servers.\nPrior to transferring the data from all possible sources into the UDM, preprocessing and data \npreparation is required. The raw data presents itself in very diverse formats, for example all sort \nof databases such as SQL or No-SQL or file formats like JSON or CSV. Therefore, those (raw) \ndata points are converted into a generalized data structure which is easy to work with. While this \nstructure is created, it is also assured that the different values like dates or time are converted to \ncommon formats as well. Eventually, all data points can be classified into the following five \ncategories:\nx\nInfo Point: Abstraction of any documentation about architecture, tasks, bugs, issues, \npull requests\n178\n\nx\nCode Block: Abstraction of any possible aggregation of code as it is in the current \nmoment in time. There are multiple possible types, but some would be modules, files, \nclasses, functions, etc.\nx\nCode Change: A basic data point that records every historic modification to the code \nbase and serves as a version reference.\nx\nTest: One of the basic data points comprised of the most updated information about the \ntests that are currently in use.\nx\nExecution: Record of every time a Test has been run, keeping historical data about the \nsuccesses or failures of every Test and Code Change to which every Execution is related \nto.\nThese five categories define the basis for the Unified Data Model which serves as an interface \nbetween source specific raw data and the unified feature engineering and machine learning \nalgorithms, resp. models. Thus, it completely separates feature engineering and training of \nmachine learning models from the specific (raw) data. The UDM itself provides raw and basic \nfeatures out of the box that do not require further processing. Of course, a more thorough data \nanalysis including data preprocessing and complex feature engineering based on the unified data \nis also performed to optimize the machine learning results.\nBased on the data provided by the UDM, an attempt is now being made to reduce the time \nrequired to test software together with the time a developer waits for feedback whether the \nchanges are causing test failures. This results in two different goals:\nx\nSubset selection: Given a set of Tests and Code Changes, output the subset that finds \nall the failures. The preferred method to achieve this is to analyze every test individually \nand determine a value between 0 and 1 that reflects the probability of failing. With these \nprobability values for all tests, it is possible to create an optimal subset.\nx\nTest prioritization: Given a set of Tests and Code Changes, output the optimal order of \nthe set. This optimality can be measured in several different ways, such as mean time \nor maximum wait time. The objective is to give feedback to the developer as soon as \npossible. It is also possible to use the same setup as in the subset selection algorithm \nsimply by choosing those tests with a greater risk of failure first.\nThe specific approach as an initial step towards the general solution is to identify a minimal \nsubset of tests that are very likely to fail, i.e. to fulfill the subset selection goal. No further \nprioritization of these tests will be done for the time being. For this purpose, data is collected \nfrom two large open-source projects, Pytorch [12] and Chromium [13]. This data is then merged \ninto the UDM.\nSince the UDM can manage different projects, it also ensures that non-existing features and \nmissing values in the raw data are filled with meaningful values. This is necessary so that all \nfurther steps such as feature engineering or machine learning can be performed independently of \nthe source project.\nThe UDM and the general approach of selecting a set of tests that are likely to fail are validated \nwith a simple decision tree (DCT) algorithm. The DCT was chosen because it is easy to \nunderstand and a look at the internal structure is possible at any time. This gives more insight \ninto the underlying data and features.\n179\n\n5 Results\nThe first step is defining performance metrics to compare the quality of different experiments \nobjectively. After that, multiple experiments are conducted with different sets of features. From \nthese experiments the first ones use only raw features which are directly available from the data \nand the next experiments use more complex engineered features.\n5.1 Performance / Quality metrics\nPredicting the outcome of a test is mainly a classification problem. Therefore, the data is labeled \nwith two classes: the successful tests are labeled with “success” and the failing tests with \n“failed”. Because the goal is to predict if a test fails, the failing tests are considered as positives. \nThe following performance metrics are used to evaluate the quality of the trained models and the \noverall approach.\nPrecision is a measure for false positives. This means that a successful test is predicted as \nfailed test. A low precision leads to a subset of tests with many tests that are successful.\nܲݎ݁ܿ݅ݏ݅݋݊ൌ\nܶݎݑ݁\u0003ܲ݋ݏ݅ݐ݅ݒ݁\nܶݎݑ݁\u0003ܲ݋ݏ݅ݐ݅ݒ݁൅ܨ݈ܽݏ݁\u0003ܲ݋ݏ݅ݐ݅ݒ݁ \nOn the other hand, there is the Recall which is a measure for the false negatives. False negatives \nare failing tests that were predicted as successful, so a low recall will lead to the problem that \nmany failing tests are not predicted as those.\nܴ݈݈݁ܿܽൌ\nܶݎݑ݁\u0003ܲ݋ݏ݅ݐ݅ݒ݁\nܶݎݑ݁\u0003ܲ݋ݏ݅ݐ݅ݒ݁൅ܨ݈ܽݏ݁\u0003ܰ݁݃ܽݐ݅ݒ݁ \nThe overall Accuracy is simply a measure for how many tests were correctly predicted.\nܣܿܿݑݎܽܿݕൌ\nܶݎݑ݁\u0003ܲ݋ݏ݅ݐ݅ݒ݁൅ܶݎݑ݁\u0003ܰ݁݃ܽݐ݅ݒ݁\nܶݎݑ݁\u0003ܲ݋ݏ݅ݐ݅ݒ݁൅ܶݎݑ݁\u0003ܰ݁݃ܽݐ݅ݒ݁൅ܨ݈ܽݏ݁\u0003ܲ݋ݏ݅ݐ݅ݒ݁൅ܨ݈ܽݏ݁\u0003ܰ݁݃ܽݐ݅ݒ݁ \nThe focus lies on improving the recall as much as possible without decreasing the overall \naccuracy and precision too much. This is due to the higher-level goal in predicting failing tests \nas early as possible. With a high recall fewer failing tests will be wrongly predicted.\n5.2 Raw features\nFor the first prototype, it was decided not to work with individual test cases, but on the next \nhigher hierarchical level: test suites. Test suites combine several tests that belong together \nthematically. This makes it much easier to deal with the huge amount of data in the first step. \nAnother advantage is that it is easier to create a measure of complexity. In the case of test suites, \ncomplexity is the number of associated test cases. If individual test cases were considered, such \na complexity measure could be, for example, the number of lines of code or even the number of \nfunction calls. This information is not directly available in the open-source projects considered \nand is very complicated to generate from the existing data. Another limitation chosen for the first \nprototype is to set the focus on whole commits and not on single files that changed. A commit \ncan consist of changes in multiple files.\nOn this basis, initial experiments were conducted with a small subset of the collected data to \nbetter assess the underlying data. Figure 4 shows the results of a decision tree when all directly \nusable features from the raw data are included. These are for example added, removed, changed \nlines of code, number of changed files, number of comments in an issue, number of reviews, \nnumber of tests inside a test suite, etc.\n180\n\nFig. 4. Confusion matrix and metrics using only raw features\nThese results are from a 10-fold cross validation. The overall accuracy of the decision tree is \nclose to 100%, as expected, because there is a strong imbalance in the data: there is much more \ndata from successful tests than from failing ones. But despite this imbalance, already more than \n50% of the failed tests are correctly identified as such, which is reflected by a recall of 53.99%. \nIn principle, this shows that it is possible to predict the probability that a test will fail.\n5.3 Additional and more complex features\nSince it is not sufficient to predict only half of the failed tests, the recall must be increased even \nfurther. This is achieved by encoding the unused information in the raw data so that it can be \nused to train machine learning models and by engineering entirely new features from the existing \ndata.\nAn example of such raw data that can still be usefully encoded is the build target, i.e. the \nunderlying architecture or the operating system. In the raw data this is only present as a simple \nstring containing all this information. A feature that was created completely new is the number \nof historical failures of a test; more precisely: how often a test failed in the last n runs.\nWith these new features, a recall of 66.4% is already achieved, as can be seen in Figure 5.\nCompared to the previous results, this value is already an improvement of 22.98%.\n181\n\nFig. 5. Confusion matrix and metrics with additional features\nIt is expected that with further features the recall can be increased significantly without degrading \nthe accuracy or precision. In the raw data, there is still a lot of information available, from which \nsuitable features are yet to be generated through appropriate encoding and feature engineering.\n6 Conclusion\nThe results from the experiments show that the introduced Unified Data Model works. The data \nfrom different software projects can be unified with it, which was examined at the example of \ntwo open-source projects. This makes it possible to carry out further data processing and feature \nengineering independently of the specific project. Also, the used machine learning algorithm can \nbe exchanged now without adjustments. This makes it much easier to compare the performance \nof different algorithms and to select the best suitable one. The results also show that, despite the \nstrong imbalance in the data in favor of successful tests, a recall of over 50% can be achieved \neven with a relatively small set of features. That is, over half of the tests that fail are correctly \npredicted. This recall could even be improved significantly with the first additionally engineered\nfeatures.\nThe next steps are now to validate the Unified Data Model using a third open-source project \nas well as the first results of the machine learning model with significantly larger data sets. Also, \nthe recall must be improved by further feature engineering and appropriate encoding of the still \nunused information in the raw data. Furthermore, it should also be investigated what results can \nbe achieved if the granularity is changed away from test suites back to individual test cases. \nLikewise, it could improve the recall again clearly, if the focus lies no longer on the individual \ncommits, but directly on the changed files. This would also give the opportunity to use some sort \nof code/test dependency if available as feature. It should also be possible to learn this dependency \nfrom historical test executions if it shouldn’t be available.\nReferences\n1. Atif Memon, Zebao Gao, et al. “Taming Google-Scale Continuous Testing”, IEEE/ACM \n39th International Conference on Software Engineering, 2017\n182\n\n2. Quinten David Soetens et al., “ChangeဨBased Test Selection in the Presence of Developer \nTests”, 17th European Conference on Software Maintenance and Reengineering, 2013\n3. Elmar Juergens et al., “Regression Test Selection of Manual System Tests in Practice”, \n15th European Conference on Software Maintenance and Reengineering, 2011\n4. Everton Note Narciso et al., “Test Case Selection: A Systematic Literature Review”, \nInternational Journal of Software Engineering and Knowledge Engineering, Vol. 24 No. \n04 pp. 653-676, 2014\n5. Armin Najafi et al., “Improving Test Effectiveness Using Test Executions History: An \nIndustrial Experience Report”, IEEE/ACM 41st International Conference on Software \nEngineering, 2019\n6. S. Yoo, M. Harman, “Regression Testing Minimisation, Selection and Prioritisation: A \nSurvey”, Software Testing, Verification & Reliability, ACM Digital Library, 2012\n7. Bo Jiang et al., “Input-based adaptive randomized test case prioritization: A local beam \nsearch approach”, Journal of Systems and Software, 105: 91-106, 2015\n8. Mateusz Machalica et al., “Predictive Test Selection”, IEEE/ACM 41st International \nConference on Software Engineering, 2019\n9. Helge Spieker et al., “Reinforcement Learning for Automatic Test Case Prioritization and \nSelection in Continuous Integration”, ISSTA 2017: Proceedings of the 26th ACM \nSIGSOFT International Symposium on Software Testing and Analysis, pp. 12-22, 2017\n10. Rudolf Ramler et al., “Tool Support for Change-Based Regression Testing: An Industry \nExperience Report”, International Conference on Software Quality, Springer Press, pp. \n133-152, 2016\n11. Christian Salomon et al., “Sherlock: A tool prototype for change-based regression \ntesting”, ASQT 2013 - Selected Topics to the User Conference on Software Quality, Test \nand Innovation 2013, OCG, Vol. 303 pp. 33-36 2014\n12. Adam Paszke et al., “Pytorch: An Imperative Style, High-Performance Deep Learning \nLibrary”, Advances in Neural Information Processing Systems 32, pp. 8024-8035, 2019\n13. The Chromium Authors, “Chromium” [Online] https://www.chromium.org/Home\n[Accessed 24.08.2021]\n183\n\n \n \nSuitability analysis of machine learning algorithms: Processing \nthree-dimensional spatial data for automated robot control  \n \nBenjamin Peric, Michael Engler \n \nHochschule Furtwangen University, Faculty of Business Administration and Engineering, Robert-\nGerwig-Platz 1, D-78120 Furtwangen, Germany \n \n \nbenjamin.peric@hs-furtwangen.de \n \nAbstract. Global competition, rapidly rearranging market requirements and shorter product \nlife cycles are expressed in constantly changing environmental conditions, which further \ncomplicate the demands on the production process. Given smaller batch sizes in small to \nmedium-sized companies, the importance of flexibly varying handling tasks, which must \nbe implemented through a robot gripping system, increases. Standardized workflows are \ndifficult to establish in undefined environments since the products to be handled vary \nstrongly in orientation and position.  \n The work aims to determine whether artificial intelligence can be developed through the \ncombination of a color camera including an infrared depth measurement, which enables \nindustrial robots to interact with the environment. The following two research questions \narise: 1. to what extent can the potentials of artificial intelligence and its success of the \nrecent period be adapted for the application of a robot gripping process and 2. how this \nsymbiosis effects the use of industrial applications. The combination of intelligently \ncontrolled robotics using artificial intelligence and the processing of data without server-\ndriven computing power at the end device form the basis of the investigation. The behavior \nof neural networks in scenarios with a small amount of data is the focus of the question. \nThe realization of artificial intelligence is carried out in an iterative approach and the \ndevelopment process is available in written form.  \n The overall context of the approach is questioned via a suitability analysis to gain an \nunderstanding of possible applications and to name the limits of the system in the given \nscenario. With this approach, it can be examined which factors support the use of neural \nnetworks in the outlined context and whether they can be used successfully, despite of \nadditional aggravating environmental influences. \nKeywords: Artificial Intelligence; Neural Networks; Small Data; Robotics; 3D-Data \n1\u0001 Introduction \nSelf-learning computer programs are conquering economic structures as a sustainable branch of \nindustry. The efficiency increase in all areas of a company indicates the potential of digital data \nprocessing. Machine learning algorithms can be identified as an essential driver for monitoring, \nregulation and control of industrial processes [1]. The degree of automatization is to be \nrecognized as a fundamental prerequisite for the long-term safeguarding of competitiveness in \nmanufacturing industries along with the key technology of artificial intelligence. The increase in \nflexibility, humanity, quality and productivity lies at the core of digitalization and Industry 4.0 \n184\n\n \n \nin this branch of industry [2]. Autonomously controlled robot processes, digital quality assurance \nrequirements, operational resource planning and preventive process analyses are mostly based \non intelligent sensor technology [3]. \n \nRobotic applications are becoming increasingly challenging in the field of automation due to \ngrowing demands for flexibility. These requirements increase exponentially because of smaller \nbatch sizes which creates an even more difficult starting situation in small to medium sizes \ncompanies. Common computer vision approaches could enable applications to deal with a \ndynamic scenery characterized by the opposing trend currents of automation and flexibility. \nAutomated object manipulation, motion planning and even navigation through a dynamic \nscenery are the focus of these problems. Application areas within logistics, assembly and \nproduction include these problem issues [4]. \n \nIn addition, the initial situation inhibits access to data collection and data processing. Large \ninformation structures often enable machine learning and are widely accepted as a prerequisite. \nTherefore, the impact of small data is the core of the following investigation.  \n \nLow computing power at the end device is another supplemental condition that is closely \nrooted in the initial scenario. Digital infrastructures cannot be established as a requirement in the \nmanufacturing industry of small to medium-sized enterprises. Server-driven calculations are \ntherefore not available as a solution option for answering the research questions posed [5]. \n2\u0001 Trend Research \nTrend research is a methodological tool that attempts to identify the development process based \non observations of technical and social changes. The newly acquired knowledge serves as an aid \nfor the user to name individual trends, avoid surprises, assess interactions and enable \ninterpretations through the collection of observations [6]. The scope is limited here to the analysis \nof already recognized trends and their origins. This approach aims to narrow down the relevant \ncontent of the suitability analysis by identifying the potentials and accurately define the core \nfields for the use case of the work. The high complexity inevitably requires the reduction of \nmanageable factors. \n2.1\u0001Drivers of Artificial Intelligence  \n \nThe applications of artificial intelligence have gained momentum in the last decade. The rapid \nincrease in development can be attributed to the interaction of several factors, which can be seen \nin the following figure. \n \nFig. 1. Factors influencing the development of artificial intelligence  \n \nOne of the strongest influencing factors is the massive increase in the computing power of \nintelligent circuits. Already in 1958, a regularity was derived by G. Moore through observations, \nin which the doubling of the performance can be continuously determined within a period of two \n185\n\n \n \nyears. The so-called Moore's law comprises more than 32 doubling cycles in the present period \n[7].  \n \nThe applications of machine learning algorithms benefit especially when processing large \namounts of data due to the exorbitant increase in computing power. The handling of large data \nstructures can be described with the term Big Data. At the same time, it implies the possibility \nof being able to gain target-oriented insights by skillfully processing and analyzing mass data \n[8]. Symbiosis can be derived in which both the processing task and the medium to be processed \nwill benefit from the constant growth [9].  \n \nIn a business context, providing useful information, at the right time and in a usable form is \ncritical to success. In addition to real physical data sets, so-called digital twins also serve to \ncalculate and assess real scenarios. These furthermore support the creation process of well-\nfounded data structures [10].  \n \nThis core value creation process can be derived from the investments within the technology, \nwhich have increased fiftyfold to over 15 billion in the last ten years [11]. The leadership position \nis being fought by big players such as Google, Amazon, and Facebook on an international level. \nHowever, access to data sets is not only reserved for the technology giants. The German Federal \nStatistical Office estimates a tripling of the sensor market by 2025 [12]. The digitalization of \nindustrial production and the high degree of connectivity favor the evaluation of the collected \ninformation. In the literature, terms such as \"Internet of Things\" or \"Internet of Everything\" are \ntherefore already chosen to describe the development [13]. \n2.2\u0001Alignment: Research and real-life applications \n \nThe following chapter offers a more specific insight into concrete fields of robot applications, \nwhich are optimized and implemented with the help of neural networks.  \n \nThe subject area of robotics mostly covers technical apparatuses that usually interact with the \nphysical world employing mechanical movements. Actuators are operated via kinematic \nvariables by control systems, whereby a specific task can be performed automatically due to its \nstructure [14].  \n \nOne first milestone was published in October 2020 by Knapp AG in an interview. Through \ncooperation with Covariant Embodied Intelligence Inc., the transfer of research results to a real-\nlife scenario has taken place. As an operational packaging robot, the system supports an \nelectronics distributor for wholesale within logistics. Smaller goods are automatically \ntransported to the robot arm via a conveyor belt. The gripper system recognizes the type, the lay, \nand the position of the objects. Transported via a pneumatic suction cup, each of these arrives at \nthe desired packaging location. According to the company's information, this categorizes and \nrecognizes almost 78,000 different small goods, whereby the products are initially unordered in \na carton. A short-term maximum of approximately 600 objects per hour is stated. The system \ndetects the products with an accuracy of more than 99% during operation. The gripping process \nappears very precise considering the error rate of less than one percent. Nevertheless, the \nmanufacturers state a daily working time of just 14 hours [15]. \n \nThe selected example impressively shows the different demands between research work and \nreal-life solutions. An error rate of less than one percent is to be recognized as a groundbreaking \nand remarkable performance in the context of new research results. Within one of the most well-\nknown international competitions, such as the ImageNet Challenge, a similar network could \nreplace the previous titleholders [16]. However, these results represent the minimum level of \n186\n\n \n \nacceptance in real-world use cases. The following short thought experiment illustrates the reason \nfor this fact through an overall system effectiveness calculation. \n \nWith an average operating speed of approximately 400 objects per hour, with the assumed \nerror rate of one percent, 96 objects are incorrectly detected during the day and therefore not \nprocessed. If the cause of this problem can be solved within five minutes, a machine downtime \nof approximately eight hours per day can be expected, assuming shift operation as the work \ndesign in this example. The downtime under consideration is only caused by a purely technical \nsystem error. Other recovery times, such as maintenance and servicing, occupancy, or manning \ntimes are disregarded in this consideration. Therefore, the specified working time of the \"Pick-\nit-Easy\" robot seems realistically evaluated. \n2.3\u0001Challenges of Artificial Intelligence in the manufacturing industry \n \nIn the following chapter, some decisive influencing factors are named which limit the use of a \nmachine learning solution in a corporate context. Based on this very compact presentation, the \nlimits of this technology will be critically examined and considered. \n \nFailure of drivers as amplifiers. One of the obvious challenges in implementing meaningful \nAI-based robotics solutions is the absence of the drivers in business applications.  \n \nHardware requirements. High computing capacities and memory requirements are a condition \nof practical applications. The necessity has an additional effect on the acquisition and operating \ncosts of the system, which are particularly significant when a battery is used.  \n \nPerformance standards. Whereas momentary successes of classification in worldwide \ncompetitions reach unprecedented accuracies, these results represent only the minimum of \nacceptance for real requirements. \n \nSpecialist personnel and interface disciplines. The pure basic knowledge of machine learning \nprocesses can only be implemented successfully in combination with specialist know-how. The \ngenerated results can only be validated based on a critical examination. The application of AI-\nbased systems is to be defined as a highly interdisciplinary work process, which requires the \ncombination of trained professional competencies [17].  \n \nFlexible in use - rigid in applications. Artificial intelligence can be embedded in almost any \ncore activity of a company across industries. The integrity of the technology is highly dependent \non each individual dataset used. The quality and quantity of training data ultimately determine \nthe validity of the entire system. Changing environmental conditions limit the use of neural \nnetworks enormously. As soon as these changes are not reflected in the dataset used, this harms \nthe capabilities of the whole system [18]. \n2.4\u0001Experimental approach \n \nThe simulation of a simple gripping and joining process serves as the essence of the \ninvestigation. The analysis is based on a wooden game with small geometric wooden figures. \nThe handling of unknown figures in a partly undefined environment is to be tested. The following \ninvestigation addresses common computer vision tasks, which allow object recognition. Besides \n187\n\n \n \nthe classification and detection, the localization and positioning of objects is the focus of this \nwork. Six degrees of freedom are available to the target objects, which must be defined before \ngrasping. In addition to three translational movements, the geometric bodies can rotate around \nall three spatial axes [19]. \n \nApplications within the sub-discipline of \"Transfer-Learning\" represent a variant to deal with \nvery small amounts of data. This application area uses pre-trained networks, which are connected \nbeforehand of the algorithms. The stored knowledge of the exorbitantly large networks transfers \nto the use case by interconnection and the probability of successful generalization increases in \nthe ideal case [20].  \n \nThe number of parameters in structures such as AlexNet, VGGNet, or ResNet reaches into \nthe high millions. Due to the predefined application criteria, this discipline is left out in the \nfollowing analysis. Consequently, approaches are chosen which hardly require any major \ncomputing power in operation. In addition to the redefinition of learning processes via a so-\ncalled Siamese network, compression via knowledge distillation, synthetic data generation, and \ndata augmentation is the focus of the experiments. \n \n \nA lightweight and very compact time of flight design with stereovision is offered by the \ninfrared depth camera \"Intel Realsense D435\". The manufacturer of the product offers a \ncomprehensive development platform compatible with free programming libraries and includes \nthe most widely used programming languages [21]. Therefore, this depth sensor technology is \nthe basis of the following work to create three-dimensional datasets and perception of the \nenvironment. \n3\u0001 One-Shot-Learning: Rotation determination of unknown objects \nGrasping objects are often located in a two-dimensional disordered initial situation. The \nlocalization of the target objects is only one part of the necessary scene determination. The \nrotation of an object cannot be determined with the help of the image segmentation illustrated \nabove. If the components lie on a surface, it is necessary to extract the position and rotation of \nthe objects, based on which the gripping and joining process can be derived.  \n \nIt is assumed that the position of the target objects is already clearly determined. \nConsequently, the grasping objects are perceived in the zenith from the top view. Now the \nrotation of the object figures on the image plane must be determined. The rotation symmetry \nproperties of the objects specify the maximum rotation on the image plane. \n \nSynthetic data preparation. A single image is captured of each target object. Using image \nprocessing techniques, the subsequently visualized test figures are aligned and rotated with a \nrotation step size of half a degree. Consequently, each class receives a single original image to \ntrain. As a test set, five new images are taken of each object, which is augmented using the same \nmethodology. The trainingset of the objects can be seen in the figure below. \n \nFig. 2. Rotation determination of three geometric figures \n188\n\n \n \nProblem definition. The rotation is to be determined via a regression. Trigonometric relations \nrepresent periodic processes as mathematical elementary functions. Therefore, the use of a \ndirection vector allows a regression with the support of oscillation functions. All rotationally \nsymmetrical properties can be mapped via compression and extension of the oscillation \nfunctions. Here, the determination of the direction vectors is the focus of the neural regression. \nThe neural network outputs the real vector elements in this approach. \n \nCNN for rotation determination. Two convolutional layers with a kernel of size 3 x 3 scan the \ncharacteristics of the figures in 6 and 12 feature maps using relu activation functions. Via a dense \nlayer, the data reach the output, which contains two neurons. The tangent hyperbolic function \nhas the same range of values as the sine and cosine, so the network can determine the real vector \nelements using the mean squared error. \nTable 1. Comparison: Vector and unit vector regression for rotation determination \nApproach \n\u0001 \u0002\u0004\u0003\u0007\u0001\n\u0001 Standard deviation \n\u0001 Maximum \nIntolerance < 2.5° \nVector output \n4.28° \n2.49° \n21.46° \n54.91% \nUnit vector output \n1.25° \n0.95° \n5.63° \n90.65% \n \nThe validation shows that the regressive determination of the output over a simple vector fails. \nOnly 55% of the test data fall within the set tolerance. The regression is therefore extended by \nthe determination of the unit vector. The unit vector amount must always have the length of one. \nAs soon as this condition is included in the output layer, it improves the rotation determination \nof the three target objects enormously, as shown in the table above. \n \n4\u0001 Handling small data sets: Classification of unknown objects \nThe small geometric figures are to be classified and recognized based on their shapes. This \nenables the robot system to deal with unknown scenery. \n \nData acquisition. Four small target figures are measured as a test by the depth camera and stored \nin a data set. In total, this comprises 10,000 images per class. The captured images have a size \nof 100 x 100 pixels. The subsequent neural networks each receive a greatly reduced data number \nof 500 images per class to approximate the problem. The remaining data points are available to \nthe test as validation. The data set represents the point clouds of the target objects from all \nviewing directions and varies strongly in the distance to the target objects. \n \nThe problem of small data. The phenomenon of overfitting occurs especially with small data \nsets, where the networks over-specify on the existing data points. Overfitting massively \ncounteracts the primary goal of generalization, which is why neural networks can respond poorly \nto new input in this case [22]. \n \nAugmentation. Synthetic augmentation provides an effective method for extracting meaningful \ninformation structures despite having few representative data points [23].  \n \nThe operations to augment the set of data points can be implemented using an integrated \ndevelopment tool called \"Keras-Experimental\" within the TensorFlow programming library and \nprepended to the architecture of the network.  \n189\n\nThe following methods are randomly selected and executed for each input. \n•\nZooming in and out of the original image up to a maximum of 20% of the existing\nimage dimension\n•\nMirroring around the horizontal and vertical axis\n•\nAverage intensity contrast change by a maximum of 10% of the original image\n•\nShifting the image by a maximum of 10% in the horizontal or vertical direction\nStructure of Siamese Neural Network. The Siamese Neural Network (SNN) is an architecture \nthat enables the handling of very small amounts of data. It constantly receives temporally \nstaggered impulses, which are to be linked together. The two input images are processed in \nparallel by the same convolutional neural network (CNN) architecture [24]. The SNN receives \ninput pairs of data images with a size of 100 x 100 pixels. Two convolutional layers including \nmax-pooling, alongside 24 and 48 feature maps respectively, process the data each with a kernel \nof 3 x 3 weights. Using a dropout of 50%, the data is transformed into the dimension of a vector \nwith 50 elements as a result of a dense layer. The parallel processing of the input images enables \nthe SNN to compare the vectors over the Euclidean distance of the intermediate output \u0012\u0018 and \n\u0012\u0019 , which represent the similarity of the input pair in an abstract form. This scalar value is \ncalculated using a sigmoid activation function. \n\u0017 \u0012\u0018\t \u0012\u0019 * \u0017 \u0012\u0018 ) \u0012\u0019\n* \r\u000b+\r ( \u0010\u001a %&\u001a%' , \n(1) \nThe more similar the input images are, the smaller is the Euclidean distance of the vectors and \nthe output approaches zero. Hereby the similarity of the three-dimensional shapes of the objects \ncan be measured. A simple comparison image set of the figures to be compared is sufficient for \nclassification since the closest match of the unknown input to the comparison set can be used as \na discriminator.  \n \nTo validate this architecture, almost identical CNNs are used, which map the number of \nclasses to be distinguished in the output layer. With the help of a softmax activation function, \nthe class probability distributions of the target object can be directly specified. The SNN uses \nthe binary cross-entropy due to the binary similarity output structure, whereas the simple CNNs \nuse the pure cross-entropy loss.  \nTable 2. Comparison: CNN with augmentation vs. Siamese Neural Network \nNetwork \nEpochs \nTrain: 500 images \nTest: 9500 images \nCNN without augmentation \n100 \n99.17 % \n66.24 % \nCNN with augmentation \n1000 \n91.17 % \n81.87 % \nSNN with augmentation \n500 \n99.01 % \n90.21 % \nThe very small datasets cannot be successfully distinguished without augmentation. The \nclassification task of only four targets cannot be performed by simple CNNs for this small \ndataset. The augmentation methods allow the approximation of the problem, which can \ncounteract the phenomenon of overfitting. The SNN outperforms the results by more than 10%, \ndespite a nearly equal architecture of the model. However, a tenfold calculation time of an epoch \nmust be accepted by doubling the input data. The validation of the SNNs is repeatedly done by \npairing the test data. The achieved performance of more than 90% is to be considered as very \ngood in this context since the use of several comparison images per class can additionally raise \nthe result.  \n190\n\n \n \n5\u0001 U-Net Compression with Knowledge Distillation: Image Segmentation \nThe three-dimensional scenery only acquires a complete meaning through the understanding \nwithin the pixel level. In addition to the classification, the object recognition and extrapolation \nof the object surfaces is therefore the focus of the next task. The recognition of a reference surface \nenables the robot system to form a normal, which can be used to determine gripping points. \nObject recognition is one of the central issues of a gripping process, in addition to grip \nevaluation, behavior coordination, and the determination of minimum holding forces [25]. \n \nData acquisition. The question reflects a binary image segmentation of the point clouds. The \ngeneration of the target masks is typically done by hand, which is why the data preparation itself \ntakes a very time-consuming process. However, the Intel-Realsense sensor technology offers \nsome synergy effects. The recorded point clouds can be overlaid with a color image, whereby \nboth data streams reproduce the same scenery. With the help of a simple color filter, the color \nchannels can be used to distinguish the body surfaces. The top of the searched target figure is cut \nout from the rest of the environment in the color stream, allowing the corresponding mask to be \nformed to the depth data. The captured scene contains eight different geometric figures. The \nobjects are set in motion on a tabletop, creating a dynamic environment. Different viewing \ndirections and distances are again included in the dataset. The point clouds and the mask images \nassume a dimension of 200 x 200 pixels and contain 7,500 data points each. The test-training \nsplit assumes the ratio of 30 - 70%. \n \nFig. 3. U-NET Architecture for binary image segmentation  \n \nU-Net architecture. The original U-Net structure is used for binary image segmentation and \nreduced in size for the present application as shown in the figure above [26]. Via convolutional \nlayers, the input data is compressed and brought back to the original size. During deconvolution, \nan additional dropout of 10% is integrated after each max-pooling layer, which is not visible in \nthe figure above. \n \nKnowledge Distillation. Considering a binary pixel classification, a sigmoid activation function \nis present in the output layer. The pure U-Net architecture outputs the pixel-probability mask \u0001\u0011, \nwhich is compared to the target masks \u0014 via the binary cross-entropy. \n \n\u000f\u001b\u001c\u001e \u0011\t \u0014 * )\n-\u0001\u0014!\" \u0006\b\u0005 \u0011!\" ( \r ) \u0011!\" \u000e \u0006\b\u0005 \r ) \u0011!\" \u0001.\n+!\t\",\n \n \n(2) \nThe architecture is trained using the existing data sets and their target masks. This Teacher \nNetwork is then used to develop a compressed architecture using Knowledge Distillation. The \ncompressed U-Net structure receives as knowledge distillation loss the pure binary cross-entropy \nin combination with the Kullback-Leibler divergence [27]. \n191\n\n \n \n\u000f\u001f\u001d \u0011#\t \u0011$\t \u0014 * \u0015\u000f\u001b\u001c\u001e \u0014$\t \u0011 ( \r ) \u0015 \u000e \u000f\u001f \u0011$\n\u000e \u0011#\n\u000e  \n \n(3) \nThe products of the teacher and student architecture are aligned via the Kullback-Leibler \ndivergence. However, even softer probability distributions \u0011\u000e of the student and teacher outputs \nare used. The divergence considers the temperature \u0016 during the final computation with the \nactivation function \u0011\u000e * \u0001\u0017 \u0013\u000b\u0016 . The binary cross-entropy falls with the value \u0015 * \f\n\r to 10% \nin the combined loss of the student. The temperature \u0016 receives the value 3. Hereby, the student \nnot only receives the hard mask targets but also the corresponding soft probability distributions \nof the larger teacher network to approximate the problem.  \nTable 3. Comparison: Compression of U-Net Architectures with Knowledge Distillation \nU-Net Network \nParameter \nAccuracy  \nLoss \nIoU of the target class \nTeacher \n80,485 \n99.02 % \n0.0257 \n85.72 % \nStudent from scratch \n7,753 \n97.63 % \n0.0592 \n67.54 % \nStudent with KD  \n7,753 \n98.74 % \n0.0323 \n81.27 % \n \nGiven the number of parameters that can be trained, the student U-Net architecture represents a \ntenfold reduction of the teacher, while the basic architecture remains the same. Accuracy does \nnot correctly represent the binary image segmentation problem. Therefore, the Intersection over \nUnion (IoU) is used for validation. The direct comparison of the student architectures with and \nwithout knowledge distillation shows optimization of the IoU of 14 percentage points. The \ntenfold compression therefore takes place with a drop of less than 4 percentage points. \n \n6\u0001 Conclusion  \nThe development of neural networks on the end device extremely complicates the development \nof neural networks. The experiments address extreme cases of an application under the chosen \nconditions. It was shown that some approaches facilitate the use of small data. Considering real \napplication scenarios, however, the gap between the research results and real requirements \nbecomes strongly apparent. \n \n7\u0001 References \n1.\u0001 BMWi: Strategie Künstliche Intelligenz der Bundesregierung. pp. 15-20. BMWi \nPublikation Schlüsseltechnologien, Berlin (2018).  \n2.\u0001 Jeske T., Lennings F.: Produktivitätsmanagement 4.0: Praxiserprobte Vorgehensweisen \nzur Nutzung der Digitalisierung in der Industrie. pp. 15-18. Springer Vieweg, Berlin \n(2021).  \n3.\u0001 Seifert I. et al.: Potentiale der Künstlichen Intelligenz im produzierenden Gewerbe in \nDeutschland. pp. 12-15. Institut für Innovation und Technik in der VDI/ VDE Innovation \n+ Technik GmbH, Berlin (2018). \n4.\u0001 Wittpahl V.: iit – Themenband: Künstliche Intelligenz Technologie | Anwendung | \nGesellschaft. pp. 110-112. Springer Vieweg, Berlin (2019). \n5.\u0001 Arnold N. und Wangermann T.: Digitalisierung und Künstliche Intelligenz: \nOrientierungspunkte. pp. 7. Konrad-Adenauer-Stiftung, Berlin (2018).   \n192\n\n \n \n6.\u0001 Blechschmidt J.: Quick Guide Trendmanagement: Wie Sie Trendwissen in Ihrem \nUnternehmen wirksam nutzen. pp. 13. Springer Gabler-Verlag, Berlin (2020).  \n7.\u0001 Kreutzer R., Sirrenberg M.: Künstliche Intelligenz verstehen: Grundlagen – Use Cases – \nunternehmenseigene KI-Journey. pp. 74. Springer Fachmedien, Wiesbaden (2019). \n8.\u0001 Wagener A.: Künstliche Intelligenz im Marketing – ein Crashkurs. pp. 38-43. Haufe-\nLexware GmbH & Co. KG, Freiburg (2019) \n9.\u0001 Scheier C., Held D.: Künstliche Intelligenz in der Markenführung: Der effiziente Weg \nden Erfolg von Marken zu steuern. pp. 33. Haufe-Lexware GmbH & Co. KG, Freiburg \n(2019). \n10.\u0001Pistorius J.: Industrie 4.0 – Schlüsseltechnologien für die Produktion: Grundlagen \nPotentiale Anwendungen. pp. 10 and 41-46. Springer Verlag, Berlin (2020).  \n11.\u0001Lünendonk L. et al.: Künstliche Intelligenz: Eine Studie zum Status quo in deutschen \nUnternehmen und zu zukünftigen Anwendungsfällen. pp. 7. Lünendonk & Hossenfelder \nGmbH, Mindelheim (2019).  \n12.\u0001Statistisches Bundesamt: Dossier: Internet of Things. [Data set]. pp. 15 and 29. [Status: \n12th August 2021]. Available from: https://www.statista.com/topics/2637/. \n13.\u0001Pistorius J.: Industrie 4.0 – Schlüsseltechnologien für die Produktion: Grundlagen \nPotentiale Anwendungen. pp. 9 and 12. Springer Verlag, Berlin (2020).  \n14.\u0001Haun M.: Handbuch Robotik: Programmieren und Einsatz intelligenter Roboter. pp. 18-\n22. Springer Verlag, Berlin. (2007).  \n15.\u0001Covariant: Case Study: Making Obeta´s warehouse more resilient with KNAPP and \nCOVARIANT´s AI-powered robot. Embodied Intelligence Inc. [Status: 11th August \n2021]. Available from: https://covariant.ai/case-studies/obeta. \n16.\u0001CC-BY-SA 4.0.: Image Classification on ImageNet. [Status: 13th Mai 2021]. Available \nfrom: https://paperswithcode.com/sota/image-classification-on-imagenet. \n17.\u0001Stowasser S., Suchy O. et al.: Einführung von KI-Systemen in Unternehmen: \nGestaltungsansätze für das Change-Management. pp. 23-25. Plattform Lernender \nSysteme, München. (2020).  \n18.\u0001Abdelkafi N. et al.: Künstliche Intelligenz (KI) im Unternehmenskontext: \nLiteraturanalyse und Thesenpapier. pp. 19-24. Apress Inc, New York City (2019). \n19.\u0001Wolf A. und Schunk H.: Greifer in Bewegung: Faszination der Automatisierung von \nHandhabungsaufgaben. pp. 109. Carl Hanser Verlag, München (2016). \n20.\u0001Yang Q. et al.: Transfer Learning. pp. 13. University Press, Cambridge (2020).  \n21.\u0001Intel Corporation: Intel RealSense Depth Camera D435. [Status: 12th August 2021]. \nAvailable from: https://www.intelrealsense.com/depth-camera-d435/. \n22.\u0001Ukil A.: Intelligent Systems and Signal Processing in Power Engineering. pp. 111-115. \nSpringer Verlag, Berlin (2007).  \n23.\u0001Koonce B.: Convolutional Neural Networks with Swift for Tensorflow: Image \nRecognition and Dataset Categorization. pp. 36-38. Apress, New York City (2021).  \n24.\u0001Jadon S., Garg A.: Hands-On One-shot Learning with Python: Learn to implement fast \nand accurate deep learning models with fewer training samples using PyTorch. pp. 27-\n30. Packt Publishing Ltd., Birmingham (2020).  \n25.\u0001Haase T.: Greifplanung und Greifskills für reaktives Greifen. pp. 10-15. KIT Scientific \nPublishing, Karlsruhe (2011). \n26.\u0001Ronneberger O., Fischer P., Brox T.: U-Net: Convolutional Networks for Biomedical \nImage Segmentation. University of Freiburg, Freiburg (2015). arXiv:1505.04597v1 \n[cs.CV].  \n27.\u0001Hinton G., Vinyals O., Dean J.: Distilling the Knowledge in a Neural Network. (2015) \narXiv:1503.02531v1 [stat.ML]. \n193\n\nVizNN: Visual Data Augmentation with\nConvolutional Neural Networks for Cybersecurity\nInvestigation\nAm´elie Raymond1, Baptiste Brument1, and Pierre Parrend2,3\n1 T´el´ecom-Physique – University of Strasbourg, France\namelie.raymond@etu.unistra.fr, baptiste.brument@etu.unistra.fr\n2 EPITA\npierre.parrend@epita.fr\n3 ICube laboratory\nUniversity of Strasbourg\nAbstract. One of the key challenges of Security Operating Centers (SOCs) is to\nprovide rich information to the security analyst to ease the investigation phase\nin front of a cyberattack. This requires the combination of supervision with de-\ntection capabilities. Supervision enables the security analysts to gain an overview\non the security state of the information system under protection. Detection uses\nadvanced algorithms to extract suspicious events from the huge amount of traces\nproduced by the system. To enable coupling an eﬃcient supervision with perfor-\nmance detection, the use of visualisation-based analysis is a appealing approach,\nwhich into the bargain provides an elegant solution for data augmentation and\nthus improved detection performance. We propose VizNN, a Convolutional Neu-\nral Networks for analysing trace features through their graphical representation.\nVizNN enables to gain a visual overview of the traces of interests, and Convo-\nlutional Neural Networks leverage a scalability capability. An evaluation of the\nproposed scheme is performed against reference classiﬁers for detecting attacks,\nXGBoost and Random Forests.\nKeywords: Data augmentation, Visualisation, Neural Network, Benchmark, Cy-\nbersecurity, Investigation\n1\nIntroduction\nSecurity Operating Centre (SOCs) continuously experience increasing amounts of super-\nvision data, that require scalable processing capability coupled with ﬁne-grained detection\nof known and unknown (zero-days) attacks. New solutions are thus required to back the\ninvestigation eﬀorts of security analyst teams. The core requirements of these solutions\nare: explicability and traceability of alert to the original trace; investigation support\nthrough visualisation; scalability of detection.\nThe availability of scalable detection algorithms is thus key for building eﬃcient SOCs\ncapable of handling the data deluge. To this aim, neural networks such as Convolutional\nNeural Networks [1] are very competitive candidates, but still lack of maturity for rapid\noperational deployment. One of the key challenges for using neural networks in operation\nenvironment is the capability of performing suitable data augmentation in the analysis\nﬂow [2, 3]. Data augmentation can serve several goals: completing the data representation\nis areas where available information do not support satisfactory learning (scarce zones\nof valid data, or imbalanced dataset); altering the information to avoid overﬁtting and\n194\n\nanticipate random modiﬁcations of the observed behaviours; changing the data format,\nfor instance to apply algorithms with speciﬁc data input such as images to other data\ntypes such as text, sequences or qualitative features.\nIn this paper, we propose VizNN, a visual data augmentation model with Convo-\nlutional Neural Networks (CNN) for cybersecurity investigation. Its goal is to leverage\nscalability and detection capability of CNNs to the analysis of quantitative logs for de-\ntection of known attacks. Alternative detection solutions such as Random Forests [4] and\nXGBoost [5] also provide excellent results for medium size dataset. They are used as\nreference for benchmarking.\nTo evaluate the relevance and eﬃciency of the proposed approach, we apply it to\nthe analysis of security properties of the DoH protocol [6]. DoH (DNS-Over-HTTPS)\naims at encrypting DNS requests by encapsulating them in a classic HTTPS stream.\nIts goal is to solve the vulnerabilities of DNS (Domain Name Systems) which is used\nfor each and every request on the Internet to bind human-readable domain names with\nmachine-readable IP addresses. DNS experiences several majors weaknesses in its default\nconﬁguration [7, 8], which urge the community to propose adequate countermeasures.\nThe evaluation of VizNN scheme is performed on CIRA-CIC-DoHBrw-20204 dataset\n[9], created by the Canadian Institute of Cybersecurity (CIC) and funded by the Canadian\nInternet Registration Authority (CIRA). Data is collected from the top 10,000 visited\nwebsites according to Alexa rankings. The raw data, in PCAP format, was processed and\nconverted into CSV ﬁles using the DOHMeter tool.\nThe paper is organised as follows. Section 2 introduces the related works. Section\n3 deﬁnes the data processing methodology, section 4 details the data preparation and\nexploration phase, whereas section 5 deﬁnes the propose data anaysis scheme. Section 6\nevaluates the results. Section 7 concludes this work.\n2\nRelated Work\n2.1\nNeural networks for cybersecurity\nThe advent of Deep Learning (DL) for the analysis of cybersecurity events takes its roots\nin the limitation of pre-existing Machine Learning (ML) algorithms. Machine Learning\nis used mainly for classiﬁcation or regression, and keeps relying on feature engineering\n[10]. The expected advantages of Deep Learning is its capacity to perform well when data\namount increases, and to provide very eﬃcient test operation even though training phase\nis usually signiﬁcantly longer. One of the key factors for the performance of Deep Learning\nis the numerous matrix operations it relies on, which let it be preferably executed over\nGPU.\nDeep Learning comes in numerous ﬂavours when applied to cybersecurity: Deep Neu-\nral Networks, which extracts hidden patterns from in the internal layers of the network,\nRecurrent Neural Networks such as LSTM (Long Short Term Memory) [11] which retain\nthe memory of previous states, Convolutional Neural Networks [1, 12] to process data\nhaving a high degree of similarity, Restricted Boltzmann Machines for data generation\nor classiﬁcation, Deep Belief Networks as brick of larger neural nets, Deep auto-encoders\nfor trace de-noising or classiﬁcation [13, 14].\n4 https://www.unb.ca/cic/datasets/dohbrw-2020.html\n195\n\n2.2\nData augmentation for neural network learning\nData augmentation is the process of artiﬁcially enriching real data with synthetic mock-\nups to improve learning, in particular to remove learning bias [2]. One typical application\nof data augmentation is the densiﬁcation of data zones where events are legit but scarce\nto avoid the generation of false negatives. The principle of data augmentation originally\nreferred to statistical data enhancement like SMOTE [15], which is meant for handling\nimbalanced data. It increasingly refers now to the generation and enrichment of images to\nbe analysed, in particular in the context of Convolutional Neural Networks. The objective\nis to leverage the capability of data volume scaling of deep learning approaches. It proves\nto be eﬃcient for the detection of object landmarks such as invariance in shape, pose\nand illumination [16].\nImage augmentation can be performed through basic image manipulations such as\ngeometric transformations, ﬂipping, colouring, cropping, noise injection, through geo-\nmetric and photometric transformations such as kernel ﬁlters, mixing images, random\nerasing, or through deep-learning such as feature space augmentation, adversarial train-\ning, Generative Adversarial Networks or neural style transfer [2]. The combination of\nthese operations, especially the simple ones, can be performed automatically to improve\nthe validation accuracy like with AutoAugment tool [3]. The ﬁrst image augmentation\ntool can be considered to be the auto-encoder of Hinton, which is applied for dimension\nreduction, noise reduction, data and image generation bases on multi-layer architec-\nture with internal small-dimension layers [17]. More recent solutions focus on Generative\nAdversarial Networks (GAN) for data and image generation, which base on a pair of\nDeconvolutional/Convolutional Neural Networks [18]. This approach has made radical\nqualitative progress since its inception in 2014 [19].\n2.3\nData augmentation for cybersecurity\nData augmentation for cybersecurity is used both for supervised and for unsupervised\ndetection. It is used for supervised detection of malwares to address the variability of\nmalicious code by adding noise in the training mode under Gaussian, Poisson or Laplace\nmodel. Then, a Convolutional Neural Networks performs the learning operation [20].\nFor unsupervised anomaly detection, the strategy is to oversample normal, rare data\nwhich usually causes most false positives. This approach is only applicable to well-deﬁned\ndata distribution if one does not want to inject excessive bias in the dataset. Lim proposes\nto augment not the input data, but a representative latent vector at the core of the\nneural network, through multivariate Gaussian sample generation [21]. It integrates an\nadversarial auto-encoder (AAE), which is an extension of GAN [22].\n3\nData processing\nPerformed data analysis is performed in two steps: ﬁrst, a generic DAP (Data Analy-\nsis Process), which is meant for reuse independently of the analysis methodology; then,\nVizNN, the visual data augmentation model we propose for Convolutional Neural Net-\nworks.\n3.1\nDAP - the Data Analysis Process\nThe Data Analysis Process entails following steps:\n196\n\n◦Capture: the system behaviour is collected and gathered in a dedicated datalake for\nimmediate analysis or later reference. The behaviour can be represented as actions\n(logs), system measures (scalar probes) or action measures (scalar measures derived\nfrom logs).\n◦Cleanup: incomplete or inconsistent data is removed, such as action or probe occur-\nrences (data lines) with missing values or features (data columns) with non discrim-\ninating values.\n◦Standardisation: data is normalised or enhanced with metadata, in particular through\nFAIRiﬁcation5 (i.e to make it Findable; Accessible; Interoperable; Reusable).\n◦DE – Data Exploration: a manual scrutiny of the data is performed to highlight its\nparticularities.\n◦DA – Data Analysis: automated statistical or Machine Learning algorithms are ap-\nplied to the data in order to perform anomaly detection (in unknown rare events such\nas an abnormal access to a resource) or classiﬁcation (discriminate known events such\nas a known attack).\n◦Visualisation: the data or extractions thereof is presented to the user.\n◦Investigation: the expert, here the security analyst, performs computer-assisted com-\nplementary inspection of the data to understand the output of the automated analysis\nand to identify behaviours not characterised by automation.\n3.2\nVizNN - Visual data augmentation for Convolutional Neural Network\nThe VizNN pipeline, shown in Figure 1, consists of ﬁve steps: data import and clean-up;\nselection of the features you want to keep; creation of images; preparation of images;\ntraining of the CNN model.\nData import and cleanup The import and cleaning steps are similar to those of a Machine\nLearning project.\nAfter importing the data, missing values are completed or deleted. Categorical features\nare then converted to numerical ones. A correlation analysis may be relevant to remove\nunnecessary features. Finally, a ”label” column is added to each row for classiﬁcation.\nSelection of the features you want to keep Rather than converting the entire dataset into\nimages, a feature selection step is performed to only keep the columns that maximize\nperformance.\nTherefore, a basic XGBoost model is trained in order to retrieve the list of most\nimportant features on the model’s gain.\nOur proposal is to create images with the N most important features. To create images,\nthis number cannot be a prime number since the width and the height have to be integers.\nThe data is then ﬁltered so that only the N most important features are kept.\nCreation of images The ﬁltered DataFrame is converted to a list of grayscale Image\nobjects using PIL library6.\nImages then undergo data augmentation through resizing with bicubic interpolation to\ngenerate new pixels and therefore enlarge them.\n5 https://www.go-fair.org/fair-principles/fairiﬁcation-process/\n6 https://pillow.readthedocs.io/en/stable/reference/Image.html\n197\n\nFig. 1. Full pipeline process\nFig. 2. Image creation process\nFig. 3. Examples of augmentated feature images for VizNN\nPreparation of images Several operations are performed on the images to transform them\ninto CNN inputs.\nFirst, the list of Image objects is converted to a NumPy array. Second, the data is\nsplit into a training and a testing dataset with the ”StratiﬁedShuﬄeSplit” method. It\nshuﬄes and separates the data while respecting the proportion of each class. Thus, it\nensures that even with unbalanced data, models are trained and tested on all classes with\nrepresentative proportions.\nThird, a dimension is added to the image so that it has three dimensions: width, height\nand number of channels(1 for gray-scale images and 3 for RGB images). Finally, images\nare normalized to have, for each pixel, values between 0 and 1.\nTraining of the CNN model The Convolutional Neural Networks is trained using the\nimages generated by the reference dataset.\n4\nData Preparation and Exploration\nData exploration is performed by data cleaning and subsequent scrutiny through highly\neﬃcient, through very widespread, learning algorithms: Random Forests and XGBoost.\n4.1\nThe dataset\nThe CIRA-CIC-DoHBrw dataset entails two kind of traﬃc: HTTPS traﬃc without DOH\nand DOH traﬃc. In case of DOH traﬃc, attacks and normal traﬃc were distinguished.\nThus, 4 subdatasets are considered: “non DOH”, “DOH”, “benign DOH” and “malicious\nDOH”. These datasets include 33 features that deﬁne the traﬃc.\nThis study focuses on the classiﬁcation of DOH traﬃc (benign or malicious). The\ntwo corresponding CSV ﬁles are imported and combined into one DataFrame with an\n198\n\nadded column entitled ”label”. Label 0 corresponds to benign DOH traﬃc and label 1\nto malicious DOH traﬃc.\nThe dataset is unbalanced: there are 67% of attacks and 23% of benign traﬃc. Several\ncolumns such as ”sourceIP”, ”DestinationPort” were removed since they are artiﬁcally\ncrafted for analysis stakes. Afterwards, the ”TimeStamps” column is transformed into\ncategorical data. In this way, four categories are used to describe the period of the day:\n”morning”, ”day”, ”evening” and ”night”.\n4.2\nRandom Forests\nRandom Forests is a classiﬁcation algorithm based on ensemble learning, which consists\nof using so-called “weak” models to make predictions and then to combine them into a\nlarger model. It operates by creating a given number of decision trees. More speciﬁcally,\nan improved version of bagging is used in order to reduce the correlation between each\ntree. The samples of the training data are provided as an input for each tree. The latter\nthen make their prediction. The results are aggregated using a majority rule which means\nthat the ﬁnal predicted class is the class that was predicted the most by the individual\ndecision tree.\nFig. 4. The learning curves for Random Forests\nFig. 5. The learning curves for XGBoost\nFigure 4 shows the loss curve for Random Forests learning. Note that the ordinate\nspans from 0 to 0.045, which makes it a very close zoom. Training curve do not stabilise\nbefore 14,000 iterations.\nThe extraction of feature importance by gain, for the Random Forests algorithm,\nshows that the main features are statistics derived from the packet length: mode, mean,\nmedian and standard deviation. Note that the original dataset entails such aggregated\ndata, and not directly raw communication traces.\n4.3\nXGBoost\nXGBoost stands for Extreme Gradient Boosting [5]. This algorithm is also based on\nensemble learning strategy. Gradient boosting is a subclass of boosting algorithms. In\nboosting, each sub-model is weighted. This allows greater ﬂexibility as it gives the ability\nto give more importance to certain models in certain learning cases. Thus, this type of\n199\n\nmodels is particularly eﬀective when the training data is not balanced. The speciﬁcity of\ngradient boosting is that the contributions of the decision tree models to the ﬁnal model\nare calculated from the optimization of the gradient descent. This technique gives very\naccurate results. XGBoost is considered one of the best algorithms in the current state\nof the art classiﬁcation algorithms.\nFigure 5 shows the loss curve for XGBoost learning.\nThe extraction of feature importance by gain, for the XGBoost algorithm, shows that\nthe main features there are packet length mode, number of ﬂow bytes received, duration,\nas well as packet length statistics.\n5\nDA – Data Analysis with VizNN\nThis section presents the implementation of VizNN and the search for optimal model\nparameters for the data preparation ﬂow. The Convolutional Neural Networks takes\na series of images representing the features to be analysed as input. It is built by a\nconvolutional layer, maxpooling layer, renewed convolutional and maxpooling layers, two\nﬂatten and dense neuron layers, and a two-value output layer. RELU activation is used.\nEach experiment is performed with 50-folds cross-validation, a validation set consisting\nof 20% of the training data and over 20 epochs. Figure 6 shows this architecture.\nFig. 6. VizNN architecture\nInﬂuence of image size To evaluate the impact of image size on the quality of the de-\ntection, we ﬁrst evaluate the detection performance for square images with side ranging\nfrom 50 to 95 pixels. Images are created using all dataset features. Several models with\nthe same architecture are then trained on these images, each model testing a diﬀerent im-\nage size. The results indicate that image size has a signiﬁcant inﬂuence on performance,\nas shown in Figure 7. According to the scores, 75 × 75 is the size that maximises the\nperformance. Hence, the size is set as 75 × 75 for the rest of the study.\nInﬂuence of the number of features Another image-speciﬁc parameter that can inﬂuence\nthe results is the number of features used to create images. Following the same procedure\nas for the image size search, several models are trained with images created from diﬀerent\nnumber of features. For the dataset used for the experiment, keeping the 16 (out of 33)\nmost important features from XGBoost’s gain provides the best scores, as shown in\nFigure 8. Also, the model is under-trained when there are not enough features because\nthe training images are not representative enough. On the contrary, if too many features\n200\n\nare kept, the model is over-trained.\nFor the rest of the study, the number of features is thus set to 16.\nFig. 7. Performance of VizNN according to the\nsize of the images\nFig. 8. Performance of VizNN according to the\nnumber of features\nInﬂuence of the architecture Once the image-related parameters are set, several CNN\narchitectures are tested by changing the number of layers and ﬁlters. As shown on Fig-\nure 9, an architecture with 32 and 48 for the ﬁrst and the second convolutional layer\nrespectively provides the best detection capability for the dataset under consideration.\nFigure 10 shows the loss for resulting architecture. Learning stabilised after 12 epochs\nfor both training and validation.\nFig. 9. Impact of CNN architecture on perfor-\nmance\nFig. 10. The learning curves for VizNN Model\nThese evaluations for parameter settings show that each parameter has a speciﬁc\noptimum for optimising learning performance on the current dataset. It is highly likely\nthat these values are dependant on the data, and can not be considered as general\nrecommendations. Rather, the described process should be perform over again when the\nsecurity context changes.\n6\nEvaluation\nThis section presents the overall results of the experiments on CIRA-CIC-DoHBrw-2020\ndataset. Learning performance is evaluated using the AUPRC (Area under the Preci-\nsion Recall Curve, suitable for imbalanced data), precision and recall metrics. Train-\ning and prediction time is evaluated for the VizNN and the two reference algorithms,\n201\n\nRandom Forests and XGBoost. Each score corresponds to an average over a 50-folds\ncross-validation process.\nFig. 11. Comparison of scores by model with and without SMOTE\nFigure 11 shows the comparison of the diﬀerent models: Random Forests, XGBoost\nand VizNN, without and with SMOTE (Synthetic Minority Over-Sampling TEchnique)\noversampling strategy.\nThe ﬁrst observation is that XGBoost outperforms Random Forests and VizNN for\nthe three metrics AUPRC, precision and recall. Indeed, for the tests without SMOTE\nXGBoost reaches 0.9998 in AUPRC and 0.9997 for the precision and the recall. Random\nForests comes in second place with an AUPRC of 0.9997, a precision of 0.9995 and a\nrecall of 0.9995 VizNN gets an AUPRC of 0.9991, a recall of 0.9990 and a precision of\n0.9990. Concerning the experiments on the dataset increased by SMOTE, the results\nare slightly diﬀerent but the order remains unchanged. XGBoost provides an AUPRC of\n0.9998, a precision of 0.9996 and a recall of 0.9996. Random Forests gets better results\nand reaches the same scores than XGBoost, namely 0.9998 for its AUPRC, 0.9996 of\nprecision and 0.9996 for the recall. Finally, VizNN yields to 0.9986 in AUPRC, 0.9987 in\nprecision and 0.9987 in recall.\nFor all these evaluation rounds, VizNN proves to be very competitive though slightly\nbehind the reference algorithms – less that 0,08%.\nFig. 12. Comparison of training time\nFig. 13. Comparison of prediction time\n202\n\nFigure 12 shows the respective training times of Random Forests, XGBoost and\nVizNN. Training requires 9 seconds for Random Forests, 7.9 seconds for XGBoost, and\n210.7 seconds for VizNN.\nFigure 13 shows the respective prediction times of Random Forests, XGBoost and\nVizNN. Prediction requires 0.04 seconds for Random Forests, 0.2 seconds for XGBoost,\nand 2.2 seconds for VizNN.\nIn VizNN, the detection process is performed through a CNN which architecture is the\nfollowing one: a convolutional layer with 32 ﬁlters and RELU activation; a maxpooling\nlayer; a convolutional layer with 64 ﬁlters and RELU activation; a second maxpooling\nlayer; and three ﬁnal ﬁltering layers. The conﬁguration of our experimental setup has\noptimal results for 16 features, image size of 75x75 pixels and an architecture of 32 and\n48 layers.\nThe evaluation of the proposed scheme shows that it is highly competitive with the\nreference algorithms XGBoost and Random Forest. It is performed by detecting attacks\npresent in CIRA-CIC-DoHBrw-2020 dataset, which entails 60178 rows and 35 columns.\nThe AUPRC metric achieves 0,9991 for VizNN, against 0,99982 for XGBoost and 0,99975\nfor Random Forest. The use of SMOTE has a slightly positive impact on detection\ncapability in the Random Forest case only (AUPRC = 0, 99970), and slightly negative\nimpact for VizNN (0,9985) and for XGBoost (0,99976).\n7\nConclusions\nIn this work, we propose VizNN, a new scheme for applying Convolutional Neural Net-\nworks to the detection of cybersecurity attacks through data augmentation. VizNN works\nby generating images from the system logs. It deliberately ignore non-representative in-\nformation such as IP addresses and ports, and uses one-hot encoding of categorical infor-\nmation and timestamps. VizNN selects the most importance features as ranked by the\ngain they bring to learning, converts this features to image and resize them for optimizing\nthe detection.\nVizNN succeeds at integrating data augmentation in the analysis ﬂow and at applying\na highly scalable detection approach through Convolutional Neural Networks. It enhances\nthe visibility of the analysis process by providing intermediate graphical representations\nof the system states to the security analyst. Nonetheless, VizNN still experience several\nlimitations that need to be solved in future works. First, the graphical representations\nneed important experience to be readable by the security analyst, like medical X-Rays\nrequire experienced practitioners. Second, the explicability and traceability of alerts to\noriginal traces are still better performed by alternative solutions like Random Forests or\nXGBoost.\nThe next step of this study is the evaluate to proposed scheme for massive datasets\n(> 106 data entries) to challenge the scalability capability of the various schemes under\nevaluation. Such an evaluation would be more representative of the amount of data that\na Security Operating Centre (SOC) handles daily.\nReferences\n1. Kravchik, M., Shabtai, A.:\nDetecting cyber attacks in industrial control systems using\nconvolutional neural networks. In: Proceedings of the 2018 Workshop on Cyber-Physical\nSystems Security and PrivaCy. (2018) 72–83\n203\n\n2. Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation for deep learning.\nJournal of Big Data 6(1) (2019) 1–48\n3. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning aug-\nmentation strategies from data. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. (2019) 113–123\n4. Breiman, L.: Random forests. Machine learning 45(1) (2001) 5–32\n5. Chen, T., Guestrin, C.: Xgboost: A scalable tree boosting system. In: Proceedings of the\n22nd acm sigkdd international conference on knowledge discovery and data mining. (2016)\n785–794\n6. Hounsel, A., Borgolte, K., Schmitt, P., Holland, J., Feamster, N.: Analyzing the costs (and\nbeneﬁts) of dns, dot, and doh for the modern web. In: Proceedings of the Applied Networking\nResearch Workshop. (2019) 20–22\n7. Ariyapperuma, S., Mitchell, C.J.: Security vulnerabilities in dns and dnssec. In: The Second\nInternational Conference on Availability, Reliability and Security (ARES’07), IEEE (2007)\n335–342\n8. Afek, Y., Bremler-Barr, A., Shaﬁr, L.:\nNxnsattack: Recursive {DNS} ineﬃciencies and\nvulnerabilities. In: 29th {USENIX} Security Symposium ({USENIX} Security 20). (2020)\n631–648\n9. Banadaki, Y.M.: Detecting malicious dns over https traﬃc in domain name system using\nmachine learning classiﬁers. Journal of Computer Sciences and Applications 8(2) (2020)\n46–55\n10. Xin, Y., Kong, L., Liu, Z., Chen, Y., Li, Y., Zhu, H., Gao, M., Hou, H., Wang, C.: Machine\nlearning and deep learning methods for cybersecurity. IEEE Access 6 (2018) 35365–35381\n11. Gasmi, H., Bouras, A., Laval, J.: Lstm recurrent neural networks for cybersecurity named\nentity recognition. ICSEA 11 (2018) 2018\n12. Kwon, D., Natarajan, K., Suh, S.C., Kim, H., Kim, J.: An empirical study on network\nanomaly detection using convolutional neural networks. In: 2018 IEEE 38th International\nConference on Distributed Computing Systems (ICDCS), IEEE (2018) 1595–1598\n13. Berman, D.S., Buczak, A.L., Chavis, J.S., Corbett, C.L.: A survey of deep learning methods\nfor cyber security. Information 10(4) (2019) 122\n14. Ferrag, M.A., Maglaras, L., Moschoyiannis, S., Janicke, H.: Deep learning for cyber security\nintrusion detection: Approaches, datasets, and comparative study. Journal of Information\nSecurity and Applications 50 (2020) 102419\n15. Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P.:\nSmote: synthetic minority\nover-sampling technique. Journal of artiﬁcial intelligence research 16 (2002) 321–357\n16. Jakab, T., Gupta, A., Bilen, H., Vedaldi, A.: Unsupervised learning of object landmarks\nthrough conditional image generation. In: Advances in neural information processing sys-\ntems. (2018) 4016–4027\n17. Hinton, G.E., Krizhevsky, A., Wang, S.D.: Transforming auto-encoders. In: International\nconference on artiﬁcial neural networks, Springer (2011) 44–51\n18. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A., Bengio, Y.: Generative adversarial networks. arXiv preprint arXiv:1406.2661\n(2014)\n19. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville,\nA., Bengio, Y.: Generative adversarial networks. Communications of the ACM 63(11) (2020)\n139–144\n20. Catak, F.O., Ahmed, J., Sahinbas, K., Khand, Z.H.: Data augmentation based malware\ndetection using convolutional neural networks. PeerJ Computer Science 7 (2021) e346\n21. Lim, S.K., Loo, Y., Tran, N.T., Cheung, N.M., Roig, G., Elovici, Y.: Doping: Generative data\naugmentation for unsupervised anomaly detection with gan. In: 2018 IEEE International\nConference on Data Mining (ICDM), IEEE (2018) 1122–1127\n22. Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., Frey, B.:\nAdversarial autoencoders.\narXiv preprint arXiv:1511.05644 (2015)\n204\n\nTowards generating complex programs represented as\nnode-trees with reinforcement learning\nAndreas Reich and Ruxandra Lasowski\nHochschule Furtwangen University\nreich@digitalmedia-design.com\nruxandra.lasowski@hs-furtwangen.de\nAbstract. In this work we propose to use humanly pre-built functions which\nwe refer to as nodes, to synthesize complex programs. As e.g. in the node-tree\nprogramming style of Houdini (sidefx.com), we propose to generate programs by\nconcatenating nodes non-linearly to node-trees which allows for nesting functions\ninside functions. We implemented a reinforcement learning environment and per-\nformed tests with state-of-the-art reinforcement learning algorithms. We conclude,\nthat automatically generating complex programs by generating node-trees is possi-\nble and present a new approach of injecting training samples into the reinforcement\nlearning process.\nKeywords: Neural program synthesis; node-trees; machine learning; reinforce-\nment learning; supervised learning; sample injection\n1\nIntroduction\nMany modern computer programs, especially in the 3D sector, oﬀer users to interact with\ntheir software via so-called nodes, constructing so-called node-trees. Nodes are atomic\nparts of node-trees. Every node represents a function, a pre-programmed sequence of\ncomputer code visually. Node-trees are a visual high-level representation of non-linear\nsequences of nodes and are simpler to understand then regular code [1]. Due to the\nincreasing prevalence of nodes in computer software, the idea arose to utilize machine\nlearning to automatically generate node-trees with neural networks. Because nodes rep-\nresent computer code, automatically generating node-trees with AI means automatically\ngenerating computer code with AI. Therefore, our approach is classiﬁed in the ﬁeld of\nneural program synthesis.\nThe task of program synthesis is to automatically ﬁnd programs for a given program-\nming language that satisfy the intent of users within constraints [2]. Researchers like\nBunel et. al. use supervised and reinforcement learning techniques to generate programs\nby concatenating low-level nodes linearly [3]. In contrast to their approach we propose\nto to generate programs concatenating high-level nodes non-linearly because this allows\nfor more complex programs when using the same count of nodes. Our approach aims for\nautomating the manual node-tree generation pipeline with AI that uses non-linear and\nhigh-level nodes. Using high-level nodes in the generation process is beneﬁcial:\nWhen compiled, all non-linear node-trees are transformed into linear sequences of\ncode. The use of high-level nodes that consist of many low-level nodes is beneﬁcial for\ncertain use cases since more complex tasks can be solved with high-level nodes than\nlow-level nodes (if the correct nodes are available) because more code is executed.\nMost existing node-driven software solutions have a highly optimized set of high-\nlevel nodes that are software-speciﬁc. Consequently, users can perform a great variety\n205\n\nof tasks solely using few domain-speciﬁc nodes. Automatically creating and combining\nthese specialized nodes with the help of AI would save users a lot of time and resources,\nwhile keeping full editability of generated node-tress.\nDeveloping algorithms that search for valid node-trees in a search space (space of all\npossible connections) is challenging because the search space grows exponentially when\nadding nodes or depth and best regular program synthesis algorithms like Chlorophyll++\nare currently able to ﬁnd programs in a search space of 1079 [4]. This means if the\naforementioned algorithm would work with nodes it could reliably ﬁnd programs out\nof a pool of 100 individual nodes and a depth of 39 nodes. According to Bod´ık ﬁnding\nprograms in a search space of 1079 is not suﬃcient to solve complex problems, e.g. like\nimplementing the MD5 hashsum algorithm, located in a search space of 105943 [4], since\nthe algorithm works with low-level functions.\nHowever, the generation of programs becomes easier if few high-level functions, that\nachieve the same result as many low-level functions, are combined. For instance, a pro-\ngram could just consist of 5 pre-assembled, high-level functions that themselves consist\nof hundreds of lines of code. Most algorithms could ﬁnd such a program, just consisting\nof 5 nodes, with ease. The complexity of ﬁnding valid programs is constrained to the size\nof the search space. This means the complexity of ﬁnding node-trees built from high-level\nand low-level nodes are equal since the complexity is constrained to valid connections.\nNonetheless, training an AI with high-level nodes is more costly computationally since\nmore code needs to be executed.\nThis paper strives to point out the beneﬁts of working with high-level over low-level\nfunctions. Performing some tests with the 3D Engine Blender and utilizing a node-based\nmodeling tool showed that a speciﬁc node-tree in Blender with as little as 4 nodes already\nrepresents more than 1000 lines of computer code, whereas approaches from the Google\nBrain team are able to reliably generate programs with up to 25 lines of code with AI\n[5]. Figure 1 visualizes the diﬀerence between the count of nodes with the complexity of\nthe resulting programs.\nﬁgure 1: complexity of high-level and low-level nodes\nA possible real-world use case for using automatically synthesized node-trees could be\nthe software Substance Designer with a completely node-driven workﬂow for texture and\nmaterial creation. The Substance Designer documentation states that there are about\n300 individual (high-level) nodes, yet also that most of these nodes only ﬁnd application\nin rare, speciﬁc use cases [6]. Assuming one could build basic substance materials with\n206\n\n50 diﬀerent nodes and a node-tree length of 40, the complexity of the search space would\nbe 5040 ≈1068 – less than what is currently possible in regular program synthesis.\nNevertheless, a lot of the Substance Designer nodes have internal states that need to be\nadjusted, so the number of nodes necessary increases since changing an internal state can\nalso be represented by the creation of a speciﬁc node.\nAssuming one could create nearly all common Substance Designer materials by using\nall 300 nodes and 300 extra nodes that represent internal state changes, the creation of\nnode-trees with lengths of 100 nodes leads to a search space of 600100 ≈10278. Compared\nto solving MD5 with low-level functions and a search space complexity of 105943 [4], a\nsearch space with the complexity of 10278 would still be way out of scope, yet more\nrealistic to be achievable in the upcoming decades.\nBesides Substance Designer, there are many node-based software solutions that oﬀer\na set of highly specialized nodes for diﬀerent use cases. Making use of a predeﬁned\nset of nodes is beneﬁcial since most applications with node collections are capable of\nperforming almost all tasks in a speciﬁc domain. We therefore identiﬁed certain areas\nwhere generating nodes with AI could be useful:\n– Parameterization of 3D meshes (input: 3D mesh, output: node-tree approximating\nthe modelling steps necessary to generate the mesh)\n– Parameterization of photoscanned point clouds (input: point cloud, output: node-tree\napproximating the modelling steps necessary to generate the point cloud)\n– Creation of 2D materials from images (input: 2D image, output: node-tree approxi-\nmating a PBR-material)\n– Reverse-engineering of functions and programs for optimization (input: program/function,\noutput: optimized node-tree that approximates the program/function)\n2\nPurpose and Methods\nWe start our research for node-tree synthesis with the task of calculating a number with\ncombinations of mathematical low-level nodes, e.g. plus and multiplication nodes, that\nare organized in a tree. In this way, we ﬁrst heavily abstract more complex tasks to show\nfeasibility. Figure 3 shows a node-tree reliably generated by our AI. We do not directly\ntry to synthesize node-trees for complex graphic industry software because they require\npipeline API’s that could be implemented once the feasibility is shown. The purpose of our\nresearch is to show that we are able to automatically create node-trees from nodes with\nAI. Furthermore we want to point out the beneﬁts of creating more complex programs\nby using high-level nodes instead of low-level nodes. Positive results could enable node-\ndriven software solutions to utilize AI to automate processes that currently are solely\nperformed by human users.\nWe use the qualitative method to investigate the state-of-the-art through literature\nresearch, especially concerning the comparison of publications and results of other re-\nsearchers. Furthermore, we use the quantitative method to build experiments to prove\nthat creating node-trees with supervised and reinforcement learning is possible.\nTo investigate the feasibility of generating node-trees with AI we implement an Ope-\nnAI Gym [7] reinforcement learning environment for automated node-tree generation and\nperform about 50 training sessions over the course of nine months. We use the reinforce-\nment learning neural network architecture DDPG [8] and adjust its parameters.\n207\n\nTo detect valid programs our actor explores a space of 825 (≈3.7 ∗1022) in which\nit can perform 25 actions out of a pool of 8 individual actions. The following types of\nactions can be performed by the network:\n• creating and automatically\nswitching to new nodes\n• changing internal states of\nnodes\n• switching back to the\nprevious node\nﬁgure 2: All actions that can be performed by the network\nThe following states are observable by the network:\n• speciﬁcation/goal number\n• relative distance to target\n• number of nodes in total\n• count of open connections\n• type of the active node\n• id of the active node\n• internal state of the active node\nSince our actor is not capable of ﬁnding valid samples without guidance we use an ap-\nproach which we refer to as sample injection (ﬁgure 4). During exploration we randomly\ninject a valid action sequence (complete episode), which fulﬁls a given speciﬁcation, into\nthe current batch instead of the actors own chosen action sequence. This yields good\nresults, since the actor learns from these optimal action sequences and is still capable\nof exploring the environment. This method is a combination of supervised- and rein-\nforcement learning. The supervised samples are random sequences of valid connections\nbetween nodes that lead to a result (node-tree) which can be used as a speciﬁcation for\ntraining. Therefore the actor can learn how to use which node in which context in or-\nder to fulﬁll a speciﬁcation. Due to this on-the-ﬂy generation process no data collection\nis necessary. Moreover the actor can learn how to use all available nodes in diﬀerent\ncontexts.\nﬁgure 3: node-tree found reliably in a search\nspace of the size of 825\n(8 individual actions and max. 25 actions)\nﬁgure 4: sample injection\nBecause the search space in which valid node-trees can be found is very large our\nmachine learning agent only receives sparse rewards and needs to learn from many sam-\nples. Therefore we decrease the learning rate to 10−5 and increase the replay buﬀer size\nto 107. For the most successful training sessions we train the network for ≈20 million\nsteps. We use the following reward function, which rewards the actor for approximating\na speciﬁcation. Since we use mathematical nodes the goal number serves as speciﬁcation\nand the current result of a node tree can be used to calculate the current distance which\n208\n\nis minimized:\n((goalNumber −currentDistance) −(goalNumber −previousDistance))\ngoalNumber\n∗incentive\nDDPG uses the random Ornstein-Uhlenbeck process [9] which results in few valid\nprograms to be found. We therefore complement the process with a supervised process\nto inject samples to enhance results. For most training sessions we inject samples 10%\nof the time instead of the action of the neural network. To reduce the impact of outliers\nwe use the Huber loss [10] to enhance regression quality.\nFigure 5 shows the network architecture of DDPG for the actor (left side) and the\ncritic (right side). Figure 6 shows the parameters used for training. Figure 7 shows the\npseudocode of DDPG with sample injection.\nﬁgure 5: DDPG network architecture: left: actor, right: critic\nﬁgure 6: parameters used for training DDPG\n3\nResults\nOur experiments show that our trained AI agent is capable of generating certain node-\ntrees reliably from low-level nodes when given a speciﬁcation. Some generated node-trees\nare found with 100% accuracy in regard to a given speciﬁcation whereas other node-\ntrees approximate a given speciﬁcation to some degree. Our results therefore show the\nfeasibility of generating node-trees from low-level nodes with AI. Hence, We observe\nthat more complex speciﬁcations, that need more nodes to be solved, lead to decreasing\naccuracy of our agent (table 1). This is due to the fact that the search space grows\nexponentially.\n209\n\nﬁgure 7: DDPG with sample injection pseudocode\n4\nLimitations\nDue to time and hardware limitations we are not able to ﬁnd programs in a search space\nof 1079 and therefore work with a smaller search space. Furthermore, we are only able\nto set up our experiments using low-level nodes. However, the size of the search space\nof high- and low-level nodes are equal when the same count of actions and nodes are\navailable. Therefore, the results of our experiments can be transferred and therefore also\napply to high-level nodes.\nTable 1. Node-trees reliably generated by our algorithm in a search space with the size of 825\nand their accuracy in relation to a given speciﬁcation (sorted by accuracy)\nSpeciﬁcation\nExemplary action trajectory\nAccuracy\n1,2,3,4,5,6,9\nadd, num, set(2), back, num, set(2)\n100%\n10\nadd, multiply, num, set(3), back, num, set(3)\n90%\n7,8\nadd, add, num, set(3), back, num, set(3), back, num, set(2)\n86%\n11\nadd, multiply, num, set(3), back, num, set(3)\n81%\n12\nadd, multiply, num, set(3), back, num, set(3)\n75%\n210\n\n5\nConclusions\nIn this work we use reinforcement learning in combination with supervised learning and\nthe technique of sample injection to tackle the problem of solving the combinatorial\nsearch over node-trees that lead to the user speciﬁed result. We think that our technique\ncould be used to ease many tasks in computer graphics like 3D mesh generation, VFX\ncompositing, material generation and node-tree based scripting. We show that it is pos-\nsible to automatically create node-trees from nodes with AI. Furthermore we point out\nthat the size of the search space when using low-level nodes and the size of the search\nspace when using high-level nodes are equal. This means that one can create more com-\nplex programs with the same amount of high-level nodes since high-level nodes consist\nof more lines of code. We therefore conclude that our technique will ﬁnd application in\nnode-driven software solutions and will leverage academia’s interest in node-based neural\nprogram synthesis.\nReferences\n1. Blackwell, A.F.: Metacognitive theories of visual programming. IEEE symposium on visual\nlanguages (1996) 240–244\n2. Gulwani, S.: Program synthesis. FNT in Programming Languages 4 (2017) 1–2\n3. Bunel, R., Hausknecht, M., Devlin, J., Singh, R., Kohli, P.: Leveraging grammar and rein-\nforcement learning for neural program synthesis. ICLR 2018 (2018)\n4. Bod´ık, R.: Program synthesis. opportunities for the next decade. (2015)\n5. Abolaﬁa, D., Norouzi, M., Shen, J., Zhao, R., Le, Q.V.: Neural program synthesis with\npriority queue training. (2018)\n6. Adobe: Substance designer - node library. (2021)\n7. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., Zaremba,\nW.: Openai gym (2016)\n8. Lillicrap, T.P., et al.: Continuous control with deep reinforcement learning. (2015)\n9. Uhlenbeck, G.E., Ornstein: On the theory of the brownian motion. (1930)\n10. Huber, P.J.: A robust version of the probability ratio test. The Annals of Mathematical\nStatistics (1965)\n211\n\nUse of Artiﬁcal Intelligence and Image Segmentation\nfor 3-Dimensional Modeling\nMichael Weber1,2, Tobias Weiß1,2, Franck Gechter2, and Reiner Kriesten1\n1 Institute of Energy Eﬃcient Mobility\nHochschule Karlsruhe - University of Applied Sciences, HKA\nKarlsruhe, Germany\n{michael.weber, tobias.weiss, reiner.kriesten}@h-ka.de\n2 CIAD (UMR 7533)\nUniv. Bourgogne Franche-Comte, UTBM\nBelfort, France\nLORIA-MOSEL (UMR 7503)\nUniversit´e de Lorraine\nNancy, France\nfranck.gechter@utbm.fr\nAbstract. To use Augmented Reality in an automotive vehicle for testing Ad-\nvanced Driver Assistance Systems a new development approach with high com-\nputing power is needed. Reasons for this are a high vehicle speed as well as fewer\npossible orientation points on an urban test track compared to using AR appli-\ncations inside a building. With the help of Image Segmentation, Artiﬁcial Intelli-\ngence for Object Detection, and Visual Simultaneous Localization and Mapping a\n3-Dimensional Model with precise information of the urban test site is to be gener-\nated. Through the use of AI and Image Segmentation, it is expected to signiﬁcantly\nimprove performance like computing speed and accuracy for AR applications in\nautomotive vehicles.\nKeywords: Artiﬁcial Intelligence, Augmented Reality, Advanced Driver Assis-\ntance Systems, Visual Simultaneous Localization and Mapping, 3-Dimensional\nModeling, Image Segmentation, Object Detection\n1\nIntroduction\nCamera-based Advanced Driver Assistance Systems (ADAS) such as the active lane de-\nparture warning system and traﬃc sign recognition support the driver, oﬀer comfort,\nand take responsibility for increasing road safety. These complex systems go through an\nextensive testing phase, which results in optimization potential regarding quality, repro-\nducibility, and costs. ADAS in the future will support ever-larger proportions of driving\nsituations in increasingly complex scenarios. Due to the increasing complexity of vehicle\ncommunication and the rising demands on these systems in terms of reliability to func-\ntion safely even in a complex environment and to support the driver and increase safety,\nthe test scenarios for ADAS are constantly further developed and adapted to higher re-\nquirements. European New Car Assessment Programme (Euro NCAP) has introduced a\nseries of new safety tests for ADAS into its program and created a road map until the\nyear 2025 [1] [2].\nToday’s test methods can be separated into two categories. On the one hand, the test-\ning of the ADAS with the help of virtual worlds and on the other hand, the testing in\n212\n\nreality on the test track using objects in real life. The central idea of the virtual test\nprocedure is to transfer vehicle behavior to virtual test drives as realistically as possible.\nThe approach for virtual tests is aimed at beneﬁt from the advantages of simulation in\nterms of reproducibility, ﬂexibility, and reduction of eﬀort. In this way, speciﬁcations and\nsolutions derived from them should be able to be tested and evaluated at an early stage\nof the development process. The use of suitable simulation methods enables the eﬃcient\ndesign, development, and application of vehicles and vehicle components. However, vir-\ntual development methods cannot yet replace real-life driving tests in all respects. Due\nto the complex physical conditions in which a vehicle is transferred when testing ADAS,\nreal-life driving tests are still necessary to the current status. For example, the weather,\nthe surface texture of the road, and other inﬂuencing parameters take a decisive role in\nthe evaluation process of ADAS test drives [3] [4].\nThe presented research background of this paper combines the advantages of ADAS-tests\nin a virtual simulation and these of ADAS-tests in a real environment. The camera im-\nages of the vehicle are augmented with additional virtual information. The augmentation\nof virtual road lanes allows, for example, the testing of a lane departure warning sys-\ntem independent of the test track. Scenarios such as the appearance of temporary lane\nmarkings or the absence of sections can be tested on the same test area. Narrowing and\nwidening of lane markings can be represented as well as international diﬀerences between\nroad markings. For testing traﬃc jam assistance systems, vehicles driving ahead can be\naugmented with camera images. In the ﬁrst phase of testing, second vehicles including\ndrivers can thus be dispensed with, reducing the costs of the tests and increasing the\nsafety of the test engineers. Furthermore, ADAS-test cases with traﬃc signs as well as\npedestrians and cyclists can be augmented situationally and quickly.\nFurthermore, by using Augmented Reality (AR) for testing camera-based ADAS, new\npossibilities for testing complex, critical, and even forbidden test cases arise. For example,\nfor testing the lane departure warning system, the traﬃc lane can be inserted into the\nimage in any given width, regarding the lane and the white stripe itself. Therefore it is\npossible to test the system to its limits, a feature not possible by testing in reality on the\ntest track.\nFor the use of AR, the system must be located (position and orientation) in its environ-\nment. This technology requires precise 3-dimensional (3D) modeling based on existing\nsensors. Usually, AR-applications are designed for human users and are mostly used inside\nbuildings. Through a variety of orientation points inside a building and the movement\nspeed of the user at walking speed, this technology is already quite advanced. The ap-\nproach of this research project, by contrast, is being developed for a Electronic Control\nUnit (ECU), which requires a novel development approach with high computing power\ndue to the high vehicle speed. Furthermore, compared to using AR-applications inside\na building, fewer orientation points are available on a test site, so a new concept has to\nbe developed here as well. The target of research described in this paper is to use Image\nSegmentation to analyze the environment of an mostly urban test site. Based on these\nresults, a 3D model of the environment is to be created. In a further step, the 3D model\nis to be added by objects such as traﬃc signs, road markings, pedestrians, cyclists, etc.\nThe use of Artiﬁcial Intelligence (AI) should provide precise information on the depth of\nthe environment using 2-dimensional (2D) image sequences. Through the use of AI and\nImage Segmentation, it is expected to signiﬁcantly improve the performance like com-\nputing speed and accuracy of the environment model. Moreover, conventional algorithms\nsuch as Simultaneous Localization and Mapping (SLAM) will be used for comparison\nwithin the research project.\n213\n\nFig. 1. Augmented Reality application showing a possible scenery\nThe main contributions of this paper include:\n1. An overview of challenges for the use of AR in the automotive vehicles with regard to\ncamera-based ADAS.\n2. An introduction of a novel approach based on visual SLAM (vSLAM) and using of AI\nfor object identiﬁcation and thus increasing the accuracy and reproducibility of ADAS\nin automotive vehicles.\n2\nNecessary Criteria for Augmented Reality\nTo use AR in ADAS of automotive vehicles diﬀerent criteria are necessary compared\nto conventional AR-applications like on a smartphone. This section will describe the\ncontrasting criteria for this approach.\n2.1\nAugmented Reality for Conventional Applications\nAccording to a proposal by Azuma, Augmented Reality can be deﬁned as a combination\nof three fundamental features: the combination of real and virtual worlds and precise\nthree-dimensional registration of the real and virtual objects, both in an interactive real-\ntime environment [5].The basic principle of AR is best known by the mobile phone game\nPok´emon Go, published in 2016 by Niantic [6]. Within this game, the users can interact\nwith digital creatures through their smartphones. These creatures are placed virtually in\nthe environment of the user. Such an AR application can be seen in Figure 1 [6]. Figure\n2 shows the three parts of the algorithms behind augmented reality: image analysis, 3D\nmodelling, and augmentation.\nThe image analysis serves to detect points or regions of interest within the given im-\nage. Feature detections like corner detection or edge detection are often used for this\n214\n\nFig. 2. Augmented Reality steps\nstep [7]. With the results of the image analysis, a three-dimensional model of the envi-\nronment is created. The kinds of algorithms used for this step vary depending on the\ntype of AR application. For AR in unknown locations, simultaneous localization and\nmapping (SLAM) or structure from motion (SfM) algorithms are widely spread [8]. The\naugmentation is based on the results of the 3D modelling. The scene model is usually\nprovided as a positional description of a plane or a coordinate system representing the\nreal world [9]. With this information, a virtual object can be placed upon the plane or in\nthe coordinate system with adequate characteristics such as size and orientation. After\nthe object placement, virtual content is combined with the real-world image [10].\nThere are several publications of applications for AR. These applications vary heav-\nily in their ﬁelds, from the usage of AR in psychology [11] to the use in operating rooms\nin hospitals [12] to mobile games [6] to military applications [13]. What all these appli-\ncations have in common is that the reality of a human is augmented. With the human\nas the user of AR, there are some implicit consequences for the application. One of them\nis that the human user is, in most cases, lenient towards virtual objects not placed pre-\ncisely within a small range of error. Furthermore, the velocity of human movement and\ntherefore the distance travelled by any given time is limited. By these restrictions, the\nrequirements for localization, mapping, object placement, and runtime are not as high\nas in an automotive environment, as is discussed in the next chapter.\n2.2\nAugmented Reality for Advanced Driver Assistance Systems (ADAS)\nin Automotive Vehicles\nFor the use of AR in automotive vehicles and the associated speciﬁc use of ADAS-sensor\ntechnology on conventional test tracks, special criteria are to consider. For instance, the\ntest sites are located outside of buildings and are usually therefore low textured [14]. In\naddition, there are quick scene changes due to the speed of the automotive vehicle. These\npoints lead to the fact that conventional AR-SLAM approaches cannot perform the nec-\nessary localization- and mapping-process for SLAM-Algorithm (a reference to section 3)\nwith the desired accuracy and resolution. Due to the desired integration of the presented\napproach into a serial automotive vehicle without any additional sensor technology, the\naim is to generate information about the depth and texturing of the environment based\nsolely on the installed camera. This camera is usually a mono-view-camera system that is\noften integrated into the rear view mirror of the automotive vehicle. Mono-view-camera\nsystems are established vehicle hardware, which is mostly used in low-priced series mod-\nels due to their compact design, high resolution, robustness, long-range and low cost.\nOn the other hand, high-priced vehicles use stereo-view-camera systems, which enable\nspatial vision like a human [15].\nIn addition to the low textured environment of the test track and the fast change of\n215\n\nscenery, other aspects such as weather inﬂuences like rain and the sun position, soiling\nof the windshield, bumps of the road surface and the lack of road markings have to be\nconsidered [16]. Furthermore, when augmented reality is used in automotive vehicles, the\nend-user is not the human driver, but an ECU. This implies that very high accuracy\nand a high realism, e.g. correct shadowing and occlusion of the augmented objects are\nrequired in the overall process [17]. In comparison to a human driver, the ECU must not\ndetect any diﬀerence between reality and the augmented reality, otherwise, the ECU will\nbe transferred to an error state. It is also highly relevant to consider the constant further\ndevelopment of ADAS, which persistently demands increased requirements for realistic\ntest scenarios. This approach aims to achieve the same driving behavior as in reality.\nIn addition to accuracy, the runtime of the overall algorithm is also of great impor-\ntance. Nowadays camera systems work with a frame rate of 30 to 60 Frames per Seconds\n[fps]. The resulting maximum overall runtime for handling one frame can be found in\nTable 1.\nFramerate\nMaximum runtime\n10 fps\n1\n10 s = 0.1000 s\n30 fps\n1\n30 s = 0.0333 s\n40 fps\n1\n40 s = 0.0250 s\n45 fps\n1\n45 s = 0.0222 s\n50 fps\n1\n50 s = 0.0200 s\n60 fps\n1\n60 s = 0.0167 s\nTable 1. Several Framerate and the according maximum runtime.\nFor a successful evaluation of ADAS-test scenarios, the AR system must be able to orient\nitself in the environment very accurately [18]. One cause is the missing feedback about\nthe impact intensity of test dummies when crashing them. For this reason, it is necessary\nto know the exact position of the car on the test track to calculate the intensity of the\nimpact based on the braking distance. When using Euro NCAP test scenarios, velocities\nup to\n130 km\nh\n\u0010= 36.111 m\ns\n(1)\nare tested. The AR algorithm must have a faster runtime compared to the speed of the\ncamera system. The distance d the vehicle covers within a frame at any given velocity\nand framerate can be calculated by:\nd =\nvV ehicle [ m\ns ]\nFramerate [ frames\ns\n]\n(2)\nAt a speed of 130 km\nh and a camera framerate of 30 fps, the vehicle travels\nd = 36.111 [ m\ns ]\n30 [ frames\ns\n]\n= 1.204\nm\nframe.\n(3)\n216\n\nAccordingly, for a framerate of 60 fps at the same speed, a distance of\nd = 36.111 [ m\ns ]\n60 [ frames\ns\n]\n= 0.602\nm\nframe\n(4)\nis covered. A deceleration of one frame means a deviation of the test results of 0.602 to\n1.204 meters.\nBased on the high speed of the car and the camera, and the high need for precision in\nobject placement, it is clear that the requirements for this application of Augmented\nReality are far more strict than for the usual application for human users.\n3\nDevelopment Approach\nSimultaneous Localization and Mapping (SLAM) is a method for obtaining the 3D struc-\nture of an unknown environment and sensor motion in the environment. This system\nwas initially intended to achieve autonomous control of robots [19]. Due to continuous\ndevelopment, SLAM-based applications have also found their way into mobile device ap-\nplications and self-driving cars. To increase the accuracy of SLAM algorithms, various\napproaches allow the integration of diﬀerent sensors, such as laser range sensors, rotary\nencoders, inertial sensors, Global Position Systems (GPS), and cameras. These algo-\nrithms are summarized in the following papers [20] [21] [22] [23]. Since cameras primarily\nare used for the most part in automotive vehicles, the approach presented in this paper\nis based on a subcategory of SLAM algorithms - visual Simultaneous Localization and\nMapping (vSLAM). In the following section, the State of the Art for vSLAM-techniques\nare described. Based on these methods a new approach for AR using SLAM in automotive\nvehicles is presented.\n3.1\nState of the Art - Visual Simultaneous Localization and Mapping\n(vSLAM)\nThe approach of vSLAM uses only visual inputs to perform localisation and mapping.\nThis means that no vehicle sensors other than the vehicles camera system are needed to\ncreate a 3D model of the environment thus making this approach more ﬂexible than LI-\nDARS, Radars, and Ultrasonics. The framework of vSLAM-algorithm is mainly composed\nof three basic modules: Initialization, Tracking, Mapping, and two additional modules:\nRelocalization and Global Map Optimization (including Loop Closing) [24].\nBasic modules:\n1. Initialization: To use vSLAM, the fundamental step is to deﬁne a speciﬁc coordinate\nsystem for camera position estimation and 3D reconstruction in an unknown environ-\nment. Therefore, the global coordinate system should be deﬁned ﬁrst during initializa-\ntion. A part of the environment is therefore reconstructed as an initial map in the global\ncoordinate system [24].\n2. Tracking: After the initialization process, tracking and mapping are performed. Track-\ning involves following the reconstructed map in the image to continuously estimate the\ncamera position of the image to the map. For this purpose, distinctive matches between\nthe captured image and the created map are ﬁrst determined by feature matching or\nfeature tracking in the image [24].\n217\n\n3. Mapping: The mapping process expands the map by understanding and calculating\nthe 3D structure of an environment when the camera detects unknown regions where\nmapping has not been done before [24].\nAdditional modules:\n4. Relocalization: When tracking has failed, Relocalization is required. Reasons for this\ncan be, among others, fast camera movements. In this case, relocalization makes it pos-\nsible to recompute the current camera position about the reconstructed map [24].\n5. Global Map Optimization (including Loop Closing): The map usually contains a cu-\nmulative estimation error corresponding to the distance of the camera movement. To\neliminate this error, Global Map Optimization is usually performed. In this method, the\nmap is reﬁned considering the consistency of the whole map information. If previously\nrecorded map elements are recognized, loops are closed and the cumulative estimation\nerror can be corrected from the beginning to the present. Loop Closing is a method for\nobtaining reference information. While closing loops, a closed loop is ﬁrst searched by\ncomparing a current image with previously acquired images. Generally, relocalization is\nused to recover the camera position and loop detection is used to obtain a geometrically\nconsistent map. Pose Graph Optimization is widely used to suppress the cumulative error\nby optimizing the camera positions. Bundle Adjustment (BA) is also used to minimize\nthe map reprojection error by optimizing the map and the camera positions. In large\nenvironments, this optimization method is used to eﬃciently minimize estimation errors.\nIn small environments, BA can be performed without loop closure as the cumulative\nerror is small [24].\nFor the use of SLAM in automotive vehicles and the associated properties such as fast\nscene changes and low texturing of the environment, various approaches are available\nusing vSLAM-Algorithm, which can be found in [14]. In this paper, diﬀerent SLAM ap-\nproaches are compared based on accuracy and robustness, among others. Some other\napproaches, which are not compared in [14] but seem promising for the presented ap-\nproach in this paper, are brieﬂy described in the following:\nORB-SLAM:\nThe ORB-SLAM algorithm was ﬁrst presented in 2015 and seems to be the current\nstate of the art as it has higher accuracy than comparable SLAM algorithms [25]. Here,\nORB-SLAM represents a complete SLAM system for monocular, stereo, and RGB-D\ncameras. The system operates in real-time and achieves remarkable results in terms of\naccuracy and robustness in a variety of diﬀerent environments. ORB-SLAM is used for\nindoor sequences, drones, and cars driving through a city. The ORB-SLAM consists of\nthree main parallel threads: Tracking, Local Mapping, and Loop Closing. A fourth thread\ncan be created to execute the BA after a closed loop. This algorithm is a feature-based\napproach, which represents the detected points in a three-dimensional MapPoint [14].\nFigure 3 shows a MapPoint, which is created using image sequences captured in-house.\nThe MapPoint shows a recognized house in an urban environment using ORB2-SLAM.\nVarious advancements and improvements in terms of accuracy, robustness, etc. can be\nfound in further developments based on this approach of ORB-SLAM (ORB2-SLAM\n[18] and ORB3-SLAM [14]). While the performance of ORB-SLAM is impressive in well-\nstructured sequences, error conditions can occur in poorly structured sequences or when\nfeature points temporarily disappear, e.g., due to motion blur [26].\n218\n\nFig. 3. MapPoint with red dots and green line for trajectory based on ORB2 with own image\nmaterial in an urban environment\nDS-SLAM:\nORB-SLAM exhibits excellent performance in most practical situations. However, some\nproblems are not solved by the ORB-SLAM. First, the ORB-SLAM algorithm exhibits\nweaknesses in exceptionally dynamic and harsh environments. On the other hand, the\nmap model created is based on geometric information like the MapPoint in Figure 3. This\nMapPoint does not provide a higher-level understanding of the environment. The DS-\nSLAM approach, ﬁrstly presented in 2018, combines the ORB-SLAM with the Semantic\nSegmentation approach using artiﬁcial intelligence to achieve a higher-level understand-\ning of the environment. This approach further intends to increase the robustness of the\nSLAM system in dynamic environments. Based on ORB2-SLAM the DS-SLAM consists\nof basic SLAM-Modules like Tracking, Mapping, and Loop-Closing. Furthermore, DS-\nSLAM has two additional threads like Sementic Segmentation and Dense Map Creation.\nUsing these additional threads improves the localization and mapping concerning robust-\nness and accuracy in dynamic scenarios [26].\nPL-SLAM:\nAnother approach to increase accuracy in poorly textured environments is PL-SLAM\n(Point and Line Simultaneous Localisation and Mapping), ﬁrstly presented in 2017. PL-\nSLAM extends the point-based approach known from ORB-SLAM with a line-based\nmethod. This line-based approach enables an improvement in terms of occlusions and\nfalse detections. Besides the improvement in poorly textured environments, this ap-\nproach also shows increased performance in very well-textured environments, without\nsigniﬁcantly degrading the eﬃciency of this algorithm. Like the ORB-SLAM algorithm,\n219\n\nthe PL-SLAM has the basic SLAM modules for initialization, tracking, mapping, and\nloop closing. The extension of this approach is to use line-based algorithms in parallel\nwith the point-based algorithms in each SLAM module. This approach ensures that the\nresulting map is more valuable and more diverse in 3D elements to derive important\nhigher-level scene structures such as planes, voids, ground surfaces, etc. [27].\nBased on the presented approaches in this section, in a further step single features of\nthese SLAM-Algorithm are to extend and insert in a new approach for testing of ADAS\nin automotive vehicles. An introduction for the next steps is presented in the following\nsection.\n3.2\nUse of Object Detection and vSLAM for AR in Automotive\nFor the use of AR in automotive vehicles, the approach should consist of using a state-\nof-the-art method and extending the feature point detection with an object detection.\nThis should improve the following criteria:\n- Robustness against blurred eﬀects.\n- Increase the accuracy of the 3D environment through improved depth information.\n- Detect occlusions and improve 3D environment detail.\n- Achieve robustness against weather eﬀects.\n- Increase realism for the control unit as end device.\n- Increase computational speed with improved accuracy.\n- Achieve higher-level understanding of the environment.\nTo achieve these criteria, the following features are to be extracted from the vSLAM\napproaches presented and examined in more detail for further research investigations:\nORB-SLAM:\n- State-of-the-Art method to generating a MapPoint.\n- Feature-Point approach should represent the basic framework.\n- Selection of which criteria can be used from ORB or the further developments ORB2\nand ORB3 based on it.\nDS-SLAM:\n- Approach of AI and Image Segmentation to generate an Object Detection.\n- Creation of a Dense-Map for overlay on MapPoint.\n- Achieve a higher-level understanding of the environment.\nPL-SLAM:\n- Based on edges to improve occlusions and improve object detection.\n- Better 3D-reconstruction of objects through the detection of edges, points and lines.\n- Improved realism through correct lighting and shadowing of augmented objects.\nBy cleverly combining the individual elements of the previously known SLAM algorithms,\naugmented reality in automobiles could be used in high-speed ADAS tests. In addition\nto the increased computing speed, increased accuracy should be achieved to be able to\n220\n\nFig. 4. Top: Original Image in an Urban Environment;\nbottom left: Feature-Point-Detection using ORB2-SLAM-Algorithm;\nbottom right: Object Detection using Image Segmentation\nmake a meaningful assessment of the performance of the ADAS tests. Figure 4 shows the\noriginal image, the feature point detection of the ORB2 algorithm, and the image seg-\nmentation used so far for object detection. The next step is to combine these approaches\nusing AI.\n4\nConclusion\nIn this paper, we have proposed an approach to use Augmented Reality in automotive\nvehicles. We modeled the problem of creating an urban environment to use AR for testing\nin high-speed ADAS. Our approach is based on a combination of vSLAM-Algorithms like\nORB-SLAM, DS-SLAM, and PL-SLAM within the combination of Artiﬁcial Intelligence\nto use Object Detection. This should help to generate a better overall performance con-\ncerning computing speed and accuracy.\nThe creation of a virtual 3D environment with a superior understanding of the individual\nobjects should, in a further step, make it possible to augment other sensors such as the\ncar’s radar and lidar with objects in addition to the camera data. This should once again\nincrease the overall performance of the entire system.\n221\n\nReferences\n1. Bengler, K., Dietmayer, K., Farber, B., Maurer, M., Stiller, C., Winner, H.: Three decades of\ndriver assistance systems: Review and future perspectives. IEEE Intelligent Transportation\nSystems Magazine (2014) 6–22\n2. Schuldt, F., Saust, F., Lichte, B., Maurer, M., Scholz, S.: Eﬃziente systematische testgener-\nierung f¨ur fahrerassistenzsysteme in virtuellen umgebungen. (2013)\n3. Kim, B.J., Lee, S.B.: A study on the evaluation method of autonomous emergency vehicle\nbraking for pedestrians test using monocular cameras. Applied Sciences (2020)\n4. Miquet, C., Schwab, S., Pfeﬀer, R., Zofka, M., B¨ar, T., Schamm, T., Z¨ollner, J.: New test\nmethod for reproducible real-time tests of adas ecus: ”vehicle-in-the-loop” connects real-\nworld vehicle with the virtual world. (06 2014)\n5. Azuma, R.T.: A Survey of Augmented Reality. Presence: Teleoperators and Virtual Envi-\nronments (08 1997) 355–385\n6. Kr¨uger, J., M¨ollers, F., Vogelgesang, S.: Pok´emon go. Informatik-Spektrum (12 2016)\n7. State, A., Hirota, G., Chen, D., Garrett, W., Livingston, M.: Superior augmented reality\nregistration by integrating landmark tracking and magnetic tracking. proceedings of ACM\nSiggraph ’94 (Computer Graphics) (08 1996)\n8. Saputra, M.R.U., Markham, A., Trigoni, N.:\nVisual slam and structure from motion in\ndynamic environments: A survey. ACM Comput. Surv. (February 2018)\n9. Afanasyev, I., Sagitov, A., Magid, E.: Ros-based slam for a gazebo-simulated mobile robot\nin image-based 3d model of indoor environment. (10 2015) 273–283\n10. Chekhlov, D., Gee, A.P., Calway, A., Mayol-Cuevas, W.:\nNinja on a plane: Automatic\ndiscovery of physical planes for augmented reality using visual slam. In: 2007 6th IEEE and\nACM International Symposium on Mixed and Augmented Reality. (2007) 153–156\n11. Juan, M., Alcaniz, M., Monserrat, C., Botella, C., Banos, R., Guerrero, B.: Using augmented\nreality to treat phobias. IEEE Computer Graphics and Applications (2005) 31–37\n12. Kersten-Oertel, M., Gerard, I., Drouin, S., Mok, K., Sirhan, D., Sinclair, D., Collins, L.:\nAugmented reality in neurovascular surgery: feasibility and ﬁrst uses in the operating room.\nInternational journal of computer assisted radiology and surgery (02 2015)\n13. Livingston, M., Rosenblum, L., Brown, D., Schmidt, G., Julier, S., Baillot, Y., Swan, J., Ai,\nZ., Maassel, P. In: Military Applications of Augmented Reality. (07 2011) 671–706\n14. Campos, C., Elvira, R., Rodr´ıguez, J.J.G., M. Montiel, J.M., D. Tard´os, J.: Orb-slam3: An\naccurate open-source library for visual, visual–inertial, and multimap slam. IEEE Transac-\ntions on Robotics (2021) 1–17\n15. Winner, H., Hakuli, S., Lotz, F., Singer, C.: Handbook of driver assistance systems - basic\ninformation, components and systems for active safety and comfort. (08 2016) 1592\n16. Dabral, S., Kamath, S., Appia, V., Mody, M., Zhang, B., Batur, U.: Trends in camera based\nautomotive driver assistance systems (adas). (08 2014) 1110–1115\n17. Nilsson, J., Odblom, A., Fredriksson, J., Zafar, A. In: Using Augmentation Techniques for\nPerformance Evaluation in Automotive Safety. (07 2011)\n18. Mur-Artal, R., Tard´os, J.D.: ORB-SLAM2: an open-source SLAM system for monocular,\nstereo and RGB-D cameras. CoRR (2016)\n19. Chatila, R., Laumond, J.: Position referencing and consistent world modeling for mobile\nrobots. In: Proceedings. 1985 IEEE International Conference on Robotics and Automation.\nVolume 2. (1985) 138–145\n20. Durrant-Whyte, H., Bailey, T.:\nSimultaneous localization and mapping: part i.\nIEEE\nRobotics Automation Magazine (2006) 99–110\n21. Bailey, T., Durrant-Whyte, H.: Simultaneous localization and mapping (slam): part ii. IEEE\nRobotics Automation Magazine (2006) 108–117\n22. Cadena, C., Carlone, L., Carrillo, H., Latif, Y., Scaramuzza, D., Neira, J., Reid, I., Leonard,\nJ.: Simultaneous localization and mapping: Present, future, and the robust-perception age.\nIEEE Transactions on Robotics (06 2016)\n23. Aulinas, J., P´etillot, Y., Salvi, J., Llad´o, X.: The slam problem: a survey. In: CCIA. (2008)\n222\n\n24. Taketomi, T., Uchiyama, H., Ikeda, S.: Visual slam algorithms: a survey from 2010 to 2016.\nIPSJ Transactions on Computer Vision and Applications (12 2017)\n25. Mur-Artal, R., Montiel, J., Tardos, J.: Orb-slam: a versatile and accurate monocular slam\nsystem. IEEE Transactions on Robotics (10 2015) 1147 – 1163\n26. Yu, C., Liu, Z., Liu, X., Xie, F., Yang, Y., Wei, Q., Qiao, F.: DS-SLAM: A semantic visual\nSLAM towards dynamic environments. CoRR (2018)\n27. Pumarola, A., Vakhitov, A., Agudo, A., Sanfeliu, A., Moreno-Noguer, F.: Pl-slam: Real-time\nmonocular visual slam with points and lines. (06 2017)\n223\n\nImproving Temporal Consistency in Aerial Based\nCrowd Monitoring Using Bayes Filters\nJan Calvin Kramer1, Thomas Golda2, Jonas Hansert1, and Thomas Schlegel1\n1 Karlsruhe University of Applied Sciences, Institute of Ubiquitous Mobility Systems IUMS\n{Jan Calvin.Kramer, Jonas.Hansert, Thomas.Schlegel}@h-ka.de\n2 Fraunhofer Institute for Optronics, System Technologies and Image Exploitation IOSB\nThomas.Golda@iosb.fraunhofer.de\nAbstract. In order to monitor mass events, crowd managers continuously require\nreliable measurements of the crowd count. For this purpose, a variety of deep\nlearning algorithms has been developed. Most of these so-called crowd counting\nalgorithms return good results for still imagery but return oscillating crowd counts\nfor video data. This is because, most crowd counting algorithms evaluate video\ndata frame by frame and ignore the temporal relation between adjacent frames. In\nthis paper, a variety of Bayesian ﬁlters is presented that successfully smooth the\noscillating counts which in turn can lead crowd managers to trust the system more.\nThe proposed ﬁlters work on top of the crowd counting algorithms’ estimates.\nThus, they can be easily used with any existing crowd counting algorithm that\noutputs a density map for a given input image.\nKeywords: crowd counting, crowd count, density map, crowd manager, mass\npanic, video data, bayesian ﬁlters, kalman ﬁlter, particle ﬁlter, aerial imagery\n1\nIntroduction\nMass events take place every day all over the world. Despite the joy these events bring to\nmany people, they always come with the threat of turning into a stampede. To prevent\nthis, crowd managers must be aware of the current crowd count and density and take\nthe right measures in time.\nFor this purpose, a variety of algorithms have been developed since 2006. These so-called\ncrowd counting algorithms take an image as their input and estimate the amount of\npeople in the picture. Recent crowd counting algorithms go even further. Using Convo-\nlutional Neural Networks, they estimate a whole density map for a given input image.\nThe density map itself holds the information of the crowd count that can be obtained\nby adding up the density map’s pixel values. Gao et al. give a good insight into current\nadvances and the huge amount of diﬀerent crowd counting algorithms that exist [1].\nDespite the many advances, current crowd counting algorithms do not consider the tem-\nporal relation between adjacent frames. Most crowd counting algorithms evaluate video\ndata frame by frame and ignore the temporal relation. This often leads to ﬂuctuating\ncounts that the crowd manager cannot rely on. Only a few isolated approaches, e.g.,\nthe Temporal Aware Network [2], exist, that try to incorporate the temporal relation\nof adjacent frames. In contrast to their approach of presenting a new architecture that\nincorporates the temporal relation, we developed a variety of Bayesian ﬁlters that work\non top of the counts estimated by current crowd counting algorithms. The ﬁlters can be\nused with any modern crowd counting algorithm that outputs a density map for a given\ninput image.\n224\n\n2\nRelated Work\nThe following section gives a brief overview of the existing technologies this work builds\nupon. Firstly, two crowd counting algorithms, i.e., the CSRNet and MRCNet, are intro-\nduced. The crowd counters are used throughout this work to estimate the crowd counts\nof diﬀerent data sets and to reproduce the problem of having oscillating counts. Secondly,\nBayesian ﬁlters on which the developed concepts are based are brieﬂy discussed.\n2.1\nCSRNet\nThe Congested Scene Recognition Network (called CSRNet hereafter) is a crowd counting\nalgorithm developed by Li et al. in 2018 [3]. The network is divided into two parts, i.e.\nthe front- and back-end. Both parts exclusively rely on (dilated) convolutional and max\npooling layers.\nThe CSRNet’s front-end consists of the VGG-16’s ﬁrst 10 layers. It extracts several\nfeatures from an input image. The features are stored in a so-called feature map whose\nresolution is 1/8 of the resolution of the input image. The feature map is further processed\nby the back-end of the network. The back-end uses several dilated convolutional layers.\nThus, it is able to extract even deeper features without further shrinking the resolution\nof the feature map. In order to output a density map that has the same size as the\ninput image, the CSRNet uses bilinear interpolation with a factor of 8. A more detailed\ndescription can be taken from [3].\n2.2\nMRCNet\nThe Multi-Resolution Crowd Network (called MRCNet hereafter) is a crowd counting\nalgorithm developed by Bahmanyar et al. from the German Aerospace Center [4]. The\nnetwork relies on an encoder-decoder-structure. Analogous to the front-end of the CSR-\nNet, the encoder of the MRCNet relies on the VGG-16. Yet, it does not only use the ﬁrst\nten layers but the ﬁrst ﬁve CNN blocks of the VGG-16 that consist of 13 convolutional\nlayers and ﬁve max-pooling layers in total. Since it uses more pooling layers, the size of\nthe outputted feature map is 1/32 the size of the input image. Such a drastic reduction of\nthe resolution can accidentally lead to people being removed. To prevent this, the feature\nmaps at diﬀerent stages of the encoder are added element-wise to the feature maps of\nthe decoder.\nThe decoder of the MRCNet consists of ﬁve CNN blocks as well. However, instead of\nusing pooling layers at the end of each CNN block, the decoder uses up-sampling layers.\nEach up-sampling layer increases the size of the feature map by a factor of two. Thus, at\nthe end of the decoder, the MRCNet outputs a density map that has the same resolution\nas the input. Prior to outputting the density map, the MRCNet outputs a feature map\nwhose resolution is 1/4 the resolution of the input image. This feature map is used to\nestimate the overall amount of people in an image. By estimating the amount of people\nat an early stage of the decoder, the remaining part of the decoder can be further used\nto output a full-resolution density map that has a higher localization precision. Further\ninformation about the MRCNet can be taken from [4].\n2.3\nBayesian Filters\nBayesian ﬁlters have been around for quite a while. In short, they are a set of algorithms\nthat iteratively estimate the hidden state of a system, e.g., the current crowd count, using\n225\n\nimprecise measurements, e.g., estimated density maps, and a model of the system state,\ne.g., a traﬃc ﬂow model [5].\nThe Kalman and particle ﬁlter are a subset of the Bayesian ﬁlters. They diﬀer in that the\nKalman ﬁlter returns an optimal solution under certain restrictions whereas the particle\nﬁlter returns a good approximation while being less restrictive. The Kalman ﬁlter assumes\nthe underlying model to be linear and discrete in the time domain. Furthermore, the\nprocess and measurement noise are assumed to be Gaussian with a zero mean. If these\nrestrictions are not fully complied with, the particle ﬁlter is likely to return even better\nresults than the Kalman ﬁlter.\nA thorough understanding of both ﬁlters is necessary to fully understand the concepts\nthat follow. A good insight is given by [5], [6] and [7].\n3\nConcepts\nThe following chapter explains the developed concepts of this paper.\n3.1\nKalman Filter\nA complete concept of a Kalman ﬁlter requires the deﬁnition of the state vector, state\ntransition matrix, measurement matrix, process noise variance and measurement noise\nvariance.\nTo not further increase the computational costs that come with crowd counting, the state\nvector considers only the crowd count and not the density map as a whole. The crowd\ncount is further expected not to change between two consecutive frames. This assumption\nleads to a linear state transition matrix that only consists of the value 1.\nThe accuracy of this assumption depends mainly on two variables, i.e., the area under\nconsideration (denoted by A) and the frame rate of the video (denoted by f). For either\nA going towards zero or f going towards inﬁnity, the assumption that the crowd count\ndoes not change becomes true. Thus, the concept is supposed to return better results for\nvideo data with higher frame rates and scenes with smaller areas. The scene of an image\ncan be artiﬁcially reduced by applying the Kalman ﬁlter on grids rather than the whole\nimage. Experiments testing this behavior are conducted in the next chapter.\nYet, in practice, neither A becomes zero nor f goes towards inﬁnity. This makes the\nstate transition matrix inaccurate. To model this error, one must deﬁne the process noise\nvariance (matrix).\nData-Driven Process Noise Variance. To get an estimate for the process noise\nvariance, the information given by the training data is used. Given the annotations of the\ntraining data, one can calculate the change of pedestrians between two consecutive frames\nby subtracting their annotated crowd counts. The calculated pedestrian changes can be\nﬁtted to a Gaussian curve in a next step. Assuming that the set of pedestrian changes is\nnormally distributed with a zero mean, the variance of the Gaussian curve corresponds\nto the actual variance of the process noise when the Kalman ﬁlter was applied to the\ntraining data. To obtain an estimate that generalises better on data sets with diﬀerent\ncrowd counts, one must consider the percentage change of pedestrians rather than the\nabsolute change. Let ck−1 and ck be the crowd count of two consecutive frames, then the\npercentage change of pedestrians (Δcrel) can be calculated as follows:\nΔcrel = ck −ck−1\nck−1\n(1)\n226\n\nFitting the percentage changes to a Gaussian curve results in a relative process noise\nvariance denoted by σrel. To retrieve an absolute estimate of the process noise variance,\none must iteratively multiply the relative variance with the previous posterior estimate\nof the Kalman ﬁlter and the frame rates’ ratio:\nσk = ¯ck−1 · σrel · ftraining\nftesting\n(2)\nData-Driven Measurement Noise Variance. To model the measurement noise vari-\nance, a data driven approach similar to the one of the process noise variance is used.\nYet, instead of computing pedestrian changes of the training data, the performance of\nthe crowd counter on the validation data is used. For this purpose, the percentage dif-\nferences between the crowd counter’s estimate and the corresponding ground truth is\ncalculated. The percentage diﬀerences are also ﬁtted to a Gaussian curve. Again, the\nvariance that is obtained using this approach is a relative value that must be iteratively\nmultiplied with the current measurement of the crowd count.\nData-Driven Observation Matrix. The ﬁnal parameter that must be modelled to\nobtain a complete concept of a Kalman ﬁlter is the observation matrix. The observation\nmatrix says how the measurements must be processed before they are further used within\nthe Kalman ﬁlter. Therefore, if the measurements that are passed to the Kalman ﬁlter\ndo not have an error with a zero mean, the observation matrix can theoretically be used\nto process the measurements in such a way that their error has a zero mean afterwards.\nFor this purpose, the Gaussian curve, that is obtained during the computation of the\nvariance of the measurement noise, is used. The relative mean denoted by μrel expresses\nthe average deviation of the ground truth from the estimates computed by the crowd\ncounter on the validation set. The oversimpliﬁed assumption that the error of the crowd\ncounter depends only on the crowd count, lets one use the simple term 1 −μrel for the\nobservation matrix.\n3.2\nParticle Filter\nThe developed particle ﬁlter only considers the crowd count as well. Yet, a more complex\nmodel for the state transition matrix is used. The model of choice is the macroscopic\nfundamental diagram of traﬃc ﬂow. Let Q be the ﬂow, Q∗be the ﬂow density, v0 be the\nvelocity of the pedestrians, w be the width through a gateway in meters, ρ and ρmax be\nthe current density and maximum density of pedestrians respectively, then the ﬂow of\npedestrians can be calculated as follows:\nQ∗(ρ) = ρ · v0 · (1 −\nρ\nρmax\n) [pedestrians\nm · s\n]\n(3)\nQ = Q∗· w [pedestrians\ns\n]\n(4)\nAssuming that ρmax = 5 pedestrians\nm2\nand v0 = 1.4 m\ns , it only requires the current density\nand the width of the gateway to estimate the ﬂow of pedestrians [8].\nUnfortunately, the traﬃc ﬂow model comes with a major restriction that it assumes the\nﬂow of pedestrians to be unidirectional. To loosen this restriction and obtain a more\nrealistic model, the ﬁlter does not assume the overall ﬂow of a scene to be unidirectional.\n227\n\nFlows of open borders that are located at the edge of the image are assumed to be\nindependent from each other and unidirectional. Making this assumption, three questions\narise that must be further clariﬁed:\n– How does the system detect open borders at the edge of the image?\n– What area around an open border must be considered to estimate the density at the\nopen border?\n– Given the ﬂow at an open border, how does one estimate the direction of ﬂow?\n3.3\nDetection of Open Borders\nThe annotation of open borders is assumed to be manually done by the user at the\nbeginning. The user is expected to create a mask for the ﬁrst frame of a video. Areas\nwhere people can possibly walk are supposed to be colored in white, whereas areas where\npeople cannot be, e.g., a frontage, must be colored in black. By reading in the mask\nand dividing the pixel values by 255, one obtains a 2 dimensional matrix of zeros and\nones. The information where the open borders are located can then be easily obtained\nby looking for non-zero sequences in its outermost rows and columns.\nIt is further used as an alternative measurement matrix that is multiplied element-wise\nwith incoming density maps. Subsequently, all elements of the resulting matrix are added\nup to obtain the crowd count that is further processed by the particle ﬁlter. It should be\nnoted that this approach might delete some rightfully annotated persons. This is because\nthe ground truth density maps that are used to train the crowd counting algorithms\nare created by blurring the given head annotations using a Gaussian kernel. Thus, per-\nsons standing nearby a frontage might overlay the frontage in a density map. Yet, this\napproach ensures that areas where pedestrians can impossibly be, e.g., a sea, are not\nwrongfully labeled by the crowd counter.\n3.4\nDetermining the Density at an Open Border\nTo estimate the density at an open border j at the previous point in time k −1, a\nrectangular cutout of the density map at k −1 is used. One side of the rectangle is the\nopen border itself, whereas the length of the other side is determined by the maximum\ndistance pedestrians are assumed to walk between two consecutive frames (called step\nsize hereafter). Let v0 be the pedestrian’s estimated velocity and Δt be the time between\ntwo consecutive frames, then the step size can be calculated as follows:\nΔsm = Δt · v0 [m]\n(5)\nΔspixel = Δt · v0 · 1/g [pixel]\n(6)\nwhere g corresponds to the ground sampling distance in meter per pixel.\nGiven a cutout of a density map, you can easily calculate the estimated crowd count\nwithin the cutout (denoted by cj,k−1) by adding up the pixel values of the cutout. Let\nfurther aj and bj be the lengths of the rectangle in pixels, then the density of the rect-\nangle can be calculated as follows:\nρj,k−1 =\ncj,k−1\naj · bj · g2 [pedestrians\nm2\n]\n(7)\n228\n\nwhere\n– ρj,k−1 [pedestrians/m2] is the density of the j’th rectangle at k-1,\n– g [m/pixel] is the ground sampling distance in meter per pixel.\nGiven ρj,k−1, the estimated ﬂow at the rectangle can be calculated using the equations 3\nand 4. It should be noted that w corresponds to the length of the open border in meters.\n3.5\nDetermining the Flow of Direction at an Open Border\nIt is further important to know whether pedestrians are either leaving or entering the\nscene. To estimate the ﬂow of direction, Gunnar Farneback’s algorithm is used. For each\nopen border, the algorithm is given an enlarged rectangular cutout of the previous and\ncurrent frame (not the density map!). The algorithm estimates the movement of the\npixels in the x- and y-direction within the enlarged rectangle [9]. Although the algorithm\ncalculates the movement in both directions, only one direction is relevant. For open\nborders located on the left or right of the image, the x-direction is of interest, whereas\nfor open borders located at the top or bottom of the image the movement in the y-\ndirection is of interest. By adding up the magnitudes by which the pixels are estimated\nto move along the relevant direction, you can obtain the magnitude of the pixels’ overall\nmovement along the direction. This value is denoted by mj,k−1 [pixel] in the following.\nWhereas negative values of mj,k−1 measured on the left side or at the bottom of an image\nindicate an outﬂow, they indicate an inﬂow when they are measured at the top or on the\nright side of an image.\nIf you would use a rectangle with the same dimensions as the rectangle used to determine\nthe density, you would probably run into the problem that the algorithm would detect no\nmovement when all pedestrians in the rectangle were to leave the rectangle. Therefore, an\nenlarged rectangle is used that holds pedestrians that are unable to leave the rectangle\nbetween two consecutive frames. One side of the enlarged rectangle is the open border\nitself. The other side is three times the step size.\nGiven this information, one can calculate the maximum magnitude the algorithm can\ndetect. Let wj be the length of the open border in pixel, then the maximum magnitude\ncan be calculated as follows:\nmj,max = 2 · Δs2\npixel · wj ·\n1\npixel2 [pixel]\n(8)\nWhereas Q is an estimate of the pedestrian’s ﬂow obtained by the fundamental traﬃc\nﬂow model, the ratio of mj,k−1/mj,max is an actual measurement. To get an estimate of\nthe pedestrian change, both values are combined:\nΔcj,k−1 = Q(ρj,k−1) · Δt · |mj,k−1|\nmj,max\n[pedestrians]\n(9)\nDepending on whether mj,k−1 indicates an outﬂow, Δcj,k−1 must be further multiplied\nby −1:\nΔcj,k−1 =\n\u0011\n+Δcj,k−1\ninflow\n−Δcj,k−1\noutflow\n(10)\nTo get the total change of pedestrians that is expected between the consecutive frames,\none must add up all Δc:\nΔck−1 =\nk=n\n\u0005\nj=1\nΔcj,k−1\n(11)\n229\n\nAdding this result to the samples of the posterior distribution at k−1 returns the samples\nof the priori distribution at k. Let ¯c i\nk−1 be the i’th sample from the posterior distribution\nat k −1, then the priori estimate of the i’th sample can be calculated as follows:\n¯c i\nk|k−1 = ¯c i\nk−1 + Δck−1\n(12)\n4\nExperiments\nTo train and test the crowd counting algorithms and ﬁlters, diﬀerent data sets are used.\nTable 1 shows the fundamental diﬀerences of the data sets. Tests in the aerial domain\nare conducted as follows: Firstly, the crowd counters are pre-trained using the DLR-ACD\ndata set [4] to overcome the lack of video data in that domain. Then, the crowd counting\nalgorithms are ﬁne-tuned on the VisDrone-CC2020 data set [10]. Subsequently, tests on\nthe VisDrone-CC2020 and AgoraSet [11] are conducted.\nFinally, tests on the WorldExpo’10 data set [12] are conducted. Due to the diﬀerent\ndomain, the crowd counting algorithms must be trained from scratch prior to testing.\nTable 1. Comparison of the data sets. N is the number of annotated frames; FPS is the number\nof annotated frames per second; GSD is the ground sampling distance in cm/pixel; AR is the\naverage resolution and IP is the interval in which the annotated crowd counts lay. The GSDs of\nthe AgoraSet and VisDrone-CC2020 are estimated values. All speciﬁed values of the AgoraSet\nrefer to the the frames 418-1936 of its ﬁrst sequence\nN\nFPS\nAerial\nArtiﬁcial\nGSD\nAR\nIP\nDLR-ACD\n33\nimage\ntrue\nfalse\n4.5 - 15\n3619×5226\n285-24368\nVisDrone-CC2020\n3360\n1\ntrue\nfalse\n0.118 - 0.706\n1920×1080\n25-421\nAgoraSet\n1519\n25\ntrue\ntrue\n4.5\n640×480\n1-180\nWorldExpo’10\n3980\n1/30\nfalse\nfalse\nperspective\n576×720\n1-253\nTo determine the performance of the ﬁlters, the Mean Absolute Error of the estimated\ncrowd counts and the Mean Absolute Error of the estimated crowd counts’ slope is\ncalculated. Let c be the crowd count and ¯c be the estimated crowd count, then the MAE\nof the estimated crowd count and its slope can be calculated as follows:\nMAEcrowd count =\nk=n\n\nk=1\n|ck −¯ck|\nn\n(13)\nMAEslope =\nk=n\n\nk=2\n|ck −ck−1 −(¯ck −¯ck−1)|\nn −1\n(14)\nBoth MAEs indicate better performance for values closer to zero and worse performance\nfor higher values. They diﬀer in that the MAEslope shows how good the ﬁlters smooth\nthe data and the MAEcrowd count shows how close the counts are to the actual crowd\ncount. Since the focus of this paper lays on smoothing the counts, it is the overriding\ngoal to reduce the MAEslope without increasing the MAEcrowd count.\n230\n\n4.1\nVisDrone-CC2020\nTests on the VisDrone data set are conducted to test the behavior of the ﬁlters on a data\nset from the aerial domain the crowd counters are ﬁne-tuned on.\nTable 2 shows the results. In general, the ﬁlters smooth the temporal courses of the\nestimated crowd counts. This is shown by a decrease in the MAEslope. The values of the\nraw crowd counters are reduced by 56% to 65%.\nThe Kalman ﬁlter applied to the whole frame smooths the data the most. This contradicts\nthe initial assumption that the Kalman ﬁlter works better on smaller grids. The problem\nhere is that the pixel values of the estimated density maps are not consistently positive.\nTherefore, if the density map is divided into smaller grids, some of the grids contain\nnegative crowd counts. If one inputs these negative values into a Kalman ﬁlter, the ﬁlter\noutputs arbitrarily high or low numbers over time. To solve this problem, the count is\nset to zero, the process noise variance is set to 1, and the measurement noise variance is\nset to 1000 when a negative crowd count occurs.\nAlthough this approach makes the grid-based Kalman ﬁlter work, it does not enable the\nﬁlter to develop its full potential. Yet, it comes with a more than welcome side eﬀect.\nBy setting the negative crowd counts of single grids to zero, the grid-based Kalman ﬁlter\nreturns the best results for the MAEcrowd count.\nThe displayed results of the grid-based Kalman ﬁlter are obtained using 1x1 meter grids.\nTests with 3x3 and 10x10 meter grids were conducted. Yet, it turned out that on the\nVisDrone as well as on the other data sets, 1x1 meter grids return the best results.\nTable 2. Results on the VisDrone-CC2020 data set. The MAEslope is reduced by 56% to 65%.\nAll ﬁlters successfully smooth the estimated crowd counts\nMAEcrowd count\nMAEslope\nCSRNet\nMRCNet\nCSRNet\nMRCNet\nUnﬁltered\n29.30\n21.08\n4.23\n3.98\nKalman\n30.89 (+5%)\n18.31 (-13%)\n1.47 (-65%)\n1.44 (-64%)\nKalmangrid\n25.78 (-12%)\n16.52 (-22%)\n1.60 (-62%)\n1.75 (-56%)\nParticle\n28.63 (-2%)\n17.00 (-19%)\n1.59 (-62%)\n1.62 (-59%)\n4.2\nAgoraSet\nIn order to see how the ﬁlters work on data with more realistic frame rates (25 fps),\ntests on the AgoraSet are conducted. Although its frame rate covers a more realistic use\ncase, its data is artiﬁcial. To the best of our knowledge, non-artiﬁcial alternatives are not\npublicly available due to the high eﬀort that comes with annotating such data sets.\nWhen speaking of the AgoraSet only the ﬁrst sequence of the AgoraSet is meant. This\nis because, all sequences cover a pretty similar scenario from a crowd counting perspec-\ntive. The backgrounds are monotonous and do not signiﬁcantly diﬀer. In addition to\nthat, pretty much all of the sequences’ temporal courses of the crowd count resemble a\nparabola with a downward opening.\nThe tests were directly conducted after the tests on the VisDrone-CC2020 data set with-\nout ﬁne-tuning the crowd counters on the AgoraSet or reconﬁguring the Kalman ﬁlter’s\n231\n\nparameters. Nevertheless, the ﬁlters are able to smooth the temporal courses of the crowd\ncount even more. The MAEslope is reduced by 68% to 88%. Again, the Kalman ﬁlter ap-\nplied to the whole image smooths the data the best. Yet, the particle and grid-based\nKalman ﬁlter return good results as well. If only the MAEcrowd count is considered, the\nparticle ﬁlter returns the best results.\nThe results show that the ﬁlters smooth the data even better on data sets with higher\nframe rates. In addition to that, the tests indicate that the initialization of the Kalman\nﬁlter’s parameters on a previous data set does not impair its performance on another\ndata set.\nTable 3. Results on the ﬁrst sequence of the AgoraSet. Although the crowd counting algorithms\nand ﬁlters have not seen data from the AgoraSet before, the ﬁlters are able to smooth the\nestimated crowd counts even more\nMAEcrowd count\nMAEslope\nCSRNet\nMRCNet\nCSRNet\nMRCNet\nUnﬁltered\n26.06\n43.72\n1.5\n2.01\nKalman\n27.55 (+6%)\n44.45 (+2%)\n0.27 (-82%)\n0.25 (-88%)\nKalmangrid\n25.49 (-2%)\n43.98 (+1%)\n0.45 (-70%)\n0.65 (-68%)\nParticle\n25.40 (-3%)\n43.84 (±0%)\n0.39 (-74%)\n0.42 (-79%)\n4.3\nWorldExpo’10\nFinally, tests are conducted on a perspective data set that has a signiﬁcantly lower frame\nrate than the previous ones. Since the WorldExpo’10 data set does not contain aerial im-\nages, the particle ﬁlter as conceptualised in this paper cannot be applied. The grid-based\nKalman ﬁlter also runs into the problem that its hard to divide a perspective image into\nsame sized grids. To handle this problem, the face length of a person in the front and\nback of a random frame is measured. Assuming that the average face length of a person\nis 23 centimeters, one can calculate two GSDs - one for the front and one for the back\nof the image. The average of the two GSDs is used to determine the grids. It should be\nnoted that this approach is only a hot-ﬁx to make the ﬁlter work and that grids in the\nback still cover larger areas than grids in the front.\nTable 4 shows the results. It can be seen that the MAEslope does not signiﬁcantly improve.\nHowever, this is much more due to the low frame rate and not the perspective of the\ndata set. The time between two frames of the WorldExpo’10 data set is 30 seconds. Due\nto the large time interval between the frames, it can be said that consecutive frames do\nnot hold any temporal information that could be incorporated by the ﬁlters. To sum up,\nthe results stress out the importance of the frame rate when applying the ﬁlters. If the\ntime interval between consecutive frames becomes too large, the ﬁlters do not improve\nthe estimates of the crowd counters. Yet, since a frame rate of 1/30 fps does not capture\na realistic scenario, the results of the tests on the WorldExpo’10 do not contradict with\nthe applicability of the ﬁlters in a real-life situation.\n232\n\nTable 4. Results on the WorldExpo’10 data set. Due to the low frame rate, adjacent frames do\nnot share temporal information that can be incorporated by the ﬁlters\nMAEcrowd count\nMAEslope\nCSRNet\nMRCNet\nCSRNet\nMRCNet\nUnﬁltered\n17.15\n18.12\n8.8\n8.6\nKalman\n19.74 (+15%)\n18.14 (±0%)\n8.2 (-7%)\n8.4 (-2%)\nKalmangrid\n19.73 (+15%)\n17.95 (-1%)\n9.54 (+8%)\n8.9 (+3%)\n5\nConclusions\nThe paper shows that the oscillating counts estimated by current crowd counters for\nvideo data can be smoothed using Bayesian ﬁlters. As a measurement of the false oscil-\nlation the MAEslope is introduced. Provided that the frame rate of a data set is large\nenough that consecutive frames share temporal information, all of the three ﬁlters are\nable to smooth the temporal course of the crowd count without signiﬁcantly increasing\nthe MAEcrowd count. Future work may further address the maturation of the concepts.\nThe development of Bayesian ﬁlters that smooth the estimated crowd counts is still in its\ninfancy. A variety of traﬃc ﬂow models exist that can be used to develop new concepts\nthat may smooth the crowd counts even more.\nReferences\n1. Gao, G., Gao, J., Liu, Q., Wang, Q., Wang, Y.: Cnn-based density estimation and crowd\ncounting: A survey (2020)\n2. Wu, X., Xu, B., Zheng, Y., Ye, H., Yang, J., He, L.: Fast video crowd counting with a\ntemporal aware network. Neurocomputing 403 (Aug 2020) 13–20\n3. Li, Y., Zhang, X., Chen, D.: Csrnet: Dilated convolutional neural networks for understanding\nthe highly congested scenes (2018)\n4. Bahmanyar, R., Vig, E., Reinartz, P.: Mrcnet: Crowd counting and density map estimation\nin aerial and ground imagery (2019)\n5. Elmar, G.:\nBayes–Filter zur Genauigkeitsverbesserung und Unsicherheitsermittlung von\ndynamischen Koordinatenmessungen. dissertation, Friedrich–Alexander–Universit¨at (2014)\n6. Rhudy, M.B., Salguero, R.A., Holappa, K.: A KALMAN FILTERING TUTORIAL FOR\nUNDERGRADUATE STUDENTS. (February 2017) Accessed: 2021-01-23.\n7. Simon, Maskel an Neil, G.: A tutorial on particle ﬁlters for on-line nonlinear/non-gaussian\nBayesian tracking - Target Tracking. (2001)\n8. Treiber, M.: Skript zur vorlesung verkehrsdynamik und -simulation (2017)\n9. Farnebaeck, G.: Two-frame motion estimation based on polynomial expansion. In: Scandi-\nnavian Conference on Image Analysis. Volume 2749. (2003) 363–370\n10. Du, D., Wen, L., Zhu, P., Fan, H., Hu, Q., Ling, H., Shah, M., ..., Zhao, Z.: Visdrone-cc2020:\nThe vision meets drone crowd counting challenge results (2021)\n11. Allain, P., Courty, N., Corpetti, T.: Agoraset: a dataset for crowd video analysis. In: 1st\nICPR International Workshop on Pattern Recognition and Crowd Analysis. (2012)\n12. Zhang, C., Li, H., Wang, X., Yang, X.: Cross-scene crowd counting via deep convolutional\nneural networks. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). (2015) 833–841\n233\n\nPotentials of Semantic Image Segmentation Using\nVisual Attention Networks for People with Dementia\nLiane Meßmer and Christoph Reich\nHochschule Furtwangen – University of Applied Sciences\nl.messmer@hs-furtwangen.de\nchristoph.reich@hs-furtwangen.de\nAbstract. Due to the increasing number of dementia patients, it is time to in-\nclude the care sector in digitization as well. Digital media, for example, can be\nused on tablets in memory care and have considerable potential for reminiscence\ntherapy for people with dementia. The time consuming assembly of digital media\ncontent has to be automated for the caretakers.\nThis work analyzes the potentials of semantic image segmentation with Visual\nAttention Networks for reminiscence therapy sessions. These approaches enable\nthe selection of digital images to satisfy the patients individual experience and\nbiographically. A detailed comparison of various Visual Attention Networks eval-\nuated by the BLEU score is shown. The most promising networks for semantic\nimage segmentation are VGG16 and VGG19.\nKeywords: Dementia, Alzheimer, Visual Attention Network, CNN, RNN, LSTM,\nGRU, Inceptionv3, VGG, ResNet, Semantic Segmentation, Natural Language Pro-\ncessing, Health Care, Reminiscence Therapy, Memory Triggering\n1\nIntroduction\nThe German Federal Statistical Oﬃce examines the population development until the\nyear 2060. They conclude that the percentage of people older than 67 years will increase\nfrom 19% in 2018 to 27% in 2060 [1]. As people live longer than ever before and there-\nfore, the number of dementia patients is also increasing. Today, more than 44 Million\npeople worldwide are living with dementia, 1.5 Million of them live in Germany [2].\nNon-pharmacological methods are eﬀective in improving the lives of dementia patients.\nReminiscence therapy belongs to the non-pharmacological techniques, used to address\nthe activation process of people with Dementia (pwD) [3].\nNowadays, Reminiscence therapy sessions use digital support systems, which consists\nof digital content such as images, movies or music. These can be applied for example on\nmobile devices. Images, movies or music is used for life review and to evoke memories\nin patients [4], [5]. Thus, demand-oriented and technical solutions cause a valuable con-\ntribution to the care of pwD. Their potential is far from exhausted. Yet, a well known\nchallenge is the identiﬁcation of suitable content as well as the design and evaluation of\nhigh-quality reminiscence care services are very labor-intensive. Besides, this task places\nhigh demands on the caretakers qualiﬁcations.\nCurrently, the reminiscence session content must be identiﬁed and evaluated by care-\ntakers, because the content of a reminiscence session should be suitable and individual\nfor pwD. So, in practice, a very limited pool of standard content is often used.\nTo support the reminiscence therapies the caretaker would select images according to\nthe following image characteristics such as objects, colors, shapes, number of objects or\n234\n\nmeaning according to life themes [6]. In particular, automated, individual and biography-\nrelated media selection improves the quality of the reminiscence session and reduces the\nworkload of the caretakers by shortening the preparation time. However, this potential\nof automation relieves the caretaker in terms of reminiscence sessions. It also gives care\ngiving relatives the opportunity to include memory-triggering content in their care.\nThe approach of this paper is semantic image segmentation, to extract the features of\nan image. Semantic segmentation is one of the high-level tasks that pave the way to full\nunderstanding of a scene [7]. The importance of full scene understanding as a core com-\nputer vision problem aﬃrms by the fact, that an increasing number of applications using\nthe knowledge derivation from images. Some of these applications include medical assis-\ntance systems or human-computer interactions. With the popularity of Deep Learning,\nmany semantic segmentation problems are addressed with Deep Learning approaches.\nThey far exceed other methods in terms of accuracy and eﬃciency.\nThe goal of this work is to analyzes the potentials of semantic image segmenta-\ntion using diﬀerent Convolutional Neural Networks (Inceptionv3, VGG16, VGG19 and\nResNet101) in combination with a Recurrent Neural Network (LSTM, GRU) related to\npeople with dementia, to generate automatic descriptions from images (also called image\ncaptioning).\nThis work consists of 8 chapters. Chapter 2 deals with the related work. Life themes\nidentiﬁed to activate pwD are described chapter 3. In Chapter 4, the concepts of visual\nattention networks are presented and explained. The data used for training, as well as\nthe training process, are described in Chapters 5 and 6. Finally, the results are presented\nin Chapter 7 and Chapter 8 describes the conclusion and future work.\n2\nRelated Work\n2.1\nReminiscence Aid Systems\nThe Computer Interactive Reminiscence and Conversation Aid (CIRCA) project, pro-\nposed by Astell et al. [8] was the ﬁrst project that developed an application for digital\nreminiscence therapy to support people with dementia. Over the years, it was supple-\nmented by diﬀerent new technologies, like a speciﬁc interface for the interaction with the\nsystem [9] or a touch screen computer to enable an easier interaction with the system\n[10]. Today, CIRCA is an interactive multimedia application. The latest publication of\nthe project “Computer Interactive Reminiscence and Conversation Aid groups - Deliv-\nering cognitive stimulation with technology” demonstrates the eﬀectiveness of CIRCA\nfor group interventions [11]. The growing process of the project shows the eﬀectiveness\nof digital assistance systems in the area of reminiscence therapies. The diﬀerence with\nour work is that we do not want to use random content for a reminiscence session, but\nindividual content that ﬁts the biography of a patient. Therefore, our system should be\nable to describe images, that match the life themes of pwD, automatically.\nCar´os et al. [12] presented in their work “Automatic Reminiscence Therapy for De-\nmentia” a solution approach to automate reminiscence therapy, which uses artiﬁcial in-\ntelligence based systems. Their system is called “Elisabot” and consists of a system that\nuses personal images of users and generates questions about their lives, using Visual\nAttention Networks (VATs) In our work we compare diﬀerent architectures of Visual\nAttention Networks to ﬁnd the potentials of these Networks and whether their generated\nimage captions ﬁt to pwDs needs. All encoder models are pretrained on the ImageNet\ndataset.\n235\n\nIn the work “Image Captioning and Comparison of Diﬀerent Encoders” by Pal et al.\n[13] a comparison is presented of diﬀerent encoder implementations as they are used in a\nVisual Attention Network, for the automatic generation of image captions. As encoder,\nthey compare diﬀerent convolutional neural networks, these are Inception v3, VGG16,\nVGG19 and InceptionResNetV2. The result is that the Inceptionv3 Encoder works best.\nSince the range of the BLEU score is lowest for the model. Similar to our work, they\nuse BLEU score for result evaluation. We use the MS COCO dataset for training and\ninstead of using the InceptionResNetV2 for model comparison, a ResNet-101 is used. Fur-\nthermore, this work additionally presents diﬀerences between Long-Short-Term-Memory\n(LSTM) and Gated Recurrent Unit (GRU) decoder .\n2.2\nVisual Attention Networks\nThe caption generation in this paper is based on the work “Show, Attend and Tell”\nproposed by Xu et al. [14]. They describe a mechanism that generates image captions\nbased on Convolutional Neural Network (CNN) encoder and Recurrent Neural Network\n(RNN) decoder by using an attention layer in the network. The Encoder extracts speciﬁc\nfeatures from an image and generates a set of feature vectors, which were referred to\nas annotation vectors in this context. The attention layer takes an annotation vector, to\nfocus on a speciﬁc part of an image, because every vector is a representation corresponding\nto a part of an image. This allows the decoder to selectively focus on speciﬁc parts of\nan image by selecting a subset of all annotation vectors [15]. The results are automatic\ngenerated, textual image captions in a natural language.\nEncoder Recent works have represented the successful deployment of Convolutional En-\ncoder. Therefore, our work is focused on image caption generation using a CNN as En-\ncoder [16], [17]. There are diﬀerent CNN implementations for image feature extraction,\nwhich are compared in this work: VGG16, VGG19, ResNet-101 and Inceptionv3.\nSimonyan et al. [18] proposed multiple Versions of VGG Networks in their work “Very\nDeep Convolutional Networks for Large-Scale Image Recognition”. They diﬀer mainly in\ntheir depth, i.e. in their number of layers. As the name suggests, one network has 16\nlayers and the other 19.\nIn the work “Deep Residual Learning for Image Recognition”, the Residual Network\n(ResNet) Architecture is described from He et al. [19]. The special feature of a ResNet\nis that each layer in the network consists of several blocks. As the depth of the network\nincreases, the number of operations in a block increases too, but not the total number\nof layers. Thus, ResNets solve the problem of vanishing gradients as neural networks\nbecome deeper.\nThe architecture of the Inceptionv3 network emerges from the GoogleNet architecture\nand was proposed from Szegdy et al. [20] in the work “Rethinking the Inception Architec-\nture for Computer Vision”. The model is a combination of many ideas that have emerged\nin recent years. The network consists of several symmetric and asymmetric blocks, which\ncan contain diﬀerent types of layers. For example, convolutions, average pooling, max\npooling, etc. In total, the network has 42 layers [21].\nDecoder As decoder, a Recurrent Neural Network (RNN) is used. The ﬁrst RNN was\npublished in the work “Finding structure in time” from Jeﬀrey Elman [22]. This network\nis capable of reading the annotation vectors extracted by the CNN in previous steps. The\nimportant features of RNNs are the memory cells. A normal feed forward network has an\ninput, hidden and output layer. The RNN loops the hidden layer to process sequential\n236\n\ndata. With this looping mechanism, the RNN allows ﬂowing information from one step\nto the next step. During training a RNN Model is enrolled (each word acts as layer) and\ntrained with backpropagation trough stochastic gradient descent [23]. RNNs often have\na problem known as short term memory. As the number of words increases, so does the\ndepth of the network. The more steps an RNN has to process, the greater the problem of\nretaining the information from the previous steps. This phenomenon occurs due to the\nbackpropagation used for training the network.\nThere are two approaches to tackle the problems of RNNs caused by short-Term-\nMemory: Long-Short-Term-Memory (LSTM) and Gated-Recurrent Unit (GRU). The\nLSTM Network was proposed from Hochreiter et al. [24] in the work “Long Short-Term\nMemory” and the GRU Network was proposed from Cho et al. [25] in the work “Learn-\ning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Trans-\nlation”. The underlaying structure of these networks is the same as the structure from\nan RNN extended by a mechanism that can learn long-term dependecies using “gates”.\nLSTMs are using three gates to process the data while GRUs only use one gate for data\nselection [26].\n3\nPeople with Dementia Life Themes\nThe reminiscence sessions should include content that matches the biographical needs of\na pwD. So that an activation in reminiscence sessions can take place [27]. Life themes\nrepresent a generalized categorization of life stages or events that play a role in a person’s\nlife. The goal of these particular subjects is to evoke memories in pwD, that are associated\nwith a life theme. Life themes can be represented by pictures, videos, music or physical\nobjects. This work focuses on picture retrieval. The following table 1 shows the life\nthemes we identiﬁed and used for image feature extraction in this work. As the table\nshows, there are not only positive activation for pwD, but also negative/fearful issues,\nwhich should be avoided. If these negative loaded images are also labeled, they can be\ndiscarded before using them in a reminiscence session. Anxiety-producing image content\nshould be screened out before sessions. This prevents a session from having a negative\nimpact on a patient’s well-being.\nTable 1. Life themes used in reminiscence sessions for pwD\nLife Themes\nTravel\nAnimals\nProfessions\nHobbies and Activities\nPreferences and Habits\nExterior Appearance\nReligion\nEducation\nNature\nChildhood and Youth\nHome\nTradition and Culture\nLiterature\nMedia\nTheather\nGarden\nPeople\nFears and Disease\nFood\n237\n\n4\nVisual Attention Networks\nAutomatic generation of image descriptions is a diﬃcult task in the ﬁeld of full scene\nunderstanding. The model must transform large disparate sets of data into a natural\nlanguage. To address this problem, Visual Attention Networks (VATs) are used [14].\nFig. 1. Visual Attention Network Architecture\nA VAT consists of two sub-models, as shown in Figure 1. a) Encoder: First, an image\nis given as input to the CNN to obtain vectorial representation of input images, on the\nlast hidden layer of the network. b) Decoder: The feature vector is used from the RNN\ndecoder as input to generate sentences that contain the objects and the relationships\nbetween them [14],[28]. The visual system of a human has the function to pay attention\non diﬀerent parts of an image instead of processing the scene as a whole [29]. Based on\nthis human attention mechanism, an attention layer is integrated into a visual attention\nnetwork. In this paper we use soft attention mechanisms for training [30]. The areas\nselected by attention mechanism are captured by the RNN for further processing. Based\non this process suitable textual descriptions are generated.\n5\nDataset Used for Training and Evaluation\nThe dataset used for training should contain everyday objects, which match the life\nthemes from section 3. There are several datasets that contain labeled content for image\ncaptioning, for example MS COCO, Pascal VOC or Flickr30k. Since the MS COCO\ndataset [31] has the most objects in common with the life themes described in section 1,\nthis dataset was used for training in this work. The matching life themes are for example\n“Animal”, “Person” and “Food” . Each image from the MS COCO Caption dataset is\ndescribed (labeled) with 5 diﬀerent sentences. This work primarily targets the description\nof dogs and cats images, so these categories are ﬁltered from the dataset. Dog images fall\ninto two categories in the context of Reminiscence Therapy: Dog images that activate\npositive memories in the pwD and dog images that might trigger negative memories. For\nexample dangerous looking dogs, aggressive dogs or snarling dogs. In total, 4298 images\nof the category “cat” and 4562 images from the categorie “dog”, with a total of 43200\nimage descriptions, are ﬁltered from the dataset.\nSince the MS COCO dataset only contains images with friendly looking dogs, the\ndataset is extended with the category “Angry Dogs” and ﬁlled with our own image\n238\n\ncontent. Each image is described with 5 description sentences, similar to the the caption\nstyle of MS COCO dataset. The number of images in this category amounts to 360\ntraining images with 1800 descriptions. In total, our dataset consists of 9000 images,\nwith 45000 image descriptions. For training, we use a random 80/20 split on the dataset,\nto split it into train and validation set.\n6\nTraining\nThis work compares diﬀerent networks which are used in a Visual Attention Network Ar-\nchitecture. We use ResNet-101, VGG16, VGG19 and Inceptionv3 as encoder and Gated\nRecurrent Unit (GRU) or Long-Short-Term Memory (LSTM) as decoder, to compare\nthem with each other. The CNNs are all pretrained on the ImageNet [32] dataset. This\ndataset contains many objects related to the Life themes of people with dementia. Ad-\nditional we use MS COCO Dataset for training the RNN, as described above.\nFor training, we use a ﬁxed-length image caption of 9 words per sentence, because the\nnetwork performes poor on long input or output sequences. This is caused by rolling out\nthe RNN, where each word represents a hidden layer. The longer a caption is, the slower\nthe network is trained. In addition, for our use case of image descriptions for the use in\nreminiscence therapy, captions of length 9 are suﬃcient to make sense of the content and\nat the same time short enough to not impair the performance of the model.\nIn total, we get 6660 diﬀerent words as vocabulary, from which we use all words that\noccur more than three times in the vocabulary. This results in 2206 words. Unknown\nwords, are provided with the token <unk>. We trained the Networks with a batch size\nof 8 and 100 Epochs.\n7\nEvaluation and Comparison of the VGG16, VGG19,\nResNet-101 and Inceptionv3 Model\nFor evaluation and comparison, a dataset, containing 10 images for each class in the\ntraining dataset (cat, dog, angry dog), was created. These images are with 5 captions per\nimage for result evaluation with BLEU score. In the area of image caption generation it’s\nhard to evaluate resulting captions formally. We only have ﬁve reference sentences for an\nimage, but there are much more image descriptions that match the image content. Thus,\nwe decided to evaluate the results formally with BLEU score and verify the results with\nhuman evaluation. BLEU is a method for automatic evaluation of machine translation\n[33], it’s quickly, inexpensive, and language-independent. The metric correlates highly\nwith human evaluation, because it measures the closeness of the machine translation to\nhuman reference translations by taking translation length, word choice and word order\ninto consideration. For our evaluation we use 2-gram BLEU score. This approach does\nnot check the speciﬁc order of all words in the generated caption, but only the adjacent\nones and compares them with the reference descriptions.\nThe following Table 2 shows the average calculated BLEU score, using diﬀerent\nencoder-decoder combinations, for (a) Inceptionv3, (b) ResNet101, (c) VGG16 and (d)\nVGG19. Every category contains 10 images for inference. The generated captions of an\nRNN are not identical, since such a model has no ﬁxed hidden layer size. They always\ndepend on the generated caption length. Therefore, each model was trained ﬁve times\n(GRU-1, GRU-2,...,GRU5 or LSTM-1, LSTM-2,...,LSTM-5). At the end of the tables,\nthe respective average value (ø) is shown, which is used for result comparison. The scores\n239\n\nin the tables show that ResNet101 behaves the worst. The BLEU scores are below the\nscores of the other networks, regardless of which model combination was used. Incep-\ntionv3 has the best average BLEU for the category “Cat”, combined with a GRU model.\nThe best results are produced by VGG16 and VGG19 models, with VGG19 performing\nbest in combination with a GRU decoder and the VGG16 in combination with an LSTM\ndecoder. The Inceptionv3 model outperforms the VGG16 model in the category “Cat”\nwith a BLEU score of 0.02322 compared to 0.2005. The diﬀerence is so small that for all\nclasses in general, the VGG16 model perfomes better.\nAll models provide the worst results for the category “Dog” and the best for the\ncategory “Angry Dog”. This is because the images in the angry dog category were self-\nlabeled, speciﬁcally tailored to our problem set. The other two categories are labeled\nwith general captions from the COCO dataset. In general, the BLEU scores are stable\nfor each model in each class; no outliers are apparent.\nTable 2. Calculated BLEU scores\n(a) BLEU score for Inceptionv3\nDecoder\nCat\nDog\nAngry Dog\nGRU-1\n0.2377\n0.1018\n0.6600\nGRU-2\n0.2629\n0.1619\n0.6423\nGRU-3\n0.2793\n0.1650\n0.6526\nGRU-4\n0.1701\n0.1846\n0.5806\nGRU-5\n0.2114\n0.1467\n0.6682\nø\n0.2322\n0.1250\n0.6407\nLSTM-1\n0.2511\n0.1126\n0.5386\nLSTM-2\n0.1620\n0.0702\n0.5750\nLSTM-3\n0.2336\n0.0967\n0.5910\nLSTM-4\n0.1718\n0.0845\n0.6724\nLSTM-5\n0.1616\n0.0759\n0.5704\nø\n0.1960\n0.0879\n0.5894\n(b) BLEU score for ResNet101\nDecoder\nCat\nDog\nAngry Dog\nGRU-1\n0.0875\n0.0500\n0.3789\nGRU-2\n0.1889\n0.0500\n0.2951\nGRU-3\n0.0634\n0.0375\n0.3361\nGRU-4\n0.0375\n0.0625\n0.3020\nGRU-5\n0.1000\n0.0625\n0.3896\nø\n0.0954\n0.0525\n0.3403\nLSTM-1\n0.1697\n0.0611\n0.2402\nLSTM-2\n0.0960\n0.0500\n0.2339\nLSTM-3\n0.1625\n0.1000\n0.2000\nLSTM-4\n0.1000\n0.0723\n0.2978\nLSTM-5\n0.0986\n0.0611\n0.2216\nø\n0.1253\n0.0689\n0.2387\n(c) BLEU score for VGG16\nDecoder\nCat\nDog\nAngry Dog\nGRU-1\n0.2154\n0.0666\n0.5346\nGRU-2\n0.1785\n0.0767\n0.5974\nGRU-3\n0.2236\n0.1142\n0.6947\nGRU-4\n0.2339\n0.1077\n0.5626\nGRU-5\n0.2378\n0.0583\n0.6252\nø\n0.2178\n0.0847\n0.6029\nLSTM-1\n0.1918\n0.0875\n0.6417\nLSTM-2\n0.2430\n0.0916\n0.6165\nLSTM-3\n0.2085\n0.1139\n0.6167\nLSTM-4\n0.1805\n0.1111\n0.6032\nLSTM-5\n0.1791\n0.0800\n0.6990\nø\n0.2005\n0.0968\n0.6354\n(d) BLEU score for VGG19\nDecoder\nCat\nDog\nAngry Dog\nGRU-1\n0.1932\n0.0951\n0.6087\nGRU-2\n0.1883\n0.1468\n0.6545\nGRU-3\n0.2231\n0.1571\n0.6934\nGRU-4\n0.2261\n0.1303\n0.6989\nGRU-5\n0.1723\n0.1105\n0.7291\nø\n0.2006\n0.1279\n0.6769\nLSTM-1\n0.1684\n0.0382\n0.5244\nLSTM-2\n0.2682\n0.0454\n0.4502\nLSTM-3\n0.2220\n0.0737\n0.6332\nLSTM-4\n0.2316\n0.0722\n0.5741\nLSTM-5\n0.1986\n0.0737\n0.6888\nø\n0.2177\n0.0606\n0.5741\n240\n\nThe results, calculated by BLEU score are veriﬁed manually. The best and worst\nBLEU scores for eachmodel are taken and the corresponding generated captions were\nchecked. For each model, the best and worst results for all categories are shown in Figure\n2. Figure (a) shows the generated captions for cats, (b) shows the captions for dogs and\n(c) the captions for angry dogs.\n(a) Resulting Captions “Cat”[31]\n(b) Resulting Captions “Dog”[34]\n(c) Resulting Captions “Angry Dog”[35]\nFig. 2. Resulting Captions\nThe resulting captions of the category dogs are as good as the BLEU score describes\nthem. Only one bad caption was produced by the ResNet101, GRU-2. The other results\ncoincide with the BLEU score, ResNet101 predictions are generally worse than the others\nand VGG16, VGG19 generate the best image captions. By comparing the BLEU scores\nwith human evaluation, we came to the conclusion: the better the BLEU score, the better\nthe caption.\n241\n\n8\nConclusion\nThis work reveals that the use of Visual Attention Networks in the context of reminiscence\nsessions for dementia patients has signiﬁcant potential. The result of the comparison from\ndiﬀerent encoder-decoder combinations is that the use of VGG16-LSTM and VGG19-\nGRU Models generating promising results. This approach allows activation sessions to\nbe simpler, faster and tailored for a patient’s needs. Thereby, higher quality and quantity\nof reminiscence sessions is created. At the same time, the life of a dementia patient is\npositively inﬂuenced.\nIn the future, it’s important to extend the dataset with more categories, to match\nmore life themes from pwD. In addition, it’s possible to extend the system with the\nability to describe not only images automatically. There are also music and videos that\nmatch the life themes of pwD.\nReferences\n1. Bundesamt, S.: 14. koordinierte Bev¨olkerungsvorausberechnung - Ergebnisse f¨ur Deutsch-\nland (Jul 2021) [Online; accessed 14. Jul. 2021].\n2. Association, A.: Alzheimer’s & Dementia Help | Germany | Alzheimer’s Association (Jul\n2021) [Online; accessed 14. Jul. 2021].\n3. Khait, A.A., Shellman, J.: Uses of Reminiscence in Dementia Care. Innovation in Aging\n4(Supplement 1) (12 2020) 287–287\n4. Lazar, A., Thompson, H., Demiris, G.: A systematic review of the use of technology for\nreminiscence therapy. Health Education & Behavior 41(1 suppl) (2014) 51S–61S PMID:\n25274711.\n5. ´Latha, K., Bhandary, P., Tejaswini, S., Sahana, M.: Reminiscence therapy: An overview.\nMiddle East Journal of Age and Ageing 11 (2014)\n6. Ji, Z., Yao, W., Pi, H., Lu, W., He, J., Wang, H.: A Survey of Personalised Image Retrieval\nand Recommendation. In Du, D., Li, L., Zhu, E., He, K., eds.: Theoretical Computer Science,\nSingapore, Springer Singapore (2017) 233–247\n7. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic image seg-\nmentation with deep convolutional nets and fully connected crfs (2016)\n8. Alm, N., Astell, A., Ellis, M., Dye, R., Gowans, G., Campbell, J.: A cognitive prosthesis\nand communication support for people with dementia. Neuropsychological Rehabilitation\n14(1-2) (2004) 117–134\n9. Gowans, G., Dye, R., Alm, N., Vaughan, P., Astell, A., Ellis, M.: Designing the interface\nbetween dementia patients, caregivers and computer-based intervention. The Design Journal\n10(1) (2007) 12–23\n10. Astell, A.J., Ellis, M.P., Bernardi, L., Alm, N., Dye, R., Gowans, G., Campbell, J.: Us-\ning a touch screen computer to support relationships between people with dementia and\ncaregivers. Interacting with Computers 22(4) (07 2010) 267–275\n11. Astell, A.J., Smith, S.K., Potter, S., Preston-Jones, E.: Computer interactive reminiscence\nand conversation aid groups—delivering cognitive stimulation with technology. Alzheimer’s\n& Dementia: Translational Research & Clinical Interventions 4(1) (2018) 481–487\n12. Caros, M., Garolera, M., Radeva, P., i Nieto, X.G.: Automatic reminiscence therapy for\ndementia (2021)\n13. Pal, A., Kar, S., Taneja, A., Jadoun, V.K.: Image captioning and comparison of diﬀerent\nencoders. Journal of Physics: Conference Series 1478 (apr 2020) 012004\n14. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A.C., Salakhutdinov, R., Zemel, R.S., Bengio,\nY.: Show, attend and tell: Neural image caption generation with visual attention. CoRR\nabs/1502.03044 (2015)\n15. Sarkar, S.: Image Captioning using Attention Mechanism - The Startup - Medium. Medium\n(Jun 2021)\n242\n\n16. Aneja, J., Deshpande, A., Schwing, A.: Convolutional image captioning (2017)\n17. Katiyar, S., Borgohain, S.K.: Analysis of convolutional decoder for image caption generation\n(2021)\n18. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recog-\nnition (2015)\n19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n20. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the inception archi-\ntecture for computer vision (2015)\n21. Maeda-Guti´errez, V., Galv´an-Tejada, C.E., Zanella-Calzada, L.A., Celaya-Padilla, J.M.,\nGalv´an-Tejada, J.I., Gamboa-Rosales, H., Luna-Garc´ıa, H., Magallanes-Quintanar, R.,\nGuerrero M´endez, C.A., Olvera-Olvera, C.A.: Comparison of convolutional neural network\narchitectures for classiﬁcation of tomato plant diseases. Applied Sciences 10(4) (2020)\n22. Elman, J.L.: Finding structure in time. Cognitive Science 14(2) (1990) 179–211\n23. Sherstinsky, A.: Fundamentals of recurrent neural network (rnn) and long short-term mem-\nory (lstm) network. Physica D: Nonlinear Phenomena 404 (Mar 2020) 132306\n24. Hochreiter, S., Schmidhuber, J.: Long Short-Term Memory. Neural Computation 9(8) (11\n1997) 1735–1780\n25. Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Ben-\ngio, Y.: Learning phrase representations using rnn encoder-decoder for statistical machine\ntranslation (2014)\n26. Gao, Y., Glowacka, D.: Deep gate recurrent neural network. In Durrant, R.J., Kim, K.E.,\neds.: Proceedings of The 8th Asian Conference on Machine Learning. Volume 63 of Proceed-\nings of Machine Learning Research., The University of Waikato, Hamilton, New Zealand,\nPMLR (16–18 Nov 2016) 350–365\n27. Huber, S., Berner, R., Uhlig, M., Klein, P., Hurtienne, J.: Tangible objects for reminiscing\nin dementia care. In: Proceedings of the Thirteenth International Conference on Tangi-\nble, Embedded, and Embodied Interaction. TEI ’19, New York, NY, USA, Association for\nComputing Machinery (2019) 15–24\n28. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.:\nShow and tell: A neural image caption\ngenerator (2015)\n29. Wang, W., Shen, J.: Deep visual attention prediction. IEEE Transactions on Image Pro-\ncessing 27(5) (May 2018) 2368–2378\n30. Sharma, S., Kiros, R., Salakhutdinov, R.: Action recognition using visual attention. CoRR\nabs/1511.04119 (2015)\n31. Lin, T.Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan,\nD., Zitnick, C.L., Doll´ar, P.: Microsoft coco: Common objects in context (2015)\n32. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hi-\nerarchical image database.\nIn: 2009 IEEE Conference on Computer Vision and Pattern\nRecognition. (2009) 248–255\n33. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: A method for automatic evaluation of\nmachine translation. In: Proceedings of the 40th Annual Meeting on Association for Com-\nputational Linguistics. ACL ’02, USA, Association for Computational Linguistics (2002)\n311–318\n34. : Pixabay - Dog Picture (Aug 2021) [Online; accessed 31. Aug. 2021].\n35. : iStock - Angry Dog Picture (Aug 2021) [Online; accessed 31. Aug. 2021].\n243",
    "pdf_filename": "Artificial Intellgence -- Application in Life Sciences and Beyond. The Upper Rhine Artificial Intelligence Symposium UR-AI 2021.pdf"
}