{
    "title": "Neural Networks for Chess",
    "context": "",
    "body": "Neural Networks for Chess\nThe magic of deep and reinforcement\nlearning revealed\nDominik Klein\nJune 11, 2022\n\n\nContents\n1\nIntroduction\n11\n2\nA Crash Course into Neural Networks\n19\n2.1\nThe Perceptron\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2.2\nBack-Propagation and Gradient Descent . . . . . . . . . . . . . . .\n24\n2.3\nClassiﬁcation vs Regression . . . . . . . . . . . . . . . . . . . . . .\n35\n2.4\nPutting it into Practice\n. . . . . . . . . . . . . . . . . . . . . . . . .\n38\n2.5\nInherent Limits of a Single Perceptron . . . . . . . . . . . . . . . .\n40\n2.6\nMultilayer Perceptrons . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n2.7\nVectorization and GPUs\n. . . . . . . . . . . . . . . . . . . . . . . .\n48\n2.8\nConvolutional Layers . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n2.9\nSqueeze-and-Excitation Networks . . . . . . . . . . . . . . . . . . .\n61\n2.10 Fully Connected Layers . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n2.11 Batch normalization and Rectiﬁed Linear Units . . . . . . . . . . .\n63\n2.12 Residual Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n2.13 Overﬁtting and Underﬁtting . . . . . . . . . . . . . . . . . . . . . .\n69\n2.14 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n3\nSearching\n75\n3.1\nMinimax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n3.2\nAlpha-Beta Search . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n3.3\nAdvanced Techniques\n. . . . . . . . . . . . . . . . . . . . . . . . .\n91\n3.4\nA Note on Deep Blue . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n\nCONTENTS\n3.5\nMonte-Carlo Tree Search . . . . . . . . . . . . . . . . . . . . . . . . 100\n3.6\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n4\nModern AI Approaches - A Deep Dive\n123\n4.1\nAlphaGo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n4.2\nAlphaGo Zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n4.3\nAlphaZero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n4.4\nLeela Chess Zero (Lc0) . . . . . . . . . . . . . . . . . . . . . . . . . 174\n4.5\nFat Fritz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n4.6\nEﬃciently Updateable Neural Networks (NNUE) . . . . . . . . . . 191\n4.7\nFat Fritz 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n4.8\nMaia\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n4.9\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\n5\nHexapawnZero\n231\n5.1\nThe Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n5.2\nGame Logic\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236\n5.3\nSupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 242\n5.4\n“Zero-like” Reinforcement Learning . . . . . . . . . . . . . . . . . 249\n5.5\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\n6\nConclusion and Outlook\n261\n\nCONTENTS\n\n6\nCONTENTS\n\nPreface\nThis book is a brief introduction into modern computer chess. Chess engines\nusing artiﬁcial intelligence based on deep-learning made quite an impact in the\nchess community. AlphaZero, the chess monster developed by Google’s re-\nsearch subsidiary DeepMind suddenly seemed to be able to play beautiful, al-\nmost human-like chess. Some games showed piece sacriﬁces with no immediate\ntactical gain but positional long-term advantages that common chess engines\nwere unable to ﬁnd. Some even proclaimed jokingly the return of the era of\nRomantic chess.1\nFrom the perspective of a chess player, the inner workings of a chess engine\nare often a mystery.\nThey are usually understood as black boxes that only\ngenius programmers and researchers can understand. In the famous Deep Blue\nversus Garry Kasparov (re)match in 1997, where the world chess champion\nKasparov lost to an IBM-developed super computer dubbed Deep Blue, he\nmade allegations that the Deep Blue team cheated. He hinted that a speciﬁc\nmove in a position in Game 2 of the match was not the result of computer\ncalculation but that the move was the result of human intervention.\nHe stated that it seemed very unlikely that a chess computer could have made\nthat particular move.\nHis argument was that the computer abstained from\nwinning a pawn in order to get a better piece placement instead. At the time he\nsimply could not comprehend that a chess engine was capable of such a move.\n1referring to the style of chess that was common in the 18th century, namely wild but beautiful\nbattles and sacriﬁces.\n7\n\n8\nCONTENTS\nWith some distance Kasparov recounts the match from his perspective [KG18]\nand corrects his misconceptions somewhat.\nContrasted to that, Deep Blue\ndeveloper Feng-hsiung Hsu’s technically detailed recount of the events paints a\nvery diﬀerent story [Hsu04]. Both are very interesting reads and I recommend\ngiving them both a try.\nNevertheless one thing must be stated clearly: There is no reason to believe that\nDeepBlue was not able to ﬁnd the move in question in that position. This is\neven acknowledged by Kasparov in his book where he states that he was wrong\nand owes the Deep Blue team an apology. He notes that current computers ﬁnd\nthe best move in seconds. It takes great courage to admit one’s faults. But even\nengines released in 1997 were able to ﬁnd the correct move as we will see in\nChapter 3.\nKasparov dominated chess over decades. How come that such a chess genius\ncompletely misjudged the situation? We can only speculate but it seems he\nassessed the chess engine purely from a chess player’s perspective neglecting its\ninner technical workings. Moreover he apparently relied on the chess program\nFritz to train and assess chess computers [KG18] even though Fritz at that time\nwas known to be somewhat of a “pawn snatcher” with high tactical vision but\nsubpar positional evaluation compared to other engines.\nWith more technical understanding and by taking the engineering perspective\ninto account we can pose the question if Kasparov would have been able to\nprolong the inevitable loss of humans versus chess computers for some more\ntime. We can only speculate, but the psychological side of things, namely being\nsurprised and stunned by engine moves should not be underestimated: After\nthe ﬁfth game, Kasparov said he had not been in the mood for playing, and\nwhen asked to elaborate on his outlook, he said: “I’m a human being. [...]\nWhen I see something that is well beyond my understanding, I’m afraid.”2\nAllegedly Kasparov was perplexed after 44... Rd1 in Game 1 of the 1997 re-\nmatch and he and his team deeply analyzed why DeepBlue preferred it to\n2https://www.latimes.com/archives/la-xpm-1997-05-14-ls-58417-story.html and\nhttps://www.nytimes.com/1997/05/12/nyregion/swift-and-slashing-computer-topples-ka\nsparov.html, accessed 14.07.2021\n\nCONTENTS\n9\n44...Rf5; coming to the conclusion that DeepBlue must have seen mate in 20 or\nmore [Hsu04].\nAccording to Hsu [Hsu04] it was just a bug in the program. With a diﬀerent\nperspective on the technical challenges involved in designing and programming\na massively parallel system with custom hardware, Kasparov’s team might have\ngiven this possibility a much higher probability.\nThis book is not about the Kasparov versus Deep Blue match. But this story\nclearly illustrates that it is interesting and maybe even worthwhile to look behind\nthe curtain and ﬁgure out the inner workings of chess engines, even if one is\nnot a computer scientist but just a chess player enjoying the game. It’s one\nthing to look in awe and admire the game changer AlphaZero, but it’s another\none to ﬁgure out how AlphaZero and similar engines work and get a better\nunderstanding what they are capable of.\nChess programming is a quite technical domain. It is hard to get started and the\nentry barrier is seemingly high. Famous Science Fiction author Arthur C. Clarke\nonce wrote that “any suﬃciently advanced technology is indistinguishable from\nmagic” [Cla99]. Here we want to reveal that magic: This book is for all those\nwho do not want to put up with shallow and superﬁcial explanations such as\n“This engine works better because it now has artiﬁcial intelligence”. This book\nis for those who want to look deeper, love to take things apart and dig in the\nmud to ﬁgure out how it all works.\nWe will make a journey into computer chess. In this book you will learn what a\nneural network is and how it is trained, how classical alpha-beta searchers work\nand how all this was combined to create the strongest chess engine available\ntoday, Stockﬁsh NNUE.\nConfused by all this lingo? That’s ok since we will all sort it out step by step.\nYet this book is not an graduate textbook. We will cover all the central ideas\nwith lots of examples but try to abstract away unnecessary details and make\nsure you will (hopefully) not get lost in mathematical details.\nIndeed, there is math involved to understand neural networks, but it is really not\nthat much. Also, there is surprisingly little prior technical knowledge required.\n\n10\nCONTENTS\nIn fact, there are only two major mathematical concepts that you have to know.\nFirst, there is basic calculus, namely how to calculate a derivative. Second you\nneed to understand matrix multiplications. You most likely remember these\nthings from high school, and if not, I suggest that you dig up your old high-\nschool math textbook and do a quick brush up. But fortunately that’s mostly it,\nreally.\nThis book contains programming examples in Python. This is a really newbie\nfriendly and simple programming language. But make no mistake — by no\nmeans it is required to use these programming examples or become a program-\nmer to understand the concepts described in this book. To illustrate this you can\nthink of it like being an architect: It helps if you are familiar with masonry and\nmaybe even worked in construction, but it is by no means a must to understand\nhow houses are designed. If you like programming though, these example\nsupport learning the presented concepts and serve as a starting point for your\nown experiments.\nAll source code presented in this book is also available online at\nhttp://github.com/asdfjkl/neural_network_chess\nIf you are a chess player and work carefully through this book, you will have a\nbetter understanding how chess engines work, what their limitations are, and\nhow to interpret their results. If new engines and approaches appear, you will\nhave a better understanding what their advancements imply for the game. You\nwill also be able to evaluate the marketing lingo by commercial vendors.\nAnd if you encounter yet another newspaper article about the “revolutions of\nartiﬁcial intelligence and deep learning”, you will have a much better ability\nto judge on whether the claims made there are accurate or a dubious science\nﬁction fantasy at best.\nHave a lot of fun!\n\n1\nIntroduction\nI think we should all say things\nbut pretend that other, very smart\npeople said them and we are\nmerely quoting them\nAristotle\nChess is made up of tactics and strategy. For tactics you calculate moves and\nreplies and look if any of the move sequences leads to forced advantage or\ndisadvantage. Essentially you search among all possible variations. For strategy\nyou judge a position without too much calculation.\nMy father taught me the rules of chess, probably at the age of eight or nine, but I\nlater lost interest in the game. I re-discovered chess during the 2014 world chess\nchampionship between Carlsen and Anand, started playing and searched for\nhigh quality teaching material. Then I stumbled upon Jeremy Silman’s Reassess\nYour Chess [Sil10]. There it was — all aspects to evaluate a position listed and\nexplained: pawn structures, strong and weak squares, open lines, good and bad\nsquares for your pieces, king safety — everything was there and formulated\nin a way such that an amateur can understand it.\nSilman explains how to\ncharacterizes a position by imbalances, i.e. advantages and disadvantages that\n11\n\n12\n1. INTRODUCTION\n8 rmbl0skZ\n7 oponZpop\n6 0Z0ZpZ0Z\n5 Z0ZpO0Z0\n4 0a0O0Z0Z\n3 Z0MBZNZ0\n2 POPZ0OPO\n1 S0AQJ0ZR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 1.1: White to move.\ncharacterize the position. He advocates to use this kind of evaluation technique\nto ﬁnd good moves in a position.\nI later stumbled upon Willy Hendriks’ book Move First, Think Later [Hen12].\nHendriks is very critical of Silman’s method of evaluation by imbalances. His\nargument is that we rarely ﬁnd good moves by just looking at a position, and\nthat we should instead immediately search and apply moves that come to our\nmind — hence the title — in order to come to an understanding of the position\nat hand and ﬁnd the best next move.\nFrom a computer scientist’s perspective, this is however a pseudo-debate: Of\ncourse we need both, search1 and evaluation.\nLet’s have look at the famous Greek gift sacriﬁce shown in Figure 1.1. How do\nwe evaluate this position? There is equal material of course. White has maybe a\nsmall space advantage. Thus without searching we would evaluate the position\nwith a small advantage for White. But after searching through several candidate\n1Chess players often refer to searching as tactics, or calculation. But since we focus on computer\nchess, let’s stick to search for now.\n\n13\n8 rZblkZ0s\n7 opopZpop\n6 0anZ0m0Z\n5 Z0Z0Z0Z0\n4 0ZBOPZ0Z\n3 Z0Z0ZNZ0\n2 PO0Z0OPO\n1 SNAQJ0ZR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 1.2: M. Euwe - K. Jutte, Amsterdam 1927. White to move.\nmoves, we quickly ﬁnd the sequence 1.Bxh7+! Kxh7 2.Ng5+. Now we have\nto search deeper for all possible replies after each move and ﬁnd that in the\nresulting positions, either White checkmates or wins material. In other words,\nthe resulting positions are very easy to evaluate with high conﬁdence. Based on\nthese evaluations, we re-evaluate the original position as \"White has a signiﬁcant\nadvantage\".\nHowever often when we calculate, we cannot reach a checkmating position or\none where the evaluation is easy due to signiﬁcant material advantages. Of\ncourse, every chess game ends in a ﬁnal position eventually, but it is in general\nimpossible to search so deeply. Thus we have to stop after a certain number of\nmoves, and then evaluate the resulting position by structural features, such as\nSilman’s imbalances.\nFor example, take a look at Figure 1.22.\nWe can go through several move\nsequences, in particular d5!\nand all subsequent replies by Black; consider\n2This is an interesting position, since even modern chess engines require some time to ﬁnd d5!\nStockﬁsh 12 for example looks at Nc3 ﬁrst until it ﬁnally switches to d5 and notices the signiﬁcant\nadvantage.\n\n14\n1. INTRODUCTION\nespecially Ne7 followed by e5. Only by using Silman’s imbalance criteria and\nwithout any further search, we can evaluate the resulting position(s): There is\na signiﬁcant positional advantage in all subsequent variations for White, but\nnot an immediate material gain or checkmating sequence. We can then use\nthe evaluations of the resulting positions to (re)evaluate the original position\nof Figure 1.2. In other words, White has a signiﬁcant advantage in the current\nposition, due to his space advantage in the center.\nI take it that Willy Hendriks’ major point is that humans rarely do it in the\nstructural way that is often taught, i.e.\n• ﬁnd a number of candidate moves. Then for each of them sequentially...\n• calculate all subsequent positions and variation trees that result from those\ncandidate moves as deep as possible\n• evaluate all the resulting positions\n• ﬁnally, based on all these evaluations, (re)evaluate the current position\nand select the best move\nBecause really, that’s how computers do it! Humans probably jump back and\nforth, forget candidate moves, focus on one speciﬁc line, then decide by instinct\ninstead of by rational thought and become angry when realizing that a piece\nhas just been lost.\nAt least that’s how I do it.\nIn the end however, it all centers around searching among (candidate) moves,\nand evaluating positions. And since we cannot always search so deep that we\nend up in a ﬁnal state of a game, we need to stop after a number of moves, and\nevaluate a non-ﬁnal position.\nFor a long time, humans were always better in evaluating positions and worse\nat searching. This was especially true for chess positions that are diﬃcult to\njudge, i.e. where no speciﬁc criteria exist, so that we can calculate or prove an\nadvantage. Typical examples are very closed positions like the one in Figure 1.3.\nWhite has a material advantage here. Rybka wants to avoid a draw by repetition\nin a position with a material advantage, and will ﬁnally make a bad move,\n\n15\n8 0a0Z0Z0Z\n7 Z0Z0ZpZ0\n6 pZpZkZpZ\n5 OpZpZnOp\n4 0OPOpO0O\n3 Z0Z0O0ZR\n2 0ZKZ0Z0Z\n1 ZRZ0Z0Z0\na\nb\nc\nd\ne\nf\ng\nh\nFigure 1.3: Rybka (Computer) vs Hikaru Nakamura, after move 174. ICC, 2008.\nBlack to move.\nmisjudging the corresponding positional disadvantage. Black won the game\neventually. It’s the speciﬁc pawn structure that will result in White’s eventual\nloss if he tries to force a win, and the rooks cannot prevent this.\nThe advantage in searching that computers have is very obvious: Computers are\njust better at number crunching. Any pocket calculator is faster at multiplying\nnumbers than a human.\nThe weakness in evaluating positions is something that have plagued computers\nfor a long time. Humans learn how to judge a position by intuition. It is often\ndiﬃcult to express in words why a position is better or worse. Stefan Meyer-\nKahlen, author of the chess program Shredder, once told this story:3 In some\npositions, Shredder made a particularly bad positional move. He asked a strong\nplayer what the problem was in order to generate a potential ﬁx, i.e. he had to\nﬁgure out what the exact problem was. But he only got the reply “Nah, you just\ndon’t play like that”.\n3Eric van Reem: Let’s talk about chess #12: Stefan Meyer-Kahlen, https://letscast.fm/site\ns/let-s-talk-about-chess-753d92ea/episode/12-stefan-meyer-kahlen-shredderchess\n\n16\n1. INTRODUCTION\nThe revolution that AlphaZero brought, the real game changer is generating\nthat ability to intuitively evaluate a chess position like a human, and to not rely\non a ﬁxed rule set like e.g. pawn counting. Technically, chess engines use neural\nnetworks to create an abstract model of a human’s brain, in order to process and\njudge a chess position.\nBut how do good players become so good at chess? If we listen to Grandmaster’s\nadvice, frequently mentioned is the following strategy:\n• Look at your own games without an engine. Look careful at the mistakes\nyou and your opponent made and learn from these mistakes to play better\nnext time.\n• Look carefully at games of Grandmasters. Improve by looking at their\napproaches and plans in various types of positions.\nInterestingly, following this strategy is precisely what computer chess programs\ndo nowadays to gain a (super-)human understanding of chess positions. Either\nby taking large databases of games of chess Grandmasters, or by looking and\nlearning from their own games.\nFor the ﬁrst step we obviously have to play a lot of games. In the famous novel\nSchachnovelle by Stefan Zweig, a chess player becomes very good at chess (and\nalmost insane) by splitting his personality and playing endless games of chess\nbetween these two personalities. Computers have no insanity issues (so far),\nand hence, learning by self-play has become the de-facto standard in training\nchess programs or rather the underlying neural networks. This is also mostly\nthe preferred approach compared to looking at large databases of games of\nhuman players.\nYou could also say that computers ﬁnally ﬁgured out how humans learn and\nplay and this is why they get so strong. What more of a compliment could there\nbe to us mere mortals?\n\n17\nHow To Read This Book\nChapter 2 is all about neural networks.\nHere we discuss the mathematical\nconcepts and lay down the foundation for all subsequent discussions about the\nrecent improvements based on artiﬁcial intelligence.\nIf this is too bothersome to read due to the math involved, you can also simply\nchose to understand a neural network as a black box that, given a chess position,\nanswers with a positional evaluation; skip this chapter, and come back to it\nlater. If you chose to go with it, you will learn how to approximate unknown,\ncomplex functions, detect Baikinman in images, and construct fancy complex\nneural networks for which you unfortunately very likely lack the computational\npower to train them.\nIn Chapter 3 we look at search methods that chess engines employ. We learn\nhow “classic” chess engines employ alpha-beta search, and how Monte-Carlo\ntree search provides an alternative for chess and also other, more diﬃcult games\nsuch as Shogi or Go. As an example, we will also implement alpha-beta search\nand Monte-Carlo tree search and test it with a simple chess position.\nChapter 4 is where it all comes together. We take a deep look at how AlphaZero\nand Leela Chess Zero work technically, and how they evolved from AlphaGo.\nWe also take a look at the latest revolution in computer chess, namely the\ncombination of classic alpha-beta searchers in chess and Shogi with simpler\nneural networks. These are current state-of-the art and are able to beat Leela\nChess Zero. And they can even run on a standard home computer!\nIn Chapter 5 we will implement our own neural network based game engine,\nsimilar to AlphaZero or Leela Chess Zero.\nSince the game of chess is too\ncomplex to train eﬀectively on standard PC, we will limit ourselves to a simpler\nvariant of chess called Hexapawn. Still, since all methods are there and since\nthe AlphaZero approach is quite agnostic to the particular rules of a game, this\nimplementation can easily be extended to more complex problems including\nchess — provided, the huge required computational resources are available.\nFinally in Chapter 6 we conclude the recent developments and wildly speculate\non what’s there to come.\n\n18\n1. INTRODUCTION\nHow to read this book then? If you are\n• mostly interested in AlphaZero, then read Chapter 2 until multilayer per-\nceptrons in order to get a brief understanding about neural networks.\nThen head to Chapter 3 and read the section about Monte Carlo tree\nsearch and proceed to Chapter 4. Skip the section about AlphaGo and\nproceed directly to the sections on both AlphaGo Zero and AlphaZero.\n• mostly interested in eﬃciently updatable neural networks, then also read\nChapter 2 until multilayer perceptrons in order to get a brief understand-\ning about neural networks, read the section about alpha-beta search in\nChapter 3 and the section about NNUE in Chapter 4.\n• are mostly interested in implementing your own version of AlphaZero\nand have a programmer’s perspective, then just head over to Chapter 5\nand have look at HexapawnZero.\nThen look-up everything on-the go\nwhile you make your way through the code.\nIdeally however you carefully read this book from start to ﬁnish!\n\n2\nA Crash Course into Neural\nNetworks\nThere must be some kind of way\noutta here\nSaid the joker to the thief\nThere’s too much convolution\nI can’t get no relief\nYann LeCun\nIn the movie Pi (1998), math genius and Go enthusiast Max Cohen is convinced\nthat everything in the world can be understood through numbers. The more he\ndelves into his theories, the more he is getting at the verge of becoming insane.\nIt is now our task to study the mathematical and programming concepts of\nneural networks that power modern strong Go and chess engines. Since we do\nnot want to become insane, obviously great care must be taken and we should\nnot take this task lightly.\nMaxCohenby the way preventedhimselffromgetting insaneby self-performing\nan impromptu trepanning. Or maybe that scene was just supposed to be a hal-\n19\n\n\n2.1. THE PERCEPTRON\n21\nexample the two points (0, 5\n3) and (1, 2\n3). In order to ﬁnd suitable 푎and 푏we just\nput the two points as 푥and 푦into 푦= 푎푥+ 푏and obtain two equations:\n5\n3 = 푎0 + 푏\nand\n2\n3 = 푎1 + 푏.\nBy the ﬁrst equation we get 푏= 5\n3, and the second equation leads to 푎= 2\n3 −푏=\n2\n3 −5\n3 = −1. This line is plotted in Figure 2.1.\nGiven some pairs of values it is of course easy to determine the underlying\nfunction if you know the form of the function in advance - like 푦= 푎푥+ 푏. But\nit is diﬃcult to learn the function if you do not know the form of the function at\nall, and if the function is complex. Consider the following examples:\n• Given a person’s income, her age, her job education, her place of living\nand probably other variables, what is the most likely amount of credit\ndebt that she can repay?\n• Given two facial images, do they show the same person or not?\n• Given an arbitrary chess position and assuming perfect play, will White\nwin the game eventually, will Black win eventually, or is it a draw?\nNote that all of these questions can be expressed as mathematical functions over\nsets of numbers. In the ﬁrst case all input variables are numbers, and the credit\ndebt is a number as well. In the second example you could consider images\nas being composed of pixels where each pixel is a triple of red, green and blue\nvalues in the range between 0.0 and 1.0, and the function maps to one if we have\nthe same person, and to zero otherwise. In the third example you could encode\na chess position as a number — we will see later how this can be done — and\nthe function outputs 1 if White wins, −1 if Black wins, and 0 if we have a draw.\nThe goal is now to ﬁnd forms of functions that are general enough and can be\nadjusted to simulate other, very complex functions whose true deﬁnition we do\nnot know — like for the above mentioned examples.\nFor this, researchers took inspiration from the brain. A brain is made up of a\nweb of neurons that are interconnected via so-called synapses. Depending on\n\n\n2.1. THE PERCEPTRON\n23\nenemy is pretty weak. In all other cases you’ll screw up the game eventually\nand loose. Now consider the perceptron in Figure 2.3. It has three inputs, but\nTable 2.1: Logical AND\nQueen Up\nWeak Enemy\nWill Win\n0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n1\nthe ﬁrst input 푥0 is always set to 1, as mentioned above. So variables 푥1 and 푥2\ncorrespond to our inputs queen up and enemy pretty weak. Let’s check whether\nthe output of the perceptron actually calculates our function, i.e. when and if\nwe will win the game:\n푥1 = 0 and 푥2 = 0 :\n푣= −10 + 0 ∗6 + 0 ∗6 = −10\n푥1 = 1 and 푥2 = 0 :\n푣= −10 + 1 ∗6 + 0 ∗6 = −4\n푥1 = 0 and 푥2 = 1 :\n푣= −10 + 0 ∗6 + 1 ∗6 = −4\n푥1 = 1 and 푥2 = 1 :\n푣= −10 + 1 ∗6 + 1 ∗6 = 2\nTurns out this perceptron does not simulate our desired function. But it’s close:\nWhenever we desire the output of the AND function to be 1, the output of our\nsimulation is a positive value. On the other hand whenever we want the output\nof the AND function to be 0, the output of our simulation is strictly smaller than\nzero. So let’s deﬁne our activation function to be a simple thresholding:\n푎(푣) =\n(\n0\nif 푣< 0\n1\nif 푣≥0\n\n\n\n\n2.2. BACK-PROPAGATION AND GRADIENT DESCENT\n27\nthe chosen weights?\nAn obvious way is to take the diﬀerence between the\noutput of the network and the expected result. Let’s write out푖for the output of\nthe network of example 푖, and let’s write 푟푖for the expected outcome of example\n푖. Then one way to measure the error of the network is to take the square of the\ndiﬀerence, i.e. (out푖−푟푖)2.\nFor the second example (푥1 = 1, 푥2 = 1) we have an expected outcome 푟2 = 1,\ni.e. given the output out2 of the network, the error is (out2 −1)2. This function\nis depicted in Figure 2.5.\nAs one can see, the square of diﬀerences puts emphasis on larger errors: A small\ndiﬀerences of 0.1 between out푖and 푟푖results in an error value of 0.01, whereas\na large diﬀerence of 2 results in an error value of 4.\nOur network successfully simulates the intended function if the error of the\nnetwork is minimal. Thus, given an example, we would like to minimize the\nerror of the network. Ideally we’d have an error of 0. Taking a look at Figure 2.5\nor at the expression (out2 −1)2, it is not diﬃcult to see that the error function\nhas a (global) minimum for out2 = 1.\nNow, how do we ﬁnd the minimum of a function? You might remember the\nfollowing three steps from high-school:\n1. Compute the ﬁrst derivative and check where it zeros\n2. Compute the second derivative and check whether you actually found a\nlocal minimum and not a maximum.\n3. We now have one or more local minima. Now compare the function value\nfor all those local minima identiﬁed in the ﬁrst two steps in order to get\nthe global minimum.\nThat’s a valid approach, but diﬃcult to implement in practice. Given a value\n푥0, a much easier approach – especially if we deal with very complex functions\n– is this: to ﬁnd a (local) minimum of a function 푓(푥) we\n• Compute the ﬁrst derivative 푑\n푑푥푓(푥)\n• Evaluate the ﬁrst derivative at 푥0. If\n푑\n푑푥푓(푥0) > 0, then 푓(푥) decreases if\n\n\n2.2. BACK-PROPAGATION AND GRADIENT DESCENT\n29\nalready decreased the error signiﬁcantly.\nWe can repeat this another time, again considering the slope at this point:\n휕푥2\n휕푥(0.45) = 0.9, i.e. the derivative is again positive, and we need to decrease 푥0\nto move in the direction of the minimum. Now we have 0.45−0.5 = −0.05. This\nis the leftmost point marked in Figure 2.6. The error is then (−0.05−0)2 = 0.0025.\nWe slightly overshot the global minimum of 푥0 = 0 a bit, but we are very near\nthe minimum as the error is almost zero.\nNow we have a simple method to compute the local minimum of the error\nfunction. But how does this relate to the network? Sure, we can minimize the\nerror function, but the input to the error function is the output of the network,\ni.e. out푖which we cannot directly change - we can only adjust the weights of\nthe network! Also, we just took a look at the error function for the ﬁrst example,\nwhich was (out1 −0)2. For the second example the error function is slightly\ndiﬀerent with (out2 −1)2.\nTo measure the error of the network w.r.t. 푁examples, we can simply take the\naverage of all individual errors, i.e. Í푁\n푖=1\n1\nerr 푖to get the total error.\nNext, to minimize the total error, let’s take a look at the deﬁnition of the input of\nthe error function, namely the output of the network. This is per deﬁnition:\nout푖= sigmoid(1 ∗푤0 + 푥1 ∗푤1 + 푥2 ∗푤2)\nIn order to estimate how the error changes if we make slight changes to 푤1,\nassume everything else in that function is constant, including 푤0 and 푤2. But\nthat’s precisely the partial derivative w.r.t. 푤1. The partial derivative answers\nthe question: Assuming everything is constant, should we slightly increase or\ndecrease 푤1 in order to lower the global overall error? And for 푤2, the same\nquestion is answered by using the partial derivative w.r.t. 푤2.\nWe now have a plan to train a neural network:\n1. initialize the network with random weights\n2. take a batch of examples, and compute the error of the network w.r.t. these\nexamples\n\n30\n2. A CRASH COURSE INTO NEURAL NETWORKS\n3. compute the global error by averaging over all individual errors\n4. for all weights 푤푖, compute the partial derivative w.r.t. 푤푖\n5. depending on the partial derivative, increase or decrease slightly each\nweight 푤푖\nLet’s put this into practice for our example. We will restrict ourselves to the\nnon-bias weights 푤1 and 푤2. Using the deﬁnition of the net together with the\nactivation and error functions, what we compute for our two examples is:\n퐸total(푤1, 푤2) = 1\n2\n2\nÕ\n푖=1\n\u0012\n1\n1 + 푒−(1∗푤0+푥푖\n1∗푤1+푥푖\n2∗푤2) −푟푖\n\u00132\nand we want to compute 휕퐸total\n휕푤1 and 휕퐸total\n휕푤2 . Note here that 퐸total can be expressed\nas a composition of several functions. We leave out the function arguments on\nthe left side for better readability:\n퐸total = 1\n2\n\u0000(out1 −푟1)2 + (out2 −푟2)2\u0001\nout푖=\n1\n1 + 푒−net푖\nnet푖= 1 ∗푤0 + 푥1 ∗푤1 + 푥2 ∗푤2\nNow recall the chain rule for computing derivatives. We have\n휕퐸total\n휕푤1\n= 휕퐸total\n휕out1\n∗휕out1\n휕net1\n∗휕net1\n휕푤1\n,\nand\n휕퐸total\n휕푤2\n= 휕퐸total\n휕out2\n∗휕out2\n휕net2\n∗휕net2\n휕푤2\nLet’s compute 휕퐸tota푙\n휕푤1 as an example:\n휕퐸total\n휕out1\n=\n휕\n휕out1\n\u0012\n1\n2\n\u0000(out1 −푟1)2 + (out2 −푟2)2\u0001\u0013\n=\n\u0012\n2 ∗1\n2(out1 −푟1)2−1\n\u0013\n∗1\n= out1 −푟1 = 0.95 −0 = 0.95\n\n2.2. BACK-PROPAGATION AND GRADIENT DESCENT\n31\nNote here that since we take the partial derivative w.r.t. out1 we can consider\nout2 as a constant whose derivative is zero. Also note that the 1 in the second\nline at the very right stems from the inner derivative of out1.\nLet’s tackle the next derivative from the chain. The derivative of the sigmoid\nfunction is a tricky one, but note that 휕sigmoid(푥)\n휕푥\n= sigmoid(푥)(1 −sigmoid(푥)).\nIndeed, we have\n휕out1\n휕net1\n=\n휕\n휕net1\n\u0012\n1\n1 + 푒−푛푒푡1\n\u0013\n=\n푒net1\n(1 + 푒푛푒푡1)2\n= out1 ∗(1 −out1) = 0.95 ∗(1 −0.95) ≈0.048\nLast, we have\n휕net1\n휕푤1\n=\n휕\n휕푤1\n(푤0 ∗1 + 푤1 ∗1 + 푤2 ∗0) = 푤1 = 2\nPutting all this together we have 휕퐸total\n휕푤1\n= 0.95 ∗0.048 ∗2 = 0.0912. The partial\nderivative is positive, so we should decrease 푤1 a little bit.\nBy how much? That’s a diﬃcult question. If your remember Figure 2.6 you\nnotice that if we take too large steps, we could overstep the minimum. If we take\nvery small steps this should not happen, but it could take quite some iterations\nuntil we come close to the minimum. A general rule of thumb is to update a\nweight as 푤푖‘ = 푤푖−휂휕퐸total\n휕푤1\nwhere 휂is just some heuristically chosen positive\nconstant factor, called the learning rate. Note how the partial derivative takes\ncare of the correct sign (e.g. here the derivative is positive, thus 휂휕퐸total\n휕푤1\nis also\npositive, and we correctly subtract from 푤1. Let’s take 휂= 10, and we have\n푤1 = 2 −0.912 = 1.088.\nThen for 휕퐸total\n휕푤2 we can do the same computation. You can verify yourself that we\nobtain −0.01 ∗0.0099 ∗3 = −0.000297. We have 푤2 = 3 + 0.0002978 = 3.000297.\n\n32\n2. A CRASH COURSE INTO NEURAL NETWORKS\nUsing the network with the updated weights we get\n푥1 = 1 and 푥2 = 0 :\nsigmoid(1 + 1.088 ∗1 + 3.000297 ∗0) ≈0.75\n푥1 = 1 and 푥2 = 1 :\nsigmoid(1 + 1.088 ∗1 + 3.000297 ∗1) ≈0.99\nAs we can see, the network evaluates the example 푥1 = 1, 푥2 = 1 still almost at\n1, but signiﬁcantly reduced the output of the example of 푥1 = 1, 푥2 = 0 towards\nthe desired output of 0. One can expect that after some more iterations the\nnetwork will yield weights which correctly simulate the desired AND function.\nBefore putting this into practice, there is a small amount of terminology and\nobservations that we need to make sense of. The algorithm is called back propa-\ngation by gradient descent. If we think of one calculation of the network from left\nto right as a forward pass, then after all, we are propagating the error back —\ni.e. from right to left — through the network to update the weights. Roughly,\na gradient is just the vector of all partial derivatives, and this is precisely what\nwe employ here for the update rule to descent towards a minimum of the error\nfunction.\nSince we only updated two weights, we can visualize the error as a function of\nthe two weights in a three-dimensional plot. That is precisely the function 퐸total\nthat describes the whole network, and depicted in Figure 2.7.\nGradient descent works by computing the current error w.r.t. the initial random\nweights 푤1 and 푤2.\nThe error is a point located somewhere in the plot of\nFigure 2.7 “on top of the hill”. It then computes the gradient in order to make\na small step in a direction where the error slightly decreases. Making a lot of\nthese small steps results in reaching the minimum.\nIn the example, we had actually three inputs, however the ﬁrst one was set to\na ﬁxed value of one. The weight corresponding to that input creates a small\nbias. If we look at the network equation we notice that if all inputs are zero,\nthe output of the network before applying the activation function is exactly the\nbias weight. That weight is still subject to change by gradient descent though.\nThe role of a bias is just as the name implies — without caring for speciﬁc input\nvalues, it tunes the overall network to output values that minimize the error.\n\n\n34\n2. A CRASH COURSE INTO NEURAL NETWORKS\nIn the example, we took two samples — namely 푥1 = 1, 푥2 = 0 and 푥1 = 1, 푥2 = 1\nor shorter [1, 0] and [1, 1] —, computed the output of the network, computed the\nerrors for each sample and averaged the error over these two samples. Then we\napplied gradient descent to update the weights of the network. Instead we could\nhave also employed a diﬀerent strategy: Just take one sample, compute the error\nand update the weights of the network immediately. Yet another possibility is\nto take all samples — here [0, 0], [1, 0], [0, 1] and [1, 1]. The number of samples\none takes to update the weights of the network once is referred to as the batch\nsize.\nIn practice, one usually has lots, if not millions of samples. In such a case it is not\neﬃcient to apply all samples at once. It has also been observed that the network\ndoes not learn very well in such a case. On the other hand taking only one\nexample is slow. Typical batch sizes are 32, or 64, 128, and 256, but in general it\nis diﬃcult for a particular problem to choose the optimal batch size in advance.\nIn our example there are only four samples. Suppose we choose a batch size\nof 4. One step of gradient descent is not enough to signiﬁcantly change the\nweights of the network. Instead, we have to repeat the four examples over and\nover for the network to reach a minimal error. We refer to one application of\na batch as one epoch. In the example, we might need at least a few hundred\nnumber of epochs until the network learns the AND function. Of course this\ndepends on both the initial randomly chosen weights as well as the learning\nrate.\nWe chose the mean squared error as the error function, but as mentioned there\nare other ways to quantify the error of the networks as well, e.g. by taking\nthe absolute diﬀerence of the output and the expected result of a sample. In\nour example for the AND function, the output of the network was a single\nnumerical value. If we do classiﬁcation instead of regression, we might want\nto somehow measure the correctly versus incorrectly classiﬁed objects that we\nput in the network. We will see a suitable error function for that scenario in the\nnext section. In general, machine learning frameworks refer to the value that is\nto be minimized during learning not as the error but as the loss of the network.\nThe activation function used here was the sigmoid function.\nBut there is a\n\n2.3. CLASSIFICATION VS REGRESSION\n35\nplethora of other functions as well, each with its own mathematical and practical\nproperties. Without diving into details, a popular current choice is a rectiﬁer, or\nrectiﬁed linear unit, or short ReLU. It is deﬁned simply as 푓(푥) = max(0, 푥). Since\nReLU’s play quite an important role in modern neural networks, we will revisit\nthem in a later section.\nThe learning rate 휂= 10 in our example was chosen ad-hoc. As can be observed\nin Figure 2.6, a too large learning rate can result in that we step over a minimum\nagain and again and never reach it. On the other hand a low learning rate will\nresult in only slight changes of the weights of the network, and thus progress is\nslow and learning takes forever. There are other ways to update the network.\nQuite eﬃcient is an algorithm dubbed Adam [KB15]. Here the underlying idea\nis to employ individual learning rates for each weight depending on properties\nof the gradients. Adam usually converges much faster to the minimum than\ngradient descent, but can also fail to converge in cases where gradient descent\neventually converges.\nIn general, choosing the best parameters to train a network eﬀectively is an active\narea of research. Both from a mathematical standpoint in that researchers try to\nidentify mathematical properties that make training the network more powerful\nand eﬀective, but also from an engineering standpoint in that some parameters\nwork better for certain classes of problems than others.\n2.3\nClassiﬁcation vs Regression\nIn the previous section we discussed a problem of learning a speciﬁc value\n(the result of the AND function). Another problem that was stated before is:\nGiven a person’s income, her age, her job education, her place of living and\nprobably other variables, what is the most likely amount of credit debt that\nshe can repay? In such problems we want to learn to predict one speciﬁc value.\nHence, a corresponding neural network has one speciﬁc output node. This kind\nof problem is referred to as regression.\nWhat about other problems, such as: Given images of three known persons and\none image with an unknown person, is the unknown person one of the three\n\n36\n2. A CRASH COURSE INTO NEURAL NETWORKS\npersons? Or is the person not one of these known persons?\nA straight-forward way of encoding this as a regression problem would be:\nA numerical output value of 0 represents an unknown person, a value of 1\nrepresents the ﬁrst person, 2 the second, and 3 the third person.\nTrain the\nnetwork with the images of the known persons, and get a prediction for the\nimage with the unknown person. Then, if the predicted value is close to say 1\nwith a value of 1.1, we conclude it is the ﬁrst person. If the prediction is around\n1.5 we could conclude it is either the ﬁrst or the second person.\nBut what if the image shows a person looking quite close to both the ﬁrst and\nthird, but not the second person? A value of 2 would be the average value, but\nthen we would falsely conclude the second person is shown.\nSuch problems are about classiﬁcation: Given a choice among 푛several cat-\negories, which one ﬁts best? A much better network architecture is then to\ncreate an output layer with 푛nodes. On each node the output value indicates\nhow probable/good the category ﬁts. For training, we extend the expected\noutcomes for samples to a vector. For example for the images of the (known)\nﬁrst person, an expected outcome is the vector [1.0, 0.0, 0.0], i.e. the ﬁrst output\nnode should result in a 1.0 (= category ﬁrst person ﬁts a 100%), and the second\nand third output nodes should be 0.0 (= category second and third person do\nnot ﬁt). For training with images of the second and third person the expected\noutcome would be [0.0, 1.0, 0.0] and [0.0, 0.0, 1.0]. A prediction of the network\nfor an unknown image with values [0.7, 0.1, 0.2] would then be: With a high\nconﬁdence 70 percent this image shows the ﬁrst person, it’s very likely not the\nsecond person (10 percent), and the chance that it’s the third person is about 20\npercent.\nIf we want to interpret the output of the network as probabilities we need to\nmake sure that the sum of all outputs equals 1, otherwise these numbers don’t\nmake a lot of sense. We can ensure this by applying the softmax function. Let\n[표1, . . . , 표푛] be the outputs of the network. The softmax function 휎is deﬁned as\n휎(o) =\n푒표푖\nÍ푛\n푗=1 표푗\nfor 푖= 1, . . . 푛\n\n2.3. CLASSIFICATION VS REGRESSION\n37\nHow do we measure the error, i.e. loss of our network? Softmax is often used\ntogether with negative log likelihood: We take the negated logarithm of the output\nprobability w.r.t. the correct label.\nSuppose we have a batch of two examples. One shows an image of the ﬁrst\nperson, and the output vector is [0.7, 0.1, 0.2], and another shows the image\nof the second person with an output vector of [0.6, 0.3, 0.1]. We sum over the\nnegated logarithms for the correct labels and divide by two, i.e.\n−log(0.7) −log(0.3)\n2\n≈0.78\nThis is our loss value.\nNow let’s assume that we updated the network via\ngradient descent, and suppose the network now outputs some slightly more\naccurate classiﬁcation for the second example, say [0.3, 0.6, 0.1]. We then have\n−log(0.7) −log(0.6)\n2\n≈0.43\nIndeed, if the network becomes moor accurate, our loss decreases.\nOne thing to note here is that instead of decreasing the loss, we could also omit\nthe negation part and use directly the logarithm. Now we do no longer seek\nto minimize the error, but to maximize the (log) probabilities. Gradient descent\nupdates all weights of the network by subtracting a fraction of the gradient, i.e.\npartial derivatives. In this scenario we can simply add a fraction of the gradient\nand perform gradient ascent.\nYet another way to measure loss is to take the output of the network and apply\ncategorical cross entropy. Assume again there are 푛outputs 표1, . . . , 표푛. For each\nwe have an expected outcome 푒1, . . . , 푒푛. Then we deﬁne as the loss\nloss = −\n푛\nÕ\n푖=1\n푒푖· log 표푖\nFor the ﬁrst example above we would obtain 1·log(0.7)+0·log(0.1)+0·log(0.2) ≈\n−0.35.\nNote that we might not always have binary values for the expected\n\n38\n2. A CRASH COURSE INTO NEURAL NETWORKS\noutcome. Depending on what you are training for, the expected outcome could\nbe a vector of probabilities itself that sums up to one. Imagine for example\nprobabilities for moves in a speciﬁc chess position. There could be more than\none good move. An expected outcome might be [0.5, 0.4, 0.1]: Out of three\npossible moves, the ﬁrst two are almost equally good moves, but the last one\nlooses.\n2.4\nPutting it into Practice\nLet’s put the previous section into practice and create a small neural network\nto learn the AND function. This is depicted in Listing 2.1. Let’s go quickly\nthrough the code:\nListing 2.1: Neural Network for AND\n1 import numpy as np\n2 import tensorflow as tf\n3 from tensorflow import keras\n4 import random\n5\n6 random.seed (42)\n7 np.random.seed (42)\n8 tf.random.set_seed (42)\n9\n10 x = []\n11 y = []\n12\n13 x = [[0 ,0] ,[0 ,1] ,[1 ,0] ,[1 ,1]]\n14 y = [ 0, 0, 0, 1]\n15\n16 model = tf.keras.models.Sequential ()\n17 model.add(tf.keras.Input(shape =(2,)))\n18 model.add(tf.keras.layers.Dense(units=1, activation=’sigmoid ’))\n19 print(model.summary ())\n20\n21 model.compile(optimizer=keras.optimizers.SGD(learning_rate =0.1),\nloss=keras.losses.MeanSquaredError ())\n22\n23 print(np.array(x).shape)\n\n2.4. PUTTING IT INTO PRACTICE\n39\n24\n25 model.fit(np.array(x), np.array(y),batch_size =4, epochs =50000)\n26\n27 q = model.predict( np.array( [[0 ,1] ,[1 ,1]] )\n)\n28 print(q)\n• In lines 1 to 4 we import several packages: numpy is a general numeric\npackage for python, tensorflow is a high-performance, low-level neu-\nral network framework and keras is a high-level python interface for\ntensorflow.\n• As mentioned, neural network weights are initialized randomly. In order\nfor you to reproduce the results mentioned here, we also import random\nand seed all random number generators from python, numpy and ten-\nsorﬂow with a ﬁxed value, so that we always generate the same random\nnumbers.\n• In lines 10 to 14 we create samples and expected outcomes. The array 푥\ncontains all four possible pairs of input values. The array 푦contains the\ncorresponding desired output. Note that only for input [1, 1] we expect\nan output of 1.\n• In lines 16 to 18 we deﬁne the neural network architecture. Considering\nFigure 2.3, we start from left to right, i.e. we ﬁrst deﬁne the input layer.\nOur input layer has precisely two inputs, i.e. one for 푥1 and one for 푥2.\nNote that we do not count 푥0, as it is always added automatically by keras\nitself.\n• In line 18 we add the output layer. The output layer consists only of a single\nnode (unit). We use the sigmoid function as the activation function.\n• In line 21 we compile the model. For learning we use stochastic gradient\ndescent (SGD), which denotes the back-propagation algorithm by using\ngradient descent. The loss function is mean squared error.\n• In line 25 we train our network. As a batch size we use 4 - after all, we\nonly have four samples - and we repeat our batch 50000 times. On my\ncomputer this takes a few minutes to compute\n\n40\n2. A CRASH COURSE INTO NEURAL NETWORKS\n• Finally, in line 27 we feed the network some samples and get the prediction\nof the network. Note that these could be potentially unknown examples.\nHere the network predicts approximately 0.033 for sample [0, 1] and 0.96\nfor sample [1, 1]. That’s quite near the expected outcomes of 0 and 1.\nTraining our network succeeded!\nAs you can see we specify a lot of parameters when we deﬁne and compile\nour network. You can try diﬀerent parameters and check the outcome your-\nself. For example, replace keras.optimizers.SGD by another optimizer such\nas keras.optimizers.Adam, change the learning rate, change the number of\nepochs, decrease the batch size, or change the loss function to the absolute error\n(MeanAbsoluteError). All these parameters have an eﬀect how fast and how good\nthe network trains.\nYou can also try to compute another simple binary function, the exclusive or, or\nXOR. This function is deﬁned as: The output is 1, if the input arguments diﬀer,\nand 0 otherwise. Namely\n• 0 XOR 0 maps to 0\n• 0 XOR 1 maps to 1\n• 1 XOR 0 maps to 1\n• 1 XOR 1 maps to 0\nTo train the network for XOR, we only have to change the array of expected\noutcomes by setting 푦= [0, 1, 1, 0]. I challenge you to ﬁnd parameters on which\nthe network succeeds in training. It seems to be impossible. But why?\n2.5\nInherent Limits of a Single Perceptron\nLet’s have a deeper look. What does a perceptron actually calculate? According\nto our deﬁnition, we have\n푣= 1 ∗푤0 + 푥1 ∗푤1 + 푥2 ∗푤2 + · · · + 푥푛∗푤푛\n\n\n\n\n44\n2. A CRASH COURSE INTO NEURAL NETWORKS\nare just our training data, and otherwise it is the result of a computation of one\nnode.\nConsider for example the node marked 1 in Figure 2.10. Suppose we have 푛\nnodes in the input layer. The value at 1 is then computed as\n푤1\n0 ∗푥0 + 푤1\n1 ∗푥1 + · · · + 푤1\n푛∗푥푛\nand similar for node 2 we have\n푤2\n0 ∗푥0 + 푤2\n1 ∗푥1 + · · · + 푤2\n푛∗푥푛\nNote that at each node we could also apply an activation function that modiﬁes\nthe sum that we just computed before passing it to the next layer. We’ll skip\nwriting this out, as we would soon be lost in indices!\nWe can also extend back-propagation w.r.t. multiple layers. The chain rule for\nthe partial derivatives simply contains more chains, and again more computa-\ntion has to be done, but the underlying principle is the same: We start from right\nto left, compute the partial derivative for each node of the rightmost layer, then\ncompute the partial derivative for each node of the layer before, and continue\nuntil we reach the leftmost input layer.\nLet’s use our newly gained knowledge and revisit the XOR function. We just\nneed to replace a few lines of our simple perceptron and insert another layer\ninbetween with lots of (8) nodes. We simply change the lines of Listing 2.2 to\nthe ones shown in Listing 2.3.\nListing 2.2: Neural Network for AND\n1 model = tf.keras.models.Sequential ()\n2 model.add(tf.keras.Input(shape =(2,)))\n3 model.add(tf.keras.layers.Dense(units=1, activation=’sigmoid ’))\nListing 2.3: Neural Network for XOR\n1 model = tf.keras.models.Sequential ()\n2 model.add(tf.keras.Input(shape =(2,)))\n3 model.add(tf.keras.layers.Dense(units=8, activation=’sigmoid ’))\n4 model.add(tf.keras.layers.Dense(units=1, activation=’sigmoid ’))\n\n2.6. MULTILAYER PERCEPTRONS\n45\nSuccess! After a few minutes of training, the network predicts 0.97 for input\n[0, 1] and 0.02 for input 1, 1.\nThe term deep learning was coined by using deep networks — that is, having a\nlot of layers inbetween input and output. They are also called hidden layers,\nsince we can not directly observe the inputs to and outputs from these layers\n— our training data consists only of inputs to the ﬁrst layer, and we compare\nthe expected outcome with the prediction of the network that we get from the\noutput of the last layer. It is precisely the addition of many hidden layers that\nmakes deep learning so powerful. Of course this comes at a cost, namely the\nrequired computational eﬀort.\nBefore we continue, lets have a look at another function. Let’s try to create\na network that learns the function 푓(푥) = 푥2. The source code is shown in\nListing 2.4.\nListing 2.4: Neural Network for 푥2\n1 import numpy as np\n2 import tensorflow as tf\n3 from tensorflow import keras\n4 import matplotlib.pyplot as plt\n5 import random\n6\n7 random.seed (42)\n8 np.random.seed (42)\n9 tf.random.set_seed (42)\n10\n11 x = []\n12 y = []\n13 for i in range (0 ,10000):\n14\nxi = random.randint (0 ,5000)\n15\nxi = xi/100\n16\nif(xi==2.0 or xi ==4.0):\n17\ncontinue\n18\nyi = xi**2\n19\nx.append ([xi])\n20\ny.append(yi)\n21\n22 print(np.array(x)[0:20])\n\n46\n2. A CRASH COURSE INTO NEURAL NETWORKS\n23\n24 model = tf.keras.models.Sequential ()\n25 model.add(tf.keras.Input(shape =(1,)))\n26 model.add(tf.keras.layers.Dense(units=8, activation =\"relu\"))\n27 model.add(tf.keras.layers.Dense(units=8, activation =\"relu\"))\n28 model.add(tf.keras.layers.Dense(units=1, activation =\" linear \"))\n29 print(model.summary ())\n30\n31 model.compile(optimizer=keras.optimizers.Adam(),\n32\nloss=keras.losses.MeanSquaredError ())\n33\n34\n35 print(np.array(x).shape)\n36\n37 model.fit(np.array(x), np.array(y),\n38\nbatch_size =256, epochs =5000)\n39\n40\n41 q = model.predict( np.array( [[2] ,[4]] )\n)\n42 print(q)\n43\n44 original = [ x**2 for x in range (0,50)]\n45\n46 plt.figure ()\n47 plt.plot(original)\n48 plt.show()\n49\n50 predicted = model.predict(np.array([ x for x in range (0,50) ]))\n51\n52 plt.figure ()\n53 plt.plot(predicted)\n54 plt.show()\nThe network deﬁnitions should be clear by now, but there are a few diﬀerences\nin the source code compared to the other examples:\n• In lines 13 to 20 we create some input values in the range 0...50 and add\nthem to the array 푥. We put the corresponding squared value in the array\n푦. We explicitly avoid to add values 2 and 4 to the list of training data,\nbecause we want to evaluate how well the network predicts unknown\n\n2.6. MULTILAYER PERCEPTRONS\n47\nvalues. We of course expect predictions near 4 and 16 respectively.\n• In line 41 we compute the prediction of the network for the input values 2\nand 4. We get prediction values around 3.2 and 23 which is close enough.\n• In lines 42 to 54 we plot the original function in the range 0...50 and we\nalso plot the predictions of the network in that range. The plot essentially\nshows the function 푦= 푥2. Apparently, our network works quite well.\nAll in all, we succeed in training a network to compute the function 푓(푥) = 푥2.\nLet’s investigate some more. Try to change the lines of Listing 2.5 to the ones of\nListing 2.6.\nListing 2.5: Out of Domain Predictions\n1 original = [ x**2 for x in range (0,50)]\n2 ...\n3 predicted = model.predict(np.array([ x for x in range (0,50) ]))\nListing 2.6: Out of Domain Predictions\n1 original = [ x**2 for x in range (50 ,100)]\n2 ...\n3 predicted = model.predict(np.array([ x for x in range (50 ,100) ]))\nIn other words, let’s investigate how well our network predicts the completely\ndiﬀerent range of input values 50...100 for which there was no training data\nwhatsoever — all our samples were in the range 0...50. One might expect that\nthe networks performs well on all ranges of our function 푓(푥) = 푥2. But as can\nbe seen in the generated plot, this is not the case. In general, networks perform\nquite bad if they are fed with data that is completely out of their domain.\nMore intuitively spoken: Suppose you create a network that identiﬁes chess\npieces in an image, and train it with pictures of chess pieces. You would not\nexpect it to be good at identifying Shogi pieces.\nIt does not mean that the\nnetwork cannot recognize chess pieces from images that it has not seen during\ntraining — after all, our network was good at predicting input values 2 and 4\n\n\n2.8. CONVOLUTIONAL LAYERS\n49\nas matrices\nx =\n\n푥0\n...\n푥푛\n\nand w =\n\u0014\n푤0\n푤1\n· · ·\n푤푛\n푣0\n푣1\n· · ·\n푣푛\n\u0015\n,\nIf we then compute the matrix product wx we have\nwx =\n\u0014\n푤0 ∗푥0 + 푤1 ∗푥1 + · · · + 푤푛∗푥푛\n푣0 ∗푥0 + 푣1 ∗푥1 + · · · + 푣푛∗푥푛\n\u0015\n,\n(2.1)\nand this is precisely the input of the hidden layer! In other words: Computing\nforward passes of a neural network can be reformulated into computations over\n(potentially large) matrices. This is also known as vectorizing a computational\nproblem.\nNote that each entry of the matrix in Equation 2.1 is completely independent\nof each other. That’s why it is easy to compute each entry of the result matrix\nin parallel. CPUs are intended as general processing units, i.e. no matter what\nproblem is given, they almost always perform with decent speed. But for com-\nputer graphics, we mostly need fast matrix operations, and that’s what GPUs\nare highly optimized for – at the cost of not being very universal computing\nmachines. This is why having a fast GPU will help you train networks much\nfaster, often in the dimension of a two-digit factor.\nNeedless to says that back-propagation can also be reformulated in terms of\nmatrix operations. So if you really want to program and experiment with neural\nnetworks, a fast graphic card is a must. Even better is of course working at a\ncompany that is capable and willing to design and produce their own dedicated\nprocessors just for accelerating neural network training.\n2.8\nConvolutional Layers\nLet’s again reconsider the problem of image recognition. As mentioned, sup-\npose you are given a facial image that is digitally stored on a passport and want\nto compare it to a live image taken during border control. Do these show the\nsame person?\n\n50\n2. A CRASH COURSE INTO NEURAL NETWORKS\nFigure 2.12: Baikinman. Leader of the Viruses.\nHow would one intuitively approach such a problem? A natural idea is to look\nfor unique features of the person’s face like the shape of the head, eye, nose\nor ears, or the person’s eye and hair color. Maybe there are also birth marks,\nslight deviations in skin color, wrinkles... the list goes on and on. The next step\nwould then to come up with a list of most important features, and try to write\ncomputer programs that extract these information from the image.\nThis is exactly what researches tried to do in the early days of image processing\nand face recognition. Let’s start with a small example of edge detection. After\nall, when looking for the outline of the head, we need to separate it from the\nbackground of the image.\nConsider Figure 2.12. The stuﬀed animal in the grayscale image is Baikinman, the\nevil villain from the Japanese kid’s show Anpanman. Let’s try to construct a ﬁlter\nthat after processing the image with that ﬁlter gives us the shape of Baikinman.\nRemember that a grayscale image is nothing else that a two-dimensional matrix\nof values, usually in the range 0...255. We can use an edge detection ﬁlter that\nis given by the matrix\n\n1\n0\n−1\n1\n0\n−1\n1\n0\n−1\n\n\n\n52\n2. A CRASH COURSE INTO NEURAL NETWORKS\nand this makes up the upper left pixel value of our ﬁltered image. We continue,\nas illustrated in Figure 2.13, ending up with a slightly smaller image of size 4푥4.\nIf we want to, we can further ﬁlter the image for better illustration by setting\neach pixel value that is above a certain threshold to 0 and below to 255, thereby\ncreating a strictly black/white image.\nIt’s straight-forward to implement this in Python, and the source-code is shown\nin Listing 2.7.\nListing 2.7: Edge Detection for Baikinman\n1 import imageio\n2 import numpy as np\n3 from skimage import img_as_ubyte\n4\n5 im = imageio.imread(’baikinman.jpg ’)\n6\n7 yy = im.shape [0]\n8 xx = im.shape [1]\n9\n10 filter1 = [ [1, 1, 1],\n11\n[0, 0, 0],\n12\n[-1, -1, -1]]\n13\n14 sobel = [ [1, 2, 1],\n15\n[0, 0, 0],\n16\n[-1, -2, -1]]\n17\n18 filtered_image = []\n19 for y in range(0,yy -3):\n20\nrow = []\n21\nfor x in range(0,xx -3):\n22\nval = 0.0\n23\nfor i in range (0,3):\n24\nfor j in range (0,3):\n25\nval += im[y+i, x+j] * filter1[i][j]\n26\nrow.append(val)\n27\nfiltered_image.append(row)\n28\n29 img1 = np.array(filtered_image)\n30 imageio.imwrite (\" baikinman_filter_no_thresh.png\", img1)\n\n2.8. CONVOLUTIONAL LAYERS\n53\nFigure 2.14: Baikinman, applied ﬁlter and thresholding.\nFirst we read the image using the imageio library, then loop through the image,\nand write the result back to a ﬁle. Thresholding is here omitted for brevity.\nFigure 2.14 shows the ﬁltered and thresholded image of Baikinman. It seems\nthat after all, Baikinman’s unique features are his scary teeth!\nCan we do better?\nWell there is another ﬁlter based on the so-called Sobel\noperator. The diﬀerence is a subtle change in the ﬁlter matrix, here we use\n\n1\n0\n−1\n2\n0\n−2\n1\n0\n−1\n\nHow did researchers initially came up with these values?\nActually, I have\n\n54\n2. A CRASH COURSE INTO NEURAL NETWORKS\nFigure 2.15: Baikinman, applied sobel operator and thresholding.\n\n2.8. CONVOLUTIONAL LAYERS\n55\nno clue. Probably by analysing the pixel values of example images and then\nanalysing how a sharp threshold looks like and then ﬁnally coming up with\nnumbers for the ﬁlter matrix.\nSo, which one is better? Actually, it’s hard to see (cf. Figure 2.15) and personally\nI see almost no diﬀerence here. Selecting a suitable ﬁlter also certainly depends\non what we want to do with the ﬁltered image (here: face recognition). Can we\ncome up with better values?\nYou probably already ﬁgured where I am going at. Equation (2.2) looks sus-\npiciously close to the computation behind one layer of a neural network. In\nother words: We can simply translate this ﬁlter operation — convolution — to a\nneural network, and let the network ﬁgure out suitable weights — the entries\nof the ﬁlter matrix — by itself using gradient descent and backpropagation.\nThe translation of this ﬁlter operation to a convolutional layer is illustrated in\nFigure 2.16. We take each pixel as a potential input value.\nThen we just wire the nine values of each sub-image and connect them to an\noutput value. Each arrow here has one weight associated which corresponds\nto one entry of the above mentioned ﬁlter matrix.\nThe associated weights\nthen make up the actual ﬁlter. Or rather, the ﬁlter is designed speciﬁcally for\nthe particular network, namely by training the network and thus updating the\nweights. Next we do this for all other combinations of pixel values. Note that\nwe reuse the same weights thereby signiﬁcantly reducing their total number, as\nillustrated for example for weight 푤8 — here we apply one ﬁlter to all sub-images.\nWe can of course always increase the number of ﬁlters.\nThere are four natural extensions that we will discuss just very brieﬂy:\n1. First, we can extend a ﬁlter to handle color images. We simply apply it\nto either all or some of the red, green and blue color channels and extend\nthe summation by applying it to the channels as well.\n2. Second, we can of course apply (train) several ﬁlters in parallel within\none layer. We just have to add another ﬁlter matrix, and output nodes\nat the output layer, and wire the weights that represent the ﬁlter matrix\naccordingly.\n\n\n2.8. CONVOLUTIONAL LAYERS\n57\n3. Similar to a standard perceptron, we can add a small bias value to the\nresult of the summation, and apply a (non-linear) activation function like\nsigmoid afterwards.\n4. When iterating over the source image, we stepped through the image one\npixel at a time. Of course we can use larger steps and consider only i.e.\nevery second, every third or every tenth pixel, and simply not consider\nthe pixels in between. Of course this comes as the cost of precision, but\ncan be much faster. The step-size is referred to as stride.\nWe will skip the speciﬁc details of these extensions though. What is rather\ninteresting though is to visualize what the learned ﬁlter actually does. Let’s\nconsider a small example of the VGG16 model. This is a neural network for im-\nage recognition with pre-trained weights released by Visual Geometry Group\n(VGG), at the Department of Engineering Science, University of Oxford under\nthe Creative Commons Attribution 4.0 International Public License (CC BY 4.0).\nIt is one of the examples delivered with the Keras framework, and ideally suited\nto play around with and investigate a complex network without having to actu-\nally train it yourself. We can now visualize the output of the ﬁrst convolutional\nlayer. Let’s ﬁrst have a look on the numerical values of the ﬁlters with Listing 2.8.\nListing 2.8: Visualize Convolution Filter\n1 from tensorflow.keras.applications.vgg16 import VGG16\n2 model = VGG16()\n3 print(model.summary ())\n4 filt , bias = model.layers [1]. get_weights ()\n5 print(filt.shape)\n6 print(filt [:,:,0,0])\nIn that listing, we ﬁrst load the network including its weights, and then print a\nsummary of the model. You can verify that there is indeed an input layer fol-\nlowed by the ﬁrstconvolutionallayer. Wecanuse theKerasfunctionget_weights()\nto get the actual weights of that layer, which are equivalent to the ﬁlter values.\nThe array of ﬁlter values has shape (3, 3, 3, 64), i.e. we have 64 ﬁlters of di-\nmension (3, 3) — like the ﬁlter for the edge detector —, and we have three of\nthose for each color channel, namely red, green and blue. The last line gives the\n\n58\n2. A CRASH COURSE INTO NEURAL NETWORKS\nnumerical values of the ﬁrst ﬁlter for the ﬁrst color channel as2\n\n0.42\n0.37\n−0.06\n0.27\n0.03\n−0.36\n−0.05\n−0.26\n−0.35\n\nIt’s even more fun to see what the convolutional layer does to Baikinman. We\ncan visualize this by creating a new neural network, which consists of only the\ninput layer and the ﬁrst convolutional layer of the original network. The output\nof the network is then the output of the convolutional layer, i.e.\nthe ﬁlters\napplied to our input image of Baikinman.\nThis is shown in Listing 2.9.\nListing 2.9: Visualize Several Convolution Filters\n1 from\ntensorflow.keras.applications.vgg16 import VGG16\n2 from\ntensorflow.keras.applications.vgg16 import preprocess_input\n3 from\ntensorflow.keras.preprocessing.image import load_img\n4 from\ntensorflow.keras.preprocessing.image import img_to_array\n5 from\ntensorflow.keras.models import Model\n6 from numpy import expand_dims\n7 import\nmatplotlib.pyplot as plt\n8\n9 model = VGG16()\n10 model = Model(inputs=model.inputs , outputs=model.layers [1]. output)\n11 model.summary ()\n12\n13 image = load_img(’baikinman.jpg ’, target_size =(224, 224))\n14\n15 image = img_to_array(image)\n16 image = expand_dims(image , axis =0)\n17 image = preprocess_input(image)\n18\n19 filtered_image = model.predict(image)\n20\n21 filter_index = 3\n22\n2This was at the time of writing. If the model is re-trained at any stage, your values might\ndiﬀer\n\n\n\n2.9. SQUEEZE-AND-EXCITATION NETWORKS\n61\nHow would a Grandmaster assess a chess position? Of course she would look at\ntactics in a position, like winning a pawn or a piece. But of great importance is\nalso the (visual) structure. Are there open ﬁles? Where are the knights placed?\nDo the bishops have open lines, or are they blocked by pawns? Entire books\nhaven been written about pawn structures [Sha18]. While it of course does not\nmake sense to feed directly literal images of chess positions into a network, the\nlogical 8x8 representation of a chessboard has already an image-like structure.\nThus we can hope that a convolutional layer is suitable to learn ﬁlters that\nextract abstract, structural information about a position.\nIt is precisely this\npattern recognition task that humans are very good at, but which are very\nhard to formulate as a small set of objectively computable rules. We will see\nlater when discussing speciﬁc network structures of successful neural network\napproaches that convolutional layers are a crucial tool to tackle this pattern\nrecognition task.\n2.9\nSqueeze-and-Excitation Networks\nWe only brieﬂy mentioned how to extend convolutional layers to handle diﬀer-\nent channels. When we think of images, this is naturally color channels. In other\nmachine learning contexts a channel might be something completely diﬀerent.\nFor example in chess, we might encode a chess position by creating one image\nof all white rooks, one image of all white knights, and continue like that for all\nwhite and all black pieces. One channel encodes here the position of one piece\ntype of one player. But let’s stick to color images for now, as that’s probably\nmore intuitive.\nWe noted how an extension of convolutional layers can be done for multiple\n(color channels), namely apply a ﬁlter to either all or some of the red, green and\nblue color channels and extend the summation accordingly.\nBut this also means that we weigh each channel equally. Now imagine we are\ntrying to create a network that is able to predict whether a color image contains\nan image of Baikinman or not. Baikinman has very distinct colors: A dark\nblue, almost completely black body, bright white teeth, purple hands and feet,\nand some parts of his face are purple, too. It is not diﬃcult to imagine that\n\n62\n2. A CRASH COURSE INTO NEURAL NETWORKS\nsome color channels are more important than other color channels. Therefore\nit might make sense to weight each channel diﬀerently when computing the\noverall output (i.e. the sum) of the convolutional layer.\nHowever we can even go one step further. There could be interdependencies\nbetween the channels. Consider the purple hands and feet of Baikinman. If we\nhave a color image encoded by cyan, magenta and yellow color channels, the\npurple is encoded with a high value of cyan, a high value of magenta and a\nmedium/low value of yellow. Then for a ﬁlter that detects the hands and feet\nof Baikinman, there is clearly an interdependency between the color channels.\nSqueeze and Excitation Networks are motivated by this idea. Quite a recent im-\nprovement suggested by Hu et al. [HSA+20], they are built upon convolutional\nor residual layers by explicitly modelling interdependencies between channels.\nSqueeze and Excitation Networks improved image recognition tasks on certain\nbenchmarks by 25 percent. At the time of writing they are not yet available\nas standardized building blocks in typical deep learning frameworks such as\ntensorﬂow, but they are also not too diﬃcult to construct either.\nFrom a broader perspective, Squeeze and Excitation networks are more of an\nevolution than a revolution in deep learning. A signiﬁcant one nonetheless,\nbut still more of an evolution. It therefore makes sense to skip the mathemat-\nical details of constructing Squeeze and Excitation networks and instead just\nunderstand it as a slightly advanced version of convolutional or residual layers.\n2.10\nFully Connected Layers\nA fully connected layer is just as what the name implies. Every neuron of one\nlayer is connected to all neurons of the next layer. Such a structure is often\nfound right before the output. For example in Figure 2.10, if the output layer\nis used for some classiﬁcation task, and each node represents some possible\ncategory, we want to make sure that we use all the information that the network\nhas computed so far to produce the ﬁnal results. Therefore — as depicted — the\nrightmost hidden-layer connects every node with every node from the output\nlayer.\n\n2.11. BATCH NORMALIZATION AND RECTIFIED LINEAR UNITS\n63\n2.11\nBatch normalization and Rectiﬁed Linear Units\nLet’s start this section by quoting a quite recent (2018) research paper by San-\nturkar et al. [STIM18].\nBatch Normalization (BatchNorm) is a widely adopted technique\nthat enables faster and more stable training of deep neural networks\n(DNNs).\nDespite its pervasiveness, the exact reasons for Batch-\nNorm’s eﬀectiveness are still poorly understood.\nIndeed, whereas in the previous chapters there were small examples that illus-\ntrate the underlying principles and even allow to calculate things by hand, for\nbatch normalization it is hard to give a convincing example. Instead we’ll only\nshow the general motivating idea, and skip the mathematical details.\nThis diﬃculty of illustrating actually holds true for most of the remaining net-\nwork layers presented in this chapter.\nThese layers, often introduced very\nrecently — Batch normalization was proposed for example in 2015 by Ioﬀe\nand Szegedy [IS15] — represent the engineering advances and reﬁnements that\nmade the general concept work out in practice. Fortunately there are only few\nlayers left to cover.\nWe’ll step back a moment and discuss ﬁrst input normalization. Let’s go back\nto our example on predicting credit worthiness, i.e. taken a bank’s perspective,\nhow much money should we at most lend to someone based on some input\nparameters. Suppose now, the input parameters are age and monthly income.\nSuppose we have only three training examples, namely Jack, Jill and Hipp. We\ninvestigated them thoroughly and know exactly what maximum credit score in\nthe range of 0...1 they should be given. Their data are depicted in Table 2.2. Are\nTable 2.2: Credit Prediction, Training Data\nName\nAge\nIncome (Month)\nMax. Credit\nJack\n26\n2,500\n0.4\nJill\n37\n3,000\n0.5\nHipp\n65\n26,000\n0.9\n\n64\n2. A CRASH COURSE INTO NEURAL NETWORKS\nthese data realistic? Probably not, but let’s still stick with it. Suppose we have\nthe same network structure as in our AND example. We initialize the network\nwith random weights 푤0 = 1, 푤1 = 2 and 푤2 = 3. If we input the training data\nof Jack, the network outputs\nsigmoid(1 ∗1 + 26 ∗2 + 2500 ∗3) = sigmoid(7553) ≈1\nWe immediately notice that the output value is completely dominated by the\nsalary.\nIt’s as if we almost disregard the age!\nLet’s continue with just the\ntraining data of Jack and look how the network learns from this example. We\nhave 휕퐸total\n휕out1 = 1−0.4 = 0.6, and 휕out1\n휕net1 = 0.6∗(1−0.6) = 0.24. We could continue to\ncalculate the update of the weights, but let skip this part. Instead, how would\nthe next learning step from this example look like after updating the weights\nslightly? Well, unless the weight 푤2 changes dramatically, we will have again\nthe sigmoid over some very large number, resulting in almost 1 for the output\nof the net. Nothing will change for quite some iterations. We summarize\n• the network output will be dominated by the salary\n• the network update will be dominated by the salary\n• the network will learn very slowly\nWe can mitigate this eﬀect by normalizing the inputs of the network to a stan-\ndardized scale.\nConsider for example the age in the example. We ﬁrst compute the mean age as\n26+37+65\n3\n= 128\n3 ≈42.66. The variance is given by\n1\n3[(26 −42.66)2 + (37 −42.66)2 + (65 −42.66)2] = 269.55 ≈16.412\nNext we normalize the ages by subtracting the mean and dividing through the\nstandard deviation, which is deﬁned as the square-root of the variance. We get\nfor Jack 26−42.66\n16.41\n≈−1.015; for Jill we have 37−42.66\n16.41\n≈−0.34, and ﬁnally for Hipp\nwe get 65−42.66\n16.41\n≈1.36.\nWhen we have a very deep network, there will be several layer with activation\nfunctions before the network gives an output. The above described eﬀect may\n\n2.11. BATCH NORMALIZATION AND RECTIFIED LINEAR UNITS\n65\nnot only occurs at the input level, but also at all the immediate values. That is,\nif the input to some immediate layer has a very uneven range, this might aﬀect\nthe performance of the network.\nThe underlying idea of batch normalization is to create a network layer that\nnormalizes intermediate data.\nSuch batch normalization layers can then be\ninserted between other layers of the network.\nUsually, batch normalization is applied after computing the linear combination\nof the outputs but before applying the activation function of a layer. Suppose z\nis such an immediate layer, and that there are 푚examples in the current batch\nthat we want to learn. Then we compute the mean 휇= 1\n푚\nÍ푚\n푖=1 푧푖and standard\ndeviation 휎2 = 1\n푚\nÍ푚\n푖=1(푧푖−휇)2. Given input 푧푖we then normalize it by 푧′\n푖= 푧푖−휇\n휎.\nThis is all similar as for input normalization.\nHowever after applying this normalization step, the mean is exactly zero, and\nthe variance is exactly one. That might not be true for this speciﬁc layer, e.g.\nit could be that at this particular layer the values that come in have a diﬀerent\ndistribution. What we want to make sure by batch normalization is that we also\nlearn how these values are distributed. This is especially true since we learn\nin batches, and looking at all training data, the values might be very diﬀerent\nthan looking at the current batch. Therefore one usually adds parameters 훼\nand 훽, and computes 푧′′\n푖= 훼푧′\n푖+ 훽, which is then the ﬁnal output after batch\nnormalization that is fed to the activation function.\nAgain, we will skip the mathematical details on how to integrate this with\nback-propagation. After all, there is no need to implement this manually. In\nDeep-Learning Frameworks, inserting a batch normalization layer is just one\nline of code during network creation.\nThere is yet another way to look at this example. The slope of the sigmoid\nfunction is such that for very large values, the slope (i.e.\nthe derivative or\ngradient) is almost zero. If we have a very deep network, we have a very long\nchain of derivatives due to the chain rule. If each step in the chain creates\na derivative that is a small value almost near zero, and we multiply all these\nsmall values, the resulting value will be so small, that it is very diﬃcult to\n\n66\n2. A CRASH COURSE INTO NEURAL NETWORKS\nnumerically distinguish it from zero. The gradient vanishes, and this is known\nas the vanishing gradient problem. How can we mitigate that?\n1. If we look at the update rule 푤푖= 휈∗.., we might consider increasing the\nlearning rate 휈by a lot and hope that a large 휈multiplied by a value that\nis almost zero gives a sensible output. However for a high learning rate\nwe’ve already seen that could lead to overstepping a minima. Thus, this\nis not really a good option.\n2. Apply input normalization and batch normalization. Note that it’s now clear\nwhy we apply batch normalization before the activation function: To make\nsure we feed the activation function values where there is a nonzero slope.\n3. Choose a better activation function!\nIndeed, the third option is important.\nThe sigmoid function is very suited\nfor the output of the network, for example if we want to have something akin\nto a probabilistic output as in “these photos might show the same person with\nconﬁdence 0.8”. For the intermediate layers however it is not so suitable, because\nit’s range is very limited: For large values, we essentially have 0 as the value of\nthe slope.\nA more suitable activation function for deep networks is adding a rectiﬁed\nlinear activation function or ReLU for short. It is deﬁned as\n훼(푥) =\n(\n푥,\nif 푥> 0\n0,\notherwise\nThis helps a lot with the vanishing gradient problem, since for the derivative of\nthe linear part of the function (for 푥> 0) we always have sensible values for the\nderivative, and when multiplying all these — larger than zero — values via the\nchain rule we help to circumvent the vanishing gradient problem. Note that a\npure linear function however is not enough to learn complex functions, and the\nzeroization part fulﬁlls this requirement of having non-linearity.\n\n2.12. RESIDUAL LAYERS\n67\n2.12\nResidual Layers\nThere is one last diﬃcult layer that we have to discuss. I think the motivating\nquote for this layer is that\nThe diﬀerence between theory and practice is that in theory it works,\nbut in practice it doesn’t.\nIf we make a network deeper, it should learn better. After all, we have more lay-\ners, and therefore can construct a better approximation of the original function.\nHowever what researchers observed in practice with deep networks is that\ninitially the error goes down and down, then reaches a kind of minimum,\nand with lots and lots of examples often gets worse. This is counterintuitive,\nas according to theory, the more examples we use for training, the better the\nnetwork should perform.\nOne reason is the above mentioned problem with vanishing gradients (the\nopposite eﬀect can occur, too, then dubbed exploding gradients). But there\nis another problem.\nSuppose the network was already trained with lots of\nexamples. There is a small error left, but it is very small. Next we encounter a\ntraining example which does not really contain new information. The weights\nin the network actually don’t really need an update, they should more or less be\nthe same – the output of the network for these examples is already pretty much\ncorrect. What we are learning at that point for the majority of examples is the\nidentity function 푓(푥) = 푥. But it turns out that the identity function is actually\ndiﬃcult to learn for deep networks, as information is not passed straight down\nfrom one layer to the following layers.\nLet’s say you are an ambitious chess amateur, but at one point don’t want to make\nprogress any more. Well the best way to do this is probably just to skip any\nlesson! And that’s exactly the intuition between residual layers (resp. residual\nnetworks): Once in a while, just skip some layers and directly pass information\ndown to deeper layers.\nConsider the network illustrated in Figure 2.19.\nAt the very left, we start\nwith some inputs x0 = 푥1, . . . , 푥푛. Then we multiply by some weights, let’s\n\n\n2.13. OVERFITTING AND UNDERFITTING\n69\nhave\n휕퐸total\n휕x0\n= 휕퐸total\n휕훼\n휕훼\n휕x0\n= 휕퐸total\n휕훼\n\u0012 휕퐹(푥)\n휕푥\n+ 1\n\u0013\n= 휕퐸total\n휕훼\n+ 휕퐸total\n휕훼\n휕퐹(푥)\n휕푥\nIn other words: When we back-propagate the error to the layer before 푥0, we\nconsider both the error that is propagated regularly through the network — the\nchain 휕퐸total\n휕훼\n휕퐹(푥)\n휕푥\nis exactly what we would get if we completely forget about the\nskip connection — but also add the error at the output layer 휕퐸total\n휕훼.\nIn this example we skipped two weight layers, and the second weight layer was\nactually the output layer of the network. Needless to say that we can generalize\nthis structure to skip an arbitrary number of layers. Also, the end-point of the\nskip connection does not need to be the output layer of the network. We can\nalso of course add multiple skip connections. The important point to remember\nis that residual networks [HZRS16] use skip connections.\n2.13\nOverﬁtting and Underﬁtting\nThere is a joke among German university students that goes along the lines\nthat a professor asks an engineering student to rote memorize a phone book.4\nThe engineering students answers \"Why?\" Whereas when the professors asks a\nmedical student the same questions the answer will be \"Until when?\".\nRote memorization is just that — you will learn the facts that you study but not\nmore. Much focus is put by most schools of medicine into rote memorization\nsuch that later when doctors are faced with quick decision making they are able\nto immediately recall their important knowledge.\nBut rote memorization is simply not enough. We need to extract patterns and\ncommon themes, we need to generalize in order to be able to face new situations.\nFor neural networks we have the two terms underﬁtting and overﬁtting.\nUnderﬁtting means we have been just lazy and did not learn enough. If we\nquery our network it will spit out answers but these answers are not better than\n4Yup it’s an old joke and will likely no longer work in near future.\n\n70\n2. A CRASH COURSE INTO NEURAL NETWORKS\njust guessing. We need to train the network enough in order to be able to learn\n— and this itself means we need enough training material and training time.\nOverﬁtting means we have learned the training data but did not generalize\nenough. Our networks performs very well on the training data but when faced\nwith slightly diﬀerent data the network performs poorly. There are multiple\npotential reasons for that and it is not always easy to spot the reasons. Typical\nsolutions are to add more training data such that the network also sees such kind\nof slightly diﬀerent data or if there is just no more training data to artiﬁcially\ncreate some by slightly distorting the existing training data to make the network\nmore robust. But it could also be that our network is just too complex and\nover-engineered and reducing the complexity of it might force the network to\ngeneralize more.\nWe need not to delve into the reasons and how to overcome these remedies\nin all detail but just have to remember that these phenomena can occur and\nthat sometimes decisions w.r.t. network architecture or training are rooted in\navoiding both overﬁtting and underﬁtting.\n2.14\nSummary\nWe have seen that there is lot of parameters to choose from when designing a\nneural network. We have learning rates and batch sizes, activation functions,\ndiﬀerent layers and the question to put them where and in which order, and\nalso the question on how deep the network should be.\nIt is diﬃcult to choose the parameters in advance, and in fact subject to current\nresearch. Designing networks in practice therefore involves lots of practical\nexperimentation and ﬁne tuning.\nWe will discuss in detail the network structure of modern chess AI systems\nsuch as Google’s AlphaZero in Chapter 4. Here, we have covered all required\nnetwork elements to understand such network structures.\nWhat’s missing then?\nLet’s ﬁrst create a hypothetical neural network that plays chess. First, we need\n\n2.14. SUMMARY\n71\n8 rmblkans\n7 opopopop\n6 0Z0Z0Z0Z\n5 Z0Z0Z0Z0\n4 0Z0Z0Z0Z\n3 Z0Z0Z0O0\n2 POPOPOBO\n1 SNAQJ0MR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 2.20: White bishop on g2.\nto deﬁne the input of the network. That should be a chess position that encodes\nall information that deﬁne the state of the game. A naive way would be to\nsimply take the binary representation of string with Forsyth-Edwards-Notation\n(FEN)5. Such a representation is far from being optimal: We would expect that\npositions with a similar structure would result in similar inputs to the network.\nBut consider the positions depicted in Figure 2.20 and Figure 2.21 which have\nthe FEN strings\nFEN: rnbqkbnr/pppppppp/8/8/8/6P1/PPPPPPBP/RNBQK1NR w KQkq - 0 1\nand\nFEN: rnbqkbnr/pppppppp/8/8/8/5BP1/PPPPPP1P/RNBQK1NR b KQkq - 0 1\nWe have diﬀerent FEN strings, which results in quite diﬀerent bit strings. There\nare better encodings of course which we will encounter in Chapter 4.\n5A FEN string encodes the position of all pieces, as well as information about and dates back\nto the 19th century.\n\n72\n2. A CRASH COURSE INTO NEURAL NETWORKS\n8 rmblkans\n7 opopopop\n6 0Z0Z0Z0Z\n5 Z0Z0Z0Z0\n4 0Z0Z0Z0Z\n3 Z0Z0ZBO0\n2 POPOPO0O\n1 SNAQJ0MR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 2.21: White bishop on f3.\nWe then build a fancy deep neural network and throw in all the tools we have\nlearned so far. For the output we can chose to consider either regression or\nclassiﬁcation. Both more or less yield the same result:\n1. In a regresssion model, we would simply output the expected likeliness\nthat White wins (+1.0), Black wins (−1.0) or that the game is a draw (0.0).\nThe output layer could for example use hyperbolic tangent as an activation\nfunction.\n2. For a classiﬁcation model, we would enumerate all ﬁnitely many possible\nmoves on a chessboard. Suppose these are 푛moves. We then create an\noutput layer with 푛nodes. We could add a softmax layer after that. For\nexample if there are three possible moves, an output of (0.3, 0.6, 0.1) would\nindicate that the ﬁrst move has a change of 30 percent to be played, the\nsecond move has a change of 60 percent to be played, and the last move\nhas a 10 percent change of being played.\nNote that both models can be used to play chess. Whereas the ﬁrst network\njust outputs a position evaluation, we can use this in the following way to play:\nConsider all moves, apply each move, and write down the output value of the\n\n2.14. SUMMARY\n73\nregression model for that position. Next compare all output values, and then\nchose the move that leads to the next position with the highest evaluation.\nWhat’s missing is of course training data. Ideally we’d have a Grandmaster\nsitting next to us while training that given a chess position immediately answers\nwith the best move. If such a GM is not available, the next best thing is probably\nto use a large chess database with strong Grandmaster games, go through all\npositions and then take the move played in a position as the expected output.\nUnfortunately, this alone doesn’t seem to work, even with all the advanced\ntechniques discussed. Or at least noone so far was able to design and train\nsuch a network in a way that matches the best traditional chess programs. We\nneed some more ideas on how to make a computer play chess which involve\nsearching techniques. These are subject of the next chapter.\nThe ﬁrst closest thing to a successful neural network that plays chess was Giraﬀe,\nan experimental implementation created by Matthew Lai as a master thesis\nproject [Mat15].\nNamely the ﬁrst part of Lai’s master thesis was just the above mentioned network\nwith the regression model. Lai used a clever way of encoding chess positions\ntogether with a (not very deep) neural network with two hidden layers, each\nusing ReLU’s as the activation function. The tanh function was used to generate\noutputs between −1 (Black wins) and 1 (White wins). After quite some training,\nGiraﬀe assessed a set of test positions with an approximate ELO rating of 2400.\nThis doesn’t seem a lot since traditional chess programs at that time were reach-\ning ELO ratings of 3000. But this is quite an achievement since it was the ﬁrst\ntime that it was shown that a neural network can be trained such that it achieves\nIM or low GM chess expertise when assessing a chess position.\nNote that it is really just that: After training we have a static network structure\nplus pre-computed weights that, given a position and one forward computa-\ntion through the net immediately answers with a reasonable assessment of the\nposition. It is even more incredible once we compare it to traditional chess\nprograms, which use tree-search techniques.\nThe ﬁrst use of neural networks for chess that actually resulted in a strong chess\n\n74\n2. A CRASH COURSE INTO NEURAL NETWORKS\nengine was by David et al. [DNW16]. There, they combined a strong neural\nnetwork together with alpha-beta search.\nThe neural network was used to\nevaluate chess position, and the alpha-beta search provided the necessary tac-\ntical strength. Their program, dubbed DeepChess achieved signiﬁcant strength.\nWhile it was not among the strongest chess engines at that time, it was able to\nbeat Robert Hyatt’s well-known open source program Crafty [HN97].\n\n3\nSearching\nYou, Venerable One, may indeed\nbe a seeker, for, striving toward\nyour goal, there is much you do\nnot see which is right before your\neyes.\nMathias Feista\naoperating Deep Fritz and address-\ning Vladimir Kramnik after 34...Qe3 in\nGame 2 of the 2006 Man vs Machine\nmatch in Bonn, Germany\nIn the seminal work “Think Like a Grandmaster” [Kot71] Alexander Kotov\nteaches an easy and straight-forward method to think like a grandmaster. His\nmethod to ﬁnd the best move in a position is based on these simple steps:\n• consider all interesting moves in a given position\n• then for each of these moves, calculate all the resulting variations. Once\nyou calculated a line however, never come back to that line, just take your\ninitial analysis for sure.\n75\n\n76\n3. SEARCHING\n• calculate all lines for the opponent\n• using this “analysis tree” of variations, ﬁnally assess the position and\nchoose the most promising move\nThe problem with this great approach is that I do not know any amateur or\ngrandmaster who thinks like that. But maybe the underlying reason is that I do\nnot know any grandmaster in the ﬁrst place. However, it sounds like a strategy\nthat a computer could execute.\nFirst note that a few things are slightly vague here. In particular how deep shall\nwe calculate? Unless we are in an endgame where some lines actually end up\nin a terminal position, i.e. a mate, a stalemate or a draw, we will have to stop\nsomewhere and assess the current position: Is it equal? Or does one side have\nan advantage?\nWe have seen in the last chapter how to create an extremely powerful and\nsophisticated evaluation function using a deep neural network with various\nlayers. Such an evaluation function however was not available in the early days\nof computer chess. And as of now, there is no way to combine a deep neural\nnetwork based evaluation function with minimax or alpha-beta search — but\nmore on that later.\nSo let’s start with something that probably every kid is taught when starting\nchess: Counting pawns. And slightly tweak that by considering piece place-\nments, i.e. making sure that pieces that are placed in the center are considered\nto be of more value than pieces that are placed on the rim.\nWe’ll use the numbers in Table 3.1 as a base. By giving a bishop ten points\nmore than a knight, the evaluation function will favor the bishop pair. Also\ntwo rooks are rated higher than one queen. All this is debatable of course, but\nthen again: This is just a very rough evaluation. We can slightly tweak this by\nadding resp. subtracting points depending on where a piece is placed. This is\ndepicted in Table 3.2. For example, a knight placed on e4 would not be rated\nwith 310 points, but rather with 310 + 20 = 330 points. Again, this is debatable,\nespecially when it comes to complex middlegames or endgames, but it suﬃces\nas a rough heuristic.\n\n77\nTable 3.1: Evaluation by Pawn Counting\nPawn\nKnight\nBishop\nRook\nQueen\n100\n310\n320\n500\n900\nTable 3.2: Evaluation by Piece Placement\nA\nB\nC\nD\nE\nF\nG\nH\n8\n-50\n-40\n-30\n-30\n-30\n-30\n-40\n-50\n7\n-40\n-20\n0\n0\n0\n0\n-20\n-40\n6\n-30\n0\n10\n15\n15\n10\n0\n-30\n5\n-30\n5\n15\n20\n20\n15\n5\n-30\n4\n-30\n0\n15\n20\n20\n15\n0\n-30\n4\n-30\n5\n10\n15\n15\n10\n5\n-30\n2\n-40\n-20\n0\n5\n5\n0\n-20\n-40\n1\n-50\n-40\n-30\n-30\n-30\n-30\n-40\n-50\nThere are better evaluation functions of course. For example you could try to\ndetect whether the game is at the beginning, in the middle game or whether\nthere is an endgame position. Depending on that it might be better to either\nmove the king outside of the center into safety or towards the center. In middle\ngame positions you could evaluate king safety, i.e. has the king castled? Are\npawns in front of the king? Did these pawns move? Are there pawns on the\nseventh resp. second rank?\nIdeally of course we would use a deep neural network for evaluation, i.e. a\nnetwork as introduced in the previous chapter.\nTo understand classical minimax and alpha-beta search however, from now on\nwe will just a assume that we have an evaluation function that can be computed\nfast, like the trivial one above.\n\n78\n3. SEARCHING\nImplementing the Evaluation Function\nThere is an excellent and easy to use chess library dubbed python-chess for\nPython. With it, it is straight-forward to implement the evaluation function that\nwe described above. First, we deﬁne an array with the piece square values in\nListing 3.1.\nListing 3.1: Piece Square Table\n1 pieceSquareTable = [\n2\n[ -50,-40,-30,-30,-30,-30,-40,-50 ],\n3\n[ -40,-20,\n0,\n0,\n0,\n0,-20,-40 ],\n4\n[ -30,\n0, 10, 15, 15, 10,\n0,-30 ],\n5\n[ -30,\n5, 15, 20, 20, 15,\n5,-30 ],\n6\n[ -30,\n0, 15, 20, 20, 15,\n0,-30 ],\n7\n[ -30,\n5, 10, 15, 15, 10,\n5,-30 ],\n8\n[ -40,-20,\n0,\n5,\n5,\n0,-20,-40 ],\n9\n[ -50,-40,-30,-30,-30,-30,-40,-50 ] ]\nThe central data structure of python-chess is a board that encodes a chess\nposition. Given a board, we simply iterate through all possible squares and\ndepending on the piece that is placed on the square, we sum the value of the\npiece together with the value from the piece-square table to get an overall score.\nWe do this both for White and for Black. Finally we return the diﬀerence of\nthe overall score for White and Black. This is depicted in Listing 3.2. We can\nquickly test the evaluation function. Figure 3.1 is a position where it is White to\nmove and he can checkmate.\nThe evaluation function will of course not consider the checkmate and just\ncalculate the evaluation based on counting pieces and the placement of the\npieces.\nListing 3.2: Computing the Evaluation Score\n1 def eval(board):\n2\nscoreWhite = 0\n3\nscoreBlack = 0\n4\nfor i in range (0,8):\n5\nfor j in range (0,8):\n\n79\n6\nsquareIJ = chess.square(i,j)\n7\npieceIJ = board.piece_at(squareIJ)\n8\nif str(pieceIJ) == \"P\":\n9\nscoreWhite += (100 + pieceSquareTable[i][j])\n10\nif str(pieceIJ) == \"N\":\n11\nscoreWhite += (310 + pieceSquareTable[i][j])\n12\nif str(pieceIJ) == \"B\":\n13\nscoreWhite += (320 + pieceSquareTable[i][j])\n14\nif str(pieceIJ) == \"R\":\n15\nscoreWhite += (500 + pieceSquareTable[i][j])\n16\nif str(pieceIJ) == \"Q\":\n17\nscoreWhite += (900 + pieceSquareTable[i][j])\n18\nif str(pieceIJ) == \"p\":\n19\nscoreBlack += (100 + pieceSquareTable[i][j])\n20\nif str(pieceIJ) == \"n\":\n21\nscoreBlack += (310 + pieceSquareTable[i][j])\n22\nif str(pieceIJ) == \"b\":\n23\nscoreBlack += (320 + pieceSquareTable[i][j])\n24\nif str(pieceIJ) == \"r\":\n25\nscoreBlack += (500 + pieceSquareTable[i][j])\n26\nif str(pieceIJ) == \"q\":\n27\nscoreBlack += (900 + pieceSquareTable[i][j])\n28\nreturn scoreWhite - scoreBlack\nWe then create a board with the corresponding FEN string and call the evalua-\ntion function (cf. Listing 3.3).\nListing 3.3: Calling the Evaluation Function\n1 board = chess.Board(\" r1bqkb1r/pppp1ppp /2n2n2/4p2Q/2B1P3 /8/ PPPP1PPP/\nRNB1K1NR w KQkq - 4 4\")\n2 print(\" current evaluation \")\n3 print(eval(board))\nWhite is considered to be slightly worse with a value of −55 since Black has\nplaced more pieces in the center. As we can see, the evaluation function give a\nvery rough assessment of the position but naturally fails to consider any kind\nof tactical threats. That’s why we need search algorithms.\n\n80\n3. SEARCHING\n8 rZblka0s\n7 opopZpop\n6 0ZnZ0m0Z\n5 Z0Z0o0ZQ\n4 0ZBZPZ0Z\n3 Z0Z0Z0Z0\n2 POPO0OPO\n1 SNA0J0MR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 3.1: Mate in one.\n3.1\nMinimax\nLet’s reconsider Kotov’s suggestion on how to calculate or how chess players\nshould calculate in general, but deﬁne things more formally and without any\ndisambiguity — so that a computer can understand and execute the approach\nalgorithmically. But before let’s take a look at a concrete example so that we can\nunderstand the general principle. For that we consider an endgame position\nwith three opposing pawns, depicted in Figure 3.2. To make things easier we\nwill only consider the upper right corner of the position and forget about the\nking position. We will also slightly change the winning conditions: White wins\nif he can place a pawn on the third rank. Black will win if he can block or\ncapture all White’s pawns. Note that there are no draws possible. We will also\nassume we have an evaluation function that can assess any position, and that\nwill output a 10 if White wins, and a 0 if Black wins. There is no explanation\nhow this evaluation function works; we will just assume it is there. All this is\nsimply to get an example with fewer possible moves and variations such that it\nis still possible to manually calculate all options.\nBefore we start, some more deﬁnitions.\nWhen illustrating move sequences\n\n3.1. MINIMAX\n81\n8 0Z0Z0Z0Z\n7 Z0Z0Zpo0\n6 0Z0Z0ZpZ\n5 Z0Z0ZPZP\n4 0Z0Z0Z0Z\n3 J0Z0Z0Z0\n2 0Z0Z0Z0Z\n1 j0Z0Z0Z0\na\nb\nc\nd\ne\nf\ng\nh\nFigure 3.2: Endgames with opposing pawns\nand resulting positions, we do so by using a tree. An example is depicted in\nFigure 3.3. On top, there is the root node. From the root node there are diﬀerent\nbranches we can follow which lead to other nodes, the child nodes of the root\nnode. At the bottom there are nodes which have no successors. These are\ndubbed leaf nodes. It has some resemblance to a real tree if you consider this\nupside down.\nWe will start with position (1) in Figure 3.3. It is White to move. There are four\npossible moves:\n• White can move the left pawn forward.\n• White can capture the center black pawn with his left pawn.\n• White can capture the center black pawn with his right pawn.\n• White can move the right pawn forward.\nWe now need to consider each of these. We start by executing the ﬁrst option and\nmove the left white pawn forward. Now it is Black’s turn. She has three options:\nCapture the white pawn on the right resulting in position (4), move forward\n\n\n3.1. MINIMAX\n83\nthe middle pawn resulting in position (5), or capture the left pawn resulting in\nposition (6). We could go on and consider all possible replies by White for each\nof these moves, but let’s assume we stop here and call our evaluation function.\nSince in positions (4), (5) and (6) White will always have a free pawn and thus\nbe able to reach the third rank, the evaluation will return an evaluation of 10\nfor each of these positions. As mentioned, we’ll leave open how the evaluation\nfunction gets to this conclusion, we’ll just assume it is there.\nWe can do the same for other possible moves. For example if White in the initial\nposition captures the center pawn with his right pawn and we consider all\npossible replies from Black, we end up in positions (7) and (8). These evaluate\nto 0, since White cannot break through and get a passer anymore.\nNow how can we use this information of the evaluation of positions of leaf\nnodes? Suppose we want to calculate the evaluation of node (2). It is Black to\nmove here. We will not consider hope chess, we will always assume that Black\nplays the best possible move (w.r.t. the evaluation of the child nodes). In other\nwords, Black tries to minimize the evaluation outcome. He will choose the move\nthat leads to the child node with the smallest evaluation. Here he can chose\nbetween three nodes, however all result in an evaluation of 10. The minimum of\n10, 10 and 10 is however 10. Similar for evaluation of node (3), all moves lead\nto a position which is evaluated with 0. Here, the minimum is 0.\nSo far this is all not very interesting. So let’s have a look on how these evaluations\nof node (2) and (3) help us to get a better evaluation of the root node. In the\nroot node, it is White to play. White will of course chose the move that leads\nto the highest possible evaluation, i.e. he wants to maximize his outcome. For\nnode (2) he now knows that if Black plays perfectly, the position is evaluated\nwith 10. For node (3), the position was evaluated with 0. We haven’t calculated\nit, but assume that for all other moves from the root node, we will also end up\nwith evaluations of 0. White chooses the maximum of all possible options, i.e.\nhe will play the left pawn forward leading to a node with an evaluation value\nof 10. As this is the maximum, we consider the root node also to be evaluated\nwith a value of 10, i.e. winning for White.\nWe formulate this approach as an algorithm:\n\n84\n3. SEARCHING\n• First we create a search tree. Given a position, we consider all moves for\nthe side whose turn it is, say White. For each move we consider then\nall replies by Black. Again, for each of these moves by Black, there are a\nnumber of moves by White and so on.\n• At some point, we have to stop. Either because there are no more legal\nmoves (i.e. the game is ﬁnished due to checkmate or a draw), or just\nbecause we ran out of time and space since the tree grows very quickly. A\nsimple way to limit the size of the search tree is to always stop at a ﬁxed\ndistance from the root, i.e. at a ﬁxed depth of the tree.\n• For each leaf node of the generated tree, we have to evaluate the position by\nour evaluation function. This could be the pawn counting function that\nwe introduced earlier. We augment this evaluation function for positions\nwhere White won by checkmate with evaluating the position as ∞, with\ndrawn positions as 0, and with positions where Black won by checkmate\nwith −∞.\n• Now we propagate the results back to the root node step by step. Given a\nnode where all child node have evaluations we\n– take the maximum value of all child nodes if it is White’s turn\n– take the minimal value of all child nodes if it is Black’s turn\nThis procedure is known as minimax [vN28]. Note that minimax is quite inde-\npendent of chess. In particular it can be applied to all two-player games similar\nto chess, as long as there is an evaluation function that, given a position of the\ngame, can give a numerical assessment of the position.\nUsing python-chess, minimax has a straight-forward implementation, shown\nin Listing 3.4.\nListing 3.4: The minimax algorithm\n1 def minimax(board , depth , maximize):\n2\nif(board.is_checkmate ()):\n3\nif(board.turn == chess.WHITE):\n4\nreturn\n-10000\n\n3.1. MINIMAX\n85\n5\nelse:\n6\nreturn 10000\n7\nif(board.is_stalemate () or board.is_insufficient_material ()):\n8\nreturn 0\n9\nif(maximize):\n10\nbest_value = -99999\n11\nfor move in board.legal_moves:\n12\nboard.push(move)\n13\nbest_value = max(best_value ,\n14\nminimax(board , depth -1, not maximize))\n15\nboard.pop()\n16\nreturn best_value\n17\nif(minimize):\n18\nbest_value = 99999\n19\nfor move in board.legal_moves:\n20\nboard.push(move)\n21\nbest_value = min(best_value ,\n22\nminimax(board , depth -1, not maximize))\n23\nboard.pop()\n24\nreturn best_value\nThe function bestmove takes three parameters: The board that should be eval-\nuated, the current depth (to limit the search depth) and a boolean parameter\nthat indicates whether we should maximize or minimize the evaluation of the\ncurrent board.\nIf we have a checkmate, instead of returning inﬁnity we return a very large\npositive (resp.\nnegative) integer depending on whether White checkmated\nBlack or vice versa. If the game is drawn, we return 0. In all other cases we\ngenerate all legal moves of the current position. For each move we apply this\nmove to reach the next position. Then it remains to distinguish if it is White’s\nturn and we want to take the maximum result w.r.t. that board position, or if it is\nBlack’s turn and we want take the minimum for that position. After considering\nall legal moves, we have eﬀectively computed an evaluation of the current board\nand return this evaluation that was stored in the variable best_value.\nIn fact, we have already implemented a chess engine — albeit a very simple one.\nGiven a position, we can use minimax to get the next best move; cf. Listing 3.5.\n\n86\n3. SEARCHING\nListing 3.5: Compute Best Move in Position\n1 def getNextMove(depth , board , maximize):\n2\nlegals = board.legal_moves\n3\nbestMove = None\n4\nbestValue = -99999\n5\nif(not maximize):\n6\nbestValue = 99999\n7\nfor move in legals:\n8\nboard.push(move)\n9\nvalue = minimax(board , depth - 1, (not maximize))\n10\nboard.pop()\n11\nif maximize:\n12\nif value > bestValue:\n13\nbestValue = value\n14\nbestMove = move\n15\nelse:\n16\nif value < bestValue:\n17\nbestValue = value\n18\nbestMove = move\n19\nreturn (bestMove , bestValue)\nFirst, we generate all possible legal moves in the current position. If it is White’s\nturn, we are looking for the move with the highest score, and initialize the\nevaluation score for the best move in the variable bestValue with a very low\nnumber, so that any move returned by minimax will be better than this default\nvalue. If it is Black’s turn we seek to minimize the score, and thus initialize\nbestValue with a very high number. Next we iterate through all legal moves,\nand compare the score to the best one we’ve found so far. If we have found a\nbetter move, we remember it in the variable bestMove. After iterating through\nall moves, we ﬁnally return the best move found so far.\n3.2\nAlpha-Beta Search\nWith the minimax algorithm we have to visit a lot of nodes in the tree, i.e. search\nand evaluate a lot of positions. How many? Let’s start a small experiment.\nConsider again the position in Figure 3.1.\n\n3.2. ALPHA-BETA SEARCH\n87\nWe can use this position for two purposes. First, let’s see if our minimax im-\nplementation actually works and ﬁnds the (obvious) checkmate Qxf7. Second,\nlet’s see how many nodes have to be visited to ﬁnd the checkmate. Using our\nimplementation of getNextMove this is straight-forward. First let’s ﬁx a depth 푑\nof halfmoves to search the tree. Then we have to create the board object with\nthe position and call getNextMove. In Python this consists of just the two lines\nshown in Listing 3.6\nListing 3.6: Alpha-Beta Position Evaluation\n1 board = chess.Board(\" r1bqkb1r/pppp1ppp /2n2n2/4p2Q/2B1P3 /8/ PPPP1PPP/\nRNB1K1NR w KQkq - 4 4\")\n2 print(getNextMove (4, board , True))\nFor Figure 3.1 and depth 3 this actually takes a few minutes on my machine.\nAgain this is Python, and far away from an eﬃcient chess engine implementa-\ntion. The checkmating move Qxf7 is found eventually. Adding a counter gives\nus the visited nodes — in this example we had to visit 46828 nodes. Can we do\nbetter and reduce the number of nodes that we visit, yet still be sure to ﬁnd the\ncheckmate?\nLet’s have another look at the example that was introduced with minimax,\ndepicted in Figure 3.4. We start at the root, then visit node (2), and further\nexpand to nodes (6), (7) and (8). Our expansion stops here, we call our evaluation\nfunction, and propagate the result back to node (2). Here we take the minimum\nof the three child nodes, which results in an evaluation of 10 for node (2). We\nthen go back to the root node, and start expanding the next possible move,\nresulting in a visit to node (3). There are two possible moves available at node\n(3). We start with capturing back with the pawn, visit node (9) where we reach\nour search depth limit and evaluate the position to get an evaluation value of 0.\nAt this moment let’s pause a little bit before doing any further work, and consider\nthe information that is currently available to us. In node (3) it is Black to play.\nThe left child gave him a 0. If the other child is larger than 0, Black will choose\nthe best (minimal) node below, and that is precisely the child with a 0. If the\nother child is even smaller than 0, Black will chose that one of course. In other\n\n\n3.2. ALPHA-BETA SEARCH\n89\nwords, Black has at least a node with a value of 0 available, possibly even lower.\nNow let’s go up further in the tree, back to the root node (1). Here, it is White\nto play. White tries to maximize the outcome of the game. We know that Black\ncan at least get a 0 if White chooses the child node (3). But from our previous\nevaluation we already know that the node (2) will get White a 10, so he will\nalways prefer node (2) over node (3). In other words, the choice of White does\nnot depend on the evaluation of node (10) any more. We can stop any further\ninvestigation below node (3), and instead directly hop over to node (4). After\nevaluating node (11), we also know that Black can get value 0 here. In the same\nmanner we can stop any further investigation and hop over to node (5). The\nsame spiel continues.\nIn the end, we saved visiting a lot of nodes. In fact, we only ever fully visited\nthe leftmost branch of the tree, i.e. all the nodes below node (2) until we hit our\nsearch depth limit. For all other nodes, we skipped the search and evaluation\nof several parts of the tree. We did so by always recording the current best\nevaluation w.r.t. the current subtree for White (Black), and we call this the alpha\n(beta) value. If we can cut oﬀa branch and skip visiting its nodes, we say that\nwe did an alpha (beta) cutoﬀ. Hence the name of the algorithm: alpha-beta search.\nThe algorithm is depicted in Listing 3.7. Here we reuse the evaluation function\nfrom the minimax implementation.\nThe computation of the best move in a\ncurrent position is also the same as before. We only replace the search algorithm\nitself.\nListing 3.7: Alpha Beta Search\n1 def alphaBeta(board , depth , alpha , beta , maximize):\n2\nif(board.is_checkmate ()):\n3\nif(board.turn == chess.WHITE):\n4\nreturn\n-10000\n5\nelse:\n6\nreturn 10000\n7\nif depth == 0:\n8\nreturn eval(board)\n9\nlegals = board.legal_moves\n10\nif(maximize):\n11\nbestVal = -99999\n\n90\n3. SEARCHING\n12\nfor move in legals:\n13\nboard.push(move)\n14\nbestVal = max(bestVal , alphaBeta(board , depth -1, alpha ,\nbeta , (not maximize)))\n15\nboard.pop()\n16\nalpha = max(alpha , bestVal)\n17\nif alpha >= beta:\n18\nreturn bestVal\n19\nreturn bestVal\n20\nelse:\n21\nbestVal = 99999\n22\nfor move in legals:\n23\nboard.push(move)\n24\nbestVal = min(bestVal , alphaBeta(board , depth - 1,\nalpha , beta , (not maximize)))\n25\nboard.pop()\n26\nbeta = min(beta , bestVal)\n27\nif beta <= alpha:\n28\nreturn bestVal\n29\nreturn bestVal\nLet’s match the example with the implementation. We initialize the algorithm\nwith a very small value for alpha, and a very large value for beta, indicating that\nwe cannot cut any branch in the beginning without searching. Now let’s focus\non lines 12-19 and node (1). For each move we compute the evaluation of the\nsubtree, and compare it with our current alpha value and take the maximum of\nboth. Running alpha-beta search on node (2) gives us a best value of 10, which\nis our new alpha value. Now we start visiting node (3) with an alpha value of\n10. After visiting node (9) and going back to node (3) we have a beta value of\n0 — from our current information, this is the best that Black can do. At this\npoint the beta value — value 0 — is smaller than the alpha value — value 10\n— (line 17). Instead of ﬁnishing the for-loop that starts in line 12, we instead\nimmediately return our evaluation, thereby cutting oﬀnode (10).\nRunning alpha-beta search with the same depth as minimax in the position\nshown in Figure 3.1 returns in a fraction of the time compared to minimax. The\nnode counts is down from 46828 to 9050. Alpha-beta search allowed us to skip\nlots of nodes. For deeper searches the eﬀect becomes even more important!\n\n3.3. ADVANCED TECHNIQUES\n91\n8 rZblka0s\n7 opopZpop\n6 0ZnZ0m0Z\n5 Z0Z0o0Z0\n4 0ZBZPZ0Z\n3 Z0Z0ZQZ0\n2 POPO0OPO\n1 SNA0J0MR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 3.5: Quiescence Search\n3.3\nAdvanced Techniques\nAlmost all (classical) chess engines are alpha-beta searchers, i.e. they combine\nalpha-beta search with a fast-to-compute and handcrafted evaluation function.\nTo create a competitive chess engine some additional advanced techniques have\nto be employed however.\nOne major issue is how to eﬃciently implement the chess logic. How do you\nrepresent the board, how do you quickly generate all legal moves for a given\nposition? These very technical issues are not discussed here, as they are rather\nindependent of the algorithmic aspects of engine programming.\nFrom a logical perspective, there are also additional techniques required in\norder to create a useful chess engine. The ﬁrst, and probably biggest issue is\nrelated to the depth of a search and captures. Consider the position depicted\nin Figure 3.5. Imagine that we use alpha-beta search with a ﬁxed depth, and\nthe position depicted in the ﬁgure occurs in the search tree just so that there\nis one half-move left before we reach our depth limit. One legal move is Qxf6.\nAfter that we end up in a leaf-node, so the evaluation function will be used\n\n92\n3. SEARCHING\nto evaluate the position. And if we just count pawns then great — White is\nup a knight! This evaluation is reported back into the upper layers of the tree,\nand suddenly this position is deemed very advantageous for White. Of course\nthat’s completely wrong because Black will recapture with the pawn on g7 or\nthe queen on d8 and will then be up a queen against a knight.\nThe solution to this problem is to search for a ﬁxed depth, but if a position is\nnot deemed quiet, we search some more. This is dubbed quiescence search. The\nquestion is of course how to deﬁne when a position is quiet or not. A common\nsimple heuristic for a given position is to consider all re-captures w.r.t. one ﬁeld\nuntil no more re-captures are possible. So in Figure 3.5, after White’s Qxf6 we\nwould continue the search but not w.r.t. all possible moves by Black, but only\nby possible re-captures, i.e. consider the position after 1.Qxf6 Qxf6 and 1.Qxf6\ngxf6. As there are no more re-captures possible, the position is quiet and the\nsearch stops. The evaluation would then be more realistic and the disastrous\neﬀect of the exchange becomes obvious to the engine.\nThe concept of quiescence search is as old as computer chess itself, and already\nmentioned in Shannon’s legendary paper [Sha50] where he writes: A very im-\nportant point about the simple type of evaluation function given above (and general\nprinciples of chess) is that they can only be applied in relatively quiescent positions.\nAnother major issue is the order in how moves are searched. If we have limited\ntime, it makes a huge diﬀerence whether we ﬁrst look at interesting moves, or\njust consider all possible moves in random order. Thinking about the example\nabove, it might not make a lot of sense to look at all capture moves of the\nqueen ﬁrst, as it is unlikely that there is a huge material gain — in most cases,\nwe would simply trade the queen for some lower valued piece and just loose\nmaterial. There are static ways to do it – i.e. heuristically evaluate the position –\nor methods that employ information we gathered during the tree search itself.\nConsider for example the case where White makes a threat. There might be one\nspeciﬁc move that counters this threat, resulting in a beta-cutoﬀin the alpha-\nbeta algorithm, i.e. as soon as Black makes this move we can stop searching\nsince we know that White’s threat was not that good in the ﬁrst place. In our\nalpha-beta cutoﬀexample that is e.g. the Black’s move leading from position (3)\nto (9). We consider this as a killer move. It was very good in this position, and\n\n3.3. ADVANCED TECHNIQUES\n93\ntherefore we will also consider it in similar positions ﬁrst. This is dubbed killer\nheuristics.\nAn analysis of move orderings and the relation to alpha-beta was given in Eric\nThé’s master thesis [Eri92]. Killer heuristics were invented by Barbara Liskov\nin her PhD thesis [Hub68].\nAnother simple yet very eﬀective technique to improve search is the use of null\nmove pruning. It is quite advantageous in chess to be able to make two moves\nin a row. Suppose we consider a move by white, then let White immediately\nexecute another move (skipping Black’s turn), and then evaluate the position. If\nthe position does not give White an advantage despite being able to move twice,\nthen the ﬁrst move was probably a bad move to begin with. Null move pruning\nwas used quite early in chess programs, and the ﬁrst time it was mentioned in\nthe academic literature is most likely due to Adelson-Velsky et al. [AAD75].\nAnother simple trick to speed up search is transposition tables. A lot of move\nsequences in chess lead to the same position, and once we have analyzed one\nposition, we should try to re-use the result instead of re-creating the full search\ntree below that position. To do so, we simply reserve a small amount of memory\nwhere we store positions that we have encountered as well as their evaluation.\nIdeally this is not just the evaluation score computed by the evaluation function\nbut rather the result of a deeper search. Then when we encounter a new position,\nwe compare the current position to all positions stored in memory and if we\nﬁnd a hit, we simply re-use the existing evaluation instead of re-examining it by\nsearch. Here it is important to be able to quickly check if two chess positions are\nequal, and we can so by hashing. This is why this technique is often dubbed hash\nmaps. Of course we have only ﬁnite memory and therefore have to delete some\n(older) positions in order to store new ones once the hash map is full. When\nusing computer chess engines, there is often a parameter to adjust the size of\nthe hash map.\nLast, a huge amount of applied research is spent in tuning the evaluation func-\ntion. We can include as much chess knowledge that we want. Hsu, one of the\ncreators of the groundbreaking Deep Blue chess program for example writes in\nhis memoir [Hsu04] about the famous Deep Blue vs Kasparov match that they\n\n94\n3. SEARCHING\nincorporated an evaluation feature “potentially open ﬁle”, i.e. rooks would get\nhigher scores if they were placed on ﬁles that could potentially open up later\non after some capture sequence. He describes how that guided Deep Blue in\none particular position into placing the rooks optimally. Of course computing a\ncomplex evaluation function uses time, which is then lost to explore the search\ntree. Thus we have to make some compromise between a sophisticated (but\ntime-consuming) evaluation function where we can search only a few layers\ndown in the tree versus taking a dead-simple but fast evaluation function with\nwhich we can search much deeper.\nThe ground breaking open-source chess program Fruit1 for example used a\nrather simple evaluation function. However since it could thus search deeper,\nit beat a lot of competitors.\nThis illustrated that a sophisticated evaluation\nfunction is not always an advantage if it slows down search too much.\nOf course, ideally we would not handcraft an evaluation function, but rather\nautomatically create an near-optimal evaluation function by using neural net-\nworks, as we have explored in the earlier chapters. But more on this challenge\nlater on.\nOnly the most important optimizations are mentioned here. There is a plethora\nof techniques, reﬁnements and optimizations.\nThese kind of optimizations,\ntogether with implementation details, long diﬀerentiated good from very good\nchess programs. This is also what made chess programming so fascinating to\nprogrammers. A small neat idea or heuristic on how to improve the search\ntogether with some implementation optimization and your program gained\nanother 50 or 100 Elo points. With neural networks on the horizon, these days\nseem to be over.\n3.4\nA Note on Deep Blue\nFor alpha-beta searchers we need both fast search speed and good positional\nevaluation.\nThe case of DeepBlue greatly illustrates how important a good\nevaluation function is. Or in other words: An alpha-beta searcher with a weak,\n1https://www.fruitchess.com\n\n3.4. A NOTE ON DEEP BLUE\n95\nor rather incorrect evaluation function will result in a weak chess engine, no\nmatter how powerful the search is.\nWhen it comes to alpha-beta searchers, Deep Blue was probably the culmination\nof all 90s technology combined. A true chess monster. But let’s start at the\nbeginning.\nDuring the late 70s and early 80s, computers became a commodity item. Driven\nby the microcomputer revolution, it was now feasible to not only implement the\nchess logic but also achieve reasonable club player strength.\nCommon and aﬀordable were 8-bit CPUs, like the ones typically found in home\ncomputers of that era, e.g. the Zilog Z80 or MOS 6502. But dedicated chess\ncomputer units were also common, and they used embedded chips you probably\nnever heard of, like the Hitachi 6301Y.\nThese chips were cheap all-purpose commodity products and naturally very re-\nsource constrained. Or slow as a dog, to put it bluntly. Nevertheless enthusiastic\nspecialist programmers implemented impressive programs on them.\nThere was another direction of chess programming however that was more\ncentered around university research. Remember that chess was still a deﬁning\nproblem in artiﬁcial intelligence research at the time. These researchers had\naccess to supercomputers.\nAnd prototypical hardware design tools.\nThese\nenabled them to design special purpose circuitry for dedicated tasks such as\nchips optimized for digital signal processing.\nIt was this idea out of which Deep Blue was born. Namely to create dedicated\nchess chips: special circuitry for all the costly operations in chess; especially\nmove generation and position evaluation. And not only that, Deep Blue con-\nsisted of a massively parallel CPU design that was able to execute alpha-beta\nsearch in parallel. This resulted in a chess computer that could search and\nevaluate positions several magnitudes faster than anything that was available\non commodity hardware at the time; cf. Table 4.4 to get an intuition of its search\nspeed.\nHowever such a design came also with several challenges:\n\n96\n3. SEARCHING\nCreating a parallel version of alpha-beta search looks trivial on paper. Just ex-\necute diﬀerent branches (e.g. diﬀerent subtrees w.r.t. diﬀerent moves in a given\nposition) on diﬀerent processors. However anyone who has ever conducted\nexperiments in parallel programming will understand that this is an incredibly\ndiﬃcult task. You have to manage inter-process communication without having\ndead-locks, i.e. a situation where one processor waits for the result of a compu-\ntation of the second processor and vice-versa — so they will wait forever. You\nhave to balance the processor load evenly between the processes. And you also\nhave to simply implement the algorithm correctly, which is notoriously diﬃcult.\nI was once attending a scientiﬁc conference where I met someone working at the\nresearch division of a very large and widely known software company. He told\nme that the programmers of the software products were joking that you should\nalways create multi-threaded (i.e. executing in parallel) implementations, even\nif multi-threading is technically not required. The reason is that if there is a bug\nin the product, a customer will have a hard time to deterministically reproduce\nthe bug, and you won’t get bothered with customers and not reprimanded by\nmanagement for your bugs.\nThe researcher also said that he wasn’t really hundred percent sure though\nwhether the programmers meant that as a joke or for real.\nThe same goes for hardware design. Remember the famous Pentium FDIV bug?\nThe ﬁrst generation of Pentium processors had a bug that resulted in incorrect\nﬂoating point calculations. How come that such a large and skilled design team\nat Intel failed to spot the bug prior to release? It’s because debugging a chip is\nnotoriously diﬃcult once it’s put into silicon — especially with 90s technology.\nIn fact, the problems with such large CPU designs led to an advance in model-\nchecking and formal methods in order to ensure correctness of designs in the\nlate 90s. But when the DeepBlue team developed their chips such technology\nwas not readily available.\nThe source code and hardware of DeepBlue is not openly available. But from\nHsu’s own account of the events [Hsu04] we can infer that a signiﬁcant amount\nof time was spent with bug-hunting. In fact Hsu describes that he spent weeks\ndebugging problems with ensuring that the en-passent rule was recognized\n\n3.4. A NOTE ON DEEP BLUE\n97\ncorrectly. He also mentions that they had a “ghost queen” problem, i.e. under\ncertain conditions a queen would suddenly appear on the board in the corner\nand hence screw up the evaluation of a position completely. He also mentions\nhow DeepBlue just crashed several times during games and had to be rebooted.\nAnybody who has ever implemented a chess engine from scratch can probably\nrelate.\nAlso remember that it is crucial for an alpha-beta searcher to have a good eval-\nuation function available. We will see the eﬀect of an evaluation function later\nwhen we talk about NNUE technology. Whereas DeepBlue’s evaluation func-\ntion could be fed with several parameters and was implemented in hardware,\nit was still required to actually select suitable parameters and weight them ac-\ncordingly. If you do not implement chess knowledge in the evaluation function\nand just rely on, say, pawn counting, you will get an incredibly powerful pawn\nsnatcher who will utterly fail in any position where some positional assess-\nment is required. In fact, DeepBlue actually lost against Fritz with White at\nthe 1995 World Computer Chess Championship in Hong Kong, despite having\nmagnitudes more computing power available.\nAfter DeepBlue lost its ﬁrst match against Garry Kasparov in 1996, Grandmaster\nJoel Benjamin was hired by IBM to improve DeepBlue’s chess knowledge, i.e.\nits evaluation function.\nIt is interesting to look at all this from two diﬀerent perspectives. From the\ndevelopers perspective you have an incredibly powerful system which is noto-\nriously diﬃcult to program for, has several bugs, and lacks a good evaluation\nfunction.\nFrom an outsider’s perspective without a background in computer science you\nhave a chess computer whose developers claim how powerful it is, but which ac-\ntually plays bad chess and loses to a chess program running on a standard home\ncomputer like Fritz. It is not diﬃcult then to completely misjudge DeepBlues\ntrue capabilities.\nThere was a lot of controversy about the famous 37.Be4 move from the second\ngame of the 1997 Kasparov versus DeepBlue rematch. After the game Kasparov\nstated:\n\n98\n3. SEARCHING\nFigure 3.6: Rebel 8 in DosBox eventually ﬁnds the move Be4.\n“We saw something that went beyond our wildest expectations of how well a\ncomputer would be able to foresee the long term positional consequences of its\ndecisions. The machine refused to move to a position that had a decisive short-\nterm advantage – showing a very human sense of danger [...] Even today, weeks\nlater, no other chess-playing program in the world has been able to correctly\nevaluate the consequences of Deep Blue’s position.”2\nWe must put this in context: This was stated after the match ended and Kasparov\n2Garry Kasparov: IBM owes mankind a rematch. Time, vol. 149, no. 21, 1997.\n\n3.4. A NOTE ON DEEP BLUE\n99\nlost. During the match he alleged that the DeepBlue team must have cheated,\nand that the move in question (37. Be4) must have been the result of human\nintervention, as most chess programs could not resist of trying to snatch a\npawn with 37.Qb6 in that position instead of playing 37.Be4. In other words,\nthis statement reiterates the narrative that cheating happened, as it alleges that\nsince no (other) chess program was able to play that move, DeepBlue probably\nalso wasn’t able to ﬁnd it without human intervention.\nLet’s have a look how Rebel 8.0 evaluates the position. Back then Rebel was\nknown to be not only one of the strongest chess programs available but also\none that had a comparatively good positional evaluation compared to other\nprograms. Rebel 8.0 was released in 1996, well before the 1997 rematch with\nthe controversial move in question, and is nowadays a free download on Ed\nSchröder’s (Rebel’s main programmer) web-page 3. Computing resources were\nvery limited at the time though. As an experiment I let run Rebel 8.0 for a\nfew hours on my current machine in the DOS emulator DosBox. At ﬁrst Rebel\ndid not even consider 37.Be4 among its four best lines and strongly preferred\n37.Qb6, trying to grab the b6 pawn. But after a few hours of calculation it\nsuddenly prefers 37.Be4, as Figure 3.6 undoubtedly shows — the four best lines\nin descending order are shown in the middle box in the right4.\nSo what can we conclude from that? It ﬁrst shows that programs with good\npositional evaluation were very well able to see the move in question, contra-\ndicting Kasparov’s statement. We can also see that for a strong chess program,\nthorough positional evaluation is crucial and that we cannot compensate for a\nweak evaluation function with pure processing power. And last, we can also see\nthat it was very easy to misjudge the technical capabilities of DeepBlue with-\nout having knowledge about the intricacies of (hardware-based) chess engine\nimplementations.\n3http://www.rebel13.nl\n4Note that the game information shows Adams, M. vs Salov, V. This is simply the result of my\nlaziness of editing the game meta-data of a previously opened game.\n\n\n\n\n3.5. MONTE-CARLO TREE SEARCH\n103\nabove. Compared to chess however there seems to be much more focus on\npositional understanding and pattern recognition, due to the sheer size of the\nboard. Stones can be placed at seemingly unrelated locations far apart on the\nboard, and later when more and more stones are placed then suddenly their\nrelation matters. This brings us to a ﬁrst major observation when we compare Go\nwith chess: Opposed to chess, there is no simple evaluation function available\nwhen trying to assess an arbitrary position. Amateurs struggle with that, and\nit takes years of practice and talent to develop a positional understanding of the\ngame.\nIn chess, we can count pieces. In Go that doesn’t make much sense, because\nyou could have way more stones placed on the board building a group, but if\nthese stones are all captured at once later in the game because it turns out that\nyou cannot keep the group alive, then these stones are worthless.\nBuilding suitable evaluation functions is something that computer-go develop-\ners have struggled for ages. Before the revolution that happened with AlphaGo,\ncomplex evaluation functions incorporating lots of heuristic pattern detection\nwere used. These were however costly to compute. Remember that for alpha-\nbeta search to work, we need a fast and reasonable accurate evaluation function.\nIf the evaluation function is so slow that in practice we can only search one move\nahead, we basically do not search at all, but mostly rely just on the evaluation\nfunction to determine the next move.\nAnother complicating factor is the complexity of the game. As mentioned, Go\nis placed on a 19 times 19 board. Therefore the beginning player (Black) has\n19 ∗19 + 1 = 362 moves to choose from — in Go it is possible to pass, hence the\nextra move. When one stone is placed, the opponent has 362 −1 = 361 possible\nmoves and so on. Capturing stones and building groups of course limits the\namount of possible moves, but it is still quite high.\nCompare that with the amount of possible opening moves in chess.\nIn the\ninitial position, White can make two possible moves with each pawn, resulting\nin 16 possible moves. In addition there are four possible moves with the white\nknights, resulting in overall 20 possible opening moves for White. Black has\nthen 20 possible replies. After moving some pawns the amount of possible\n\n104\n3. SEARCHING\nmoves increases of course, but we can still see that the amount of possible\nmoves in a chess position is several magnitudes smaller than in Go.\nAccording to Matsubara et al. [MIG96] we can estimate the branching factor (i.e.\nthe average number of possible moves in a given position) for Go with 250 and\nfor chess with 35. Note that in addition, in chess there are often nonsensical\nmoves that are easy to spot — think of the previous section and quiescence\nsearch, where we can spot nonsensical captures. Whereas in Go it is not that\neasy. Also in Go, games just have way more moves until the game ends due to\nthe board size. It is also interesting to compare the branching factors of chess\nand Go with Shogi, the Japanese variant of chess. It diﬀers from western chess\nby having a slightly larger board (9푥9) and that captured pieces can be put\nback into the game – similar to Crazyhouse or Bughouse chess. Intuitively this\ncreates more complexity, and the branching factor is estimated to be around\n80. Interestingly, alpha-beta search works (just barely) for Shogi, and Shogi\nprograms have only recently reached grandmaster strength. In comparison,\ntheir strength has always been behind compared to computer chess, and despite\nvery novel and revolutionary techniques, today they still are nowhere near as\npowerful as their chess counterparts. We will have a look at these revolutionary\ntechniques, which have also been ported back to current chess programs, in the\nchapter about NNUE.\nTo summarize: Alpha-Beta does not work for Go, because\n• of the high branching factor of the game and\n• the fact that there is no fast and good evaluation function available.\nTherefore it makes sense to take a look at search techniques that work even with\nhigh branching and do not rely on handcrafted evaluation functions.\n3.5.2\nMCTS/UCT\nIn order to tackle the goal of creating an algorithm that can handle games with\na large branching factor and does not rely on an evaluation function, we’ll take\nan inspiration from chess opening theory.\n\n3.5. MONTE-CARLO TREE SEARCH\n105\nWhen professional chess player prepare openings, they rely on their own anal-\nysis as well as statistics. You probably have already seen opening databases.\nIn such databases, information about played games is stored and accumulated.\nFor example when opening the Lichess.org opening browser with the initial\nposition, we get the following information for the moves 1.e4 and 1.a3. The\ninformation is accumulated for games of top players.\n• For 1.e4, we know that this move was used in more than a million games.\nAbout 33 percent of all games were won by White, 42 percent were drawn,\nand 25 percent were won by Black.\n• For 1.a3, only 574 games were played by top players. Here 29 percent were\nwon by White, about 40 percent were drawn, and 32 percent were won by\nBlack.\nIt seems natural to think that 1.e4 better than 1.a3 by looking at the statistics.\nWe might be a little wary to be conﬁdent about that, since there were so few\ngames played with 1.a3.\nIt could be that 1.a3 is an excellent move and the\nsmall sample size is not representative. Another aspect to consider here is that\nthe default opening book of Lichess.org considers only games by higher-rated\nplayers. Clearly, good players chose better moves on average, and therefore this\nshould give a better indication on what is good and what is bad.\nBut what if we do not have such a database available? How about playing\nrandom games? Think of the following situation. You are in position where\nthere are only two moves; one move wins a queen, and another move allows\nthe opponent to move his queen away into safety. We could execute each move,\nand then for both of the resulting position play a few thousand random games.\nBy that I mean to just randomly select valid moves in every position that follows,\nuntil the game ends in a win for White, a draw, or a win for Black. We could then\ncompile the statistics for these random tryouts. We expect that for the move\nthat wins the queen, that on average we should have a higher winning ratio.\nAnd that’s the whole idea of monte-carlo tree search. To determine the next move,\njust execute each valid move, play a number of random games, compile the\nstatistics for these random tryouts, and compare the results to determine the\nnext best move. Such an approach has several advantages:\n\n106\n3. SEARCHING\n• There is no need for an evaluation function. We just play random games\nuntil the game ends.\nThis also means that monte-carlo tree search is\ncompletely independent of any domain-speciﬁc knowledge, and can thus\nbe used for any kind of full-information deterministic two-player game.\n• We can potentially handle games with a large branching factor. We just\nﬁx a number of random playouts and don’t explore all subsequent replies\nby each player. Of course this has the potential to fail: Suppose you are\nWhite, in a tricky endgame position, and trying to ﬁnd the next move.\nSuppose further that there are two moves. For the ﬁrst move by White,\nalmost all replies by Black result in a win for White, but there is just one\nspeciﬁc reply where Black saves a draw. For the other move, White always\nwins with perfect play (there might be still possible draws, if White i.e.\naccidentally stalemates). Then relying on random playouts might miss\nthis one speciﬁc reply for the ﬁrst move, and report the two moves as\nstatistically equal, or even falsely claim that the ﬁrst move is better. On\nthe other hand, Black’s speciﬁc reply might be also missed by a human\nplayer, and even with these imperfect statistics this approach might still\nmake a strong computer opponent.\n• We do not rely on a huge database, but can still create reliable statistics —\nexcept for the issues mentioned above — as random playouts are usually\nfast to execute. We can therefore quickly play and generate lots of games\nfor each position of the tree.\nIn order to describe monte-carlo tree search more formally, we can break down\nthe algorithm into four main operations: selection, expansion, simulation and\nbackpropagation.\n• Selection In this step, we select a node in the search tree according to\nsome rules. Initially we start with the current position as the root node of\nthe search tree. After going through some iterations we will have created\na larger search tree. In that case we move down in the game tree until we\nﬁnd and ﬁx a leaf node. Of course we will need to ﬁnd a strategy to select\na move in every step. Such a strategy should select moves such that the\nmost promising moves are selected and that we do not spend too much\n\n3.5. MONTE-CARLO TREE SEARCH\n107\ntime in unpromising nodes. We will later see how such a strategy can\nbe constructed regardless of any knowledge about the game by just using\nstatistics. For now we will just assume that such a strategy exists.\n• Expansion In this step we create a new node by applying moves at the\nnode selected in the selection step. In the most simplest version of the\nalgorithm, we just add precisely one move and add the resulting single\nnode. This move could be selected at random, or according to some other\nstrategy. We will stick with the simple version of just adding one node\nhere.\n• Simulation In this step we run a computation to get statistical information\nabout the new node. The most straight-forward way is to play a certain\nnumber of random games, i.e. select random moves until the game ﬁnishes\nin a win, draw or loss.\n• Back-Propagation In this step we take the statistical information that we\ncomputed by executing the random games and propagate it back up to\nthe root of the game tree.\nWe make sure to store and keep statistical\ninformation in each node on the winning statistics that we acquired for\nthe new node.\nWe can visualize these operations as follows: We start at the root node, and\nﬁrst select moves subsequently until we reach a leaf node (Figure 3.11). Here\nwe expand the leaf node by applying a (random) move, and create a new child\nnode (Figure 3.12). From this child node we play random games (Figure 3.13)\nand ﬁnally propagate the results through all parent nodes back to the root node\n(Figure 3.14).\nThere is one important wording that we need to make clear here. Usually, the\nword leaf in a game tree denotes nodes where no move is possible. For chess,\nthese are positions where the game is won, drawn or lost. Here we denote such\nnodes as terminal nodes. Leaf nodes on the other hand are nodes where not all\npossible child nodes have been expanded (and visited).\nConsider Figure 3.15: On ﬁrst sight, one might consider only nodes marked\n1/1 and 3/5 as leaf nodes. After all, for the node marked 2/5, we have already\n\n\n\n\n\n112\n3. SEARCHING\nFigure 3.16: An example of MCTS selection, expansion, simulation and back-\npropagation.\n\n3.5. MONTE-CARLO TREE SEARCH\n113\nnodes up to root of this newly expanded node. Here there is only one node.\nWe update the visit counter by changing it from 5 to 6, and we also update the\nevaluation (add one win). We would then continue to select a node, expand a\npreviously non-visited node, run a simulation, and update all parent nodes.\nWe simpliﬁed things here a little bit. Not all games have a binary outcome. In\nchess for example either White wins, there is a draw, or White loses the game.\nLooking at arbitrary games, we will have some kind of evaluation that our\nsimulation generates. Note that this should not be confused with an evaluation\nfunction as for e.g. alpha-beta search. There, the evaluation function evaluates\nnon-ﬁnal positions of the game. Here, we apply a simulation by random playout,\nand our evaluation result is based on the result of this playout. For example\nfor chess, we could encode win/draw/loss as (1, 0, −1) or by (1.0, 0.5, 0.0). In\nback-propagation, parent nodes up to the root node are then updated according\nto the following formula. Given a new evaluation 퐸, the average evaluation 푀\nand the number of times a node was visited 푉, updates are deﬁned by\n푀′ = 푀∗푉+ 퐸\n푉+ 1\n,\n푉′ = 푉+ 1\nConsider for example Figure 3.17. As in the previous example, we have selection\nand expansion on the left side. The simulation results in an evaluation of 0.1.\nThe parent node is then updated as\n푀′ = 0.4 ∗5 + 0.1\n5 + 1\n= 0.35 and 푉′ = 5 + 1 = 6\nThere are still two major issues left open that you are probably wondering about\nat this point. The ﬁrst one is: How is the selection operation deﬁned? Obviously\nselection has a huge impact on the outcome of MCTS. The second one is: Once\nwe ﬁnish our MCT search, how do we select a move? For the ﬁrst issue, there\nare two natural strategies:\n• Explore: We should try to visit all nodes and try not to search too selectively.\nOtherwise we might overlook an excellent move because we never select\nand expand the corresponding node.\n\n\n3.5. MONTE-CARLO TREE SEARCH\n115\nFor more details see [ACF02] where a detailed analysis of the multi armed\nbandit problem is given.\nThere might be smarter ways to do the selection operation. If only we had\nsome kind of oracle that could give us an assessment of which moves are more\npromising in a given position, so that we can immediately direct the node\nselection and expansion in the right direction and skip unpromising nodes.\nLike a fancy neural network, that can give us a quick assessment of the winning\nprobability of each child node... Without revealing too much at this point, this\nis precisely the intuition behind most of the neural-network based engines that\nwe will have a look at in the next chapter.\nThe second open issue was how to select a move once we ﬁnish our MCT search.\nThere are various strategies. We could select the direct child of the root node\nwith the highest average evaluation. On the other hand, UCT provides us with\na good ratio between exploration and exploitation. Meaning that if a leaf node\nhas been selected over and over again, the selection step already identiﬁed the\nbest child by taking into account the UCT value which itself is based on 푀,\nthe average evaluation of a node. The most common, and somewhat slightly\nunintuitive strategy is therefore to select the direct child of the root node that\nhas been visited most, i.e. the one with highest value 푉.\nWe have now gone through the basic MCTS/UCT algorithm. Let’s have some\nfun and implement this in Python to create a simple, MCTS-based chess pro-\ngram. Since it is in Python, it will be very slow and ineﬃcient, but straight-\nforward to implement. Moreover it practically illustrates the algorithm and\nunderlying concept. The core data structure here is a tree node, cf. Listing 3.8.\nListing 3.8: MCTS - Tree Node\n1 class TreeNode ():\n2\n3\ndef __init__(self , board):\n4\nself.M = 0\n5\nself.V = 0\n6\nself.visitedMovesAndNodes = []\n7\nself.nonVisitedLegalMoves = []\n8\nself.board = board\n\n116\n3. SEARCHING\n9\nself.parent = None\n10\nfor m in self.board.legal_moves:\n11\nself.nonVisitedLegalMoves.append(m)\n12\n13\ndef isMCTSLeafNode(self):\n14\nreturn len(self.nonVisitedLegalMoves) != 0\n15\n16\ndef isTerminalNode(self):\n17\nreturn len(self.nonVisitedLegalMoves) == 0 and len(self.\nvisitedMovesAndNodes) == 0\nIn the tree node we store the current evaluation in variable M, as well as the\nvisit count V. We also maintain two lists: One that stores all moves that we\nalready expanded at least once, and the corresponding child nodes. The other\nlist stores all moves that we have not yet expanded, but could expand later. It\nis then trivial to check if a tree node is a leaf node (if there are possible, but\nnon-expanded moves it’s a leaf node) and whether it’s a terminal node (there\nare no more possible moves, because e.g. it’s a checkmate).\nListing 3.9 shows the four operations of MCTS, i.e. selection, expansion, simu-\nlation and back-propagation.\nListing 3.9: MCTS - Operations\n1 def uctValue(node , parent):\n2\nval = node.M + 1.4142 * math.sqrt(math.log(parent.V) / node.V)\n3\nreturn val\n4\n5 def select(node):\n6\nif(node.isMCTSLeafNode () or node.isTerminalNode ()):\n7\nreturn node\n8\nelse:\n9\nmaxUctChild = None\n10\nmaxUctValue =\n-1000000.\n11\nfor move , child in node.visitedMovesAndNodes:\n12\nuctValChild = uctValue(child , node)\n13\nif(uctValChild > maxUctValue):\n14\nmaxUctChild = child\n15\nmaxUctValue = uctValChild\n16\nif(maxUctChild == None):\n\n3.5. MONTE-CARLO TREE SEARCH\n117\n17\nraise ValueError (\"could not identify child with best\nuct value\")\n18\nelse:\n19\nreturn select(maxUctChild)\n20\n21 def expand(node):\n22\nmoveToExpand = node.nonVisitedLegalMoves.pop()\n23\nboard = node.board.copy()\n24\nboard.push(moveToExpand)\n25\nchildNode = TreeNode(board)\n26\nchildNode.parent = node\n27\nnode.visitedMovesAndNodes.append (( moveToExpand , childNode))\n28\nreturn childNode\n29\n30 def simulate(node):\n31\nboard = node.board.copy()\n32\nwhile(board.outcome(claim_draw = True) == None):\n33\nls = []\n34\nfor m in board.legal_moves:\n35\nls.append(m)\n36\nmove = random.choice(ls)\n37\nboard.push(move)\n38\npayout = 0.5\n39\no = board.outcome(claim_draw = True)\n40\nif(o.winner == PLAYER):\n41\npayout = 1\n42\nif(o.winner == OPPONENT):\n43\npayout = 0.5\n44\nif(o.winner == None):\n45\npayout = 0\n46\nreturn payout\n47\n48 def backpropagate(node , payout):\n49\nnode.M = ((node.M * node.V) + payout) / (node.V + 1)\n50\nnode.V = node.V + 1\n51\nif(node.parent != None):\n52\nreturn backpropagate(node.parent , payout)\nIn the selection step, we ﬁrst test if the node is a leaf node. If so, we return\nthe node — this is the choice of our selection. Otherwise we iterate through all\nchild nodes, compute the uct-value for each node, and select the one with the\n\n118\n3. SEARCHING\nhighest uct-value.\nIn the expansion step, we remove one move from the list of those moves that we\nhave not considered yet. We create a new child node, and set all member values\nof the child node accordingly. Last, we add the move as well as the newly\ngenerated child node to the list of the node, where we maintain the visited\nchildren. We return the child node as the expanded node.\nIn the simulation step, we continue to randomly select legal moves of the current\nboard position, and apply them to the board until the game ends with a win,\ndraw or loss. Depending on the outcome we return a payoﬀof 1.0 (win), 0.5\n(draw) or 0 (loss).\nIn the back-propagation step, we update the average evaluation as well as the\nnode count according to the previously deﬁned formula. Then we recursively\napply this step to all parent nodes up to the root node.\nWe can test the implementation with the checkmate example shown in Figure 3.1\nas illustrated in Listing 3.10.\nListing 3.10: MCTS - Checkmate\n1 board = chess.Board(\" r1bqkb1r/pppp1ppp /2n2n2/4p2Q/2B1P3 /8/ PPPP1PPP/\nRNB1K1NR w KQkq - 4 4\")\n2 root = TreeNode(board)\n3 for i in range (0 ,200):\n4\nnode = select(root)\n5\n# if the selected node is a terminal , we cannot expand\n6\n# any child node. in this case , count this as a win/draw/loss\n7\nif(not node.isTerminalNode ()):\n8\nnode = expand(node)\n9\npayout = simulate(node)\n10\nbackpropagate(node , payout)\n11 root.visitedMovesAndNodes.sort(key=lambda x:x[1].V, reverse=True)\n12 print([ (m.uci(), child.M, child.V) for m, child in root.\nvisitedMovesAndNodes [0:10]])\nFirst we create the board position, and the root of the tree. Then we apply\na number of iterations of MCTS (here 200). Here we need to make sure that\n\n3.5. MONTE-CARLO TREE SEARCH\n119\n8 rZ0lkans\n7 opo0Zpop\n6 0Zno0Z0Z\n5 Z0Z0o0Zb\n4 0Z0ZPZ0Z\n3 ZBM0ZNZP\n2 POPO0OPZ\n1 S0AQZRJ0\na\nb\nc\nd\ne\nf\ng\nh\nFigure 3.18: Legal’s trap\nwe do not accidentally try to expand nodes that are terminal, i.e. where the\ngame ended. For example when looking at the child node after applying the\nmove Qxf7, there are obviously no moves to expand anymore. A simulation\nthen starts directly at this node. The simulation itself is then just determining\nthat the game is in a ﬁnal state, and will immediately compute the payoﬀ. After\napplying MCTS we take a look at the root node to determine the best next move.\nWe sort all moves according to the number of visits to that node. We note that\nthe most-visited node was the one with Qxf7 which was visited 15 times.\nHow about more complex positions? Let’s try Legal’s trap, shown in Figure 3.18.\nHere 1.Nxe5 wins a pawn, since after 1...Bxd1? 2.Bxf7+ Ke7 3.Nd5++ wins by\ncheckmate. The best reply by Black after the move 1.Nxe5 is 1...Nxe5, after\nWhite answers 2.Qxh5 and is a pawn up.\nNote that here the white bishop\nis placed on b3, whereas the position is typically presented with the bishop\non c4.\nThe reason is that the exchange gets slightly more deep then until\nthe advantage materializes, i.e. we need to check the sequence 1.Nxe5 Nxe5\n2.Qxh5 Nxc4 3.Qxc4 with a depth of ﬁve half-moves. Here we try to keep the\nsearch times low. Unfortunately, MCTS fails to identify the advantage with 2000\niterations. Is it that diﬃcult? How about alpha-beta search. When we apply our\n\n120\n3. SEARCHING\nsimplistic alpha-beta searcher with getNextMove(5, board, True)), the correct\nmove Nxe5 is found with a an evaluation of 280. How come that MCTS fails\nhere?\nThe ﬁrst issue is that after playing Nxe5, we notice that it is seemingly a very\nbad move. If we execute random games from that position, and Black captures\nthe white queen, there is only one very speciﬁc combination that enables White\nto win the game. In order to detect this, and in order to make sure the selection-\nstep moves in the right direction along that sequence of moves, we need a lot of\niterations. Just a few random playouts will not pick up that sequence of moves,\nand probably end in favor of Black, especially if the queen is captured and White\ndoes not reply accordingly. This is a peculiarity of chess, where very speciﬁc\ntactics often occur, and far outweigh any positional aspects. This is diﬀerent in\nother games such as Go for example.\nThe second issue is that we need a lot of playouts. However move generation is\nactually quite computationally intensive in chess. Chess moves seem intuitive\nfor the advanced player, but the rules are actually quite complex:\n• there are a lot of diﬀerent pieces, and each has it’s own rules\n• pieces may be prevented from moving if that places the own king in check\n• the knight can jump over other pieces whereas other pieces can not\n• pawns have incredible complex rules. They can move two ﬁelds in the\ninitial position, but only one ﬁeld otherwise. Except for captures. And\nen-passent. And then when they reach the ﬁnal row they become another\npiece\n• castling is a special move in that two pieces are moved at once. Several\ncomplex conditions must be met prior to castling and after executing the\ncastling move.\nAll that has to be computed, and fast move generation is actually very important\nwhen creating powerful chess engines. In addition to the issues mentioned\nabove, Python is a very slow language — it trades ease of programming with\nexecution speed. This is why the engines sketches shown in this book are so\n\n3.6. SUMMARY\n121\nslow.\nNow compare that with Go. There is only one kind of piece, a stone. A move\nconsists of placing that stone on a free cross on the board. Done. Computing\nthe possible legal moves is quite trivial.\nAll this aﬀects heavily the amount of simulations that we can do in a certain\namount of time. And this is why textbook MCTS as presented here is actually\nquite a bad choice for computer chess. There are of course exceptions to this\nrule. We will come back to that in the next chapter.\n3.6\nSummary\nLet’s quickly summarize: Alpha-Beta is a powerful search algorithm for game\ntrees. Its drawback is that it requires domain-speciﬁc knowledge for the evalu-\nation function. Also, the branching factor of the game must not be too large —\nin the extreme case, alpha-beta will search only one level deep, and essentially\nboil down to just directly applying the evaluation function once.\nMonte-Carlo Tree Search is domain independent and can be applied to any\ndeterministic two player game. If both alpha-beta and MCTS work, usually\nalpha-beta is more accurate and faster. But for games with a very high branch-\ning factor and/or no good evaluation function, MCTS can provide a powerful\nalternative to alpha-beta search.\nSpeaking of domain independence — is it a feature or a curse? In terms of chess\nwe could actually use a lot of domain knowledge. After all, for a computer it\nis much simpler to assess a position in chess compared to say Go. Even pawn\ncounting provides a meaningful evaluation in a lot of cases.\nAs for the timeline of MCTS: Brügmann [Brü93] ﬁrst applied Monte-Carlo tech-\nniques, i.e. random playouts to the game of Go. Coulom [Cou06] then applied\nMonte-Carlo techniques to tree search and combined these two. Finally, Kocsis\net al. [KS06] made the connection with the multi-armed bandit problem, and\ndeﬁned UCT.\n\n122\n3. SEARCHING\n\n4\nModern AI Approaches - A Deep\nDive\nI had seen some so-called research\nin AI that really deserved the\nbullshit label.\nFeng-hsiung Hsu\nAlphaGo was the breakthrough. The major achievement that both researchers\nand the public will talk about in future generations. Go was not just another\ngame that was mastered by computers, it was the deﬁning open question in\nartiﬁcial intelligence. As mentioned before, the extremely high branching factor\nand the diﬃculty of deﬁning objective criteria to evaluate a game position were\nthe reasons that Go was deemed unsolvable at the time being and in the future.\nTherefore this major achievement should not be understated.\nStill, one has to put this achievement in context. The theoretical groundwork had\nbeen laid out before. The foundation of neural networks and learning dates back\nto the 1980s and 1990s — there is a vast history and here we only mention [Fuk80,\nLBD+89, LBBH98] as examples. Network designs and structures were further\n123\n\n124\n4. MODERN AI APPROACHES - A DEEP DIVE\nexplored and enhanced in the early 2000s. Still nobody deemed it possible to\ncreate a Go engine with these techniques. Not only are very deep and large\nnetworks often diﬃcult to control — when a network fails to converge there\nis often no direct cause that one can identify, and it takes a lot of practical\nexperimentation to create suitable network designs for the problem in question.\nBut also the sheer amount of computational power was estimated to be too large.\nNo university could aﬀord to spent that amount of computing power to a board\ngame with a high possibility of failure and wasted resources. The DeepMind\nteam with the backing of Alphabet Inc. however, could.\nNevertheless, if we take a look at the approach that was taken — and we\nwill, in the next section — one cannot deny that it looks a little bit crude and\noverly complicated. This is not untypical of a scientiﬁc breakthrough: You just\ntry out a bunch of diﬀerent things, re-combine them and mangle with diﬀerent\ntechniques until you ﬁnd something that ﬁnally works. Still, it is very interesting\nto study the techniques that were used in this breakthrough, which is subject to\nSection 4.1.\nHowever, the simpliﬁcations introduced by Alpha(Go)Zero are by no means less\nof an achievement. The whole learning pipeline was simpliﬁed and combined\nwith the search part — making the overall algorithm much more eﬃcient and\neasier to implement as well. The techniques of AlphaGoZero and AlphaZero\nare introduced in Section 4.2 and Section 4.3 — and if you are merely interested\nin state of the art neural-network engines for chess, it makes sense to directly\nstart from there.\nUnfortunately, the implementations of AlphaGo and Alpha(Go)Zero were never\nmade public.\nLeela Zero (Go) and Leela Chess Zero were attempts to re-\nimplement the algorithms presented by the DeepMind team in an open-source\nmanner and make them available to the general public. Whereas DeepMind\nhad a server farm at their disposal, the enormous computational power re-\nquired for training was provided by volunteers all over the world who shared\ntheir computer’s power to the beneﬁt of the global chess community. As devel-\nopers were re-implementing the algorithms presented in the scientiﬁc papers\nof the DeepMind team, the underlying hardware that the DeepMind team used\n(speciﬁcally designed processors dubbed tensor processing units or TPUs) was\n\n125\nnot available. Moreover research in neural network design progresses fast, and\nall this required some slight changes in the architecture of Leela Chess Zero\ncompared to Alpha(Go)Zero. LeelaChess Zero is subject of Section 4.4\nFat Fritz (and especially later Fat Fritz 2) made quite some headlines when they\nwere introduced to the public by Chessbase GmbH and sold commercially -\nafter all, they are heavily based on open-source implementations with certain\nchanges. We will take a look at Fat Fritz in Section 4.5 and also try to give\na (subjective) evaluation whether we should put Fat Fritz more in the copycat\ncategory or in the legitimate improvement category.\nAnother real achievement and scientiﬁc breakthrough happened without much\npublicity: Eﬃciently Updateable Neural Networks, abbreviated NNUE (in re-\nverse order). NNUE was originally developed for Shogi. In chess, the results by\nAlphaZero were quickly reproduced by a worldwide community of volunteers.\nThe Shogi community is naturally much smaller, and therefore no powerful\nwidely available AI based engine appeared; the results of AlphaZero could not\nbe reproduced without the much needed computing power. Yu Nasu, a com-\nputer science researcher and computer Shogi enthusiast came up with the novel\nidea of designing a neural network for position evaluation that was optimized\nto run on oﬀ-the-shelf CPUs. Whereas the network itself is certainly not even\nremotely as powerful as a deep convolutional network like the ones used for Al-\nphaZero, it is on the one hand still more powerful than a handcrafted evaluation\nfunction, and on the other hand fast enough to be combined with alpha-beta\nsearch. The beneﬁcial impact of this network design was quickly identiﬁed by\nthe Shogi community but somehow overlooked by the chess or AI community\nin general. Part of the reason maybe that the original paper about NNUE was\navailable in Japanese only.\nThe computer Shogi scene took a lot of inspiration from the Stockﬁsh project, e.g.\nthe very eﬃcient implementation of (alpha-beta) search with all optimizations\nand search enhancements. A Japanese AI programmer that goes under the\npseudonym yaneurao then felt that the Shogi community should “give back” to\nthe chess community, and created a port of the NNUE network structure back\nto Stockﬁsh, replacing the handcrafted evaluation function of Stockﬁsh by the\nneural network. This yielded a chess engine that not only beat LeelaChess Zero,\n\n126\n4. MODERN AI APPROACHES - A DEEP DIVE\nbut was also capable of running on commodity hardware — with no graphic\ncard at all! This amazing step forward is subject of Section 4.6.\nLast, but not least, Chessbase took the existing open source code from the\nStockﬁsh project with the NNUE enhancement and trained the neural network\nin a slightly diﬀerent way. The result was then Fat Fritz 2, being subject of\nSection 4.7.\nMost attempts at using neural networks for chess engines aim at making chess\nengines stronger. A completely diﬀerent research direction is to use them to\nmake engines play more human-like. An experimental engine that takes this\ndirection is Maia which is subject of Section 4.8.\n4.1\nAlphaGo\nAt the heart of AlphaGo are ﬁve neural networks:\n1. A policy network that gets as input a Go position (plus some extra features)\nand outputs probabilities for moves.1\nThis network was trained with\nmillions of positions from games of top Go players, i.e. for training the\ninput was the Go position, and the expected result was the move of the\ntop player with probability one, and the other moves with a probability\nof zero. This network is dubbed the SL policy network. Here, SL stands for\nsupervised learning.\n2. The SL policy was then improved by policy gradient reinforcement learn-\ning into the RL policy network. RL stands for reinforcement learning. The\nunderlying idea is here to start with the SL policy network, play lots of\ngames against itself and observe the result. With these observations as\ntraining data, the network is then improved and this is the ﬁrst iteration\nof the RL network. Take the RL network, let it play against the SL pol-\nicy network, obtain training data, and improve the RL network. Then\n1The term policy network stems from AI research. Here, a policy deﬁnes how an actor behaves\nin a given situation. In the context of board games, this simply means that the actor is a player,\nthe situation she is in is deﬁned by the board position, and the policy deﬁnes the next move.\n\n4.1. ALPHAGO\n127\ncontinue this approach and iterate a lot of times, but each time take ran-\ndom previous instances of the RL network as opponents (as if you would\nplay against lots of diﬀerent opponents with diﬀerent strengths and weak-\nnesses). Finally, obtain the RL policy network.\n3. A policy network dubbed rollout policy that takes as input tactical informa-\ntion and patterns of a Go position and the last move that led to that position\nand outputs probabilities for moves. This network is very simplistic, being\na linear softmax classiﬁer. Its advantage is that getting move probabilities\nfor a Go position is several dimensions faster than using the large SL pol-\nicy network — reported values are two microseconds to compute move\nprobabilities for the rollout policy compared to three milliseconds(!) for\nthe SL policy network. The network is trained similar to the SL policy\nnetwork, i.e. by taking positions from games of top Go players together\nwith the moves chosen by the players as training examples.\n4. A policy network dubbed tree policy. This one is very similar to the rollout\npolicy network, but uses more input features. It is thus slightly slower than\nthe rollout policy, but still much faster than the SL or RL policy networks.\nWhereas the rollout policy network is used during the simulation step of\nMCTS, the tree policy is used during the expansion step. Training for this\nnetwork is done in the same ways as for the rollout policy.\n5. Last, train a value network. The value network receives as input the current\nposition, and outputs a value, essentially predicting how likely it is to\nwin or lose from that current position. More precisely, it outputs a value\nthat predicts who is going to win if the game continues from the current\nposition and both players play according to a given policy. To train this\nvalue network, ﬁrst the (ﬁnal) RL policy network was used to play a lot of\ngames, thereby creating training data consisting of positions as input, and\nresults of how the game ended when played according to the RL policy\nnetwork.\nAfter these ﬁve neural networks are created, we’re done with all training. The\nGo engine then works as follows: Monte Carlo Tree Search is used for a given\nposition to determine the next best move. The MCT search however diﬀers\n\n\n\n130\n4. MODERN AI APPROACHES - A DEEP DIVE\ncomponents. First we’ll examine the networks in detail. Then we will have a\nlook at the modiﬁed Monte Carlo Tree Search.\n4.1.1\nThe SL Policy Network\nNetwork Input\nThe input of the network consists not only of the current state\nof the board, but also some extra features that give additional information. All\nthese input features are encoded as binary values. Of course we could think of\nother ways to encode input features, e.g. with some integer encoding of numbers\nas is typical in computing. However designing the input as simple as possible\nhelps the network to train faster, as it directly sees the input and does not have\nto spent time learning the encoding.\nA go board is 19x19 lines, which naturally results in 19x19 values. Such a 19x19\narray is called a plane. Each value of the plane is binary, i.e. can be either 0 or 1.\nAll in all there are 48 input planes. In other words, the input to the network are\n19푥19푥48 = 17328 binary values. You can imagine that this is a large network\nindeed.\nBut let’s start with the encoding of the current board position. If we have only\nbinary values available, we need three plains to represent the current position:\nOne plane where a 1 indicates that there is a black stone placed at that position\nand where a 0 denotes that there is not a black stone placed there. Then one\nmore plane to encode the white stones, and ﬁnally one plane to encode all empty\nﬁelds. This is very similar to the encoding that the successor AlphaGoZero\nused; cf. Figure 4.6. Note that here, we always encode a board position from the\nperspective of the player whose move it is. That is, the ﬁrst plane contains the\nposition of the black stones if it is Black to move, and it contains the position of\nthe white stones if it is White to move. The second plane contains the position\nof the stones of the respective other side. The third plane contains the positions\nof all empty points. The idea of this network is to create a neural network that\noutputs the best move for a given position for the side that is about to move. It\n\n4.1. ALPHAGO\n131\nreally does not matter if the stones of that side have White or Black color.3\nThen there are two categories of additional features used as an input.\nThe\nﬁrst category is some simple additional information that makes it easier for the\nnetwork to identify the border of the Go board. It has to do with convolutional\nﬁlters of which the neural network makes heavy use of.\nRecall how a convolutional ﬁlter works (cf. Chapter 2). In particular let’s take\nanother look at Figure 2.13. A convolutional ﬁlter works by putting an 푛× 푛, for\nexample a 3 × 3 kernel over an image (or here a 19x19 Go board representation).\nThen, depending on the ﬁlter size, a certain number of pixels (here ﬁelds of\nthe Go board) are convoluted into one pixel. If we do this in a straight-forward\nmanner over an image of say, size 4x4, we end up with a smaller board after\napplying the ﬁlter. Since we want to keep the size of the Go board over all\nlayers of the network, zero padding is applied. For the SL network, the 19x19\nboard input is zero-padded into a 23x23 board. However doing this without any\nadditional input makes it hard for the neural network to distinguish whether\n0’s at the edge are actually really zeros that are a result of the board position,\nor are a result of the zero padding. In other words, the edge of the board gets\nfuzzy or blurred. The solution to this is to add a constant plane of size 19x19\nthat has just ones, and another constant plane of size 19x19 of just zeros.\nThe other category of input features is encodings of simple properties of the\nboard position. Technically they are not required. A network could simply learn\nthese features itself by lots of training. However in order to reduce the size of\nthe network and training time, it makes much sense to pre-calculate this simple\ninformation and add it as input features, so that some burden of learning is\ntaken away from the network.\nA lot of these features are trivial to recognize for those who are familiar with the\ngame of Go. What is actually interesting is not necessarily the choice of these\ninput features, but rather the one-hot encoding style, which is used for all of\n3Technically this is not true. As Black has an advantage by moving ﬁrst, there is compensation\nin the form of komi points given to the white player. But this is not relevant if we want to ﬁnd the\nbest move in a given position, only if we want to predict the game outcome. Hence the color (and\nthus the komi) is encoded as an input feature in those networks that predict outcomes.\n\n132\n4. MODERN AI APPROACHES - A DEEP DIVE\nthese additional features. One-hot encoding means you have one bit for every\ncategory. Therefore, we will not discuss all of these features, but only discuss\nthe encoding of liberties as an example.\nA liberty in Go is simply an empty adjacent point next to a placed stone. When\nthere are connected stones of one color, their liberties become important. Re-\nmember that for a group, liberties are needed for the group to survive; cf.\nSection 3.5.1. Since we want to pass this information as input features to the\nneural network, we create eight 19x19 planes. In the ﬁrst plane, each point with\na placed stone that has one liberty is set to 1, and the other points are set to zero.\nIn the second plane, each point with a placed stone that has two liberties is set to\n1, and the other points are set to zero. We repeat this scheme up to the seventh\nplane. Finally in the eight plane all points with stones that have eight or more\nliberties are set to 1, and the other points are set to 0. This style of encoding is\nknown as one hot encoding.\nThis makes it easy for the network to recognize if a group is alive or not and\nwhether it can be captured or not, simply by determining if there are enough\nliberties.\nThe same encoding style is used for four more of such kind of tactical informa-\ntion about the current board position. Together these make up for 19x19x8x5\ninput features. Three more planes are used to encode simpler information and\nonly take one plane each.\nAll in all we have for the network the following features: The points with stones\nof the player, the points with stones of the opponent and all empty points (three\nplanes), a constant plane of ones and a plane of zeros (two planes), one-hot\nencodings of how many turns since a move was played, the number of liberties,\nhow many stones of the opponent would be captured if a stone would be placed\nat a given point, how many of one’s own stones would be captured if a stone\nwould be placed at a given point, how many liberties there are if a stone would\nbe placed at a given point (eight planes encoding this information for each\npoint for each for these ﬁve features), whether a stone placed at a given point is\na ladder capture (one plane), a ladder escape (one plane), and whether a move\nis a legal move at a given point in the ﬁrst place (one plane).\n\n4.1. ALPHAGO\n133\nNetwork Output\nThe network output consists of 19 × 19 + 1 = 362 move\nprobabilities, i.e. for all possible moves, including the pass move, a probability\nis given, and the sum of all probabilities add up to 1.\nThis is achieved by\napplying a softmax activation function at the ﬁnal output of the network.\nNetwork Architecture\nThe architecture of the SL policy network is shown\nin Figure 4.3. The ﬁrst step consists of zero-padding the 19 × 19 planes into\n23 × 23 to not shrink the input when adding a subsequent convolution layer.\nAs discussed before, in order to make sure the network recognizes the size of\nthe board as 19 × 19, two constant planes of ones and zeros are added as input\nfeatures.\nNext a convolution layer is added with 192 ﬁlters4, stride 1, and kernel size of\n5 × 5.\nThe next layers make the network a truly deep one. The following structure\nis repeated ten times: First the network is zero-padded to have 21 × 21 planes,\nnext a convolution layer is applied with 192 ﬁlters, kernel size 3 × 3 and stride\n1, and afterwards a rectiﬁer linear unit (ReLU) is applied.\nFinally a convolutional layer with only one ﬁlter, kernel size 1 × 1 and stride 1\nis applied, before using a softmax activation function that generates the output\nof the network.\nNetwork Training\nThere is a free online platform where players can compete\nagainst other Go players, the KGS Go Server5. The DeepMind Team acquired the\ndatabase of all games, extracted games from higher-rated players, and sampled\nabout 30 million positions together with the move played by the expert player\nin that position. These positions together with the move were then used to\ntrain the SL policy network by stochastic gradient ascent to maximize the log-\nprobabilities of the moves; cf. Section 2.3.\n4Actually, 192 ﬁlters were used in the AlphaGo version that played matches against the\nEuropean Go Champion Fan Hui. They also tested diﬀerent numbers of ﬁlters, but in their setup\n192 seemed to give the best trade-oﬀbetween strength and evaluation time.\n5http://www.gokgs.com\n\n\n4.1. ALPHAGO\n135\nThe trained network predicted 57 percent of expert moves from a test set of\npositions. This actually does not sound much, but was an increase of more than\n10 percentage points compared to the state of the art.\nUnfortunately, there are no precise numbers given on how much it took to train\nthe SL policy network, and what hardware architecture was used (much more\nprecise information is given for AlphaZero). However from [SSS+17] we can\ninfer that “AlphaGo Lee (the version that played against elite Go player Lee Sedol)\nwas trained over several months”, even though that leaves open how much time\nwas spent on training the SL policy network, and how much time was spent on\nimproving the SL policy network to create the RL policy network.\n4.1.2\nThe RL policy network\nThe RL policy network is created by taking the SL policy network and improving\nit by applying reinforcement learning. In other words, network input, network\noutput and network architecture are the same as with the SL policy network;\ncf. Figure 4.3. We thus have to take a look at how the reinforcement learning\nprocedure works.\nPolicy Gradient Reinforcement Learning\nThe original paper by the Deep-\nMind team does not explicitly mention why the policy network was not further\ntrained by more human experts. That is to create examples by sampling posi-\ntions from games of very good players together with the move played by the\nplayer in that position. The simple truth might be that there were no more than\n30 million positions available. The question is now how to get more training\ndata. The idea is to generate training data by self play and apply policy gradient\nreinforcement learning.\nRemember that when having expert games, the training of the policy network\nworks roughly like this:\n1. feed one (or a batch of) positions in the neural network\n2. for each, get the output vector of move probabilities (via a softmax activa-\ntion function)\n\n136\n4. MODERN AI APPROACHES - A DEEP DIVE\n3. compute the partial derivatives of the log-probabilities, i.e. compute\n휕log(푝(푚|푠))\n휕휔\nwhere 푝(푚|푠) is the probability that the network outputs for move 푚that\nwas played by the expert human player in position 푠.\n4. use those partial derivatives to perform gradient ascent (resp. descent\nwith the negated logarithm) and update the weights of the network.\nIn that procedure we train the move policy by using the move labels. We just\nassume that the human expert knows what he is doing. The intuition between\npolicy gradient reinforcement learning is to just randomly generate samples,\nobserve the result of the game, and then use the result to gradually tune the\nmoves by gradient ascent such that the policy networks plays moves which are\nmore likely to win the game.\nAfter all, if the game was won in the end, we did something correct when playing\nthe game. Some moves must have been good moves. The rough procedure,\nnamely policy gradient reinforcement learning is then\n1. play a game of random moves. When the game is ﬁnished, we have a result\nof +1 (Black wins) or −1 (White wins). Note that in general, there are no\ndraws in Go.\n2. feed one (or all) positions into the neural network and get an output vector\nof move probabilities (via a softmax activation function)\n3. compute the partial derivatives of the log-probabilities. However if the\ngame was won, we want to reward the moves that were taken. Hence we\nmultiply the probabilities by the outcome of the game, i.e. compute\n휕log(푝(푚|푠))\n휕휔\n∗(+1)\nif the game was won, and compute\n휕log(푝(푚|푠))\n휕휔\n∗(−1)\n\n4.1. ALPHAGO\n137\nif the game was lost. Here, 푝(푚|푠) is the probability that the network\noutputs the move 푚that was actually taken in the random playout.\n4. use those partial derivatives to perform gradient ascent and update the\nweights of the network.\nThe logic is here: The moves that were taken in positions that eventually lead to\nwinning the game must be all good, so we should tune the networks such that\nthe probabilities of taking these moves in those positions are maximized. On\nthe other hand, the moves that were taken in positions that eventually lead to\nlosing the game must be all bad, so we should tune the network such that the\nprobabilities of taking these moves in those positions should be penalized.\nActually, is that true? Thinking of chess, there are several games with 1.e4 in\nthe initial position where White wins, and then there are also several games\nwere White loses. Is 1.e4 a good move in the initial position or not? Are we not\nalternatively maximizing and minimizing the probability of such a move when\nwe encounter games where White wins and White looses? In other words, does\nthe ﬁnal outcome really have a lot to do with the action we took a long time\nbefore the end of the game?\nWell, turns out it actually does. You just need to play a lot of games. Proving that\npolicy gradient reinforcement actually converges to an optimal policy (albeit a\npossible local optimum, as this is gradient descent/ascent as introduced in\nChapter 2) is complicated and involves lots of math. If you are interested, a\ngood starting point is [SB18].\nPolicy gradient learning can easily fail to converge in practice. This is probably\nwhy they ﬁrst trained a network by supervised learning (i.e. learning by using\nhuman expert moves) to kickstart a powerful network, and then used policy\ngradient reinforcement learning only to improve the existing network. There\nwere also some tricks used to prevent overﬁtting of the network, i.e. the games\nare not really played randomly, but rather between the policy network and\nanother policy network that is randomly chosen among some earlier iterations\nof the network. This training loop is depicted in Listing 4.1. Here we start by\ninitially using the SL policy network both for the RL policy network and the\nopponent. Then we apply a training loop. First we play a lot of games of the RL\n\n138\n4. MODERN AI APPROACHES - A DEEP DIVE\npolicy network against the opponent. Using these games and their outcomes,\nwe improve the RL policy network by gradient policy reinforcement learning.\nThen we randomly choose a next opponent among the previous incarnations of\nthe RL policy network, and repeat the training loop. This training by playing\nagainst random opponents as opposed to going with a ﬁxed policy network is\ncrucial here since otherwise the RL policy network would stick too much to the\ncurrent policy.\nVery interesting is by how much the existing SL policy network was improved\nby policy gradient reinforcement learning, resulting in the RL policy network.\nThey report that the RL policy network won approximately in 80 percent of all\ngames against the SL policy network.\nAnother interesting fact is how strong the RL policy network was compared\nto existing Go programs. The Go program Pachi originated from Petr Baudis\nmaster thesis on Monte Carlo Tree Search [Pet11]. Pachi implemented state-of-\nthe-art techniques for computer Go and used MCTS to ﬁnd best moves. Even\nthough Pachi was not the strongest Go program at the time — some commercial\nvendors had stronger programs available — it was one of the strongest (if\nnot the strongest) open source Go program and ranked high at computer Go\ncompetitions. However by just taking the RL policy network without any kind\nof search, the RL policy network won 85 percent of all games against Pachi.\nBecause it’s so amazing, let’s repeat that: Without any kind of search, just by\nevaluating a position by the RL policy network and playing the move with the highest\nprobability, the RL policy network won 85 percent of all games against Pachi.\nIn the end however, we should not focus too much on technical details of\npolicy gradient reinforcement learning. It is a very general algorithm that can\nbe applied to all kinds of games. But in the context of Go, Shogi and chess,\nthe DeepMind team found a much more elegant, simpler and faster approach,\nwhich we will take a look at in the chapter about AlphaZero.\n4.1.3\nThe Rollout Policy Network\nIn standard MCTS, we use random playouts of games during the simulation\nstep. For Go, this is very simple and fast to compute. Remember that move\n\n\n140\n4. MODERN AI APPROACHES - A DEEP DIVE\nand are based on both the last move that led to the current position, and on pat-\nterns w.r.t. potential moves in the current position. They are all binary features,\ni.e. a 1 indicates that the pattern in question is present, and a 0 indicates that it\nis not present.\nTo give a better intuition on how this works, let’s consider a small example,\nnamely nakade. Consider the board in Figure 4.4. Can this group be alive? It is\nsimilar to the discussion of Go rules at the beginning of this Chapter. If Black\ncan place a stone inside at the marked position (and this is what nakade actually\nmeans, i.e. putting a stone inside) than she has two eyes and the group is alive.\nIf however it is White’s move, we have the following variations. White will put\na stone at the marked position. Then\n1. if Black puts a stone left or right next to the white stone, White can put a\nstone on the last free point and capture everything.\n2. if Black does not put a stone inside, then there are two points left open.\nWhite will put one stone next to the marked position, and only one point\nis unoccupied. Then\n(a) if White does nothing, Black will place a stone at the last unoccupied\npoint and capture everything\n(b) if White places a stone at the last unoccupied point and captures\nBlack’s stones, two points are emptied. Black will put a stone on one\nof these unoccupied points and White will lose the group, too.\nAs you can imagine, there are a number of typical patterns that can occur on\na Go board that ﬁt into this tactical nakade category. Obviously they are highly\nimportant when ﬁguring out a next move. What is done for the rollout policy\nnetwork is to scan the 19 × 19 points, scan at each point whether a move there\nis a capturing move and whether this would match a nakade pattern. Then set a\n1 if this is true, and a 0 otherwise.\nThe input of the rollout policy network consists of a 109747 bit input vector.\nEach bit indicates if one of the patterns was detected for a given point w.r.t. the\nlast move that led to the position. Note that using such kind of handcrafted\npattern detection to identify tactical motifs is not uncommon in computer Go,\n\n4.1. ALPHAGO\n141\nand had been heavily used in state-of-the-art Go programs. Compared to those\nhowever, the amount of handcrafted features that AlphaGo uses is rather low.\n4.1.4\nThe Tree Policy Network\nThe tree policy network is also a linear softmax based network. Essentially, it\nis the same network as the rollout policy network, but uses approximately a\n30 percent larger input vector. Simply the amount of patterns was increased.\nThis increases computational cost when calculating the move probabilities for\na given position, but it is still much faster than utilizing the SL policy network.\n4.1.5\nThe Value Network\nNetwork Output\nLet’s start with the output of the network for once before\ntaking a look at the input. Suppose you have a move policy that, given a Go\nposition, tells you what the next best move is. Now suppose further you are\ngiven an arbitrary Go position and have the policy play against itself from that\nposition until the game ends. The output of the value network is just a single\nvalue between −1 and 1. It denotes the expected result of the game from a given\nposition that is played according to a given policy. It is important not to confuse\nthis and interpret it as if the value network answers the question “Who will win\nthe game?”, because instead it answers the question “Who will win the game if\nthe game continues according to the given move policy?”. The latter question\nis much more limiting, since the answer could be totally worthless if the move\npolicy is garbage. In other words, we need a very good move policy in order to\nmake the value network useful!\nNetwork Input\nThe input to the value is similar as for the SL and RL policy\nnetwork, namely a Go position as well as additional features. As mentioned\nbefore, for the SL and RL policy network the Go position is always encoded\nfrom the perspective of the player whose turn it is. Consider Figure 4.6 for\nexample where it is Black to move. Therefore the position would result in the\nthree planes encoding the position of the black stones, the White stones and the\nempty stones. If it is White’s turn however, the encoding would simply switch\nthe colors; the bit plane for the black stones becomes the one for the white stones\n\n142\n4. MODERN AI APPROACHES - A DEEP DIVE\nand vice versa. This is very handy, since the network can train to predict the\nbest move regardless of having to care about colors and turns. After all, we just\nneed output probabilities for all possible next moves, and to get this output we\ndon’t need to care about the color.\nFor the value network the situation is diﬀerent of course. We need to be precise\nif it is Black’s or White’s turn. This is not just about switching signs! Similar to\nchess, having the ﬁrst move could make a huge diﬀerence in a given position;\ncf. the footnote about komi when discussing the SL policy network. Hence, the\ninput of the value network is the same as for the SL/RL policy network plus\nan additional plane consisting of either only ones or only zeros that indicates\nwhether it is Black or White to move.\nNetwork Input\nThe structure is pretty much the same as for the RL and SL\npolicy networks. The input layer is the same except that we need to now also\ninput one plane that encodes the side that is about to move. The main diﬀerence\nis of course the output layer. Instead of outputting move probabilities, we want\nto predict the outcome of a game. Adding a fully connected layer with a tanh\nactivation function is used for this. The tanh ensure that we always get an output\nin the range −1 (White wins) and 1 (Black wins). Ideally the network would\nalways output either 1 or −1. However since the network is unable to exactly\npredict the outcome for every possible Go position it will output intermediate\nvalues.\nNetwork Training\nThe network is trained by 30 million Go positions together\nwith the outcome of the game when continuing from this position by self-play\nwith the RL policy network. These Go positions itself stem from 30 million\nfully self-played games by the RL policy network from the initial position. One\nposition plus the outcome was then selected from this game.\nThe average length of a Go game is within a range of about 200 moves. In\nother words one game of Go gives us about 200 positions. Then why should we\nnot just self-play 150000 games, and sample 150000 * 200 = 30 million positions\nfrom it? It surely would be much faster to just play 150000 instead of 30 million\ngames!\n\n\n144\n4. MODERN AI APPROACHES - A DEEP DIVE\nThe reason is overﬁtting. When the network was trained with full games, the\nnetwork would adjust too much to the speciﬁcs of these games. In other words\nthe network tended to memorize full games, instead of abstracting the underlying\npatters and actually learn6 from the games.\nAs for the training method itself, stochastic gradient descent is used w.r.t. mini-\nmizing the mean-squared error between the predicted outcome of the network\nand the actual result that is the outcome of self-play by the RL policy network.\n4.1.6\nMCT Search\nAfter all networks are trained, the actual Go engine is based upon MCT search.\nThe MCT search follows the standard MCTS principle, i.e. consists of the steps\nselection, expansion, simulation and backpropagation. However where UCT-MCTS\nwas solely based on random tryouts, the MCTS procedure here makes heavy\nuse of the trained neural networks:\n• During the expansion step, both the tree policy network as well as the\nSL policy network are used to identify promising moves and prioritize on\nthese identiﬁed moves. The tree policy network is used to very quickly get\nprobabilities for moves in the leaf-position. The SL policy network takes\nlonger to process the leaf-position, but as soon as its output is available, the\nprobabilities of the tree policy network are replaced by the probabilities\nof the SL policy network.\n• During the simulation step, the current position is evaluated in two ways:\nFirst, by applying the value network and getting an expected result. Sec-\nond, by games self-played by the rollout policy network. These two results\nare then combined using some weighting factor.\nLet’s have more detailed look into how this works. In UCT-MCTS we stored\ntwo values, the visit count as well as the number of winning games (or some\nwin ratio). Moreover we could describe all steps as operations on nodes. Since\n6I ﬁnd the similarity to humans interesting.\nEspecially beginners in chess often tend to\nmemorize opening lines without really understanding the ideas and structures instead of tackling\nmore diﬃcult areas like endgames where probably much more can be learned about the game.\nAt least personally I cannot say that I am free of guilt in that area...\n\n4.1. ALPHAGO\n145\nwe here have probabilities on moves, we associate the following three values\nwith edges (i.e. with the moves that connect Go positions):\n1. A 푄value; dubbed the action value.\nHere, we will simply call it the\nmove value. This value denotes how good a move actually is. It is thus a\nsimilar measure compared to the number of winning games in standard\nUCT-MCTS, but its calculation takes all the network outputs into account.\n2. A value 푁; dubbed the visit count. This is the same as in standard UCT-\nMCTS.\n3. A value 푃; dubbed prior probability. This stores the probability of a move\ngiven by the tree policy network (to get a quick estimate) and as soon as\nan evaluation of the SL policy network is available, it permanently stores\nthe output value of the SL policy network.\nThe steps of MCTS then work as follows to update these values 푄, 푁and 푃:\n• Selection. Starting from the root, a successor node is selected until a leaf-\nnode, i.e. a node with non-visited moves is found. At each step during\nselection in a position, the next move is chosen by comparing the sum\n푄+ 푢of the 푄value of that move as well as some bonus 푢. This sum is\ncompared w.r.t. all moves, and the one with the highest value is selected.\nThe bonus 푢is proportional to\n푃\n1+푁7. If we start the search and we do\nnot encounter a leaf-node, we will have visited the node very few times,\nsay once, i.e. 푁= 1. The value 푃is just a constant probability given by\nthe SL policy network. For the sake of the example, suppose this is 0.5.\nThe initial bonus would thus be 0.5\n2 = 0.25. Now let’s say that we have\nvisited the node 99 times. The bonus then decays to 0.5\n100 = 0.005 which is\nneglectable. In other words: Initially we have not run a lot of simulations,\nand therefore trust the SL policy network to pre-select good moves. Once\nwe run more and more simulations, we trust more and more the result of\nthe MCT search and minimize the eﬀect of the SL policy network.\n7The correct formula to compute 푢for some move 푚is actually 푐· 푃·\n√Í\n푚′ 푁푚′\n1+푁푚\n. Here Í\n푚′ 푁푚′\nsums over the node count of all moves available in the current position, and 푐is some empirically\nchosen constant factor that adjusts the eﬀect of 푢\n\n146\n4. MODERN AI APPROACHES - A DEEP DIVE\n• Expansion. In this step, prior probabilities 푃for each possible move are\ncomputed by the SL policy network. Since that takes a lot of time, the\nprior probabilities are ﬁrst computed by the tree policy network, and once\nthey are available from the SL policy network, immediately replaced.\n• Simulation. The leaf node is then evaluated in two kinds of ways: First\nthe value network outputs an expected outcome, say 푣. Then a game is\nplayed by self-play using the rollout policy network, which results in a\nresult, say 푧. These are then summed using a mixing parameter 휆into a\nleaf evaluation value 푉, i.e. 푉= (1 −휆)푣+ 휆푧.\n• Backpropagation. We now go upwards from the leaf node with new eval-\nuation value 푉in the tree up to root. For each edge, i.e. each move, we\nupdate the visit count with 푁= 푁+ 1 by one. Accordingly, the 푄value\nof that move is 푄=\n1\n푁+1푉.\nNow you have some intuition about the calculations that go on during the MCT\nsearch of AlphaGo. There are some challenges which are technical in nature:\nSearching iteratively like this is simply too slow. Since all operations, especially\nthe ones involving the SL policy network and the value network are costly,\nthe MCTS algorithm was heavily parallelized. Computations of the SL policy\nnetwork, the value network, playouts by the rollout policy network, the compu-\ntation of initial probabilities by the tree policy network and MCTS operations on\nthe tree happen in parallel and are aligned with each other. Therefore, MCTS\noperations are slightly more complex in practice. The backpropagation step for\nexample can then be simply extended to increase the visit count w.r.t. all simula-\ntions that passed through that move. The 푄value of a move is then updated by\ntaking the mean w.r.t. all simulations that passed through that move. Unless we\nwant to explicitly implement AlphaGo ourselves, this is of minor importance\nhowever.\n4.1.7\nOdds and Ends\nIn many ways the whole approach taken looks slightly odd. That is not meant\nin any way to criticize the tremendous achievement that AlphaGo constitutes.\nIt looks however similar to a pattern that often occurs when great achievements\n\n4.1. ALPHAGO\n147\nwere made. First one discovers one particular part that works extremely well.\nBased on that part one tries to create a complete solution by trying out various\nthings, and prioritizes solutions based on engineering rather than questioning\nthe whole solution in the ﬁrst place. After making everything work, one can\nthen afterwards re-study the solution, identify strengths and weaknesses and\noptimize and simplify everything.\nWe can only speculate in which order things were discovered. One explanation\ncould be that training the SL policy network by millions of Go positions from\nthe KGS Go server was the ﬁrst step. It was not the ﬁrst time this was tried, but\nthe best previous approach reached an accuracy of 44.4 percent [CS15] whereas\nthe SL policy network of the DeepMind team reached an accuracy of 57 percent.\nThis alone constitutes a major improvement. We can then speculate that based\non this achievement they tried to make MCTS work eﬃciently. The introduction\nof the rollout policy and tree policy networks suggests that the performance of\nMCTS was a major obstacle. This is likely the reason that they implemented\na fully parallel MCTS. They report that the ﬁnal version of AlphaGo used 40\nsearch threads with 48 CPUs (for the search) and 8 GPUs (for the evaluation of\nthe neural networks).\nAlphaGo beat other programs by several magnitudes. The best Go program at\nthe time was CrazyStone by French computer scientist Rémi Coulom. Whereas\nCrazyStone was approximately slightly below 2000 Elo, AlphaGo was just short\nof 3000 Elo points (it crossed the 3000 Elo margin with an even more parallelized\nversion that used 1202 CPUs and 176 GPUs). One should mention however that\nRémi was quick to adapt the neural network based ideas into CrazyStone and\nas of today it is again a very strong Go program.\nAlphaGo also beat European champion Fan Hui in a small match over ten games\nby 8 vs 2. An enhanced version later beat the at the time best Go player Lee\nSedol in a ﬁve game match by 4 to 1. Again, the tremendous achievement by\nthe DeepMind team cannot be overstated.\nStill we list some oddities which serve as a good reason to look into the even\nmore tremendous achievement that was AlphaGoZero, described in the next\nSection.\n\n148\n4. MODERN AI APPROACHES - A DEEP DIVE\n• There are no more than ﬁve(!) diﬀerent neural networks involved. The RL\npolicy isn’t even used directly, it just serves as an intermediary to train the\nvalue network. Moreover the SL policy network, the RL policy network\nand the value network share almost the same architecture (with the ex-\nception of the output layer), yet they are trained completely independent.\nCan this be streamlined?\n• Oddly, the SL policy network is used to create the prior probabilities\nduring MCTS expansion. One would expect that the stronger RL policy\nnetwork is used for that. However the RL policy network was actually\nweaker than the SL policy network. The DeepMind team hypothesized\nthat it is presumably because humans select a diverse beam of promising moves,\nwhereas RL optimizes for the single best move. Nevertheless they report that\nthe RL policy network indeed was stronger than the SL policy network\nwhen training the value network. So the reinforcement learning of the SL\npolicy network to get the RL policy network seemed to be worthwhile.\n• For simulation, both the value network and the fast rollout policy are used\nto get an evaluation for the position. Oddly, none of them alone reaches\neven closely the 3000 Elo level of everything combined; the value network\nalone reaches roughly 2200 Elo points, and the rollout policy network\nreaches approximately 2400 Elo points.\n• After training all networks, a huge computational power is used to per-\nform MCT search during which much insight is gained w.r.t. the game of\nGo itself. Yet this insight is not made use of for improving the networks,\ninstead the MCT search is re-started (modulo some caching) in each new\nstate of the game. In contrary, the rather universal gradient policy rein-\nforcement learning is used to improve the SL policy network. Surely there\nis potential for utilizing the results of the MCT search somehow.\nIt is clear that the DeepMind team identiﬁed these issues. After all, all those\nissues were addressed and solved by the “zero”-style engines, which are subject\nof the next sections.\n\n4.2. ALPHAGO ZERO\n149\n4.2\nAlphaGo Zero\nIn case you started the chapter at the beginning and slightly struggled with the\nAlphaGo chapter with the ﬁve networks and their diﬀerent training methods,\npolicy gradient reinforcement learning and a complex MCT search, here is some\nword of comfort: It gets easier. Much easier actually.\nSpeaking in terms of metaphors: The breakthrough of AlphaGo can probably\nbe described as building a pyramid by moving huge stones from one point to\nanother by sheer brute force and muscle power. The breakthrough of AlphaGo\nZero is then ﬁguring out that the same can be accomplished much easier by\ninventing the wheel and putting the stones on a cart. The result is the same if\nnot more eﬃcient, but the method is way more elegant.\nIn AlphaGo Zero we have just one single network. The input of the network is\nmuch simpliﬁed, but more on that later. The output of the network combines\nthe policy network(s) and value network of AlphaGo into one single structure.\nAnd instead of employing various diﬀerent kinds of training methods for each\nnetwork as done for AlphaGo, here the training is done by just using the results\nof MCT searches.\n4.2.1\nThe Network\nThe network is a deep residual neural network, i.e. it is a deep convolutional\nneural network with skip connections.\nCompared to the networks used in\nAlphaGo, the use of skip connections is the biggest diﬀerence. Moreover it\ncombines the policy and value network that were used in AlphaGo into one\nsingle network.\nNetwork Input\nThe input of the network is the location of the Black and\nwhite stones of the current and the last seven positions as well as the side\nwhose current turn it is. The last seven positions are required since there are\nrules w.r.t. avoiding repetitions of positions. The positions are always encoded\nfrom the perspective of the player whose turn it is. In Go, Black moves ﬁrst.\nHaving the ﬁrst move is a slight advantage in Go. As mentioned before: To\n\n150\n4. MODERN AI APPROACHES - A DEEP DIVE\nadjust for that there is a rule called komi. When a Go games ﬁnishes, one counts\nthe occupied territory to determine the winner. Komi means that White gets\na few extra points (agreed upon before the start) when counting the territory.\nTherefore it is important to know whether it is Black’s or White’s turn in a given\nposition, because a position might look advantageous to Black but for example\nis just equal or even better for White when we consider komi. Aside from komi\nthough, a Go position is mirror-invariant.\nThe input consists of 17 binary planes; each of size 19 × 19. The ﬁrst plane\nencodes the current position of the stones of the current player. A bit is set\nto 1 if a stone of the player is placed on a point and in all other cases (a stone\nof the other player or empty) the bit is set to 0. The next seven planes encode\nthe positions of the stones of the current player of the last seven positions that\noccurred previously in the game. The next eight planes encode the same for the\nother player. Finally there is one plane with all bits set to 1 if it is Black to play,\nand 0 if it is White to play. Figure 4.6 gives a brief example of this encoding. In\nparticular no Go-speciﬁc features like tactical information are provided as the\ninput. The input really is just the bare minimum to determine the current state\nof the game.\nNetwork Output\nThe output of the network is constructed of two heads, the\npolicy head and the value head.\nThe policy head outputs probabilities of all\npossible 19 × 19 + 1 moves — the one is added here for the option of passing,\nwhich is a legal move in Go. The value head outputs a number in the range\n−1...1 where −1 denotes that the player with the current move loses from this\nposition, whereas 1 denotes that he wins from this position.\nCompared to\nAlphaGo, this network architecture allows to combine the policy and the value\nnetwork that were used in AlphaGo into a single network. The beneﬁt w.r.t. the\ncomputational eﬀort is obvious.\nNetwork Architecture\nThe overall network architecture is depicted in Fig-\nure 4.7. Again, it is interesting to compare the architecture to the one used in\nAlphaGo. Besides the two heads for the output of the move probabilities and\nthe value, the biggest change is the heavy use of residual layers (skip connec-\n\n\n152\n4. MODERN AI APPROACHES - A DEEP DIVE\ntions) compared to just using convolutional layers alone. As we have discussed\nin Section 2.12, residual layers can help to train a deep network much more\neﬃciently which is apparently the case here. The feature inputs are initially fed\nto a convolutional layer with 256 ﬁlters, a 3 × 3 kernel and stride 1, followed by\nbatch normalization and rectiﬁer nonlinearity. Next there is a chain of residual\nblocks. The DeepMind team experimented both with a smaller network of 19\nresidual blocks and a larger network of 39 residual blocks. Each residual block\nconsists of two convolutional layers with 256 ﬁlters, a 3 × 3 kernel and stride 1\nfollowed by batch normalization and rectiﬁer nonlinearity. Each residual block\nalso features a skip connection that connects the input of the residual block\ndirectly to the output.\nAfter these residual blocks, the output is fed into two heads. The policy head\nfeatures one convolutional layer with two ﬁlters, a 1 × 1 kernel and stride 1 with\nbatch normalization and rectiﬁer nonlinearity followed by one fully connected\nlayer with 19 × 19 + 1 outputs. The outputs correspond to the probabilities for\neach possible move on the Go board including the pass move.\nThe value head starts with one convolutional layer with one ﬁlter, a 1×1 kernel and\nstride 1 followed by — surprise, surprise — batch normalization and rectiﬁer\nnonlinearity. This is then fed into a fully connected layer that has 256 outputs,\nfollowed by rectiﬁer nonlinearity. Finally these outputs are connected to the\ninputs of one ﬁnal fully connected layer with a single output and tanh activation\nwhich provides the value in the range −1 . . . 1.\n4.2.2\nMCTS and Reinforcement Learning\nNow suppose we already have ﬁnished training our network and are about to\nplay a game of Go. We will now use MCTS to ﬁnd the best next move in a\ngiven position. The MCTS approach is similar to the one used in AlphaGo but\nnaturally much simpler since we have just one single network. Very interesting\nhere to note is that no random rollouts are used in the simulation step, we just\nemploy the network for the evaluation of leaf nodes. This is a major diﬀerence\ncompared to classical MCTS. Let’s discuss the four steps, namely selection, ex-\npansion, simulation and backpropagation. As before in AlphaGo we operate on a\n\n\n154\n4. MODERN AI APPROACHES - A DEEP DIVE\nsearch tree where nodes correspond to Go positions; and edges that connect\nnodes correspond to moves. Each edge stores four values, the move value 푄,\nsome helper variable 푊that stores the sum of move values to support the com-\nputation of 푄, the visit count 푁, and a prior probability 푃. These values are\ninitialized as 푄= 푊= 푁= 0, and an evaluation of the current position with\nthe network provides the initial probability value for 푃.\n• Selection. We start at the root and at each step choose the next node\nby taking the one that maximizes 푄+ 푢. Given a move 푚, the value 푢\nis computed as 푐· 푃·\n√Í\n푚′ 푁푚′\n1+푁푚\n.\nHere 푚is the move that leads to the\nnext node, Í\n푚′ 푁푚′ sums up the node count of all possible moves (i.e.\npractically spoken that’s the node count of the parent), and 푐is some\nconstant that adjusts the impact of the overall bonus value 푢. Similar as in\nAlphaGo, the value 푢ensures that we ﬁrst explore those moves with an\ninitially high prior probability 푃, but later when we have searched more,\nwe focus more on those moves with a high move value 푄— if we revisit\nthe move 푚over and over then 푁푚grows and therefore 푢gets smaller.\nWe continue to select moves until we ﬁnd a leaf node, i.e. a node with a\nposition which has unvisited moves.\n• Expansion.\nThe position of the leaf node is evaluated by the neural\nnetwork to get some evaluation value 푣. The network also provides prob-\nabilities 푝푚for each possible move 푚in the position of the leaf node. Then\nthe leaf node is expanded by adding an edge for each possible move to a\nsuccessor position. Each edge is initialized with values 푄= 푊= 푁= 0\nand the probability 푃is set as 푃= 푝푚.\n• Simulation. There is no explicit evaluation step anymore. The evaluation\nis combined with the expansion step: The computation of the evaluation\nvalue 푣replaces the rollout simulations that we typically do in MCTS. We\nsimply rely on the network for simulation.\n• Backpropagation. Starting from the leaf node, the values 푄, 푊and 푁\nare updated up to root. Each edge that is passed through is updated by\n푊= 푊+ 푣, 푁= 푁+ 1, and 푄= 푊/푁.\n\n4.2. ALPHAGO ZERO\n155\nFinally after the search (time) is over and a move has to be selected, we compute\nthe following policy value for each possible move 푚in the root position\n휋푚=\n푁1/휏\n푚\nÍ\n푛푁1/휏\n푛\nassuming 푛possible moves at the root. The move 푚with the highest value 휋푚\nis then selected. This is almost the same as simply taking the move with the\nhighest visit count, but the value 휏slightly adjusts the level exploration.\nAs always, to get a better intuition of how this all works, let’s consider a small\ntoy example with actual numbers. Consider Figure 4.8. There are only two\nmoves available in the root position. We have to decide whether we take the left\nor right move. For that we compute the value 푈= 푄+ 푢. For the left move we\nhave\n푢= 푐· 푃·\np\n푁left + 푁right\n1 + 푁left\n= 푐· 0.1 ·\n√\n3 + 2\n1 + 3 ≈푐· 0.56\nFor simplicity let’s assume that our constant factor 푐that adjusts the impact of\n푢to be simply set to 1. Then we have 푈= 푄+ 푢= 0.2 + 0.56 = 0.76. For the\nright move we obtain\n푈= 푄+ 푢= 0.4 + 1.0 · 0.7 ·\n√\n3 + 2\n1 + 2 ≈0.4 + 0.74 = 1.14\nSince 0.56 < 1.14 we select the right move. Now we continue and have to again\ncompare the left and right move. For the left move we get\n푈= 푄+ 푢= 0.5 + 0.6 ·\n√\n1 + 1\n2\n= 0.5 + 0.42 = 0.92\nand for the right move we have\n푈= 푄+ 푢= 0.4 + 0.7 ·\n√\n1 + 1\n2\n= 0.4 + 0.49 = 0.89\nSince 0.92 > 0.89, we take the left move. We are now in a leaf position and have\nto apply expansion and simulation.\n\n156\n4. MODERN AI APPROACHES - A DEEP DIVE\nThe expansion step is shown in Figure 4.9. During expansion we generate all\npossible legal moves in that position and for each move create a successor node.\nMoreover we feed the position of the leaf node into our network and obtain\nmove probabilities. For each legal move we store the corresponding probability\nas prior probability 푃, i.e. associate it with the edge. The remaining parameters\n푁, 푊and 푄are initialized to 0.\nWe already fed the position of the leaf node into the network and not only\nobtained move probabilities but also an evaluation value 푣of that position. The\nsimulation step is just that, i.e. obtaining the value 푣for that position. During\nbackpropagation we use that value 푣to update all values going from the leaf node\nupwards in the tree up to the root note. This is illustrated in Figure 4.10.\nThe ﬁrst edge on that path is updated by adding the value 푣to the accumulated\nvalues 푊, i.e. 푊:= 푊+ 푣= 0.5 + 0.7. Moreover we update the visit count\n푁:= 푁+ 1 = 2, and obtain the new move value 푄:= 푊\n푁= 1.2\n2 = 0.6. We follow\nto update the next edge above with 푊= 0.8 + 0.7 = 1.5, 푁= 2 + 1 = 3 and\n푄= 1.5\n3 = 0.5. This ﬁnishes one iteration of MCTS.\nNote in particular how the comparatively good position evaluation of the leaf\nnode 푣= 0.7 aﬀects the move value of the upper layers, i.e. how the evaluation\nof the right move just below root is changed from 푄= 0.3 to 푄= 0.5.\nNow we’ve seen how MCTS works when a powerful neural network is available\nto provide move probabilities and a position evaluation. But how do we train\nthe network in the ﬁrst place?\nNow here comes the real magic that makes\nAlphaGo Zero (and all derivatives) so elegant and beautiful: We just randomly\ninstantiate the network, self-play games and then use the results of MCTS to\ntrain and improve the network. Then we continue.\nLet’s get down to the details. It works like this: First we self-play a game with\nMCTS. To compute the values during MCTS, we employ the current generation\nof the network.\nThe self-played game will provide us with a sequence of\npositions and moves until the game ends with a win for Black or White. Let’s\ncall this result 푟. In each step of the MCT search we computed probabilities for\nall moves in the position. Note that these move probabilities are not just the\nresult of one evaluation of the network (these are the prior probabilities 푃) but\n\n\n\n4.2. ALPHAGO ZERO\n159\nexpected result of the game and 푣the output value of the network, and Θ be\nthe weights of the network. The overall loss of the network is given by\nloss = (푧−푣)2 −휋휋휋푇log 푝푝푝+ 푐∥Θ∥2\nHere 푐is a regularization parameter. Regularization is just some technique to\navoid overﬁtting; we’ll ignore the details here.\nThen there is of course the question how to organize the self play. We could\nimmediately replace the old network by the improved network after just one\nself-played game. In AlphaGo Zero a slightly diﬀerent approach was used. First\na number of games are played with the current network. For these games the\nparameter 휏is set to 1 for the ﬁrst moves in order to make sure that it is not\nthe case that the same positions are played over and over again and diﬀerent\npositions are generated in the opening phase. Later 휏is set to almost 0 to get the\nbest moves according to the MCT search. After a number of games are played,\nthe old and the new network are compared against each other by playing a\nnumber of games. Only if the new network convincingly beats the old network,\nthe new network will replace the old network.\nThis ensures that we make\nconstant progress and are not stuck in a local minimum during training. The\nreported numbers are 25000 games to train one iteration of a network, 400 games\nbetween the old and the new network to compare the performance between the\ntwo, and a winning threshold of 55 percent that decides if the new network is\nreally stronger than the old one.\nIf you take a look at the simulation step you might wonder why we need to\ncreate a network that outputs move probabilities. After all, during evaluation\nonly the value output 푣is really used to update the 푄values which eventually\ndecide which move we take. But make no mistake: The prior probabilities\nthat are generated during the expansion step are equally important in directing\nthe MCT search! It is these probabilities that make sure that MCTS explores\nthe most promising and strongest moves and does not waste time on legal but\nweak moves.\nIt is precisely this approach of ruling out bad moves by just\nassessing the position without any tactical calculation of the search tree that\nmakes AlphaGo Zero so human-like in the approach. Professional players are\nstrong at tactics, but what’s even more important is that they know when to\n\n\n4.2. ALPHAGO ZERO\n161\nWhat is more eﬃcient and leads to a stronger engine, using positions sampled\nfrom real games of strong professional players, or training by reinforcement\nlearning using self-play as done for AlphaGo Zero. For that, the DeepMind\nteam generated a network with a similar architecture based on residual blocks\nand trained it by position samples from expert games from the KGS Go server.\nThen they compared the performance of these two networks using the same\nMCTS approach for ﬁnding the best move in a given position.\nWe will return to the pros and cons of supervised vs reinforcement learning in\nthe context of chess in Chapter 5, but for now let’s recapitulate the ﬁndings by\nDeepMind:\nThere are three very interesting facts to note.\nThe ﬁrst is that initially the\nsupervised network gained a high rating very quickly, beating the network\ntrained by reinforcement learning. After about 20 hours of training time on\ntheir hardware however the network trained by reinforcement learning became\nstrictly stronger than the supervised one. After about 40 hours of training, the\nsupervised network reached a plateau at a rating level of about 3500 Elo. The\nnetwork trained by reinforcement learning continued to improve and maxed\nout at a rating level of over 4000 Elo.9\nSecond the networks ability to predict outcomes of games was tested. To test\ntheir performance, the networks were fed positions from a dataset of games of\nprofessional Go players. As before, initially the network trained by supervised\nlearning was better at predicting the outcome of games, but after about 15 hours\nof training, the network trained by reinforcement learning was strictly better at\npredicting the outcome of games on that dataset.\nNow for the third, somewhat surprising fact. The networks were also tested on\nthe question: Which one is better at predicting expert moves? Namely given a\nGo position from this database of expert games, the task was to predict which\nmove was played by the expert in that position. And it turns out that the net-\n9We should note that with the larger network with 40 blocks AlphaGo Zero even reached a\npeak rating of 5185 Elo. An interesting side node is that this large 40 block network reached\nan Elo rating of 3055 without any kind of search, i.e. just by selecting the move with the highest\nprobability in a given position.\n\n162\n4. MODERN AI APPROACHES - A DEEP DIVE\nwork trained by reinforcement learning was strictly worse than the network\ntrained by supervised learning for any given time of training. How can this be\nif the network trained by reinforcement learning is stronger by 500 Elo com-\npared to the one trained by supervised learning? Well it looks as if the network\ntrained by reinforcement learning gained superhuman strength. It apparently\ndiscovered patterns during self-play that humans do not understand. It there-\nfore predicts the best move according to its superhuman understanding and\nstrategy in a given position and not the weaker move that was played by the ex-\npert player. This is a fascinating discovery and strong indication that computer\nGo has passed a signiﬁcant milestone. From a computer scientist’s view this is\nprobably even more proof that AlphaGo Zero has superhuman strength than\nthe demonstration matches against Lee Sedol. Although they were of course\nmuch more spectacular than some diagram in a paper.\nExcept maybe for computer scientists.\n4.3\nAlphaZero\nAlphaZero is essentially AlphaGo Zero adapted to chess and Shogi plus some\nslight simpliﬁcations for the training pipeline. The general approach, i.e. the\nnetwork structure and training pipeline is mostly the same. Due to its impact\nin the chess world this section nevertheless tries to describe everything in full\ndetail with the risk of some repetitions.\nThe idea is that you can read this\nsection mostly on its own with some occasional peeking into the section about\nAlphaGo Zero.\nLet’s start with a brief overview to get an intuition how AlphaZero works:\n• There is one large and deep neural neural network.\nThe input to the\nnetwork are chess (resp. Shogi or Go) positions. The output is twofold:\nFirst probabilities for every possible chess (resp. Shogi or Go) move are\noutput. These indicate how good a move is in that position. Second, a\nvalue is output for that position. This value indicates how likely it is that\nWhite wins, Black wins or that there is a draw.\n• Once the network is trained the engine works by using MCT search to ﬁnd\n\n4.3. ALPHAZERO\n163\nthe best move in a given position. MCTS diﬀers from textbook MCTS in\nthat it is guided by the network: during selection the network is queried\nfor move probabilities in a position. Child nodes are then selected w.r.t.\nthe moves with the highest probabilities.10 Second, for simulation there\nare no random playouts. Instead the network is queried with the position\nof the node, and the value that the network outputs is employed instead\nof using results of random playouts.\n• The network itself is also trained using MCTS. A number of games are\nself-played by the engine. For each move an MCT search is conducted\nusing the current state of the network. The MCT search results in a more\naccurate assessments of a position w.r.t. the evaluation value and best\nmoves. Positions of these games together with the results of the MCT\nsearch for that position as well as the ﬁnal outcome of the played game\nare then used as training data for the network.\nAs mentioned, the approach is very similar to AlphaGo Zero.\nThere are a\nnumber of diﬀerences though.\nThe rules for chess and Shogi are far more\ncomplex than for Go. Not only are there several diﬀerent pieces but the rules on\nhow they move are also more involved. It is therefore in particular interesting to\ntake a look at the input encoding of a chess or Shogi position as well as the output\n(encoding) of the network. The other major diﬀerence is some simpliﬁcation in\nthe training pipeline.\nNetwork Input\nThere are numerous ways to encode a chess position into a\nsequence of bits.\nProbably every chess player is for example familiar with\nForsyth-Edwards-Notation (FEN). One could simply take the bit string from\nthe ASCII values of the letters of the FEN string. This however is extremely\nineﬃcient. Not only has the network to learn chess knowledge — i.e. who is\ngoing to win and which moves are best — but also learn to decode the position\nﬁrst. Deﬁning an encoding such that the network does not have this burden is\nthus quite important. The following approach was used by the DeepMind team\n10Here, some exploration factor is employed such that the MCTS does not solely rely on the\nnetwork for selection, especially in the initial phase.\n\n164\n4. MODERN AI APPROACHES - A DEEP DIVE\nfor AlphaZero, even though they mentioned that the overall approach worked\nalso well for other reasonable encodings.\nThe basic building block of the encoding is a plane of size 8 × 8, similar as\ndone for Go. There are 119 planes used in AlphaZero to encode all necessary\ninformation about the game state for chess. Each plane consists of only bits, i.e.\nzeros and ones. Several planes are required to encode one position:\n• Six planes are required to encode the position of the pieces of the white\nplayer, i.e. one plane for the position of the white pawns, the white rooks,\nthe white knights, bishops, queens and the king. In a plane a bit is set to\none if there is a piece of that type on the square, and set to zero otherwise.\nFor example if White has pawns on d4 and e4, the plane for the white\npawns is set to 1 at indices (3, 3) and (4, 3). Here we start counting from\nzero and the ﬁrst index refers to the ﬁle and the second one to the rank.\n• Another six planes are required to encode the position of the pieces of the\nBlack player.\n• Another two planes are used to record the number of repetitions of the\npositions.\nThe ﬁrst plane is set to all ones if a position has occurred\nonce before and to zeros otherwise, and the second plane serves a similar\npurpose to encode if a position has occurred twice before.\n• In order to cope with (i.e. detect possible forced draws) three-fold rep-\netition the above mentioned planes for the eight previous positions are\nencoded as well. If the game just started, these history planes are simply\nall set to zeros. Of course creating a history of previous positions can also\nhelp to detect patterns during training.\nThis basically encodes the most important aspects of a game state in chess.\nThere are however a few more features required. There is one additional plane\nthat simply encodes the color of the player whose turn it is, i.e. a plane of just\nones or zeros if it is White resp. Black to move. Another four planes are used to\nencode castling rights: One for white-kingside castles, one for white-qeenside\ncastles, one one for black-kingside castles, and ﬁnally one for black-queenside\ncastles. These planes are set to all ones if the right to castling exists, and to zeros\n\n4.3. ALPHAZERO\n165\notherwise.\nFinally we need two counters. One counter encodes the total move count11 and\none counter is for progress. The latter counts the number of moves where no\nprogress has been made, i.e. no capture has been made and no pawn has been\nmoved (the 50 moves rule). The counters are given directly as numbers as a\n(real valued) input to the network.\nAn example of the input encoding for the six planes that encode the position\nof the pieces is given in Figure 4.11. The position is from Capablanca vs Corzo,\nHavana 1901 and one of many interesting endgames of Capablanca.\nAs mentioned, the same approach was used for Go and Shogi.\nThe input\nencoding for Go is straight-forward (one plane for the black stones, one plane\nfor the white stones, and one plane to encode the color of the player to move),\nand the input encoding for Shogi follows a similar approach as used for chess.\nNetwork Output\nThe output for the evaluation value is simple. Just output\nthe value. The challenge is the output for the move probabilities. After all,\ndepending on the position there are diﬀerent numbers of possible legal moves.\nOn the other hand we need a constant number of outputs for our network.\nThe general idea to cope with that is to consider every possible source square and\nthen enumerate all possible moves of a “superpiece” that can move like a queen\nand a knight. For each of these possible moves we create one output. Naturally\nthere will be outputs that have a positive probability but that correspond to\nillegal moves. One can cope with that by simply setting these move probabilities\nto zero and re-compute the remaining probabilities such that their sum equals\none.\nTo illustrate the approach let’s have a look at Table 4.1. We have three columns,\none column for the source position, one column for the direction in which the\npiece moves, and one column for the number of squares that the piece moves.\nEach row corresponds to one output of the network.\n11It is not clear why the total move count is required. This feature is e.g. omitted in Leela Chess\nZero.\n\n\n4.3. ALPHAZERO\n167\nTable 4.1: Output Move Encoding - \"Queen-like\" Moves\nSource Square\nDirection\nNumber of Squares\na1\nUp\n7\na1\nUp\n6\n...\n...\n...\na1\nUp\n1\na1\nUp Right\n7\n...\n...\n...\na1\nUp Right\n1\n...\n...\n...\n...\n...\n...\nh8\nDown Left\n1\nWe start by considering a piece at position a1 that moves upwards seven squares.\nThis corresponds to the move a1a8 for whatever piece was on a1. The next\noutput of the network is the same except that the piece moves now only six\nsquares.\nThis corresponds to the move a1a7.\nWe continue like that where\nwe enumerate all possibilities for all potential source squares, directions and\nnumber of squares until we have encoded all potential (queen-like) moves.\nHow about knight moves? We follow the same approach to encode potential\nknight moves, i.e. for each combination of source square, and the eight potential\nknight moves (two up and right, two right and up, two right and down, two\ndown and right, two down and left, two left and down, two left and up, two up\nand left)12 we create one output. The approach is illustrated in Table 4.2.\nLast we need to care about potential underpromotions. Promotions can occur\n12A quite unknown fact is that there is an additional chess rule in Japan that says that you get\nan extra queen if you move your knight up, up, down, down, left, right, left, right, on a square\non the b-ﬁle and then on a square on the a-ﬁle.\n\n168\n4. MODERN AI APPROACHES - A DEEP DIVE\nwhen a pawn moves to the eighth (ﬁrst) rank, or if a pawn captures a piece\nupper left or upper right at the eighth rank resp. ﬁrst rank. This is is depicted\nin Table 4.3. Any combination of source square, move type (advance, capture\nto the left or capture to the right) as well as the promotion’s piece type are\nconsidered. This is again ineﬃcient since a pawn promotion from a1 can never\nhappen — but it makes sense to simply keep the 8푥8 structure for the simplicity\nof implementation. Pawn moves that queen and are encoded in the outputs for\nqueen-like moves are assumed to promote to a queen.\nTable 4.2: Output Move Encoding - \"Knight-like\" Moves\nSource Square\nDirections\na1\ntwo up, one right\na1\ntwo right, one up\n...\n...\na1\ntwo up, one left\na2\ntwo up, one right\n...\n...\na1\ntwo up, one left\n...\n...\n...\n...\nh8\ntwo up, one right\nThis all looks slightly cumbersome. After all, we create outputs for potential\nmoves that can never happen. If there is a pawn on a7, then no matter what\npiece there is on a7, a move Qa1a8 is not possible, so a positive output of the\nnetwork has no meaning. If there is a rook on a1, a move from a1 in the upper\nright direction can never happen. However the encoding ensures that each\npotential move corresponds to one speciﬁc output of the network. After all, it is\ncompletely irrelevant which piece is actually placed on a square. As mentioned,\nwe can simply mask out illegal outputs to zero. Given training positions and\n\n4.3. ALPHAZERO\n169\ncorresponding expected move probabilities where illegal moves are masked, the\nnetwork will learn by itself that it should not try to do illegal moves. And if the\nnetwork outputs a probability greater than zero for an illegal move during play,\nwe can simply set this probability to zero and and re-calculate the probabilities\nsuch that the probabilities of the legal moves add up to one.\nTable 4.3: Output Move Encoding - Underpromotions\nSource Square\nMove Type\nPromotes To...\na1\nadvance\nknight\na1\nadvance\nbishop\na1\nadvance\nrook\n...\n...\n...\na2\nadvance\nknight\na2\nadvance\nbishop\na2\nadvance\nrook\na2\ncapture left\nknight\na2\ncapture left\nbishop\na2\ncapture left\nrook\na2\ncapture right\nknight\na2\ncapture right\nbishop\na2\ncapture right\nrook\n...\n...\n...\nh8\ncapture right\nrook\nThe last task is to calculate how many outputs we actually get if we follow this\nencoding approach. For the ﬁrst table we have 64 potential source positions,\neight possible (queen) directions and seven possible number of squares that a\npiece moves, i.e. 64 × 8 × 7 = 3584 outputs. For the knight-like moves we have\n64 potential source positions and for each source position eight possible knight\nmoves, i.e. 64 × 8 = 512 outputs. Last for the underpromotions we have 64\n\n170\n4. MODERN AI APPROACHES - A DEEP DIVE\npotential source squares (even though technically only 16 are legally possible\nsince a pawn must be on the second (Black) or seventh (White) rank to queen\nwith the next move), three possible move directions (advance the pawn, capture\nto the left or capture to the right) and three possible underpromotion piece types\n(knight, bishop and rook). Thus we have 64 × 3 × 3 = 576 outputs. All in all\nthere are thus 3584 + 512 + 576 = 4672 outputs. Again, this illustrates the sheer\nsize and required computational eﬀort to train such a network.\nWhy such a complicated output structure?\nAn alternative output structure\nwould be to simply enumerate all possible moves, i.e. a1b1, a1c1, a1d1 and so\non. In addition to enumerating all combinations of source-square and to-square\nwe need some additional outputs for promotions; for example we need both\na7a8 for a normal piece to move from a7 to a8 as well as a7a8q for a pawn that\nqueens. This would give less than 2000 outputs, and is also referred to as a “ﬂat\ndistribution” in the AlphaZero paper [SHS+17, SHS+18].\nCompared to that, the approach used for AlphaZero is very source-square\ncentric, i.e. the network might be able to train faster by roughly guessing which\npiece to move in a position (i.e. which source-square to select), and only then\nreﬁne with more training batches where to actually place that piece. And indeed,\naccording to the AlphaZero creators they tried both encoding styles and both\nworked, but the slightly more complicated one with the 4672 outputs trained\nfaster.\nNote that AlphaZero used a similar encoding for Shogi, whereas the outputs\nfor Go were encoded as a ﬂat output just enumerating all legal moves. But\nremember that moves in Go are of trivial nature, it just means placing a stone\non a point — there is no source and target square, and there are no diﬀerent\npieces like in chess or Shogi but just stones.\n4.3.1\nNetwork Architecture\nNot much to see here. Except for the network input and output — i.e. the input\nvector that encodes a chess or Shogi position and the encoding of all possible\nmoves — the network architecture follows precisely the one used for AlphaGo\nZero based on resnets. For their experiments, the DeepMind team used the\n\n4.3. ALPHAZERO\n171\n“small’ version with 19 residual blocks. For an overview of the architecture cf.\nFigure 4.7.\n4.3.2\nMCTS and Reinforcement Learning\nThe approach used for MCTS combined with reinforcement learning is essen-\ntially the same as the approach for AlphaGo Zero. The major diﬀerence is a\nslightly diﬀerent training pipeline. All the details are already described in Sec-\ntion 4.2, one just needs to think of chess positions and chess moves instead of Go\npositions and Go moves. Therefore we only brieﬂy recapitulate the approach\nhere.\nAssume we have a trained network that, given a chess position, outputs move\nprobabilities that denote how good a move is and also outputs an evaluation\nvalue in the range−1, . . . , 1 that denotes whether the game from the perspective\nof the current player with perfect play of both sides is going to be a loss, draw\nor a win. Then given a position, MCTS is used to ﬁnd the best move in that\nposition. Here MCTS diﬀers from textbook MCTS-UCT:\n• During selection we don’t use the standard UCT value computation. In-\nstead in each step, the network is queried to get move probabilities for that\nposition. The child node with the best move probability is selected. There\nis also some small correction factor added such that MCTS also explores\nmoves that are currently deemed good but not the best moves. This is\nin order to facilitate exploration and not to miss an excellent move that is\ncurrently not evaluated as the best move by the network.\n• During simulation we don’t play random games at all to compute an eval-\nuation value. Instead the network is queried to get the evaluation value.\nThis value is used in the subsequent backpropagation.\nAgain, this is absolutely the same procedure as the one for AlphaGo Zero.\nThe training pipeline is also almost the same as the one used in AlphaGo Zero.\nInitially we start with random values for the weights of the neural network.\nThen we generate training data by using MCTS with the current network to\nself-play games. In each step of the game, MCTS is used to ﬁnd the best moves\n\n172\n4. MODERN AI APPROACHES - A DEEP DIVE\nand get move probabilities (they used 800 simulations in each MCTS step). Note\nthat these move probabilities diﬀer from the move probabilities of the network,\nsince the search will generate more knowledge about the position than what the\nnetwork currently has. When the game ﬁnishes we also have a result.\nTo illustrate this let’s assume a self-played game generated the moves:\n1.e4 e5 2.f4 exf4 3.Bc4 Qh4+ 4.Kf1 b5 5.Bxb5 Nf6 6.Nf3 Qh6 7.d3 Nh5 8.Nh4 Qg5\n9.Nf5 c6 10.g4 Nf6 11.Rg1 cxb5 12.h4 Qg6 13.h5 Qg5 14.Qf3 Ng8 15.Bxf4 Qf6\n16.Nc3 Bc5 17.Nd5 Qxb2 18.Bd6 Bxg1 19. e5 Qxa1+ 20. Ke2 Na6 21.Nxg7+ Kd8\n22.Qf6+ Nxf6 23.Be7# 1-0\nYes, this is the famous game Anderssen vs Kieseritzky, the “Immortal Game”\nand yes, it is highly unlikely that a self-played game with the network will\ngenerate precisely this sequence of moves. Let’s assume this for the sake of\nargument. For each step, we have a triple of (current position, move probabilities\ngenerated by MCT search, game result). For example we could have:\n1. (initial position, [푒4 : 0.4, 푑4 : 0.4, 푐4 : 0.1, . . .], 1 −0 )\n2. (position after 1.e4, [푒5 : 0.3, 푐5 : 0.3, 푒6 : 0.2, . . .], 1 −0 )\n3. (. . ., . . ., . . .], 1 −0 )\n4. (position after 22...Nxf6, [퐵푒7 : 0.99, 푁푥푓6 : 0.01, . . .], 1 −0)\nBut these are exactly the training data for our network! The input of the network\nis the position of the game, and the expected result is the vector of move\nprobabilities and the result of the game as the expected evaluation value. As for\nthe loss function, network and training: Again I refer to Chapter 4.2.2 since it’s\nall the same. What changes really is only that we have chess positions and chess\nmoves instead of Go positions and Go moves, and that there is the possibility\nof a draw as an outcome of the game. Same for Shogi by the way.\nAs already hinted in the introduction of this chapter: The major diﬀerence\ncompared to AlphaGo Zero is that the training there was done with several\nspeciﬁc iterations. First a network was ﬁxed as the current network. Then games\nwere self-played using MCTS and the current network was used to support\nthe MCT search and generate training data as mentioned above. The current\n\n4.3. ALPHAZERO\n173\nnetwork was then trained with this data to generate a new network. This newly\ngenerated network was then compared with the current network. Only if the\nnewly generated network could beat the current network in a number of games\nwith a winning ratio of 55 percent or higher, the current network was replaced\nwith this newly generated network, and the cycle was repeated. This was to\nensure that we are not stuck in a local minimum and make steady progress\nduring learning.\nIt turns out however that this is apparently not necessary. Alpha Zero was\ntrained by continously updating the network and immediately using the up-\ndated network without any kind of evaluation, e.g. by matches of the old and\nnewly generated network. Just to clarify: Learning is still done with batches of\npositions of course.\n4.3.3\nOdds and Ends\nAlphaZero builds upon the success of AlphaGo and AlphaGo Zero. It not only\nsimpliﬁes a lot of the complexity of AlphaGo but is also an evolutionary step as\nit contains the very much simpliﬁed training pipeline of AlphaGo Zero. This\nshould not be misunderstood in the sense as that implementing this training\npipeline is absolutely easy — while it still involves some programming eﬀort, it\nespecially also still requires huge computational resources. Nevertheless we will\nimplement a (computationally) simpliﬁed version of AlphaZero in Chapter 5.\nNevertheless from an algorithmic level, everything is very much simpliﬁed. For\nexample they completely eliminated not only the need for supervised learning\n(i.e. the need of a huge training data set, which is often not available) but also\nfor policy gradient reinforcement learning.\nEspecially if you managed to survive all the math details about networks, and\nmaybe even the complicated chapter about AlphaGo, you might very likely be\nable to appreciate this simplicity and elegance.\nOne aspect that should be stressed here is that AlphaZero is very much oblivious\nto the speciﬁc rules of a game. In AlphaGo Zero some speciﬁc properties of the\ngame (the symmetry of positions to generate more training data) were utilized.\n\n174\n4. MODERN AI APPROACHES - A DEEP DIVE\nThis is not the case with AlphaZero; they even tried out Go without exploiting\nthe symmetry of positions. Except for the speciﬁc rules of a game that must be\nutilized for move generation during MCT search and the design of the network\ninput and output, AlphaZero is more like a framework to solve any kind of\ndeterministic two player game. It really does not matter whether it is Go, Shogi,\nchess, Xiangqi, Reversi, Othello, Amazons or any other kind of such board\ngame.\nWhat is also remarkable is the huge diﬀerence in the approach compared to\nalpha-beta searchers: We have a very narrow search tree where only a fraction of\nall possible legal moves are actually explored. Moreover compared to alpha-beta\nsearchers an almost insane amount of time is spent on evaluation rather than on\nsearching. Whereas alpha-beta searchers search millions of positions employing\na very fast evaluation function, Alpha Zero searches only a fraction of those\npositions, since querying the network takes a lot of time and computational\nresources.\nThis very much resembles how humans play chess. A huge amount of time\nis spent on evaluating positions. This capability of being able to evaluate a\nposition without calculating much tactics, i.e. just having a “feeling” for the\nposition and grasping its positional nuances is probably the most diﬃcult task\nto learn for a chess player.\nThere are very scholarly books about that even\nfor club players like “Reassess Your Chess” [Sil10] or “Chess Strategy for Club\nPlayers” [Gro09]. Such books help, but still lots of coaches are convinced that the\nonly way to get that chess understanding is to play lots of serious tournament\ngames. And that’s precisely what AlphaZero does to get good, mimicking the\nhuman approach to chess.\n4.4\nLeela Chess Zero (Lc0)\nIt is December 2017 and DeepMind released the AlphaZero preprint on the\ninternet, including all those sample games against Stockﬁsh. The whole chess\nand computer science crowd is excited - but DeepMind does not release Alp-\nhaZero to the public. Everyone wants to actually use these new technologies.\nThe magic happened. We know that it works. But nobody can use it. What to\n\n4.4. LEELA CHESS ZERO (LC0)\n175\ndo?\nThere were immediate eﬀorts to reproduce the results for Go after AlphaGo\nZero: Gian-Carlo Pascutto started Leela Zero, a distributed eﬀort to get a strong\nGo engine released under an open-source license. This code was then ported\nand adapted to chess by several volunteers. The result was LeelaChess Zero, or\nshort Lc0. This made an AlphaZero-like engine accessible to everyone. The port\ndid not follow AlphaZero in every detail, taking freedom in changing things if\nthey did not ﬁt or if better approaches were found. Let’s have a look under the\nhood and see how Lc0 works.\n4.4.1\nThe Network\nNetwork Input\nThe input encoding follows the approach taken for AlphaZero.\nThe main diﬀerence is that the move count is no longer encoded — it is techni-\ncally not required since it’s just some superﬂuous extra-information. We should\nalso mention that Leela Chess Zero is an ongoing project, and naturally improve-\nments and code changes happen. The input format was subject to such changes\nas well, for example to cope with chess variants such as Chess960 or Armaged-\ndon, or simply to experiment with encodings. The encoding described here is\nthe classic encoding, referred to in source code as INPUT_CLASSICAL_112_PLANE.\nFor those who want to look up things in code, the relevant source ﬁles are\nlc0/src/neural/encoder.cc and lc0/src/neural/encoder_test.cc.\nThe input consists of 112 planes of size 8 × 8. Information w.r.t. the placement\nof pieces is encoded from the perspective of the player whose current turn it\nis.\nAssume that we take that player’s perspective.\nThe ﬁrst plane encodes\nthe position of our own pawns. The second plane encodes the position of our\nknights, then our bishops, rooks, queens and ﬁnally the king. Starting from\nplane 6 we encode the position of the enemy’s pawns, then knights, bishops,\nrooks, queens and the enemy’s king. Plane 12 is set to all ones if one or more\nrepetitions occurred.\nThese 12 planes are repeated to encode not only the current position, but also\nthe seven previous ones.\nPlanes 104 to 107 are set to 1 if White can castle\nqueenside, White can castle kingside, Black can castle queenside and Black can\n\n176\n4. MODERN AI APPROACHES - A DEEP DIVE\ncastle kingside (in that order). Plane 108 is set to all ones if it is Black’s turn and\nto 0 otherwise. Plane 109 encodes the number of moves where no capture has\nbeen made and no pawn has been moved, i.e. the 50 moves rule. Plane 110 used\nto be a move counter, but is simply set to always 0 in current generations of Lc0.\nLast, plane 111 is set to all ones. This is, as previously mentioned, to help the\nnetwork detect the edge of the board when using convolutional ﬁlters.\nNetwork Output\nOriginally Lc0 used a ﬂat distribution over all possible\nmoves13, i.e. start with a1b1, a1c1 and continue up to h8g8 plus the moves\npromoting a pawn. This results in 1858 outputs. Later, a bug in the network\nstructure was found. Before adding the fully connected output layer, the policy\nhead of the network convolved the last residual output into an output of “only”\n128 neurons.14 These 128 neurons were then fully connected to the 1858 net-\nwork outputs. Clearly there is an information bottleneck — such bugs can easily\nbe overlooked — as the number of 128 intermediate neurons is just very low.\nConsequently, the network output was changed and the structure was changed\nto one similar to AlphaZero15.\n4.4.2\nNetwork Architecture\nLc0 is an ongoing project and therefore there is no “ﬁxed and ﬁnal” architec-\nture.\nInstead the architecture is subject to changes and optimizations.\nThe\narchitecture described here is the current one (as of 2021), but we also point out\ndiﬀerences to the architecture previously used.\nEssentially, the network architecture follows closely to the one used for Al-\nphaZero, but replaces the ResNet building blocks by Squeeze and Excitation\nblocks. Also, as was the case for AlphaZero, the network is characterized by\nhow many of these blocks are used, and also how many ﬁlters are used in the\nconvolutional layers. Typical combinations of the number of blocks and ﬁlters\nare 10 blocks with 128 ﬁlters, 20 blocks with 256 ﬁlters and 24 blocks with 320\nﬁlters.\n13cf. the array in “decode_training.py” where training data is decoded for debugging purposes\n14https://github.com/glinscott/leela-chess/issues/47\n15https://lczero.org/blog/2019/07/end-of-era\n\n4.4. LEELA CHESS ZERO (LC0)\n177\nThe architecture is shown in Figure 4.12. The input features are passed to one\ninitial convolutional layer. Then, the main building blocks follow. One block\nconsists of two convolutional layers, followed by a Squeeze and Excitation block\nand ReLU activation. In case you are wondering why there is no batch normal-\nization: This is employed directly in the convolutional layers. Normalization is\nhere folded into each convolutional layer — a small optimization technique.\nFinally there are three heads. The ﬁrst head is the policy head that outputs move\nprobabilities, consisting of two convolutional layers (no activation function).\nThe output used is the 73 × 8 × 8 = 4672 output move encoding that was also\nused in AlphaZero.\nThe value head consists of a fully connected layer and softmax activation. It\nhas three outputs instead of the single output of AlphaZero. The three outputs\nexplicitly model the probability of winning, drawing and loosing.\nRecently a third head has been introduced, dubbed the moves left head. This head\nconsists of a convolutional layer, followed twice by a fully connected one with\nReLU activation. The single output predicts the remaining number of moves\nuntil the game ﬁnishes. It is not absolutely clear how and if this is going to be a\nuseful head, but\n• it prevents that random moves are output that do not worsen the position\nand do not lead to a forced draw (i.e. do not violate the 50 moves rule),\nbut also do not lead to the end of the game. In other words this might\nprevent random moves that do not make any progress\n• in tournament controls it might be useful to use this information to allocate\nmore or less calculation time at a given point in time during the game.\nFor example we could identify the opening phase, middle game phase\nand end game phase by the number of moves left and allocate more time\nduring the middle game.\nAs mentioned before this is the current state-of-the-art architecture used by Lc0.\nThe previously used network had a few small diﬀerences:\n• The value head was a fully connected layer that output one value and used\ntanh for activation. This single value denotes the winning probability — as\n\n178\n4. MODERN AI APPROACHES - A DEEP DIVE\ndone for AlphaZero. In other words, there is only one value indicating the\nwinning probability, instead of the triple explicitly modelling the winning,\ndrawing and loss probability as done in the current network architecture.\n• The third head, modelling the number of moves left in the game, does not\nexist.\n• The policy head output used a ﬂat head, i.e. associated one move (source\nsquare plus destination square) with one output, resulting in 1858 outputs\nfor all possible moves.\n4.4.3\nMCTS and Reinforcement Learning\nConceptually the whole training approach is closely modeled after the one used\nfor AlphaZero. The main problem that Lc0 faced and faces is that the DeepMind\nteam had access to a sheer unbelievable amount of computing power to train\nthe network.\nIn order to replicate the results by DeepMind, the authors of Lc0 did a clever\ntrick by giving the computer chess community a chance to contribute to the\neﬀort in a united manner. Interestingly, the resource-intensive part in training\nthe network is not training itself, i.e. feeding in training data and updating its\nweights by backpropagation. Of course this is no easy task either, but can be\nhandled by a reasonable powerful computer system.\nThe diﬃcult part is actually to generate training games. After all, during each\nstep of generating a training game we do not only have to apply MCT search\nexcessively, but also query the network in its current state over and over again.\nTherefore it is precisely this “training by self-play”-part that has been done as a\ncommunity eﬀort.\nFigure 4.13 gives a brief overview.\nThe heart is the network that is hosted\non a central sever instance. The network itself — or rather its weights — is\nthen distributed to volunteers. They use the current instance of the network\nto generate millions of training games using self-play of the current network\ncombined with MCT search; the same approach that was used in the case of\nAlphaZero.\n\n\n\n4.4. LEELA CHESS ZERO (LC0)\n181\nOnce enough self-played games have been generated, the states of these games\n(i.e. the triple of current position, vector of move probabilities as well as the\nresult) are sent back to the central server. This training data is then used to train\nthe current network and improve its weights. The updated network is then\ndistributed to the volunteers and the whole cycle continues.\nAs a practical remark w.r.t. using Lc0: Users of chess engines are accustomed to\nget an evaluation of the current position in pawns. For example a value of 1.00\nmeans White’s position is better to the equivalent of being a pawn up.\nThe current version of Lc0 outputs win, draw and loss probabilities — as neural\nnetwork based chess engines get more and more common, users will get more\nand more accustomed to interpret this diﬀerent kind of output.\nIn the previous version of the network architecture, the value head would output\nonly a value in the range [−1, 1]. Given the output of the value head 푄, this was\nthen converted to a pawn-based evaluation by the empirically chosen formula\nPawns = 111.714640912 ∗tan(1.5620688421 ∗푄)\nThis means an evaluation of Lc0 of 0 (probably a draw) results in a 0.00 pawn-\nbased evaluation. An evaluation of Lc0 of 0.5 (White is clearly better) results\nin 1.11 pawns, and 0.7 (White will likely win this) results in a pawn based\nevaluation 2.15 (more than two pawns up).\n4.4.4\nOdds and Ends\nThe impact of Lc0 should not be underestimated. AlphaZero made a huge\nimpact in the chess (and Go and Shogi) community, but the results could not\nbe independently reproduced. It’s not that the DeepMind team hid anything,\nbut the required computing power was simply too diﬃcult to get for an outside\nparty. Creating the neural network structure, the MCTS implementation and\nthe distributed training framework is a monumental task. As of writing this,\nthe Github repository where the code of Lc0 is hosted counts 1323 commits of\ncode changes. If we assume two hours of work that go into one commit, we are\nat roughly 2600 hours of work. If we further assume an 8 hours work day, that’s\n325 days of work! Or in other terms: In the US there is an average of 1700 hours\n\n182\n4. MODERN AI APPROACHES - A DEEP DIVE\nof work per year per employee, meaning that if we assume a single developer,\nthat’s 1.5 years(!) of work. All done on a volunteer basis for free!16.\nLc0 not only brought the power of neural-network based engines to the masses,\nit also made it available to elite Grandmasters as well. It is apparently used\nheavily by Grandmasters to ﬁnd opening novelties and thus changes indirectly\nthe chess that we play today. All thanks to the Lc0 developers and their volunteer\nwork!\n4.5\nFat Fritz\nFat Fritz is a commercial, neural network based chess engine sold by ChessBase\nGmbH. Before we delve into its inner workings, let’s quickly recapitulate on\nsupervised learning.\nIn the previous chapters you learned about the quite elegant reinforcement\nlearning algorithm discovered by DeepMind. Self-play generated games which\nitself consist of triples\n(푠1,휋휋휋1, 푧1), (푠2,휋휋휋2, 푧2), . . . , (푠푛,휋휋휋푛, 푧푛)\nHere 푠푖is the current chess position of the game, 휋휋휋푖is a vector of probabilities\nfor each move in that position. These are the result of an MCT search of the\ncurrent position based on the current neural network. Last, 푧푖is the result of\nthe game.\nInstead of using this reinforcement learning approach one can of course just use\nexisting chess games, for example games played by strong players, and train the\nnetwork with these games. In that case we of course do not have a vector of\nmove probabilities in each position, but just the move that was played by the\nplayer.\nHowever we can simply treat this as a vector of move probabilities\nwhere we set the probability of the move that was played at 1.0 (i.e. 100 percent\nprobability) and the other moves to 0.0 (i.e. probability 0). In other words if we\n16As an interesting side note, the free tool git-estimate available at https://github.com/lui\ngitni/git-estimate that calculates work hours out of the logs of the version control system git\nconﬁrms my very rough estimate by guessing an overall amount of 2178.32 hours of work\n\n4.5. FAT FRITZ\n183\nhave an implementation of the neural network with the associated self-training\npipeline, it is trivial to apply supervised learning as well.\nWith Fat Fritz, Chessbase took the existing open-source implementation of\nLeela Chess including its training pipeline but with an untrained network,\nand applied both supervised and unsupervised learning. According to their\nmarketing announcements17\nThe philosophy behind Fat Fritz has been to make it the strongest\nand most versatile neural network by including material from all\nsources with no such ’zero’ restrictions, such as millions of the best\ngames in history played by humans, games by the best engines in-\ncluding Stockﬁsh, Rybka, Houdini, and more, endgame tablebases,\nopenings, and so on. If it was deemed a possible source of improve-\nment, ‘zero’ or not, it was used. Even millions of exclusive self-play\ngames were created[...]\nFrom this description it is unclear how many and which games were used\nfor supervised learning, and how much training was spent on reinforcement\nlearning. As Chessbase is also a vendor of chess databases, we can assume\nthat their main product Mega Database was used, a collection of approximately\n8 million chess games dating from the year 1475 until now. They also sell a\ncorrespondence chess database with approx 1.6 million games. As nowadays\ncorrespondence players utilize chess engines, these games are usually of very\nhigh quality. Last, there are several endgame tablebases sold by Chessbase as\nwell as opening books. It is unclear however if any of the opening books overlap\nwith the Mega Database. To my best knowledge there is no information on which\nkind or how many engine matches were played.\nThere are several questions that arise now:\n1. Does this approach result in a strong chess engine?\n2. What are the advantages and disadvantages of employing supervised\nlearning in that context? Does it make sense from a scientiﬁc standpoint?\n17https://en.chessbase.com/post/fritz-17-with-fat-fritz-and-goodies, accessed June\n25th, 2021.\n\n184\n4. MODERN AI APPROACHES - A DEEP DIVE\n3. Which of the three — supervised, unsupervised or a mix of both — will\nresult in the strongest engine?\n4. What is the added value compared to Leela Chess Zero — after all, Fat\nFritz retails for 60 to 80 Euro whereas Leela Chess Zero is a free community\neﬀort.\nThe ﬁrst question can be answered easily in that Fat Fritz is a strong chess engine\nthat was competitive. The other questions are more complicated though.\n4.5.1\nA Critical Evaluation\nSo what are the advantages and disadvantages of employing supervised learn-\ning in our context?\nFirst we must understand that when we train a network, we want it to learn from\nthe training data that we feed in. Ideally the network extracts and learns patterns\nand structures and thus evaluates chess positions similar as the evaluation of\nthe training data, and also computes similar move probabilities compared to\nthe ones in the training data. If we have bad training data, say chess games\nfrom amateurs with lots of blunders, then our network is trained to blunder as\nwell. Thus a big disadvantage of supervised learning is simply availability and\nquality of training data.\nEven if we have enough training data and our network achieves good perfor-\nmance, there is the open question whether our network is saturated or if we\nare just stuck in a local maximum and could even improve the strength of our\nnetwork by more training data.\nOn the other hand if such training data is available, it is often easy to assess\nthe quality of the training data. It is also often easier to train a network with\nsupervised methods than to train one with unsupervised methods. Even though\nit has to be noted that in this particular case of training chess/Go/Shogi engines,\nthe training pipeline developed by DeepMind is particularly simple.\nAnother big advantage by supervised learning with high quality training data\nis that it is usually much faster. Think of AlphaZero: The weights of the network\n\n4.5. FAT FRITZ\n185\nare initially random. This more or less random network is then used in combi-\nnation with MCTS to generate self-play games. These games will naturally of\nvery poor quality initially and it will take quite some time until the network as\nwell as the quality of the generated games improve.\nIf we instead directly train the network with high-quality games of grandmas-\nters, the initial progress will be much faster.\nWhich brings us to the next\nquestion: Which of the three — supervised, unsupervised or a mix of both —\nwill result in the strongest engine?\nChessBase is as a commercial operation selling chess products and engines. We\ncannot expect scientiﬁc evaluations or peer-reviewed papers from them like in\nthe case of DeepMind. In other words aside from marketing, no investigation\nof supervised learning vs. reinforcement learning in the case of computer chess\nhas been made. Therefore it is diﬃcult to get an objective view on whether the\napproach used to train Fat Fritz is scientiﬁcally valid or not.\nOur next best information is the scientiﬁc data provided by DeepMind. How-\never they only thoroughly compared supervised vs. reinforcement learning for\nthe case of Go, not for chess. We can assume with large certainty that the result\nwould hold for chess as well, but as of now, there is no data to prove it.\nWe have talked about the data provided by DeepMind in their paper on Al-\nphaGoZero [SSS+17] in Section 4.2, but let’s brieﬂy summarize what they in-\nvestigated and re-think of it in the context of Fat Fritz. They used the network\narchitecture of AlphaGo Zero and then trained that network once with super-\nvised learning using the KGS dataset and once with reinforcement learning\naccording to their self-learning pipeline. The KGS dataset they used contained\napproximately 30 million positions [SHM+16].\nThe ﬁrst comparison is training time vs. Elo rating. Even after only a few hours\nof training, the supervised network achieved an ELO rating of nearly 3000,\nwhereas the network trained with reinforcement learning required almost 15\nhours18 of training to achieve that level. After 20 hours of training however the\n1815 hours on their computing cluster which of course translates to months or even years of\ntraining time on a standard home computer\n\n186\n4. MODERN AI APPROACHES - A DEEP DIVE\nnetwork trained with reinforcment learning strictly surpassed the supervised\none and eventually reached an ELO of over 4000 after 70 hours of training when\nthe experiment was stopped. The supervised network leveled out after about\n40 hours of training.\nThe conclusion we can draw from that is that supervised learning will achieve\ngood results faster, but with enough training time a network trained with rein-\nforcement learning will surpass it. This of course holds especially when we run\nout of training data. But as mentioned in Section 4.2, other data from DeepMind\nsuggest that this eﬀect is not just because of a lack of training data but rather\nthat reinforcement learning results in a superior engine.\nNamely the next thing they compared was training time vs. prediction accuracy\nw.r.t. a dataset of moves played by strong human players (the GoKifu dataset).\nThat is, given a position from that dataset and the move that the human expert\nplayer selected, the network is queried to predict that move. Here at any point in\ntime, even after 70 hours of training, the network using supervised training was\nstrictly better at this task with a prediction accuracy of about 53 percent reached\nafter 70 hours of training. The network trained with reinforcement learning\nonly reached about 49 percent (this is actually a large gap; getting even a few\nmore percent is a challenging task). This looks strange, isn’t the network trained\nwith reinforcement learning the stronger player? The answer will become clear\nvery soon.\nLast they compared training time vs. the mean squared error of predicting\ngame outcomes on the GoKifu dataset, i.e. given a position, who is going to\nwin the game? Here initially the network trained by supervised learning has a\nlower error, but after approximately 15 hours of training the error of the network\ntrained by reinforcement learning is strictly smaller then the network trained\nby supervised learning. After 70 hours of training, the diﬀerence is an MSE of\nabout 0.18 (reinforcement learning) vs. 0.21 (supervised learning).\nLet’s summarize: After a suﬃcient amount of training\n1. the network trained with reinforcement learning yields the stronger en-\ngine by more than 500 Elo points\n\n4.5. FAT FRITZ\n187\n2. the network trained with reinforcement learning is worse at predicting\nhuman expert moves than the network trained by supervised learning on\ngames of human players\n3. yet the network trained with reinforcement learning is better at predicting\ngame outcomes. In other words it has a better positional understanding\nand evaluation of a position.\nThis leads to the conclusion that the network trained by reinforcement learning\ngained superhuman strength. Of course it is bad at predicting moves of human\nexpert players because it chooses not to play such inferior moves! Or as the\nDeepmind team put it: AlphaGo Zero achieved superhuman performance.\nA scientiﬁc evaluation for supervised vs. reinforcement learning for chess would\nsolve the question whether these ﬁndings would hold for chess as well. Aside\nfrom the above mentioned research questions regarding supervised vs. rein-\nforcement learning, an additional question would be how a network behaves\nif it is ﬁrst trained (kickstarted) by supervised learning and then improved by\nreinforcement learning and vice-versa.\nWe must also not forget two diﬀerences to Go that might impact the results.\nAccording to the data provided by Deepmind [SHM+16], the KGS dataset con-\ntained games that resulted in 30 million positions. MegaBase contains about\n8 million chess games. If we assume a game length of 20 moves (to account\nfor transpositions in the opening and endgame), we have already 160 million\npositions available for training — an order of a magnitude more positions! Sec-\nond, chess is a very drawish game and existing classical chess engines already\nreached a very strong level of play. Of course AlphaZero beat existing classi-\ncal alpha-beta searchers convincingly in chess, but the ELO gap was nowhere\nnear the situation in computer Go or computer Shogi. With smaller margins in\nimprovements, it might be more diﬃcult to see clear trends and separate them\nfrom artifacts, like e.g. being stuck in a local minimum in training.\nAssuming however that the results for Go transfer to chess we can try to make\nsense w.r.t. Fat Fritz’ performance.\nClearly training with supervised learn-\ning with games of human expert players will result in getting a strong en-\ngine with much less training time compared to the reinforcement learning\n\n188\n4. MODERN AI APPROACHES - A DEEP DIVE\napproach. Training without reinforcement learning might miss “superhuman”\nmoves since such superhuman moves are apparently missing in training data\ncomposed of games by human players. In such case there is no reason to believe\nthat training by supervised learning does give any advantage when the ﬁnal\ngoal is to create the strongest possible engine.\n4.5.2\nFree and Open Source and the GNU GPL\nLeela Chess Zero is developed by volunteers as free software under a particular\nlicense, the GNU General Public License. Software is usually distributed in\nbinary form which only a computer can really understand. During development\nsoftware is written in source-code and then translated to binary form by a\ncompiler. In order to modify, improve or change software we need the source\ncode.\nWhile I am not a lawyer, the GNU GPL works as follows: Software is distributed\nwith its source code. Anyone can change, modify or improve the software —\nbut if you do so you have to make your changes public under the same license.\nThe intention is that others can beneﬁt from your changes as well.\nAs you have gained an understanding on how neural networks and MCTS\nworks you can imagine that Leela Chess Zero consists of several components:\n• the (fast and eﬃcient) implementation of the neural network and its asso-\nciated data-format of the weights for distribution\n• the input and output encodings of chess positions\n• the training pipeline\nChessBase took all these components without any signiﬁcant changes. Instead\nthey changed a few parameters as well as the name of the engine and the authors\nnotice19. They then invested a lot of computing resources to train the network in\na diﬀerent way, apparently mostly by supervised learning, packaged the engine\ntogether with a graphical user interface and sold the whole package for around\n60 Euro.\n19https://github.com/LeelaChessZero/lc0/compare/release/0.23...DanielUranga:fritz\n\n4.5. FAT FRITZ\n189\nIt is perfectly valid to sell free software that is licensed under the GNU GPL.\nOf course this makes only sense if there is some perceived added value. If such\nadded value does not exist, one could simply download the (original) version\nof the software for free.\nLinux distributions work like that. Vendors collect various free software com-\nponents like the Linux kernel, a graphical user interface, drivers and more,\npackage them together with a custom install tool, add a manual and provide\ncustomer support in particular for the case where Linux is run as a server, and\nship and sell that package as a Linux distribution. Users are willing to pay\nfor that since it is very cumbersome to download, install and combine these\nsoftware packages all by yourself. Whereas there are also a lot of free Linux\ndistributions, a lot of users — especially the ones that run mission critical sys-\ntems — choose to buy the commercial version with commercial support. The\nmakers of Linux distributions on the other hand also use some part of their\nproﬁts to invest in the further development of key components of Linux such\nas the kernel. This is perceived as a symbiosis and thus software developers\nare motivated to spend their free time and resources to improve Linux and its\ncodebase.\nIn the case of Fat Fritz there was no giving back in terms of code to Lc0. More-\nover Chessbase was not always very clear about the fact that FatFritz is just\nLeela Chess Zero with the network trained in diﬀerent way. For example on\nmy German retail package of Fritz 17, Leela Chess Zero is nowhere mentioned.\nIn fact when buying it I was not aware that Fat Fritz was based on Lc0; I hon-\nestly thought Chessbase independently developed their own in-house neural\nnetwork based engine from scratch.\nThe perceived beneﬁt of Fat Fritz is at least questionable since as we discussed,\nit can be asked if applying supervised learning to train the network does yield\nany beneﬁt compared to just training the network by reinforcement learning —\nas done by the Lc0 community. Their network(s) can be downloaded and used\nfor free.\nUnderstandably Fat Fritz was not really warmly welcomed by the chess enthusi-\nasts and programmers community. And the situation somewhat escalated when\n\n190\n4. MODERN AI APPROACHES - A DEEP DIVE\nFat Fritz 2 was introduced — but that is subject to one of the other chapters.\nWe should however mention here that computer chess has unfortunately a\nhistory of alleged GPL violations. Chess engines used to be black art, and there\nwas a small community of programmers who lived well from developing chess\nengines; ﬁrst for dedicated chess computers and then later for home and MS-\nDOS computers. This all started to change when French programmer Fabien\nLetouzey developed his chess engine Fruit and made it freely available under\nthe GNU GPL. Not only was it a strong chess engine, coming in second at\nthe World Computer Chess Championship (WCCC) 2005, it was also clean and\nwell-written code. A lot of programmers took inspiration from Fruit. Most\nadhered to the GNU GPL releasing their sources as well, but some did not.\nAllegedly we must say, as no court case has ever been ﬁled by Fabien.\nThen the commercial chess engine Rybka by Vasik Rajlich started to appear and\nquickly gained traction winning the Computer Chess Championships in 2007,\n2008, 2009 and 2010. When allegations were made that Rybka’s code base was\nsourced in Fruit, things got nasty. After an investigation, the International Com-\nputer Games Association — the organization behind the WCCC — concluded\nin June 2011 that Rybka was plagiarized, stripped Rybka from all its titles and\nbanned Vasik Rajlich for life from competing in WCCC events.\nAs Rajlich refused to provide his source code to the investigation committee,\nthe evidence for a GPL violation was circumstantial, and based on the analysis\nof the binary. The whole issue remains a heated debate up until today.\nWhich is not to say that the GNU GPL is not enforceable. Open source enthusiast\nHarald Welte started the project gpl-violations.org when he noticed that\nmany large companies borrowed Linux source code without adhering to the\nGPL. Several court cases were ﬁled and to my best knowledge they never lost\na case. Their most spectacular case was probably against Fortinet which used\nthe Linux kernel for their product FortiOS without providing its sources. The\nresult was a settlement where Fortinet would provide all sources in question20.\nNevertheless suing in court is an expensive and risky endeavour and gpl-vi\nolations.org could only do that with the support of volunteers as well as\n20http://www.cnet.com/news/fortinet-settles-gpl-violation-suit\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n191\ndonations. It is thus understandable that no court case has been ﬁled in the\nRybka controversy by Fabien Letouzey, even though that would probably end\nthe dispute, one way or the other.\nThe very successful chess engine of the Fritz chess software package from\nChessBase was developed by Frans Morsch. When he retired after Fritz 13 after\na long career in chess engine programming, ChessBase was left without an\nengine author. Despite the open dispute around Rybka, they then hired Vasik\nRajlich to write the engine for Fritz 15 and Fritz 16. As he likely didn’t start\nfrom scratch, we can assume that both were based on Rybka.\nIt seems ChessBase really isn’t afraid to ﬁsh in muddy waters.\nAgain and again.\n4.6\nEﬃciently Updateable Neural Networks (NNUE)\nBy achieving superhuman strength in Go — one of major long-lasting open\nchallenges in artiﬁcial intelligence — AlphaZero rightfully made a deep impact\nnot only in the Go, Shogi and chess world, but also even in mainstream media.\nBut there has been another revolution going on which mostly went unnoticed\naside from a very interested crowd in the computer Shogi and computer chess\nworld. That revolution is eﬃciently updateable neural networks (NNUE). But\nlet’s start at the beginning.\nShogi is mainly a Japanese thing. Whereas Go is a fundamental diﬀerent game\ncompared to chess and widespread in Asian countries and even gained traction\nin the west, Shogi can be considered as a Japanese variant of chess. Similar to\nwestern chess it likely originated in India and derived from the game Chatu-\nranga, but whereas Chaturanga developed in the middle east and the western\nhemisphere into modern chess, it developed into Shogi in Japan and the chinese\ngame Xiangqi. The Shogi scene is thus much smaller, and the computer Shogi\nscene is even more smaller.\nAs mentioned before, one major diﬀerence between Shogi and chess is that\ncaptured pieces can re-enter the game, similar to Crazyhouse or Bughouse chess.\n\n192\n4. MODERN AI APPROACHES - A DEEP DIVE\nThe branching factor, i.e. the average number of possible moves in a position is\nthus much larger than in chess (but smaller than in Go). Similar to chess, the\nbest traditional computer Shogi programs before AlphaZero were alpha-beta\nsearchers with custom evaluation functions. But whereas for Go and chess the\nresults of AlphaZero were quickly reproduced by an international community\ndue to Leela Zero and Leela Chess Zero, that never happened with Shogi.\nThen in 2018, computer Shogi enthusiast Yu Nasu posed the question of why not\ncombine alpha-beta search with a neural network for evaluation. For chess, this\nhad been already done with DeepChess [DNW16], which reached grandmaster\nlevel but was never close to the state of the art of traditional chess programs.\nHow come? In fact why did AlphaZero use MCT search during training and\nplay, instead of alpha-beta search?\nTo understand the general problem with such an approach we need to think\nabout the time that a neural network requires to compute output values from\nthe input. That required time is very small or very large, depending on the\ncontext of the underlying use-case. Suppose that one pass through a neural\nnetwork requires 100 milliseconds of computation time and consider the face\ncomparison task that we described in Chapter 2, i.e. a border oﬃcial queries\na neural network with a photograph taken of a person upon entry together\nwith the photo stored on that person’s electronic passport. Then the network\ncomputes a score value that indicates whether these two photographs show the\nsame person. If that takes 100 milliseconds that is a perfectly valid time.\nNow imagine that we want to use such kind of network for alpha-beta search.\nThis means we can only process ten chess positions per second — that is far too\nslow! To just give an intuition, Table 4.4 shows typical numbers of processed\nchess positions per second.21 Of course a neural network as capable as the very\ndeep network used for AlphaZero would be nice to have for alpha-beta search.\nBut even a much simpler network would do — as long as it is more powerful\nthan a handcrafted evaluation function, it will increase engine power. Moreover\nwe have to note that while AlphaZero’s results against Stockﬁsh and state of the\n21Of course, the number are highly dependent on which computing power is available on the\nsystem. The numbers for Stockﬁsh 8 and AlphaZero are the ones reported by the Deepmind\nteam.\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n193\nTable 4.4: Chess Engines Speed\nPositions per Second\nSearch Algorithm\nDeepBlue\n200,000,000\nAlpha-Beta\nStockﬁsh 8\n70,000,000\nAlpha-Beta\nAlphaZero\n80,000\nMCTS\nart Shogi programs were impressive, the gap between them was nowhere near\nas large as compared to the best available Go programs.\nIn 2018 Yu Nasu proposed to use a very speciﬁc kind of network which he\ndubbed eﬃciently updatable neural networks (NNUE). And this is where he used\nreally all tricks of the trade.\nHe considered all layers of abstractions: The\nnetwork architecture, the nature of Shogi gameplay, as well as the hardware\narchitectures of modern CPUs. At the end it all ﬁts neatly together and his\nNNUE network achieved a speed that made it usable for alpha-beta search in\ncomputerShogi. As a computer scientistI haveto say that I was full ofexcitement\nand enjoyment when ﬁnally understanding this very neat architecture.\nMotohiro Isozaki teamed up with Yu Nasu to integrate an NNUE network into\nhis state-of-the art open source computer Shogi program YaneuraOu. The whole\nNNUE revolution went largely unnoticed in the computer chess community.\nFirst, the network was speciﬁc to Shogi and it was not clear whether the idea\nwould work for western chess as well. Second the original paper is in Japanese\nonly22 and the language barrier is simply very high.\nAs mentioned the open-source computer Shogi scene is smaller and limited in\nresources compared to western chess. When developing YaneuraOu, Motohiro\nIsozaki used GPL licensed code from Stockﬁsh and adapted it into YaneuraOu\nwhich is also open source software licensed under the GPL; in particular he\ntook inspiration of the well-engineered search algorithm of Stockﬁsh.\nNevertheless he felt that merely abiding to the license terms was not enough. In\na true open source spirit he felt obliged to give back, and ported an experimental\n22I provide a rough translation at http://github.com/asdfjkl/nnue\n\n194\n4. MODERN AI APPROACHES - A DEEP DIVE\nversion of the NNUE network back to Stockﬁsh. This made some huge impact\nand eventually resulted in Stockﬁsh 12 which uses NNUE for position evaluation\ninstead of the old-fashioned handcrafted evaluation function. NNUE brought\nan increase of around 80 Elo23 points to Stockﬁsh and as of now, Stockﬁsh is the\nnumber one engine beating Leela Chess Zero by a signiﬁcant margin.\nNo book about computers is complete with really bad car analogies. So let’s get\nit over with: When AlphaZero is the Benz Patent-Motorwagen, then NNUE is\nthe Ford Model T of computer chess. We should not underestimate AlphaZero’s\nachievement, but due to the huge computing power required, the underlying\nchess engine was not available for the general public. Leela Chess Zero tried to\nremedy that, but even Leela Chess Zero requires signiﬁcant hardware to have\nit run at an acceptable speed — the best and latest graphic card is a must. With\nNNUE we can run the best of both worlds — classic alpha-beta search and\na neural network for position evaluation — on a standard personal computer\nwithout requiring an expensive graphic card.\nNow let’s take a deep dive into NNUEs. As always, we start with the input and\noutput of the network before taking a look at the architecture.\n4.6.1\nNetwork Input and Output\nThe output is the most simple one. We have one single output node and that\nvalue denotes the evaluation of the position in centipawns.\nThe input on the other hand is slightly... quirky. It is dubbed HalfKP which\nstands for Half-King-Piece relationship and is a binary encoding of a chess\nposition. For that we ﬁrst take the role of the player whose current turn it is.\nThen we enlist all possible triples\n(own king position, own piece type, position of that piece)\nas well as\n(own king position, enemy piece type, position of that piece).\n23for current chess engines that’s a lot\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n195\n8 rmbZkans\n7 opo0lpop\n6 0Z0o0Z0Z\n5 Z0Z0o0Z0\n4 0Z0OPZ0Z\n3 Z0Z0ZNZ0\n2 POPZ0OPO\n1 SNAQJBZR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.14: Typical position in the Philidor opening. White to play.\nThis is the ﬁrst half of the encoded position. Then we take the role of the other\nplayer and repeat it. This is the second half of the encoded position.\nIt is best illustrated with a small example. Consider Figure 4.14. Black just\nplayed Qe7 and it is White to play. Thus we ﬁrst take the role of the white\nplayer. We ﬁx a king position and a piece type other than the king — note that\nwe distinguish between our own and enemy pieces — and start enumerating\nthem. Assume we always start at a1 and work towards h8:\n• own king on a1, own pawn on a1: 024\n• own king on a1, own pawn on a2: 0\n• own king on a1, own pawn on a3: 0\n• own king on a1, own pawn on a4: 0\n• . . .\n24This is of course impossible and will always be zero. Omitting it is also possible, but including\nit makes programming the encoding slightly easier.\n\n196\n4. MODERN AI APPROACHES - A DEEP DIVE\nThis is pretty boring but once we are at “own king on e1” things start to get\ninteresting:\n• own king on e1, own pawn on a1: 0\n• own king on e1, own pawn on a2: 1\n• own king on e1, own pawn on a3: 0\n• own king on e1, own pawn on a4: 0\n• . . .\n• own king on e1, own rook on a1: 1\n• own king on e1, own rook on a2: 0\n• . . .\n• own king on e1, enemy pawn on a1: 0\n• . . .\n• own king on e1, enemy pawn on a7: 1\n• . . .\n• own king on e1, enemy rook on h8: 1\nYou probably get the idea. Next we repeat that for the enemy player (here this\nis Black). We get ones for “enemy king on e8, own pawn on a2”, “enemy king\non e8, enemy rook on a1” and so on as well as “enemy king on e8, enemy pawn\non a7” and “enemy king on e8, enemy rook on a8”.\nThe let’s just quickly calculate on how many bits we require for this encoding.\nLet’s start with the ﬁrst half, i.e. ﬁxing the position of our own king. There are\n64 possible squares for the king. Next there are ten possible pieces that we can\nconsider except for our own king and the enemy king, namely our own ﬁve\npiece types (rook, knight, bishop, queen and pawn) and the same again for the\nenemy. There are potentially 63 positions for these pieces each as a piece cannot\nbe on the same square as the king, but as mentioned before when implementing\nthis in software it is probably easier to just consider all 64 possible squares. This\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n197\n8 rZbjqans\n7 opo0Zpop\n6 0ZnZ0Z0Z\n5 Z0Zpo0Z0\n4 0Z0O0Z0Z\n3 Z0Z0O0Z0\n2 POPL0OPO\n1 SNAKZBMR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.15: Typical position in the Philidor opening. White to play.\nmakes 64 × 64 × 10 = 4096025 input bits, and thus 2 × 40960 = 81920 input bits\nfor both halves. The whole idea of this rather odd encoding will become clear\nin the next section. But without going into details let’s note a few properties of\nthis encoding.\n1. Mirrored positions result in same input values since we consider a position\nfrom the player whose move it is, agnostic of the actual color. Consider\nfor example Figure 4.15 and assume that it is Black to move. Then the\nencoding is exactly the same as for the position depicted in Figure 4.14.\n2. With the exception of the end game, kings are typically moved much less\nin a chess game than other pieces. Thus when making a move, only a\nfew bits of the input change. Consider Figure 4.16 which occurs from the\nposition of Figure 4.14 after the move Nc3. In principle only the following\nbits change:\n• own king on e1, own knight on b1 from 1 to 0\n25In Stockﬁsh it’s actually a few more bits, but that is an artifact of the straight-forward port\nfrom Shogi where more input bits are required.\n\n198\n4. MODERN AI APPROACHES - A DEEP DIVE\n• own king on e1, own knight on c3 from 0 to 1\n• enemy king on e8, own knight on b1 from 1 to 0\n• enemy king on e8, own knight on c3 from 1 to 0\nOf course we also have to take into account that the perspective of the\nplayer whose current turn it is has changed. Therefore we have to also ﬂip\n“own” with “enemy” but this is a mere technicality:\n• enemy king on e1, enemy knight on b1 from 1 to 0\n• enemy king on e1, enemy knight on c3 from 0 to 1\n• own king on e8, enemy knight on b1 from 1 to 0\n• own king on e8, enemy knight on c3 from 1 to 0\n3. The input encoding is highly ineﬃcient in that there are way more input\nbits than necessary. A more straight-forward way to encode inputs is to\njust use bitboards as done in e.g. AlphaZero. There we take 8x8 bits for\neach piece type and set a bit at position (x,y) to one if that piece is present\non (x,y) and to 0 otherwise. Now there are six possible pieces for the white\nplayer (king, queen, bishop, knight, rook, pawn) as well as six possible\npieces for the black player. All in all we merely need 64 × 6 × 6 = 768\nbits opposed to 40960 bits in our case. As inputs will be associated with\nweights when building our network, the network will be heavily over-\nparameterized, i.e. there are more parameters in the network than (the-\noretically) required, especially when considering the amount of possible\ntraining data. It is known that overparameterized networks tend to learn\nbetter and generalize well. From a stochastic viewpoint it is an interest-\ning problem to understand why this works, and this is subject of current\nresearch [DZPS19, ALL19]. Usually however this overparametrization is\npart of the inner network structure, i.e. by creating a deeper network by\nadding more hidden layers or increasing the size of the hidden layers.\nHere it is explicitly modeled by the input layer. The reason for this rather\nodd choice is entirely due to optimizing the network for ultra-fast com-\nputation, while still maintaining some sort of overparametrization in the\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n199\nhope that the network will learn well and generalize from the training\ndata. How this input encoding allows ultra-fast computations is subject\nto the next section.\n4. The encoding focuses heavily on the relation of pieces to their king. While\nit is true that a king rarely moves (at least during the opening and middle\ngame), this choice also strikes as slightly odd. This choice of encoding\nseems to be entirely based in the game of Shogi. As mentioned before,\nShogi is more similar to Bughouse or Crazyhouse chess in that captured\npieces can be put pack in the game as own pieces.\nThus there is an\nincreased chance of being checkmated. Moreover Shogi has no castling\nmove. Instead, players manually build a fortress around their king by\nputting pieces around it such as gold or silver generals and pawns. Such\nkind of fortress is known as a castle (kakoi) in Shogi. There are diﬀerent\nkinds of castles and a whole theory around the diﬀerent castle types.\nApparently the positional relation of own and enemy pieces to one’s own\nking are quite important. As mentioned, NNUE was originally developed\nfor Shogi and later ported to Stockﬁsh, and it was empirically discovered\nthat such kind of encoding works for chess as well — but there is currently\nno satisfying theoretical explanation for that.\n4.6.2\nNetwork Architecture\nThe whole network is shown in Figure 4.17. It is a comparatively small network\nwith only three hidden layers. The input bits are divided into two halves: The\nﬁrst half comprises all bits where we consider the location our (or rather the\nplayer whose turn it is in the current position) own king plus the pieces in\nrelation to our own king. The second half comprises all bits where we consider\nthe location of the enemy and the pieces in relation to that position.\nThe next layer consists of two halves of 256 nodes each. Each of the halves of\nthe input layer is fully connected to the corresponding half of 256 nodes (but\nthere is no connection between non-corresponding halves). This layer is then\nfully connected to a next layer of 32 nodes — ﬁnally mixing everything together\n— which is itself fully connected to another layer of 32 nodes. This layer is then\n\n200\n4. MODERN AI APPROACHES - A DEEP DIVE\n8 rmbZka0s\n7 opo0lpop\n6 0Z0o0m0Z\n5 Z0Z0o0Z0\n4 0Z0OPZ0Z\n3 Z0M0ZNZ0\n2 POPZ0OPO\n1 S0AQJBZR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.16: Mirrored Philidor opening position. Black to play.\nfully connected to a ﬁnal single node which outputs the evaluation value.\nThroughout the whole network, (clipped) ReLU is used as the activation func-\ntions. Values smaller or equal to zero are mapped to 0, values in between 0 and\n1 are mapped to themselves, and values larger or equal than 1 are clipped to 1.\nThere is one non-standard oddity: The weights that are used to connect the ﬁrst\nhalf of the input layer (i.e. the half of one’s own king) to the corresponding half\nof the ﬁrst layer are the same weights that are used to connect the second half of\nthe input layer to the second half of the ﬁrst layer. They are shared in the sense\nthat mirrored piece-square relations share the same weight.\nConsider for example the weight that corresponds to the arrow that connects\nthe bit “own king on e1, own pawn on d2” to the ﬁrst node of the ﬁrst layer. This\nis the same weight that is used for the bit “enemy king on e8, enemy pawn on\nd7”. At ﬁrst sight this really does not make any sense, especially if we consider\nthat it should make a huge diﬀerence if it is Black or White to move and thus the\nweights should be diﬀerent. While this is correct and sharing weights in that\n\n\n\n\n204\n4. MODERN AI APPROACHES - A DEEP DIVE\nway is certainly restricting in terms of expressiveness26, this special arrangement\nmakes it possible to eﬃciently compute the two 256 values of the ﬁrst layer for\na given position as we will see below. First however we will take a look on how\nto eﬃciently compute the values of the second, third and ﬁnal layer.\nThe general underlying idea is that for these layers, we consider a speciﬁc way of\nrepresenting the weights by 8 bit values so that we can use acceleration features\nof modern CPUs, namely single-instruction-multiple-data (SIMD) extensions.\nWe already talked brieﬂy about vectorization in Chapter 2 but let’s dive further\ninto the topic now.\nI should mention that in the further discussions I will\ncompletely omit the bias weights of the network for the sake of clarity. Adding\nbiases just means we add constant values to the weight computation and all\nideas work in the same way, but notation will become more intricate.\nWe will now take a look on how to compute the values for the second layer.\nThe second layer receives 512 inputs from the ﬁrst layer which are then used to\ncompute the values for the 32 nodes of the second layer. Let write 푤푖\n푗for the\nweight that connects input 푗from the ﬁrst layer to node 푖of the second layer,\nand let’s denote with 푥푖the 푖th input. We thus have two matrices\nw =\n\n푤0\n0\n푤0\n1\n. . .\n푤0\n511\n푤1\n0\n푤1\n1\n. . .\n푤1\n511\n...\n푤31\n0\n푤31\n1\n. . .\n푤31\n511\n\n, and x =\n\n푥0\n푥1\n...\n푥511\n\n(4.1)\nMultiplying these matrices wx yields precisely the values of the second layer\n26We can somehow take care of this misjudgement when interpreting the ﬁnal output value\nby e.g. adding a small bonus to the evaluation for the side whose turn it is.\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n205\nthat we need:\nwx =\n\n푤0\n0푥0 + 푤0\n1푥1 + . . . 푤0\n511푥511\n푤1\n0푥0 + 푤1\n1푥1 + . . . 푤1\n511푥511\n...\n푤31\n0 푥0 + 푤31\n1 푥1 + . . . + 푤31\n511푥511\n\n(4.2)\nCPUs are general all-purpose processing units. They can quickly add, subtract\nand multiply integer numbers. The size of these integers is determined by the\nword size of a CPU. A 64 bit CPU for example has registers where one register\ncan hold 64 bit.\nA CPU inherently however does not necessarily have good support for ﬂoating\npoint numbers. Early computer systems like the 8 bit home computers of the\n80s had no ﬂoating point support at all. Floating point operations had to be\nsimulated in software which is very slow. Even early Intel chips had no such\nsupport — the ﬁrst Intel chip with integrated ﬂoating point support was the\n486 DX processor line. For 486 SX processors and earlier you had to buy a\nseparate 80x87 co-processor. The reason for this is that space on the chip is\nvery expensive and chip makers have think carefully on what users want and\nare willing to pay for. Even today with an integrated ﬂoating point processor,\ninteger operations are simply more eﬃcient to compute.\nSince multimedia application, such as video decoding, became more and more\npopular, Intel decided to add various extensions for mathematical operations.\nThey began with so called multimedia extensions (MMX) and later added single-\ninstruction multiple-data (SIMD) extensions. As an example we will take a look\nat the SIMD operation (V)PMADDUBSW which works on two inputs. Often,\nsuch an input is a single integer value, but SIMD instructions are special. The bit\nsize of each of these inputs depends on the version of (V)PMADDUBSW; various\nextensions have been added by Intel over time. For the ease of presentation we\nassume that both inputs have a size of 256 bits.\nConsider Figure 4.18. The 256 bits of each input are split into 8 bit subwords\n푎0, 푎1, . . . , 푎31 and 푏0, 푏1, . . . , 푏31.\n\n206\n4. MODERN AI APPROACHES - A DEEP DIVE\nEach of these 8 bit subwords 푎푖and 푏푖are interpreted as an integer value in the\nrange of 0 . . . 28 −1 = 255. (V)PMADDUBSW then computes\n푎0 ∗푏0 + 푎1 ∗푏1, 푎2 ∗푏2 + 푎3 ∗푏3, . . . 푎30 ∗푏30 + 푎31 ∗푏31.\nAnd this computation is very fast, as the circuit for this computation is present\nas silicon on the CPU.\nThe ﬁrst step in optimizing the network for fast computation with a CPU was\nthus to forget about ﬂoating point numbers. Each intermediate value as well as\nthe weights are therefore expressed as 8 bit integer values. Of course we loose\nprecision as we can only distinguish 28 = 256 diﬀerent weight values. On the\nother hand we use the clipped ReLU as the activation function, so we will clip\nanyway at values below of 0 and above of 1, so we need precision mostly to\nexpress the range between 0 . . . 1.\nWe can then compute for each 푖in the range 0 . . . 31 the value 푤푖\n0푥0 + 푤푖\n1푥1 +\n. . . + 푤푖\n511푥511 by multiple calls to (V)PMADDUBSW. Start with\n푤0\n푗, 푤1\n푗, . . . 푤31\n푗and 푥0, . . . 푥31\nas inputs and use (V)PMADDUBSW to get\n푤0\n푗푥0 + 푤1\n푗푥1, 푤2\n푗푥2 + 푤3\n푗푥3, . . . 푤30\n푗푥30 + 푤31\n푗푥31.\nThen repeat this with multiple calls for indices 32 to 512, and sum up all the\nresults.\nThis is only the brief idea.\nThe ReLU activation function can be computed\nwith similar optimizations using SIMD extensions. As there are diﬀerent SIMD\ncommands depending on each processor generation, further optimizations are\npossible.\nNevertheless, these computations are not even the main bottleneck. Instead this\nis the input layer. After all, for the ﬁrst layer we have 512 nodes and for the\nsecond and third layer we have 32 nodes. But for the input layer we have more\nthan 80000 values to compute!\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n207\nOne important point of the input layer is to note that a lot of input bits will be\nzero. All bits with own king on the correct square will be set to one, but all the\nother bits where our king is on a diﬀerent square will be zero and these are of\ncourse many more. The same holds for the enemy king. These inputs are then\nmultiplied by weights and summed up to create the values of the ﬁrst layer, but\nmultiplying something with zero yields zero, and if we add up a lot of zeros we\nstill have zero.\nNow what happens if we have computed the values for the ﬁrst layer from\nthe input layer and then apply a move? Let’s assume it was White to move, i.e.\nWhite was “our side” and thus comprises the ﬁrst half of the input bits, whereas\nBlack corresponds to the second half of the input bits. After the next move we\nswitch sides and the ﬁrst half has the input bits of Black (who is now to move\nand thus “our side”), and White will become the enemy side and thus ﬁll the\ninput bits of the second half. In both cases the bits w.r.t. White’s move change.\nHowever note that weights are shared for a ﬂipped position and therefore not\nmuch changes w.r.t. the computation of the values of previous positions. Both\nfor the ﬁrst and second half almost all input bits are the same as we have an\n(almost) ﬂipped version of each half where only the bits corresponding to the\nlast move changed. The idea is now to just add or subtract the diﬀerences that\nare the result of the move from the previous values to get the new values instead\nof recalculating all values. This will become clearer once we write everything\ndown in matrix notation. Let 푤푗\n푖be the weight of input 푗that connects to node\n푖of the ﬁrst layer, and let 푥푖denote the input bit 푖. For the sake of the example\nwe consider only one half of inputs — of course this re-computation has to be\ndone for both sides. We have\nw =\n\n푤0\n0\n푤0\n1\n. . .\n푤0\n40959\n푤1\n0\n푤1\n1\n. . .\n푤1\n40959\n...\n푤255\n0\n푤255\n1\n. . .\n푤255\n40959\n\n, and x =\n\n푥0\n푥1\n...\n푥40959\n\n(4.3)\n\n208\n4. MODERN AI APPROACHES - A DEEP DIVE\nand we want to compute\nwx =\n\n푤0\n0푥0 + 푤0\n1푥1 + . . . 푤0\n40959푥40959\n푤1\n0푥0 + 푤1\n1푥1 + . . . 푤1\n40959푥40959\n...\n푤255\n0\n푥0 + 푤255\n1\n푥1 + . . . + 푤255\n40959푥40959\n\n(4.4)\nHowever a lot of the 푥푖are zero and thus instead of multiplying the two matrices\n(as would be done on a general neural network framework with GPU support)\nit makes much more sense to compute the desired values iteratively.\n1. We ﬁrst initialize a result variable 푟=\n\n0\n...\n0\n\n2. Then for each 푖in the range 0 · · · 40959\n• if 푥푖is zero then immediately skip and continue with 푖:= 푖+ 1\n• if 푥푖is not zero, then set 푟:= 푟+ 푥푖\n\n푤0\n푖...\n푤255\n푖\n\nNote that 푟= wx. Since there are only few cases where 푥푖is diﬀerent from 0,\nthis will be a short loop. Moreover since 푥푖is an input bit it can only be either 0\nor 1, and thus\n푥푖\n\n푤0\n푖...\n푤255\n푖\n\nsimpliﬁes to just\n\n푤0\n푖...\n푤255\n푖\n\nNow we are almost there. Suppose that we are in a position where we computed\nthe values of the ﬁrst layer with the method above and obtained a vector 푟that\nhas one column and 255 rows. Assume we now execute a move that is not a king\nmove. Then there is only one input bit that changes from 1 to 0 for the location\nwhere the piece is picked up — say 푥42 — and one input bit that changes from\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n209\n0 to 1 where the piece is placed, say 푥55. To get the new values 푟′ we now have\nto take the old vector 푟, subtract the weights that are now zero due to 푥42 and\nadd the weights for 푥55. In other words\n푟′ = 푟−\n\n푤0\n42\n...\n푤255\n42\n\n+\n\n푤0\n55\n...\n푤255\n55\n\nNote how much simpler this operation is than recomputing everything. Let’s\nadd a chess annotation symbol to the last sentence to make sure the emphasis\nconveys: Note how much simpler this operation is than recomputing every-\nthing!!\nNow we also understand where the term eﬃciently updatable neural network stems\nfrom. We can eﬃciently update the network weights by computing incremental\ndiﬀerences w.r.t. a move that changes the position.\nWe can even make this faster by using SIMD instructions VPADDW and VP-\nSUBW that bulk add or subtract values and are depicted in Figure 4.19. They\ncome for diﬀerent bit sizes; for simplicity assume they operate on 128 bit values.\nThe bits of each 128 bit input are split into 16 bit subwords 푎푖\n푎0, 푎1, . . . , 푎32 and 푏0, 푏1, . . . , 푏32.\nEach of these 16 bit subword 푎푖and 푏푖is interpreted as an integer value in the\nrange of 0 . . . 216 −1 = 65535. VPADDW then computes27\n푎0 + 푏0, 푎1 + 푏1, . . . , 푎16 + 푏16\nand VPSUBW does the same thing for subtraction. If we represent the weights\nof the input layer as 16 bit values we can use multiple calls to VPADDW and\nVPSUBW to compute the diﬀerence as deﬁned above. Since each vector holds\n255 rows, we will require roughly 256/32 = 8 calls to VPADDW and VPSUBW\neach.\nThat’s pretty fast.\n27Some issues can arise when the result does not ﬁt into the desired 16 target bits but we won’t\ndelve into these technicalities here.\n\n210\n4. MODERN AI APPROACHES - A DEEP DIVE\n4.6.3\nTraining the Network\nTraining the network is pretty straight forward. We need pairs of positions and\nevaluation scores. We can bootstrap a network by taking positions from existing\ngames or games of self play, and let an existing chess engine evaluate a position\ndeeply to output a score. We then use these position/score pairs to train our\nneural network.\nWe can even apply some kind of reinforcement learning in that we let our\nbootstrapped engine with the trained neural network re-analyze deeply each\nposition or new positions and output new evaluation scores. We then use these\nnew pairs of position/evaluation score to further train the network and repeat.\nInterestingly at the time of writing this, Stockﬁsh uses Leela Chess Zero’s train-\ning data, i.e. positions from self-played games of Leela and her evaluation\nscores.\nNote that this is a much less complex network compared to say Leela Chess\nZero. Training requires nowhere near the eﬀort. Training a network is probably\nnot something you would start on a Friday afternoon and expect it to be ﬁnished\nbefore 5pm, but creating one is certainly doable in a few weeks. More time and\na lot of experimentation is of course required if you want to create a network\nthat is strictly better than the existing one used by Stockﬁsh.\nIt is nice to see how the open-source spirit of sharing ideas and code among\nLeela Chess Zero, Stockﬁsh and YaneuraOu brought great progress and in the\nend beneﬁted everyone.\n4.6.4\nNNUE in Practice\nIn order to evaluate the performance of AlphaZero, the DeepMind team con-\nducted several test matches against the — back then — state of the art chess\nengine Stockﬁsh 8. That version of Stockﬁsh was a heavily optimized alpha-beta\nsearcher with a carefully hand-crafted and semi-automatically tested evaluation\nfunction. AlphaZero beat Stockﬁsh 8 convincingly. In particular AlphaZero\nshowed superior positional understanding and identiﬁed long-term advan-\ntages which Stockﬁsh was unable to spot. A detailed analysis of the games is\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n211\nprovided in [SR19].\nLet’s have a look at one of the games, namely AlphaZero vs Stockﬁsh Games:\nGame 8. An interesting analysis is given by Grandmaster Daniel King on his\nYoutube channel28. The game goes\n1. d4 Nf6 2. c4 e6 3. Nf3 b6 4. g3 Bb7 5. Bg2 Bb4+ 6. Bd2 Be7 7. Nc3 c6 8. e4\nd5 9. e5 Ne4 10. O-O Ba6 11. b3 Nxc3 12. Bxc3 dxc4 13. b4 b5 14. Nd2 O-O\n15. Ne4 Bb7 16. Qg4 Nd7 17. Nc5 Nxc5 18. dxc5 a5 19. a3 axb4 20. axb4 Rxa1\n21. Rxa1 Qd3 22. Rc1 Ra8 23. h4 Qd8 24. Be4 Qc8 25. Kg2 Qc7 26. Qh5 g6 27.\nQg4 Bf8 28. h5 Rd8 29. Qh4 Qe7 30. Qf6 Qe8 31. Rh1 Rd7 32. hxg6 fxg6 33.\nQh4 Qe7 34. Qg4 Rd8 35. Bb2 Qf7 36. Bc1 c3 37. Be3 Be7 38. Qe2 Bf8 39. Qc2\nBg7 40. Qxc3 Qd7 41. Rc1 Qc7 42. Bg5 Rf8 43. f4 h6 44. Bf6 Bxf6 45. exf6 Qf7\n46. Ra1 Qxf6 47. Qxf6 Rxf6 48. Ra7 Rf7 49. Bxg6 Rd7 50. Kf2 Kf8 51. g4 Bc8\n52. Ra8 Rc7 53. Ke3 h5 54. gxh5 Kg7 55. Ra2 Re7 56. Be4 e5 57. Bxc6 exf4+ 58.\nKxf4 Rf7+ 59. Ke5 Rf5+ 60. Kd6 Rxh5 61. Rg2+ Kf6 62. Kc7 Bf5 63. Kb6 Rh4 64.\nKa5 Bg4 65. Bxb5 Ke7 66. Rg3 Bc8 67. Re3+ Kf7 68. Be2 1 - 0.\nDaniel King identiﬁed two key positions where Stockﬁsh 8 failed to assess the\nposition correctly and that were essential for the outcome of the game. And\nthere is another position that is interesting in the sense that the current Stockﬁsh\nidentiﬁes the move by Stockﬁsh 8 in that position as a crucial mistake.\nThe ﬁrst position is shown in Figure 4.20.\nAlphaZero is a pawn down but\nstill ﬁnds this position very playable and thinks there is more than enough\ncompensation. Let’s compare the following engines: It is interesting to have\nFritz 6 take a look at it for historical reasons. Stockﬁsh 8 was the version Al-\nphaZero played against.\nStockﬁsh 11 was the last Stockﬁsh release prior to\nincorporating NNUE whereas Stockﬁsh 12 was the ﬁrst version to include\nit.\nFinally, Stockﬁsh 14 was the latest Stockﬁsh when writing these lines.\nEach engines was run for a few minutes with six threads on an Intel Core\ni3 10100F with 512 MB hash size; with the exception of Fritz 6 which of\ncourse only runs single threaded and with a mere 1 MB hashtable. The bi-\nnaries are stockfish_8_x64_popcnt.exe, stockfish_20011801_x64_modern.exe,\nstockfish_20090216_x64_avx2.exe as well as stockfish_14_x64_avx2.exe for\n28https://www.youtube.com/watch?v=Ud8F-cNsa-k\n\n212\n4. MODERN AI APPROACHES - A DEEP DIVE\n8 rm0lkZ0s\n7 o0Z0apop\n6 bZpZpZ0Z\n5 ZpZ0O0Z0\n4 0OpO0Z0Z\n3 Z0A0ZNO0\n2 PZ0Z0OBO\n1 S0ZQZRJ0\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.20: White to move. AlphaZero is a pawn down.\nStockﬁsh 14. These evaluations are of course not tournament conditions but\nare just intended to give a more general hint how NNUE aﬀects positional\nevaluations.\n• Fritz 6 is happy to have snatched a pawn and thinks Black has an advantage\nof around -0.66.\n• Stockﬁsh 8 searching with around 7,500,000 nodes per second thinks Black\nhas a slight advantage of approximately -0.12.\n• Stockﬁsh 11 agrees while also searching with around 7,500,000 nodes per\nsecond and evaluates this at -0.13.\n• Stockﬁsh 12 searches with approximately 4,600,000 nodes per second —\nthe neural network evaluation takes time! Version 12 thinks this is an\nequal position (0.00) despite Black being a pawn up.\n• Stockﬁsh 14 (5,000,000 nodes per second) agrees with Stockﬁsh 12 and\nconsiders this position absolutely equal.\nThe second key position according to Daniel King is shown in Figure 4.21. Here\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n213\n8 0Z0s0akZ\n7 ZbZ0lpZp\n6 0ZpZpLpZ\n5 ZpO0O0ZP\n4 0OpZBZ0Z\n3 Z0A0Z0O0\n2 0Z0Z0OKZ\n1 Z0S0Z0Z0\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.21: White to move. AlphaZero is able to assess this endgame.\nthe ability of AlphaZero to assess the endgame is crucial. Let’s again check how\nour zoo of engines evaluates that position from Black’s point of view.\n• Fritz 6: I am doing great here and about to win. After all, I am a pawn up\n(-0.66).\n• Stockﬁsh 8: Ok, maybe White has some compensation for the pawn, but\nstill I am a pawn up. It’s equal then (0.00).\n• Stockﬁsh 11: I think something went wrong here (+0.25).\n• Stockﬁsh 12: You guys screwed up here. Like seriously. We are about to\nloose (+1.04).\n• Stockﬁsh 14: Yup, that deﬁnitely won’t work out in the end (+1.12).\nAs can be seen, the impact of the NNUE evaluation is quite remarkable. When\nwe compare Stockﬁsh version 11 and 12, they diﬀer in evaluation by 0.79 pawns\n— that’s not only quite a diﬀerence but it also changes the perspective on the\noutcome of the game completely!\nIt is also interesting to let Stockﬁsh 14 analyze Stockﬁsh 8’s moves and try to\n\n214\n4. MODERN AI APPROACHES - A DEEP DIVE\n8 0Z0Z0skZ\n7 ZbZ0apop\n6 0ZpZpZ0Z\n5 ZpO0O0Z0\n4 0OpZ0ZQZ\n3 Z0AqZ0O0\n2 0Z0Z0OBO\n1 Z0S0Z0J0\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.22: Black to move.\nﬁgure out where the biggest mistake happened. Turns out this is apparently\nthe position in Figure 4.22. Here Stockﬁsh 14 thinks that White has already an\nedge (+0.47) and suggests to play 22...Rd8 instead of 22...Ra8, the latter being\nassessed as a blunder (+1.00). Stockﬁsh 8 on the other hand thinks Black has a\nslight edge here (-0.35) and does not see any complications and of course prefers\n22...Ra8, and Stockﬁsh 11 comes to the same conclusion. Stockﬁsh 12 on the\nother hand seems to also suﬀer from not being able to look far enough ahead.\nIt prefers 22...Ra8 and evaluates this at 0.00 requiring a few more moves to\nunderstand what’s going on. Only after 23...Qd8 it realizes White’s advantage\n(+0.91). Stockﬁsh 8 on the other hand still thinks everything is perfectly ﬁne\nafter 23...Qd8 (-0.38).\nAnd Fritz 6 thinks at this point that it is only a matter of time until Black wins\n(-0.87). After all, Black is a pawn up! What could possibly go wrong?!\nAs we can see, Stockﬁsh made a huge leap forward in assessing long-term\npositional advantages. And this is mostly due to NNUE. Moreover we must\nremember that Stockﬁsh is running on rather low-end oﬃce computer, whereas\nAlphaZero requires an array of specially developed dedicated TPUs or at least\n\n4.6. EFFICIENTLY UPDATEABLE NEURAL NETWORKS (NNUE)\n215\nmodern GPUs.\n4.6.5\nSummary\nHandcrafting evaluation functions for chess has always been a tedious task with\nlots of trial and error. It is very diﬃcult to formulate our human understanding\nof chess into an objectively correct mathematical evaluation function. In the\npodcast “Let’s talk about chess”, Eric van Reem asked the author of the chess\nengine Shredder Stefan Meyer-Kahlen whether he ever got important feedback\nby chess grandmasters on what went wrong in games played by his engine.\nStefan said that one time when he inquired on what speciﬁcally was wrong\nabout a move he got the reply \"Nah you just don’t play like that\".\nIt is very diﬃcult to translate \"Nah you just don’t play like that\" into a mathe-\nmatical formula that computes evaluation scores for a position.\nSimilarly in his book about the Deep Blue matches with Kasparov [Hsu04],\nFeng-hsiung Hsu writes about the Deep Blue team’s struggle to create a good\nevaluation function despite the raw processing power of the machine. He excit-\nedly tells that they handcrafted an evaluation feature of their chess chips [Hsu99]\nfor “potentially open ﬁles”, i.e. ﬁles that are currently not open but could open\nin the future so that it is wise to put rooks there for the case that they open later\non. He also explains how this might have helped in one game against Garry\nKasparov.\nThe idea to use neural networks to automatically construct evaluation functions\nis not new. However before NNUE the only alpha-beta searcher that used a\nneural network for evaluation was DeepChess [DNW16], and that achieved\n“only” Grandmaster strength. While this was quite an achievement on its own,\nit was far from the super-human strength of current chess engines at the time.\nDespite all the smart design choice w.r.t. the network architecture and imple-\nmentation tricks using SIMD extension: After Motohiro Isozaki’s code patches\nwere oﬃcially integrated into Stockﬁsh, the search depth almost halved due to\nthe time spent for computing evaluation scores. Yet the strength of the engine\nimproved by 80 Elo points due to it’s better understanding of chess positions.\n\n216\n4. MODERN AI APPROACHES - A DEEP DIVE\nNevertheless the time penalty of course is indeed a weaknesses.\nThe latest\ndevelopment versions of Stockﬁsh use a hybrid approach where NNUE is only\napplied for quiet positions (where a deep positional understanding is required)\nand for open positions (where pawn snatching might be enough to win) the\nprevious handcrafted but quicker evaluation function is used. Stockﬁsh gained\nanother 20 Elo points by that.\n4.7\nFat Fritz 2\nFat Fritz 2 is based on Stockﬁsh and it’s NNUE implementation and created\nquite some controversy. In an oﬃcial blog post by the Lichess operators it was\ncalled “a ripoﬀ” 29. Stockﬁsh authors noted that they feel that customers buying\nFat Fritz 2 get very little added value for money30. Tord Romstad, author of the chess\nengine Glaurung and one of the ﬁrst developers of Stockﬁsh said on Twitter:\nI’m so disappointed in ChessBase for selling FF2. It might be legal, but it’s morally\nquestionable and shockingly unoriginal.31\nWe will not join this discussion but simply describe the changes that Chessbase\ndid, including their main artiﬁcial intelligence expert Albert Silver. They took\nthe source code of Stockﬁsh and32\n• changed the name of the ﬁle that stores the network weights to “Fat-\nFritz2_v1.bin”\n• changed one constant value that is used to scale the output of the neural\nnetwork into a centipawn value that is better understood by Stockﬁsh’s\nsearch function from 679 to 1210\n29https://lichess.org/blog/YCvy7xMAACIA8007/fat-fritz-2-is-a-rip-off, accessed July\n9th, 2021.\n30https://stockfishchess.org/blog/2021/statement-on-fat-fritz-2/, accessed July 9th,\n2021.\n31https://twitter.com/tordr/status/1359428424255823875, accessed July 9th, 2021.\n32https://github.com/official-stockfish/Stockfish/compare/550fed3343089357dc89ec\nf78ce8eb4b35bcab88...DanielUranga:faef72afbf10273ca8688a4ba1c7863426c93c6e, accessed\nJuly 9th, 2021.\n\n4.8. MAIA\n217\n• changed the authors to “Stockﬁsh Devs and Albert Silver (neural net-\nwork)” and the engine id string from “Stockﬁsh” to “Fat Fritz 2”.\n• The original input weights were not invariant to ﬂipping a position but to\nrotation, an artifact from Shogi. This was changed to ﬂipping and involved\naltering the value 63 to 56 at one position in the source code\n• the layer sizes of the network were changed as this: The size of the ﬁrst\nlayer was increased from 2 × 256 to 2 × 512, and the sizes of the second\nand third layers were decreased from 32 to 16\nOf course they also trained their own network. As of writing this, the resulting\nengine is strictly weaker than the current version of Stockﬁsh in the TCEC\nbenchmark33.\nThe retail version comes with the following note in the ﬁne print: “The Fat Fritz\n2 Chess Engine is based on the software Stockﬁsh. The Fat Fritz 2 Chess Engine\nand the software Stockﬁsh are licensed under the GNU General Public License\nVersion 3. You will receive further information during installation.”\nI will leave it up to you to decide whether you ﬁnd Fat Fritz 2 worth shelling\nout 80 Euro for, and whether you agree or disagree with the Lichess developers\nthat Fat Fritz 2 is “a ripoﬀ”. The previous section that described all the genius\nthat went into creating, designing and implementing NNUE provides the basis\nto understand and value the diﬀerences that ChessBase introduced.34\n4.8\nMaia\nHow to become really good at playing bad chess?\nIn 2016 I was still trying to get better at chess. I sort of have given up. Not\nthe hope of becoming better, but the actual training due to time reasons. But\neven back than I was very time-constrained.\nOne very eﬀective method of\nimproving is apparently to play long over-the-board games and later analyze\n33https://tcec-chess.com/\n34As of writing these lines, a lawsuit has been ﬁled by the Stockﬁsh developers against Chess-\nbase w.r.t. an alleged GPL violation.\n\n218\n4. MODERN AI APPROACHES - A DEEP DIVE\nthese games and one’s mistakes in great detail. Unfortunately, this is also very\ntime-consuming which is why I tried to play serious games against the computer.\nAfter all you can save approximately half the time compared to playing against\na human opponent as even with only one second for every move, the computer\nis more than strong enough.\nThe major drawback I stumbled upon however is that weakening the engine\ndown to my patzer level often resulted in very unnatural play. After a few\nmoves, the computer would out of the blue sack a bishop for a pawn without\nany compensation. After that sacriﬁce however it would play like a grandmaster\nuntil the end of the game.\nThe reason is how alpha-beta searchers are usually made weaker: The result of\na search will be a list of moves and their evaluations scores. Usually we sort this\nlist by the evaluation values and take the best move. To get a weaker move we\ncan have the user specify a handicap value in centipawns, or let the user select\na desired ELO value and approximate a corresponding centipawn value to that\nELO number. We can also slightly randomize the value within a given range so\nthat the engine plays with more variety. Then just subtract this handicap value\nfrom the evaluation value of the best move and ﬁnd a move with an evaluation\nthat is close to that result. Thus this move is roughly a handicap worse than the\nbest move.\nThe problem is now to make sure that this selected move makes any kind of\nsense. We can check for very obvious blunders by comparing the evaluation\nvalue of the best move with the evaluation value of the second best move. If the\nsecond move is say −5.0 and the best move is 1.0, then it is obviously a blunder\nand we should play the best move instead.\nBut sometimes the situation is not so clear. Consider the position shown in\nFigure 4.23. It is White to move. On my system the chess engine Fruit 2.1\nconsiders Bb3 the (obvious) best move with an evaluation score of 0.0. The\nsecond best move is d4 with an evaluation of roughly −1.5 and Fruit gives\nseveral lines all around −1.5 for a continuation, for example the line 7.b4 bxa4\n8.d5 Na5. Now imagine we are playing with a handicap of say 2.00, i.e. the\nengine is supposed to play with a handicap of roughly two pawns in each\n\n4.8. MAIA\n219\n8 rZblkZ0s\n7 Z0opapop\n6 pZnZ0m0Z\n5 ZpZ0o0Z0\n4 BZ0ZPZ0Z\n3 Z0Z0ZNZ0\n2 POPO0OPO\n1 SNAQS0J0\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.23: White to move. There is no reasonable alternative to 퐵푏3 that any\nchessplayer would play.\nposition. Is d4 a blunder? By our deﬁnition above probably not, the diﬀerence\nis “only” 1.5 centipawns and we are playing with an even higher handicap.\nI think there is no discussion required to understand that any human chess\nplayer, even a complete patzer like myself, would move his bishop. In other\nwords if a human player plays Black in this position as a training match against\na computer, any move except from Bb3 would feel very unnatural.\nWe can try to remedy this situation by e.g. checking whether material is sac-\nriﬁced and then always consider this a blunder, but even for such scenarios\nwe can ﬁnd examples where this leads to unnatural play. I mean even if we\nonly consider the very best move an engine gives, these moves are at times\nvery unnatural as watching any live commentary will prove, and moves are dis-\nmissed as “yeah but this is an engine move...”. If we add some kind of artiﬁcial\nweakening, the situation certainly does not improve.\nThen in January 2017 I stumbled upon the DeepChess paper [DNW16]. Need-\nless to say I was impressed. I was quick to send out the following email to Eli\nDavid, one of its authors:\n\n220\n4. MODERN AI APPROACHES - A DEEP DIVE\nFrom: Dominik Klein\nTo: Eli (Omid) David\nDear Mr. David,\nwith high interest I read your paper on DeepChess. [...]\nA frequent complaint from like myself is that computers are not\nvery good at playing _bad_. That is when trying to lower the\nchess playing ability of computer programs, the most simple\napproach is to limit search depth, or - what I've seen often\nand what seems to be implemented in Fritz - is that less good\nmoves are considered (i.e. via uci- multi pv), then some\nrandom centipawn value is chosen and within that threshold\na lesser good move is chosen.\nThese are very crude approaches and result in very un-human\nplay. I.e. for Fritz, the engine often sacs a piece early\non, and then plays like a grandmaster with very good technical skills.\nHave you ever considered adjusting the training goal in\nDeepChess to reflect human play? For example you took the\nCCRL games as an input to train your neural networks.\nInstead one could take a set of games played by amateurs\n(i.e. FICS with players rated, say, ELO 1500) as input\nin the hope that learning then will generate a position\nevaluation that is incorrect but amateur-like (i.e. overlooking\ntypical patterns, like pins, longer combinations, positional\nmisjudgements etc.)\nking regards\n- Dominik Klein\nto which he replied:\n\n4.8. MAIA\n221\nFrom: Eli (Omid) David\nTo: Dominik Klein\nCompletely agree with your observation. DeepChess may work\nwhen trained on datasets of players with different ELO points.\nI haven't had a chance to test it, but it is interesting to check...\nOk, this computer science researchers’ lingo is a language of its own. Let me\nquickly translate this into plain English for you.\nHi Eli,\nI have this super cool idea but neither the time nor the will\nto put any effort into executing it. Could you do it for me?\n- Dominik\nFrom: Eli (Omid) David\nTo: Dominik Klein\nNope, you are on your own.\n- Omid\nOr in other words: One should never underestimate the hours of work that\ngo into a research paper with \"just\" ten or twenty pages. Sketching out the\napproach, the programming, the nifty details — all that takes an incredible\namount of time. And on top of that are the computational resources required\nto train such a deep network.\nI was therefore very glad that someone independently came up with the idea and\nactually did put the eﬀort into executing it. The result is Maia, an experimental\nresearch engine [MSKA20].\nThe idea is quickly explained. Young et al. created a network with a similar\nnetwork architecture to the network of AlphaZero. But then they trained dif-\nferent networks, all by supervised learning. First they collected games from\nthe free internet chess platform Lichess. They sorted these games according to\nthe player’s ratings, e.g. from 1200 to 1299, from 1300 to 1399 and so on. They\nconsidered only those games where both players were in the same rating range.\nNext they trained the network on these games (approximately 12 million games\nfor each range) and tested move-matching accuracy on a number of games that\n\n222\n4. MODERN AI APPROACHES - A DEEP DIVE\nwere not used for training. Namely they queried their networks with a posi-\ntion from a real game and asked which move the network would select. Then\nthey checked whether this move was actually played in the game by the human\nplayer.\nMove matching accuracy varies with rating and among diﬀerent approaches,\nbut especially for lower rating levels the diﬀerence to existing engines is quite\noutstanding. For example for a rating level of 1100 their network achieves a\nmatching accuracy of more than 50 perecent, whereas Stockﬁsh is only in the 35\npercent range. For higher levels the diﬀerences get lower — best engine moves\ncoincide with the best moves humans play — but their network still outperforms\nLeela Chess Zero and Stockﬁsh by 5 percent move accuracy.\nSo can we immediately turn these networks into chess engines? Sort of. There\nis still the problem of blunders of course. Especially on lower levels. There is\nespecially the question on whether we should use these networks to search a\nposition, at least a limited number of plys. Here they decided to not conduct any\ntree search, the move probabilities that a network outputs are solely responsible\nto select the next move.\nThese ranges of networks are dubbed Maia. And what’s even more important\nis that they cooperated with the Lichess project and you can play against these\nnetworks online. So we can test how well Maia works in practice. As I am a bad\nchess player you should take my analysis with a grain of salt, but then again:\nIf there is one thing that I can understand, then it is to tell if my opponent is a\npatzer like myself. Here is a quick blitz game that I played as White against the\nbot Maia1. It was initially trained to play around 1100, but its current Lichess\nblitz rating is 1487.\nThe game started with 1.e4 e5 2.Nf3 Qe7 3.d4 d6 4.Bc4\nNow Maia answered with 4...Be6. While Philidor defense is a common open-\ning among amateurs, the move Be6 feels slightly awkward, albeit not entirely\nimpossible. First, most amateurs will remember the guideline “knights before\nbishops” w.r.t. development in the opening. Second, while 5.d5 is probably not\neven best in this position, it is a tempo move that is easily spotted even by an\namateur.\n\n4.8. MAIA\n223\n8 rmbZkans\n7 opo0lpop\n6 0Z0o0Z0Z\n5 Z0Z0o0Z0\n4 0ZBOPZ0Z\n3 Z0Z0ZNZ0\n2 POPZ0OPO\n1 SNAQJ0ZR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.24: Author vs. Maia, position after 4.Bc4\n5.d5 Bd7 6.Bg5 f6 7.Bh4\nYou can verify my claim that I am not a very skilled player here. The black-\nsquared bishop will now have a very lone time for the rest of the game\n7...g5 8.Bg3 h5 9.h4 g4 10.Nfd2 Bh6 11.Nc3 Bxd2+ 12.Qxd2 c6\nAccording to Nimzowitsch you should attack a pawn chain at the base, i.e. at\nit’s weakest point. But which amateur reads Nimzowitsch’ lavish rants? Thus I\nthink this is a very realistic mistake an amateur would make.\n13.dxc6 Bxc6 14.Nd5 Bxd5 15.Bxd5 Nc6 16.O-O-O Nd4 17.c3 Nb5 18.Bc4 a6\n19.Bxb5+ axb5 20.Qxd6 Qxd6 21.Rxd6 Rxa2\nI am not sure whether this feels realistic. Even amateurs get the notion of open\nﬁles and will identify that giving White the d-ﬁle is probably not a good idea.\nRd8 might be even worse, but I think it is the somewhat more natural move for\na weak player, instead of snatching the pawn on a2.\n22.Kc2 Ne7 23.Rhd1 O-O 24.Rd7 Nc6 25.Rxb7 b4\nWhile b4 might be objectively the best move here to get some kind of initiative,\n\n224\n4. MODERN AI APPROACHES - A DEEP DIVE\n8 rm0Zkans\n7 opobl0op\n6 0Z0o0o0Z\n5 Z0ZPo0A0\n4 0ZBZPZ0Z\n3 Z0Z0ZNZ0\n2 POPZ0OPO\n1 SNZQJ0ZR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.25: Author vs. Maia, position after 4.Bh4\n8 rm0ZkZns\n7 opobl0Z0\n6 0Z0o0o0Z\n5 Z0ZPo0Zp\n4 0ZBZPZpO\n3 Z0M0Z0A0\n2 POPL0OPZ\n1 S0Z0J0ZR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.26: Author vs. Maia, position after 12...c6\n\n4.8. MAIA\n225\n8 rZ0ZkZns\n7 ZpZ0Z0Z0\n6 0Z0S0o0Z\n5 ZpZ0o0Zp\n4 0Z0ZPZpO\n3 Z0O0Z0A0\n2 PO0Z0OPZ\n1 Z0J0Z0ZR\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.27: Author vs. Maia, position after 21.Rxd6\n8 0Z0Z0skZ\n7 ZRZ0Z0Z0\n6 0ZnZ0o0Z\n5 Z0Z0o0Zp\n4 0o0ZPZpO\n3 Z0O0Z0A0\n2 rOKZ0OPZ\n1 Z0ZRZ0Z0\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.28: Author vs. Maia, position after 25.b4\n\n226\n4. MODERN AI APPROACHES - A DEEP DIVE\n8 0Z0Z0skZ\n7 ZRZRZ0Z0\n6 0ZnZ0o0Z\n5 Z0Z0o0Zp\n4 0o0ZPZpO\n3 Z0O0Z0A0\n2 rOKZ0OPZ\n1 Z0Z0Z0Z0\na\nb\nc\nd\ne\nf\ng\nh\nFigure 4.29: Author vs. Maia, position after 26.Rdd7\nletting White invade the seventh rank with two rooks is also something I think\nmost weak player would not allow or at least try to ﬁght against.\n26.Rdd7 bxc3\nNow bxc3 feels very strange. There is a very basic mate threat that most human\namateurs will spot. It seems this is where the decision to let the network spit\nout moves without any kind of calculation backﬁres.\n27.Kxc3 Na5 28.Rg7+ Kh8 29.Rh7+ Kg8 30.Rbg7#\nAs you can see, Maia is not perfect yet. But on the other hand the game also\ndid not feel completely unrealistic and much closer to playing a human being\nthan most of the other artiﬁcially weakened engines out there. I am also quite\nsure that at some point the played games on Lichess will be used to improve\nthe approach.\nI encourage you to try it out on your own. Not only is it free, but you can play\nagainst and thus train with the Maia bots at any time anywhere as long as you\nhave an internet connection.\n\n4.9. CONCLUSION\n227\n4.9\nConclusion\nWe have seen how neural networks outperform handcrafted evaluation func-\ntions. They can be trained by reinforcement learning through self-play or by\nsupervised learning with existing games, but it seems that reinforcement learn-\ning will achieve better results in the end, as there is no inherent limit in the\namount of training data. On the other hand supervised learning with high\nquality training material naturally provides faster training; and since the com-\nputing power required to train deep networks for two-player games such as Go,\nShogi and chess is quite signiﬁcant, this advantage should not be underesti-\nmated.\nFor computer Go there seems to be currently no alternative than to create a very\ndeep network and train it. This will create a network with incredible positional\nunderstanding of Go — remember that even without any kind of search, the\nnetworks trained by Deepmind were very strong. Combined with Monte Carlo\nTree Search we get a computer Go engine with superhuman strength. For other\nmore complex strategy games — think computer games like StarCraft — with\nan incredible high branching factor this approach works as well [VBC+19]. This\nuniversality makes it also very appealing since we do not have to invest any\nkind of thought into encoding domain logic. Just throw the algorithms at your\nproblem and the reinforcement learning procedure will ﬁgure everything out.\nFor Shogi and chess the situation is not so clear. Whereas in Go we had no strong\n— as in Grandmaster strength — computer programs, the lower branching fac-\ntor of Shogi and chess allowed to implement alpha-beta searchers. And these\nalready outperformed humans. In chess the breakthrough was the DeepBlue\nmatch in 1997, and for Shogi there were some interesting computer matches\nbetween 2007 and 2014 ultimately drifting the edge towards the computers.\nTherefore it is only logical that combining alpha-beta search with neural net-\nworks is likely the most promising approach, as shown by the NNUE network\nstructure introduced by Yu Nasu.\nAlphaZero has been described as a game changer [SR19]. This is certainly true\nfor computer Go, but not necessarily for chess and Shogi. Sure, AlphaZero beat\nStockﬁsh convincingly, no doubt. But the ELO diﬀerence was comparatively\n\n228\n4. MODERN AI APPROACHES - A DEEP DIVE\nlow35. This is not meant to diminish the great achievement of AlphaZero, but\nit just hints that alpha-beta search is just a very powerful tool if the branching\nfactor of a game is not too high.\nWhat’s the future like, then? As industry demand is growing, a lot of resources\nare spent into making neural networks train and evaluate faster. More and more\nchip vendors donate silicon space to integrate neural network accelarators into\ntheir chips. Intel introduced a number of dedicated instructions dubbed Deep\nLearning Boost. Apple added a “Apple Neural Engine” to their M1 chip. And\nmore and more, fast GPUs are integrated into CPUs as well.\nMoreover there are also interesting developments on the software side. Apple\ncreated their ML Compute framework, and Microsoft created DirectML, an\nabstraction layer that enables potentially any graphic card to speed up deep\nlearning via Window’s DirectX interface. They even created an experimental\nport of tensorﬂow with DirectML support.\nAll this is unfortunately just not enough to create a deep neural network whose\nevaluation is fast enough to support a chess alpha-beta searcher. Therefore\nencoding and architecture quirks will remain, even though the current NNUE\narchitecture looks very much crafted for Shogi and it is likely that architectures\nmore suited to chess will be discovered.\nOver a larger time span though it seems likely that alpha-beta searchers with\nstrong neural networks for position evaluation will become feasible and prevail\nover competing approaches. With more and more computing power available\nwe will likely also see NNUE quirks vanish and more “standard” networks to\nappear. Nevertheless the computational aspect will remain very challenging\ndue to the amount of network queries that alpha-beta search induces.\nThe question of creating computer opponents that do not just have human\nor super-human strength but rather make enjoyable human-like opponents is\nrarely investigated. And if so the (applied) research with regard to entertain-\nment computing tends to go more in the direction of creating game models that\nhook up users and create patterns of addiction, like all those freemium strategy\n35Figure 1 in the AlphaZero paper [SHS+18] hints at less than 100 Elo points.\n\n4.9. CONCLUSION\n229\ngames or loot boxes; essentially masking gambling as computer games.\nThere is not much research on what makes games actually enjoyable for humans.\nIida et al. [SPI14] investigated why and how games progress and are reﬁned\nand relate that to the emotional impact they have on us. While it does not\ndirectly focus on creating a convincing human-like engine for chess or Shogi,\nit is nevertheless an interesting read. The development started by the team\naround Maia is therefore a refreshing step into a very interesting direction. This\nespecially holds if we want to create chess engines for analysis that are not just\nvery good in terms of raw ELO performance but rather play or suggest moves\nthat can be understood (and later on played if used for game preparation) by\nmere humans. It is nevertheless this diﬀerent kind of chess understanding that\nis the source of all the hype around AlphaZero.\n\n230\n4. MODERN AI APPROACHES - A DEEP DIVE\n\n5\nHexapawnZero\nIf you want a thing done well, do\nit yourself!\nHaruko Obokata\nIn order to get an intuition and a better understanding how AlphaZero works\nin practice, it is worthwhile to implement it by yourself. However as we dis-\ncussed, training a competitive network for chess takes enormous computational\nresources, and the assumption is that you do not have a small data center in\nyour basement. We will therefore have look at a much simpler game. Often Tic\nTac Toe is used for such experiments, but while it is simple, it has unfortunately\nnot even a remote resemblance of chess.\nThe most simple game that somewhat looks like chess is probably Hexapawn.\nIt was invented by popular science writer Martin Gardner in 1962 [Gar62] to\nshowcase simple game mechanics and game trees. The rules are very simple:\nHexapawn is played on a 3x3 board. Each side has three pawns on the ﬁrst\nand third row respectively as seen in Figure 5.1. Pawns move as usual with the\nexception of not moving two steps at the beginning — and there is obviously\nno en-passent. A player wins if he can queen a pawn. Moreover there is no\n231\n\n232\n5. HEXAPAWNZERO\n3 opo\n2 0Z0\n1 OPO\na\nb\nc\nFigure 5.1: Hexapawn. Initial position.\nstalemate or draws: If it is a player’s turn and he has no move, then he loses the\ngame.\nHexapawn is solved game, i.e. we know for each position who is going to win if\nhe plays optimally. In fact, it is not diﬃcult to verify that Black will always win\nthe game if he plays with an optimal strategy: White can start with a2, b2 or c2,\nbut the cases a2 and c2 are symmetrical. So let’s start with a2. Black answers\nwith bxa2, and then\n1. If White plays bxa2, Black wins with c2, as White has no move.\n2. If White plays b2 or c2, Black qeens with a1.\nNow if White starts the game with b2 then Black answers axb2.\n1. If White plays axb2, Black moves c2 and wins as White has no move.\n2. If White plays a2 or c2, Black queens with b1.\n3. If White plays cxb2, Black answers with c2. White’s only legal move is a2\nand Black queens with c1.\nIn other words this game is extremely trivial to solve and perfectly suited for\nour experiments.\nIn this section we will design a neural network akin to AlphaZero. It will accept\nstates of the board as input and output move probabilities that denote how good\na moves is resp. which move should be played, as well as a value that outputs\n\n5.1. THE NETWORK\n233\nthe likely outcome of the game from the current state.\nWe will train this network with two diﬀerent approaches and compare them.\nFirst, we will use supervised learning, i.e. we will generate training data. This\ntraining data consists of positions, the optimal move in a given position and the\n(calculated) outcome of the game in that position — note that such an outcome\nis known.\nSecond we will apply a “Zero”-like approach. That is we will implement the\ntraining loop of AlphaZero using Monte Carlo Tree Search in combination with\nself-play and reinforcement learning.\nWe will verify the eﬀectiveness of our training by comparing the power of the\ntrained supervised and \"Zero\"-network against an opponent who plays just\nrandom moves. A well-trained network that plays Black should beat such an\nopponent in the vast majority of all cases.\nLet’s start with the network architecture, i.e. the network itself, as well as net-\nwork input and output.\n5.1\nThe Network\nNetwork Architecture\nThe network architecture is quite simplistic: We have\nan input bit vector of size 21. This input encodes a Hexapawn positions as well\nas the current turn, cf. below. The network itself has ﬁve hidden layers, namely\nconnected layers each of size 128 with rectiﬁed linear activation.\nNote that a network of this size is probably a complete overkill for such a simple\ngame, but it is still computationally very cheap. Note also that we completely\nforget about network constructs such as convolutional layers or the like. These\nare required if we want to learn patterns and train our network to abstract\nstrategic information and create a positional understanding. However the game\ntree of Hexapawn is so trivial that there really is no positional understanding\nrequired. It suﬃces if our network simply remembers all board states and the\ncorresponding correct move and game outcome. This maybe puts the joy out\nof creating HexapawnZero, but it also makes training the network very simple.\n\n234\n5. HEXAPAWNZERO\nThe output of the network has two heads. The policy head is a fully connected\nlayer with softmax activation and 28 outputs. These encode all possible moves\nof both the white and black player. Each output provides the move probability\nfor the corresponding move and all outputs sum up to one.\nThe value head is just one output with tanh activation.\nIt provides a value\nbetween -1 and 1 that denotes who is going to win in that position if both\nplayers play optimally.\nFor the policy head we will use categorical crossentropy to measure the diﬀer-\nence between a training example and the output of the network. For the value\nhead we will employ the mean squared error1. The overall loss of our network\nis then simply deﬁned as the sum of the crossentropy and the mean squared\nerror. We will train the network using good old gradient descent. It is straight\nforward to put this into Python code, create the network and let keras initialize\nit with random initial values. We will then save the current (untrained) state of\nthe network for our experiments.\nListing 5.1: Hexapawn Network\n1 inp = Input ((21,))\n2\n3 l1 = Dense (128, activation=’relu ’)(inp)\n4 l2 = Dense (128, activation=’relu ’)(l1)\n5 l3 = Dense (128, activation=’relu ’)(l2)\n6 l4 = Dense (128, activation=’relu ’)(l3)\n7 l5 = Dense (128, activation=’relu ’)(l4)\n8\n9 policyOut = Dense(28, name=’policyHead ’, activation=’softmax ’)(l5)\n10 valueOut = Dense(1, activation=’tanh ’, name=’valueHead ’)(l5)\n11\n12 bce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n13 model = Model(inp , [policyOut ,valueOut ])\n14 model.compile(optimizer = ’SGD ’,\n15\nloss={’valueHead ’ : ’mean_squared_error ’,\n16\n’policyHead ’ : bce})\n17\n18 model.save(’random_model.keras ’)\n1cf. Chapter 2\n\n5.1. THE NETWORK\n235\n3 Zpo\n2 pO0\n1 O0O\na\nb\nc\nFigure 5.2: Hexapawn. White to move.\nNetwork Input\nWe will encode the position of the white and black pawns\nas bitvectors. For that we will iterate twice through the board; ﬁrst for the\nwhite pawns and then for the black pawns. We go through the position starting\nwith a3 and then moving to c3, next a2 to c2 and ﬁnally a1 to c1. Finally after\nencoding the pawn positions we will add three ones if it is White to move, and\nthree zeros otherwise. An example of the encoding is shown in Figure 5.3, here\nthis results in the 21 bit vector\n000\n010\n101\n011\n100\n000\n111.\nNetwork Output\nWe simply enumerate all possible moves (of both Black\nand White) and associate each move with a unique number.\nEach number\ncorresponds to one output of the policy head of the network.\nWe start with white moves forward and map them from 0 to 5:\n푎1 −푎2 : 0\n푏1 −푏2 : 1\n푐1 −푐2 : 2\n푎2 −푎3 : 3\n푏2 −푏3 : 4\n푐2 −푐3 : 5\n\n236\n5. HEXAPAWNZERO\nThe same approach is used for the black moves forward:\n푎3 −푎2 : 6\n푏3 −푏2 : 7\n푐3 −푐2 : 8\n푎2 −푎1 : 9\n푏2 −푏1 : 10\n푐2 −푐1 : 11\nNext we consider all possible captures by White:\n푎1 −푏2 : 12\n푏1 −푎2 : 13\n푏1 −푐2 : 14\n푐1 −푏2 : 15\n푎2 −푏3 : 16\n푏2 −푎3 : 17\n푏2 −푐3 : 18\n푐2 −푏3 : 19\nand all possible pawn captures by Black:\n푎3 −푏2 : 20\n푏3 −푎2 : 21\n푏3 −푐2 : 22\n푐3 −푏2 : 23\n푎2 −푏1 : 24\n푏2 −푎1 : 25\n푏2 −푐1 : 26\n푐2 −푏1 : 27\nOf course not all moves are possible in each position and during training or\nsearch the network may output illegal moves with a probability larger than 0.\nIn such a case we have to mask out these probabilities.\n5.2\nGame Logic\nEncoding the game logic requires some boiler plate code but is not too diﬃcult.\nAt the center is the object Board which encodes all information about a current\nposition. The current state is stored in the list board. Index 0 here corresponds\nto the square a3, index 2 is c3 and so on. Accordingly we have a function that\nsets the starting position by setting the indices of board with the corresponding\nvalues.\nListing 5.2: Board Initialization\n1 class Board():\n2\n\n\n238\n5. HEXAPAWNZERO\n3\nEMPTY = 0\n4\nWHITE = 1\n5\nBLACK = 2\n6\n7\ndef __init__(self):\n8\nself.turn = self.WHITE\n9\nself.outputIndex = {}\n10\nself.board = [self.EMPTY , self.EMPTY , self.EMPTY ,\n11\nself.EMPTY , self.EMPTY , self.EMPTY ,\n12\nself.EMPTY , self.EMPTY , self.EMPTY ]\n13\n14\ndef setStartingPosition(self):\n15\nself.board = [self.BLACK , self.BLACK , self.BLACK ,\n16\nself.EMPTY , self.EMPTY , self.EMPTY ,\n17\nself.WHITE , self.WHITE , self.WHITE ]\nA move is encoded as a tuple where the ﬁrst element is the index of the source-\nsquare and the second element is the index of the target square. To get the\noutput index for a given move we create the python dictionary outputIndex.\nThis dictionary stores information such that the string representation of a move\nyields the number of the output of the neural network. For that we use the\nconvenience function getNetworkOutputIndex.\nListing 5.3: Output Indices\n1 # white forward moves\n2 self.outputIndex [\"(6, 3)\"] = 0\n3 self.outputIndex [\"(7, 4)\"] = 1\n4 self.outputIndex [\"(8, 5)\"] = 2\n5 self.outputIndex [\"(3, 0)\"] = 3\n6 self.outputIndex [\"(4, 1)\"] = 4\n7 self.outputIndex [\"(5, 2)\"] = 5\n8 ...\n9 def getNetworkOutputIndex(self , move):\n10\nreturn self.outputIndex[str(move)]\nApplying a move simply means setting the value of the target square to that\nof the source square, and then setting the source square to empty. Moreover\nwe have to change the turn from White to Black (or vice versa). Also, we reset\n\n5.2. GAME LOGIC\n239\nself.legal_moves — this is the cache that contains all legal moves of the current\nposition. As applying a move changes the position, we will need to re-compute\nall legal moves the next time they are required.\nListing 5.4: Applying a Move\n1 def applyMove(self , move):\n2\nfromSquare = move [0]\n3\ntoSquare = move [1]\n4\nself.board[toSquare] = self.board[fromSquare]\n5\nself.board[fromSquare] = self.EMPTY\n6\nif(self.turn == self.WHITE):\n7\nself.turn = self.BLACK\n8\nelse:\n9\nself.turn = self.WHITE\n10\nself.legal_moves = None\nTo generate moves we iterate over all squares of the board and check whether\nthe square is occupied with a pawn that has the color of the current player. The\ntarget square of a forward move of a pawn is +3 resp. −3. If that square is\nempty, a forward pawn move is a legal move.\nCapture moves are slightly more intricate.\nFor those we create two arrays,\nnamely one array that contains target squares of white pawn captures and one\nfor black pawn captures. For example WHITE_PAWN_CAPTURES[4] returns [0,2],\nsince from source square 4 (square b2) there are two potential capture squares,\nnamely 0 (square a3) and 2 (square c3). Given a source square, we use these\narrays to get all potential capture squares. If a capture square contains a pawn\nof the enemy color, we have a legal pawn capture move.\nAll moves are accumulated in self.legal_moves and returned. Here the array\nself.legal_moves acts as some kind of cache, i.e. once we computed all legal\nmoves, we just return this list instead of re-computing all legal moves over and\nover again.\nListing 5.5: Move Generation\n1 def __init__(self):\n2\n...\n\n240\n5. HEXAPAWNZERO\n3\nself.WHITE_PAWN_CAPTURES = [\n4\n[],\n5\n[],\n6\n[],\n7\n[1],\n8\n[0,2],\n9\n[1],\n10\n[4],\n11\n[3,5],\n12\n[4]\n13\n]\n14\nself.BLACK_PAWN_CAPTURES = [\n15\n[4],\n16\n[3,5],\n17\n[4],\n18\n[7],\n19\n[6,8],\n20\n[7],\n21\n[],\n22\n[],\n23\n[]\n24\n]\n25\n26 def generateMoves(self):\n27\nif(self.legal_moves == None):\n28\nmoves = []\n29\nfor i in range(0, 9):\n30\nif(self.board[i] == self.turn):\n31\nif(self.turn == self.WHITE):\n32\n# check if we can move one square up\n33\ntoSquare = i - 3\n34\nif(toSquare >= 0):\n35\nif(self.board[toSquare] == self.EMPTY):\n36\nmoves.append ((i, toSquare))\n37\npotCaptureSquares = self.WHITE_PAWN_CAPTURES[i]\n38\nfor toSquare in potCaptureSquares:\n39\nif(self.board[toSquare] == self.BLACK):\n40\nmoves.append ((i, toSquare))\n41\nif (self.turn == self.BLACK):\n42\ntoSquare = i + 3\n43\nif(toSquare < 9):\n44\nif (self.board[toSquare] == self.EMPTY):\n\n5.2. GAME LOGIC\n241\n45\nmoves.append ((i, toSquare))\n46\npotCaptureSquares = self.BLACK_PAWN_CAPTURES[i]\n47\nfor toSquare in potCaptureSquares:\n48\nif (self.board[toSquare] == self.WHITE):\n49\nmoves.append ((i, toSquare))\n50\nself.legal_moves = moves\n51\nreturn self.legal_moves\nThe function isTerminal is used to check if a board is in a terminal position, i.e.\nif we have a winner. The function returns (False, None) if the position is not\nterminal, and it returns (True, Winner) where Winner is either Black or White\nif the position is terminal. A position is terminal if either Black or White have\nqueened a pawn, or if it is White’s turn (Black’s turn) and he has no legal move.\nListing 5.6: Detecting Terminal Position\n1 def isTerminal(self):\n2\nwinner = None\n3\nif(self.board [6] == Board.BLACK or\n4\nself.board [7] == self.BLACK or\n5\nself.board [8] == self.BLACK):\n6\nwinner = self.BLACK\n7\nif (self.board [0] == self.WHITE or\n8\nself.board [1] == self.WHITE or\n9\nself.board [2] == self.WHITE):\n10\nwinner = self.WHITE\n11\nif(winner != None):\n12\nreturn (True , winner)\n13\nelse:\n14\nif(len(self.generateMoves ()) == 0):\n15\nif(self.turn == Board.WHITE):\n16\nreturn (True , Board.BLACK)\n17\nelse:\n18\nreturn (True , Board.WHITE)\n19\nelse:\n20\nreturn (False , None)\nEncoding a Hexapawn position to a bitvector as input for the neural network is\nalso straight forward. We create an empty list and then iterate over the whole\n\n242\n5. HEXAPAWNZERO\nboard. Whenever we encounter a white pawn, we add a one to the list, and\na zero otherwise. We do the same for the black pawns. Finally we add three\nones2 if it is White to move, and three zeros otherwise.\nListing 5.7: Creating Network Input\n1 def toNetworkInput(self):\n2\nposVec = []\n3\nfor i in range (0,9):\n4\nif(self.board[i] == Board.WHITE):\n5\nposVec.append (1)\n6\nelse:\n7\nposVec.append (0)\n8\nfor i in range (0,9):\n9\nif(self.board[i] == Board.BLACK):\n10\nposVec.append (1)\n11\nelse:\n12\nposVec.append (0)\n13\nfor i in range (0,3):\n14\nif(self.turn == Board.WHITE):\n15\nposVec.append (1)\n16\nelse:\n17\nposVec.append (0)\n18\nreturn posVec\n5.3\nSupervised Learning\nOne main challenge in supervised learning is how to get high quality training\ndata.\nUnfortunately there is no professional online Hexapawn league from\nwhere we could snatch game data. As the game is trivial, we could simply\nwrite down all possible game positions together with the best move and game\noutcome down ourselves; but despite the comparatively small game, that’s still\na tedious amount of work — there are 188 possible distinct game states as we\nwill discover soon.\n2Probably one bit would suﬃce for the network to recognize turns - but we really do not have\nto consider input size or performance issues for a game with such low game complexity.\n\n5.3. SUPERVISED LEARNING\n243\nWe have already seen in Chapter 3 how minimax can be used to solve games,\nand that’s what we are going to employ. We will start from the initial position\nand then try out every possible possible combination of moves until a game\nends. In each position we will run minimax to get the best move and the game\noutcome. Since the game tree is of such shallow depth, minimax has no trouble\nsearching until a terminal state is encountered.\nGenerating Training Data\nFirst we deﬁne a function getBestMove() that takes\nas input a Hexapawn gamestate.\nThe function then iterates through every\npossible legal move in that position, applies the move and runs minimax on the\nresulting position. It compares all outcomes to ﬁnd the best possible move in\nthat position. The game outcome as returned by minimax is also recorded. Note\nthat when there are two or more possible moves that are best (i.e. winning) then\nonly one of them is returned. But to play optimally that is all that is required.\nWe also create a recursive3 function visitNodes() that takes as input a Hexa-\npawn gamestate. This function ﬁrst checks if the encountered position is termi-\nnal. If so we return, as a terminal position is useless as training data (there is no\nbest move to play anymore). Otherwise the function generates all legals moves,\napplies each move, runs getBestMove() on the position and creates the training\ndata. The training data consist of triples:\n1. The game position itself, encoded as a suitable bitvector as input for the\nneural network\n2. A move probability vector of size 28 corresponding to the output of the\npolicy head of the neural network. Here we set all probabilities to 0 except\nfor the best move for which we set the probability to one.\n3. A value of either 1 or -1 (win or lose from the perspective of the White\nplayer) that corresponds to the desired output of the value head of the\nnetwork.\nThese three features of the training data are stored separately in the lists\npositions, moveProbs and outcomes. We also put the currently encountered\n3i.e. a function that calls itself\n\n244\n5. HEXAPAWNZERO\nposition into a Python dictionary in order to later count the amount of distinct\npositions that we encountered. Afterwards the function applies each move and\ncalls itself recursively to generate training data for the subsequent positions.\nFinally we create a board with the initial position and run visitNodes() on that\nposition. We save the generated training data as numpy arrays. As output by\nthe program (the length of the dictionary), there are 188 distinct positions. We\nnow have one training example for every possible game state. That should be\nenough to train our network!\nListing 5.8: Creating Training Data\n1 def getBestMoveRes(board):\n2\nbestMove = None\n3\nbestVal = 1000000000\n4\nif(board.turn == board.WHITE):\n5\nbestVal =\n-1000000000\n6\nfor m in board.generateMoves ():\n7\ntmp = copy.deepcopy(board)\n8\ntmp.applyMove(m)\n9\nmVal = minimax(tmp , 30, tmp.turn == board.WHITE)\n10\nif(board.turn == board.WHITE and mVal > bestVal):\n11\nbestVal = mVal\n12\nbestMove = m\n13\nif(board.turn == board.BLACK and mVal < bestVal):\n14\nbestVal = mVal\n15\nbestMove = m\n16\nreturn bestMove , bestVal\n17\n18 positions = []\n19 moveProbs = []\n20 outcomes = []\n21\n22 gameStates = {}\n23\n24 def visitNodes(board):\n25\ngameStates[str(board)] = 1\n26\nterm , _ = board.isTerminal ()\n27\nif(term):\n28\nreturn\n29\nelse:\n\n5.3. SUPERVISED LEARNING\n245\n30\nbestMove , bestVal = getBestMoveRes(board)\n31\npositions.append(board.toNetworkInput ())\n32\nmoveProb = [ 0 for x in range (0,28) ]\n33\nidx = board.getNetworkOutputIndex(bestMove)\n34\nmoveProb[idx] = 1\n35\nmoveProbs.append(moveProb)\n36\nif(bestVal > 0):\n37\noutcomes.append (1)\n38\nif(bestVal == 0):\n39\noutcomes.append (0)\n40\nif(bestVal < 0):\n41\noutcomes.append (-1)\n42\nfor m in board.generateMoves ():\n43\nnext = copy.deepcopy(board)\n44\nnext.applyMove(m)\n45\nvisitNodes(next)\n46\n47 board = Board()\n48 board.setStartingPosition ()\n49 visitNodes(board)\n50 print (\"# of reachable distinct positions :\")\n51 print(len(gameStates))\n52 np.save(\" positions\", np.array(positions))\n53 np.save(\" moveprobs\", np.array(moveProbs))\n54 np.save(\" outcomes\", np.array(outcomes))\nTraining and Evaluation\nTraining itself is almost trivial. We have already\ncreated the network architecture and saved a copy of the network with randomly\ninitialized weights. All that remains to do now is to load that initial network,\nthe training data and train it; here we use 512 epochs with a batch size of 16.\nThis is probably a slight overkill, but due to the comparatively low amount of\ntraining data the network is still trained in seconds, even on a CPU. To get some\nmore insight we can print out the shape of the training data. As we can see,\nthere were 118 (non-terminal) game states that we encountered during training\ndata generation.\nListing 5.9: Supervised Learning\n1 model = keras.models.load_model (\"../ common/random_model.keras\")\n\n246\n5. HEXAPAWNZERO\n2\n3 inputData = np.load (\"../ minimax/positions.npy\")\n4 policyOutcomes = np.load (\"../ minimax/moveprobs.npy\")\n5 valueOutcomes = np.load (\"../ minimax/outcomes.npy\")\n6\n7 print(policyOutcomes.shape)\n8\n9 model.fit(inputData ,[ policyOutcomes , valueOutcomes], epochs =512,\nbatch_size =16)\n10 model.save(’supervised_model.keras ’)\nIt remains now to see how successful we were with our training. A simple way\nto evaluate the network’s performance is to have it play against a player that\nrandomly selects moves. Here we create two functions. The ﬁrst is rand_vs_net.\nIt takes as input a game state and then applies moves until the game ends\nin a terminal state.\nIf it is White to move, we generate all legal moves use\nrandom.randint to randomly select one of them. If it is Black to move, we query\nthe network for move probabilities. The network might want to play illegal\nmoves, so we mask out probabilities for illegal moves by setting them to 0.\nTechnically we actually do it the opposite way: We create a vector of zeros,\nand then assign the network’s probabilities to that vector but only for legal\nmoves. Next we iterate through all legal moves and by using the corresponding\nmove index for that move we ﬁnd the maximum probability as output by the\nnetwork. This is then the move we apply. If we reach a ﬁnal position, we return\nthe winner.\nWe also crate a second function rand_vs_rand where both White and Black play\nagainst each other by randomly selecting moves.\nNext we play a hundred games twice with each function. Here we create a board\nwith the initial position and let the random player and the network compete\nagainst each other as well as the two random players. Each time we record\nthe number of wins and losses. Note that the game was solved and Black will\nalways win if he plays optimally.\nIt is also worthwhile to let the random player play against the untrained network\nwith randomly initialized weights. While it is unlikely, we could also have been\n\n5.3. SUPERVISED LEARNING\n247\njust lucky, i.e. the randomly selected initialized weights were already such that\nthe network chooses an optimal move in every positions or a lot of positions —\nin other words it plays better than randomly selecting moves.\nListing 5.10: Network Evaluation\n1 model = keras.models.load_model (\" supervised_model.keras\")\n2 # model = keras.models.load_model (\"../ common/random_model.keras\")\n3\n4 def fst(a):\n5\nreturn a[0]\n6\n7 def rand_vs_net(board):\n8\nrecord = []\n9\nwhile(not fst(board.isTerminal ())):\n10\nif(board.turn == Board.WHITE):\n11\nmoves = board.generateMoves ()\n12\nm = moves[random.randint(0, len(moves) -1)]\n13\nboard.applyMove(m)\n14\nrecord.append(m)\n15\ncontinue\n16\nelse:\n17\nq = model.predict(np.array([board.toNetworkInput ()]))\n18\nmasked_output = [ 0 for x in range (0,28)]\n19\nfor m in board.generateMoves ():\n20\nm_idx = board.getNetworkOutputIndex(m)\n21\nmasked_output[m_idx] = q[0][0][ m_idx]\n22\nbest_idx = np.argmax(masked_output)\n23\nsel_move = None\n24\nfor m in board.generateMoves ():\n25\nm_idx = board.getNetworkOutputIndex(m)\n26\nif(best_idx == m_idx):\n27\nsel_move = m\n28\nboard.applyMove(sel_move)\n29\nrecord.append(sel_move)\n30\ncontinue\n31\nterminal , winner = board.isTerminal ()\n32\nreturn winner\n33\n34 def rand_vs_rand(board):\n35\nwhile(not fst(board.isTerminal ())):\n36\nmoves = board.generateMoves ()\n\n248\n5. HEXAPAWNZERO\n37\nm = moves[random.randint(0, len(moves) -1)]\n38\nboard.applyMove(m)\n39\ncontinue\n40\nterminal , winner = board.isTerminal ()\n41\nreturn winner\n42\n43 whiteWins = 0\n44 blackWins = 0\n45 draws = 0\n46 for i in range (0 ,100):\n47\nboard = Board()\n48\nboard.setStartingPosition ()\n49\nwinner = rand_vs_net(board)\n50\nif(winner == Board.WHITE):\n51\nwhiteWins += 1\n52\nif(winner == Board.BLACK):\n53\nblackWins += 1\n54 all = whiteWins + blackWins + draws\n55 print(\"Rand vs Supervised Network: \"+str(whiteWins/all) +\n56\n\"/\"+ str(blackWins/all))\n57\n58\n59 whiteWins = 0\n60 blackWins = 0\n61 draws = 0\n62 for i in range (0 ,100):\n63\nboard = Board()\n64\nboard.setStartingPosition ()\n65\nwinner = rand_vs_rand(board)\n66\nif(winner == Board.WHITE):\n67\nwhiteWins += 1\n68\nif(winner == Board.BLACK):\n69\nblackWins += 1\n70 all = whiteWins + blackWins + draws\n71 print(\"Rand vs Rand Network: \"+str(whiteWins/all) +\n72\n\"/\"+ str(blackWins/all))\nIf we run the script we will get an output similar to Table 5.14:\n4Running this on your computer, your numbers might vary slightly due to the randomly\ninitialization.\n\n5.4. “ZERO-LIKE” REINFORCEMENT LEARNING\n249\nTable 5.1: Supervised Network Evaluation\nMatchup: Random Player ...\nWhite Wins\nBlack Wins\nvs Trained Network\n0%\n100%\nvs Untrained Network\n34%\n66%\nvs Random Player\n62%\n38%\nAs we can see, the randomly initialized network indeed plays slightly better\nthan just randomly selecting moves. Our trained network however wins all\ngames. It seems the training worked out pretty well.\n5.4\n“Zero-like” Reinforcement Learning\nWe now pursue a \"Zero\"-like approach to train our network, that is reinforce-\nment learning in combination with Monte Carlo Trees Search.\nThe general\noutline is:\n• initialize the network with random weights\n• self-play a number of games. At each move, MCTS is used to get move\nprobabilities over all legal moves in that position.\nThe MCTS itself is\nguided by the network which is queried during MCTS for move probabil-\nities and game outcomes. Apply the best move and continue until we end\nup in a ﬁnal state of the game. We can also try out other moves than the\nbest one in order to explore and get a broader knowledge about the game.\n• We gathered a number of positions together with move probabilities and\nthe ﬁnal outcome of the game. We use these to train the network and\nrepeat where we use the improved network in the next round of self-play\ngames for MCTS.\nThe main task is now to implement MCTS such that it is guided by the network,\nand to implement the training loop. To evaluate the trained network we will\nuse the same approach as for supervised learning, i.e. we will let the network\ncompete against a random player. But ﬁrst let’s start with MCTS.\n\n250\n5. HEXAPAWNZERO\nMonte Carlo Tree Search\nOur MCTS approach follows closely the algorithm\nthat is sketched out by AlphaZero and AlphaGoZero. We distinguish between\nnodes and edges.\nListing 5.11: MCTS Nodes and Edges\n1 class Edge():\n2\ndef __init__(self , move , parentNode):\n3\nself.parentNode = parentNode\n4\nself.move = move\n5\nself.N = 0\n6\nself.W = 0\n7\nself.Q = 0\n8\nself.P = 0\n9\n10 class Node():\n11\ndef __init__(self , board , parentEdge):\n12\nself.board = board\n13\nself.parentEdge = parentEdge\n14\nself.childEdgeNode = []\n15\n16\ndef expand(self , network):\n17\nmoves = self.board.generateMoves ()\n18\nfor m in moves:\n19\nchild_board = copy.deepcopy(self.board)\n20\nchild_board.applyMove(m)\n21\nchild_edge = Edge(m, self)\n22\nchildNode = Node(child_board , child_edge)\n23\nself.childEdgeNode.append ((child_edge ,childNode))\n24\nq = network.predict(np.array([self.board.toNetworkInput ()])\n)\n25\nprob_sum = 0.\n26\nfor (edge ,_) in self.childEdgeNode:\n27\nm_idx = self.board.getNetworkOutputIndex(edge.move)\n28\nedge.P = q[0][0][ m_idx]\n29\nprob_sum += edge.P\n30\nfor edge ,_ in self.childEdgeNode:\n31\nedge.P /= prob_sum\n32\nv = q[1][0][0]\n33\nreturn v\n34\n35\ndef isLeaf(self):\n\n5.4. “ZERO-LIKE” REINFORCEMENT LEARNING\n251\n36\nreturn self.childEdgeNode == []\nA node keeps the current state of the board. There are outgoing edges from\nnodes, and one edge corresponds to a particular move. During expansion of a\nnode we ﬁrst generate all legal moves and for each move an edge that contains\nthe information. Each edge then leads to a new node that keeps the board\nposition that results from applying the move to the current board position.\nDuring expansion we also query the neural network with the current board\nposition. The policy head of the network provides move probabilities for all\nthe possible moves, and the value head provides an evaluation of the position.\nWe store each move probability in the corresponding edge.\nWe return the\nevaluation of the value head to the caller of the expansion function, so that\nMCTS can use this information for backpropagation. MCTS also needs to know\nwhether a node is expanded or not. This is trivial to check by testing if there\nare any childs at all.\nBeside the move and a link to the parent node, an edge stores the four important\nvalues N, W, Q, and P. As described in Chapter 4, N counts how many times\nthe node was visited, W is the total reward, Q is the total reward divided by the\nnumber of times we visited that node, and P is the initial probability that was\nprovided by the network during expansion.\nNote that we store these values as absolute values and not from the perspective\nof the player whose turn it is. Thus if the position is favourable for Black, the\ntotal reward will be negative.\nHaving classes built that represent edges and nodes, we are ready to implement\nMonte Carlo Tree Search. During creation of our MCT searcher we need to\nset several parameters: The network that will be used during the search, the\nroot node where we will start our search as well as parameters 휏that is used\nto derive the ﬁnal move probabilities and 푐puct that is used during selection for\ncomputation of the UCT value (cf. Chapter 4).\nRemember that MCTS consists of four steps: Selection, Expansion, Evaluation\nand Backpropagation. Let’s start with selection. Here we move from the root\n\n252\n5. HEXAPAWNZERO\nnode down to a leaf node. If we are at a leaf node we are ﬁnished and return\nthat node. Otherwise we will consider all possible move in that position, i.e. all\nedges.\nListing 5.12: MCTS Class\n1 class MCTS():\n2\ndef __init__(self , network):\n3\nself.network = network\n4\nself.rootNode = None\n5\nself.tau = 1.0\n6\nself.c_puct = 1.0\nWe compute the UCT value for each and then select the edge that maximizes\nthe sum of the total reward of that edge and the UCT value, i.e. edge.Q +\nuctValue(edge). Remember that the UCT value was computed as\n푐puct · 푃·\npÍ\n푚′ 푁푚′\n1 + 푁푚\nNote that Í\n푚′ 푁푚′ is just the parent visit count.\nNo matter if it is Black or White to play, we seek the best move from the\nperspective of the current player. Since we store rewards independent on whose\nturn it is, we need to invert the reward value Q, as we seek the child that\nmaximizes the sum, but the best possible reward Q for Black is negative. Last, if\nthere are several childs with the same value of the total reward plus UCT value,\nthen we randomly select one child in order to have more exploration. Finally\nwe move to the selected child and repeat.\nListing 5.13: MCTS Selection\n1 def uctValue(self , edge , parentN):\n2\nreturn self.c_puct * edge.P * (math.sqrt(parentN) / (1+ edge.N))\n3\n4 def select(self , node):\n5\nif(node.isLeaf ()):\n6\nreturn node\n7\nelse:\n\n5.4. “ZERO-LIKE” REINFORCEMENT LEARNING\n253\n8\nmaxUctChild = None\n9\nmaxUctValue =\n-100000000.\n10\nfor edge , child_node in node.childEdgeNode:\n11\nuctVal = self.uctValue(edge , edge.parentNode.parentEdge\n.N)\n12\nval = edge.Q\n13\nif(edge.parentNode.board.turn == Board.BLACK):\n14\nval = -edge.Q\n15\nuctValChild = val + uctVal\n16\nif(uctValChild > maxUctValue):\n17\nmaxUctChild = child_node\n18\nmaxUctValue = uctValChild\n19\nallBestChilds = []\n20\nfor edge , child_node in node.childEdgeNode:\n21\nuctVal = self.uctValue(edge , edge.parentNode.parentEdge\n.N)\n22\nval = edge.Q\n23\nif(edge.parentNode.board.turn == Board.BLACK):\n24\nval = -edge.Q\n25\nuctValChild = val + uctVal\n26\nif(uctValChild == maxUctValue):\n27\nallBestChilds.append(child_node)\n28\nif(maxUctChild == None):\n29\nraise ValueError (\"could not identify child with best\nuct value\")\n30\nelse:\n31\nif(len(allBestChilds) > 1):\n32\nidx = random.randint(0, len(allBestChilds) -1)\n33\nreturn self.select(allBestChilds[idx])\n34\nelse:\n35\nreturn self.select(maxUctChild)\nWe can combine expansion and evaluation in one function. If a node is terminal\nwe do not to query the network for an evaluation value and can instead take the\ngame result and propagate it as the reward back in the tree up to the root node.\nIf the node is not terminal we expand the node, thereby yielding the evaluation\nvalue of the network for that node and propagate it back in the tree.\nBackpropagation is straight forward. For each edge we increase the visit count,\nadd the reward to the total reward 푊and update the mean reward by dividing\n\n254\n5. HEXAPAWNZERO\nthe total reward through the visit count. We then check if the edge has a parent\nnode (and subsequent parent edge) and repeat backpropagation. If we instead\nreached the root we stop.\nListing 5.14: MCTS Expansion\n1 def expandAndEvaluate(self , node):\n2\nterminal , winner = node.board.isTerminal ()\n3\nif(terminal == True):\n4\nv = 0.0\n5\nif(winner == Board.WHITE):\n6\nv = 1.0\n7\nif(winner == Board.BLACK):\n8\nv = -1.0\n9\nself.backpropagate(v, node.parentEdge)\n10\nreturn\n11\nv = node.expand(self.network)\n12\nself.backpropagate(v, node.parentEdge)\nListing 5.15: MCTS Backpropagation\n1 def backpropagate(self , v, edge):\n2\nedge.N += 1\n3\nedge.W = edge.W + v\n4\nedge.Q = edge.W / edge.N\n5\nif(edge.parentNode != None):\n6\nif(edge.parentNode.parentEdge != None):\n7\nself.backpropagate(v, edge.parentNode.parentEdge)\nIt remains to implement the actual MCT search procedure. Here we are given\na node which is the root node where we start the search. Before starting the\nactual search, we expand this root node. Then we iterate a ﬁxed number of times\n(here a 100 times) and execute selection, expansion, evaluation and backpropagation\n– the latter is called by the evaluation function directly. The policy value that is\nreturned by an MCT search was deﬁned as\n휋푚=\n푁1/휏\n푚\nÍ\n푛푁1/휏\n푛\n\n5.4. “ZERO-LIKE” REINFORCEMENT LEARNING\n255\ni.e. essentially turning the visit counts into probabilities by dividing each count\nthrough the sum of all visit counts for the root edge. As described in Chapter 4,\nthe parameter 휏can be used in order to facility more exploration. For Hexapawn\nwe just set it to a ﬁxed value of 1.0 during initialization of our MCTS object.\nListing 5.16: MCT Search\n1 def search(self , rootNode):\n2\nself.rootNode = rootNode\n3\n_ = self.rootNode.expand(self.network)\n4\nfor i in range (0 ,100):\n5\nselected_node = self.select(rootNode)\n6\nself.expandAndEvaluate(selected_node)\n7\nN_sum = 0\n8\nmoveProbs = []\n9\nfor edge , _ in rootNode.childEdgeNode:\n10\nN_sum += edge.N\n11\nfor (edge , node) in rootNode.childEdgeNode:\n12\nprob = (edge.N ** (1 / self.tau)) / ((N_sum) ** (1/ self.tau\n))\n13\nmoveProbs.append ((edge.move , prob , edge.N, edge.Q))\n14\nreturn moveProbs\nTraining and Evaluation\nTo implement the self-play pipeline we deﬁne a\nclass ReinfLearn. During initialization we provide the network to that class.\nThe class has only one function playGame() which is called to self-play a game\nand collect the positions, move probabilities and outcomes of this game.\nFirst we set up lists that will hold the positions (already encoded as network\ninput bit vectors), the move probabilities and the outcome values. Next we set\nup a Hexapawn board in the initial position. We then start to play the game.\nFirst we append the current position to the positionsData list. Then we set up\nthe MCT search object by creating a root node from the current position and\nexecute the MCT search. The result are move probabilities for each legal move.\nNext we create a move probability vector where we initialize each value with 0.\nThen we set the move probability for the legal moves to those values returned\n\n256\n5. HEXAPAWNZERO\nby the MCT search. Thus move probabilities for illegal moves in that vector are\nmasked to 0.\nWe now need to select a move to play.\nHere we slightly deviate from the\nAlphaZero algorithm. The reason is that Hexapawn is a very small game that\nlasts only a few moves. Hence, the MCT search is very successful, i.e. even if\nthe network is initialized with random values and not trained at all, the MCT\nsearch will almost always ﬁnd the very best move in each position. If we now\nselect the best move we will play the same game over and over again without\nexploring all possible game states.\nThe problem also exists in AlphaZero, and to have more exploration they used\ntwo parameters: First, they adjusted the parameter 휏for MCT search. Second\nthey added random noise at the root node during MCT search. Here we will\nresort to a simpler solution that works just as well. Note that outputVec con-\ntains probabilities that add up to one. We can interpret this as a multinomial\nprobability distribution and randomly select a move according to that probability\ndistribution. Without going into mathematical details, the way this works is\nbest illustrated by an example. Suppose we have the vector of move probabilities\n0.1, 0.2, 0.7\nWe now want to randomly select one of the three possible indices (and thereby\nthe associated moves), but we do not want to do that randomly with the same\nprobability. Instead we desire to select the index 0 only in about 10 percent of\nall cases, index 1 in about 20 percent of all cases and index 2 in about 70 percent\nof all cases. In other words we want to randomly select moves, but still favor\nthose moves which are more promising according to the MCT search. This is\nprecisely what calling np.random.multinomial and subsequent selection of a\nmove does. Next we append the move probabilities for the current position to\nour list, apply the move and continue.\nIf we reached a ﬁnal position we need to evaluate who won the game. We add\nthis information 푛times to our list of outcomes where 푛is the number of posi-\ntions that we encountered during the game. This means we now have collected\n푛training examples which consist of 푛positions (in the list positionsData), 푛\n\n5.4. “ZERO-LIKE” REINFORCEMENT LEARNING\n257\nsets of move probabilities (in the list moveProbsData) and 푛game outcomes (the\nsame value for each position of the game of course, in the list valuesData).\nListing 5.17: “AlphaZero”-like Training (1)\n1 def fst(x):\n2\nreturn x[0]\n3\n4 class ReinfLearn ():\n5\n6\ndef __init__(self , model):\n7\nself.model = model\n8\n9\ndef playGame(self):\n10\npositionsData = []\n11\nmoveProbsData = []\n12\nvaluesData = []\n13\n14\ng = Board()\n15\ng.setStartingPosition ()\n16\n17\nwhile((not fst(g.isTerminal ()))):\n18\npositionsData.append(g.toNetworkInput ())\n19\n20\nrootEdge = mcts.Edge(None , None)\n21\nrootEdge.N = 1\n22\nrootNode = mcts.Node(g, rootEdge)\n23\nmctsSearcher = mcts.MCTS(self.model)\n24\n25\nmoveProbs = mctsSearcher.search(rootNode)\n26\noutputVec = [ 0.0 for x in range(0, 28)]\n27\nfor (move , prob , _, _) in moveProbs:\n28\nmove_idx = g.getNetworkOutputIndex(move)\n29\noutputVec[move_idx] = prob\n30\n31\nrand_idx = np.random.multinomial (1, outputVec)\n32\nidx = np.where(rand_idx ==1) [0][0]\n33\nnextMove = None\n34\n35\nfor move , _, _, _ in moveProbs:\n36\nmove_idx = g.getNetworkOutputIndex(move)\n37\nif(move_idx == idx):\n\n258\n5. HEXAPAWNZERO\n38\nnextMove = move\n39\nmoveProbsData.append(outputVec)\n40\ng.applyMove(nextMove)\n41\nelse:\n42\n_, winner = g.isTerminal ()\n43\nfor i in range(0, len(moveProbsData)):\n44\nif(winner == Board.BLACK):\n45\nvaluesData.append (-1.0)\n46\nif(winner == Board.WHITE):\n47\nvaluesData.append (1.0)\n48\nreturn (positionsData , moveProbsData , valuesData)\nWe are now ready to train our network. We load the randomly initialized model,\nsetup the MCT search object and the reinforcement learner, and start training.\nHere we used 21 training loops, and save the current network at iteration 0, 10\nand 20. In each loop we play ten games and obtain training data from these ten\ngames. We then use this training data to train and improve the network and\nrepeat.\nListing 5.18: “AlphaZero”-like Training (2)\n1 model = keras.models.load_model (\"../ common/random_model.keras\")\n2 mctsSearcher = mcts.MCTS(model)\n3 learner = ReinfLearn(model)\n4 for i in (range (0,11)):\n5\nprint(\" Training Iteration: \"+str(i))\n6\nallPos = []\n7\nallMovProbs = []\n8\nallValues = []\n9\nfor j in tqdm(range (0,10)):\n10\npos , movProbs , values = learner.playGame ()\n11\nallPos += pos\n12\nallMovProbs += movProbs\n13\nallValues += values\n14\nnpPos = np.array(allPos)\n15\nnpProbs = np.array(allMovProbs)\n16\nnpVals = np.array(allValues)\n17\nmodel.fit(npPos ,[npProbs , npVals], epochs =256, batch_size =16)\n18\nif(i%10 == 0):\n19\nmodel.save(’model_it ’+str(i)+’.keras ’)\n\n5.5. SUMMARY\n259\nThe evaluation of our trained network is the same as before. We simply reload\n(one of the) networks trained by reinforcement learning instead of the one\ntrained by supervised learning.\nMatchup: Random Player ...\nWhite Wins\nBlack Wins\n... vs Trained Network (iteration 0)\n0%\n100%\n... vs Trained Network (iteration 10)\n0%\n100%\n... vs Untrained Network\n34%\n66%\n... vs Random Player\n62%\n38%\nTable 5.2: Supervised Network Evaluation\nOn my system I get the results depicted in Table 5.2. Again, due to the random\ninitialization of the network your numbers may vary, but we can see that even\none iteration of training suﬃces to ensure that the trained network wins all\ngames with Black.\n5.5\nSummary\nWehaveseenhowtoimplementanAlphaZero-likelearning approach in Python.\nIt is apparent that both supervised and reinforcement learning work here for\nHexapawn. Clearly, supervised learning is much simpler to implement, but\nrequires training data to be available. Whether such training data is readily\navailable or not heavily depends on the problem in question.\nHexapawn is way too trivial to draw any conclusions as to what approach is\nbetter. Both, supervised and reinforcement learning create strategies that work\na hundred percent, something that we cannot do for complex games such as\nchess. Especially, for our network here it is enough to simply remember all\ncrucial positions, as there are very few game states. No abstraction or pattern\nrecognition was required and therefore the network architecture did not require\nany advanced building blocks such as convolutional layers.\nNote how small — in terms of lines of code — the implementation was, es-\npecially, but not only, for supervised learning. Note also how universal the\n\n260\n5. HEXAPAWNZERO\napproach is. To change the implementation to other problem domains it suf-\nﬁces to exchange the game logic. Everything else, i.e. the network architecture\nas well as MCT search, remain largely the same.\n\n6\nConclusion and Outlook\nI hate to write. I love to have\nwritten.\nDouglas Adams\nI hope you had as much fun reading this book as I had writing it.\nIn fact these are stereotypical closing words and thinking about it, at times,\nwriting this was not so much fun at all. Instead it was a lot of work. If you\ncarefully read this book it must have been a lot of work, too. So congratulations\nfor that — I hope you feel that it was worth the eﬀort and still enjoyed the whole\nexperience.\nWe have covered neural networks and their building blocks. These are used\nnot only in chess programming but of course in other domains as well. A lot of\n“revolutionary” recent artiﬁcial intelligence based solutions are based on such\nneural networks. Therefore taking a look at those concepts should enable you\nto get an easier understanding of those solutions as well.\nI hope that the introduction into current state-of-the art engines like Leela Chess\nZero and Stockﬁsh NNUE also gave you a better understanding on what chess\n261\n\n262\n6. CONCLUSION AND OUTLOOK\nengines are capable of and what the current limits are, especially if we want to\nuse them on commodity hardware.\nPredictions are incredibly diﬃcult, especially when they concern the future.\nBut if I want to make one predictions it’s that alpha-beta searchers will prevail\nfor chess. Moreover if neural network circuitry and accelerators will become\nmore common on desktop and mobile computers, it is likely that other, more\npowerful ways of combining alpha-beta search with fast neural network based\nposition evaluation will start to appear.\nAnd hopefully it will enable chess\nprogrammers to use these techniques not only to build the strongest engine,\nbut also to build learning tools and realistic sparring partners in order to help\nhumans become better chess players.\n\nBibliography\n[AAD75]\nG. M. Adelson-Velskiy, Vladimir L. Arlazarov, and M. V. Donskoy.\nSome methods of controlling the tree search in chess programs. Artif.\nIntell., 6(4):361–371, 1975.\n[ACF02]\nPeter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. Finite-time Anal-\nysis of the Multiarmed Bandit Problem.\nMachine Learning, 47(2-\n3):235–256, 2002.\n[ALL19]\nZeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and\ngeneralization in overparameterized neural networks, going be-\nyond two layers. In Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages\n6155–6166, 2019.\n[Brü93]\nBernd Brügmann. Monte Carlo Go. Technical report, Max-Planck-\nInstitute of Physics, Munich, 1993.\n[Cla99]\nArthur C. Clarke. Proﬁles of the Future: An Inquiry into the Limits of\nthe Possible. Weidenfeld & Nicolson, 3rd edition, 1999.\n[Cou06]\nRémi Coulom. Eﬃcient selectivity and backup operators in monte-\ncarlo tree search. In H. Jaap van den Herik, Paolo Ciancarini, and\nH. H. L. M. Donkers, editors, Computers and Games, 5th International\nConference, CG 2006, Turin, Italy, May 29-31, 2006. Revised Papers, vol-\n263\n\n264\nBIBLIOGRAPHY\nume 4630 of Lecture Notes in Computer Science, pages 72–83. Springer,\n2006.\n[CS15]\nChristopher Clark and Amos J. Storkey. Training Deep Convolu-\ntional Neural Networks to Play Go. In Proceedings of the 32nd Inter-\nnational Conference on Machine Learning, ICML 2015, Lille, France, 6-11\nJuly 2015, volume 37 of JMLR Workshop and Conference Proceedings,\npages 1766–1774. JMLR.org, 2015.\n[DNW16] Omid E. David, Nathan S. Netanyahu, and Lior Wolf. DeepChess:\nEnd-to-End Deep Neural Network for Automatic Learning in Chess.\nIn Artiﬁcial Neural Networks and Machine Learning - ICANN 2016 - 25th\nInternational Conference on Artiﬁcial Neural Networks, Barcelona, Spain,\nSeptember 6-9, 2016, Proceedings, Part II, volume 9887 of Lecture Notes\nin Computer Science, pages 88–96. Springer, 2016.\n[DZPS19] Simon S. Du, Xiyu Zhai, Barnabás Póczos, and Aarti Singh. Gradient\nDescent Provably Optimizes Over-parameterized Neural Networks.\nIn 7th International Conference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n[Eri92]\nEric Thé. An Analysis of Move Ordering on the Eﬃciency of Alpha-\nBeta Search. Master’s thesis, McGill University, 1992.\n[Fuk80]\nKunihiko Fukushima. Neocognitron: A self-organizing neural net-\nwork model for a mechanism of pattern recognition unaﬀected by\nshift in position. Biological Cybernetics, 36:193––202, 1980.\n[Gar62]\nMartin Gardner.\nMathematical Games.\nScientiﬁc American, 1962.\nreprinted in “The Unexpected Hanging and Other Mathematical\nDiversions” by Martin Gardner, pp. 93ﬀ.\n[Gro09]\nHerman Grooten. Chess Strategy for Club Players: The Road to Positional\nAdvantage. NEW IN CHESS, 2nd edition, 2009.\n[Hen12]\nWilly Hendriks. Move First, Think Later: Sense and Nonsense in Im-\nproving Your Chess. NEW IN CHESS, 2012.\n\nBIBLIOGRAPHY\n265\n[HN97]\nRobert M. Hyatt and Monty Newborn. Crafty Goes Deep. Journal of\nthe International Computer Games Association, 20(2):79–86, 1997.\n[HSA+20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-\nand-Excitation Networks.\nIEEE Trans. Pattern Anal. Mach. Intell.,\n42(8):2011–2023, 2020.\n[Hsu99]\nFeng-Hsiung Hsu. IBM’s Deep Blue Chess Grandmaster Chips. IEEE\nMicro, 19(2):70–81, 1999.\n[Hsu04]\nFeng-Hsiung Hsu. Behind Deep Blue: Building the Computer that De-\nfeated the World Chess Champion. Princeton University Press, 2004.\n[Hub68]\nBarbara Jane Huberman. A Program to Play Chess End Games. PhD\nthesis, Stanford University, 1968.\n[HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In Proc. IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.\n[IS15]\nSergey Ioﬀe and Christian Szegedy. Batch normalization: Acceler-\nating deep network training by reducing internal covariate shift. In\nFrancis R. Bach and David M. Blei, editors, Proceedings of the 32nd\nInternational Conference on Machine Learning, ICML 2015, Lille, France,\n6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceed-\nings, pages 448–456. JMLR.org, 2015.\n[KB15]\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. In Yoshua Bengio and Yann LeCun, editors, 3rd Inter-\nnational Conference on Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n[KG18]\nGarry Kasparov and Mig Greengard. Deep Thinking: Where Ma-\nchine Intelligence Ends and Human Creativity Begins. John Murray, 1st\nedition, 2018.\n[Kot71]\nAlexander Kotov. Think Like a Grandmaster. Chess Digest Inc., 1971.\n\n266\nBIBLIOGRAPHY\n[KS06]\nLevente Kocsis and Csaba Szepesvári. Bandit based monte-carlo\nplanning.\nIn Johannes Fürnkranz, Tobias Scheﬀer, and Myra\nSpiliopoulou, editors, Machine Learning:\nECML 2006, 17th Euro-\npean Conference on Machine Learning, Berlin, Germany, September 18-22,\n2006, Proceedings, volume 4212 of Lecture Notes in Computer Science,\npages 282–293. Springer, 2006.\n[LBBH98] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner.\nGradient-based\nlearning applied to document recognition. Proceedings of the IEEE,\n86(11):2278–2324, 1998.\n[LBD+89]\nYann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson,\nRichard E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel.\nBackpropagation Applied to Handwritten Zip Code Recognition.\nNeural Computation, 1(4):541–551, 1989.\n[Mat15]\nMatthew Lai. Giraﬀe: Using Deep Reinforcement Learning to Play\nChess. Master’s thesis, Imperial College London, 2015.\n[MIG96]\nHitoshi Matsubara, Hiroyuki Iida, and Reĳer Grimbergen. Natural\nDevelopments in Game Research. Journal of the International Computer\nGames Association, 19(2):103–112, 1996.\n[MSKA20] Reid McIlroy-Young, Siddhartha Sen, Jon M. Kleinberg, and Ashton\nAnderson. Aligning superhuman AI with human behavior: Chess\nas a model system. In KDD ’20: The 26th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, Virtual Event, CA, USA, August\n23-27, 2020, pages 1677–1687. ACM, 2020.\n[Pet11]\nPetr. Baudis.\nMCTS with Information Sharing.\nMaster’s thesis,\nCharles University in Prague, 2011.\n[SB18]\nRichard S. Sutton and Andrew G. Barto. Reinforcement learning - An\nIntroduction. Adaptive Computation and Machine Learning. MIT\nPress, 2nd edition, 2018.\n[Sha50]\nClaude E. Shannon. Programming a Computer for Playing Chess.\nPhilosophical Magazine, 41(314), 1950.\n\nBIBLIOGRAPHY\n267\n[Sha18]\nSam Shankland. Small Steps to Giant Improvement: Master Pawn Play\nin Chess. Quality Chess, 2018.\n[SHM+16] David Silver,\nAja Huang,\nChris J. Maddison,\nArthur Guez,\nLaurent Sifre, George van den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot,\nSander Dieleman, Dominik Grewe, John Nham, Nal Kalchbren-\nner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray\nKavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the\ngame of Go with deep neural networks and tree search. Nature,\n529(7587):484–489, 2016.\n[SHS+17]\nDavid\nSilver,\nThomas\nHubert,\nJulian\nSchrittwieser,\nIoannis\nAntonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent\nSifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap,\nKaren Simonyan, and Demis Hassabis. Mastering Chess and Shogi\nby Self-Play with a General Reinforcement Learning Algorithm.\nCoRR, abs/1712.01815, 2017.\n[SHS+18]\nDavid\nSilver,\nThomas\nHubert,\nJulian\nSchrittwieser,\nIoannis\nAntonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent\nSifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen\nSimonyan, and Demis Hassabis. A general reinforcement learning\nalgorithm that masters chess, shogi, and Go through self-play. Sci-\nence, 362(6419):1140–1144, 2018.\n[Sil10]\nJeremy Silman. How to Reassess Your Chess: Chess Mastery Through\nImbalances. Siles Press, 4th edition, 2010.\n[SPI14]\nArie Pratama Sutiono, Ayu Purwarianti, and Hiroyuki Iida. A Math-\nematical Model of Game Reﬁnement. In Proc. 6th INTETAIN, volume\n136 of Lecture Notes of the Institute for Computer Sciences, Social Infor-\nmatics and Telecommunications Engineering, pages 148–151, 2014.\n[SR19]\nMatthew Sadler and Natasha Regan.\nGame Changer: AlphaZero’s\nGroundbreaking Chess Strategies and the Promise of AI. NEW IN CHESS,\n2019.\n\n268\nBIBLIOGRAPHY\n[SSS+17]\nDavid Silver,\nJulian Schrittwieser,\nKaren Simonyan,\nIoannis\nAntonoglou, Aja Huang, ArthurGuez, Thomas Hubert, Lucas Baker,\nMatthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan\nHui, Laurent Sifre, George van den Driessche, Thore Graepel, and\nDemis Hassabis. Mastering the game of Go without human knowl-\nedge. Nat., 550(7676):354–359, 2017.\n[STIM18]\nShibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander\nMadry. How does batch normalization help optimization? In Samy\nBengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,\nNicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neu-\nral Information Processing Systems 31: Annual Conference on Neural In-\nformation Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,\nMontréal, Canada, pages 2488–2498, 2018.\n[VBC+19] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël\nMathieu, Andrew Dudzik, Junyoung Chung, David H. Choi,\nRichard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan\nHorgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre,\nTrevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezh-\nnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Bud-\nden, Yury Sulsky, James Molloy, Tom Le Paine, Çaglar Gülçehre,\nZiyu Wang, Tobias Pfaﬀ, Yuhuai Wu, Roman Ring, Dani Yogatama,\nDario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul, Tim-\nothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps,\nand David Silver. Grandmaster level in StarCraft II using multi-agent\nreinforcement learning. Nat., 575(7782):350–354, 2019.\n[vN28]\nJohn von Neumann.\nZur Theorie der Gesellschaftsspiele.\nMath.\nAnn., 100:295–320, 1928.",
    "pdf_filename": "Neural Networks for Chess.pdf"
}