{
    "title": "Assessing the impact of machine intelligence on human behaviour - an interdisciplinary endeavour",
    "context": "",
    "body": "Assessing the impact of machine \nintelligence on human behaviour: an \ninterdisciplinary endeavour  \nProceedings of 1st HUMAINT workshop, Barcelona, \nSpain, March 5-6, 2018 \nCentre for Advanced Studies \nGómez, Emilia (editor) \n2018  \n\n \n \nThis publication is a Conference and Workshop report by the Joint Research Centre (JRC), the European \nCommission’s science and knowledge service. It aims to provide evidence-based scientific support to the \nEuropean policymaking process. The scientific output expressed does not imply a policy position of the \nEuropean Commission. Neither the European Commission nor any person acting on behalf of the Commission is \nresponsible for the use that might be made of this publication. \n \nContact information  \nName: Emilia Gómez  \nEmail: Emilia.GOMEZ-GUTIERREZ@ec.europa.eu  \n \nJRC Science Hub \nhttps://ec.europa.eu/jrc  \n \n \nJRC111773 \n \n \nLuxembourg: Publications Office of the European Union, Seville: European Commission, 2018 \n \n© European Union, 2018 \n \nReuse is authorised provided the source is acknowledged. The reuse policy of European Commission documents \nis regulated by Decision 2011/833/EU (OJ L 330, 14.12.2011, p. 39). \nFor any use or reproduction of photos or other material that is not under the EU copyright, permission must be \nsought directly from the copyright holders. \n \nHow to cite this report: Gómez, E. (editor) Assessing the impact of machine intelligence on human behaviour: \nan interdisciplinary endeavour, Joint Research Centre Conference and Workshop Reports, Publications Office of \nthe European Union, Seville, 2018, PUBSY No JRC111773. \n \nAll images © European Union 2018 or belong to the authors \n\n \ni \nContents \nAcknowledgements ................................................................................................ 5 \nAbstract ............................................................................................................... 6 \n1 Human behaviour and machine intelligence in the digital transformation project \nHUMAINT: objectives and workshop ......................................................................... 7 \n1.1 Workshop details ......................................................................................... 7 \n1.2 Goals and structure of the report ................................................................... 9 \n \nPart I: Human vs Machine Intelligence ................................................................... 10 \n2 Unintuitive properties of deep neural networks .................................................... 11 \n2.1 Statement ................................................................................................ 11 \n2.2 Challenges ................................................................................................ 12 \nReferences ...................................................................................................... 12 \n3 Whole brain dynamics and model ...................................................................... 13 \n3.1 Statement ................................................................................................ 13 \nReferences ...................................................................................................... 14 \n4 Extended minds and machines .......................................................................... 15 \n4.1 Statement ................................................................................................ 15 \n4.2 Challenges ................................................................................................ 16 \nReferences ...................................................................................................... 16 \n5 Slow and fast biases in decision making ............................................................. 17 \n5.1 State of the art .......................................................................................... 17 \n5.2 Challenges ................................................................................................ 17 \n6 Human vs machine intelligence: an interdisciplinary discussion ............................. 18 \n \nPart II: Algorithms’ impact on human behaviour ...................................................... 20 \n7 Artificial Intelligence: an interesting leverage point to rethink humans' relations to \nmachines…and to themselves. ............................................................................... 21 \n7.1 Statement ................................................................................................ 21 \n7.2 Position regarding the research questions ..................................................... 21 \n7.3 Challenges ................................................................................................ 22 \nReferences ...................................................................................................... 22 \n8 Algorithmic discrimination ................................................................................. 23 \n8.1 Statement ................................................................................................ 23 \n8.2 Context .................................................................................................... 23 \n8.3 Challenges ................................................................................................ 26 \n9 Making quality decisions about the uses of algorithms and AI ................................ 28 \n9.1 Statement ................................................................................................ 28 \n\n \nii \n10 Summary of the Panel Discussion “Algorithms’ Impact on Human Behaviour” .......... 31 \n \nPart III: Evaluation and regulation of algorithms ..................................................... 33 \n11 Reality, requirements, regulation: Points of intersection with the machine-learning \npipeline .............................................................................................................. 34 \n11.1 \nStatement .......................................................................................... 34 \n11.2 \nChallenges .......................................................................................... 36 \nReferences ...................................................................................................... 36 \n12 Benchmarks and performance measures in artificial intelligence ............................ 37 \n13 The IEEE P7003 Standard for Algorithmic Bias Considerations ............................... 39 \n13.1 \nStatement .......................................................................................... 39 \n13.2 \nFuture challenges ................................................................................ 39 \n14 Algorithms and markets: a need for regulation? .................................................. 40 \n14.1 \nStatement .......................................................................................... 40 \n14.2 \nResearch questions .............................................................................. 40 \n15 Evaluation and regulation of algorithms: summary of discussions .......................... 43 \n \nPart IV: Application domains and new paradigms..................................................... 44 \n16 Machine learning in healthcare and computer-assisted treatment .......................... 45 \n16.1 \nStatement .......................................................................................... 45 \n16.2 \nFuture challenges ................................................................................ 45 \n17 The influence of “intelligent” technologies on the way we discover and experience \nmusic ................................................................................................................. 46 \n17.1 \nStatement .......................................................................................... 46 \n17.2 \nChallenges .......................................................................................... 47 \n18 HumanAI ........................................................................................................ 48 \n18.1 \nStatement .......................................................................................... 48 \n18.2 \nChallenges .......................................................................................... 48 \nReferences ...................................................................................................... 49 \n19 Do humans know which AI applications they do need? ......................................... 50 \n \nPart V: Considerations and conclusions .................................................................. 52 \n20 Characterising the trajectories of artificial and natural intelligence ......................... 53 \n20.1 \nState of the art: An Atlas of Intelligence ................................................. 53 \n20.2 \nChallenges .......................................................................................... 54 \nReferences ...................................................................................................... 54 \n21 Considerations related to cognitive development children ..................................... 56 \n21.1 \nDifferences in machinery ...................................................................... 56 \n21.2 \nDevelopmental changes in interaction with computers .............................. 57 \n\n \niii \nReferences ...................................................................................................... 57 \n22 The tyranny of data? The bright and dark sides of algorithmic decision making for \npublic policy making ............................................................................................ 58 \n22.1 \nStatement: Data-driven Algorithms for Public Policy Making ...................... 58 \n22.2 \nChallenges .......................................................................................... 59 \nReferences ...................................................................................................... 62 \n23 Quantum Computing and Machine Learning ........................................................ 66 \n23.1 \nWhat is quantum computing? ................................................................ 67 \n23.2 \nWhat is the holy grail of quantum computing? ......................................... 67 \n23.3 \nQuantum Machine Learning ................................................................... 68 \n23.4 \nChallenges .......................................................................................... 68 \n24 Conclusions and future work ............................................................................. 70 \n25 List of workshop participants and report contributors ........................................... 72 \nList of figures ...................................................................................................... 74 \n \n\n\n \n5 \n \nAcknowledgements \nThis workshop was organized by the Centre for Advanced Studies, Joint Research Centre \nwith the local support of the Department of Information and Communication \nTechnologies, Universitat Pompeu Fabra.  \nAuthors (editor plus authors in alphabetical order)  \nEmilia Gómez (editor)  \nCarlos Castillo \nVicky Charisi \nVerónica Dahl  \nGustavo Deco \nBlagoj Delipetrev \nNicole Dewandre \nMiguel Ángel González-Ballester \nFabien Gouyon \nJosé Hernández-Orallo \nPerfecto Herrera \nAnders Jonsson  \nAnsgar Koene \nMartha Larson  \nRamón López de Mántaras \nBertin Martens \nMarius Miron \nRubén Moreno-Bote \nNuria Oliver \nAntonio Puertas Gallardo \nHeike Schweitzer  \nNuria Sebastian \nXavier Serra \nJoan Serrà \nSongül Tolan \nKarina Vold \n\n \n6 \n \nAbstract \nThis document contains the outcome of the first Human behaviour and machine \nintelligence (HUMAINT) workshop that took place 5-6 March 2018 in Barcelona, Spain. \nThe workshop was organized in the context of a new research programme at the Centre \nfor Advanced Studies, Joint Research Centre of the European Commission, which focuses \non studying the potential impact of artificial intelligence on human behaviour.  \nThe workshop gathered an interdisciplinary group of experts to establish the state of the \nart research in the field and a list of future research challenges to be addressed on the \ntopic of human and machine intelligence, algorithm’s potential impact on human \ncognitive capabilities and decision making, and evaluation and regulation needs. \nThe document is made of short position statements and identification of challenges \nprovided by each expert, and incorporates the result of the discussions carried out during \nthe workshop. In the conclusion section, we provide a list of emerging research topics \nand strategies to be addressed in the near future.  \n \n\n \n7 \n \n1 Human behaviour and machine intelligence in the digital \ntransformation project HUMAINT: objectives and \nworkshop  \nEmilia Gómez \nCentre \nfor \nAdvanced \nStudies, \nJoint \nResearch \nCentre, \nEuropean \nCommission \nUniversitat Pompeu Fabra \nOver the last few years, thanks to an increase in data availability and computing power, \ndeep learning techniques have been applied to different research problems related to \ncomputer vision, natural language processing, music processing or bioinformatics. Some \nof these models are said to surpass human-level performance (e.g. image recognition \n(He et al., 2015) and model highly abstract human concepts such as emotion (Kim et al., \n2013) or culture. The practical exploitation of such algorithms brings up a discussion on \nthe impact of these algorithms into the ways human behave: \n \nOn one side, machine intelligence provides cognitive assistance and \ncomplement humans to interpret data more efficiently and discover hidden \nknowledge in large data resources.  \n \nOn the other side, these algorithms may also affect the way we perform some \ncognitive tasks and thus affect autonomy and decision making. This is \nespecially relevant when algorithms perform tasks in a high level of abstraction \nand when they may contradict and influence human interpretations.  \nThe goal of the Human behaviour and Machine Intelligence (HUMAINT) project, carried \nout at the Centre for Advanced Studies, Joint Research Centre of the European \nCommission, is to (1) provide a scientific understanding of machine vs human \nintelligence; (2) analyse the influence of current algorithms on human behaviour and (3) \ninvestigate to what extent these findings should influence the European regulatory \nframework. \nThis document summarizes the results of the first HUMAINT workshop, which took place \nin Barcelona on 5-6 March 2018, and brought together key researchers from \ncomplementary disciplines and backgrounds. The workshop was defined with two main \ngoals:  \n1. Build an interdisciplinary roadmap on human vs machine intelligence, potential \nalgorithm’s impact on human cognitive capabilities and decision making, and \nevaluation and regulation needs. We intend to study the state of the art, identify \nfuture research challenges, and reach a consensus on practical way to address \nthese challenges.  \n2. Build a community of researchers for the HUMAINT project to collaborate with. \n1.1 Workshop details  \nThe workshop was structured in a set of short position presentations followed by panel \ndiscussions. \nPresentation \nslides \ncan \nbe \nfound \nat \nthe \nworkshop \nweb \npage \nhttps://ec.europa.eu/jrc/communities/community/event/humaint-kick-workshop.  \n \nDAY 1 \n9.00-9.30: Welcome (Jutta Thielen-del-Pozo, Vanesa Daza, Emilia Gómez) \n \n(1) Human vs machine intelligence \n9:30-12:00: Presentations \n\n \n8 \n \n \nJoan Serrà. Unintuitive properties of deep neural networks.  \n \nGustavo Deco. Whole brain modelling and applications. \n \nKarina Vold. Extended minds and machines.  \n \nRubén Moreno-Bote. Slow and fast biases in decision making.   \n \n12:00-13:00: Panel discussion. Presentation and moderator:  Ramón López de Mántaras  \n \n13:00-14:00: Lunch \n \n(2) Algorithms’ impact on human behaviour \n14:00-16:30: Presentations \n \nHenk Scholten. Digital transformation and governance of societies.  \n \nNicole Dewandre. Artificial intelligence: an interesting leverage point to rethink \nhumans' relations to machines…and to themselves.  \n \nCarlos Castillo. Algorithmic bias.  \n \nFabien Giraldin. Experience Design in the Machine Learning Era. \n \n16:30-17:30: Panel discussion. Presentation and moderator: Verónica Dahl  \n \nDAY 2 \n(3) Evaluation and regulation of algorithms \n9:00 - 11:30: Presentations \n \nAlessandro Annoni. Digital transformation and artificial intelligence: the policy-\noriented perspective. \n \nMartha Larson.  Reality, requirements, regulation: points of intersection with the \nmachine learning pipeline. \n \nAnders Jonsson. Benchmarks and performance measures in artificial intelligence. \n \nAnsgar Koene. The IEEE P7003 Standard for Algorithmic Bias Considerations. \n \nHeike Schweitzer. Algorithmic decision-making - in need of (which) regulation? \n \n11:30 - 12:30: Panel discussion. Presentation and moderator: Xavier Serra. \n \n12:30-14:00: Lunch \n \n (4) Application domains and new paradigms \n14:00 - 15:00: Presentations \n \nSergi Jordà. Enhancing or Mimicking Human [Musical] Creativity? The Bright and \nthe Dark Sides of the Moon. \n \nMiguel Ángel González-Ballester. Machine learning in healthcare and computer-\nassisted treatment. \n\n \n9 \n \n \nFabien Gouyon. The influence of machine intelligence on the music industry. \n \nBlagoj Delipetrev. HumanAI. \n \nLuc Steels. Will AI lead to digital immortality? \n \n15:00 - 16:00: Panel discussion. Presentation and moderator: Perfecto Herrera. \n \n16:00 - 17:00: Wrap-up session. Moderator: Emilia Gómez. \n1.2 Goals and structure of the report  \nThe goal of this report is to provide an interdisciplinary state of the art overview on the \ninteraction between human and machine intelligence and identify which are the research \nchallenges related to this interaction and the practical ways to address them.  \nIn order to do so, the document is structured in five parts. The first four parts are related \nto the four sessions of our workshop. Each part contains a series of original contributions \nand position statements provided by workshop presenters. They are complementary to \ntheir presentation slides and contain their statements with respect to a set of questions \nproposed beforehand. In addition, each part includes a summary of the workshop \ndiscussions provided by panel moderators. In part V, the report incorporates input from \nother scholars involved in our discussions that were not able to present at the workshop. \nThe report finishes with some general conclusions of this interdisciplinary discussion and \nof the HUMAINT project and some directions for future research within the HUMAINT \nproject and beyond.  \n\n \n10 \n \nPart I: Human vs Machine Intelligence \nIn this part of the document, we address the following research questions:  \n— Which are the fundamental differences between human and machine intelligence? \n— How do algorithms complement or replace human tasks now and how will they do this \nin the future? \n— Will algorithms that take over some of our tasks affect the balance between human \nand machine intelligence? \nWe present a set of statements which address these questions from different disciplines \nand views, and provide a summary of discussions.  \n\n \n11 \n \n2 Unintuitive properties of deep neural networks \nJoan Serrà \nTelefónica Research \n2.1 Statement \nDeep neural networks are currently a hot topic, not only within both academia and \nindustry, but also among society and the media. However, interestingly, the current \nsuccess and practice of deep learning seems to be uncorrelated with its theoretical, more \nformal understanding. In particular, we find a number of unintuitive properties both in \ntheir design and operation that do not have yet an agreed explanation. Interestingly, \nsome of these unintuitive properties are shared with humans, although current neural \nnetwork approaches and the underlying mechanisms that lead to those properties do not \nresemble human mechanisms. \n \nNeural networks can make dumb errors — neural networks can produce totally \nunexpected outputs from inputs with perceptually-irrelevant changes, which are \ncommonly called adversarial examples. Humans can be also confused by ‘adversarial \nexamples’: we all have seen images that we guessed were something (or a part of \nsomething) and later we were told they were not. However, the point here is that \nhuman adversarial examples do not correspond to those of neural networks because, \nin the latter case, they can be perceptually the same (Szegedy et al., 2014). \n \nThe solution space is unknown — as with many other machine learning \nalgorithms, the training of neural networks proceeds by finding a combination of \nnumbers, called network parameters or weights that yield the highest performance \nor, more properly, the minimum loss on some data. There are well known \nmethodologies to find such a minimum for a few parameters with theoretical \nguarantees. However, deep neural networks are typically in the range of millions of \nparameters, for which a suitable combination that minimizes a certain loss must be \nfound. The losses of current deep networks are non-convex, with multiple local \nminima and potentially many obstacles (Li et al., 2017).  \n \nNeural networks can easily memorize — recent work empirically shows that \nfinite-sized networks can model any finite-sized data set, even if this is made of \nshuffled data, random data, or random labels (Zhang et al., 2017). This has the \nobvious implication that neural networks can remember any data seen during \ntraining, no matter the nature of that data. What is not so obvious is that, still, if the \ndata is not totally random, neural networks are totally capable of extrapolating their \nmemories to unseen cases and generalize. Doing so when the number of model \nparameters is several orders of magnitude larger than the number of training \ninstances is what is intriguing and contradicts conventional machine learning wisdom. \n \nNeural networks can be compressed — one can drastically reduce the number of \nparameters of a trained neural network and still maintain its performance on both \nseen and unseen data (Han et al., 2016). In some cases, the amount of pruning or \ncompression is surprising: up to 100 times depending on the data set and network \narchitecture. Besides practical considerations, the compressibility of networks poses \nseveral questions: Do we need a large network in the first place? Is there some \narchitecture twist that combined with current minimum-finding algorithms allows to \ndiscover good parameter combinations for those small networks? Or is it just a matter \nof discovering new minimum-finding algorithms? \n \nLearning is influenced by initialization and example order — As with human \nlearning, current network learning depends on the order in which we present the \nexamples. Practitioners know that different sample orderings yield different \nperformances and, in particular, that early examples have more influence on the final \naccuracy (Erhan et al., 2010). Furthermore, it is now a classic trick to pre-train a \n\n \n12 \n \nneural network in an unsupervised way or to transfer knowledge from a related task \nto benefit from additional sources. In addition, it is easy to show that random \ninitializations can affect the final accuracy or, in the worst case, just prevent the \nnetwork to learn at all.  \n \nNeural networks forget what they learn — This phenomenon is known as \ncatastrophic forgetting or catastrophic interference (McCloskey & Cohen, 1989). \nEssentially, when a neural network that has been trained for a certain task is reused \nfor learning a new task, it completely forgets how to perform the former. Beyond the \nphilosophical objective of mimicking human learning and whereas machines should be \nable to do so or not, the problem of catastrophic forgetting has important \nconsequences for the current development of systems that consider a large number \nof (potentially multimodal) tasks, and for those which aim towards a more general \nconcept of intelligence. Some research is devoted to tackle catastrophic forgetting, \nbut a general solution for a compact model is still not yet fully in place (Serrà et al., \n2018). \n2.2 Challenges \n \nAdversarial examples are a big challenge right now. Perhaps we are not going to be \nable to solve the situation until some of the other the inner workings of neural \nnetworks are properly understood. \n \nGeneralization \nis \nanother \nprincipal \nhurdle. \nCurrent \nstatistical \ntheories \nfor \ngeneralization are based on quite old models (like support vector machines and the \nlike) that do not represent the state-of-the-art in many machine learning tasks. \n \nA flexible and straightforward solution to the problem of catastrophic forgetting, \nespecially considering limited resources. \nReferences \n— Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., & Bengio, S. (2010). \nWhy does unsupervised pre-training help deep learning? Journal of Machine Learning \nResearch, 11, 625–660. \n— Han, S., Mao, H., & Dally, W. J. (2016). Deep compression: compressing deep neural \nnetworks with pruning, trained quantization and Huffman coding. In Proc. of the Int. \nConf. on Learning Representations (ICLR). \n— Li, H., Xu, Z., Taylor, G., & Goldstein, T. (2017). Visualizing the loss landscape of \nneural nets. ArXiv: 1719, 09913. \n— McCloskey, M., & Cohen, N. (1989). Catastrophic interference in connectionist \nnetworks: the sequential learning problem. Psychology of Learning and Motivation, \n24, 109–165. \n— Serrà, J., Surís, D., Miron, M., & Karatzoglou, A. (2018). Overcoming catastrophic \nforgetting with hard attention to the task. ArXiv: 1801.01423. \n— Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., & \nFergus, R. (2014). Intriguing properties of neural networks. In Proc. of the Int. Conf. \non Learning Representations (ICLR). \n— Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). Understanding deep \nlearning requires rethinking generalization. In Proc. of the Int. Conf. on Learning \nRepresentations (ICLR). \n \n\n \n13 \n \n3 Whole brain dynamics and model  \nGustavo Deco \nUniversitat Pompeu Fabra \n3.1 Statement  \nWhole-brain computational models aim to balance between complexity and realism \nin order to describe the most important features of the brain in vivo. This balance is \nextremely difficult to achieve because of the astronomical number of neurons and the \nunderspecified connectivity at the neural level. Thus, the most successful whole-brain \ncomputational models have taken their lead from statistical physics where it has been \nshown that macroscopic physical systems obey laws that are independent of their \nmesoscopic constituents. The emerging collective macroscopic behaviour of brain models \nhas been shown to depend only weakly on individual neuron behaviour (Breakspear and \nJirsa, 2007). Thus, these models typically use mesoscopic top-down approximations of \nbrain complexity with dynamical networks of local brain area attractor networks. The \nsimplest models use basic neural mass or mean-field models to capture changes in mean \nfiring rate, while the most advanced models use a dynamic mean field model derived \nfrom a proper reduction of a detailed spiking neuron model [see (Cabral et al., 2017; \nDeco and Kringelbach, 2014) and references therein for a review]. \nThe link between anatomical structure and functional dynamics, introduced more \nthan a decade ago (Jirsa et al., 2002), is at the heart of whole-brain network models. \nStructural connectivity data on the millimetre scale can be obtained in vivo by diffusion \nweighted/tensor imaging (DWI/DTI) combined with probabilistic tractography. The global \ndynamics of the whole-brain model results from the mutual interactions of local node \ndynamics coupled through the underlying empirical anatomical structural connectivity \nmatrix. The structural matrix denotes the density of fibres between a pair of cortical \nareas ascertained from DTI-based tractography. Typically, the temporal dynamics of local \nbrain areas in these models is taken to be either asynchronous (spiking models or their \nrespective mean-field reduction) or oscillatory (Deco and Kringelbach, 2014). \nAdding the temporal dimension to standard FC analysis paves new ways to \ncharacterize the switching behavior of resting-state activity. However, the best \nmethodology to assess it is still under debate. The most commonly used strategy has \nbeen to calculate successive FC(t) matrices using a sliding-window. Recurrent FC \nconfigurations are then captured by applying unsupervised clustering to all the FC(t)s \nobtained over time. However, the sliding-window approach has limitations associated to \nthe window size, which affects the temporal resolution and statistical validation. \nRecently, new methods have been proposed to calculate the FC(t) at a quasi-\ninstantaneous level, namely Phase Coherence Connectivity or Multiplication of Temporal \nDerivatives, which allow for a higher temporal resolution with the caveat of being more \nsusceptible to high-frequency noise fluctuations. To overcome this issue, we hereby \npropose to focus on the dominant FC pattern captured by the leading eigenvector of \nBOLD phase coherence matrices. The key idea of this task is to focus on spatiotemporal \ndynamical biomarkers instead of the classical static grand averaged biomarkers (e.g. \nFC): In concrete, both during task and at-rest, we will identify whole brain dynamical \nmicro brain states, by clustering the dominant dynamic functional connectivity (FC) \npatterns captured by the leading eigenvector of those matrices. Recurrent FC patterns – \nor micro states – will be detected and characterized in terms of lifetime, probability of \noccurrence and switching profiles in link with the subjects’ performance on the Battery of \nbehavioral tests, evolution of disease, and recovery. \n \nThis type of whole-brain modeling could be used for crucial translational \napplications. The basic idea here is to exhaustively stimulate off-line a realistic subject \nspecific fitted whole-brain model in order to detect which type and locus of stimulation is \n\n \n14 \n \nmore effective to reestablish a healthy dynamic of the whole brain (both under resting \nand task conditions) in order to expect that under that condition Hebbian learning will \ncause a meaningful recovery.  \nThus, multimodal neuroimaging (DTI, fMRI) is essential for having patient specific \ntailored whole-brain models which can be studied exhaustively. Whole-brain models \ncould be fitted in particular by the novel spatio-temporal dynamical features mentioned \nabove which characterize the network dynamics in probability microstates space. In \nparallel, based on healthy control groups, we can characterize also those same features.  \nThe idea is to discover, which kind of external stimulation (type and locus) would \npromote a transition from the patient specific affected probability microstate space to a \nhealthy one. This study can be done exhaustively in off-line simulations. After that, one \ncan try in vivo, with TMS, if those reestablishment of healthy spatio-temporal dynamics \ncauses recovery. \nReferences \n— Breakspear, M. and Jirsa, V. K. (2007) Neuronal dynamics and brain connectivity. . \nIn: Handbook of brain connectivity. pp. 3-64. Eds. V. K. Jirsa, A. R. McIntosh. \nSpringer: Berlin Heidelberg, New York. \n— Cabral, J., Kringelbach, M. L. and Deco, G. (2017) Functional connectivity dynamically \nevolves on multiple time-scales over a static structural connectome: Models and \nmechanisms. Neuroimage, in press \n— Deco, G. and Kringelbach, M. L. (2014) Great Expectations: Using Whole-Brain \nComputational \nConnectomics \nfor \nUnderstanding \nNeuropsychiatric \nDisorders. \nNeuron84, 892-905. \n— Jirsa, V. K., Jantzen, K. J., Fuchs, A. and Kelso, J. A. S. (2002) Spatiotemporal \nforward solution of the EEG and MEG using network modeling. Medical Imaging, IEEE \nTransactions on 21, 493-504. \n \n \n\n \n15 \n \n4 Extended minds and machines \nKarina Vold \nLeverhulme Centre for the Future of Intelligence and Faculty of Philosophy, University of \nCambridge \n4.1 Statement \nWhile there are have been many astonishing feats by AI in the last decade—even in just \nthe last year—there continue to be many fundamental differences between human and \nmachine intelligence. For one, many of the headline-making accomplishments by AI have \nbeen in highly specialized domains, or in what experts call Artificial Narrow Intelligence \n(ANI). Humans possess a more general kind of intelligence. We must, after all, perform a \nwide-range of tasks in order to successfully navigate our complex environments. \nSpecialists are working towards building Artificial General Intelligence (AGI)—machines \ncapable of skilled performance in a wide range activities. But there exist fundamental \ndifferences between human and machine intelligence that may complicate this project. \nOne difference that is often pointed to between humans and machines, especially \namongst philosophers, is consciousness. There is nothing it is like to be a machine. \nMachines can be damaged, but they do not feel pain (Dehaene, Lau, and Kouider 2017). \nThere is no uncontroversial scientific or philosophical theory of consciousness but, given \nthe central role of phenomenology in human life and the success of our species, it seems \nat least prima facie plausible that consciousness has a cognitive function (for dissenting \nopinions see Chalmers 1996; Jackson 1982). In this case, without consciousness \ncomputers may never have human-like intelligence. \nAnother fundamental difference is that human intelligence has a long evolutionary \nhistory. The physical world has put many constraints on human intelligence: our brains \nneed to be small enough to fit through a human birth canal and light enough to be \ncarried around on our necks. Furthermore, our biochemical processing speeds are slow, \nrunning on less power than a refrigerator lightbulb. This might have been useful when we \nhad to conserve enough energy to scavenge for food, to build shelters, and to procreate, \nbut computers do not have to do any of these things. Machines already process \ninformation more quickly and efficiently than the human brain (Reardon 2018) and all of \ntheir resources can be expended on one, very narrow task—hence their success in \nspecialized domains.  \nThese differences raise questions about whether we could ever build humanlike \nintelligence, as well as why we should want to build machines that mimic our own \nintellectual constraints, especially if it is possible to bypass human intelligence entirely \nand leapfrog into what experts call ‘superintelligence’, which would surpass humans in \nmany or all cognitive tasks. \nHumans have done remarkably well considering the constraints on our biological bodies. \nAn increasingly popular set of views in philosophy of mind and cognition maintains that \nwe have achieved cognitive success by finding ways of moving our thinking outside of our \nbodies. We created language, for example, a complex representational system that \nenables us to communicate ideas and build on them over time. We also created \ntechnologies, from pens and paper to smartphones, which allow us to augment our \nbiological capacities and simplify the cognitive tasks our brains need to complete (Clark \nand Chalmers 1998). This points to yet another difference between human intelligence \nand machine intelligence: our cognitive functions crucially depend on our bodies, our \nenvironments, and our tools. Our intelligence, as it is sometimes put, is embodied, \nembedded, and extended. To attain human-like intelligence, machines too might need to \nbe embodied (perhaps even in human-like forms), embedded in their environments, and \nable to extend their cognitive functions beyond their hardware through seamless \nintegration with tools. \n\n \n16 \n \nOne concern that falls out of this idea that machines may move beyond their intended, or \noriginal, hardware base to make use of other tools to complete their tasks is whether us \nhumans will become their tools of choice. We have already seen instances where the \noutcomes of algorithms have played a role in shaping human decision-making, e.g. in the \napplication of risk-assessment algorithms in parole decision (Kehl, Guo, and Kessler \n2017). Ng (2016) argues that any mental task that a typical person can do in less than \none second of thought can be automated, which suggests that the more difficult tasks to \nautomate will be precisely those that require us to reflect. Tasks that we can do in less \nthan one second do not require consciousness (Kahneman 2011). Human consciousness \nmay need to play a key reflective role in complementing algorithms, but will have to do \nso while being guarded from the biases that influence our ‘fasting thinking’ systems.  \n4.2 Challenges \n \nDoes it make sense to draw comparisons between human intelligence and machine \nintelligence?  \n \nCan these comparisons mislead us and even harm us? How can they be used to help \nhumanity? \n \nHow is interaction with machines affecting human intelligence and cognitive \ncapacities? Is it augmenting and enhancing our capacities (Savulescu and Bostrom \n2009), is it simply changing our capacities (Carr 2010), or is it perhaps diminishing \nthem? \n \nHow can humans rely on the suggested outcomes of algorithms without being unduly \ninfluenced by them?  \n \nHow can we prevent humans from being used, nudged, or manipulated by machines? \nAnd when, if ever, is relinquishing control to machines for the best? \nReferences  \n— Carr, N. (2010). The Shallows: How the internet is changing the way we think, read and remember. \nLondon: Atlantic Books.  \n— Clark, A. and Chalmers, D. J. (1998). “The Extended Mind.” Analysis 58: 7-19. \n— Chalmers, D.J. (1996). The Conscious Mind: In search of a fundamental theory. \nOxford University Press. \n— Dehaene, S., H. Lau, and Kouider, S. (2017). “What is consciousness, and could \nmachines have it?” Science Vol. 358.6362: 486-492. \n— Jackson, F. (1982). “Epiphenomenal Qualia”, Philosophical Quarterly, 32: 127-36.  \n— Kahneman, D. (2011). Thinking, fast and slow. New York: Farrar, Straus and Giroux. \n— Kehl, D., Guo, P., and Kessler, S. (2017). “Algorithms in the Criminal Justice System: \nAssessing the Use of Risk Assessments in Sentencing” Responsive Communities \nInitiative, Berkman Klein Center for Internet & Society, Harvard Law School. \n— Ng, A. (2016). What Artificial Intelligence Can and Can’t do. Harvard Business \nReview. \n— Reardon, S. (2018). “Artificial neurons compute faster than the human brain,” \nNature, January 26, 2018. <https://www.nature.com/articles/d41586-018-01290-0> \n— Savulescu, J. and Bostrom, N. (Eds.) (2009). Human Enhancement. Oxford University \nPress. \n\n \n17 \n \n5 Slow and fast biases in decision making \nRuben Moreno-Bote \nUniversitat Pompeu Fabra  \n5.1 State of the art \nThe brain is the only known intelligent system in the whole universe. It consists of 100 \nbillion neurons working together in intricate circuits to generate complex behaviour that \nallows adaptation and survival of the species. The study of the brain is important for \nclinical aspects, but also because it can inform and inspire new waves of AI. Also, \nknowing how it works will permit smoother interactions between humans and future \n‘intelligent’ technologies.   \nAn example of the benefit of studying the brain in AI is Deep Learning, which originated \nfrom early inspirations of theoreticians of the brain: the inspiration was that artificial \nneuronal networks consisting of interconnected non-linear units could represent \nincreasingly complex ‘abstract’ variables. Thus, it is expected that new inspirations for AI \nwill come from how the brain works.  \nIt is interesting to observe that algorithms for AI could be designed to work at high \nperformance in specific problems, but do not necessarily generalize well to new domains \nor even new datasets. However, this is part the strength of algorithmic AI, as it is \npossible to design systems that work very well under well-defined and constrained \nconditions. This is beyond the scope of human brains, which are not designed to work in \nrepetitive and highly constrained or restricted conditions. Also, search algorithms are \nmore efficient in some respects than the human brain, especially when dealing with vast \namounts of data in digital format, but not necessarily so in other formats.   \nWe think that it is important to understand human and complex animals’ behaviour in \nsituations in which there is no a priori reason to expect that human behaviour can be \nworse than the algorithms designed by AI. For instance, in perceptual decision-making \ntasks, over-trained animals are expected to perform the task very efficiently, as it is \ntypically observed. However, we observe biases from previous trials that affect \nperformance. The presence of these biases is informative about how artificial the task is, \nand how much it deviates from its natural setting in which it was designed to operate. By \nknowing these biases, one can better compare human vs machine performance in \nsituations in which experimental conditions can deviate from naturalistic environments.   \n5.2 Challenges \n \nDefine tasks in which a priori there is no reason that subjects can do it wrong, and \nyet it is shown that performance is far from optimal, or biases are shown, or wrong \ntendencies are shown. \n \nComplementary, design tasks that are naturalistic and in which human behaviour is \nhighly adapted and close to optimal. Then test performance of AI algorithms in these \nnaturalistic settings. \n \nHow biases can be eradicated from behaviour? Can machines/algorithms help to \ncorrect for this? Recent work shows that blocking of certain areas improves \nperformance, as if the knob for biases could be removed.   \nReferences \n— John Hertz et al. Introduction to The Theory Of Neural Computation (Santa Fe \nInstitute Series) 1st Edition.  \n— Daniel Kahneman, Pensar rápido, pensar despacio, Barcelona, Debate, 2012. \n— Rubén Moreno Bote. ¿Cómo tomamos decisiones? Bonalletra Alcompas, 2018 \n\n \n18 \n \n6 Human vs machine intelligence: an interdisciplinary \ndiscussion  \nRamón López de Mántaras \nIIIA (Artificial Intelligence Research Institute) of the CSIC (Spanish National Research \nCouncil) \nThe Panel \"Human versus Machine Intelligence\" addressed several points based on the \nfour presentations that we had right before the panel. Namely those of Joan Serrà on \n\"Unintuitive properties of Deep Neural Networks (DNNs)\" (see Section 2), Gustavo Deco \non \"Whole brain modelling and applications\", Karina Vold on \"Extended minds and \nmachines\" (see Section 4) and \"Rubén Moreno-Bote on \"Slow and fast biases in decision \nmaking\" (see Section 5).  \nThe panel started with some initial remarks by the moderator briefly relating the four \npresentations. The main remark was to point out the very big difference between the \ncomputational approaches based on deep neural networks and the real brain. \nIndeed, from the presentations of Gustavo Deco and Joan Serrà it was pretty clear how \ncomplex the structure and the functioning of the brain is and how poor and limited are \nthe artificial neural networks (ANNs) are, including DNNs. The practice in DNNs relies too \nheavily on \"trial and error\" and this is why we do not really know why sometimes they \nwork so well whereas sometimes they make so dumb errors. The gap between theory \nand practice is very large! DNNs lack explanatory capabilities and they suffer from what \nis known as \"catastrophic forgetting\" which means that they forget the task they have \njust learned as soon as they are trained to perform a new task. This last fact in itself is \nan indicator of the big difference that exists nowadays between human and machine \nintelligence.  \nThe presentation of Gustavo addressed the issue that the brain at \"rest\" (without stimuli) \ngives an output. So, the \"resting\" brain is in fact not resting and displays spatial patterns \nof correlated activity between different areas of the brain. In summary, from his talk, and \nthe subsequent debate in the panel, we could see that the computational modelling of \nthe whole brain dynamics is extremely difficult and extremely far away from the current \ncomputational models used in AI.  \nFrom the talk of Karina Vold and the panel debate we could see other clear differences \nbetween human and machine intelligence, namely: consciousness, evolutionary \nhistory, embodiment, situated cognition (as part of cognitive extension), and very \nimportantly social intelligence.  \nIndeed, we, humans, are social agents and this fact obviously \"shapes\" and extends our \nintelligence. Such fundamental differences raise serious questions regarding \nwhether the goal of building humanlike intelligence is possible. Another \ninteresting aspect that was raised and discussed is that the technology we use can \nbecome functionally integrated into our biological cognitive capacities such that \nthe tools become part of our minds on a pair with our brains (in the sense of what Clark \nand Chalmers call \"extended minds\") and how this can affect human intelligence and our \ncognitive capacities. Another related question is how we can prevent humans from being \nmanipulated and if we should relinquish control to machines.  \nFinally, the presentation of Ruben Moreno-Diaz was about decision making in animals \nand machines and particularly on the presence of slow and fast biases in decision making \ndue to previously existing preferences and information (slow versus fast biases \ndistinction is related to how long before the previous preference was chosen or how long \nbefore we had the piece of information that affected our choice). Given that we almost \nnever start from an indifference state. One important question is whether we can avoid \nbiases. The answer seems to be no in general. However, the biases can vary in strength \nand can be partially controlled. The strength of the biases seems to be related to how \nartificial the task -that is the object of decision- is, that is how much it deviates from the \n\n \n19 \n \nnatural setting it was designed to operate. One claim of this work is that thanks to the \npresence of biases we can better compare human with machine performance in situations \nwhere the experimental conditions deviate from natural environments. \nAs a final wrap up summary we could say that the state of the art of machine \nintelligence is still extremely far from human intelligence and it is very \ncontroversial whether, in spite of the recent results based on deep learning, there has \nbeen real scientific progress towards the extremely ambitious goal of achieving human-\nlike AI. Another relevant question is: Do we really need it? \n\n \n20 \n \nPart II: Algorithms’ impact on human behaviour \nIn this part, we address the following research questions:  \n— How do algorithms, when exploited in different applications, affect human cognitive \ncapabilities? \n— How do algorithms have the potential to modify the way humans make decisions \nbased on them (e.g. influence of recommendations, personalization)? \n— Which are the suitable strategies for effective human-algorithm interaction?  \n\n \n21 \n \n7 Artificial Intelligence: an interesting leverage point to \nrethink humans' relations to machines…and to \nthemselves. \nNicole Dewandre \nJoint Research Centre, European Commission \n7.1 Statement  \nThe understanding and effect of the expression \"Artificial Intelligence\" are strongly \nconditioned by the implicit assumption that intelligence is –ideally- THE specific human \n(male) feature. \"Artificial Intelligence\" is spoken of, either from a creator's perspective, \ni.e. with pride or fascination, or from a slave's perspective, i.e. with fear and resentment. \nIn my contribution, I shall challenge both the creator's and the slave's perspectives and \ninvite to a more agnostic and human-centric approach to AI. \n1. \"Intelligence\" is much more easily granted to artefacts than to humans. Ex1: A fridge \nis deemed to be smart when it sends a signal to inform about a lack of milk. When a \nman or a woman scrutinizes the fridge to check if there is milk, this is not considered \na smart task. Ex2: Public lighting is deemed to be smart if it adapts to the type of \nuser (pedestrian, car, bicycle). If a man or women was posted on each street to turn \nthe lights on according to the type of user, this would not be qualified as a high-\nskilled job! In fact, artificial intelligence is granted to artefacts reacting to their \nenvironment, when human intelligence is, instead, granted to behaviours gaming the \nenvironment, in order to reach an objective or materialise an intention. Mere \nreactivity, when it comes to humans, is not considered as intelligence, but rather as \nweakness. This leads to rethinking human intelligence, and the role of intelligence in \ncharacterising humanness.   \n2. In some places, we need to be reminded we interact with humans and not with \nmachines. For example, a sticker encouraging saying \"Hi\" before asking for a ticket \nreveals that the default solution might have become to get a ticket from a machine \ninstead of from the hands of another man or woman. In the same vein, interacting on \nwebsites, humans are asked to demonstrate to machines that they are humans, for \nexample, through CAPTCHA, to be able to pursue the interaction. This reveals a \ngeneralisation of the fact that human-machine relationships are more and more \npositioning machine in the active role and humans in the passive or reactive mode. \nThis way to see things occults the fact that machines and artefacts are owned and \ndeveloped by agents, corporate or humans. So, instead of considering the human-\nmachine interactions, the focus should be on human (owner/developer)-machine-\nhuman (user) interactions. This leads to rethinking the radical changes in the way \n(smart) artefacts mediate human relations.   \n7.2 Position regarding the research questions \nHow do algorithms have the potential to modify the way humans make \ndecisions based on them (e.g. influence of recommendations, personalization)? \nAs the way humans make decisions is highly conditioned by their environment, and the \nenvironment being more and more pervaded by connected artefacts and algorithms, it is \nobvious that algorithms will impact the way humans make decisions. I would be cautious \nusing the term \"modify\" as if there was a \"before\" and an \"after\" algorithms in the way \nhumans make decisions. This question has to be addressed with the baseline of actual \ndecision-making (partly contingent, depending from partial information, based on some \nrandom or serendipity) and not idealised decision-making (based on perfect, unbiased \ninformation).    \n \n\n \n22 \n \nWhich are the suitable strategies for effective human-algorithm interaction? \nAs suitable strategies might depend from addressing the users' perspective or the \nowner's perspective, I shall deliberately answer this question from the user's perspective. \nFrom the users' perspective, it is essential to protect the attentional sphere of the users. \nPotentially, connected machines and AI could cannibalise human attention to a point that \nhuman attention is totally \"sucked up\" by machines, and humans are prevented from \ndirecting their attention according to their own desire or to each other. I recommend \nfocussing on the situation of users faced with multiple systems instead of thinking of \neach application separately. Besides protecting the vulnerability of our attentional \nspheres, it is also essential to re-create the conditions enabling trust, and making sure \nthat fooling each other is not a winning strategy. Instead of addressing these issues \nthrough control, I would recommend a minima- recreating the conditions allowing each of \nus to know if he or she is in an environment which \"recognizes\" him or her, and what is \nthe impact of this recognition. For example, when I look for a good or service on the \ninternet, is the price which is offered the same that anybody else would be offered? And \nif not, what is the impact of the environment adapting to me (in this example, with \ndynamic pricing)?        \n7.3 Challenges \n \nCritical approach of intelligence: what it is; its role in characterising humanness; the \narticulation between intelligence, knowledge and power. \n \nWhat is the effect of AI on human relations, and on the relation between humans and \ntheir environment (mix of artefacts and nature)? \n \nWhat does AI not change? In other words, what are the invariants with the past? \nReferences \n— Cohen, J. (2012). Configuring The Networked Self, Yale University Press. \n— Floridi, L. (Ed.), The Onlife Manifesto, Springer, 2015. \n— Pasquale, F. (2015). The Black Box Society, Harvard University Press. \n— Ganascia, J.G. (2017). Le Mythe de la Singularité. Faut-il craindre l'intelligence \narticficielle? Seuil. \n \n \n\n \n23 \n \n8 Algorithmic discrimination \nCarlos Castillo1 \nUniversitat Pompeu Fabra \n8.1 Statement  \nStatistical group discrimination is disadvantageous differential treatment against socially \nsalient groups based on statistically relevant facts (Lieppert-Rasmussen, 2013). In this \ndefinition, a group is socially salient if membership is important to the structure of social \ninteractions across a wide range of social contexts; this includes in particular categories \nthat are protected by law, such as individuals with disabilities, and groups defined by \nproperties that are a matter of anti-discrimination law, such as gender, age, religion, \nnational origin, etc. Algorithms based on statistical learning can engage in statistical \ngroup discrimination, if we understand statistically relevant facts as any information \nderived from training data — indeed, there are many examples of this (Hajian et al. \n2016). Hence, the interaction between humans and algorithms should be one in which \nthe human is able to not only understand but also challenge algorithmic decisions. The \n\"FAT\" framework (Fairness, Accountability, and Transparency) has been advanced in \nrecent years as a set of characteristics to which algorithms should adhere. \nRecommendations \n \nThere is a need for more awareness of the limitations of algorithms and the extent to \nwhich they can do harm. \n \nThere is a need for robust evaluation frameworks that do not stop at abstract \nevaluation metrics (such as accuracy and precision) but that consider other elements \nof how an algorithm can affect the lives of people. \n \nThere is a need for mechanisms of effective algorithmic transparency, which is not \nmere access to source code but a degree of algorithmic explainability that enables \nhumans to understand and challenge algorithmic decisions. \n8.2 Context \nDespite lacking “subjective” elements in their decisions, algorithms, particularly \npredictive modelling algorithms that are used to support decision-making, can be \ndiscriminatory. A more precise formulation follows. \nGeneric discrimination  \nThis and following definitions are adapted from Lippert-Rasmussen [2013]. \nX discriminates against someone Y in relation to Z if: \n1. Y has property P and Z does not have property P (or X believes Y has property P \nand X believes Z does not have property P) \n2. X treats Y worse than s/he treats or would treat Z \n3. It is because Y has P (or because X believes Y has P) and Z does not have P (or \nbecause X believes Z does not have P) that X treats Y worse than Z \nIn other words, generic discrimination is disadvantageous differential treatment. \nGroup discrimination  \nX group-discriminates against Y in relation to Z if: \n1. X generically discriminates against Y in relation to Z \n                                           \n1 C. Castillo is partially funded by La Caixa project LCF/PR/PR16/11110009. \n\n \n24 \n \n2. P is the property of belonging to a socially salient group \n3. This makes people with P worse off relative to others \nor X is motivated by animosity towards people with P, \nor by the belief that people with P are inferior \nor should not intermingle with others \n“A group is socially salient if perceived membership of it is important to the structure of \nsocial interactions across a wide range of social contexts” [Lippert-Rasmussen, 2013] \nA socially salient group could be, for instance, gay people. A non-socially salient group \ncould be, for instance, people with brown eyes. \nStatistical discrimination  \nX statistically discriminates against Y in relation to Z if: \n1. X group-discriminates against Y in relation to Z \n2. P is statistically relevant (or X believes P is statistically relevant) \nFor example:  \n \nIf an employer does not hire a highly-qualified woman because among his/her \ncurrent employees, women have a higher probability of taking parental leave, \nthen this employer is engaging in statistical discrimination. \n \nHowever, if an employer does not hire a highly-qualified woman because she has \ninformed him/her that she intends to have a child and take parental leave, then \nthis employer is engaging in non-statistical discrimination. \nIn statistical machine learning \nAn algorithm developed through statistical machine learning can statistically discriminate \nif we: \n1. Disregard intentions/animosity from the definition of group discrimination \n2. Understand the “statistically relevant” part of the definition as any information \nderived from training data. \nBelow, we provide five examples of discrimination in algorithms. \nDisparate impact. The model gives people with P a bad outcome more often, or in other \nterms, people with P experience a higher risk. \nLet’s assume the following table: \n \nFigure 1. Benefit for protected and unprotected groups. \n \nSuppose: \n\"Protected group\" =\"people with disabilities\" \n\"Benefit granted\" = \"getting a scholarship\" \nIntuitively, if: \n\n \n25 \n \na/n1, the risk that people with disabilities face of not getting a scholarship is much larger \nthan c/n2, the risk that people without disabilities face of not getting a scholarship, then \npeople with disabilities could claim they are being discriminated (see, e.g., Pedreschi et \nal. (2012)) \nDirectly and indirectly discriminatory rules. A model associates attribute P to a bad \noutcome, or attribute Q which depends on P, to a bad outcome. \nDirectly discriminatory rules  \nSuppose that from a database of decisions made in the past, after applying an \nassociations rule mining algorithm, we learn that gender = female ⇒ credit = no \nP(gender=female, credit=no) / P(gender=female) > θ \nThis means we have found evidence of direct discrimination (Hajian et al. 2013). \nLack of calibration. The same output translates to different bad outcome probs. for P \nand not P. \nA model lacks calibration if the probability of an actual bad outcome depends on the class \nand not only on the output. \nFor instance, a well-calibrated model for risk of recidivism should have the property that, \nfor every recidivism score generated by the algorithm, the probability of recidivism for \ndifferent groups if the same. \nDisparate mistreatment / Lack of equal opportunity. The false positive rate of the \nbad outcome is higher for P than not P. \nSuppose we have the following distributions of scores (taken from ProPublica’s research \non COMPAS in 2016, https://github.com/propublica/compas-analysis). \n \nFigure 2. COMPAS scores distribution. \n \nIn the left plot, we see that the average score given to white and black defendants are \ndifferent. This does not mean immediately that there is discrimination, but in the right \nplot, we observe that if we consider only those who did not commit a new crime (i.e., \nnon-recidivists), the scores are still different. This gap means disparate mistreatment, or \nlack of equal opportunity. \nUnfair rankings. The model gives people with P a lower ranking. \n\n \n26 \n \n \nFigure 3. Ranking comparison for different genres. \nThe results above are Top-10 results for job searches in XING (a recruitment site similar \nto LinkedIn), for selected professions: “Economist”, “Market Analyst”, and “Copywriter” \n(Zehlike et al., 2017). We observe that there is a difference in proportions between the \ntop-10 and the top-40 in the three cases. \nAlgorithm-human interaction \nHumans need to be able to receive explanations, and to correct outcomes. \nEffective transparency does not mean source code, it means the human can understand \nand challenge the algorithmic decisions. \nFor instance, in RISCANVI (the method used to predict recidivism in Catalonia), experts \ncan correct the assessment generated by the algorithm, indicating, for instance, that in \ntheir opinion the defendant has a higher/lower probability of recidivism that what the \nsystem generates. \n8.3 Challenges \nA personal opinion on transparency  \n \nMany \"customers\" of algorithmic decision making systems do not value transparency. \nTransparency gives them insight on an algorithm and may generate doubts. Many \nwant certainty, even if it is a false certainty. \n \nThis is a perverse incentive for developers/providers, who may exaggerate their \nclaims of accuracy. \n \nAn additional problem is that numbers, plots, charts, suggest objectivity, so lack of \nmathematical literacy becomes problematic, as people tend to trust automated \nsystems more than what they should. \nChallenges  \n \nWe need good evaluation frameworks. How can this be fixed? Improving \nmathematic literacy and using evaluation frameworks that integrate multiple \ndimensions In addition to accuracy: \"dollars saved, lives preserved, time conserved, \neffort reduced, quality of living increased\" [Wagstaff 2012] and respect to privacy, \nfairness, accountability, transparency. \n \nTo the extent that algorithms can engage in disadvantageous differential treatment \nthat leaves people of a socially salient group worse-off, based on statistical \ninformation, algorithms can discriminate. \n \nCurrent research looks at trade-offs of utility and fairness and at mechanisms for \nmitigating unfairness. \nReferences  \n— Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., & Huq, A. (2017). Algorithmic \ndecision making and the cost of fairness. In Proc. of KDD. \n\n \n27 \n \n— Hajian, S., Domingo-Ferrer, J. (2013). A methodology for direct and indirect \ndiscrimination prevention in data mining. IEEE TKDE 25(7), 11 1445-1459. \n— Hajian, S., Bonchi, F., & Castillo, C. (2016). Algorithmic bias: From discrimination \ndiscovery to fairness-aware data mining. Tutorial In Proc. KDD. ACM. Online: \nhttp://francescobonchi.com/algorithmic_bias_tutorial.html \n— Kasper Lippert-Rasmussen: Born Free and Equal? A Philosophical Inquiry Into the \nNature of Discrimination. Oxford University Press, 2013. \n— Kleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent trade-offs in the fair \ndetermination of risk scores. In Proc. ITCS. \n— D. Pedreschi, S. Ruggieri, F. Turini: A Study of Top-K Measures for Discrimination \nDiscovery. SAC 2012. \n— O'Neil, C. (2017). Weapons of math destruction: How big data increases inequality \nand threatens democracy. Broadway Books. \n— Wagstaff, K. (2012). Machine learning that matters. arXiv:1206.4656. 19 \n \n\n \n28 \n \n9 Making quality decisions about the uses of algorithms and \nAI \nVerónica Dahl  \nSimon Fraser University \n9.1 Statement  \nHow do algorithms, when exploited in different applications, affect human \ncognitive capabilities? \nWhile algorithms can be very helpful as extensions of the human brain, unregulated \nalgorithms can tend to create cognitive dissonance between our true power as humans \nand our perception of it, by giving us a disempowering sense of: \n(a) subordination to machines: we must de facto submit to their modes of \ncommunicating, rather than vice-versa- e.g. call centers debug their defective \nspeech recognition systems and decision trees for free at the public’s temporal \nexpense (an instance of the “time theft” crime), often with no helpful end results \nand no possibility to reach a human. \n(b) uncertainty: decisions are often presented as unquestionable just because an \nimpenetrable black box made them, leaving us unable to find out which criteria \nwarranted a decision. This promotes either unwarranted blind trust or defeated \nresignation to uncertainty, not only among the general public but even in the very \nresearchers developing or using the algorithms.   \nHow do algorithms have the potential to modify the way humans \nmake \ndecisions \nbased \non \nthem \n(e.g. \ninfluence \nof \nrecommendations, personalization)? \nAlgorithms being morally neutral, like tools are in general, they can either be used to \ncreate a happier state of the world, or to perpetuate and deepen inequalities and \ninjustices, placing us at greater risk of global catastrophe. In Toby Walsh’s words, the \nfuture is the product of the choices that we make today2. Given the speed at which AI \nadvances unchecked and unregulated, we had better start foreseeing (and legally \nmandating) how algorithms and decisions based on, or made through them are bound to \nchange the world, and how to best exploit them in order to gracefully adapt to the \ncoming changes, that they may  be for universal good. \nUnfortunately, we are already cognitively prone to exhibit an unwarranted level of trust \nin algorithms when we make decisions based on them: we tend to treat information from \nan AI system as is it came from a trusted colleague, when in fact the system cannot even \nexplain to us, like a colleague would, why it reached a given conclusion. A healthy \nskepticism is a must, as is the development of a proactive policy to address the \npotentially destructive consequences of algocracy (leaving decisions to machines), \ntechnological unemployment, and autonomous systems such as killer robots.  \nIt is a fact of life that algorithms will be more and more capable of making decisions that \nwere previously made by humans, and in general, of replacing humans, whose salary \ndemands cannot compete with the under-regulated way in which algorithms have been \nallowed to replace them. But with enough foresight and legal provisions, we can revert \nthis situation in ways that lead to humans universally getting to do work we value, and \nleaving the less desirable aspects of jobs to machines. As a scientific community we \ncould gather around this and similar goals in order to prompt and help legislators in \nbringing the needed changes to fruition, and ensure that the results of research in AI can \nno longer be captured by a few powerful players to the detriment of the public that \nlargely funded that research to begin with. \n                                           \n2 Toby Walsh (2018) Machines that think: the future of Artificial Intelligence. \n\n \n29 \n \nRegulation should also be put in place to prevent humans in power positions from making \npoor or abusive decisions just because these are now made possible by technology, e.g. \nautomated tax reassessments can now be made in blanket fashion and be sent \nmassively, creating a mountain of appeals that remain unsolved for years, since \nemployees can't keep up with their processing. The consequences are dire for those who \nneed tax clearance to proceed with their lives. Many end up simply paying, from either \nneed of clearance or attrition, the undue amounts exacted.  \nWhich are the suitable strategies for effective human-algorithm \ninteraction?  \nEffective beneficial human-algorithm interaction strategies require inclusive, democratic \nparticipation by humans in decision making. Algorithms should be subordinated to \nhumans, serving them in such tasks as finding facts, proposing options and counting \nvotes rather than making decisions that, because not collectively arrived at, cannot but \nbe under-informed and biased. \nProportional gender representation is particularly key to intelligent decision making3 and \nin general propitiates success for both sexes4. \nA good strategic bet for beneficial algorithmic interaction with humans might be to find \nand \nimplement \ngood \nautomated \nmethods \nfor \nparticipatory \nand \nproportionally \nrepresentative decision making, and adopt laws mandating governments to use these \ntools to obtain representative and specific mandates from the people as frequently as \nneeded- at least for the most important questions that affect us all. The state-of-the-art \nin electronic voting allows us to ground democratic power in actual choice and consent. \nThere is no excuse to continue the obsolete system of voting only once every X years for \nthe blanket, static, and not even binding platform of some political party which often \nrepresents a minority when it wins. \nThis single strategy of algorithmically enabling and legally mandating proportionally \nrepresentative decision making, if successful, could then serve to generate all other \nnecessary strategies to ensure that algorithms serve all humanity, by democratic and \nrepresentative, automated universal vote: Should flawed and/or unaccountable \nalgorithms be allowed to replace humans? Should robo-signed mass actions against \ncitizens be legal? Should algorithms pay taxes and benefits like the humans they purport \nto replace would? Should our laws allow for results of publicly-funded research to be used \nagainst the public, e.g. by creating unemployment? Should algorithms contribute to a \nfund for retraining humans into new jobs? Should our right to information be cancellable \nby the use of black box algorithms?  \nEven more importantly, this single strategy could also serve to generate the strategies \nthat are urgently needed to push our Doomsday clock back from the two minutes to \nmidnight it just hit: Should we all endorse the U.N.’s decision to ban nuclear weapons? \nWhat measures should be enacted towards ending violence, inequity, poverty, \ndominance, war, oppression, militarization, ecocide, climate change, etc?  \nWe may not have the political will, as a society that has not quite reached true and \nrepresentative democracy, to collectively and representatively induce the intelligent \ndecision-making processes needed. But at least the necessary tools for reliably and \nefficiently mechanizing these processes are within our reach. As a community of \nscientists, we can, at this point of urgent need, decide to develop them and promote \nthem into use in interaction with all other concerned groups besides ordinary citizens: \nscientists, educators, health care professionals, governmental agencies both national and \ninternational, legislators, judges, politicians, grass-root organizations, etc. Regardless of \nwhat means are chosen, our scientific community is already taking action, cf. their \n                                           \n3 https://futureoflife.org/2016/06/13/collective-intelligence-of-women-save-world/  \n \n4 https://news.ubc.ca/2014/09/30/gender-equality-olympics   \n\n \n30 \n \ncampaign urging a U.N. treaty against killer robots5, or STEM professionals’ initiative to \nmore effectively manage technology and other resources crucial to human welfare6. \n   \n \n \n                                           \n5 https://www.theguardian.com/technology/2018/apr/09/killer-robots-pressure-builds-for-ban-as-governments-meet \n6  http://demilitarize.org/milex-sign-new-statement-climate-change-military-spending/ \n \n\n \n31 \n \n10 Summary of the Panel Discussion “Algorithms’ Impact on \nHuman Behaviour” \nVerónica Dahl  \nSimon Fraser University \nThis discussion tied together Henk Scholten’s talk on “Digital Transformation and \nGovernance on Human Societies”, Nicole Dewandre’s on “AI as an interesting leverage \npoint to rethink humans’ relations to machines... and to themselves” (see Section 7); \nCarlos Castillo’s on “Algorithmic Discrimination” (see Section 8); and Fabien Giraldin’s on \n“Experience Design in the Machine Learning Era” (see Section 9). The moderator’s brief \nposition statement stressed as urgent the need to regulate algorithms, to ensure in \nparticular that the wonderfully powerful tool that AI represents is used only for beneficial \nimpact on human lives and behaviours. \nThe main questions discussed were: \n \nThe need to debunk the view of rationality as the highest human capability, \nestablishing in political terms the relational self as both free and social, so as to \napproach AI in a more human-centered (as opposed to control, malecentered) way. \n \nThe need to systematically develop a proper vocabulary and mindset that will allow us \nto define what we need to do, for whom, and with appropriate measures of how, if \nadopted, it will lead to a better state of the world.  \n \nThe need to develop a vision of fairness in the digital world, and of what it means to \nevolve with AI in a socially-mindful, rather than interest-led, way.  \n \nThe need to correctly conceptualize notions of fairness and privacy, which are \nsometimes incorrectly invoked for the sake of the rational subject’s interested wishes. \nWhile the separation line between using personal data for society’s benefit and \nprotecting it as private might be sometimes unclear, a good rule of thumb might be \nthat someone’s rights end where the rights of others begin, e.g. hiding behind \nencryption for criminal acts would warrant losing one’s “right” to such privacy. Where \nthe separation line is really blurry, laws designed for partial compliance (as exist \nalready in Europe) are usually preferable to algorithmically enforcing total \ncompliance. \n \nThe need for transparency and accountability, defined as giving the public the ability \nto challenge an algorithm’s decision (N.B. this is different from having all details \nabout the algorithm, which may be useless in terms of challenging it). At present, \nSoftware Engineering cannot verify whether our complex, often statistically-based, \nunpredictably but speedily evolving AI systems will behave as planned, but perhaps a \nway will be found in the future. Meanwhile it may be prudent to legally disallow \nalgorithms that cannot deliver transparency and accountability where it is due.  \n \nThe question of why are we being so suspicious or untrusting came up: it goes back \nto the degradation of human-to-human relationships under our globalized neoliberal \neconomic politics, which normalizes human instrumentalization. For as long as \n“progress” means to use less people (improve labour productivity), and machines are \nbeing built to dispense with humans, it is “natural” for humans to end up wondering \nwhen machines will become “human”. A way out might be to legislate that the uses of \n(publicly funded in particular) AI must benefit, not hinder, the public, e.g. by making \nalgorithms (in fact, those operating them) pay taxes and benefits, and contribute also \nto a fund for retraining into new jobs the people they’ve disloged. \n \nThe need to decide as a society, and legislate, who controls the results of AI, what is \ndone in AI, and for whom. The absence of adequate laws for common good \nendangers modern political order, since a few very powerful global companies can \nprofoundly influence many areas of daily life unchallenged, potentially generating \n\n \n32 \n \ninformational dictatorships able to manipulate the behaviours of humans and \norganizations alike, and even to erode representative democracies and world peace. \nSome possible strategies were also put forward: \n \nWe should define machine intelligence not by what exceptional people can do, but \nthrough valuing what is widely shared.  \n \nProtect the attentional sphere of the users from the demands of multiple systems. \n \nEducate governments and the public about AI’s fallibility and limitations, e.g. neural \nnet based systems cannot be totally autonomous given that they can have \ncatastrophic failures embedded which cannot be anticipated until they occur. \n \nConjure as much, and as representative, citizen participation as possible for decision-\nmaking on how to instrumentalize AI and technology in general for social benefit (e.g. \n7) . Many frameworks are possible, which must be balanced for efficiency and to not \ncause fatigue, e.g. as part of municipal bills of rights to be developed, or as machine \nassisted collective vote for the more critical issues (See more details in 8). Whatever \nthe framework, proportional representation stands out as the crucial ingredient for \ngenerating the most intelligent decisions9. \n \nWork with legislators to bring about the needed laws that will protect us from the \nmain dangers of unregulated algorithms (such as technological unemployment, \nalgocracy, killer robots), and will ensure that they are used to help solve humanity’s \npresent problems for universal benefit rather than for that of a privileged few. \n                                           \n \n8 George Monbiot (2017) Out of the Wreckage. Verso. \n9 https://futureoflife.org/2016/06/13/collective-intelligence-of-womensave-world    \n\n \n33 \n \nPart III: Evaluation and regulation of algorithms \nIn this part, we address the following research questions:  \n— How should algorithms be evaluated in a research vs industrial context?  \n— Which are policy needs in terms of the usage of algorithms into real applications? \n— Which is the research needed to support these policy needs? \n \n\n \n34 \n \n11 Reality, requirements, regulation: Points of intersection \nwith the machine-learning pipeline \nMartha Larson \nRadboud University \n11.1 Statement  \nIntelligent systems such as search engines and recommender systems play an important \nrole in mediating our consumption patterns and our decisions. We use such systems daily \nin order to find useful documents in the large amounts of content available online, and \nwe perceive their ability to greatly exceed that of a single human searching by hand. \nToday’s search engines and recommender systems go beyond documents such as \nwebpages or books to also provide users with multimedia content (e.g., images, music, \nvideos) and items, services, and opportunities in both the online and offline worlds (e.g., \nplaces to live, places to eat, jobs to apply for).  \nAt the core of search engines and recommender systems lie machine-learning \nalgorithms. These algorithms can be considered recipes that take a large amount of data \nas input and provide predictions (i.e., a list of results or recommendations) as output. In \ncontrast to conventional computer programs, it is not possible to know exactly what the \noutput will be in a given situation.   \nSearch engines and recommender systems are considered intelligent since they produce \nin a blink of an eye a result that would have, in previous eras, taken a long consultation \nwith a reference librarian, or a detailed discussion with the clerk of a video rental store. \nWe experience these systems as doing something that humans are good at doing, doing \nit faster and doing it in an environment containing seemingly unlimited information. \nHowever, how do we know that these systems are actually working the way we \nassume they are working?  \nIn order to answer this question, we must evaluate the performance of AI systems. \nUpon first consideration, it seems easy to argue that it is impossible to evaluate search \nengines and recommender systems. As stated above, the very reason why we build such \nsystems is because we find human effort alone would fail to find the results that AI is \ncapable of generating. If we as humans are not able to find the “right answer” to our \ninformation needs, given a large collection of documents, how can we possibly know that \nwhat an AI system finds for us is optimal, or even correct? The issue is compounded by \nthe problem of not knowing in advance the output of an AI system in a particular \nsituation, due to the nature of machine learning. However, the dangers are clear: we \ncannot base our decisions and behavior on systems we are not sure we can trust to be \ndoing what they are supposed to do. Giving up on evaluating AI systems is not an option. \nThe way to proceed is to return to engineering best practices. In particular, we must \nspecify a set of requirements for our systems, and verify that they meet those \nrequirements. Ultimately, the position that it is impossible to evaluate search engines \nand recommender system is untenable because adequate effort has not yet been devoted \nto establishing requirements and to evaluating systems. The history of engineering is a \nhistory of finding solutions for problems that initially appear impossible. \nIn order to come near the amount of effort it will take to apply engineering best practices \nto the evaluation of search engines and recommender systems it is important to admit \nthe possibility that AI evaluation is actually more difficult than AI itself: If AI is smart, \nAI evaluation must be smarter. Human engineers created AI in the first place. There \nis no a priori reason to assume that they are not also able to develop procedures and \nprocesses that are actually “smarter than AI”. However, writing requirements and \nevaluating systems is a tedious and costly process, and must be recognized as such. If \nwe are to insist on recommender systems and search engines that have been properly \n\n \n35 \n \nevaluated and demonstrated to be performing according to specifications, then we must \nnecessarily give up our assumption that AI is some sort of a shortcut or is necessarily \ngiving us something for free.   \nThe return to engineering best practices will mean not just focusing on system output, \nbut writing requirements for, and evaluating, every step along the machine learning \npipeline (See Figure 4). \n \nFigure 4. Machine Learning (ML) Pipeline. Evaluation of AI with respect to requirements \nand opportunities for regulation can be identified at every stage of the pipeline. \nFormulating requirements for each stage of the pipeline requires considering the \ninterplay of that stage with reality (i.e., with what the machine learning system attempts \nto capture). These stage-wise requirements provide detailed insight in what is happening \ninside an AI system. They can be seen as handles that allow us to get a grasp on the \nfunctioning of the system and to guide it toward desirable behavior and away from \nharmful behavior. The requirements represent an opportunity for regulation, which \nprevents machine learning from causing harm to individuals or society.  \nIt is important to be aware that requirement specifications are in the interest of \nindustry, and that grounding regulations in requirements has potential to \nsupport compliance. Companies are highly motivated to work according to engineering \nbest practices. A well-specified set of requirements allows a company to focus on its core \nobjectives, and be sure that the resources it is using are being directed to accomplish \nthese objectives. The benefits of requirements to companies using recommender systems \ncan be seen in concrete requirements documents, e.g., (CrowdRec project, 2014) and \njoint industry-academic visions on evaluation (Said et al., 2012). Requirements also \nallow companies to understand how they can adapt to regulations, including both \nachieving compliance and updating business models. For example, data regulations might \nmake it difficult to compete with respect to collecting users’ personal data (“Data” stage \nin Error! Reference source not found.), freeing the company from the need to devote \nesources to this area, and allowing it to shift its attention to competing with respect to \nalgorithms (“ML Algorithm” stage in Error! Reference source not found.).  The \nontribution of data protection law to the quality of data science R&D has been pointed out \nby Mireille Hildebrandt (2017), who states, “By requiring the specification of one or more \nlegitimate purposes by the controller, data protection law unwittingly contributes to \nsustainable research designs that have a bigger chance of making good sense of the data \nthan sloppy exploratory research that covers its tracks in the name of experimentation \nand the freedom to tinker.” Here, we point out that the positive impact of regulation \npromoting engineering best practices need not be unwitting.  \nEach stage of the machine-learning pipeline presents its own challenges, and for none of \nthem is it easy to write requirements.  For example, in the case of image search engines, \ntraining labels for image data (“Labels” stage) must incorporate the multiplicity of users’ \nperspectives on images (Larson et al., 2014)(van Miltenburg et al., 2017). Further, \nrequirements for the stages may interact. For example, minimizing training data (“Data” \nstage) might speed up algorithms (“ML algorithms” stage) and promote user privacy \n(“Impact” stage) (Larson et al., 2017). \nFinally, we point out that “ML Team” and “Impact” are shown in Error! Reference \nource not found. with a highlight intended to draw attention to the fact that machine \nlearning starts with people (the people on the ML team that creates systems) and \nultimately impacts people. Keeping people central in the ML pipeline requires investing \neffort into education. Education should include engineering best practices, but also \nencourage people to understand themselves and their personal worth in an era in which \n\n \n36 \n \nmore and more of our lives are mediated by digital technology, cf. e.g., the thought of \nJaron Lanier (2010). Engineering best practices have the potential to remind us that AI \nmay appear magic, but it is the magic of a magician: we enjoy the tricks, but \nrealize that they do not change physical world as we know it. With time and \neffort, we can also break AI down into its component stages, specify the requirements for \neach stage, and evaluate and regulate it using those requirements. \n11.2 Challenges \n \nPutting as much effort into designing the requirements for AI, and evaluating AI as \nwe put into creating AI in the first place: concretely, this effort includes building on \nexisting benchmarking initiatives and starting new initiatives. \n \nEnsuring that the requirements for “Output” and “Impact” are consistent with fair \ntreatment of user populations (bias elimination) and individuals (prohibiting harmful \nmicro-targeting). This in turn means new requirements on “ML Algorithms”, but also \non “Data” and “Labels”. \n \nEnsuring that the next generation of machine learning scientists is trained with the \nskills necessary to apply engineering best practices to AI evaluation, and are also \nmotivated to do so. \nReferences \n— CrowdRec Project. 2014. Crowd-powered recommendation for continuous digital \nmedia access and exchange in social networks: Second Iteration Requirements. \nDeliverable 2.3. \n— Hildebrandt, M. Privacy As Protection of the Incomputable Self: From Agnostic to \nAgonistic \nMachine \nLearning \n(December \n3, \n2017). \nAvailable \nat \nSSRN: \nhttps://ssrn.com/abstract=3081776   \n— Lanier, J. You Are Not a Gadget: A Manifesto. 2010. Alfred A. Knopf.  \n— Larson M., Melenhorst M., Menéndez M., Xu P. (2014) Using Crowdsourcing to \nCapture Complexity in Human Interpretations of Multimedia Content. In: Ionescu B., \nBenois-Pineau J., Piatrik T., Quénot G. (eds) Fusion in Computer Vision. Advances in \nComputer Vision and Pattern Recognition. Springer, Cham. \n— Larson, M.,  Zito, A., Loni, B., Cremonesi, P. 2017. Towards Minimal Necessary Data: \nThe Case for Analyzing Training Data Requirements of Recommender Algorithms. \nACM \nRecSys \nWorkshop \non \nResponsible \nRecommendation \n(FATRec \n2017). \nhttp://scholarworks.boisestate.edu/cgi/viewcontent.cgi?article=1000&context=fatrec  \n— Said, A., Tikk, D., Shi, Y., Larson, M., Stumpf, K., Cremonesi, P. 2012. Recommender \nSystems Evaluation: A 3D Benchmark. ACM RecSys Workshop on Recommender \nSystems \nEvaluation: \nBeyond \nRMSE \n(RUE \n2012). \nhttp://ceur-ws.org/Vol-\n910/paper4.pdf  \n— van Miltenburg, E., Elliott, D., Vossen, P. 2017. Cross-linguistic differences and \nsimilarities in image descriptions. Proceedings of The 10th International Natural \nLanguage Generation conference, 21–30. http://www.aclweb.org/anthology/W17-\n3503  \n \n\n \n37 \n \n12 Benchmarks and performance measures in artificial \nintelligence \nAnders Jonsson \nUniversitat Pompeu Fabra \nArtificial Intelligence (AI) is the field of computer science that studies the automatic \ngeneration of intelligent behaviour from a computational point of view. The term \n\"intelligent behaviour\" is usually defined in terms of how difficult it would be for a human \nto perform a given task (Russell, 2009). Computational problems that are historically \nconsidered part of AI include reasoning, knowledge discovery, planning, learning, natural \nlanguage processing, perception and the ability to move and manipulate objects. \n \nAI algorithms are mainly evaluated along two dimensions: 1) theoretical properties and \nperformance guarantees; and 2) empirical performance.  \nHistorically, the two dimensions carried similar weight, and most AI algorithms were \npublished on the basis of little or no empirical support. This has changed dramatically in \nrecent years, particularly with the advent of deep learning (Goodfellow et al., 2016). \nToday, empirical performance is a major factor in deciding whether a given AI algorithm \nis published, and theoretical analysis carries less weight. In fact, most deep learning \nalgorithms come with no performance guarantees whatsoever. \nEven though empirical performance is perhaps the most immediate measure of how well \nan AI algorithm works, theoretical analysis should not be overlooked as an evaluation \nmetric. Performance guarantees come in many forms: an algorithm may eventually \nconverge to the optimal performance level, or converge to a performance level that is \nwithin some bound of the optimal. Such an algorithm is guaranteed to always work well, \nno matter which task it is asked to solve. In contrast, an algorithm with no performance \nguarantees may perform very well on one task, but fail miserably on another. \n \nAn excessive focus on empirical performance provides researchers with strong incentives \nto boost the performance of their own algorithm relative to other algorithms, since this \nmeans their algorithm is more likely to be published. Researchers often make strong \nclaims about their empirical results, such as having \"solved\" a particular domain or \nachieving human-level or superhuman-level performance. These claims should often be \ntaken with a grain of salt, and independent verification and reproduction of empirical \nperformance is becoming an increasingly vital task in order to establish the correctness \nof published results and determine whether they carry over to other domains. \n \nWithin the scope of empirical evaluation, there is also the question of precisely how the \nevaluation is carried out. In the case of stochastic algorithms that depend on random \nelements, a natural performance measure is the average performance across multiple \ntrials, enhanced with a variance measure to test for robustness. However, researchers \noften publish the average of the K best trials, often without stating how many trials were \ncarried out in total (Henderson et alt., 2017). The performance of deep learning \nalgorithms is also highly dependent on other factors, such as the initial random seed, the \nvalues of hyperparameters, the network architecture, etc. Researchers often do not \npublish the details of how their algorithm was configured, making it harder to reproduce \nthe work. \nTo apply AI algorithms in real-world domains it is necessary to establish much more \nstringent evaluation criteria. Ideally, these criteria should be adopted not only by \nresearchers implementing AI algorithms in real-world domains, but by most or all \nresearchers in AI. In addition, publishing source code and data would make independent \nverification and reproduction much easier. These measures would make published results \nmuch more trustworthy and make it easier to determine which algorithms hold the \nhighest potential for real-world problems. Often, relatively basic statistics are sufficient to \n\n \n38 \n \nstrengthen the aggregate results of multiple trials, compare different alternative \nalgorithms, handle unbalanced datasets, etc. \nAnother measure that typically strengthens published results is to establish a set of \nbenchmark problems for a given domain. Sometimes benchmarks are accompanied by \ncompetitions in which algorithms square off against each other on a subset of the \nbenchmark problems. Benchmarks are normally available to the public, and their purpose \nis to provide a much more unbiased system for comparing AI algorithms. The more \nbenchmark problems exist and the more diverse they are, the more difficult it becomes \nto artificially boost the performance of a given AI algorithm. If benchmarks reflect the \ndifficulty present in real-world problems, the performance of an algorithm on the \nbenchmarks should have a higher chance of carrying over to other, similar domains. \n \nAs an illustration of the importance of benchmark problems, consider the problem of \nclassifying objects in images. State-of-the-art algorithms for image classification have \nbeen evaluated for many years as part of the ImageNet Large Scale Visual Recognition \nChallenge, or ILSVRC (Russakovsky et al., 2015). From 2010 to 2015, the error rate of \nthe winning algorithm at ILSVRC decreased steadily from 30% to 5%, which is similar to \nthe error rate observed in humans. Since the datasets used for the competition are large \nand contain a lot of labelled examples for training and evaluation, and since many \nexperiments have been carried out with humans acting as the classifier, many \nresearchers conclude that state-of-the-art algorithms have become competitive with \nhumans. \n \nBenchmarks are not without problems, however. Sometimes benchmarks do not \naccurately reflect possible real-world problems of a given domain, which may lead \nresearchers in the wrong direction. Sometimes so much computational power is required \nthat only a few select companies or organizations have the computational resources \navailable to solve all benchmark problems. The prospect of winning a competition may \nalso cause researchers to implement algorithms that do not really advance the state-of-\nthe-art. A common example are portfolios that are optimized to select between the most \nsuccessful existing algorithms by deciding for example how many seconds of \ncomputational time should be allocated to each algorithm. \nComparing the performance of AI algorithms with that of humans is not only of academic \ninterest but effectively determines when it may be beneficial to replace human expertise \nwith algorithms. An important aspect that affects the quality of such a comparison is how \neasy it is to measure success in a given problem. At least part of the reason for the \npopularity of AI in games is that success is very easy to measure. In real-world problems \nsuccess may be much harder to measure, however. Eventually, as AI algorithms become \nbetter at high-level reasoning, new performance measures will likely be needed since \nsuccess is not only measured by the performance on a single isolated task, but \nperformance across tasks and how well the algorithm can integrate information from \ndifferent tasks. \nReferences \n— Goodfellow, I., Bengio, Y., Courville, A. \"Deep Learning\", MIT Press, 2016. \n— Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., Meger, D. \"Deep \nReinforcement Learning that Matters\", arXiv:1709.06560, 2017 \n— Russakovsky \net \nal. \n\"ImageNet \nLarge \nScale \nVisual \nRecognition \nChallenge\", \nInternational Journal of Computer Vision, 2015 \n— Russell, S.J., Norvig, P. \"Artificial Intelligence: A Modern Approach\", 3rd edition, \nPrentice Hall, 2009. \n \n\n \n39 \n \n13 The IEEE P7003 Standard for Algorithmic Bias \nConsiderations \nAnsgar Koene \nUniversity of Nottingham \n13.1 Statement  \nThe rapid advance in the application of algorithmic decision making and machine learning \nmethods to real-world applications, like screening of job applicate CVs, public-sector \nresource allocation (e.g. policing) and autonomous vehicles, with potentially significant \nimpact on peoples’ lives has generated an urgent need for practical guidelines and \nindustry (self-)regulation in order to ensure that the highest standards of responsible \nconduct are applied as these powerful new algorithmic systems are developed and \ndeployed.  \nA key challenge when it comes to the regulation of algorithmic decision making systems \nis that any evaluation of the bias/fairness of these system must take into account the \ninherently socio-technical context of how the system is (intended to be) used. When used \nfor impactful decisions, the norms that an algorithmic system must obey are not just \nstatistical, but also legal, moral and cultural [Dansk and London 2017].  \nIn recognition of these challenges professional associations such as the ACM and the \nFAT/ML community have responded by publishing Principles for Algorithmic Accountability \n[ACM 2017, FATML] and a Social Impact assessment statement for Algorithms [FATML]. \nAround the same time the IEEE launched the IEEE Global Initiative on Ethics of \nAutonomous and Intelligence Systems which is developing a document [IEEE 2017] and \na series of ethics based industry standards aimed at moving the discussion beyond \nstatements of principles toward practical standards and policies. \n13.2 Future challenges \n \nThere is a need for more multidisciplinary coordinated thinking about the ways in \nwhich algorithms impact individuals and society. \n \nThere is a need for clear assessment and certification regimes to communicate to \nusers which algorithmic systems have implemented best practices for avoiding \nalgorithmic bias. \n \nThere is a need for research on effective benchmarking and impact assessment \nmethods, especially regarding social impacts that go beyond statistical assessment of \ndisparate outcomes. \nReferences \n— David Danks & Alex John London (2017). Algorithmic Bias in autonomous Systems. In \nProc. \nIJCAI \n2017. \nOnline: \nhttps://www.andrew.cmu.edu/user/ddanks/papers/IJCAI17-AlgorithmicBias-\nDistrib.pdf \n— ACM \n(2017) \nOnline: \nhttps://www.acm.org/binaries/content/assets/public-\npolicy/2017_joint_statement_algorithms.pdf \n— FATML \nOnline: \nhttps://www.fatml.org/resources/principles-for-accountable-\nalgorithms \n— The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. Ethically \nAligned Design: A Vision for Prioritizing Human Well-being with Autonomous and \nIntelligent \nSystems, \nVersion \n2. \nIEEE, \n2017. \nOnline: \nhttp://standards.ieee.org/develop/indconn/ec/autonomous_systems.html   \n\n \n40 \n \n14 Algorithms and markets: a need for regulation? \nHeike Schweitzer \nFreie Universität Berlin \n14.1 Statement \nThree main developments characterize the ongoing digitalization of the economy: (1) The \nincreasing amount and importance of automatically generated data for the creation of \nnew products and services and for the organization of all economic activities; (2) the \ndevelopment of ever more sophisticated algorithms to make economic use of that data; \nand (3) the rise of new business models based on data and its systematic analysis and \nuse. \nThese three developments fundamentally change the way we communicate, interact \nsocially and act (and are treated) on the market. As a consequence, the existing social \nand legal order has come under pressure: Its ability to ensure the fundamental values of \nour societies – among them private autonomy, privacy and a sophisticated control of \nprivate and public power – can no longer be taken for granted.  \n \nAs a reaction to the new explosion of data, data analysis capabilities – and possibly \nalso “data concentration” – we have to rethink the informational order as it has \nexisted in the past. This includes a rethinking of data protection rules as a new legal \ninfrastructure for the functioning of markets, a rethinking of the role and \nresponsibility of information intermediaries and the like. \n \nThe ever more widespread use of “autonomous agents” challenges our concepts of \nagency, responsibility and liability. We have to rethink the rules on decision-making, \ndiscuss the legal demands we want to place on decision-making by autonomous \nagents and the preconditions for attributing such decisions to natural or legal \npersons.  \n \nTo what extent do we want to bind autonomous decision-making software to anti-\ndiscrimination rules and/or other ethical standards? \n \nIf decision-making by self-learning algorithm resembles more  some sort of \n“intuitive”/unconscious decision making than a conscious/rational decision-making \n(Mireille Hildebrandt): To what extent do we require a rational justification of \ndecisions based on transparent criteria such that their fairness can be subjected to \nlegal control? When / how do we want to impose such requirements on private actors, \nas opposed to state actors? \n \nWe have to understand how the use of algorithms affects markets and analyze \nwhether adjustments to the existing rules – in particular: data protection, fair trading \nand competition rules – are needed to safeguard their well-functioning and fairness. \nTo make things more complicated, these three challenges are not separate, but deeply \ninterlinked. We have to understand and handle the interdependency of orders \n(information order, decision making systems, market order). \nThe goal must be a reconceptualization of these orders and their adaptation to the new \nchallenges of a digital society and economy in a way that preserves the fundamental \nvalues that we continue to regard as the basis of our democratic societies and social \nmarket economies. \n14.2 Research questions \nThe focus here is on how the use of algorithms affects markets and whether, due to such \neffects, new needs for regulations arise. \nIn the currently ongoing debates, the focus is on three potential risks associated with the \nexponential growth of the use of algorithms in the marketplace:  \n\n \n41 \n \n(a) Algorithms can increase market transparency – will they thereby facilitate \ncoordination among suppliers as well as buyers? Under what conditions? Product \nhomogeneity and a small number of actors in the market are familiar factors. But in an \nalgorithm-driven market environment, do we have to expect collusion also where product \nheterogeneity, innovation and heterogeneity of preferences prevail, and where products \nand services become more and more individualised? Do we need to adjust existing \ncompetition rules to deal with the risk of “algorithmic collusion”, namely abandon the \ndistinction between “independent decision-making” and collusion, so as to cover parallel \nbehavior also?  \n(b) Algorithms can facilitate price discrimination among different buyers, possibly even \nenabling some sellers to approximate perfect price discrimination. Is this a challenge to \nthe well-functioning of markets that the law should address? Does it threaten to \nundermine the general trust in the fairness of market-functioning? \nSo far, the main task algorithms perform in the platform economy is to find the right \nmatch between heterogeneous products and heterogeneous preferences. In this regard, \nthey “discriminate” according to preferences. There is a worry, though, that based on \ndetailed personal profiles and an increased algorithmic understanding of “situational” \npower algorithms will offer the same products at different prices to different consumers. \nWill this do harm to the well-functioning of markets – despite the fact that, arguably, \noutput will be maximized? What are the distributional consequences? What are the \nconsequences for trust in markets? Can buyers self-defend against such uses of \nalgorithms? Is there a risk of a costly “arms’ race”? Is there a need for transparency (a \nduty to disclose arguably already follows from unfair trading law) or for more intrusive \nregulation?  \nDoes competition law provide a layer of protection? In the “old world”, active consumers \nwere meant to provide protection also to the lazy consumers. In a world characterized by \npersonalization, this may no longer hold. Should competition law react by narrowing \nmarket definitions and expanding its concept of market power to cases of situational \npower? Or should private law react by specifying its concept of “bonos mores”? Can data \nprotection law contribute to the solution of the problem, and if so: how? \n(c) Which principles apply to the use of algorithms by digital information intermediaries – \nand in particular: by digital information intermediaries with some degree of market \npower? \nThe explosion of information needs to a new importance of information intermediaries \nthat ensure an efficient matching of parties. What is the effect of these intermediaries on \nmarket functioning? The intransparency of matching algorithms may significantly \ndecrease the risk of collusion between sellers in markets – all the more, since information \nintermediaries should generally not be interested in such collusion. Also, the presence of \ninformation intermediaries should lower the risk of consumer exploitation – at least to \nthe extent that they offer a meaningful product and price comparison. New risks can \narise when intermediaries themselves possess some degree of market power – either on \nthe business side or on the consumer side; and all the more, if the intermediaries are \nvertically integrated. Do we need a generalized principle of “digital intermediary \nneutrality” to be implemented into the relevant ranking algorithms? or a principle that \noutlaws the algorithmic priorization of vertically integrated offers? A fiduciary duty of \npersonal butlers (as a specialized version of information intermediary / agent) vis-à-vis a \nconsumer using it? Are digital information intermediaries under an obligation to explain \nthe ranking of their offers – in order to effectively outlaw discrimination and/or self-\npriorization? \n(d) Primarily, markets are meant to ensure an efficient allocation of resources. In order \nto perform this function, a degree of trust in the well-functioning and fairness of these \nmarkets must exist. In the presence of algorithm-driven markets: Do we need new rules \nfor ensuring such trust? Do we need to expand the existing anti-discrimination rules as \n\n \n42 \n \nthey apply also to algorithmic decision-making? Do we need new rules to avoid consumer \nexploitation in the light of potentially new degrees of information asymmetry? \nReferences \n— Solon Barocas / Andrew D. Selbst, Big Data’s Disparate Impact, 104 California L. Rev. \n671 (2016) \n— Bryce Goodman/Setz Flaxman, European Union regulations on algorithmic decision-\nmaking and a ‘right to explanation”, research paper 2016 \n— Mireille Hildebrandt, Smart Technologies and the End(s) of Law, Edward Elgar 2015 \n— Joshua A. Kroll / Solon Barocas/Edward W. elten / Joel R. Reidenberg/ David G. \nRobinson / Harlan Yo, Accountable Algorithms, 165 U. Pa. L. Rev. 633 (2017) \n— David Lehr/Paul Ohm, Playing with the Data: What Legal Scholars Should Learn About \nMachine Learning, 51 Univ. of California D. L. Rev. 653 (2017) \n— Frank Pasquale, The Black Box Society, Harvard University Press 2015 \n \n \n\n \n43 \n \n15 Evaluation and regulation of algorithms: summary of \ndiscussions  \nXavier Serra  \nUniversitat Pompeu Fabra \nThe panel on Evaluation and regulation of algorithms included five presentations \naddressing \ndifferent \ntopics and \naddressing \nthem \nfrom \nvery \ndifferent \npoints of \nview. Alessandro Annoni, from the Joint Research Centre of the EU, talked about \n“Digital transformation and artificial intelligence: the policy-oriented perspective” \nwhere he explained the current and future policy EU initiatives related to AI from a \nregulation perspective. Martha Larson, from Radboud University and TU Delft, talked \nabout “Reality, requirements, regulation: points of intersection with the machine \nlearning pipeline” in which she emphasized the proper development of benchmarking \nfor evaluating information retrieval systems (see Section 11). Anders Jonsson, from the \nUPF, talked about “Benchmarks and performance measures in artificial intelligence” in \nwhich he presented various approaches to benchmarking in AI from an academic \nperspective (see Section 12). Ansgar Koene, from the University of Nottingham, talked \nabout “The IEEE P7003 Standard for Algorithmic Bias Considerations” in which he \ndescribed the IEEE initiative on Ethics of Autonomous and Intelligent Systems and \nthe different standards that are being developed under it (see Section 13).  Finally, Heike \nSchweitzer, from Freie Universität Berlin, talked about “Algorithms and markets - a \nneed for regulation??” in which she introduced a legal perspective by emphasizing the \nchallenges that AI technologies bring to lawyers, presented in Section 15. \nThere was an initial list of questions that the organizers proposed for discussion: How \nshould algorithms be evaluated in a research vs industrial context?  Which are the policy \nneeds in terms of the usage of algorithms into real applications? Which is the research \nneeded to support these policy needs?. There were some relevant contributions to \nthese questions during the talks, but there was no time for discussion to address them in \ndepth. A much more extensive and focused discussion would be needed. \n \n \n \n\n \n44 \n \nPart IV: Application domains and new paradigms  \nIn this part, we address the following research topics:  \n— Presentation of several application contexts where there is an interaction between \nhuman and machine intelligence and new future paradigms in computation.  \n— Presentation of research areas that will have a future impact on how we understand \nmachine intelligence.  \n \n \n \n\n \n45 \n \n16 Machine learning in healthcare and computer-assisted \ntreatment \nMiguel-Ángel González-Ballester \nICREA and Universitat Pompeu Fabra \n16.1 Statement \nMachine learning is in a phase of renaissance that is transforming practices in multiple \nfields. Beyond the application of previously existing techniques, novel developments in \nbig data and deep learning have transformed the landscape of available methodologies. \nFurthermore, many of these developments are spearheaded by industry, which is leading \nthe application of machine learning to everyday products and services, not least in \nhealthcare. \nImage processing, in particular, has suffered a revolution in the last few years. Current \nmethods based on deep learning clearly outperform the previous state of the art, and \ntheir extrapolation to medical image analysis has shown very promising results in i.e. \nlung cancer detection.  \nComputer aided diagnosis is also benefitting from recent developments in deep learning \nand machine intelligence, particularly in enabling the analysis of large, heterogeneous \nsources of patient data, such as genetic tests, blood and cell samples, imaging \nexplorations and unstructured information from the clinical history of the patient. \nFurthermore, these tools are also being applied to study the aetiology of complex \ndiseases, by finding patterns in large patient databases. \nDespite these impressive initial success stories in medical image processing and \ncomputer-aided diagnosis, strong limitations have become apparent. Modern machine \nlearning is predominantly based on “black box” approaches, failing to provide reasoned \ninterpretations of the diagnoses they provide. Doctors (and we are far from replacing \nthem) cannot afford to incorporate tools that provide a diagnosis with no explanation \nabout the reasons behind this diagnosis. This poses a number of challenges for the \nwidespread use of machine intelligence in healthcare. \n16.2 Future challenges \n \nInterpretability is key to the future success of machine learning and artificial \nintelligence in healthcare. The combination of data-driven (empiricist) and model-\nbased (Platonic) approaches might be key to this end. \n \nThe availability of medical data is often limited by ethical and regulatory issues. \nCurrent trends in data augmentation and generative networks partly help in \nincreasing the numbers of available data, but they risk biasing databases with \nunrealistic (non-disease related) information. \n \nEmbodiment of artificial intelligence for patient care, e.g. through surgical robots, \nrobot companions for the aged, or pervasive access and monitoring of health \ninformation, is an emerging discipline that may revolutionise healthcare. \n \n\n \n46 \n \n17 The influence of “intelligent” technologies on the way we \ndiscover and experience music \nFabien Gouyon  \nPandora  \n17.1 Statement \nThere is an influential loop between technological innovation, the development of \nbusiness models governing the music industry, and the way we discover and experience \nmusic. \nInternet and internet music streaming transformed the music industry. Streaming is now \nthe primary way we listen to music, and this transformation is only at its beginning.  \nUntil recently, the way we listened to recorded music was tightly linked to a relatively \nclear business model. Namely, music discovery would be driven by diverse media (e.g. \nterrestrial radio), targeting a subsequent purchase and ownership of a physical —or \ndigital— artifact of what was discovered, consumption being done via another media \n(e.g. personal CD-player, iPod, etc.). A whole industry (a multi-billion dollar industry) \nwas based on that model, which is now put under pressure. \nUnder this ownership model, once the discovery phase happens and an item is \npurchased, the job of content creators, producers and distributors is basically done. It is \nthe listener who decides how and when to enjoy their music, with little influence from \nwho produced or distributed it. \nNow, with the advent of streaming, we are witnessing a shift from ownership to access. \nAnd with the access model, the line between discovery and consumption is now blurred, \nas the same media now serves both. There is an opportunity for content producers and \ndistributors to guide listeners in their consumption. This opens the way to a much more \nholistic experience. And in return, the listener now requires to be assisted in all aspects \nfrom search, discovery, browsing, sorting through enormous collections of tracks, \nconsumption, sharing, etc. That leaves room for many different novel models of listening \nexperiences.  \nThis is precisely where a crucial part of the music streaming industrial competition is \ncurrently happening. Diverse companies are developing at great speed new products, \nsuch as personalized playlists, radio-like lean-back propositions, etc., aiming at defining \nnew formats of music listening. This calls for developing new technologies for \nrecommending the most relevant content, as well as the most relevant vehicle for \ncontent discovery and consumption.  \nSuch technologies must have the potential to be personalized for all listeners and \nreactive in real-time, and they must balance many factors such as e.g. content \nrepetition, interactivity, or user intent. \nIn other words, the current developments in the music industry that are primarily driven \nby technological innovation are shaping the way hundredth of millions will access music, \nexperience it and socialize around it. It is therefore fair to say that researchers and \ntechnological companies alike should acknowledge their strong cultural and societal \nresponsibilities, and even further, embrace them.  \nLet’s consider a few examples. For instance our responsibilities with the listener: The \nubiquitous availability of (almost) any artefact of the world's music repertoire only a few \nclicks away imply overwhelming choices to music lovers. They need assistance, and we \nhave a responsibility in helping them navigating through, and filtering this flood of \ncontent. Recommendation and personalization technologies can help in this endeavour. \nBut they also are prone to potential algorithmic biases, and can result in a progressive \nisolation of users in their own musical bubbles, hence limiting and ultimately hurting their \nexperience.  \n\n \n47 \n \nLet’s now consider responsibilities with the music ecosystem, which we are part of: The \nfact is, music distribution is extremely unbalanced: a very small proportion of artists (the \n“head” of the distribution) account for most of what’s listened to, while the large majority \nof artists (the “tail”) remain listened by few. There are hits, and there are niches. We \nshould carefully consider the effect technology can have on this distribution. It is \n(relatively) easy to develop technology that could —directly or indirectly— have a \nfavourable distribution impact on either the “tail” or the “head”. These would likely result \nin different economic returns for the industry and for the artists, on the short-term and \non the long-term.  \nFinally, let’s also consider that technological innovation is not only influencing the music \nlistener experience as exemplified above, but it is in fact currently revolutionizing most \naspects of the music industry, from creation, to rights monitoring, marketing, \nmonetization, etc. This could spark similar reflexions on our cultural and societal \nresponsibilities as developers of these technologies. \n17.2 Challenges \n \nHow, in the development of innovative technologies, can we exhaustively identify, \nunderstand and deal with algorithmic biases? \n \nHow can we devise metrics that would approximate long-term user satisfaction? \n \nHow can we make algorithms (e.g. recommendation algorithms) more transparent to \ntheir users? And what degree of transparency is actually desired/required? \n \nHow can we balance ever more adaptive, contextual user experiences and respect for \nprivacy? How can we provide users with more control on the data they provide us?  \n \nWhat level of trust between a user and a technology/service is desirable to achieve? \nHow far can the interaction go, and are there limits to be fixed?  \n \nIn the current context of extremely fast-paced scientific and technological \ndevelopments, a very competitive and dynamic music streaming industry, and the \n\"all-you-can-consume” nature of modern media, how can we help users cope with the \noverwhelming flood of content and products, and help them engage more deeply with \ncontent? \n \nWhat should be the basic methodological steps to follow so that the novel technology \nwe develop not only follows to the latest technological trend, and responds to \nbusiness metrics, but also empowers its users in its very evolution? \n \n\n \n48 \n \n18 HumanAI \nBlagoj Delipetrev \nDigital Economy Unit, Joint Research Center, European Commission \n18.1 Statement \nHuman and Machine intelligence comparison \nThere have been millions of year of biological evolution. Almost three billions years was \nneeded for evolution to create the current human being. Evolution has increased human \nbrain size exponentially over the last 8 million years from below 250 cc to 1500 cc. The \nbrain is the source of our intelligence and separates us from all other species.  \nThe industrial revolution, 2-3 centuries ago, produced machines that replaced physical \nhuman labour. \nThe \ncomputational \nmachines \nstarted \n70 \nyears \nago \nwith \nthe \nsemiconductors and their exponential grow which leaded to the creation of artificial \nintelligence (AI) which replaced humans in cognitive tasks.  \nThe most important point is the time scale. Both human and machine evolutions are \nexponential and while human evolution is in millions of years, the machine evolution is in \ndecades. There is a distinctive difference between human intelligence and the current AI, \nbut this may not be the case in the future. Moore law is not dead, and the machine rise \ncontinues. \nDeep Learning  \nDeep learning (DL) is the flagship of the AI research and achievements in the last \ndecade. Increased computational power and vast amounts of digital data are foundation \nfor DL, which is in essence a multiple layer neural network. DL achieved many \nbreakthroughs, starting from vastly improving image recognition, NLP or translation. The \nDL combined with Reinforcement learning (RL) won the game of GO, Atari, Poker and \nlastly Dota. DL and RL are rapidly expanding.   \nAlgorithmic Impact Assessment  \nThe AI fast pace produced many systems that are used in everyday lives, government \nand public offices. There is a need of validation of these AI systems. Worldwide various \ninitiatives for algorithm impact assessment will evaluate and analyse AI systems and \ntheir decisions and make them more transparent, understandable and explainable.  \nPredictions  \nMy predictions are that “everything electrified will be cognified”, “AI is the new \nelectricity” making devices more intelligent and autonomous. Tasks described in \nproductivity and efficiency will be performed by robots and bots. Now and in near future \nAI is going to complement us in all our daily tasks, as they do already, with our \nsmartphone, computers, etc. In the middle term, the more advanced and intelligent \nmachines will completely replace humans in tasks like transportation, medical image \nrecognition, language translation, etc. In the long term is possible to have AGI or \nsomething close to it that will be capable of doing multiple cognitive tasks better than \nhuman does. This does not mean that humans will be obsolete.  \nIn the meantime, we need to address many current problems and possible AI dangers. \nOne of the most vivid dangers is autonomous weapons. Other highly important topics are \nthe rising inequality, unemployment, fairness, inclusion, social justice, etc.   \n18.2 Challenges \nThe world is on the verge of one of its most valued discoveries AI. There are huge \nbenefits in rising productivity, efficiency, improved standard, longer lifespan, better \nhealthcare, etc. AI will automate most of the tasks, leaving more time for humans to \n\n \n49 \n \nenjoy lives and be creative. AI can bring more happiness and prosperity but also dead \nand destruction. Therefore, there is a need for AI regulation for the benefit of all \nhumanity.    \nReferences  \n— “Life 3.0: Being Human in the Age of Artificial Intelligence” by Max Tegmark \n— “Sapiens: A Brief History of Humankind” by Yuval Noah Harari \n— “How to Create a Mind: The Secret of Human Thought Revealed” by Ray Kurzweil \n— \"Deep learning.\" nature 521.7553 (2015): 436. LeCun, Yann, Yoshua Bengio, and \nGeoffrey Hinton. \n— “How \nAI \ncan \nbring \non \na \nsecond \nindustrial \nrevolution” \nby \nKevin \nKelly \nhttps://www.ted.com/talks/kevin_kelly_how_ai_can_bring_on_a_second_industrial_r\nevolution \n \n\n \n50 \n \n19 Do humans know which AI applications they do need? \nPerfecto Herrera \nUniversitat Pompeu Fabra and Escola Superior de Música de Catalunya  \nIn the last century, humans have developed powerful technologies that, for the first time \nin history have the potential to quickly and irreversibly change the world as it was \npreviously known. Nuclear power and genetic engineering are application areas derived \nfrom useful essential knowledge (which cannot be questioned or censored) that had to be \nsubject to ethical and legal regulation, even by means of international agreements. The \ncurrent state of our knowledge on AI makes some of their applications to be about to \ncross (or maybe already crossing) red lines too and there have been attempts to reach a \nconsensus, \nat \nleast \nin \nthe \nscientific \ncommunity \n(see \nfor \nexample: \nhttps://futureoflife.org/ai-principles/ or http://www.iiia.csic.es/barcelonadeclaration/).  \nIn the panel on the application domains of AI that closed our kickoff-meeting we \nwitnessed some of such concerns, but also other ones that have to do with our concepts \nof humanity or creativity. \nSergi Jordà, in “Enhancing or mimicking human (musical) creativity? The bright and dark \nsides of the Moon” debated on attempts to develop “creative machines” and how many of \nthem cannot shed enough light on human creative or other cognitive processes (one of \nthe goals or justifications of some AI practitioners). In addition, creative systems are \nusually deprecated by their potential users (flesh-endowed music creators), not to \nmention the shallowness or uninterestingness of their outputs (though some outstanding \nexceptions could be considered). This rejection of creative systems could be due to a \nnarrow developing perspective that does not consider the human to be inside a loop with \nthe AI system. The idea of computers as assistants, becoming extensions of their users \nand doing the “dirty” or the short-time unfeasible work, should be promoted and \nresearched (instead of leaving them the option to make the serious decisions). This is \nsomething that Luc Steels, another of the panel participants, commented during the \npanel dialogue (“Intelligence amplification” was the short-name given to that). A final \nissue with creative systems, but also with other AI devoted to “practical” problem-solving \nis that of understanding the outputs and the inner workings leading to them, from a \nhuman perspective, as some participants also remarked with the special case of game-\nplaying AI systems. \nThe idea of assisting humans when dealing with creation is also challenging when they \nare not the creators but they enjoy an artistic creation, as Fabien Gouyon in “The \ninfluence of intelligent technologies on the way we discover and experience music” \nremarked (see Section 17). The almost permanent connection we listeners currently have \nwith music, as a stream passing by or where you live immersed into, calls for ways to \nimprove such listening experiences in “intelligent” ways (recommending truly relevant \ntitles, helping to navigate through options, personalizing musical experiences, connecting \nwith other people or groups, etc.).  \nBiases already noticed in other types of recommenders, and the potential construction of \nisolation bubbles on the side of users, should be carefully counter-acted in order to keep \nthe collective and transformative power of music at its best. \nA very different perspective and application field was discussed in “Machine learning in \nhealthcare and computer-assisted treatment” by Miguel-Ángel González-Ballester (see \nSection 16), where advantages and shortcomings of current medical AI systems, \nespecially those devoted to image-based diagnosis, were discussed. Here, again, the \nrequirement that machine decisions can be interpretable under human (professional) \ncriteria was raised. Additionally, the possibility that diagnosis might be done without \nhumans in the loop (or that machines could bias or override the view and expertise \nbrought by them) has also to be considered. An emergent topic, for which we are \nprobably unprepared yet, was AI embodiment (i.e., what happens when humans \nincorporate, as body parts, AI systems?). Intersecting several of already mentioned hot \n\n \n51 \n \nissues, Blagoj Delipetrev, in “HumanAI How to assess algorithmic impact?” compared \nhuman and machine intelligence remarking what should not be considered as such (brute \nforce approaches) and ways humans and machines could collaborate for a human-\nfavourable scenario (see Section 18). He also discussed applications in satellite image \nrecognition and on the assessment of the impact of AI algorithms. The necessity to deal \nwith natural language and concept generalization before claiming “intelligence” for many \nAI systems was relevantly remarked there.  \nOur last talk, “Will AI lead to digital immortality?”, by Luc Steels, speculated on future or \n“futuristic” applications and issues. Personal assistants are currently being developed \nunder different appearances, and they will probably become autonomous artificial \npersonae that might even impersonate different humans at the same time, not to \nmention that they might have multiple parallel lives, or that they will become immortal \nand then can continue with functions attributed to their formerly-assisted human beings. \nA list of ethical issues open by that perspective was barely touched, although, compared \nwith those that the current intensively-used systems pose, they could be left for some \nfuture HUMAINT version 2. \nDuring the open debate other important topics were lightly touched such as the apparent \noverabundance of AI-based start-up companies without clear business models (which can \ncontribute to the “hype” of the topic), the risk of being monitored in concealed ways (by \nsound recording devices intended to play with toys or to just receive commands), the \ndirect manipulation of behaviour that recommenders or notification services induce in \ntheir users, the blurring between what we thought reality is (or was) and what our \nsenses are processing, the losing of some skills (some of them considered to be \ninherently human, such as caring for other beings, for example), or the consented (or \nworryingly ignored) externalization of some of our decision-making processes. \nApplications of AI could sometimes be perceived as harmful because of their apparent \nextraordinary or superhuman “intelligence” but their most worrying aspects should be \nwatched elsewhere: the difficulties to track or explain their decisions, the difficulties to \nembed AI systems with some moral sense or the subtle or blatant invasion of privacy \nthey can facilitate. Even though there are enough examples of AI systems contributing to \na healthier and more pleasurable existence (i.e., diagnosis systems, helpers for autistic \nor elderly people) and that such systems can help us to cope with data overload and \nstrive in an increasingly complex reality, we should be watchful for some forthcoming \nlarge-scale disruption on the way we see ourselves and the surrounding world.  \nIt’s time to decide how our future should look like, instead of leaving it to be \ndevised just by what technology makes possible. \n \n\n \n52 \n \nPart V: Considerations and conclusions   \nIn this part, we first include a set of written contributions by other scholars that were not \npresented at the workshop but provided relevant input for our final considerations.  \nThen, we provide a set of conclusions to the workshop and directions for future work in \nthe HUMAINT project.  \n\n \n53 \n \n20 Characterising the trajectories of artificial and natural \nintelligence \nJosé Hernández-Orallo \nUniversitat Politècnica de València \nLeverhulme Centre for the Future of Intelligence.  \n20.1 State of the art: An Atlas of Intelligence \nThe comparison between artificial and human intelligence is usually done in an informal \nand subjective way, often leading to contradicting assessments (Kirsh 1991, Hayles \n1996, Brooks 1997, Pfeifer 2001, Shah et al 2016, Lake et al. 2017, Tegmark 2017, \nMarcus 2018). This is especially problematic because of the pace of the epistemological \nchange. Our understanding of intelligence is rapidly progressing from new discoveries in \ncomparative cognition, neuroscience and artificial intelligence. However, there is a \npossibly more relevant ontological change: artificial intelligence is creating new kinds of \nsystems, and it is hence extending the landscape of intelligence (Sloman, 1984). \nMoreover, it is still not fully recognised —and certainly not well understood— that these \ntechnological changes are also affecting human cognition. Put it simply: because of AI, \nhumans now think differently. Overall, and by all means, we have a moving target \nproblem. Can we anticipate these trajectories? \nIn the first place, we need ways of assessing what AI systems can do, what they will be \nable to do in the near future, and the pathways and resources that will be needed to get \nthere. Indeed, we need a common framework to determine which kinds of AI or hybrid \nsystems in this landscape of intelligence are even desirable (needed for society) or \npossibly undesirable (dangerous, too similar to some profession profiles, animals or \nhumans, etc.). For a recent symposium about this see: http://kindsofintelligence.org/. \nThe discussion must not be limited to the way society is affected: the irruption of AI \nsystems with new capabilities may trigger a range of alterations in the very way human \ncognition works. The changes in memory capabilities, development trajectories and \nlearning patterns that we are already observing because of the use of technology \n(negative Flynn effect, Google effect, etc.) can distinctly be regarded as cognitive atrophy \nor enhancement, but are about to change our psychometric profiles in possibly radical \nways. \nThe Leverhulme Centre for the Future of Intelligence (http://lcfi.ac.uk) is working on a \nnew initiative, an atlas of intelligence, to cover a relevant portion of the past, present \nand future landscape of intelligence: including humans, non-human animals, AI systems, \nhybrids and collectives thereof.   \nThe atlas will be based on a set of dimensions, either behavioural features (i.e., the \nfunctionalities, cognitive abilities and personality traits) or physical features (i.e., the \nmechanisms, kinds of sensors and actuators, body morphology, computational or \nneurological resources). The atlas will allow users to make several projections and \naggregations to a smaller number of dimensions. Also, despite the framework not being \nhierarchical, once populated, it could be converted into different kinds of taxonomies by \nusing different distance/similarity metrics (as homology or analogy have been used for \nliving systems) and also exploring a continuum from specialised (task-specific) systems \nto more general (task-independent) systems, including a developmental perspective. \nThe initiative is at an early stage and we welcome associates and contributors. More \ninformation about the specification and prospective maps to be considered for the atlas \ncan be found in (Bhatnagar et al. 2017, 2018). \n \n\n \n54 \n \n20.2 Challenges \n \nHow can we characterise current and future AI systems in terms of cognitive abilities \n(Hernández-Orallo 2017a,b) and compare them to humans? \no \nAre the new evaluation platforms (Castelvecchi 2016, Hernández-Orallo et al. \n2017) going in the right direction? How do the AI milestones relate or compare \nto the milestones in child development or animal evolution? \no \nCan we develop an ability-oriented analysis of job automation rather than \ntask-oriented (Frey and Osborne 2017, Brynjolfsson and Mitchell 2017)? \n \nHow can we characterise the changes in human cognition originating from technology \nand, most especially, from the interaction, replacement or enhancement with AI \nsystems? \no \nHow can we analyse the locations and trajectories of human intelligence and \nAI progress? \no \nHow can the new “cognitive ecosystems” (Hutchins 2010), including humans \nand machines, be affected by these future changes of intelligence and their \neffect on dominance topologies (Cave 2017, de Weerd et al. 2017)? \nReferences  \n— Bhatnagar, S., Alexandrova, A., Avin, S., Cave, S., Cheke, L., Crosby, M., Feyereisl, \nJ., Halina, M., Loe, B.S., O hEigeartaigh, S., Martínez-Plumed, F., Price, H., Shevlin, \nH., Weller, A., Winfield, A. and Hernández-Orallo, J. “A first survey on an atlas of \nintelligence”. (2017) http://users.dsic.upv.es/~flip/papers/Bhatnagar18_SurveyAtlas.pdf \n— Bhatnagar S., Alexandrova A., Avin S, Cave S, Cheke L, Crosby M, Feyereisl J, Halina \nM., Loe B.S., O hEigeartaigh S., Martínez-Plumed F., Price H., Shevlin H., Weller A., \nWinfield A. and Hernández-Orallo, J. (2018) “Mapping Intelligence: Requirements and \nPossibilities”, in Muller V. (ed) Philosophy and Theory of Artificial Intelligence. \n— Brooks, R. A. (1997). \"From earwigs to humans.\" Robotics and autonomous systems \n20.2-4, 291-304. \n— Brynjolfsson, E. and Mitchell, T. (2017). “What can machine learning do? Workforce \nimplications”, Science 22 Dec 2017:  Vol. 358, Issue 6370, pp. 1530-1534, DOI: \n10.1126/science.aap8062, http://science.sciencemag.org/content/358/6370/1530  \n— Castelvecchi, D. (2016). “Tech giants open virtual worlds to bevy of AI programs”. \nNature Vol 540, Issue 7633, pp. 323-324, https://www.nature.com/news/tech-giants-\nopen-virtual-worlds-to-bevy-of-ai-programs-1.21151. \n— Cave, S. (2017) “On the dark history of intelligence as domination” AEON, 21 \nFebruary, 2017. \n— de Weerd, H., Verbrugge, R. and Verheij, B. (2017) “Negotiating with other minds: \nthe role of recursive theory of mind in negotiation with incomplete information” \nAutonomous Agents and Multi-Agent Systems, March 2017, Volume 31, Issue 2, pp \n250–287. \n— Frey, C. B., and Osborne, M. A. (2017). “The future of employment: How susceptible \nare jobs to computerisation?” Technological Forecasting and Social Change, 114, \n254–280, 2017. \n— Hayles, N. K. Narratives of artificial life. Future Natural: Nature, Science, Culture. \n(1996). London: Routledge, 146-64. \n— Hernández-Orallo, J. (2017). The Measure of All Minds: Evaluating Natural and \nArtificial Intelligence, Cambridge University Press. \n— Hernández-Orallo, J. (2017). \"Evaluation in artificial intelligence: from task-oriented \nto ability-oriented measurement\", Artificial Intelligence Review 48 (3), 397-447, 2017 \n\n \n55 \n \n— Hernández-Orallo, J., Baroni, M., Bieger, J., Chmait, N., Dowe, D.L., Hofmann, K., \nMartínez-Plumed, F., Strannegård, C. and Thórisson, K.R. (2017). \"A New AI \nEvaluation Cosmos: Ready to Play the Game?\" AI Magazine, Association for the \nAdvancement of Artificial Intelligence. \n— Hutchins, E. (2010). \"Cognitive ecology\" Topics in cognitive science 2.4 (2010): 705-\n715. \n— Kirsh, D. (1991). \"Today the earwig, tomorrow man?\" Artificial intelligence 47.1-3: \n161-184. \n— Lake, B.M., Ullman, T.D., Tenenbaum, J.B., Gershman S.J.. “Building machines that \nlearn and think like people”. Behavioral and Brain Sciences, 2017. \n— Marcus G. “Deep Learning: A Critical Appraisal” arXiv preprint arXiv:1801.00631. \n2018 (also https://medium.com/@GaryMarcus/in-defense-of-skepticism-about-deep-learning-\n6e8bfd5ae0f1) \n— Pfeifer, R.. \"Embodied artificial intelligence 10 years back, 10 years forward.\" \nInformatics. Springer, Berlin, Heidelberg, 2001. \n— Shah, H., Warwick, K., Vallverdú, J., & Wu, D.” Can machines talk? Comparison of \nEliza with modern dialogue systems”. Computers in Human Behavior, 58, 278-295, \n2016. \n— Sloman, A. The Structure and Space of Possible Minds. School of Cognitive Sciences, \nUniversity of Sussex, in The Mind and the Machine: philosophical aspects of Artificial \nIntelligence, ed. Stephen Torrance, Ellis Horwood, pp 35-42,1984. \n— Tegmark, M. Life 3.0: Being Human in the Age of Artificial Intelligence. Knopf, 2017. \n \n\n \n56 \n \n21 Considerations related to cognitive development children \nNuria Sebastian  \nUniversitat Pompeu Fabra \nMy comments will turn around two issues, both related to the concept of “development” \nthat complements the current views. \n21.1 Differences in machinery \nThere is a fundamental difference between “Artificial Intelligence” and “Human \nIntelligence” related to the enormous changes that the Human hardware (the brain) \nundergoes during life. Comparatively, artificial hardware undergoes relatively small \nchanges (and the changes are not just in its size, but also in qualitative aspects). \nThe architecture of the human brain changes in fundamental dimensions during life. The \nmost dramatic changes take place during the first years of life (I will not refer to prenatal \nchanges / learning, because the point I want to make does not require to address this \nperiod, but there is learning during this time). The number of neurons increases \nexponentially mostly prenatally, but the number of synapses changes in a complex way \nafter birth.  It is worth noticing that changes do not take place in an uniform way across \nthe brain. In general, sensory-related areas develop very quickly (reaching adult levels \nby the end of the first year of life), while “thinking-planning” areas (frontal areas) reach \nadult levels after puberty. On top of complex changes in the number of synapses, the \namount of myelination (related to the effectiveness of neural transmission) diminishes \nwith age (though it does not disappear). \nThese specific patterns impose important constraints the way the brain processes the \ninputs it receives. An important feature of brain development is the extraordinary \nsynchronization between maturation of different brain areas: they become functional \nwhen they are needed. For instance: association areas become functional when “lower” \nareas are effective: there is no energy waste by having areas “waiting” for appropriate \ninputs. Functionality is the product of an exquisite interplay between internal \ndevelopment (gene-regulated, more prevalent early in life) and external input (more \nprevalent late in life). \nOn top of these substantial hardware changes, there are other important development-\nrelated \nchanges \nin \nneurotransmitters \nand \nhormones \nthat \nwill \nhave \ndramatic \nconsequences on the way the brain functions across life. A clear case is the changes in \nsleep patterns taking place in life. Newborns spend most of their time sleeping, while \nelderly people tend to sleep very few hours. There are fundamental changes in the way \nthe brain functions during sleep (not only quantitative, but also qualitatively) and it is \nwell-known the critical importance of sleep in memory-consolidation. \nFinally, newborns (from 4 hours to 4 days of age) and very young infants are able to \nperform complex computation over different types of signals (that seem to be relatively \nexperience-independent). For instance, newborns can notice the difference between \nsome (human) languages, such as Dutch and Japanese, if played forwards, but not \nbackwards (as other species such as cotton-top tamarin monkeys and long evans rats). \nIt is not until 5 months of age that human infants can distinguish English from Dutch, or \nSpanish from Catalan (it is worth noticing that by six months, infants already know \nseveral words). In a different domain, newborns prefer to orient to stimuli with a human \nface configuration than to a random one (Morton and Johnson, 1991).  \nIn summary, there are essential differences between human and artificial “hardware” and \nthey entail critical specificities regarding human learning. \n\n \n57 \n \n21.2 Developmental changes in interaction with computers \nThe presentations have assumed that users interacting with AI systems are adults. \nHowever, there are important differences in the way children and adults deal with \nartificial systems, and this is an under-studied field. \nOne of the few existing studies has investigated the well-known Uncanny Valley effect. \nThere is a vast literature investigating the fact that (human) adults feel uncomfortable \nwhen interacting with very human-like avatars/robots. \n \nFigure 5. Uncanny Valley effect. \nThere is evidence indicating that such effect may be acquired. Young children (under 9 \nyears) do not find “creepy” such very human-like avatars, importantly the feeling of \n“creepiness” is related to children’s assumption that such avatars have human-like minds \n(Brink, Gray and Wellman, 2017).  \nThe investigation of how children interact with machines is a virtually unexplored field. \nSuch investigations are critical when considering the use of AI and robots in educational \n(and health) environments. \nReferences  \n— Kimberly A., Brink, K. G. and Wellman, H. M. (2017). Creepiness Creeps In: Uncanny \nValley \nFeelings \nAre \nAcquired \nin \nChildhood. \nChild \ndevelopment, \nhttps://doi.org/10.1111/cdev.12999  \n— Morton, J., and Johnsson, M. H. (1991). CONSPEC and CONLERN: a two-process \ntheory of infant face recognition. Psychol Rev. 1991 Apr;98(2):164-81.  \n \n\n \n58 \n \n22 The tyranny of data? The bright and dark sides of \nalgorithmic decision making for public policy making \nNuria Oliver \nVodafone Research and Datapop alliance  \n22.1 Statement: Data-driven Algorithms for Public Policy Making  \nToday's vast and unprecedented availability of large-scale human behavioral data is \nprofoundly changing the world we live in. Massive streams of data are available to train \nalgorithms which, combined with increased analytical and technical capabilities, are \nenabling researchers, companies, governments and other public sector actors to resort to \ndata-driven machine learning-based algorithms to tackle complex problems (Gillespie, \n2014). Many decisions with significant individual and societal implications previously \nmade by humans alone --often by experts-- are now made or assisted by algorithms, \nincluding hiring, lending (Khandani et al., 2010), policing (Wang et al., 2013), criminal \nsentencing (Barry-Jester et al., 2015), and stock trading. Data-driven algorithmic \ndecision making may enhance overall government efficiency and public service delivery, \nby optimizing bureaucratic processes, providing real-time feedback and predicting \noutcomes (Sunstein, 2012). In a recent book with the evocative and provocative title \n``Technocracy in America\", international relations expert Parag Khanna argued that a \ndata-driven \ndirect \ntechnocracy \nis \na \nsuperior \nalternative \nto \ntoday's \n(alleged) \nrepresentative democracy, because it may dynamically capture the specific needs of the \npeople while avoiding the distortions of elected representatives and corrupt middlemen  \n(Khanna, 2017). Human decision making has often shown significant limitations and \nextreme bias in public policy, resulting in inefficient and/or unjust processes and \noutcomes (Fiske, 1998; Samuelson and Zeckhauser, 1998). The turn towards data-\ndriven algorithms can be seen as a reflection of a demand for greater objectivity, \nevidence-based decision-making, and a better understanding of our individual and \ncollective behaviors and needs. \nAt the same time, scholars and activists have pointed to a range of social, ethical and \nlegal \nissues \nassociated \nwith \nalgorithmic \ndecision-making, \nincluding \nbias \nand \ndiscrimination (Barocas and Selbst , 2016; Ramirez et al., 2016) and lack of \ntransparency and accountability (Citron and Pasquale, 2014; Pasquale, 2015; Zarsky, \n2016). For example, Barocas and Selbst (2016) showed that the use of algorithmic \ndecision making processes could result in disproportionate adverse outcomes for \ndisadvantaged groups, in ways suggestive of discrimination. Algorithmic decisions can \nreproduce and magnify patterns of discrimination, due to decision makers' prejudices or \nreflect the biases present in the society. A recent study by ProPublica of the COMPAS \nRecidivism Algorithm (an algorithm used to inform criminal sentencing decisions by \npredicting recidivism) found that the algorithm was significantly more likely to label black \ndefendants than white defendants, despite similar overall rates of prediction accuracy \nbetween the two groups (Angwin et al., 2016). Along this line, a nominee for the National \nBook Award, Cathy O'Neil's book, ``Weapons of Math Destruction\", details several case \nstudies on harms and risks to public accountability associated with big data-driven \nalgorithmic decision-making, particularly in the areas of criminal justice and education. \nIn 2014, the White House released a report titled ``Big Data: Seizing opportunities, \npreserving values''10 highlighting the discriminatory potential of Big Data, including how it \ncould undermine longstanding civil rights protections governing the use of personal \ninformation for credit, education, health, safety, employment, etc. \nFor example, data-driven algorithmic decisions about applicants for jobs, schools or \ncredit may be affected by hidden biases that tend to flag individuals from particular \ndemographic groups as unfavorable for such opportunities. Such outcomes can be self-\n                                           \n10https://obamawhitehouse.archives.gov/sites/default/files/docs/20150204_Big_Data_Seizing_Opportunities_Pr\neserving_Values_Memo.pdf \n\n \n59 \n \nreinforcing, since systematically reducing individuals' access to credit, employment and \neducation will worsen their situation, and play against them in future applications. For \nthis reason, a subsequent White House report called for ``equal opportunity by design\" \nas a guiding principle in those domains. Furthermore, the White House Office of Science \nand Technology Policy, in partnership with Microsoft Research and others, has co-hosted \nseveral public symposiums on the impacts and challenges of algorithms and Artificial \nIntelligence, specifically relating to social inequality, labor, healthcare and ethics11 \nAt the heart of the matter is the fact that technology outpaces policy in most cases; here, \ngovernance mechanisms of algorithms have not kept pace with technological \ndevelopment. Several researchers have recently argued that current control frameworks \nare not adequate for situations in which a potentially unfair or incorrect decision is made \nby a computer (Barocas and Selbst, 2016).  \nFortunately, there is increasing awareness of the detrimental effects of discriminatory \nbiases and opacity of some data-driven algorithmic decision-making systems, and of the \nneed to reduce or eliminate them. A number of research and advocacy initiatives are \nworth noting, including the Data Transparency Lab12, a ``community of technologists, \nresearchers, policymakers and industry representatives working to advance online \npersonal data transparency through research and design\", and the DARPA Explainable \nArtificial Intelligence (XAI) project13. A tutorial on the subject was held at the 2016 ACM \nKnowledge and Data Discovery conference (Hajian et al., 2016). Researchers from New \nYork University's Information Law Institute --such as Helen Nissenbaum and Solon \nBarocas-- and Microsoft Research --such as Kate Crawford and Tarleton Gillespie-- have \nheld several workshops and conferences these past few years on the ethical and legal \nchallenges related to algorithmic governance and decision-making14.  \nThis chapter is a summary of the content discussed by Lepri et al. (2017a, 2017b), where \nthe authors highlight the need for social good decision-making algorithms (i.e. algorithms \nstrongly influencing decision-making and resource optimization of public goods, such as \npublic health, safety, access to finance and fair employment) to provide transparency and \naccountability, to only use personal information --created, owned and controlled by \nindividuals-- with explicit consent, to ensure that privacy is preserved when data is \nanalyzed in aggregated and anonymized form, and to be tested and evaluated in context \nby means of living lab approaches involving citizens. \nThe opportunity to significantly improve the processes leading to decisions that affect \nmillions of lives is huge. As researchers and citizens, I believe that we should not miss on \nthis opportunity. Hence, I would like to encourage the larger community --researchers, \npractitioners, policy makers-- in a variety of fields --computer science, sociology, \neconomics, ethics, law-- to join forces so we can address today's limitations in data-\ndriven decision-making and contribute to fairer and more transparent decisions with clear \naccountability, within an ethical framework and developed by diverse teams so they can \nachieve significant positive impact. \n22.2 Challenges \nThere are several limitations and risks in the use of data-driven predictive models \ninforming decisions that might impact the daily lives of millions of people. Namely: \n21.2.1. Discrimination: Algorithmic discrimination may arise from different sources. \nFirst, input data into algorithmic decisions may be poorly weighted, leading to disparate \nimpact. For example, as a form of indirect discrimination, overemphasis of zip code \nwithin predictive policing algorithms can lead to the association of low-income African-\nAmerican neighborhoods with areas of crime and as a result, the application of specific \n                                           \n11 https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence \n12 http://www.datatransparencylab.org/ \n13 http://www.darpa.mil/program/explainable-artificial-intelligence \n14 http://www.law.nyu.edu/centers/ili/algorithmsconference \n\n \n60 \n \ntargeting based on group membership (Christin et al., 2015). Second, discrimination can \noccur from the decision to use an algorithm itself. Categorization can be considered as a \nform of direct discrimination, whereby algorithms are used for disparate treatment \n(Diakopoulos, 2015). Third, algorithms can lead to discrimination as a result of the \nmisuse of certain models in different contexts (Calders and Zliobaite, 2013). Fourth, in a \nform of feedback loop, biased training data can be used both as evidence for the use of \nalgorithms and as proof of their effectiveness (Calders and Zliobaite, 2013). The use of \nalgorithmic data-driven decision processes may also result in individuals being denied \nopportunities based not on their own action but on the actions of others with whom they \nshare some characteristics. For example, some credit card companies have lowered a \ncustomer's credit limit, not based on the customer's payment history, but rather based \non analysis of other customers with a poor repayment history that had shopped at the \nsame establishments where the customer had shopped (Ramirez et al., 2016). \nWhile several proposals have been made in the literature to tackle algorithmic \ndiscrimination and maximize fairness, we feel the urgency to establish a call for action \nbringing together researchers from different fields (including law, ethics, political \nphilosophy and machine learning) to devise, evaluate and validate in the real-world \nalternative fairness metrics for different tasks. In addition to this empirical research, we \nbelieve it will be necessary to propose a modeling framework --supported by empirical \nevidence-- that would assist practitioners and policy makers in making decisions aided by \nalgorithms that are maximally fair. \n21.2.2. \nLack \nof \nTransparency/Opacity: \nTransparency, \nwhich \nrefers \nto \nthe \nunderstandability of a specific model, can be a mechanism that facilitates accountability. \nMore specifically, transparency can be considered at the level of the entire model, at the \nlevel of individual components (e.g. parameters), and at the level of a specific algorithm. \nIn the strictest sense, a model is transparent if a person can contemplate the entire \nmodel at once. Thus, models should be characterized by low computational complexity. A \nsecond and less strict notion of transparency might be that each part of the model (e.g. \neach input, parameter, and computation) admits an intuitive explanation. A final notion \nof transparency might apply at the level of the algorithm, even without the ability to \nsimulate an entire model or to intuit the meaning of its components. However, the ability \nto access and analyze behavioral data about customers and citizens on an unprecedented \nscale gives corporations and governments powerful means to reach and influence \nsegments of the population through targeted marketing campaigns and social control \nstrategies. In particular, we are witnessing an information and knowledge asymmetry \nsituation where a powerful few have access and use resources and tools that the majority \ndo not have access to, thus leading to an --or exacerbating the existing-- asymmetry of \npower between the state and big companies on one side and the people on the other \nside, conceptualized as a “digital divide\" (Boyd and Crawford, 2012). In addition, the \nnature and use of various data-driven algorithms for social good, as well as the lack of \ncomputational or data literacy among citizens (Bhargava et al., 2015), makes algorithmic \ntransparency difficult to generalize and accountability difficult to assess (Pasquale, 2015). \nBurrell (2016) has provided a useful framework to characterize three different types of \nopacity in algorithmic decision-making: (1) intentional opacity, whose objective is the \nprotection of the intellectual property of the inventors of the algorithms. This type of \nopacity could be mitigated with legislation that would force decision-makers towards the \nuse of open source systems. The new General Data Protection Regulations (GDPR) in the \nEU with a “right to an explanation\" starting in May of 2018 is an example of such \nlegislation. But powerful commercial and governmental interests will make it difficult to \neliminate intentional opacity; (2) illiterate opacity, due to the fact that the vast majority \nof people lack the technical skills to understand the underpinnings of algorithms and \nmachine learning models built from data. This kind of opacity might be attenuated with \nstronger education programs in computational thinking and “algorithmic literacy\" and by \nenabling independent experts to advise those affected by algorithmic decision-making; \nand (3) intrinsic opacity, which arises by the nature of certain machine learning methods \nthat are difficult to interpret (e.g. deep learning models). This opacity is well known in \nthe machine learning community (usually referred to as the interpretability problem).  \n\n \n61 \n \n21.2.3. Computational violations of privacy: Reports and studies (Ramirez et al., \n2016) have focused on the misuse of personal data disclosed by users and on the \naggregation of data from different sources by entities playing as data brokers with direct \nimplications in privacy. An often overlooked element is that the computational \ndevelopments coupled with the availability of novel sources of behavioral data (e.g. social \nmedia data) now allow inferences about private information that may never have been \ndisclosed. This element is essential to understand the issues raised by these algorithmic \napproaches, as has become apparent in the recent Facebook/Cambridge Analytica data \nscandal15.  \n21.2.4. Data Literacy: It is of paramount importance that we devote resources to \ncomputational and data literacy programs aimed at all citizens, from children to the \nelderly. Otherwise, it will be very difficult, if not impossible, for us collectively as a \nsociety to make informed decisions about technologies that are not fully understood \n(Bhargava et al., 2015).  \n21.2.5. Unclear accountability: As more decisions that affect the lives of thousands of \npeople are automatically made by algorithms, we need clarity on who is responsible for \nthe decisions made by them or with algorithmic support. Transparency is generally \nthought as a key enabler of accountability. However, transparency and auditing do not \nnecessarily suffice for accountability. In fact, in a recent paper Kroll et al. (2017) have \nintroduced computational methods able to provide accountability even when some \ninformation is kept hidden.  \n21.2.6. Lack of ethical frameworks: Data-driven algorithmic decision-making poses \nimportant ethical dilemmas regarding what would be an appropriate course of action to \ntake based on the inferences carried out by the algorithms or on the specific situation \nthat the algorithm is acting upon. Hence, practitioners, developers, researchers and \npolicy makers who would use data-driven algorithms to support or automatically make \ndecisions would need to ensure that such decisions are made in accordance with a pre-\ndefined and commonly accepted ethical framework. There are several examples of ethical \nprinciples proposed in the literature for this purpose1617 and institutes and research \ncenters, such as the Digital Ethics Lab in Oxford or the AI Now Institute at NYU. \nHowever, it is an open question how to properly incorporate ethical principles in data-\ndriven algorithmic decision making processes in addition to ensuring that all the \ndevelopers and professionals involved comply with a clear Code of Conduct and Ethics. \n21.2.7. Lack of diversity: Given the broad set of use cases that data-driven algorithms \nmight be apply to, it is important to reflect on the diversity of the teams that generated \nsuch algorithms. To date, the development of the state-of-the-art data-driven, machine \nlearning-based algorithms has been carried out by somewhat homogeneous groups of \ncomputer scientists. Moving forward, we need to ensure that the teams are diverse both \nin terms of areas of expertise and demographics –particularly gender.  \n \n                                           \n15 https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal \n16 https://www.wired.com/story/should-data-scientists-adhere-to-a-hippocratic-oath/ \n17 https://futureoflife.org/ai-principles/ \n\n \n62 \n \n \nFigure 6. Summary of requirements for positive data-driven disruption. \nWhile this is an exciting time for researchers and practitioners in this new field of \ncomputational social sciences, we need to be aware of the risks associated with these \nnew approaches to decision making, including violation of privacy, lack of transparency \nand \ndiversity, \ninformation \nand \nknowledge \nasymmetry, \nsocial \nexclusion \nand \ndiscrimination. I would like to highlight three human-centric requirements that we \nconsider to be of paramount importance to enable positive disruption of data-driven \npolicy-making: user-centric data ownership and management; algorithmic transparency \nand accountability; and living labs to experiment with data-driven policies in the wild. It \nwill be only when we honor these requirements that we will be able to move from the \nfeared tyranny of data and algorithms to a data-enabled model of democratic governance \nrunning against tyrants and autocrats, and for the people. \nFor the readers interested in the topic, they can find an extended version of this chapter \nin (Lepri et al., 2017a; Lepri et al., 2017b).  \nReferences  \n— Angwin, J., Larson, J., Mattu, S., Kirchner, L.: Machine bias. ProPublica (2016). URL \nhttps://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-\nsentencing \n— Barocas, S., Selbst, A.: Big data's disparate impact. California Law Review 104, \n671{732 (2016) \n— Barry-Jester, A.M., Casselman, B., Goldstein, D.: The new science of sentencing. The \nMarshall \nProject \n(2015). \nURL \nhttps://www.themarshallproject.org/2015/08/04/the-new-science-of-\nsentencing. \n— Bhargava, R., Deahl, E., Letouze, E., Noonan, A., Sangokoya, D., Shoup, N.: Beyond \ndata literacy: Reinventing community engagement and empowerment in the age of \ndata. \nData-Pop \nAlliance \nWhite \nPaper \nSeries \n(2015). \nURL \nhttp://datapopalliance.org/wp-content/uploads/2015/11/Beyond-Data-\nLiteracy-2015.pdf \n— Boyd, d., Crawford, K.: Critical questions for big data: Provocations for a cultural, \ntechnological, and scholarly phenomenon. Information, Communication, & Society \n15(5), 662{679 (2012) \n\n \n63 \n \n— Burrell, J. (2016). How the machine `thinks': Understanding opacity in machine \nlearning algorithms. Big Data & Society 3(1)  \n— Calders, T., Verwer, S.. (2010). Three naive bayes approaches for discrimination-free \nclassification. Data Mining and Knowledge Discovery 21(2), 277:292.  \n— Calders, T., Zliobaite, I.: Why unbiased computational processes can lead to \ndiscriminative decision procedures. (2013). In: B. Custers, T. Calders, B. Schermer, \nT. Zarsky (eds.) Discrimination and Privacy in the Information Society, pp. 43:57.  \n— Caruana, R., Kangarloo, H., David, J., Dionisio, N., Sinha, U., Johnson, D. (1999). \nCase-based explanation of non-case-based learning methods. In: Proceedings of the \n1999 American Medical Informatics Association (AMIA) Symposium, pp. 212:215.  \n— Chouldechova, S.: Fair prediction with disparate impact: A study of bias in recidivism \nprediction instruments. arXiv preprint arXiv:1610.07524 (2016) \n— Christin, A., Rosenblatt, A., boyd, d.: Courts and predictive algorithms. Data & Civil \nRights Primer (2015) \n— Citron, D., Pasquale, F.: The scored society. Washington Law Review 89(1), 1{33 \n(2014) \n— Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., Huq, A.: Fair algorithms and the \nequal treatment principle. Working Paper (2017) \n— Crawford, K., Schultz, J.: Big data and due process: Toward a framework to redress \npredictive privacy harms. Boston College Law Review 55(1), 93{128 (2014) \n— Datta, A., Tschantz, M.C.: Automated experiments on ad privacy settings. In: \nProceedings on Privacy Enhancing Technologies, pp. 92{112 (2015) \n— Diakopoulos, \nN.: \nAlgorithmic \naccountability: \nJournalistic \ninvestigation \nof \ncomputational power structures. Digital Journalism (2015) \n— Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R.: Fairness throug awareness. \nIn: Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, \npp. 214-226. ACM (2012) \n— Dworkin, R.: Sovereign virtue: The theory and the practice of equality. Harvard \nUniversity Press (2000) \n— Feldman, M., Friedler, S., Moeller, J., Scheidegger, C., Venkatasubramanian, S.: \nCertifying and removing disparate impact. In: Proceedings of the 21th ACM SIGKDD \nInternational Conference on Knowledge Discovery and Data Mining, pp. 259{268 \n(2015) \n— Fiske, Susan. Stereotyping, prejudice, and discrimination. The handbook of social \npsychology. (1998) \n— Friedler, S.A., Scheidegger, C., Venkatasubramanian, S.: On the (im)possibility of \nfairness. arXiv preprint arXiv:1609.07236 (2016) \n— Gillespie, T.: The relevance of algorithms. In: T. Gillespie, P. Boczkowski, K. Foot \n(eds.) Media technologies: Essays on communication, materiality, and society, pp. \n167-193. MIT Press (2014) \n— Hajian, S., Bonchi, F., Castillo, C.: Algorithmic bias: From discrimination discovery to \nfairness-aware data mining. In: Proceedings of the 22nd ACM SIGKDD International \nConference on Knowledge Discovery and Data Mining, pp. 2125{2126. ACM (2016) \n— Hardt, M., Megiddo, N., Papadimitriou, C., Wootters, M.: Strategic classification. In: \nProceedings of the 2016 ACM Conference on Innovations in Theoretical Computer \nScience, pp. 111{122. ACM (2016) \n\n \n64 \n \n— Hardt, M., Price, E., Srebro, N.: Equality of opportunity in supervised learning. In: \nProceedings of the International on Advances in Neural Information Processing \nSystems (NIPS), pp. 3315{3323 (2016) \n— Joseph, M., Kearns, M., Morgenstern, J., Neel, S., Roth, A.: Rawlsian fairness for \nmachine learning. arXiv preprint arXiv:1610.09559 (2016) \n— Kamiran, F., Calders, T., Pechenizkiy, M.: Discrimination aware decision tree learning. \nIn: Proceedings of 2010 IEEE International Conference on Data Mining, pp. 869-874. \nIEEE (2010) \n— Kamishima, T., Akaho, S., Asoh, H., Sakuma, J.: Fairness-aware classifier with \nprejudice remover regularizer. In: Proceedings of the European Conference on \nMachine Learning and Principles of Knowledge Discovery in Databases (ECMLPKDD), \nPart II, pp. 35-50 (2011) \n— Khandani, A.E., Kim, A.J., Lo, A.W.: Consumer credit risk models via machine-\nlearning algorithms. Journal of Banking and Finance 34, 2767{2787 (2010) \n— Khanna, P.: Technocracy in America: Rise of the info-state. CreateSpace Independent \nPublishing Platform (2017) \n— Kleinberg, J., Mullainathan, S., Raghavan, M.: Inherent trade-offs in the fair \ndetermination of risk scores. In: Proceedings of the 8th Innovations in Theoretical \nComputer Science Conference. ACM (2017) \n— Kroll, J.A., Huey, J., Barocas, S., Felten, E.W., Reidenberg, J.R., Robinson, D.G., Yu, \nH.: Accountable algorithms. University of Pennsylvania Law Review 165 (2017) \n— Lepri, B., Staiano, J., Sangokoya, D., Letouze, E., Oliver, N.: The tyranny of data? \nThe bright and dark sides of data-driven decision-making for social good. In: \nCerquitelli T., Quercia D., Pasquale F. (eds) Transparent Data Mining for Big and \nSmall Data. Studies in Big Data, vol 32. Springer, 2017 \n— Lepri, B., Oliver, N., Letouzé, E. et al. “Fair, transparent and accountable algorithmic \ndecision-making processes. The premise, proposed solutions and open challenges”, \nPhilos. Technol. (2017). https://doi.org/10.1007/s13347-017-0279-x \n— Lipton, Z.C.: The mythos of model interpretability. In: 2016 ICMLWorkshop on \nHuman Interpretability in Machine Learning (2016) \n— Macnish, K.: Unblinking eyes: The ethics of automating surveillance. Ethics and \nInformation Technology 14(2), 151{167 (2012)  \n— O'Neil, C.: Weapons of math destruction: How big data increases inequality and \nthreatens democracy. Crown (2016) \n— Pager, D., Shepherd, H.: The sociology of discrimination: Racial discrimination in \nemployment, housing, credit and consumer market. Annual Review of Sociology 34, \n181-209 (2008) \n— Pasquale, F.: The Black Blox Society: The secret algorithms that control money and \ninformation. Harvard University Press (2015) \n— Pedreschi, D., Ruggieri, S., Turini, F.: Discrimination-aware data mining. In: \nProceedings of the 14th ACM SIGKDD International Conference on Knowledge \nDiscovery and Data Mining, pp. 560{568 (2008) \n— Ramirez, E., Brill, J., Ohlhausen, M., McSweeny, T.: Big data: A tool for inclusion or \nexclusion? Tech. rep., Federal Trade Commission (2016) \n— Ribeiro, M., Singh, S., Guestrin, C.: “why should I trust you?\": Explaining the \npredictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International \nConference on Knowledge Discovery and Data Mining, pp. 1135{1144 (2016) \n\n \n65 \n \n— Samuelson, W., Zeckhauser, R.: Status quo bias in decision making. Journal of Risk \nand Uncertainty (1), 7{59 (1988) \n— San Pedro, J., Proserpio, D., Oliver, N.: Mobiscore: Towards universal credit scoring \nfrom mobile phone data. In: Proceedings of the International Conference on User \nModeling, Adaptation and Personalization (UMAP), pp. 195{207 (2015) \n— Sandvig, C., Hamilton, K., Karahalios, K., Langbort, C.: Auditing algorithms: Research \nmethods for detecting discrimination on internet platforms. In: Data and \nDiscrimination: Converting Critical Concerns into Productive Inquiry, a preconference \nat the 64th Annual Meeting of the International Communication Association (2014) \n— Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: \nVisualising \nimage \nclassi_cation \nmodels \nand \nsaliency \nmaps. \narXiv \npreprint \narXiv:1312.6034 (2013) \n— Sunstein, C.: Regulation in an uncertain world. National Academy of Sciences (2012). \nURL \nhttps://www.whitehouse.gov/sites/default/_les/omb/inforeg/speeches/regul\nation-in-an-uncertain-world-06202012.pdf \n— Tobler, C.: Limits and potential of the concept of indirect discrimination. Tech. rep., \nEuropean Network of Legal Experts in Anti-Discrimination (2008) \n— Tverksy, A., Kahnemann, D.: Judgment under uncertainty: Heuristics and biases. \nScience 185(4157), 1124{1131 (1974) \n— Wang, T., Rudin, C., Wagner, D., Sevieri, R.: Learning to detect patterns of crime. In: \nMachine Learning and Knowledge Discovery in Databases, pp. 515{530. Springer \n(2013) \n— Zafar, M.B., Martinez, I.V., Rodriguez, M.D., Gummadi, K.P.: Learning fair classifiers. \narXiv preprint arXiv:1507.05259 (2015) \n— Zarsky, T.: The trouble with algorithmic decisions: An analytic road map to examine \nefficiency and fairness in automated and opaque decision making. Science, \nTechnology and Human Values 41(1), 118{132 (2016) \n— Zemel, R., Wu, Y., Swersky, K., Pitassi, T., Dwork, C.: Learning fair representation. \nIn: Proceedings of the 2013 International Conference on Machine Learning (ICML), \npp. 325-333 (2012) \n\n \n66 \n \n23 Quantum Computing and Machine Learning \nAntonio Puertas Gallardo \nJoint Research Centre, European Commission  \nWide sectors of the industry and world economy are demanding more computing power \nand those needs are actually of a new king of computation. A growing request for High \nPerformance Computing (or supercomputing) power exist amidst many areas (Finance, \nChemistry, Pharma-industry, Nuclear fusion research), Big data and Artificial Intelligence, \nin general. The more digitalization of the economy increases, the higher is the request for \na bigger and different type of supercomputing. Increasingly more systems and devices \nare clustering together, collecting data, in what is called the dawn of the Internet of \nthings (IoT). Artificial intelligence processors are discovering mind-blowing levels of \ncorrelations or formulating inferences in huge amount of data, but still there are plenty of \nsignals that a considerable numbers of companies are looking for new supercomputing \nparadigms. Classical computers (and supercomputer) are very big calculators that \nperformed very well doing calculus and analytics using step-by-step operations, however \nquantum computing will be focused on the solution of problems from a more complex \nand higher point of view.  \n \nFigure 7. Areas of interest for Quantum Computing18 \nThe capacity of data stored worldwide is increasing by 20 % on a yearly basis (nowadays \nis ranging in the order of hundredths of Exabytes) and is a compelling force to discover \nnew approaches to Artificial Intelligence (Machine Learning).  An encouraging new \nconcept in computation is been now investigated by the most prominent IT companies \nresearch laboratories and Academic world, is the forthcoming and hypothetical utilization \nof quantum computing for the optimization of the algorithms of classic machine learning. \nQuantum computing will not render the classic computers inappropriate. Personal \ncomputers, notebooks and smartphones will still be running on silicon-processors for the \nlikely future and the changeover may possibly take several years. Quantum computing \nmight be the boosting element of the new \"Fourth Industrial Revolution\", likewise it \nmight be, for example, a driver for the development of new molecules for drugs, the \ndiscovering of new materials and boost Machine learning algorithms that could not have \nbeen developed before with traditional computers. \n \n                                           \n18 Source IBM \n\n \n67 \n \n23.1 What is quantum computing?   \nA classical computer encodes information in the elementary unit of a logical bit, which \ncan take values either \"0\" or \"1\", this information is stored and processed in the way of \nstrings of bits (binary bits). Those individuals bit can have only one of two values: either \n0 or 1. A quantum computer encodes information in the so called quantum bits or \n\"qubits\" each of them can simultaneously encode both logical bits \"0\" and \"1\" at once. \nThis behaviour makes a quantum computer intrinsically parallel. The way to storage and \nprocess information in parallel, make some mathematical operations exponentially wide \nfaster related to the computational speeds of classical computers for solving the same \nkind of problems.  \n \n \n \n \n \n \n \n \n  \n     \nQuantum computation exploits a quantum physics phenomenon called \"superposition\" \nwhich allows to a qubit (or quantum system in general) to be in a superposition of more \nthan one state (not only \"0\" or \"1\" as conventional computers) at the same time. The \ndifferences between classical and quantum computers can be explained with the help of a \ncoin.  In classical computing, information is stored in bits with two states, either 0 or 1 – \n(or heads or tails). In quantum computing, information is stored in quantum bits \n(\"qubits\") that can be any state between 0 and 1 – similar to a spinning coin that can be \nboth heads and tails at the same time. Among other advantages, a quantum computer \nmakes computations by the manipulation of subatomic particles. These operations are \nfaster and with lower energy consumption if compared with the classical computers.     \nNowadays, the methods and instruments of quantum algorithms are very well founded \nand encompass a high amount of remarkable models and standards that overcome and \nbeat the best established classical methods (See figure 4). The achievement of quantum \ncomputing is arising with IBM and Righetti succeeding in making their quantum \ncomputers available on the cloud. Many people are convinced that is only a matter of \ntime until several theoretical designs can be tested on real-life machines. The innovative \nresearch discipline of Quantum machine Learning might offer the possibility to disrupt \nfuture approaches of intelligent data processing. \n \nFigure 8. Computing science domains19 \n23.2 What is the holy grail of quantum computing?    \nExponential acceleration \nIn other words, a quantum computer would be able to compute at a much faster speed \n(exponentially faster) than a classical computer. This implies that classical algorithms, \n                                           \n19 Quantum computing – weird science or the next computing revolution Morgan Stanley Research Report, August 2017 \n\n \n68 \n \nwhich would take years to solve on a current supercomputer, could take just hours or \nminutes on a quantum computer.  \n23.3 Quantum Machine Learning \nQuantum computation and quantum information have enabled us to think physically \nabout computation, and this approach has yielded many new and exciting capabilities for \ninformation processing.  Hence, it is possible to enable us to think physically (from a \nquantum physics point of view) about machine learning, especially about neural \nnetworks. The field of quantum machine learning explores how to devise and implement \nquantum software that could allow machine learning to perform faster than on classical \ncomputers. Quantum machine learning \"QML\" is the science and technology at the \nintersection of quantum information processing and machine learning.   \n \n                        \nTo figure out the scientific research and work on quantum machine learning we need to \nconsider it as a highway. On one side (one way), machine learning assists physicists to \ncontrol and manipulate quantum effects and phenomena in labs. On the other side, \nquantum physics improves the implementation and performance of machine learning. In \nquantum machine learning, quantum algorithms are developed to solve typical problems \nof machine learning using the efficiency of quantum computing. This is usually done by \nadapting classical algorithms or their expensive subroutines to run on a potential \nquantum computer. The expectation is that in the near future, such machines will be \ncommonly available for applications and can help to process the growing amounts of \nglobal information.  The emerging field also includes approaches vice versa, namely well-\nestablished methods of machine learning that can help to extend and improve quantum \ninformation theory. \nQuantum learning algorithms have been realized in a host of experimental systems \nand cover a range of applications as: \n \nSimultaneous spoken digit and speaker recognition and chaotic time-series \nprediction at data rates beyond a gigabyte per second (Brunner at al., 2013). \n \nNeural networks have been realized using liquid state nuclear magnetic resonance \n(Neigovzen et al., 2009). \n \nDefaulting on a chain of trapped ions, simulated a neural network with induced \nlong range interactions (Pons et al., 2007). \n \nSolving a Higgs optimization problem with quantum annealing for machine \nlearning (Mott et al., 2017). \n \nQuantum annealing versus classical machine learning applied to a simplified \ncomputational biology problem (Richard Y. Li, Rosa Di Felice et al, 2018) \n23.4 Challenges \nFor the time being Quantum computing is still in transition between the Labs and the \ntesting phase. This period is for the world of scientists and industry to focus on getting \nquantum-ready and to create a quantum-literate community who speaks quantum \ninformation language20 \n Artificial neural networks and machine learning have now reached a new era after \nseveral decades of improvement where applications are to explode in many fields of \nscience, industry, and technology. The Emergent Quantum information technologies \nwould eventually boost the impact on Artificial Intelligence. \n1. - Machine learning algorithms training times could be accelerated exponentially21. \n2. - Parallelization of codes would be the new normal. \n                                           \n20 https://www.symmetrymagazine.org/article/learning-to-speak-quantum \n21 https://www.youtube.com/watch?v=Q4xBlSi_fOs \n \n\n \n69 \n \n3. - Software development would be revolutionized as programmers should need to learn \nto make codes which manage all solutions at the same time (instantaneously, when the \nalgorithms are deployed into the Hardware layer). \nReferences  \n— Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, \nSeth \nLloyd. \n(2017). \nQuantum \nMachine \nLearning. \nNature \n549, \n195-202.  \nhttps://arxiv.org/pdf/1611.09347v1.pdf  \n— Daniel Brunner, Miguel C. Soriano, Claudio R. Mirasso, and Ingo Fischer. Parallel \nphotonic information processing at gigabyte per second data rates using transient \nstates. Nat. Commun., 4:1364, January 2013. \n— Dong-Ling Deng, Xiaopeng Li and S. Das Sarma. (2017). Machine learning topological \nstates. Phys. Rev. B 96, 195145. \n— Lov K. Grover. (1996). A fast quantum mechanical algorithm for database search. \nQuantum Physics. https://arxiv.org/abs/quant-ph/9605043  \n— K. Mills, M. Spanner, I. Tamblyn. Deep learning and the Schrödinger equation. Phys. \nRev. A 96, 042113. https://arxiv.org/abs/1702.01361  \n— Marisa Pons, Veronica Ahufinger, Christof Wunderlich, Anna Sanpera, Sibylle \nBraungardt, Aditi Sen(De), Ujjwal Sen, and Maciej Lewenstein. Trapped ion chain as a \nneural network: Error resistant quantum computation. Phys. Rev. Lett., 98:023003, \nJanuary 2007. \n— Peter W. Shor. (1997). Polynomial-Time Algorithms for Prime Factorization and \nDiscrete Logarithms on a Quantum Computer SIAM J. Comput., 26(5), 1484–1509.  \nhttp://epubs.siam.org/doi/10.1137/S0097539795293172  \n \n \n\n \n70 \n \n24 Conclusions and future work \nEmilia Gómez, Vicky Charisi, Bertin Martens, Marius Miron, Songül Tolan \nJoint Research Centre, European Commission \nThis report has summarized the content of the 1st workshop on Human Behaviour and \nMachine Intelligence (HUMAINT), which provides an interdisciplinary view on the main \nchallenges related to the study of the impact that machine intelligence will have on \nhuman behaviour and potential needs for policy intervention.  \nDuring the workshop, we have identified several research challenges and directions that \ncan be summarized in the following ten points: \n1. There are many \nfundamental differences between human and machine \nintelligence: consciousness, evolutionary history, embodiment, situated cognition and \nsocial intelligence. In fact, we often lack of a critical approach of intelligence: what it \nis, its role in characterising humanness, the articulation between intelligence, \nknowledge and power. For instance, intelligence is much more easily granted to \nmachines than to humans in the current media landscape. Although there are major \nscientific advancements on the human brain and its computational modelling, this is \nan extremely complex endeavour and still far from current computational models \nused in AI application.  \n2. There is not yet a full understanding of the inner workings of state-of-the-art \ndeep neural networks. As a consequence, estimation errors might be unintuitive \nfor humans and generalization capabilities cannot be assessed. This limits the \nscientific understanding of algorithms, the capability to recover from adversarial \nexamples, and complicates human supervision in practical applications. It also raises \nserious questions regarding whether the goal of building humanlike intelligence is \npossible and desirable. We should monitor AI advancements and new computing \nparadigms (e.g. quantum computing).  \n3. We need ways to evaluate what AI can do today and predict its potential future \ncapabilities. We need to define evaluation frameworks that are meaningful and in \nnaturalistic settings to match practical application contexts. In this respect, we should \nconsider engineering best practices, impact assessment methods, user satisfaction \nand business metrics, in order to develop smart and transparent benchmarking \nstrategies. We should train the next generation of machine learning developers to \napply and communicate these strategies and follow best practices to AI evaluation. \n4. We need to advance on the explainability, accountability and transparency of \nalgorithms in general and deep learning architectures in particular, both from a \nmachine learning research perspective (including theoretical understanding and \nempirical evaluation) and from a user perspective, when these methods are exploited \nin a particular application context. Humans should develop a critical thinking with \nrespect to machine intelligence, and in order to do that people need to achieve data \nand algorithm literacy, so that everyone can understand and challenge it. \n5. With respect to human vs machine intelligence, we should move from a competition \nto an interaction paradigm where we should research on best strategies for \ncollaboration and synergies exploitation between both intelligences. For \ninstance, we need to investigate on how biases can be identified in human behaviour \nand if algorithms could help to recover and correct this. Here we need to consider \nhuman(owner/developer)-machine-human(user) \ninteractions, \nsince \nmachine \nintelligence is in fact a product of human intelligence.  \n \n6. It is important to understand the interaction in the context of decision making, e.g. \nconsidering bias present in algorithms and humans and how machines can be used to \novercome human bias rather than incorporate it. This is particularly relevant in \n\n \n71 \n \ndomains where decision making affects human welfare, e.g. in recruitment processes \nor the allocation of public funds. Also, we need to address how machines can affect \nhuman attention and strategies for humans to trust machines.  \n7. We need to research on how the interaction with machines affects human \nintelligence and cognitive capacities, if changing or diminishing them. In addition, \nwe should research on how artificial intelligence changes relations between humans \nand between humans and the environment. While recent literature is focusing on the \ninteraction between AI systems and adults, there are important differences in the way \nchildren deal with artificial systems that should be further researched, being the \nnext generation to come.  \n8. Machine intelligence systems should be developed by humans in a responsible way. \nWe should formalize and incorporate ethical principles in machine intelligence \ndevelopment and evaluation. We should also foster diversity (in terms of expertise \nand demographics, particularly gender), in teams that develop and are empowered \nwith artificial intelligence to reflect varied perspectives into the developed systems. \n9. Machine intelligence has a wide range of potential economic implications. There are \nalready major concerns about the impact on human employment, wages and income \ndistribution. The growing information asymmetry between humans and intelligent \nmachines, and the potential for moral hazard and exploitation of human cognitive \nbiases, will affect human behaviour and welfare. Competition between machines with \nscalable information processing capacities and humans with limited capacities will \ninduce systemic shifts, including in the institutional structures of human societies.  \n10. There is a need to understand who controls the results of AI, what is done in AI, \nand for whom, and establish adequate forecasting, control mechanisms and legal \nprovisions to anticipate and revert situations in which algorithms can be used \nagainst people's welfare, as well as establish adequate laws that ensure algorithms \nare used *for* people's welfare. \nFinally, these mentioned points can be applied in different application domains. In this \nrespect, we concluded that there are some aspects of algorithms that can be analysed \nindependently of the application context. For instance, we agreed that the potential \nconstruction of isolation bubbles on the side of users in music recommender systems \nshould be carefully counter-acted in order to keep the collective and transformative \npower of music at its best. This is also shared in other domains.  \nHowever, there are some other issues that should be considered for particular use \ncases. In terms of algorithmic transparency, for instance, the interpretability of \nalgorithms is crucial in healthcare applications, while it can be less critical in music \nrecommendation.   \nAll of this requires multidisciplinary thinking, diverse teams and future impact \nassessment, as we should be watchful for forthcoming disruption on the way we see \nourselves and the surrounding world.  \n \n\n \n72 \n \n25 List of workshop participants and report contributors  \n \nDr. Alessandro Annoni, Head of Digital Economy Unit, Leader of Artificial Intelligence \nProject.  \n \nDr. Benito Arruñada, Economy and Business Department, Universitat Pompeu Fabra, \nBarcelona \n \nDr. Xerxes D. Arsiwalla, Institute for Bioengineering of Catalonia.  \n \nDr. Carlos Castillo, Social Computing and Web Mining. \n \nDr. Vicky Charisi, Centre for Advanced Studies, Joint Research Centre, European \nCommission. \n \nEmanuele Cuccillato, Behavioural Insights and Design for Policy Unit, Joint Research \nCentre, European Commission. \n \nProf. Veronica Dahl, Professor of Computing Science, Simon Fraser University, \nCanada. \n \nProf. Dr. Gustavo Deco, Computational neuroscience, Director of the Centre for Brain \nand Cognition, Universitat Pompeu Fabra.  \n \nDr. Blagoj Delipetrev, Digital Economy Unit, Joint Research Centre, European \nCommission. \n \nDr. Paul Desruelle, Digital Economy Unit, Joint Research Centre, European \nCommission.  \n \nNicole Dewandre, Joint Research Centre, European Commission.  \n \nDr. Fabien Giraldin (PhD), BBVA Data & Analytics.  \n \nDr. Emilia Gómez, Joint Research Centre, European Commission and Universitat \nPompeu Fabra.  \n \nProf. Dr. Miguel Ángel González-Ballester, Simulation, Imaging and Modelling for \nBiomedical Systems, ICREA and Universitat Pompeu Fabra.  \n \nDr. Fabien Gouyon, Principal researcher, Pandora. \n \n Dr. José Hernández-Orallo, Universidad Politécnica de Valencia - Leverhulme Centre \nfor the Future of Intelligence, University of Cambridge.  \n \nPerfecto Herrera, Music Technology Group - Universitat Pompeu Fabra / Escola \nSuperior de Música de Catalunya, Barcelona, Spain.  \n \nMaria Iglesias, Legal Officer, Intellectual Property and Technology Transfer, Joint \nResearch Centre, European Commission.  \n \nDr. Lorena Jaume-Palasi, co-founder and executive director of AlgorithmWatch \n(Germany).  \n \nDr. Anders Jonsson, Artificial Intelligence and Machine Learning group, Universitat \nPompeu Fabra.  \n \nDr. Sergi Jordà, Music Technology Group, Universitat Pompeu Fabra.  \n \nDr. Ansgar Koene, University of Nottingham. \n \nDr. Martha Larson, Radboud University and TU Delft. \n \nProf. Ramón López de Mantaras, Director of the IIIA (Artificial Intelligence Research \nInstitute) of the CSIC (Spanish National Research Council), Barcelona, Spain.  \n \nDr. Bertin Martens, Senior Scientist, Digital Economy Unit.  \n \nEver Meijer, Geodan. \n\n \n73 \n \n \nDr. Marius Miron, Centre for Advanced Studies, Joint Research Centre, European \nCommission. \n \nDr. Rubén Moreno-Bote, Centre for Brain and Cognition, DTIC, Universitat Pompeu \nFabra. \n \nDr. Pablo Noriega, Artificial Intelligence Institute, Spanish Council for Scientific \nResearch (CSIC). \n \nDr. Nuria Oliver, Vodafone Research and Data-Pop Alliance. \n \nAntonio Puertas Gallardo, Knowledge for Health and Consumer Safety Unit, Joint \nResearch Centre, European Commission.  \n \nJordi Pons, Universitat Pompeu Fabra.  \n \nAurelio Ruiz, Universitat Pompeu Fabra. \n \nProf. Dr. Heike Schweitzer, Free University of Berlin, Germany. \n \nProf. Dr. Nuria Sebastian-Galles, Speech Acquisition and Perception Group, Centre for \nBrain and Cognition, Universitat Pompeu Fabra. \n \nDr. Joan Serrà, Telefònica Research, Barcelona, Spain. \n \n Prof. Dr. Xavier Serra, Music Technology Group - Maria de Maeztu Strategic Program \non Data-Driven Knowledge Extraction, Department of Information and \nCommunication Technologies, Universitat Pompeu Fabra.  \n \nProf. Dr. Luc Steels, ICREA and Universitat Pompeu Fabra. \n \nDr. Jutta Thielen del Pozo, Head of Scientific Development Unit and Director of the \nCentre for Advanced Studies, Joint Research Centre, European Commission. \n \nDr. Songul Tolan, Centre for Advanced Studies, Joint Research Centre, European \nCommission. \n \nDr. Karina Vold, Leverhulme Centre for the Future of Intelligence, University of \nCambridge. \n \nProf. Henk Scholten, University of Amsterdam, Co-Lead Scientist of Digital \nTransformation - Governance Project. \n \n\n \n74 \n \nList of figures \nFigure 1. Benefit for protected and unprotected groups. ............................................24 \nFigure 2. COMPAS scores distribution. .....................................................................25 \nFigure 3. Ranking comparison for different genres. ...................................................26 \nFigure 4. Machine Learning (ML) Pipeline. Evaluation of AI with respect to requirements \nand opportunities for regulation can be identified at every stage of the pipeline. ..........35 \nFigure 5. Uncanny Valley effect. .............................................................................57 \nFigure 6. Summary of requirements for positive data-driven disruption. ......................62 \nFigure 7. Areas of interest for Quantum Computing ..................................................66 \nFigure 8. Computing science domains .....................................................................67 \n \n \n \nGETTING IN TOUCH WITH THE EU \nIn person \nAll over the European Union there are hundreds of Europe Direct information centres. You can find the \naddress of the centre nearest you at: http://europea.eu/contact \nOn the phone or by email \nEurope Direct is a service that answers your questions about the European Union. You can contact this \nservice: \n- by freephone: 00 800 6 7 8 9 10 11 (certain operators may charge for these calls), \n- at the following standard number: +32 22999696, or \n- by electronic mail via: http://europa.eu/contact \nFINDING INFORMATION ABOUT THE EU \nOnline \nInformation about the European Union in all the official languages of the EU is available on the Europa \nwebsite at: http://europa.eu \nEU publications \nYou can download or order free and priced EU publications from EU Bookshop at: \nhttp://bookshop.europa.eu. Multiple copies of free publications may be obtained by contacting Europe \nDirect or your local information centre (see http://europa.eu/contact). \n\n \n75 \n \n \nXX-NA-xxxxx-EN-N",
    "pdf_filename": "Assessing the impact of machine intelligence on human behaviour - an interdisciplinary endeavour.pdf"
}