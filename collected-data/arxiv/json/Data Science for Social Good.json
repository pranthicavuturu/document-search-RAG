{
    "title": "Data Science for Social Good",
    "context": "Data science has been described as the fourth paradigm for scientific discovery. The latest wave of data science research, pertaining to machine learning and artificial intelligence (AI), is growing exponentially and garnering millions of annual citations. However, this growth has been accompanied by a diminishing emphasis on social good challenges – our analysis reveals that the proportion of data science research focusing on social good is less than it has ever been. At the same time, the proliferation of machine learning and generative AI have sparked debates about the socio-technical prospects and challenges associated with data science; for human flourishing, organizations, and society. Against this backdrop, we present a framework for “data science for social good” (DSSG) research that considers the interplay between relevant data science research genres, social good challenges, and different levels of socio- paucity of work on DSSG in information systems (and other related disciplines) and highlight current impediments. We then use our proposed framework to introduce the articles appearing in the special issue. We hope that this article and the special issue will spur future DSSG research and help reverse the alarming trend across data science research over the past 30-plus years in which social good challenges are garnering proportionately less attention with each passing day.  1 Data science is an interdisciplinary field that applies mathematics, statistics, machine learning, and data visualization techniques to extract insights and knowledge from data that are normally big and encompass both structured and unstructured formats. In March 2019, something extraordinary and unprecedented happened – an important milestone in the (relatively brief) history of data science. The three “godfathers” of deep learning – Geoff Hinton, Yoshua Bengio, and Yann LeCun – were awarded the prestigious 2018 Turing Award (Simonite, 2019). For those unfamiliar with the award, it is to computer science what the Nobel Prize is to disciplines such as economics and physics, or the Fields Medal to math. So why was it extraordinary? There are a couple of reasons. First, deep learning is essentially a class of machine learning methods (Samtani et al., 2023; Abbasi et al., 2016). If one were to look at prior seminal machine learning methods, none ever won the award1. Decision tree induction models (Quinlan, 1986) and their important extensions, such as random forests (Breiman, 2001), did not win despite being routinely ranked as the most used machine learning method for predictive analytics in research and practice over multiple decades (Abbasi et al., 2016). The same is true for support vector machines (SVM), which popularized the idea of learning problem/domain-specific representations and have been used extensively in prior information systems (IS) research (e.g., Abbasi et al., 2010; Chau et al., 2020). For both these methods, decision trees and SVMs, the seminal papers/authors have garnered over 200,000 citations on Google Scholar. On the other hand, as of  1 Judea Pearl won the Turing Award in 2011 for “contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning” including Bayesian Networks, which have been used as a machine learning method for classification/prediction problems. However, his AI contributions are generally regarding as being broader than machine learning, whereas deep learning is widely regarded as a subset of machine learning.",
    "body": "1 \n \nData Science for Social Good \n \n \n \nAhmed Abbasi \nHuman-centered Analytics Lab \nDepartment of IT, Analytics, & Operations \nUniversity of Notre Dame \naabbasi@nd.edu  \nRoger H. L. Chiang \nDepartment of Operations, \nBusiness Analytics, & IS \nUniversity of Cincinnati \nroger.chiang@uc.edu \nJennifer J. Xu \nDepartment of Computer \nInformation Systems \nBentley University \njxu@bentley.edu \n \n \nAbstract \nData science has been described as the fourth paradigm for scientific discovery. The latest \nwave of data science research, pertaining to machine learning and artificial intelligence (AI), \nis growing exponentially and garnering millions of annual citations. However, this growth has \nbeen accompanied by a diminishing emphasis on social good challenges – our analysis reveals \nthat the proportion of data science research focusing on social good is less than it has ever \nbeen. At the same time, the proliferation of machine learning and generative AI have sparked \ndebates about the socio-technical prospects and challenges associated with data science; for \nhuman flourishing, organizations, and society. Against this backdrop, we present a framework \nfor “data science for social good” (DSSG) research that considers the interplay between \nrelevant data science research genres, social good challenges, and different levels of socio-\ntechnical abstraction. We perform an analysis of the literature to empirically demonstrate the \npaucity of work on DSSG in information systems (and other related disciplines) and highlight \ncurrent impediments. We then use our proposed framework to introduce the articles appearing \nin the special issue. We hope that this article and the special issue will spur future DSSG \nresearch and help reverse the alarming trend across data science research over the past 30-plus \nyears in which social good challenges are garnering proportionately less attention with each \npassing day. \n \n1 \nIntroduction \nData science is an interdisciplinary field that applies mathematics, statistics, machine learning, and data visualization \ntechniques to extract insights and knowledge from data that are normally big and encompass both structured and \nunstructured formats. In March 2019, something extraordinary and unprecedented happened – an important \nmilestone in the (relatively brief) history of data science. The three “godfathers” of deep learning – Geoff Hinton, \nYoshua Bengio, and Yann LeCun – were awarded the prestigious 2018 Turing Award (Simonite, 2019). For those \nunfamiliar with the award, it is to computer science what the Nobel Prize is to disciplines such as economics and \nphysics, or the Fields Medal to math. So why was it extraordinary? There are a couple of reasons. First, deep \nlearning is essentially a class of machine learning methods (Samtani et al., 2023; Abbasi et al., 2016). If one were to \nlook at prior seminal machine learning methods, none ever won the award1. Decision tree induction models \n(Quinlan, 1986) and their important extensions, such as random forests (Breiman, 2001), did not win despite being \nroutinely ranked as the most used machine learning method for predictive analytics in research and practice over \nmultiple decades (Abbasi et al., 2016). The same is true for support vector machines (SVM), which popularized the \nidea of learning problem/domain-specific representations and have been used extensively in prior information \nsystems (IS) research (e.g., Abbasi et al., 2010; Chau et al., 2020). For both these methods, decision trees and \nSVMs, the seminal papers/authors have garnered over 200,000 citations on Google Scholar. On the other hand, as of \n \n1 Judea Pearl won the Turing Award in 2011 for “contributions to artificial intelligence through the development of \na calculus for probabilistic and causal reasoning” including Bayesian Networks, which have been used as a machine \nlearning method for classification/prediction problems. However, his AI contributions are generally regarding as \nbeing broader than machine learning, whereas deep learning is widely regarded as a subset of machine learning.    \n\n2 \n \nthe publication of this article, the deep learning Turing Award winners had amassed an astounding 1.5 million \ncitations, underscoring the impact their work has had on research. Second, the depth of academic-industry \nengagement among the winners was unprecedented. At the time of the award, in addition to their academic positions \nat top universities in North America, all three had strong ties to major Silicon Valley tech companies2.  \nThe event signaled the culmination of a 30-year period in which the practice of data science, as well as academic \nresearch, has progressed as follows: data management, business intelligence (BI), statistical data mining and \npredictive analytics, and most recently, machine learning and artificial intelligence (AI) (Wixom & Watson, 2001; \nChen et al., 2012; Agarwal & Dhar, 2014; Abbasi et al., 2016; Grover et al., 2018; Berente et al., 2021). Figure 1 \nillustrates this trend over the period 1990-2020 for three waves of data science research: data management & BI, \ndata mining & analytics, and machine learning & AI. The y-axis depicts the number of new results (i.e., scholarly \narticles added) in the Google Scholar index for that given year across various data science topic keywords associated \nwith the three waves. The figure shows the rise of data management and basic BI in the 1990s and the ascent of \nanalytics in the 2000s. Being foundational to data science, both waves undoubtedly remain important and \nconsequential today. Longitudinally, both follow the concave pattern commonly observed for many technology \ntrends and research topics. In contrast, the figure also shows the exponential growth of machine learning and AI \narticles, generating an astounding 5 million results annually (fueling the aforementioned citation counts for \nfoundational deep learning research). \n \nFigure 1: Google Scholar Index Trends for Three Waves of Data Science Research \nIn recent years, the proliferation of machine learning and generative AI have sparked debates about the prospects \nand challenges, most notably with the three deep learning Turing Award winners themselves split on the risks versus \nrewards3. On the one hand, data science in the age of advanced machine learning and deep learning presents \ntremendous opportunities for organizations and society. Historically, we have seen significant economic disruptions \ndue to technology dating back to the agricultural and industrial revolutions. In the long run, these disruptions have \nimproved the human condition. Opportunities abound for enhancing well-being and alleviating disparities. On the \nother hand, the risks and pitfalls are also very real. Given the socio-technical nature of modern data science research \nand applications, the central role of technology artifacts and technology firms in the data science revolution and \nevolution, and the broader need for thought leadership in an increasingly AI-enabled world, it is important to \nunderstand research opportunities for the field of information systems. \n \n2 At the time of the award, Hinton was a VP and Engineering Fellow at Google, LeCun was VP & Chief AI Scientist \nat Facebook, and Bengio was co-founder of Element AI, which was significantly backed by Microsoft’s venture \narm. Though not an award winner, at the time, Ian Goodfellow (Bengio’s doctoral student) was Director of Machine \nLearning at Apple.     \n3 In 2023, Geoff Hinton resigned from his position at Google to warn about the dangers of the technology. Bengio \nand LeCun were on opposing teams during a 2023 Munk Debate on Artificial Intelligence.  \n\n3 \n \nJim Gray, a 1998 Turing Award winner, promoted data science as a new, fourth paradigm for scientific discovery in \nresponse to the large amounts of data generated by scientific experiments in many disciplines (Hey et al., 2009). In \nthis vein, data science complements experimental, theoretical, and simulation science as an emerging research \nparadigm for understanding nature and society through “data-intensive computing” (Bell et al., 2009; Hey et al., \n2009; p. xi). The inherently interdisciplinary nature of data science, and the fact that it is a catalyst for business \ntransformation and technology disruption, presents many research opportunities for a diverse discipline such as IS. \nThis has spurred a call for greater IS research on data science (Agarwal & Dhar, 2014; Saar-Tsechansky, 2015). \nSimilarly, there is a need for research on the development and evaluation of data science artifacts (e.g., models, \nmethods, and systems) that address broader societal challenges. A lingering question remains: what societal \nchallenges can IS-oriented data science research contribute towards – and how can we conduct such research to \nmaximize impact and relevance? \nThe purpose of the special issue was to explore the intersection of “Data Science for Social Good” (DSSG) from the \nperspective of IS. We limit our discussion to data science research as opposed to the practice of data science in \nindustry, which has already received ample attention (e.g., Davenport & Patil, 2012). Accordingly, the goal of this \narticle is three-fold. First, we present an IS-oriented framework for DSSG research that considers the interplay \nbetween relevant IS data science research genres, social good challenges (i.e., problems/outcomes), and different \ntypes of socio-technical abstractions. Second, we perform an analysis of the literature to empirically demonstrate the \npaucity of work on DSSG in IS, and other related disciplines, and highlight current impediments. Third, we use our \nproposed framework to introduce the articles appearing in the special issue. \n \n2 \nData Science and Social Good – A Motivating Example \nBefore delving into our proposed framework for DSSG, to further underscore the need for research at the \nintersection of data science and social good, it is only fitting that we use data science techniques to help understand \nthe current state of data science research. Accordingly, we analyzed all research appearing in the Google Scholar \nindex for the thirty-three-year period spanning 1990-2022. In our analysis, we used two sets of keywords: one \nrelated to data science topics (e.g., data mining, data visualization, data science, machine learning, deep learning) \nand another pertaining to social good challenges such as poverty, hunger, inequalities, clean water, sanitation, peace, \njustice, sustainable communities, and affordable clean energy (Cowls et al., 2021). For all combinations of keywords \nappearing within and across the two sets (e.g., “deep learning” and “poverty”), we gathered the number of articles \nappearing in Google Scholar annually for our analysis period. We then visualized the publication keyword co-\noccurrences using network analysis over three time periods: 1990-2000, 1990-2010, and 1990-2022. \nBefore conducting the analysis, we were certain that the longitudinal trend would be one where social good topics \nplay a more prominent role in data science research over time. We thought that our key takeaways would relate to \nthe slow rate of progress, that is, the trajectory. Or about the relatively nascent emerging role of IS in an increasingly \nvibrant DSSG landscape. What we observed – much to our chagrin – was a completely different panoramic picture. \nThe results appear in Figure 2. In each time period panel, blue nodes signify data science-related keywords, and red \nnodes connote social good challenges. Consistent with co-occurrence network analysis, edge tie strengths between \nany two nodes were quantified as the percentage of articles containing both keywords, relative to the total number of \narticles appearing across the two keywords. A spring layout algorithm was used to arrange nodes based on their \nrespective tie strengths such that keywords were arranged based on their percentage overlap with other keywords. \nPanel (a) shows that data science and social good topics are closely intertwined for the 1990-2000 period, as \nevidenced by the spatial mixture of blue and red nodes. For instance, data mining is closely connected to health and \nwellness, clean water, and affordable energy. Data visualization has strong ties with sustainable communities. \nAnalytics is discussed in close relation with well-being. Data science lies near poverty. Meta-learning is closely \nconnected with hunger. To those who have been in the data science space for over 20 years, this connection between \ndata science and societally impactful application contexts is not overly surprising. \n \n \n\n4 \n \n \n \n \nFigure 2: Co-occurrence Networks for Data Science Keywords (red nodes) and Social Good Topics (blue nodes) \nAcross Three Time Periods Between 1990-2022 \n(a) 1990-2000 \n(b) 1990-2010 \n(c) 1990-2022 \n\n5 \n \nLooking at the co-occurrence network in the middle panel (b) in Figure 2, we see that the data science and social \ngood topics are less intertwined for the 1990-2010 period. Although affordable clean energy maintains strong ties \nwith keywords such as data visualization and big data, the two sets of keywords have drifted further apart. In the \nbottom panel (c) in Figure 2, we extend our analysis to include the entire body of research spanning 1990-2022. We \nobserve that the two different node colors are further coalescing within themselves. Topics such as sanitation are \nnow more closely discussed with clean water and poverty, suggesting that social good research is making \nconnections amongst interrelated antecedent and/or consequent concepts. Regarding linkages across data science \nand social good keywords, analytics and deep learning maintain ties with hunger, well-being, and justice. However, \nthe distances within keyword sets are reducing, whereas the distances across sets are increasing. This means the two \nbodies of research are clustering within, resulting in relatively less overlap. On the surface, this trend is somewhat \nsurprising and counter-intuitive. Social good themes are undoubtedly garnering greater attention. However, our \nanalysis does not focus on the amount of research in absolute terms – those numbers are increasing for data science \nand social good. Instead, we are focusing on the percentage overlap as a proportion of all research undertaken in the \ntwo areas. We can infer that the proportion of data science research geared towards social good themes has \ndecreased over time4. \nOne could argue that the absolute quantity of research is more important than proportionality. Furthermore, our \nanalysis was intentionally simple in that we did not weight articles based on citation counts. Nonetheless, proportion \nor share-of-volume does matter. It is routinely used to examine dominant topics, diversity of research themes, and \ntemporal dynamics and trends (Mustak et al., 2021). The current trend sends a strong signal regarding where the \nlion’s share of time and effort pertaining to data science research is heading in relation to social good topics. Modern \ndata science research is dominated by deep learning studies with goals such as the creation of \"foundation models\" \nthat codify a vast array of knowledge, for instance, related to computer vision or language, with aspirations of \nartificial general intelligence (Bubeck et al. 2023). We cannot help but wonder whether data science has become so \nengrossed with generalizable, foundational methods research, guided by the common task framework (Liberman, \n2010; Donoho 2017), that it has lost sight of the noble intents and purposes that made the promise of data science as \na mechanism for good exciting in the first place. \n \n3 \nA “Data Science for Social Good” Framework \n \n3.1 \nDefining DSSG for IS \nData Science embodies a wide array of methods, including machine learning and statistics, applied to large \nquantities of structured and unstructured data. It has been defined in different ways, both in regards to its role in \npractice, and as a mechanism for conducting scientific research. For instance, Provost and Fawcett (2013, p.52) \ndefined it as “…a set of fundamental principles that support and guide the principled extraction of information and \nknowledge from data” and “as the connective tissue between data-processing technologies…and data-driven \ndecision making.” Dhar (2013, p. 64) states, “Data science might therefore imply a focus involving data and, by \nextension, statistics, or the systematic study of the organization, properties, and analysis of data and its role in \ninference, including our confidence in the inference.” Two things are apparent from these definitions: (1) data \nscience research occurs within organizational, institutional, and/or societal environments; (2) the inferences and \noutcomes are closely tied to the underlying problem contexts. Notably, one challenge when talking about data \nscience in general – similar to big data – is that the term can be very broad and widely applicable. Figure 3 presents \na Venn diagram that captures these properties as they relate to DSSG in IS: the intersections of data science research \ngenres, socio-technical environments, and social good challenges. In the remainder of the section, we discuss these \nin detail. \n \n3.1.1 Data Science Research Genres \nComputational social science draws inferences about individuals or groups from large, longitudinal, digital data on \nhuman interactions (Lazer et al., 2009; Edelmann et al., 2020). Often-cited examples include deriving patterns and \ninsights from large social networks (Aral & Walker, 2012; Agarwal & Dhar, 2014) or mining large language corpora \n(Kozlowski et al., 2019). Computational social science focuses on deriving important patterns and insights from \nstructured and unstructured data, leading to empirical generalizations.  \n \n4 We acknowledge that the uptick in social good themed conferences, special issues, and research on topics such as \njustice, poverty, clean water, and inequalities, as well as new schools/colleges focusing on urban analytics and \nclimate and sustainability, will undoubtedly help reverse this trend. The rise of generative AI is ushering a new wave \nof ethics research on algorithmic bias, fairness, privacy, and related responsible AI themes.  \n\n6 \n \n  \n \nFigure 3: Data Science for Social Good (DSSG) in Information Systems (IS): The Intersection of Research Genres, \nSocial Good Challenges, and Socio-Technical Environments \nComputationally intensive theory construction builds on this idea, but specifically emphasizes theory construction \nand the necessary prerequisite guardrails and formalism (Berente et al., 2019; Miranda et al., 2022). The process of \npattern surfacing involves consideration of the research communities’ knowledge and cumulative tradition, \nmanifesting in the form of lexical framing (Miranda et al., 2022). More specifically, lexical framing entails the use \nof practice lexicons (elements of the phenomena), method lexicons (computational techniques), and theoretical \nlexicons (concepts and associations). \nComputational design is a genre of design science that emphasizes solving business and societal problems through \nthe development of computational artifacts that are impactful, relevant, and often interdisciplinary (Rai, 2017). \nThere has been a significant uptick in interest regarding machine learning-oriented computational design research \n(Padmanabhan et al., 2022). Computational design research relates to developing novel models or algorithms that \noffer design guidelines and/or best practices that contribute to a cumulative design tradition for a class of problems \nand/or artifacts. \nIt is important to state that the three research genres discussed do not necessarily have a one-to-one mapping with \nthe data science paradigm. For instance, computational social science also belongs to the research genre taxonomy \nfor social science. Computationally intensive theory construction relates to the grounded theory approach (Berente et \nal., 2019), and also encompasses the use of simulation methods (Miranda et al., 2022), which are often not \nassociated with the data science paradigm (Hey et al., 2009). As alluded to, computational design is also a sub-genre \nof design science, and furthermore, relates to the technical branch of quantitative research alongside other \nquantitative branches such as analytical and empirical. It is also worth noting that the genres included are illustrative \nof data science, not exhaustive. For example, computational economics and machine learning-based econometrics \ncould also be considered data science. Traditional econometrics and experimental methods for causal inference also \nundoubtedly have a role to play (and already are) regarding social good research. However, because we rely on \nGray’s definition of paradigms (Hey et al., 2009), we limit our discussion to the three aforementioned genres.      \n \n3.1.2 Social Good Challenges \nOver the centuries, the term “social good” has been defined in many different ways. It has its roots in philosophy, \nwhere the idea of human flourishing through the “common good” traces back to Aristotle and refers to a good only \nattainable by the community, but shared by its members (MacIntyre, 1984; Smith, 1999). Notably, developing a list \nof what constitutes common good, as well as how to prioritize common good causes in resource-constrained \nenvironments, are both considered non-trivial due to a lack of consensus (Smith, 1999; Kraut, 2022). Some recent \nstudies refer to it as “services or products that promote human well-being on a large scale” (Mor Barak, 2020, p. \n139). Here, the products and services may refer to timely access to healthcare, clean water, quality education, equal \nrights, and so on (Mor Barak, 2020). Some social good studies have proposed the use of the United Nations \nSustainable Development Goals (SDGs) – 17 goals related to alleviating poverty, hunger, and inequalities while \npromoting health and well-being, education, gender equality, clean water, affordable clean energy, climate action, \n\n7 \n \nsustainable cities, peace and justice, and life on land and water, etc. (Cowls et al., 2021). These can be distilled down \nto social good challenges related to alleviation of disparities, climate and sustainability, peace and justice, well-\nbeing, and, of course, technology ethics (including responsible AI).  \n \n3.1.3 Socio-technical Environments \nIS research has long-standing traditions in exploring socio-technical phenomena across multiple levels of analysis. \nThese include the individual, organizational, and institutional levels. The individual level of analysis focuses on how \npeople adopt and use technologies in organizations and online environments (Davis, 1989; Goodhue & Thompson, \n1995; Pavlou & Gefen, 2004). The organizational level emphasizes the successful design, implementation, and \npractices associated with technologies within traditional organizations, as well as in innovative forms such as virtual \nteams and online communities (Markus, 1983; DeLone & McClain, 1992; Orlikowski, 1992; Ren et al., 2012; \nHirschheim et al., 1995). Finally, the institutional level of analysis refers to the societal environment of \norganizations and involves research into the diffusion and impact of technologies in industry, as well as in fields \nsuch as healthcare and governments, and in developing nations (Brynjolfsson, 1994; King et al., 1994; Swanson & \nRamiller, 1997; Angst et al., 2010). \nDSSG has a natural fit with IS for a couple of reasons. First, social good relates to humanistic outcomes – an \nimportant consideration for socio-technical IS work and complementary to research focusing on instrumental \noutcomes (Sarker et al., 2019). Second, some of the data science genres alluded to, namely computationally \nintensive theory construction and computational design, have strong roots in IS. For these reasons, we believe that \nfuture IS work on DSSG has an opportunity to align with the calls for differentiable research, “a unique perspective \nin order to identify, support, and/or legitimize research within that discipline” (Sarker et al., 2019; p. 699).    \n \n3.2 \nDSSG Framework and Examples \nFigure 4 presents a framework based on the data science (DS) genre, social good (SG) challenges, and socio-\ntechnical environmental granularities (IS). What is notable is that data science can potentially create interesting \nintersections with social good (DSSG) and IS (DS in IS), as well as the intersection of all three, represented by the \ncube (DSSG in IS). What is intentionally missing in Figure 4 are guidelines for the rigor of the data science tools, \ntechniques, and principles leveraged, depth of the social good contextualization in terms of immersion in field \nsettings and downstream implications, and the cohesion of the socio-technical environments to the research \nmotivation, undertaking, or outcomes. Readers interested in best practices on data science rigor can turn to various \ntextbooks and editorials on the matter. Those interested in perspectives on the depth of social good contextualization \ncan look at Cowls et al. (2021). We encourage readers interested in how to effectively engage with the socio-\ntechnical IS cumulative tradition to peruse Sarker et al. (2019). The framework emphasizes the DSSG and IS \nintersections that guide which cell a study or stream may fall under. In contrast, the rigor/depth/cohesion are \ncollectively indicators of quality (cell agnostic) and centrality (cell-specific) within the DSSG for IS cube. \nTable 1 presents examples of studies related to DSSG at different levels of socio-technical granularity (e.g., people, \norganizations, and institutions). The three rows relate to the DS genres, whereas the three columns depict socio-\ntechnical environments from an IS vantage point. Each cell features examples of studies that explore social good \nchallenges using a particular DS genre and a certain socio-technical granularity level. We intentionally include \nstudies appearing in top IS outlets, as well as those published by IS scholars in adjacent fields/journals, and also \nstudies conducted in related fields by sociologists and computer scientists. In doing so, we believe the chosen studies \nare interesting examples of DSSG in terms of rigor and depth of engagement with social good challenges but are not \nnecessarily exemplars of DSSG in IS in terms of cohesion with the IS socio-technical cumulative tradition. Because \nthe DSSG in IS literature is emerging (we explicitly discuss this emerging space later in Section 4.1), our goal is to \ninclude a panorama of interesting DSSG work to inspire a new wave of DSSG research in IS.     \nIn Table 1, the first row depicts studies from the computational social science genre. The mode for computational \nsocial science research examining social good challenges has been to analyze social networks with the objective of \nrevealing node characteristics, structural linkage patterns, and/or information diffusion patterns over time across \norganizations or institutions (Lazer et al., 2009; Aral & Walker, 2012). For instance, Xu and Chen (2005) construct \ncriminal activity networks for narcotics gangs in the southwestern United States responsible for methamphetamine \ntrafficking. Others have examined the role of key actors in online social movement organizations that promote racial \nand religious prejudice and violence (Chau & Xu, 2007; Zimbra et al., 2010). At the institutional level, \ncomputational social science studies have explored susceptibility to influence and the propagation of fake news \n(Vosoughi et al., 2018), with dire implications for media institutions, digital platforms, and the role of technology in \nsupporting the pursuit of evidence and truth (Lee & Ram, 2023).            \n \n\n8 \n \n \nFigure 4: Data Science for Social Good (DSSG) in Information Systems (IS): A Framework that Considers \nResearch Genres, Social Good Challenges and Contexts, and Socio-Technical Environments \n \nTable 1: Data Science for Social Good (DSSG) in Information Systems (IS): Research Examples \n \nTechnology & \nPeople \nTechnology & \nOrganizations \nTechnology & \nInstitutions \nComputational \nSocial Science \nUsing word embeddings to \nunderstand gender/ethnic \ndisparities \n(Garg et al., 2018) \nCrime network analysis  \n(Xu and Chen 2005) \n \nAnalyzing online hate \ncommunities \n(Chau and Xu 2007) \n \nAnalyzing social movement \norganizations \n(Zimbra et al. 2010) \n \nUsing LLMs to identify aspects \nof societal culture \n(Kozlowski et al. 2019) \n \nFairness in ratemaking \n(Zhang and Xu 2024) \n \nSpread of true and false news \n(Vosoughi et al. 2018) \nComputationally \nIntensive \nTheory \nConstruction \nTBD \nFirm Communication During \nDisasters \n(Yan et al. 2024) \nTBD \nComputational \nDesign \nML4H – Machine learning for \nhealth: fairness & disparities  \n(Sarkar et al. 2020) \n \nAI4SG – AI for social good \n(Cowl et al. 2021) \n \nProactively Detecting \nEmotional Distress \n(Chau et al. 2020) \nDecision support for wildfire \nmanagement \n(Gomez et al. 2024) \nML4D – Machine learning for \nthe developing world \n(De-Arteaga et al. 2018) \n \nImproving Drinking Water \nAccess and Equity in Africa \n(Zhai et al. 2023) \n \nCredibility by Design in \nListening for Public Health 3.0 \n(Kitchens et al. 2024) \n \n\n9 \n \nAn exciting development within the computational social science space is the use of text analysis to derive important \ninsights into social good challenges. Garg et al. (2018) show that word embeddings, a natural language processing \ntechnique commonly used in modern data science research and practice, can learn and reflect gender and ethnicity-\nrelated stereotypes. Using data from 1910 to 1990, they find that gender biases in word embeddings trained on \nvarious available textual data sources from across the twentieth century are highly correlated with occupational \ndifferences between men and women from those time periods. At the institutional level, Kozlowski et al. (2019) use \nword embeddings to explore “the geometry of culture” as it relates to evolving markers of social class (e.g., wealth, \neducation, etc.). The two studies illustrate how patterns codified in word embeddings and large language models \n(LLMs) can shed light on social constructs and cultural artifacts involving people, organizations, institutions, and \nsociety. They also highlight the dangers of machine learning models trained on historical data regarding the current \nand future role of AI-enabled systems in supporting social progress and a more just society (Kane et al., 2021). \nThe frequency of climate-related and man-made disasters has accelerated in recent years. Moreover, disasters often \nhave a disproportionately greater impact on those most vulnerable. Within the computational social science genre \napplied to the institutional level, Zhang and Xu (2024) explore the fairness of ratemaking in catastrophe insurance. \nUsing concepts from machine learning, they shed light on the disparate impact current ratemaking policies have on \nminorities. Given the nascent body of research on computationally intensive theory construction, we briefly discuss \nDSSG research related to this genre. Within the disaster response arena, Yan et al. (2024) propose a framework with \na novel word embedding approach to explore the impact of message orientations of firm communication during \ndisasters on public engagement. They use the emerging nature of disasters, and lack of well-established theories \ncapable of offering strong support for formal deductive hypotheses, to  motivate their use of a computationally \nintensive theory construction framework. Based on the dynamics of social good challenges, we envision ample \nopportunities for computationally intensive theory building within the DSSG in IS space. \nSimilar to computationally intensive theory construction, the computational design genre presents unique \nopportunities for IS to offer differentiable thought leadership in the DSSG in IS space. At the intersection of \ntechnology and people are efforts related to ML4H (machine learning for health) focused on “accessible diagnostic \nand prognostic systems, health equity, fairness and bias, generalization across populations or systems, improving \npatient participation in health…” (Sarkar et al., 2020; p. 2). Other examples include AI4SG (AI for social good), the \nuse of AI to improve the human condition as it relates to the United Nation’s sustainable development goals (Cowls \net al., 2021), and the use of machine learning to identify online users battling emotional distress (Chau et al., 2020). \nAt the organizational level, Gomez et al. (2024) tackle the problem of how wildfire management agencies can better \nbudget for upcoming wildfire seasons through design artifacts that couple predictive and prescriptive models that \nconsider climate uncertainty and opportunity costs of over/under-budgeting. \nComputational design research considering institution-level characteristics and application scale represents an \nimportant and exciting opportunity for DSSG in IS. How can we identify the optimal arrangement of wells for \ndrinking water in Ethiopia with limited digitized knowledge of resources, constraints, and village/community \ndynamics? This is the challenge Zhai et al. (2023) tackle, using field interviews and site visits to identify data inputs \nfor prescriptive models, including the dynamics of adjacent communities (that often coordinate on water usage and \ntransportation across neighboring villages), the impact of war, as well as efficiency and equity considerations. In the \nsame vein, De-Arteaga et al. (2018, p. 3) discuss ML4D (machine learning for the developing world) scenarios \nwhere “existing or plausible solutions in developed regions are not viable” for developing countries. At the \ninstitutional level, examples include modeling patterns of violence and human rights violations and detecting \ncorruption in international development contracts. Kitchens et al. (2024) explore the design of social listening \nplatforms for problems such as opioid/drug epidemics in online settings rife with low-credibility content.           \nFrom our discussion of the DSSG framework, we are optimistic that opportunities abound for societally impactful IS \nresearch using data science. In the next section, we discuss the current business/management/IS research landscape \nrelated to DSSG and then offer high-level guidelines of what we believe is needed to foster greater DSSG research. \n  \n4 \nThe Current State of DSSG Research \n \n4.1 \nMeta-analysis \nWe performed a literature analysis of DSSG articles published over the 11-year time-period spanning 2012–2022 to \nprovide an overview of how IS and other related disciplines have used data science-related computational genres to \naddress societal challenges, and where these existing studies could be positioned in the DSSG framework. A \nsummary of our analysis appears in Figure 5, including the genres, social good challenges, and socio-technical \nenvironments. We discuss the process undertaken before delving into the figure results and takeaways. Because the \n\n10 \n \nprimary goal of this analysis was to be illustrative as opposed to exhaustive, we focused on nine IS journals, \nincluding the Senior Scholars’ Basket of Eight and Decision Support Systems. To have a broader view of the DSSG \nresearch in other related disciplines, we expanded our literature survey to include Accounting, Finance, \nManagement, and Economics journals in the Financial Times Top 50 list and five Sociology journals identified by \nthe Scimago Journal & Country Rank: American Sociological Review, Annual Review of Sociology, Journal of \nInformation Communication & Ethics in Society, Sociological Methods & Research, and the American Journal of \nSociology. We also included two interdisciplinary journals: INFORMS Journal of Data Science and Management \nScience.  \nTo ensure the broadest coverage possible, we started the literature search using a single keyword: data. An article \nwas included in the sample if the word “data” appeared in the article’s title, abstract, subjects, or keyword list. The \nresulting set consisted of 6,920 data-related articles published in 42 journals. Two graduate research assistants read \nthe titles and abstracts of these articles to filter out articles unrelated to data science or that involved non-\ncomputational research genres. Examples of excluded articles are editorials, research commentaries and opinions \nregarding big data and technologies, empirical articles not using big data or computational methods, etc. The filtered \ndataset comprised 659 data science articles, the majority of which (540 articles) addressed various business, \neconomic, and/or instrumental objectives (e.g., revenue, productivity, efficiency). Figure 5 refers to these as the data \nscience (DS) set of articles. Only 119 articles tackled social issues aligned with our broad list of social good \nchallenges. Figure 5 refers to these as the DSSG set. \n \n \n(a) \n \n(b) \n \n(c) \nFigure 5: Number of DS articles and DSSG articles published during 2012-2022, depicted by year (a), by discipline \n(b), and by genre, social good challenge, and socio-technical environment (c). \n \nFigure 5a presents changes in the counts of DS and DSSG articles by year. This includes a sizable jump in the \nnumber of published DS articles in 2021 and onwards. However, although the number of DSSG articles per year has \ndoubled, the proportion focusing on social good has not increased. Looking at the breakdown of DS and DSSG \narticles by disciplines (Figure 5b), we find that the nine IS journals have the highest number of data science articles \npublished, followed by the two interdisciplinary journals, Management, and Sociology journals. Journals in other \ndisciplines, such as Finance, Accounting, and Economics, have markedly fewer DS articles. As mentioned above, \n\n11 \n \nbecause the majority of DS papers in these outlets address instrumental business problems, the number of published \nDSSG articles is significantly lower (under 20% of all DS research). \nIn addition, based on our proposed DSSG research framework, we hand-coded the data science research genres, \nsocial good challenges, and socio-technical environments for the 119 DSSG articles identified through our \nsystematic review (these appear in Figure 5c). In terms of the research genres, the vast majority of DSSG articles are \ndevoted to the design of computational artifacts or computational approaches to uncover social science insights. \nConversely, only a few articles seek to surface computationally-derived patterns with theory-building implications \nrelated to social good. This is intuitive given the relatively longer cumulative traditions associated with \ncomputational design (e.g., Arazy & Woo, 2007; Abbasi & Chen, 2008) and computational social science (Lazer et \nal., 2009). Regarding social good challenges, many articles investigate challenges related to well-being (e.g., \nhealthcare, pandemic monitoring, etc.). There is also a growing interest in research that examines disparities, peace, \njustice, and technology ethics. Relatively fewer articles explore sustainability-related issues. Following Ketter et al. \n(2022), we hope to see more work on sustainability and design in contexts such as smart mobility. Regarding the \nsocio-technical positioning of DSSG research, fewer articles have explored challenges at the institutional level, with \ngreater impetus on people and organizations. This is a microcosm of a broader trend related to relatively fewer \narticles focusing on technology and institutions (Barley & Orlikowski, 2023). \nOur analysis of DSSG in IS and adjacent fields reveals that there are opportunities for researchers to take a more \nsignificant thought leadership role regarding social good challenges using data science research genres. Given the \nbroader trends of exponential growth in data science research as a whole (Figure 1) and proportionally less emphasis \non social good themes (Figure 2), the opportunity and impetus for IS are perhaps even greater. In the next section, \nwe discuss guidelines for DSSG research in IS. \n \n4.2 \nGuidelines for DSSG Research \n \n4.2.1 Beyond Availability Biases – Going the Last Research Mile \nIn June 2020, the crowd-sourced business review platform Yelp added a feature allowing businesses to label \nthemselves as black-owned. The move spurred a flurry of working papers and publications examining the impact of \nsuch a label on sales and/or review ratings for black-owned businesses (Aneja et al., 2023; Babar et al., 2023), as \nwell as work on racial disparities related to borrowing through the paycheck protection program (Evans, 2021; \nChernenko & Scharfstein, 2022). In total, between January 2021 and September 2023, we observed an astounding \n108 new studies added to the Google Scholar index that examine the impact of the new “black-owned” labeling \nfeature on Yelp (as a natural experiment) or that use the label to explore other types of disparities (e.g., \ndiscrimination faced by minority small-business owners). On the one hand, this example shows that the academic \nresearch community is committed to exploring social good challenges. It suggests that the lack of earlier research on \nsuch questions is not a function of apathy. On the other hand, it also speaks to the academic research communities’ \nover-reliance on readily available, easy-to-collect data sources curated from API/platform-driven environments.   \nFor DSSG to thrive, what is needed are programmatic streams of high-impact research (Ram & Goes, 2021). \nCharacteristics of programmatic research include (Ram & Goes, 2021): (1) that it is thoughtfully and deliberately \ndesigned, rather than being opportunistic; (2) that it is geared towards tackling big questions/challenges; (3) and that \nit often relies on significant (primary) data collection efforts. Let us contrast this with the status quo for data science \nresearch. Using the computational design genre as an example, within that genre, data science artifacts are often \nevaluated and validated based on how well they perform across a set of well-established performance metrics (e.g., \naccuracy and sensitivity). The importance of such metrics has further amplified in recent years with the rise of data \nanalytics competitions, crowd-sourcing platforms, and leaderboards. While such metrics are important, and in some \nrespects, they constitute the “price of admission” for artifact design, they often fail to consider key downstream \nimplications – humanistic outcomes and societal impact. Some scholars have described this as “going the last \nresearch mile…using scientific knowledge and methods to address important unsolved classes of problems for real \npeople with real stakes in the outcomes” (Nunamaker et al., 2015, p. 11). \nTo illustrate this difference between an article and a programmatic research stream that goes “the last mile,” we can \nlook at the important work on computational design in the context of mental health. With the broader availability of \nsocial-media data labeled with the authors’ mental health status, two review articles on machine learning-based \ndepression detection from social media identified over 110 new articles published on the topic since 2016 (Liu et al., \n2022; Malhotra & Jindal, 2022;). However, almost none discuss testing their proposed models/methods in actual \nfield environments. Nor do these studies highlight the efficacy of placing this much emphasis on social media data \nrelative to clinical data settings shown to have less veracity and higher clinical validity for diagnosing mental health \nconditions (Seyedi et al., 2023). Conversely, the 2020 winner of the INFORMS ISS Design Science Award was a \n\n12 \n \nmulti-year research stream related to identifying online users battling emotional distress and developing monitoring \nand decision-support tools for workers in suicide prevention centers (Li et al., 2014; Chau et al., 2020). \nIt is important to note that in the emerging data science for social good literature, some are setting the bar for social \nimpact even higher. For instance, Cowls et al. (2021) propose five criteria for “AI 4 social good” research that \ninclude “projects built and used in the field for at least six months,” and that have “documented positive impact.” \nTaylor et al. (2019) highlight the importance of engaging with the communities affected by social good research \nwith their “seven habits of highly successful research in special populations.” We recognize that programmatic \nresearch may fall along a continuum, and not every stream will meet such stringent criteria. At the very least, we \nhope that in the early stages of research projects, more conversations will begin with “what’s important?” and “what \nmight be impactful?” rather than “what’s readily available?”  \n \n4.2.2 From the Common Task Framework to a Common Good Framework \nIn recent years, data science has progressed considerably under the common task framework (CTF) (Liberman, \n2010; Donoho, 2017). Originally designed for predictive machine learning models at DARPA in the 1980s and later \nadopted by the IARPA and NIST-sponsored Text REtrieval Conference (TREC) in the 1990s for large-scale \nevaluation in information retrieval contexts, CTF has been defined as the “quantitative comparison of alternative \nalgorithms on a fixed task” (Liberman, 2010, p. 598). It relies on the construction of publicly available training \ndatasets, mechanisms to evaluate trained models against test data, and a predefined set of key performance indicator \nmetrics (Donoho, 2017). CTF is undoubtedly a driving force behind advancements in state-of-the-art computer \nvision and natural language processing, resulting in dramatic improvements in an array of downstream tasks related \nto image recognition and language modeling capabilities. CTF has produced hundreds of thousands of research \narticles in recent years, fueling the aforementioned explosion of publications and citations pertaining to deep \nlearning.  \nCTF has drawn attention from various fields. In psychology, it inspired efforts related to reproducibility and \nreplication through the Open Science Framework (osf.io). In the social sciences, it is the basis for the novel \nexperiment design known as behavioral mega-studies (Milkman et al., 2021), which aim to use common data \ncollection to test a large number of hypothesized treatments in one fell swoop (thereby controlling for differences in \npopulations, time, and space). Statisticians attribute the rise of predictive modeling (as a major thrust) within data \nscience to CTF (Donoho, 2017). In IS, the benefits of such common benchmark datasets have been noted \n(Padmanabhan et al., 2022, p xiii), with researchers invited “to consider what it would take to construct benchmark \nevaluations in our field.” \nDespite the many merits and successes of CTF, we cannot help but feel that, in some respects, data science has been \nled astray. Case in point, in natural language processing, there are well over 100 common tasks for text \nclassification. However, only a few publicly available testbeds and tasks exist for assessing the fairness of text \nclassification models (Abbasi et al., 2021; Guo et al., 2022; Lalor et al., 2022). Access to data remains an \nimpediment to socially good data science research – a point we alluded to in our discussion of availability biases. \nWhat we need is a common good framework – a collection of social good tasks comprising publicly available data \nsets and/or access to field research, with established success criteria and guardrails for avoiding unintended \nconsequences of data science (Cowls et al., 2021).    \n \n4.2.3 Aligning Incentives – Reconsidering Research Productivity and Impact Metrics \nA major obstacle to overcoming availability biases and developing common good frameworks is status quo \nincentive structures for early-career researchers. The current flavor-of-the-month for “publish or perish” emphasizes \nwhere to publish, and how many to publish. On the question of appraising what was published, the standard-keepers \nare undiscerning. This is reflected in our academic productivity rankings (another type of leaderboard with \nunintended consequences), which rank individuals, departments, colleges, and universities based on the number of \npublications (X) in a pre-defined set of journals (Y). Using data science speak, unsurprisingly, this flavor-of-the-\nmonth is causing many scholars to treat research as a min-max problem – how to get a desired X in Y while \nminimizing the maximum effort needed, resulting in minimal viable product research. \nAdmittedly, the min-max reference might be overly cynical. Nonetheless, any reasonable “accounting of the \ncounting” in research productivity measurement will reveal that if the goal is to focus on impact, the incentive \nmisalignment is very real. We have talked to countless (pun intended) early-career scholars who reference the \nfamous PhD.com comic5 about the (lack of) “Evolution of Intellectual Freedom” across one’s career. Coincidentally, \nmany of the same early-career scholars describe their scientific process as the “need for speed” – getting their \n \n5 https://phdcomics.com/comics/archive.php?comicid=1436 \n\n13 \n \nresearch accepted before other researchers studying the same problems, using the same data, with the same \nmethodological perspectives. The silver lining is the renewed emphasis on promoting social good research through \nspecial issues at top venues, such as this one and others (Aanestad et al., 2021; Dutta et al., 2023), and impact-\noriented research awards6. We hope more such publication and recognition opportunities will be available in the \ncoming years.   \n4.2.4 Large Language Models as a Frontier for Understanding Social Good \nThese days, no discussion of data science is complete without mentioning large language models (LLMs) such as \ngenerative pre-trained transformers (GPTs) (Brown et al., 2020). In our discussion of the DSSG framework, we \nnoted the exciting trend of using LLMs for computational social science research (Ziems, 2023). We discussed how, \nin their aptly titled paper, “The Geometry of Culture,” Kozlowski et al. (2019) trained a word embedding on text \nfrom millions of books published over 100 years to perform a historical analysis of the evolution and dynamics of \nshared understandings of social class. We also described how Garg et al. (2018) used word embeddings to quantify \n100 years of gender and ethnic stereotypes. Some computational social science researchers have argued that the far \ngreater levels of specificity associated with algorithms could be useful for probing aspects of human decision-\nmaking (generally considered to be less transparent due to a myriad of reporting/disclosure biases) to help detect \ndiscrimination in human decision-making (Kleinberg et al., 2020). Terms such as digital anthropology – the use of \ndigital technologies within the anthropological methodology (Miller, 2018) – and cyber archaeology – the study of \ncultural cyber artifacts (Zimbra et al., 2010) – have previously been used to connote the confluence of digital and \nphysical worlds as it relates to understanding the human condition, groups, societies, and cultures. Advancements in \nLLMs have the potential to “revolutionize anthropological research and practice” in business/organizational settings \n(Artz, 2023), where digital traces such as documents, emails, meeting minutes, transcriptions, etc., capture genres, \npower structures, organizational routines, norms, culture, and so forth.  \nOf all the available state-of-the-art tools, methods, and artifacts for data science, we specifically mention LLMs in \npart due to their ability to capture rich interpersonal patterns across massive corpora, thereby overcoming some of \nthe data/resource accessibility limitations of traditional large-scale longitudinal field data collections. Moving \nforward, we believe LLMs will have multiple roles to play in the context of DSSG in IS: (1) as a mechanism for \novercoming availability biases in research; (2) as a computational social science tool for studying social good \nchallenges by examining patterns codified in LLMs pretrained and/or fine-tuned on relevant individual, \norganizational, or institutional corpora; (3) as a source of codified stereotypes and other related disparities in modern \ngenerative AI; (4) as the basis for a new wave of computationally intensive theory building and computational \ndesign artifacts related to social good challenges. \n5 \nIntroduction to the Special Issue \nIS scholars are interested in using their research to address various societal challenges and make a difference. This \ncan be seen by the many inquiries about and the responses to the Call for Papers on this special issue announced in \nAugust 2020. We want to thank the 44 groups of authors who submitted their extended abstracts in November 2020 \nfor comments and feedback. In April 2021, based on our feedback related to abstracts, we received 20 full-paper \nsubmissions. A team of 34 researchers with extensive knowledge, expertise, and research records in data science \nreviewed these submissions. Their valuable and insightful comments provided authors with ideas and constructive \ninstructions to improve their manuscripts. The dedication of these reviewers helped us immensely during the review \nprocess. Without their outstanding contributions, for which we are grateful, we would not have been able to publish \nthis special issue. After four rounds of extensive review and author revisions, we accepted four articles for this \nspecial issue. These four articles represent a diverse range of DSSG genres, challenges, and socio-technical \nenvironments. See Figure 6.  \nThe article entitled “ShowCase: A Data-Driven Dashboard for Federal Criminal Sentencing,” presents a study in the \ntechnology and institution’s socio-technical environment. It seeks to address the possible inequality and \ndisproportionate sentencing of minorities in the legal and justice context. Although this article is mostly in the \ncomputational design genre, it is also highly related to computational social science. The design artifact, a data-\ndriven dashboard named ShowCase, is grounded in theories from the organizations and penal justice literature. The \ndashboard can help judges make fairer and more objective decisions by integrating a variety of data points. This \nresearch has the potential to promote fairness, objectivity, and transparency in the criminal justice system. \n \n \n6 For instance, the ISS Bapna-Ghose Social Justice Award: https://www.informs.org/Recognizing-\nExcellence/Community-Prizes/Information-Systems-Society/ISS-Bapna-Ghose-Social-Justice-Best-Paper-Award \n\n14 \n \n \nFigure 6: Grouping of Four Special Issue Articles Based on our DSSG-IS Framework \n \nThe research presented in the article, entitled “Designing Conversational Dashboards for Effective Use in Crisis \nResponse,” is intended to enhance community well-being by helping government agencies and health organizations \nimprove their responses to crises such as the COVID-19 pandemic. Similar to the first article, this computational \ndesign research employs the design science methodology. It develops a conversational dashboard that allows users \nto use natural languages to interact with the system. The evaluation of the dashboard shows that users can find their \nneeded information more effectively and efficiently – important outcomes when sense-making during crisis \nresponse.  \nThe third article, entitled “Champions for Social Good: How can we Discover Social Sentiment and Attitude-driven \nPatterns in Prosocial Communication?” demonstrates how DSSG research can contribute to the efforts of enhancing \nharmony and peace at the society level. Motivated by the social media strategy of the United Nations High \nCommissioner on Refugees (UNHCR), this research studies how the Twitter communication of high-profile \nprosocial influencers or champions for change might have influenced their followers and audiences. Using a Twitter \nsentiment and attitude corpus, an analytics framework is proposed, which consists of machine learning and natural \nlanguage processing and is used to test the impact of different types of refugee-related emergencies and champion \ninfluencers, on patterns observed in social communication. This research showcases the contributions of data \nscience research to prosocial policies regarding refugee crisis awareness in a broader institutional context.  \nThe article “Taking the Person Seriously: Ethically-aware IS Research in the Era of Reinforcement Learning-based \nPersonalization” is positioned more as a research commentary. It posits that the development of reinforcement \nlearning techniques, which have been increasingly employed in personalization and adaptive control of individuals’ \nenvironments, may endanger persons and societies at scale. Five emergent features of this new personalization \nparadigm are identified, and their potential dangers are discussed, including diminished personal autonomy, social \nand political instability, mass surveillance and social control, and privacy invasions, among other concerns. Because \nthese potential issues cannot be addressed adequately by current data protection laws and regulations, this article \nproposes three directions for ethically-aware reinforcement learning-based personalization research uniquely suited \nto the strengths of IS researchers across the socio-technical spectrum. Although this article does not directly fit into \nthe three data science genres explicated in our DSSG-IS framework, per se, we believe it has far-reaching \nimplications directly related to all three genres. For instance, the article talks about ethical design, offers lexical \n\n15 \n \nframing for studying the social challenges of reinforcement learning, and provides a bevy of problem contexts where \ncomputational social science may illuminate the size and scope of issues created by reinforcement learning.     \nThese four articles add knowledge and insights to the DSSG literature and identify potential research directions for \naddressing various societal challenges and issues. They employ different research genres and focus on unique social \ngood challenges in distinct socio-technical environments. We hope readers find them informative and helpful for \ntheir future DSSG research. We also hope that IS data science research can reverse the alarming trend we observed \nacross all data science research over the past 30-plus years in which social good challenges are garnering \nproportionately less attention with each passing day.  \n \nReferences \nAanestad, Margunn, Atreyi Kankanhalli, Likoebe Maruping, Min-Seok Pang, and Sudha Ram. \"Digital technologies and social \njustice.\" MIS Quarterly (2021). \nAbbasi, A., & Chen, H. (2008). CyberGate: a design framework and system for text analysis of computer-mediated \ncommunication. MIS Quarterly, 811-837. \nAbbasi, A., Zhang, Z., Zimbra, D., Chen, H., & Nunamaker Jr, J. F. (2010). Detecting fake websites: The contribution of \nstatistical learning theory. MIS Quarterly, 435-461. \nAbbasi, A., Sarker, S., & Chiang, R. H. (2016). Big data research in information systems: Toward an inclusive research \nagenda. Journal of the Association for Information Systems, 17(2), 3. \nAbbasi, A., Dobolyi, D., Lalor, J. P., Netemeyer, R. G., Smith, K., & Yang, Y. (2021, November). Constructing a psychometric \ntestbed for fair natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural \nLanguage Processing (pp. 3748-3758). \nAgarwal, R., & Dhar, V. (2014). Big data, data science, and analytics: The opportunity and challenge for IS research. Information \nSystems Research, 25(3), 443-448. \nAneja, A., Luca, M., & Reshef, O. (2023). Black Ownership Matters: Does Revealing Race Increase Demand For Minority-\nOwned Businesses? (No. w30932). National Bureau of Economic Research. \nAngst, C. M., Agarwal, R., Sambamurthy, V., & Kelley, K. (2010). Social contagion and information technology diffusion: The \nadoption of electronic medical records in US hospitals. Management Science, 56(8), 1219-1241. \nAral, S., & Walker, D. (2012). Identifying influential and susceptible members of social networks. Science, 337(6092), 337-341. \nArazy, O., & Woo, C. (2007). Enhancing information retrieval through statistical natural language processing: a study of \ncollocation indexing. MIS Quarterly, 525-546. \nArtz, M. (2023). The Digital Turn in Business Anthropology. Journal of Business Anthropology, 12(1). \nBabar, Y., Mahdavi Adeli, A., & Burtch, G. (2023). The Effects of Online Social Identity Signals on Retailer \nDemand. Management Science, forthcoming. \nBarley, S. R., & Orlikowski, W. J. (2023). Technologies change, the charge remains the same. Administrative Science \nQuarterly & MIS Quarterly Research Curation. \nBerente, N., Seidel, S., & Safadi, H. (2019). Research commentary—data-driven computationally intensive theory \ndevelopment. Information Systems Research, 30(1), 50-64. \nBerente, N., Gu, B., Recker, J., & Santhanam, R. (2021). Managing artificial intelligence. MIS Quarterly, 45(3). \nBreiman, L. (2001). Random forests. Machine Learning, 45, 5-32. \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-\nshot learners. Advances in neural information processing systems, 33, 1877-1901. \nBrynjolfsson, E. (1993). The productivity paradox of information technology. Communications of the ACM, 36(12), 66-77. \nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., ... & Zhang, Y. (2023). Sparks of artificial general \nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712. \nChau, M., & Xu, J. (2007). Mining communities and their relationships in blogs: A study of online hate groups. International \nJournal of Human-Computer Studies, 65(1), 57-70. \nChau, M., Li, T. M., Wong, P. W., Xu, J. J., Yip, P. S., & Chen, H. (2020). Finding People with Emotional Distress in Online \nSocial Media: A Design Combining Machine Learning and Rule-Based Classification. MIS Quarterly, 44(2). \nChen, H., Chiang, R. H., & Storey, V. C. (2012). Business intelligence and analytics: From big data to big impact. MIS \nQuarterly, 1165-1188. \nChernenko, S., & Scharfstein, D. S. (2022). Racial disparities in the paycheck protection program (No. w29748). National \nBureau of Economic Research. \nCowls, J., Tsamados, A., Taddeo, M., & Floridi, L. (2021). A definition, benchmark and database of AI for social good \ninitiatives. Nature Machine Intelligence, 3(2), 111–115. \n\n16 \n \nDavenport, T. H., & Patil, D. J. (2012). Data scientist: The sexiest job of the 21st century. Harvard Business Review, 90(5), 70–\n76. \nDavis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, \n319–340. \nDe-Arteaga, M., Herlands, W., Neill, D. B., & Dubrawski, A. (2018). Machine learning for the developing world. ACM \nTransactions on Management Information Systems (TMIS), 9(2), 1–14. \nDeLone, W. H., & McLean, E. R. (1992). Information systems success: The quest for the dependent variable. Information \nSystems Research, 3(1), 60–95. \nDhar, V. (2013). Data science and prediction. Communications of the ACM, 56(12), 64–73. \nDonoho, D. (2017). 50 years of data science. Journal of Computational and Graphical Statistics, 26(4), 745–766. \nDutta, K., Kumar, A., Guo, Z., Bichler, M., Delen, D., Brooks, P., and Ramesh, R. Special Issue on Responsible AI and Data \nScience for Social Good, INFORMS Journal on Computing, 35(4) pp. 713-716. \nEdelmann, A., Wolff, T., Montagne, D., & Bail, C. A. (2020). Computational social science and sociology. Annual Review of \nSociology, 46, 61-81. \nEvans, A. (2021). The Mistreatment of Black-Owned Businesses During the First and Second Rounds of the Paycheck Protection \nProgram. Rutgers Bus. LJ, 17, 107. \nGarg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic \nstereotypes. Proceedings of the National Academy of Sciences, 115(16), E3635-E3644. \nGomez, C., Suarez, D., Akhavan-Tabatabaei, R., Medaglia, A., and Grajales, S. (2024). Integrated Decision Support for Disaster \nRisk Management: Aiding Preparedness and Response Decisions in Wildfire Management. Information Systems Research, \nforthcoming. \nGoodhue, D. L., & Thompson, R. L. (1995). Task-technology fit and individual performance. MIS Quarterly, 213–236. \nGrover, V., Chiang, R. H., Liang, T. P., & Zhang, D. (2018). Creating strategic business value from big data analytics: A research \nframework. Journal of Management Information Systems, 35(2), 388–423. \nGuo, Y., Yang, Y., & Abbasi, A. (2022, May). Auto-debias: Debiasing masked language models with automated biased prompts. \nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. \n1012–1023). \nHirschheim, R., Klein, H. K., & Lyytinen, K. (1995). Information systems development and data modeling: conceptual and \nphilosophical foundations (Vol. 9). Cambridge University Press. \nKane, G. C., Young, A. G., Majchrzak, A., & Ransbotham, S. (2021). Avoiding an oppressive future of machine learning: A \ndesign theory for emancipatory assistants. MIS Quarterly, 45(1), 371-396. \nKetter, W., Schroer, K., & Valogianni, K. (2022). Information Systems Research for Smart Sustainable Mobility: A Framework \nand Call for Action. Information Systems Research, 34(3), 1045–1065. \nKleinberg, J., Ludwig, J., Mullainathan, S., & Sunstein, C. R. (2020). Algorithms as discrimination detectors. Proceedings of the \nNational Academy of Sciences, 117(48), 30096-30100. \nKing, J. L., Gurbaxani, V., Kraemer, K. L., McFarlan, F. W., Raman, K. S., & Yap, C. S. (1994). Institutional factors in \ninformation technology innovation. Information Systems Research, 5(2), 139-169. \nKitchens, B., Claggett, J., and Abbasi, A. (2024). Timely, Granular, and Actionable: Designing a Social Listening Platform for \nPublic Health 3.0. MIS Quarterly, forthcoming. \nKozlowski, A. C., Taddy, M., & Evans, J. A. (2019). The geometry of culture: Analyzing the meanings of class through word \nembeddings. American Sociological Review, 84(5), 905-949. \nKraut, Richard, \"Aristotle’s Ethics\", The Stanford Encyclopedia of Philosophy (Fall 2022 Edition), Edward N. Zalta & Uri \nNodelman (eds.), URL = <https://plato.stanford.edu/archives/fall2022/entries/aristotle-ethics/>. \nLalor, J. P., Yang, Y., Smith, K., Forsgren, N., & Abbasi, A. (2022, July). Benchmarking intersectional biases in NLP. \nIn Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: \nHuman Language Technologies (pp. 3598-3609). \nLazer, D., Pentland, A., Adamic, L., Aral, S., Barabási, A. L., Brewer, D., ... & Van Alstyne, M. (2009). Computational social \nscience. Science, 323(5915), 721-723. \nLee, K., & Ram, S. (2023). Explainable Deep Learning for False Information Identification: An Argumentation Theory \nApproach. Information Systems Research. \nLiberman, M. (2010). Obituary: Fred Jelinek. Computational Linguistics, 36(4), 595-599. \nMacIntyre, A. (1984). After virtue: A study in moral theory, Notre Dame, IN: University of Notre Dame Press. \nMarkus, M. L. (1983). Power, politics, and MIS implementation. Communications of the ACM, 26(6), 430-444. \nMiller, Daniel. (2018) 2023. “Digital anthropology”. In The Open Encyclopedia of Anthropology, edited by Felix Stein. Facsimile \nof the first edition in The Cambridge Encyclopedia of Anthropology.  \nMiranda, S., Berente, N., Seidel, S., Safadi, H., & Burton-Jones, A. (2022). Editor's comments: Computationally intensive theory \nconstruction: A primer for authors and reviewers. MIS Quarterly, 46(2), iii-xviii. \n\n17 \n \nMor Barak, M. E. (2020). The practice and science of social good: Emerging paths to positive social impact. Research on Social \nWork Practice, 30(2), 139-150. \nMustak, M., Salminen, J., Plé, L., & Wirtz, J. (2021). Artificial intelligence in marketing: Topic modeling, scientometric analysis, \nand research agenda. Journal of Business Research, 124, 389-404. \nNunamaker Jr, J. F., Briggs, R. O., Derrick, D. C., & Schwabe, G. (2015). The last research mile: Achieving both rigor and \nrelevance in information systems research. Journal of Management Information Systems, 32(3), 10-47. \nOrlikowski, W. J. (1992). The duality of technology: Rethinking the concept of technology in organizations. Organization \nScience, 3(3), 398-427. \nPadmanabhan, B., Sahoo, N., & Burton-Jones, A. (2022). Machine learning in information systems research. MIS \nQuarterly, 46(1), iii-xix. \nPavlou, P. A., & Gefen, D. (2004). Building effective online marketplaces with institution-based trust. Information Systems \nResearch, 15(1), 37-59. \nPeffers, K., Tuunanen, T., & Niehaves, B. (2018). Design science research genres: introduction to the special issue on exemplars \nand criteria for applicable design science research. European Journal of Information Systems, 27(2), 129-139. \nProvost, F., & Fawcett, T. (2013). Data science and its relationship to big data and data-driven decision making. Big Data, 1(1), \n51-59. \nQuinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81-106. \nRai, A. (2017). Editor's comments: Diversity of design science research. MIS Quarterly, 41(1), iii-xviii. \nRen, Y., Harper, F. M., Drenner, S., Terveen, L., Kiesler, S., Riedl, J., & Kraut, R. E. (2012). Building member attachment in \nonline communities: Applying theories of group identity and interpersonal bonds. MIS Quarterly, 841-864. \nSaar-Tsechansky, M. (2015). Editor's Comments: The Business of Business Data Science in IS Journals. MIS Quarterly, 39(4), \niii-vi. \nSamtani, S., Zhu, H., Padmanabhan, B., Chai, Y., Chen, H., & Nunamaker Jr, J. F. (2023). Deep learning for information systems \nresearch. Journal of Management Information Systems, 40(1), 271-301. \nSarkar, S. K., Roy, S., Alsentzer, E., McDermott, M. B., Falck, F., Bica, I., ... & Hyland, S. L. (2020, November). Machine \nlearning for health (ML4H) 2020: Advancing healthcare for all. In Machine Learning for Health (pp. 1-11). PMLR. \nSarker, S., Chatterjee, S., Xiao, X., & Elbanna, A. (2019). The sociotechnical axis of cohesion for the IS discipline: Its historical \nlegacy and its continued relevance. MIS Quarterly, 43(3), 695-720. \nSimonite, T. (2019). The Godfathers of the AI Boom Win Computing’s Highest Honor, Wired Magazine, \nhttps://www.wired.com/story/godfathers-ai-boom-win-computings-highest-honor/, Accessed Oct. 4, 2023  \nSmith, T. W. (1999). Aristotle on the Conditions for and Limits of the Common Good. American Political Science Review, 93(3), \n625-636. \nSwanson, E. B., & Ramiller, N. C. (1997). The organizing vision in information systems innovation. Organization Science, 8(5), \n458-474. \nTaylor, H. A., Henderson, F., Abbasi, A., & Clifford, G. (2018). Cardiovascular disease in African Americans: innovative \ncommunity engagement for research recruitment and impact. American Journal of Kidney Diseases, 72(5), S43-S46. \nVosoughi, S., Roy, D., & Aral, S. (2018). The spread of true and false news online. Science, 359(6380), 1146-1151. \nWixom, B. H., & Watson, H. J. (2001). An empirical investigation of the factors affecting data warehousing success. MIS \nQuarterly, 17-41. \nYan, B., Mai, F., Wu, C., Chen, R. & Li, X. (2024) A Computational Framework for Understanding Firm Communication During \nDisasters, Information Systems Research, forthcoming. \nZhai, C., Bretthauer, K., Mejia, J., & Pedraza‐Martinez, A. (2023). Improving drinking water access and equity in rural sub‐\nsaharan africa. Production and Operations Management. \nZhang, N., & Xu, H. (2024). Fairness of ratemaking for catastrophe insurance: Lessons from machine learning. Information \nSystems Research, forthcoming. \nZiems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., & Yang, D. (2023). Can Large Language Models Transform Computational \nSocial Science?. arXiv preprint arXiv:2305.03514. \nZimbra, D., Abbasi, A., & Chen, H. (2010). A cyber-archaeology approach to social movement research: Framework and case \nstudy. Journal of Computer-Mediated Communication, 16(1), 48-70.",
    "pdf_filename": "Data Science for Social Good.pdf"
}