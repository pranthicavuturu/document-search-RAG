{
    "title": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based",
    "abstract": "Transformers have revolutionized Computer Vision (CV) and Natural Language Processing (NLP) through self- attention mechanisms. However, due to their complexity, ζ=0.5 theirlatenttokenrepresentationsareoftendifficulttointer- pret. WeintroduceanovelframeworkthatinterpretsTrans- former embeddings, uncovering meaningful semantic pat- ζ=0.35 terns within them. Based on this framework, we demon- strate that zero-shot unsupervised semantic segmentation can be performed effectively without any fine-tuning using ζ=0.2 amodelpre-trainedfortasksotherthansegmentation. Our method reveals the inherent capacity of Transformer mod- elsforunderstandinginputsemanticsandachievesstate-of- Figure1. Hierarchicalclusteringtreeshowingthegroupingofto- the-artperformanceinsemanticsegmentation,outperform- ken relevance maps for all tokens in a latent layer of the Vision ing traditional segmentation models. Specifically, our ap- Transformer,notlimitedtotheCLStoken. Eachleafnoderepre- proachachievesanaccuracyof67.2%andanmIoUof32.9 sentsasingletokenrelevancemap,whilehigher-levelnodesshow aggregated clusters based on a clustering threshold (ζ), which %ontheCOCO-Stuffdataset,aswellasanmIoUof51.9% controls the level of detail. Lower ζ values reveal finer details, onthePASCALVOCdataset. Additionally,wevalidateour while higher values create broader, more general clusters. This interpretabilityframeworkonLLMsfortextsummarization, approachdemonstrateshowpre-trainedVisionTransformerscan demonstratingitsbroadapplicabilityandrobustness. perform unsupervised semantic segmentation, identifying mean- ingfulpatternswithintokenrepresentationswithoutrequiringad- ditionaltrainingorfine-tuning. 1.Introduction In recent years, the Transformer architecture and founda- tion models, leveraging self-attention mechanisms to cap- anisms inherent to Transformers. Recent research has in- ture complex dependencies in text, have transformed Nat- troduced interpretability methods specifically designed for ural Language Processing (NLP) benchmarks [2, 51, 54, Transformers[1,6,59].However,theseapproachesprimar- 58]. Similarly, VisionTransformers(ViTs)[13]havebeen ilyfocusonexplainingfinalmodeloutputs,providinglim- adapted in Computer Vision (CV) and now serve as the itedinsightintotheintermediateprocessesthatleadtopre- backbone for various tasks such as segmentation and ob- dictions. For instance, [7] maps latent tokens into CLIP’s jectdetection[30,52].Despitetheirsuccess,understanding [37] multi-modal space to find corresponding text descrip- theinterpretabilityofTransformersremainsachallengedue tions, relying on an external text encoder for interpretabil- tothecomplexityoftheirlatenttokenrepresentations. ity. In contrast, our approach directly interprets the latent Severalmethodshavebeendevelopedtoenhancethein- spaceofViTs,elucidatingtheroleandfunctionofeachto- terpretability of CNN-based models [44, 47, 63]. While ken within the high-dimensional space without relying on some of these can be extended to Transformer architec- externalmodels. tures,theydonotfullyleveragetheuniqueattentionmech- Thispaperintroducesaframeworktointerpretlatentto- 1 4202 voN 51 ]VC.sc[ 1v98521.1142:viXra stluseRnoitatnemgeS egamIlanigirO",
    "body": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based\nUnderstanding\nHesamHosseini,GhazalHosseiniMighan,AmirabbasAfzali\n{hesam8hosseini, ghazaldesu, amir8afzali}@gmail.com\nSajjadAmini, AmirHoumansadr\nsamini@umass.edu, s amini@sharif.edu amir@cs.umass.edu\nAbstract\nTransformers have revolutionized Computer Vision (CV)\nand Natural Language Processing (NLP) through self-\nattention mechanisms. However, due to their complexity,\nζ=0.5\ntheirlatenttokenrepresentationsareoftendifficulttointer-\npret. WeintroduceanovelframeworkthatinterpretsTrans-\nformer embeddings, uncovering meaningful semantic pat- ζ=0.35\nterns within them. Based on this framework, we demon-\nstrate that zero-shot unsupervised semantic segmentation\ncan be performed effectively without any fine-tuning using ζ=0.2\namodelpre-trainedfortasksotherthansegmentation. Our\nmethod reveals the inherent capacity of Transformer mod-\nelsforunderstandinginputsemanticsandachievesstate-of- Figure1. Hierarchicalclusteringtreeshowingthegroupingofto-\nthe-artperformanceinsemanticsegmentation,outperform- ken relevance maps for all tokens in a latent layer of the Vision\ning traditional segmentation models. Specifically, our ap- Transformer,notlimitedtotheCLStoken. Eachleafnoderepre-\nproachachievesanaccuracyof67.2%andanmIoUof32.9 sentsasingletokenrelevancemap,whilehigher-levelnodesshow\naggregated clusters based on a clustering threshold (ζ), which\n%ontheCOCO-Stuffdataset,aswellasanmIoUof51.9%\ncontrols the level of detail. Lower ζ values reveal finer details,\nonthePASCALVOCdataset. Additionally,wevalidateour\nwhile higher values create broader, more general clusters. This\ninterpretabilityframeworkonLLMsfortextsummarization,\napproachdemonstrateshowpre-trainedVisionTransformerscan\ndemonstratingitsbroadapplicabilityandrobustness.\nperform unsupervised semantic segmentation, identifying mean-\ningfulpatternswithintokenrepresentationswithoutrequiringad-\nditionaltrainingorfine-tuning.\n1.Introduction\nIn recent years, the Transformer architecture and founda-\ntion models, leveraging self-attention mechanisms to cap- anisms inherent to Transformers. Recent research has in-\nture complex dependencies in text, have transformed Nat- troduced interpretability methods specifically designed for\nural Language Processing (NLP) benchmarks [2, 51, 54, Transformers[1,6,59].However,theseapproachesprimar-\n58]. Similarly, VisionTransformers(ViTs)[13]havebeen ilyfocusonexplainingfinalmodeloutputs,providinglim-\nadapted in Computer Vision (CV) and now serve as the itedinsightintotheintermediateprocessesthatleadtopre-\nbackbone for various tasks such as segmentation and ob- dictions. For instance, [7] maps latent tokens into CLIP’s\njectdetection[30,52].Despitetheirsuccess,understanding [37] multi-modal space to find corresponding text descrip-\ntheinterpretabilityofTransformersremainsachallengedue tions, relying on an external text encoder for interpretabil-\ntothecomplexityoftheirlatenttokenrepresentations. ity. In contrast, our approach directly interprets the latent\nSeveralmethodshavebeendevelopedtoenhancethein- spaceofViTs,elucidatingtheroleandfunctionofeachto-\nterpretability of CNN-based models [44, 47, 63]. While ken within the high-dimensional space without relying on\nsome of these can be extended to Transformer architec- externalmodels.\ntures,theydonotfullyleveragetheuniqueattentionmech- Thispaperintroducesaframeworktointerpretlatentto-\n1\n4202\nvoN\n51\n]VC.sc[\n1v98521.1142:viXra\nstluseRnoitatnemgeS\negamIlanigirO\nkens,offeringadeeperunderstandingoftheinternalwork- 2.RelatedWork\nings of Transformers. This understanding enables users\nThe interpretability of deep learning architectures has be-\nto perform image semantic segmentation using pre-trained\ncomeacentralfocusinAIresearch[16].Asmodelsgrowin\nTransformer-basedvisionmodelsinanunsupervised,zero-\ncomplexity,understandingtheirdecision-makingprocesses\nshot manner, without any additional training. We demon-\nis essential for ensuring transparency, reliability, and fair-\nstrate that applying semantic segmentation based on our\nness [53]. Interpretability not only aids in debugging and\ninterpretability framework achieves state-of-the-art perfor-\nperformance improvement but also builds trust in AI sys-\nmanceonbenchmarkimagesegmentationdatasets.\ntems,particularlyinfieldslikehealthcare,finance,andau-\nDrawing inspiration from [6], our method analyzes the tonomous driving. Opaque models can perpetuate biases\nsemantic information retained by latent tokens, enabling andgenerateunforeseenoutcomes, makinginterpretability\ntasks such as object selection and semantic segmentation crucial for bridging high performance with safe, practical\nwithout additional training. We demonstrate that Trans- AIdeployment[26].InTransformermodels,tokensarekey\nformers inherently understand the semantic structure of tointerpretingbehavior,withtheirrelationshiptospatiallo-\ntheir input, viewing it as a collection of distinct concepts. cations in images or sequence order adding an important\nEachlatenttokenidentifiesaspecificconceptwithsemantic layertointerpretation. Thisrelationshipenhancessemantic\nsignificance,therebysheddinglightonthedecision-making segmentationasameasureforevaluatingtoken-basedinter-\nprocessofthesemodels. pretationframeworks,whichthissectionwillexplore.\nAs shown in Section 4, our framework proves effec- 2.1.ModelInterpretation\ntive in a range of tasks, including semantic segmentation\nModel interpretability is a critical area of research in deep\nand model interpretation. Most recent unsupervised se-\nlearning, especially for complex models like transformers.\nmantic segmentation methods involve an additional train-\nTraditionaldeeplearningmodels,suchasCNNs,havebeen\ning phase to learn feature representations [18, 27, 46].\ninterpreted using various techniques like saliency maps\nOur approach, however, utilizes the understanding embed-\n[47], deconvolutional networks [63], Guided Backpropa-\nded in pre-trained models to achieve zero-shot segmenta-\ngation[48],FeatureVisualization[34],LocalInterpretable\ntion, leveraging their inherent knowledge of images. The\nModel-AgnosticExplanations(LIME)[40],SHapleyAddi-\nstronger a model’s comprehension of image content, the\ntive exPlanations (SHAP) [31], Class Activation Mapping\nmore accurately it performs segmentation. We further\n(CAM)[65],FeatureAttributionwithIntegratedGradients\ndemonstratethatourmethodiscapableofinterpretinglarge\n[50],andGrad-CAM[44],whichhighlightthemostimpor-\nlanguage models (LLMs) at the token level, validating its\ntant regions of the input that influence model predictions.\napplication in tasks such as text summarization. The main\nWhile effective for CNNs, these techniques are less suit-\ncontributionsofthispaperareasfollows:\nable for transformer architectures, as they fail to account\nfor the unique self-attention mechanisms that transform-\n• We propose a framework for interpreting latent tokens\ners rely on. Anchors is a model-agnostic interpretability\nin Transformers, providing valuable insights into model\nmethodthatprovideshigh-precision,locallyfaithfulexpla-\ndecision-makingprocesses. nations by generating if-then rules that sufficiently explain\n• By aggregating relevance maps generated by tokens us-\namodel’spredictionforspecificinputs[41].\ninghierarchicalclustering,weachievezero-shotunsuper-\nRecent interpretability research on transformers has fo-\nvisedsemanticsegmentationonpre-trainedmodels,out-\ncused on understanding how these models allocate atten-\nperforming SOTA methods that require additional train-\ntionandpropagateinformation.Aseminalcontributionwas\ning.\nmade by examining self-attention patterns in transformer-\n• Wedemonstratethecapabilityofourmethodtointerpret\nbased language models, revealing how attention is dis-\nLLMs at the token level, showcasing its practical appli-\ntributed across tokens [59]. Abnar et al. [1] advanced\ncabilitytotextualdata. Specifically, wedemonstratethe\nthisbyproposingmethodstovisualizeattentionflowacross\ninterpretation of LLM operations in the text summariza-\nlayers, aiding in the understanding of information propa-\ntiontask.\ngation. Chefer et al. [6] introduced a relevance-based ap-\nproach, computing relevance scores for each token to pro-\nThis paper is structured as follows: Section 2 reviews videdeeperinsightsintothemodel’sdecision-makingpro-\ninterpretability frameworks and previous work on seman- cess. Additionally, the sensitivity of model predictions to\nticsegmentation,highlightingourSOTAresults. Section3 inputtokenshasbeenexploredasanotherwaytointerpret\npresents our interpretability framework. In Section 4, we transformerbehavior[20].\nshowcaseitseffectivenessonimageandtexttasks. Finally, Interpretabilitymethodsforreflectingsemanticrelations\nSection5concludesthepaper. withininputsequenceshavebeeninvestigatedinNLPtasks,\n2\naimingtocharacterizehowtransformersassociatewordsor representationsforunsupervisedsegmentation[56]. Leop-\ntokens across different positions in a sentence for down- art utilized self-supervised ViTs to improve pixel group-\nstream tasks [11]. Justifying the role of the multi-head at- ing,particularlyincomplexscenes[66]. ACSegintroduced\ntention mechanism in transformers and its contribution to adaptive conceptualization for pixel-level semantic group-\nimproved performance is another area of study, revealing ing, using an Adaptive Concept Generator (ACG) to dy-\ninsightsintohowtransformersoperate[32]. Othernotable namicallyalignlearnableprototypeswithrelevantconcepts,\ninterpretability methods include Interpretability-Aware Vi- therebyaddressingover-andunder-clusteringchallengesin\nsualTransformers(IA-ViT)[36],MechanisticInterpretabil- variedimages[27].\nity [39], Concept Transformers [42], Nested Hierarchical Leveraging hidden positives for unsupervised seman-\nTransformers[64],Tracr[29],andViT-net[22]. tic segmentation enhances pixel grouping by identifying\nWhilemanymethodsfocusonexplainingmodeloutputs, andutilizingimplicitpositiverelationshipswithinthedata,\nfewer efforts have been directed toward understanding the boostingsegmentationperformancewithouttheneedforla-\nintermediateprocesseswithinthemodel. Theinternalrep- beled examples [45]. Using self-supervised learning and\nresentations, particularlythelatenttokensinVisionTrans- adaptive feature representations to enable models to dis-\nformers (ViTs), remain largely unexplored. A similar ap- coverandsegmentnovel,unseenobjectcategorieswithout\nproachistheworkby[7],whichfocusesontheCLIPmodel labeleddatahavealsobeeninvestigated[55]. Smoosegin-\n[37]. Their method involves disabling the self-attention troducesasmoothnesspriortoenhancingunsupervisedse-\nmechanismtomapthelatenttokensintoamultimodalspace mantic segmentation by promoting coherent region group-\nwithoutadditionaltraining.However,simplydisablingself- ing [25]. Unsupervised semantic segmentation is also im-\nattentionmayintroducedistributionshifts,raisingconcerns proved by leveraging depth-guided feature correlation and\nabout the validity of their results. Furthermore, their ap- targetedsamplingtoenhanceregionconsistencyandaccu-\nproach relies heavily on CLIP’s text encoder, which limits racy[46]. [17].\nits generalizability to other models. Our work overcomes Unlikeapproachesthatdependonself-training,pseudo-\nthese limitations by focusing on backward-looking analy- labeling, or complex setups, our model ULTra introduces\nsis,investigatinghowlatenttokensrelatetotheinputrather a zero-shot method for unsupervised semantic segmenta-\nthan looking ahead. Additionally, we rely solely on one tion. ULTra achieves strong segmentation performance by\nencoder, making our method more applicable to a broader directly leveraging the semantic information embedded in\nrangeofmodels. the latent tokens of pre-trained Transformer models, with-\nouttheneedforadditionaltrainingorfine-tuning. Further-\n2.2.UnsupervisedSemanticSegmentation more, ULTra emphasizes explainability within Transform-\nersbyillustratinghowthemodel’slatenttokenscontribute\nUnsupervisedsemanticsegmentationhasadvancedsignifi-\ntosegmentation, incontrasttothecommonpracticeofus-\ncantlyduetoself-supervisedlearningandclustering-based\ningonlytheCLStokentorepresenttheentireimage. This\ntechniques,whichreducerelianceonlabeleddatasets.Early\nzero-shot,explainability-drivenapproachoffersanoveldi-\napproaches, suchasInvariantInformationClustering(IIC)\nrection for unsupervised segmentation, demonstrating that\n[21], employedmutualinformationtogroupsimilarpixels\nTransformer-basedarchitecturescanachieveSOTAperfor-\nwithout using labels. Building on this foundation, PiCIE\nmancewithoutlabeleddataorextensiveadditionalprocess-\nintroducedconsistencybyintegratingphotometricandgeo-\ning.\nmetric invariances, setting an important precedent for sub-\nsequentmethods[9].\n3.Methodology\nTheemergenceofVisionTransformers(ViTs)andself-\nsupervisedlearningmarkedamajorshiftinthefield.DINO Inthissection,wepresentourapproachforinterpretingla-\nbecame a pioneering method, extracting rich, meaningful tentrepresentationsinTransformers. Webeginwithessen-\nfeatures without the need for labeled data [5]. Its effec- tialpreliminariesandnotationforclarity,followedbyade-\ntivenessinunsupervisedsegmentationlaidthegroundwork tailed explanation of our method for interpreting latent to-\nfor more recent advancements. For instance, TransFGU kensanditsapplicationtosemanticsegmentation.\n[62] performs semantic segmentation in a top-down man-\n3.1.PreliminariesandNotation\nner by deriving class activation maps from DINO models.\nSTEGO [18] leverages DINO’s features, employing con- ThearchitectureofatypicalTransformercanbeformulated\ntrastivelearningtogroupsimilarregionsandachievingno- asfollows: theinputX issplitintontokens{x }n . Af-\ni i=1\ntableimprovementsinsegmentationaccuracy. tertokenization, tokenembeddings{e }n arecomputed,\ni i=0\nSubsequent methods further refined these concepts. wheree correspondstotheCLStoken. Positionalencod-\n0\nMaskContrastincorporatedclusteringtechniquestoensure ingsPE areaddedtothei-thtokenembeddingtoincorpo-\ni\nregionconsistencyacrossdifferentviews,enhancingfeature ratespatialinformation,resultinginthelatenttokenrepre-\n3\n(a)OriginalImage (b)LatentToken’sRelevanceMap (c)PredictedBinaryMask\nFigure2. Anexampleoftokeninterpretationbyourmodelanditspredictedbinarymask. (a)Originalimage. (b)OverlayofS˜(13)onthe\ni\noriginalimagefordifferenti,wherethelocationofthei-thtokenisindicatedbythepurplesquare. (c)ThebinarymaskM(13) foreach\ni\ncorrespondingrelevancemapin(b).\nsentation z(1) = e + PE . Here, z(l) represents a latent 3.2.InterpretingLatentTokens\ni i i i\ntoken,whereldenotesthelayerindexwithl ∈ {1,...,L}\nA straightforward approach for interpreting a model in-\nandListhetotalnumberoflayersintheTransformer,and\nvolves analyzing the semantic flow from the input to the\nirepresentsthei-thtokenwithinthel-thlayer.\ncorresponding logits. This can be achieved by adding the\nFor each head h ∈ {1,...,H} in the multi-head atten-\nattention probability matrix to an identity matrix I, which\ntion mechanism, the queries, keys, and values correspond-\nincorporatesskipconnections,andthenmultiplyingthere-\ningtothei-thtokenareobtainedvialineartransformations,\nsult across layers [1]. However, a notable challenge arises\nprojectingthelatenttokenofdimensiondintodimensionk:\nfrommultipleattentionheadsineachTransformerlayer.To\naddress this, [6] proposes performing a weighted average\nQ h(l)(z( il−1))=(W h(l ,q))Tz( il−1),K h(l)(zl i−1)=(W h(l ,k))Tz( il−1), acrossattentionheads, withtheweightsdeterminedbythe\ngradientofthelogitswithrespecttotheattentionweights.\nV(l)(z(l−1))=(W(l))Tz(l−1), ∀l∈{2,...,L} (1) We employed a modified, simpler version of this method,\nh i h,v i\nutilizing only the attention weights rather than the layer’s\nwhere W(l),W(l),W(l) ∈ Rd×k. The attention weights relevance.\nh,q h,k h,v In our framework, latent tokens z(l) are represented as\nforeachtokenpair(i,j)atlayerlandheadharecomputed i\nvectors rather than direct class correspondences, introduc-\nas:\ning an additional layer of complexity. To manage this, an\n(cid:32) ⟨Q(l)(zl−1),K(l)(z(l−1))⟩(cid:33) appropriatetransformation,suchastheenergyoreuclidean\nα h(l ,) i,j =softmax j h i √ kh j . (2) normofthelatenttoken,isemployedasasurrogate. Con-\nsequently, we compute the gradient of ∥z(l)∥ with respect\ni\ntotheattentionweightstofacilitatetheanalysis.\nThen,i-thtokenisupdatedbysummingovertheweighted\nInthefollowingequations,wedefinetherelevancemap\nvaluesacrossallheads:\nS(l) ∈Rn,wherethej-thelementofthisvectorrepresents\ni\nH n theimportanceofthej-thinputtokentothetargetedlatent\nu¯( il) =(cid:88) (W c( ,l h))T (cid:88) α h(l ,) i,jV h(l)(z( jl−1)), (3) tokenz( il). Therelevancemapiscomputedas:\nh=1 j=1\nA(b,l) =I+E\n(cid:16)\n∇\n∥z(l)∥⊙A(b)(cid:17)+\n,\nwhereW ∈ Rk×d. Theupdatedtokenrepresentationu i h Ab h i h\nc,h i\n(l) (1,l) (2,l) (l−1,l)\naftertheattentionlayeriscomputedas: S =A ·A · ... ·A ,\ni i i i\nu(l) =LayerNorm(z(l−1)+u¯(l)). (4) S i(l) =S( il) [i,1:], (7)\ni i i\nwhere (·)+ means considering only positive values,\nEachtokenthenpassesthroughafeed-forwardnetwork: S, A(b,l) ∈ R(n+1)×(n+1),⊙denotestheHadamardprod-\ni\n¯z(l) =(W(l))TReLU((W(l))Tu ), (5) uct,E hrepresentsthemeanacrosstheheadsdimension,and\ni 2 1 i A(b) ∈Rh×(n+1)×(n+1)istheattentionscorematrixinthe\nz(l) =LayerNorm(u +¯z(l)). (6) h\ni i i b-thlayerandA(b) =α(b) .\nh,i,j h,i,j\nHere,W(l) ∈Rd×m,W(l) ∈Rm×d. Due to the skip connections in the transformer, most of\n1 2\n4\nthecontributionofS(l)isconcentratedonS(l)[i−1]which Class[x,y](l) = argmax S(l)[x,y]. (9)\ni i ζ c,ζ\nmakesithardtoanalyzeothertokens’contribution. Toad- c∈{1,...,k}\ndressthisissue,wereplacethiselementwiththemaximum Some examples illustrating our segmentation method is\nvalueofotherelements,therebycapturingthecontributions presentedinFigure3. Additionally, thehierarchytreeand\nofadditionaltokenstotheselectedtoken. theeffectofthethresholdaredemonstratedinFigure1.\nForvisiontasks,wefirstreshapetherelevancemapand\nthen upsample it using bilinear or cubic interpolation to\nmatch the resolution of the model’s input. The resulting\nhigher-dimensional matrix is denoted as S˜. This upsam-\nplingstepisessentialforenablingaccurateobjectselection\nandsemanticsegmentationtasks.\n3.3.ULTrainUnsupervisedTasks\nIn this section, we aim to examine ULTra’s capability to\nadapt to various tasks involving semantic knowledge. Im-\nportantly, it requires no additional training, leveraging the\ninherent understanding within transformers rather than re-\nlyingonlossfunctionsobjective,finallayeroutputs,orfine-\ntuning.\nUnsupervisedSemanticSegmentation.Aspreviouslydis-\ncussed, a relevance map can be defined for each latent to-\nken at a fixed layer, with the total number of relevance\nFigure3. ULTrasegmentationresultsonsampleimages. Thetop\nmaps equal to the number of latent tokens. In the context\nrowdisplaystheoriginalimages, themiddlerowshowsground-\nof segmentation, the goal is to assign a class label to each truth annotations, and the bottom row presents our model’s pre-\npixelwithinanimage. Toachievethis,weemploycluster- dictions.\ning techniques, such as hierarchical clustering, that do not\nrequire a predefined number of classes. These techniques 4.Experiment\ngroup the relevance maps into k distinct clusters, where k\nDatasets. In our experiments, we evaluate model perfor-\nisunknown. Ideally,weaimforktoapproximatetheactual\nmance on several semantic segmentation benchmarks, fo-\nnumberofclassespresentintheimage.\ncusing on vision-related tasks. We conducted experiments\nOur approach provides flexibility in adjusting the value\nonthreedatasets: COCO-Stuff27[4],PASCALVOC2012\nofkbymodifyingthecutoffdistancethresholdζ withinthe\n[15], and Potsdam-3 [19]. This combination of datasets\nclusteringalgorithm. Increasingζ producesfewer,broader\nprovidesadiversetestinggroundtorigorouslyevaluateour\nclustersthatcapturegeneralcategories,suchasbackground\nunsupervised zero-shot approach across both standard and\nandforeground.Conversely,reducingζallowsforfinerseg-\nchallengingperspectivesinsemanticsegmentation.\nmentation,distinguishingmorespecificfeatures,suchasan\nCOCO-Stuff27,asubsetoftheCOCOdataset[28],in-\nobject’sheadorhands. Topreventthemethodfromdispro-\ncludescomplex,real-worldsceneswithpixel-levelannota-\nportionately favoring larger objects, given that the number\ntionsacrossvariousobjectcategories. Similarly, PASCAL\nof elements in each cluster may vary, we apply min-max\nVOC 2012 serves as a classic benchmark with pixel-level\nscalingtoeachclusterindependently.\nannotations, while the Potsdam-3 dataset offers a unique\nAfterclustering,wedefinekdistinctconceptsbyaggre-\naerial, top-down perspective of urban scenes. The latter\ngating the relevance maps within each cluster. The aggre-\naddsanadditionalchallengewithitshigh-resolutionimages\ngated relevance map S˜(l)[x,y] for a cluster c is computed\nc ofbuildings,roads,andnaturalelementscapturedoverthe\nas:\nS˜(l)[x,y]= (cid:88) S˜(l)[x,y], (8) cityofPotsdam.\nc,ζ i ForourqualitativeanalysisofLLMinterpretationinthe\ni∈ϕζ(c) taskoftextsummarization,asdescribedinSection4.3,we\nwhereϕ (c) = {i | Class(S˜(l)) = c}representsthesetof utilized the TL;DR dataset [49]. The TL;DR dataset con-\nζ i tainssummarycomparisonswithhumanfeedbackcollected\nlabel assignments determined by the clustering algorithm.\nbyOpenAI.EachentryconsistsofaRedditpost,including\nForeachpixelatposition[x,y],theclasslabelisdetermined\nitstitle,originalcontent,andahuman-generatedTL;DR.\nbyidentifyingtheclustercwiththehighestrelevancevalue\nat that pixel. Mathematically, the class assignment for a Models. For all experiments in the vision tasks, we used\npixelisexpressedas: CLIP’simageencoderViT-B/32[37]. Forinterpretingtext\n5\nsummarization, as described in Section 4.3, we used the 4.2. Zero-shot Unsupervised Semantic Segmenta-\nLlama-2-7B language model [54]. All experiments were tion\nrunon8NVIDIARTX4090-24GBGPUs.\nWebenchmarkedthesegmentationcapabilityofourmethod\n4.1.Zero-shotUnsupervisedObjectSelection againstseveralapproachesintheliteratureonunsupervised\nsegmentation. Notably,unlikeothermethods,ourapproach\nTheupsampledrelevancemapS˜ i(l) canbeconvertedintoa requires no additional training. Instead, it relies solely\nbinary segmentation mask using a threshold τ, where the on a pre-trained vision transformer, which may have been\nbinarymaskM(l)isdefinedas:\ntrainedontasksunrelatedtounsupervisedsegmentation.\ni\n(cid:40) 0, ifS˜(l)[x,y]<τ, Toevaluateourmethod,weusedtheUnsupervisedmean\nM(l)[x,y]= i (10) IntersectionoverUnion(U.mIoU)andUnsupervisedpixel\ni 1, otherwise.\nAccuracy (U. Accuracy) metrics, following standard prac-\nHere, S˜(l)[x,y] represents the relevance value at position ticesinsemanticsegmentationresearch. Inallexperiments\ni\n[x,y]inS˜(l),withτ asthethreshold. WhenS˜(l)[x,y]isbe- across datasets, as shown in Tables 1, 2, and 3, we set the\ni i cutoffdistancethresholdζ =0.4.\nlowτ,M(l)[x,y]issetto0;otherwise,itissetto1,marking\ni\ntheobjectregion.\nTable 1. Comparison of unsupervised segmentation methods on\nOur findings indicate that as tokens propagate through\nthe 27-class COCO-Stuff validation set. ”W” indicates methods\nthe network, they refine their object representation while requiringadditionaltraining,while”W/O”denotesmethodswith-\nretaining the semantic meaning of their associated image outadditionaltraining.\npatches performing Object Selection. Figure 5 visually il-\nlustratesthisprocess. Foragivenpatchtokenx ,theobject Method Training U.Accuracy U.mIoU\ni\nitmoststronglyrepresentsisdenotedasclassk ,indicating IIC[21] W 21.8 6.7\ni\nthatx predominantlycorrespondstoaregionoftheobject DINO[5] W 30.5 9.6\ni\nbelonging to class k in the image. The latent token z(l) PiCIE[9] W 48.1 13.8\ni i\ngeneratesarelevancemapthathighlightsareaswithhigher ACSeg[27] W - 16.4\nvaluesassociatedwithclassk . Afterapplyingathreshold, STEGO[18] W 56.9 28.2\ni\nthismapbecomesabinarysegmentationmaskexpectedto DepthG[46] W 58.6 29.0\nexhibitahighIntersectionoverUnion(IoU)withthecorre- U2Seg[33] W 63.9 30.2\nspondingclassk regionintheimage.Anillustrativeexam- ULTra(Ours) W/O 67.2 32.9\ni\npleisshowninFigure2.\nToquantifyalignment,wecomputetheIoUbyconvert-\ning the relevance map S(l) into a binary mask M(l) and Table 2. Comparison of unsupervised segmentation methods on\ni i\ncomparing it with the ground-truth mask. We propose the thePASCALVOCvalidationset.\nInitialTokenIoU(ITIoU)metric,whichmeasureshowwell\nMethod Training U.mIoU\ntherelevancemapsofinputtokensalignwiththeirrespec-\nIIC[21] W 9.8\ntiveclassmasks. TheITIoUiscalculatedas:\nMaskContrast[56] W 35.0\nITIoU(l)(X)= C1 (cid:88)C |P1\n|\n(cid:88) IoU(M j(l),G i), (11) L Tre ao np sa Fr Gt[ U66 [] 62] W W 4 31 7. .7 2\ni\ni=1 xj∈Pi MaskDistill[57] W 42.0\nwhere C denotes the number of classes, P represents the ACSeg[27] W 47.1\ni\nsetoftokensassociatedwithclassi,M(l)isthebinaryseg- ULTra(Ours) W/O 51.9\nj\nmentation mask for token x within class i, and G is the\nj i\nground-truth mask for class i in image X. The inner sum\naverages the IoU for tokens in P for each class, and the Table 3. Comparison of unsupervised segmentation methods on\ni\noutersumthenaveragesacrossallclasses. Usingathresh- thePotsdamvalidationset.\nold of 0.2, our ITIoU metric achieves an average score of\nMethod Training U.ACC\n37.84 % on the COCO validation dataset and 39.51 % on\nIIC[21] W 65.1\ntheVOCdataset.\nDINO[5] W 71.3\nThe ITIoU metric provides a comprehensive evaluation\nSTEGO[18] W 77.0\nby incorporating a weighted average across tokens in each\nDepthG[46] W 80.4\nclass, enhancing the assessment of token alignment with\nULTra(Ours) W/O 74.6\ntheirrespectiveground-truthlabels.\n6\nHow do I [ 2 0 M ] stop feeling bad about myself for having no relationship I need help about those feelings POST : I am a 1 8 M , she ' s a 1 7 F . We '\nexperience at all ? POST : It just seems like everyone I know has at least ve got a troubles ome relationship which started as a pure friendship one\nhad a \" thing \" with someone by this point . I ' ve made out with a girl once year ago . I ' ve made mist akers , she made hers too . O ur last situation\n( who later told me that was a mistake ) and I feel like girls always reject scenario is explained in here : Now I feel like I hate her , I used to adm ire\nme or only see me as a friend . Which is perfectly acceptable , but I ' m her a lot , but I ' m really disappoint ed with her and with her character .\nstarting to get ups et that I ' ve never had any kind of relationship . I just But I just realized I still like her . So , well , yeah , I like her and hate her .\ngot rejected by a girl who I thought was into me and I ' ve been feeling And just after that bad situation happened I realized she also had that\nbad ever since . I just don ' t know what ' s wrong with me . I guess I ' m a feeling . Well , now we both hate and love each other . What to do ? What\nlittle bit skin ny ( I work out regularly though ), but I show er every day , to think ? What to feel ? add itional info : today our friend asked me for\ndress pretty well , all that stuff . help with some calculations and I made a jo ke about our physics teacher .\nShe laughed and smiled at me just like one year ago , but after she\nrealized that , she seemed kind a [ gr ouch y ](\n(a)TL;DR:I’vehadverybadluckwithgirlsmywhole (b)TL;DR:Istilllikeherbutmyrational\nlifeandIdon’tknowhowtogetmyconfidenceup. sidesays”no,sheisatrashperson”.\nFigure4.VisualizationofTokenContributionScores(λ(l))highlightingtherelevanceofcontexttokensininterpretingthesummary.Each\ni\ntokeniscoloredproportionallytoitsλ(l)value. Thesevisualizationsdemonstratethemodel’sabilitytoidentifykeysemanticelementsin\ni\nthecontextforgeneratingrelevantsummaries.\nOurresultsinTables1, 2, and3highlighttheeffective- toproducingthesummaryy.Accordingly,λ(l)iscomputed\ni\nnessoftheproposedmethodforunsupervisedsemanticseg- as:\nmentation. Unlike baseline approaches, which require ad-\n|y|\nditional training specifically tailored to segmentation, our λ(l) = 1 (cid:88) S(l) [i], ∀i∈{1,··· ,|x|}, (12)\nmethod leverages a pre-trained vision transformer without i |y| j+|x|\nj=1\nany further fine-tuning. Despite this lack of task-specific\ntraining,ourapproachconsistentlyoutperformsothermeth- where|·|denotesthenumberoftokensinthetext.\nods,demonstratingadaptabilityacrossdiversedatasets. In example (a): semantically significant words such\nas ‘relationship’, ‘experience’, ‘rejection’, and ‘never’ are\n4.3.InterpretingLLMsinTextSummarization\nprominently highlighted, reflecting the model’s interpreta-\nInthissection,weexaminehowourinterpretabilityframe- tionoftheperson’sstruggleswithrelationshipsandfeelings\nworkcanbeappliedtotextsummarizationtaskstouncover of rejection. Additionally, the highlighting of the question\ntheunderlyingmechanismsandintentofLLMs. Byvisual- atthebeginningofthecontext‘HowdoIstopfeelingbad...’\nizingtheregionsoftheinputcontextthatanLLMprioritizes suggests the model recognizes the presence of uncertainty\nwhileinterpretingagivenTL;DRsummary,wegaindeeper and a request for guidance, which is encapsulated in the\ninsightsintothemodel’sbehavioranddecision-makingpro- summaryas‘Idon’tknow.’\ncesses. AsshowninFigure4,thesevisualizationsallowus Inexample(b): λ(l) scoresrevealsthemodel’sfocuson\ni\nto pinpoint the most influential regions of a textual input words such as ‘feelings’, ‘hate’, ‘disappoint’, ‘love’, and\npromptingeneratingconciseandrelevantsummaries. ‘like’,whichcorrespondtotheperson’smixedemotionsto-\nFor the experiments, we used a Supervised Fine-Tuned wardtheirgirlfriend,asdescribedinthesummary. Theap-\n(SFT)versionofLlama-2-7BtrainedontheUltraFeedback parent contradiction between ‘love’ and ‘trashness’ in the\nBinarized(UFB)dataset[12]. Additionally,wealignedthe summaryseemstobederivedfromthesehighlightedterms,\nmodeltothetextsummarizationtaskontheTL;DRdataset suggestingthemodelunderstandstheconflictingemotions\nusing the Direct Preference Optimization (DPO) method present in the text. Furthermore, the emphasis on ‘charac-\n[38] for 1,000 iterations, with a learning rate of 5×10−6 ter’alignswiththesummary’sjudgmentaltone,suggesting\nand β = 0.5. To validate our framework, we selected the that the model interprets this word as indicative of an as-\npreferred response(TL;DR)ofeachsampleinthedataset, sessmentofpersonalitytraitsandbehaviors.\ndenoteditasy,anduseditasthesummaryofthecontextx. This token-level analysis can serve as a valuable tool\nIn this task, we concatenate the context x and the sum- foreffectivesupervisioninfutureresearch,particularlyfor\nmary y with a separator token. After feeding this input to developing interpretable fine-tuning and alignment meth-\nthemodel, wecomputetherelevancescoresoftheTL;DR ods like Reinforcement Learning from Human Feedback\ntokenswithrespecttothecontexttokens. Wethenaverage (RLHF) [10, 35, 49] and Preference Optimization [3, 14,\nthese scores for each token in x to obtain a scalar value, 38], where understanding model behavior and intent is es-\nreferred to as the Token Contribution Score, λ(l) ∈ R+, sential. Itmayalsoprovideusefulinsightsinrelatedareas,\ni\nwhich highlights the contribution of each context token in suchasChain-of-Thought[8,24,43]andTheory-of-Mind\ninterpretingthesummaryywithrespecttothecontext.This [23, 60, 61], which seek to make the reasoning processes\nprovides visual evidence of how the model processes the and intentions of LLMs more transparent. Ultimately, our\ncontext text and identifies key semantic elements relevant framework offers insights into fundamental questions in\n7\n(a)OriginalImage (b)ExplanationRelevencyMap\nFigure5. Anexampleillustratingthemodel’sdecision-makingprocessacrosslayers. Movingfromlefttorightcorrespondstodeeper\nlayersinthenetwork.ThefirstrowcorrespondstotheCLStoken,whilethesecond,third,andfourthrowsrepresentthreedifferenttokens,\nhighlightedbyredsquares.\nprovidinginsightsintothecontributionofeachlayer.Asan-\nAccuracy\n65 MeanIoU 32 ticipatedandobservedinFigure6,deeperlayersgenerally\ncarry more semantic significance. However, the contribu- 60 28\ntion diminishes in the final layers, suggesting that a depth\n55 24 of around 13 layers might be more than sufficient for the\n50 20 ViT to effectively comprehend image content. This find-\ningimpliesthatevenfewerlayersmightachievecompara-\n45 16\nble results, potentially reducing computational costs with-\n2 4 6 Layer8 10 12 outcompromisingperformance.\n(a)COCO-Stuff We observe an intriguing behavior in the initial lay-\ners,whereperformanceinitiallydeclinesbeforeimproving.\n75 Accuracy 51 ThisphenomenonisalsovisuallyevidentinFigure5,where\nMeanIoU\n72 48 the attention maps in the first layer appear to focus on the\n69 45 entire image. This suggests that, initially, the token exam-\nines the image as a whole before selectively gathering in-\n42\n66\nformationfromtokenswithsimilarcharacteristics.\n39\n63\n36\n5.Conclusion\n60 33\n2 4 6 8 10 12\nLayer\nThis paper introduces a novel interpretability framework\n(b)PASCALVOC that provides valuable insights into the decision-making\nFigure6. Ablationstudyontwoevaluationmetricsacrosslayers. processesofTransformermodels. Theframeworkisbased\nThese plots demonstrate a progressive improvement in semantic ontheinputinterpretationprovidedbyeachtokeninanar-\nsegmentationperformanceinthedeeperlayersofthetransformer\nbitrarylayer. Byaggregatingtheinterpretationofalltokens\nmodel. Thisenhancementisattributedtolatenttokenscapturing\nin the same layer, the framework enhances understanding\nmoremeaningfulsegmentstructures,resultinginincreasinglyac-\nof the Transformer’s behavior at the layer level. The rich-\ncurateandrefinedsemanticrepresentations.\nness of this understanding enables zero-shot unsupervised\nsemantic segmentation, where our method achieves state-\nof-the-art performance with an accuracy of 67.2% and an\nNLP,suchas:”Howisthemodelthinking?”or”Whatisthe\nmIoU of 32.9% on the COCO-Stuff dataset, and an mIoU\nunderlyingintentbehindthemodel’sgeneratedresponse?”.\nof51.9%onthePASCALVOCdatasetfortheunsupervised\nzero-shot version. Beyond vision tasks, the framework is\n4.4.AblationStudyontheEffectofLayerDepthin\nnotlimitedtovisionmodelsandcanalsobeappliedtoin-\nViTTokenUnderstanding\nterpret the behavior of large language models (LLMs) in\nIn this section, we analyze the impact of depth on our tasks such as text summarization. This highlights the ver-\nmodel’s interpretability and segmentation performance, satility and broad applicability of the proposed framework\n8\n)%(ycaruccA.U\n)%(ycaruccA.U\n)%(UoInaeM.U\n)%(UoInaeM.U\nacross various domains, offering potential for future work MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-\ntosimultaneouslydemystifythemultimodalunderstanding vain Gelly, et al. An image is worth 16x16 words: Trans-\nofTransformers. formers for image recognition at scale. arXiv preprint\narXiv:2010.11929,2020. 1\nReferences [14] KawinEthayarajh,WinnieXu,NiklasMuennighoff,DanJu-\nrafsky,andDouweKiela. Kto:Modelalignmentasprospect\n[1] Samira Abnar and Willem Zuidema. Quantifying atten- theoreticoptimization,2024. 7\ntionflowintransformers. arXivpreprintarXiv:2005.00928, [15] MarkEveringhamandJohnWinn. Thepascalvisualobject\n2020. 1,2,4 classeschallenge2011(voc2011)developmentkit. Pattern\n[2] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Analysis,StatisticalModellingandComputationalLearning,\nChris Bamford, Devendra Singh Chaplot, Diego de las Tech.Rep,8,2011. 5\nCasas,FlorianBressand,GiannaLengyel,GuillaumeLam- [16] LeiGaoandLingGuan.Interpretabilityofmachinelearning:\nple, Lucile Saulnier, Le´lio Renard Lavaud, Marie-Anne Recentadvancesandfutureprospects.IEEEMultiMedia,30\nLachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, (4):105–118,2023. 2\nThomas Wang, Timothe´e Lacroix, and William El Sayed. [17] Shanghua Gao, Zhong-Yu Li, Ming-Hsuan Yang, Ming-\nMistral7B,2023. arXiv:2310.06825[cs]. 1 MingCheng,JunweiHan,andPhilipTorr. Large-scaleun-\n[3] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, supervised semantic segmentation. IEEE transactions on\nDanielGuo,DanieleCalandriello,MichalValko,andRe´mi patternanalysisandmachineintelligence,45(6):7457–7476,\nMunos. Ageneraltheoreticalparadigmtounderstandlearn- 2022. 3\ningfromhumanpreferences,2023. 7 [18] MarkHamilton,ZhoutongZhang,BharathHariharan,Noah\n[4] HolgerCaesar,JasperUijlings,andVittorioFerrari. Coco- Snavely, and William T Freeman. Unsupervised semantic\nstuff:Thingandstuffclassesincontext. InComputervision segmentation by distilling feature correspondences. arXiv\nandpatternrecognition(CVPR),2018IEEEconferenceon. preprintarXiv:2203.08414,2022. 2,3,6\nIEEE,2018. 5 [19] ISPRS. 2d semantic labeling contest - potsdam. https:\n[5] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou, //www.isprs.org/education/benchmarks/\nJulienMairal,PiotrBojanowski,andArmandJoulin.Emerg- UrbanSemLab/2d-sem-label-potsdam.aspx,\ningpropertiesinself-supervisedvisiontransformers.InPro- 2018. 5\nceedingsoftheIEEE/CVFinternationalconferenceoncom- [20] SarthakJainandByronCWallace. Attentionisnotexplana-\nputervision,pages9650–9660,2021. 3,6 tion. arXivpreprintarXiv:1902.10186,2019. 2\n[6] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter- [21] XuJi,JoaoFHenriques,andAndreaVedaldi. Invariantin-\npretability beyond attention visualization. Proceedings of formation clustering for unsupervised image classification\ntheIEEE/CVFConferenceonComputerVisionandPattern and segmentation. In Proceedings of the IEEE/CVF inter-\nRecognition,pages782–791,2021. 1,2,4 nationalconferenceoncomputervision, pages9865–9874,\n[7] HaozheChen, JunfengYang, CarlVondrick, andChengzhi 2019. 3,6\nMao. Invite: Interpret and control vision-language models [22] SangwonKim,JaeyealNam,andByoungChulKo. Vit-net:\nwithtextexplanations. InTheTwelfthInternationalConfer- Interpretablevisiontransformerswithneuraltreedecoder.In\nenceonLearningRepresentations,2024. 1,3 Internationalconferenceonmachinelearning,pages11162–\n[8] ZhuangChen,JincenziWu,JinfengZhou,BosiWen,Guan- 11172.PMLR,2022. 3\nqunBi,GongyaoJiang,YaruCao,MengtingHu,Yunghwei [23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\nLai,ZexuanXiong,andMinlieHuang. Tombench: Bench- Matsuo, and Yusuke Iwasawa. Large language models are\nmarkingtheoryofmindinlargelanguagemodels,2024. 7 zero-shotreasoners,2023. 7\n[9] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath [24] MichalKosinski. Evaluatinglargelanguagemodelsinthe-\nHariharan. Picie: Unsupervisedsemanticsegmentationus- oryofmindtasks. ProceedingsoftheNationalAcademyof\ninginvarianceandequivarianceinclustering.InProceedings Sciences,121(45),2024. 7\noftheIEEE/CVFConferenceonComputerVisionandPat- [25] Mengcheng Lan, Xinjiang Wang, Yiping Ke, Jiaxing Xu,\nternRecognition,pages16794–16804,2021. 3,6 LitongFeng,andWayneZhang.Smooseg:smoothnessprior\n[10] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, forunsupervisedsemanticsegmentation. AdvancesinNeu-\nShaneLegg,andDarioAmodei. Deepreinforcementlearn- ralInformationProcessingSystems,36:11353–11373,2023.\ningfromhumanpreferences,2023. 7 3\n[11] KevinClark. Whatdoesbertlookat? ananalysisofbert’s [26] Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei,\nattention. arXivpreprintarXiv:1906.04341,2019. 3 Jinfeng Yi, and Bowen Zhou. Trustworthy ai: From prin-\n[12] GanquCui,LifanYuan,NingDing,GuanmingYao,Bingx- ciples to practices. ACM Computing Surveys, 55(9):1–46,\niang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, 2023. 2\nYankaiLin,ZhiyuanLiu,andMaosongSun. Ultrafeedback: [27] Kehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, Yian\nBoostinglanguagemodelswithscaledaifeedback,2024. 7 Zhao,GuoliSong,ChangLiu,LiYuan,andJieChen.Acseg:\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Adaptive conceptualization for unsupervised semantic seg-\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, mentation. InProceedingsoftheIEEE/CVFconferenceon\n9\ncomputervisionandpatternrecognition,pages7162–7172, tionalconferenceonknowledgediscoveryanddatamining,\n2023. 2,3,6 pages1135–1144,2016. 2\n[28] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, [41] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.\nPietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence Anchors: High-precision model-agnostic explanations. In\nZitnick. Microsoft coco: Common objects in context. In Proceedings of the AAAI conference on artificial intelli-\nComputerVision–ECCV2014: 13thEuropeanConference, gence,2018. 2\nZurich, Switzerland, September 6-12, 2014, Proceedings, [42] MattiaRigotti,ChristophMiksovic,IoanaGiurgiu,Thomas\nPartV13,pages740–755.Springer,2014. 5 Gschwind,andPaoloScotton.Attention-basedinterpretabil-\n[29] DavidLindner,Ja´nosKrama´r,SebastianFarquhar,Matthew ity with concept transformers. In International conference\nRahtz, TomMcGrath, andVladimirMikulik. Tracr: Com- onlearningrepresentations,2021. 3\npiled transformers as a laboratory for interpretability. Ad- [43] MelanieSclar,SachinKumar,PeterWest,AlaneSuhr,Yejin\nvancesinNeuralInformationProcessingSystems,36,2024. Choi,andYuliaTsvetkov. Mindinglanguagemodels’(lack\n3 of) theory of mind: A plug-and-play multi-character belief\n[30] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng tracker,2023. 7\nZhang, Stephen Lin, and Baining Guo. Swin transformer: [44] RamprasaathRSelvaraju,MichaelCogswell,AbhishekDas,\nHierarchical vision transformer using shifted windows. In Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nProceedings of the IEEE/CVF international conference on Grad-cam: Visual explanations from deep networks via\ncomputervision,pages10012–10022,2021. 1 gradient-based localization. Proceedings of the IEEE in-\nternationalconferenceoncomputervision,pages618–626,\n[31] Scott Lundberg. A unified approach to interpreting model\npredictions. arXivpreprintarXiv:1705.07874,2017. 2 2017. 1,2\n[45] HyunSeokSeong,WonJunMoon,SuBeenLee,andJae-Pil\n[32] PaulMichel,OmerLevy,andGrahamNeubig. Aresixteen\nHeo.Leveraginghiddenpositivesforunsupervisedsemantic\nheadsreallybetterthanone? Advancesinneuralinformation\nsegmentation. InProceedingsoftheIEEE/CVFconference\nprocessingsystems,32,2019. 3\non computer vision and pattern recognition, pages 19540–\n[33] DantongNiu,XudongWang,XinyangHan,LongLian,Roei\n19549,2023. 3\nHerzig, and Trevor Darrell. Unsupervised universal image\n[46] Leon Sick, Dominik Engel, Pedro Hermosilla, and Timo\nsegmentation. InProceedingsoftheIEEE/CVFConference\nRopinski. Unsupervised semantic segmentation through\nonComputerVisionandPatternRecognition,pages22744–\ndepth-guidedfeaturecorrelationandsampling. InProceed-\n22754,2024. 6\ningsoftheIEEE/CVFConferenceonComputerVisionand\n[34] ChrisOlah, AlexanderMordvintsev, andLudwigSchubert.\nPatternRecognition,pages3637–3646,2024. 2,3,6\nFeaturevisualization. Distill,2(11):e7,2017. 2\n[47] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.\n[35] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nDeep inside convolutional networks: Visualising image\nrollL.Wainwright,PamelaMishkin,ChongZhang,Sandhini\nclassification models and saliency maps. arXiv preprint\nAgarwal,KatarinaSlama,AlexRay,JohnSchulman,Jacob\narXiv:1312.6034,2014. 1,2\nHilton,FraserKelton,LukeMiller,MaddieSimens,Amanda\n[48] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas\nAskell,PeterWelinder,PaulChristiano,JanLeike,andRyan\nBrox, and Martin Riedmiller. Striving for simplicity: The\nLowe. Traininglanguagemodelstofollowinstructionswith\nallconvolutionalnet.arXivpreprintarXiv:1412.6806,2014.\nhumanfeedback,2022. 7\n2\n[36] Yao Qiang, Chengyin Li, Prashant Khanduri, and Dongx-\n[49] NisanStiennon,LongOuyang,JeffWu,DanielM.Ziegler,\niao Zhu. Interpretability-aware vision transformer. arXiv\nRyanLowe,ChelseaVoss,AlecRadford,DarioAmodei,and\npreprintarXiv:2309.08035,2023. 3\nPaulChristiano. Learningtosummarizefromhumanfeed-\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya back,2022. 5,7\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n[50] MukundSundararajan,AnkurTaly,andQiqiYan.Axiomatic\nAmandaAskell,PamelaMishkin,JackClark,etal.Learning attributionfordeepnetworks.InInternationalconferenceon\ntransferable visual models from natural language supervi- machinelearning,pages3319–3328.PMLR,2017. 2\nsion.InInternationalconferenceonmachinelearning,pages\n[51] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert\n8748–8763.PMLR,2021. 1,3,5\nDadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\n[38] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Er- Morgane Rivie`re, Mihir Sanjay Kale, Juliette Love, Pouya\nmon, Christopher D. Manning, and Chelsea Finn. Direct Tafti, Le´onard Hussenot, Pier Giuseppe Sessa, Aakanksha\npreferenceoptimization: Yourlanguagemodelissecretlya Chowdhery,AdamRoberts,AdityaBarua,AlexBotev,Alex\nrewardmodel,2024. 7 Castro-Ros, Ambrose Slone, Ame´lie He´liou, Andrea Tac-\n[39] Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and chetti,AnnaBulanova,AntoniaPaterson,BethTsai,Bobak\nZiyu Yao. A practical review of mechanistic interpretabil- Shahriari,CharlineLeLan,ChristopherA.Choquette-Choo,\nity for transformer-based language models. arXiv preprint Cle´ment Crepy, Daniel Cer, Daphne Ippolito, David Reid,\narXiv:2407.02646,2024. 3 ElenaBuchatskaya,EricNi,EricNoland,GengYan,George\n[40] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Tucker,George-ChristianMuraru,GrigoryRozhdestvenskiy,\n”whyshoulditrustyou?”explainingthepredictionsofany Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Ja-\nclassifier.InProceedingsofthe22ndACMSIGKDDinterna- cobAustin,JamesKeeling,JaneLabanowski,Jean-Baptiste\n10\nLespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Jo- for unsupervised semantic segmentation. arXiv preprint\nhan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, arXiv:2206.06363,2022. 6\nKathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, [58] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-\nLucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia\nMichael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Polosukhin. Attentionisallyouneed. AdvancesinNeural\nBachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, InformationProcessingSystems(NeurIPS),2017. 1\nPaulMichel,PetkoYotov,RahmaChaabouni,RamonaCo- [59] JesseVigandYonatanBelinkov. Analyzingthestructureof\nmanescu,ReenaJana,RohanAnil,RossMcIlroy,RuiboLiu, attention in a transformer language model. arXiv preprint\nRyan Mullins, Samuel L Smith, Sebastian Borgeaud, Ser- arXiv:1906.04284,2019. 1,2\ntanGirgin,SholtoDouglas,ShreePandya,SiamakShakeri, [60] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nSoham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Bosma,BrianIchter,FeiXia,EdChi,QuocLe,andDenny\nWojciechStokowiec,YuhuiChen,ZafaraliAhmed,Zhitao Zhou. Chain-of-thoughtpromptingelicitsreasoninginlarge\nGong,TrisWarkentin,LudovicPeran,MinhGiang,Cle´ment\nlanguagemodels,2023. 7\nFarabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu,\n[61] Yu Xia, Rui Wang, Xu Liu, Mingyan Li, Tong Yu, Xiang\nDemis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle\nChen, Julian McAuley, and Shuai Li. Beyond chain-of-\nBarral,FernandoPereira,EliCollins,ArmandJoulin,Noah\nthought: A survey of chain-of-x paradigms for llms, 2024.\nFiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy.\n7\nGemma: Openmodelsbasedongeminiresearchandtech-\n[62] ZhaoyuanYin,PichaoWang,FanWang,XianzheXu,Han-\nnology,2024. 1\nlingZhang,HaoLi,andRongJin. Transfgu:atop-downap-\n[52] Hans Thisanke, Chamli Deshan, Kavindu Chamith, Sa-\nproachtofine-grainedunsupervisedsemanticsegmentation.\nchith Seneviratne, Rajith Vidanaarachchi, and Damayanthi\nIn European conference on computer vision, pages 73–89.\nHerath. Semantic segmentation using vision transformers:\nSpringer,2022. 3,6\nAsurvey.EngineeringApplicationsofArtificialIntelligence,\n[63] Matthew D Zeiler and Rob Fergus. Visualizing and\n126:106669,2023. 1\nunderstanding convolutional networks. arXiv preprint\n[53] Bhavani Thuraisingham. Trustworthy machine learning.\narXiv:1311.2901,2014. 1,2\nIEEEIntelligentSystems,37(1):21–24,2022. 2\n[64] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Ser-\n[54] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- can O¨ Arik, and Tomas Pfister. Nested hierarchical trans-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nformer:Towardsaccurate,data-efficientandinterpretablevi-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nsualunderstanding. InProceedingsoftheAAAIConference\nale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,\nonArtificialIntelligence,pages3417–3425,2022. 3\nMoya Chen, Guillem Cucurull, David Esiobu, Jude Fer-\n[65] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nnandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia\nandAntonioTorralba.Learningdeepfeaturesfordiscrimina-\nGao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,\ntivelocalization. InProceedingsoftheIEEEconferenceon\nSagharHosseini,RuiHou,HakanInan,MarcinKardas,Vik-\ncomputervisionandpatternrecognition,pages2921–2929,\ntor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-\n2016. 2\nrenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\n[66] AdrianZieglerandYukiMAsano. Self-supervisedlearning\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nofobjectpartsforsemanticsegmentation. InProceedingsof\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\ntheIEEE/CVFConferenceonComputerVisionandPattern\nIgorMolybog,YixinNie,AndrewPoulton,JeremyReizen-\nRecognition,pages14502–14511,2022. 3,6\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang,AurelienRodriguez,RobertStojnic,SergeyEdunov,\nandThomasScialom. Llama2: Openfoundationandfine-\ntunedchatmodels,2023. 1,6\n[55] Svenja Uhlemeyer, Matthias Rottmann, and Hanno\nGottschalk. Towards unsupervised open world semantic\nsegmentation.InUncertaintyinArtificialIntelligence,pages\n1981–1991.PMLR,2022. 3\n[56] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\nGeorgoulis,andLucVanGool. Unsupervisedsemanticseg-\nmentationbycontrastingobjectmaskproposals.InProceed-\nings of the IEEE/CVF International Conference on Com-\nputerVision,pages10052–10062,2021. 3,6\n[57] Wouter Van Gansbeke, Simon Vandenhende, and Luc\nVan Gool. Discovering object masks with transformers\n11",
    "pdf_filename": "ULTra_Unveiling_Latent_Token_Interpretability_in_Transformer_Based_Understanding.pdf"
}