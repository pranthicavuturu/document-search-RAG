{
    "title": "ULTra Unveiling Latent Token Interpretability in Transformer Based Understanding",
    "context": "Transformers have revolutionized Computer Vision (CV) and Natural Language Processing (NLP) through self- attention mechanisms. However, due to their complexity, their latent token representations are often difficult to inter- pret. We introduce a novel framework that interprets Trans- former embeddings, uncovering meaningful semantic pat- terns within them. Based on this framework, we demon- strate that zero-shot unsupervised semantic segmentation can be performed effectively without any fine-tuning using a model pre-trained for tasks other than segmentation. Our method reveals the inherent capacity of Transformer mod- els for understanding input semantics and achieves state-of- the-art performance in semantic segmentation, outperform- ing traditional segmentation models. Specifically, our ap- proach achieves an accuracy of 67.2 % and an mIoU of 32.9 % on the COCO-Stuff dataset, as well as an mIoU of 51.9 % on the PASCAL VOC dataset. Additionally, we validate our interpretability framework on LLMs for text summarization, demonstrating its broad applicability and robustness. In recent years, the Transformer architecture and founda- tion models, leveraging self-attention mechanisms to cap- ture complex dependencies in text, have transformed Nat- ural Language Processing (NLP) benchmarks [2, 51, 54, 58]. Similarly, Vision Transformers (ViTs) [13] have been adapted in Computer Vision (CV) and now serve as the backbone for various tasks such as segmentation and ob- ject detection [30, 52]. Despite their success, understanding the interpretability of Transformers remains a challenge due to the complexity of their latent token representations. Several methods have been developed to enhance the in- terpretability of CNN-based models [44, 47, 63]. While some of these can be extended to Transformer architec- tures, they do not fully leverage the unique attention mech- ζ = 0.5 ζ = 0.35 ζ = 0.2 Segmentation Results Original Image Figure 1. Hierarchical clustering tree showing the grouping of to- ken relevance maps for all tokens in a latent layer of the Vision Transformer, not limited to the CLS token. Each leaf node repre- sents a single token relevance map, while higher-level nodes show aggregated clusters based on a clustering threshold (ζ), which controls the level of detail. Lower ζ values reveal finer details, while higher values create broader, more general clusters. This approach demonstrates how pre-trained Vision Transformers can perform unsupervised semantic segmentation, identifying mean- ingful patterns within token representations without requiring ad- ditional training or fine-tuning. anisms inherent to Transformers. Recent research has in- troduced interpretability methods specifically designed for Transformers [1, 6, 59]. However, these approaches primar- ily focus on explaining final model outputs, providing lim- ited insight into the intermediate processes that lead to pre- dictions. For instance, [7] maps latent tokens into CLIP’s [37] multi-modal space to find corresponding text descrip- tions, relying on an external text encoder for interpretabil- ity. In contrast, our approach directly interprets the latent space of ViTs, elucidating the role and function of each to- ken within the high-dimensional space without relying on external models. This paper introduces a framework to interpret latent to- 1 arXiv:2411.12589v1  [cs.CV]  15 Nov 2024",
    "body": "ULTra: Unveiling Latent Token Interpretability in Transformer-Based\nUnderstanding\nHesam Hosseini, Ghazal Hosseini Mighan, Amirabbas Afzali\n{hesam8hosseini, ghazaldesu, amir8afzali}@gmail.com\nSajjad Amini,\nsamini@umass.edu, s amini@sharif.edu\nAmir Houmansadr\namir@cs.umass.edu\nAbstract\nTransformers have revolutionized Computer Vision (CV)\nand Natural Language Processing (NLP) through self-\nattention mechanisms. However, due to their complexity,\ntheir latent token representations are often difficult to inter-\npret. We introduce a novel framework that interprets Trans-\nformer embeddings, uncovering meaningful semantic pat-\nterns within them. Based on this framework, we demon-\nstrate that zero-shot unsupervised semantic segmentation\ncan be performed effectively without any fine-tuning using\na model pre-trained for tasks other than segmentation. Our\nmethod reveals the inherent capacity of Transformer mod-\nels for understanding input semantics and achieves state-of-\nthe-art performance in semantic segmentation, outperform-\ning traditional segmentation models. Specifically, our ap-\nproach achieves an accuracy of 67.2 % and an mIoU of 32.9\n% on the COCO-Stuff dataset, as well as an mIoU of 51.9 %\non the PASCAL VOC dataset. Additionally, we validate our\ninterpretability framework on LLMs for text summarization,\ndemonstrating its broad applicability and robustness.\n1. Introduction\nIn recent years, the Transformer architecture and founda-\ntion models, leveraging self-attention mechanisms to cap-\nture complex dependencies in text, have transformed Nat-\nural Language Processing (NLP) benchmarks [2, 51, 54,\n58]. Similarly, Vision Transformers (ViTs) [13] have been\nadapted in Computer Vision (CV) and now serve as the\nbackbone for various tasks such as segmentation and ob-\nject detection [30, 52]. Despite their success, understanding\nthe interpretability of Transformers remains a challenge due\nto the complexity of their latent token representations.\nSeveral methods have been developed to enhance the in-\nterpretability of CNN-based models [44, 47, 63]. While\nsome of these can be extended to Transformer architec-\ntures, they do not fully leverage the unique attention mech-\nζ = 0.5\nζ = 0.35\nζ = 0.2\nSegmentation Results\nOriginal Image\nFigure 1. Hierarchical clustering tree showing the grouping of to-\nken relevance maps for all tokens in a latent layer of the Vision\nTransformer, not limited to the CLS token. Each leaf node repre-\nsents a single token relevance map, while higher-level nodes show\naggregated clusters based on a clustering threshold (ζ), which\ncontrols the level of detail. Lower ζ values reveal finer details,\nwhile higher values create broader, more general clusters. This\napproach demonstrates how pre-trained Vision Transformers can\nperform unsupervised semantic segmentation, identifying mean-\ningful patterns within token representations without requiring ad-\nditional training or fine-tuning.\nanisms inherent to Transformers. Recent research has in-\ntroduced interpretability methods specifically designed for\nTransformers [1, 6, 59]. However, these approaches primar-\nily focus on explaining final model outputs, providing lim-\nited insight into the intermediate processes that lead to pre-\ndictions. For instance, [7] maps latent tokens into CLIP’s\n[37] multi-modal space to find corresponding text descrip-\ntions, relying on an external text encoder for interpretabil-\nity. In contrast, our approach directly interprets the latent\nspace of ViTs, elucidating the role and function of each to-\nken within the high-dimensional space without relying on\nexternal models.\nThis paper introduces a framework to interpret latent to-\n1\narXiv:2411.12589v1  [cs.CV]  15 Nov 2024\n\nkens, offering a deeper understanding of the internal work-\nings of Transformers.\nThis understanding enables users\nto perform image semantic segmentation using pre-trained\nTransformer-based vision models in an unsupervised, zero-\nshot manner, without any additional training. We demon-\nstrate that applying semantic segmentation based on our\ninterpretability framework achieves state-of-the-art perfor-\nmance on benchmark image segmentation datasets.\nDrawing inspiration from [6], our method analyzes the\nsemantic information retained by latent tokens, enabling\ntasks such as object selection and semantic segmentation\nwithout additional training.\nWe demonstrate that Trans-\nformers inherently understand the semantic structure of\ntheir input, viewing it as a collection of distinct concepts.\nEach latent token identifies a specific concept with semantic\nsignificance, thereby shedding light on the decision-making\nprocess of these models.\nAs shown in Section 4, our framework proves effec-\ntive in a range of tasks, including semantic segmentation\nand model interpretation.\nMost recent unsupervised se-\nmantic segmentation methods involve an additional train-\ning phase to learn feature representations [18, 27, 46].\nOur approach, however, utilizes the understanding embed-\nded in pre-trained models to achieve zero-shot segmenta-\ntion, leveraging their inherent knowledge of images. The\nstronger a model’s comprehension of image content, the\nmore accurately it performs segmentation.\nWe further\ndemonstrate that our method is capable of interpreting large\nlanguage models (LLMs) at the token level, validating its\napplication in tasks such as text summarization. The main\ncontributions of this paper are as follows:\n• We propose a framework for interpreting latent tokens\nin Transformers, providing valuable insights into model\ndecision-making processes.\n• By aggregating relevance maps generated by tokens us-\ning hierarchical clustering, we achieve zero-shot unsuper-\nvised semantic segmentation on pre-trained models, out-\nperforming SOTA methods that require additional train-\ning.\n• We demonstrate the capability of our method to interpret\nLLMs at the token level, showcasing its practical appli-\ncability to textual data. Specifically, we demonstrate the\ninterpretation of LLM operations in the text summariza-\ntion task.\nThis paper is structured as follows: Section 2 reviews\ninterpretability frameworks and previous work on seman-\ntic segmentation, highlighting our SOTA results. Section 3\npresents our interpretability framework. In Section 4, we\nshowcase its effectiveness on image and text tasks. Finally,\nSection 5 concludes the paper.\n2. Related Work\nThe interpretability of deep learning architectures has be-\ncome a central focus in AI research [16]. As models grow in\ncomplexity, understanding their decision-making processes\nis essential for ensuring transparency, reliability, and fair-\nness [53]. Interpretability not only aids in debugging and\nperformance improvement but also builds trust in AI sys-\ntems, particularly in fields like healthcare, finance, and au-\ntonomous driving. Opaque models can perpetuate biases\nand generate unforeseen outcomes, making interpretability\ncrucial for bridging high performance with safe, practical\nAI deployment [26]. In Transformer models, tokens are key\nto interpreting behavior, with their relationship to spatial lo-\ncations in images or sequence order adding an important\nlayer to interpretation. This relationship enhances semantic\nsegmentation as a measure for evaluating token-based inter-\npretation frameworks, which this section will explore.\n2.1. Model Interpretation\nModel interpretability is a critical area of research in deep\nlearning, especially for complex models like transformers.\nTraditional deep learning models, such as CNNs, have been\ninterpreted using various techniques like saliency maps\n[47], deconvolutional networks [63], Guided Backpropa-\ngation [48], Feature Visualization [34], Local Interpretable\nModel-Agnostic Explanations (LIME) [40], SHapley Addi-\ntive exPlanations (SHAP) [31], Class Activation Mapping\n(CAM) [65], Feature Attribution with Integrated Gradients\n[50], and Grad-CAM [44], which highlight the most impor-\ntant regions of the input that influence model predictions.\nWhile effective for CNNs, these techniques are less suit-\nable for transformer architectures, as they fail to account\nfor the unique self-attention mechanisms that transform-\ners rely on. Anchors is a model-agnostic interpretability\nmethod that provides high-precision, locally faithful expla-\nnations by generating if-then rules that sufficiently explain\na model’s prediction for specific inputs [41].\nRecent interpretability research on transformers has fo-\ncused on understanding how these models allocate atten-\ntion and propagate information. A seminal contribution was\nmade by examining self-attention patterns in transformer-\nbased language models, revealing how attention is dis-\ntributed across tokens [59].\nAbnar et al.\n[1] advanced\nthis by proposing methods to visualize attention flow across\nlayers, aiding in the understanding of information propa-\ngation. Chefer et al. [6] introduced a relevance-based ap-\nproach, computing relevance scores for each token to pro-\nvide deeper insights into the model’s decision-making pro-\ncess. Additionally, the sensitivity of model predictions to\ninput tokens has been explored as another way to interpret\ntransformer behavior [20].\nInterpretability methods for reflecting semantic relations\nwithin input sequences have been investigated in NLP tasks,\n2\n\naiming to characterize how transformers associate words or\ntokens across different positions in a sentence for down-\nstream tasks [11]. Justifying the role of the multi-head at-\ntention mechanism in transformers and its contribution to\nimproved performance is another area of study, revealing\ninsights into how transformers operate [32]. Other notable\ninterpretability methods include Interpretability-Aware Vi-\nsual Transformers (IA-ViT) [36], Mechanistic Interpretabil-\nity [39], Concept Transformers [42], Nested Hierarchical\nTransformers [64], Tracr [29], and ViT-net [22].\nWhile many methods focus on explaining model outputs,\nfewer efforts have been directed toward understanding the\nintermediate processes within the model. The internal rep-\nresentations, particularly the latent tokens in Vision Trans-\nformers (ViTs), remain largely unexplored. A similar ap-\nproach is the work by [7], which focuses on the CLIP model\n[37].\nTheir method involves disabling the self-attention\nmechanism to map the latent tokens into a multimodal space\nwithout additional training. However, simply disabling self-\nattention may introduce distribution shifts, raising concerns\nabout the validity of their results. Furthermore, their ap-\nproach relies heavily on CLIP’s text encoder, which limits\nits generalizability to other models. Our work overcomes\nthese limitations by focusing on backward-looking analy-\nsis, investigating how latent tokens relate to the input rather\nthan looking ahead. Additionally, we rely solely on one\nencoder, making our method more applicable to a broader\nrange of models.\n2.2. Unsupervised Semantic Segmentation\nUnsupervised semantic segmentation has advanced signifi-\ncantly due to self-supervised learning and clustering-based\ntechniques, which reduce reliance on labeled datasets. Early\napproaches, such as Invariant Information Clustering (IIC)\n[21], employed mutual information to group similar pixels\nwithout using labels. Building on this foundation, PiCIE\nintroduced consistency by integrating photometric and geo-\nmetric invariances, setting an important precedent for sub-\nsequent methods [9].\nThe emergence of Vision Transformers (ViTs) and self-\nsupervised learning marked a major shift in the field. DINO\nbecame a pioneering method, extracting rich, meaningful\nfeatures without the need for labeled data [5]. Its effec-\ntiveness in unsupervised segmentation laid the groundwork\nfor more recent advancements.\nFor instance, TransFGU\n[62] performs semantic segmentation in a top-down man-\nner by deriving class activation maps from DINO models.\nSTEGO [18] leverages DINO’s features, employing con-\ntrastive learning to group similar regions and achieving no-\ntable improvements in segmentation accuracy.\nSubsequent methods further refined these concepts.\nMaskContrast incorporated clustering techniques to ensure\nregion consistency across different views, enhancing feature\nrepresentations for unsupervised segmentation [56]. Leop-\nart utilized self-supervised ViTs to improve pixel group-\ning, particularly in complex scenes [66]. ACSeg introduced\nadaptive conceptualization for pixel-level semantic group-\ning, using an Adaptive Concept Generator (ACG) to dy-\nnamically align learnable prototypes with relevant concepts,\nthereby addressing over- and under-clustering challenges in\nvaried images [27].\nLeveraging hidden positives for unsupervised seman-\ntic segmentation enhances pixel grouping by identifying\nand utilizing implicit positive relationships within the data,\nboosting segmentation performance without the need for la-\nbeled examples [45]. Using self-supervised learning and\nadaptive feature representations to enable models to dis-\ncover and segment novel, unseen object categories without\nlabeled data have also been investigated [55]. Smooseg in-\ntroduces a smoothness prior to enhancing unsupervised se-\nmantic segmentation by promoting coherent region group-\ning [25]. Unsupervised semantic segmentation is also im-\nproved by leveraging depth-guided feature correlation and\ntargeted sampling to enhance region consistency and accu-\nracy [46]. [17].\nUnlike approaches that depend on self-training, pseudo-\nlabeling, or complex setups, our model ULTra introduces\na zero-shot method for unsupervised semantic segmenta-\ntion. ULTra achieves strong segmentation performance by\ndirectly leveraging the semantic information embedded in\nthe latent tokens of pre-trained Transformer models, with-\nout the need for additional training or fine-tuning. Further-\nmore, ULTra emphasizes explainability within Transform-\ners by illustrating how the model’s latent tokens contribute\nto segmentation, in contrast to the common practice of us-\ning only the CLS token to represent the entire image. This\nzero-shot, explainability-driven approach offers a novel di-\nrection for unsupervised segmentation, demonstrating that\nTransformer-based architectures can achieve SOTA perfor-\nmance without labeled data or extensive additional process-\ning.\n3. Methodology\nIn this section, we present our approach for interpreting la-\ntent representations in Transformers. We begin with essen-\ntial preliminaries and notation for clarity, followed by a de-\ntailed explanation of our method for interpreting latent to-\nkens and its application to semantic segmentation.\n3.1. Preliminaries and Notation\nThe architecture of a typical Transformer can be formulated\nas follows: the input X is split into n tokens {xi}n\ni=1. Af-\nter tokenization, token embeddings {ei}n\ni=0 are computed,\nwhere e0 corresponds to the CLS token. Positional encod-\nings PEi are added to the i-th token embedding to incorpo-\nrate spatial information, resulting in the latent token repre-\n3\n\n(a) Original Image\n(b) Latent Token’s Relevance Map\n(c) Predicted Binary Mask\nFigure 2. An example of token interpretation by our model and its predicted binary mask. (a) Original image. (b) Overlay of ˜S(13)\ni\non the\noriginal image for different i, where the location of the i-th token is indicated by the purple square. (c) The binary mask M (13)\ni\nfor each\ncorresponding relevance map in (b).\nsentation z(1)\ni\n= ei + PEi. Here, z(l)\ni\nrepresents a latent\ntoken, where l denotes the layer index with l ∈{1, . . . , L}\nand L is the total number of layers in the Transformer, and\ni represents the i-th token within the l-th layer.\nFor each head h ∈{1, . . . , H} in the multi-head atten-\ntion mechanism, the queries, keys, and values correspond-\ning to the i-th token are obtained via linear transformations,\nprojecting the latent token of dimension d into dimension k:\nQ(l)\nh (z(l−1)\ni\n) = (W (l)\nh,q)T z(l−1)\ni\n, K(l)\nh (zl−1\ni\n) = (W (l)\nh,k)T z(l−1)\ni\n,\nV (l)\nh (z(l−1)\ni\n) = (W (l)\nh,v)T z(l−1)\ni\n,\n∀l ∈{2, . . . , L}\n(1)\nwhere W (l)\nh,q, W (l)\nh,k, W (l)\nh,v ∈Rd×k. The attention weights\nfor each token pair (i, j) at layer l and head h are computed\nas:\nα(l)\nh,i,j = softmaxj\n \n⟨Q(l)\nh (zl−1\ni\n), K(l)\nh (z(l−1)\nj\n)⟩\n√\nk\n!\n.\n(2)\nThen, i-th token is updated by summing over the weighted\nvalues across all heads:\n¯u(l)\ni\n=\nH\nX\nh=1\n(W (l)\nc,h)T\nn\nX\nj=1\nα(l)\nh,i,jV (l)\nh (z(l−1)\nj\n),\n(3)\nwhere Wc,h ∈Rk×d. The updated token representation ui\nafter the attention layer is computed as:\nu(l)\ni\n= LayerNorm(z(l−1)\ni\n+ ¯u(l)\ni ).\n(4)\nEach token then passes through a feed-forward network:\n¯z(l)\ni\n= (W (l)\n2 )T ReLU((W (l)\n1 )T ui),\n(5)\nz(l)\ni\n= LayerNorm(ui + ¯z(l)\ni ).\n(6)\nHere, W (l)\n1\n∈Rd×m, W (l)\n2\n∈Rm×d.\n3.2. Interpreting Latent Tokens\nA straightforward approach for interpreting a model in-\nvolves analyzing the semantic flow from the input to the\ncorresponding logits. This can be achieved by adding the\nattention probability matrix to an identity matrix I, which\nincorporates skip connections, and then multiplying the re-\nsult across layers [1]. However, a notable challenge arises\nfrom multiple attention heads in each Transformer layer. To\naddress this, [6] proposes performing a weighted average\nacross attention heads, with the weights determined by the\ngradient of the logits with respect to the attention weights.\nWe employed a modified, simpler version of this method,\nutilizing only the attention weights rather than the layer’s\nrelevance.\nIn our framework, latent tokens z(l)\ni\nare represented as\nvectors rather than direct class correspondences, introduc-\ning an additional layer of complexity. To manage this, an\nappropriate transformation, such as the energy or euclidean\nnorm of the latent token, is employed as a surrogate. Con-\nsequently, we compute the gradient of ∥z(l)\ni ∥with respect\nto the attention weights to facilitate the analysis.\nIn the following equations, we define the relevance map\nS(l)\ni\n∈Rn, where the j-th element of this vector represents\nthe importance of the j-th input token to the targeted latent\ntoken z(l)\ni . The relevance map is computed as:\nA\n(b,l)\ni\n= I + Eh\n\u0010\n∇Ab\nh∥z(l)\ni ∥⊙A(b)\nh\n\u0011+\n,\nS\n(l)\ni\n= A\n(1,l)\ni\n· A\n(2,l)\ni\n· . . . · A\n(l−1,l)\ni\n,\nS(l)\ni\n= S\n(l)\ni [i, 1 : ],\n(7)\nwhere (·)+\nmeans considering only positive values,\nS, A\n(b,l)\ni\n∈R(n+1)×(n+1), ⊙denotes the Hadamard prod-\nuct, Eh represents the mean across the heads dimension, and\nA(b)\nh\n∈Rh×(n+1)×(n+1) is the attention score matrix in the\nb-th layer and A(b)\nh,i,j = α(b)\nh,i,j.\nDue to the skip connections in the transformer, most of\n4\n\nthe contribution of S(l)\ni\nis concentrated on S(l)\ni [i −1] which\nmakes it hard to analyze other tokens’ contribution. To ad-\ndress this issue, we replace this element with the maximum\nvalue of other elements, thereby capturing the contributions\nof additional tokens to the selected token.\nFor vision tasks, we first reshape the relevance map and\nthen upsample it using bilinear or cubic interpolation to\nmatch the resolution of the model’s input. The resulting\nhigher-dimensional matrix is denoted as ˜S. This upsam-\npling step is essential for enabling accurate object selection\nand semantic segmentation tasks.\n3.3. ULTra in Unsupervised Tasks\nIn this section, we aim to examine ULTra’s capability to\nadapt to various tasks involving semantic knowledge. Im-\nportantly, it requires no additional training, leveraging the\ninherent understanding within transformers rather than re-\nlying on loss functions objective, final layer outputs, or fine-\ntuning.\nUnsupervised Semantic Segmentation. As previously dis-\ncussed, a relevance map can be defined for each latent to-\nken at a fixed layer, with the total number of relevance\nmaps equal to the number of latent tokens. In the context\nof segmentation, the goal is to assign a class label to each\npixel within an image. To achieve this, we employ cluster-\ning techniques, such as hierarchical clustering, that do not\nrequire a predefined number of classes. These techniques\ngroup the relevance maps into k distinct clusters, where k\nis unknown. Ideally, we aim for k to approximate the actual\nnumber of classes present in the image.\nOur approach provides flexibility in adjusting the value\nof k by modifying the cutoff distance threshold ζ within the\nclustering algorithm. Increasing ζ produces fewer, broader\nclusters that capture general categories, such as background\nand foreground. Conversely, reducing ζ allows for finer seg-\nmentation, distinguishing more specific features, such as an\nobject’s head or hands. To prevent the method from dispro-\nportionately favoring larger objects, given that the number\nof elements in each cluster may vary, we apply min-max\nscaling to each cluster independently.\nAfter clustering, we define k distinct concepts by aggre-\ngating the relevance maps within each cluster. The aggre-\ngated relevance map ˜S(l)\nc [x, y] for a cluster c is computed\nas:\n˜S(l)\nc,ζ[x, y] =\nX\ni∈ϕζ(c)\n˜S(l)\ni [x, y],\n(8)\nwhere ϕζ(c) = {i | Class( ˜S(l)\ni ) = c} represents the set of\nlabel assignments determined by the clustering algorithm.\nFor each pixel at position [x, y], the class label is determined\nby identifying the cluster c with the highest relevance value\nat that pixel. Mathematically, the class assignment for a\npixel is expressed as:\nClass[x, y](l)\nζ\n= argmax\nc∈{1,...,k}\nS(l)\nc,ζ[x, y].\n(9)\nSome examples illustrating our segmentation method is\npresented in Figure 3. Additionally, the hierarchy tree and\nthe effect of the threshold are demonstrated in Figure 1.\nFigure 3. ULTra segmentation results on sample images. The top\nrow displays the original images, the middle row shows ground-\ntruth annotations, and the bottom row presents our model’s pre-\ndictions.\n4. Experiment\nDatasets. In our experiments, we evaluate model perfor-\nmance on several semantic segmentation benchmarks, fo-\ncusing on vision-related tasks. We conducted experiments\non three datasets: COCO-Stuff 27 [4], PASCAL VOC 2012\n[15], and Potsdam-3 [19].\nThis combination of datasets\nprovides a diverse testing ground to rigorously evaluate our\nunsupervised zero-shot approach across both standard and\nchallenging perspectives in semantic segmentation.\nCOCO-Stuff 27, a subset of the COCO dataset [28], in-\ncludes complex, real-world scenes with pixel-level annota-\ntions across various object categories. Similarly, PASCAL\nVOC 2012 serves as a classic benchmark with pixel-level\nannotations, while the Potsdam-3 dataset offers a unique\naerial, top-down perspective of urban scenes.\nThe latter\nadds an additional challenge with its high-resolution images\nof buildings, roads, and natural elements captured over the\ncity of Potsdam.\nFor our qualitative analysis of LLM interpretation in the\ntask of text summarization, as described in Section 4.3, we\nutilized the TL;DR dataset [49]. The TL;DR dataset con-\ntains summary comparisons with human feedback collected\nby OpenAI. Each entry consists of a Reddit post, including\nits title, original content, and a human-generated TL;DR.\nModels. For all experiments in the vision tasks, we used\nCLIP’s image encoder ViT-B/32 [37]. For interpreting text\n5\n\nsummarization, as described in Section 4.3, we used the\nLlama-2-7B language model [54]. All experiments were\nrun on 8 NVIDIA RTX 4090-24GB GPUs.\n4.1. Zero-shot Unsupervised Object Selection\nThe upsampled relevance map ˜S(l)\ni\ncan be converted into a\nbinary segmentation mask using a threshold τ, where the\nbinary mask M (l)\ni\nis defined as:\nM (l)\ni [x, y] =\n(\n0,\nif ˜S(l)\ni [x, y] < τ,\n1,\notherwise.\n(10)\nHere, ˜S(l)\ni [x, y] represents the relevance value at position\n[x, y] in ˜S(l)\ni , with τ as the threshold. When ˜S(l)\ni [x, y] is be-\nlow τ, M (l)\ni [x, y] is set to 0; otherwise, it is set to 1, marking\nthe object region.\nOur findings indicate that as tokens propagate through\nthe network, they refine their object representation while\nretaining the semantic meaning of their associated image\npatches performing Object Selection. Figure 5 visually il-\nlustrates this process. For a given patch token xi, the object\nit most strongly represents is denoted as class ki, indicating\nthat xi predominantly corresponds to a region of the object\nbelonging to class ki in the image. The latent token z(l)\ni\ngenerates a relevance map that highlights areas with higher\nvalues associated with class ki. After applying a threshold,\nthis map becomes a binary segmentation mask expected to\nexhibit a high Intersection over Union (IoU) with the corre-\nsponding class ki region in the image. An illustrative exam-\nple is shown in Figure 2.\nTo quantify alignment, we compute the IoU by convert-\ning the relevance map S(l)\ni\ninto a binary mask M (l)\ni\nand\ncomparing it with the ground-truth mask. We propose the\nInitial Token IoU (ITIoU) metric, which measures how well\nthe relevance maps of input tokens align with their respec-\ntive class masks. The ITIoU is calculated as:\nITIoU(l)(X) = 1\nC\nC\nX\ni=1\n1\n|Pi|\nX\nxj∈Pi\nIoU(M (l)\nj , Gi),\n(11)\nwhere C denotes the number of classes, Pi represents the\nset of tokens associated with class i, M (l)\nj\nis the binary seg-\nmentation mask for token xj within class i, and Gi is the\nground-truth mask for class i in image X. The inner sum\naverages the IoU for tokens in Pi for each class, and the\nouter sum then averages across all classes. Using a thresh-\nold of 0.2, our ITIoU metric achieves an average score of\n37.84 % on the COCO validation dataset and 39.51 % on\nthe VOC dataset.\nThe ITIoU metric provides a comprehensive evaluation\nby incorporating a weighted average across tokens in each\nclass, enhancing the assessment of token alignment with\ntheir respective ground-truth labels.\n4.2. Zero-shot Unsupervised Semantic Segmenta-\ntion\nWe benchmarked the segmentation capability of our method\nagainst several approaches in the literature on unsupervised\nsegmentation. Notably, unlike other methods, our approach\nrequires no additional training.\nInstead, it relies solely\non a pre-trained vision transformer, which may have been\ntrained on tasks unrelated to unsupervised segmentation.\nTo evaluate our method, we used the Unsupervised mean\nIntersection over Union (U. mIoU) and Unsupervised pixel\nAccuracy (U. Accuracy) metrics, following standard prac-\ntices in semantic segmentation research. In all experiments\nacross datasets, as shown in Tables 1, 2, and 3, we set the\ncutoff distance threshold ζ = 0.4.\nTable 1. Comparison of unsupervised segmentation methods on\nthe 27-class COCO-Stuff validation set. ”W” indicates methods\nrequiring additional training, while ”W/O” denotes methods with-\nout additional training.\nMethod\nTraining\nU. Accuracy\nU. mIoU\nIIC [21]\nW\n21.8\n6.7\nDINO [5]\nW\n30.5\n9.6\nPiCIE [9]\nW\n48.1\n13.8\nACSeg [27]\nW\n-\n16.4\nSTEGO [18]\nW\n56.9\n28.2\nDepthG [46]\nW\n58.6\n29.0\nU2Seg [33]\nW\n63.9\n30.2\nULTra (Ours)\nW/O\n67.2\n32.9\nTable 2. Comparison of unsupervised segmentation methods on\nthe PASCAL VOC validation set.\nMethod\nTraining\nU. mIoU\nIIC [21]\nW\n9.8\nMaskContrast [56]\nW\n35.0\nLeopart [66]\nW\n41.7\nTransFGU [62]\nW\n37.2\nMaskDistill [57]\nW\n42.0\nACSeg [27]\nW\n47.1\nULTra (Ours)\nW/O\n51.9\nTable 3. Comparison of unsupervised segmentation methods on\nthe Potsdam validation set.\nMethod\nTraining\nU. ACC\nIIC [21]\nW\n65.1\nDINO [5]\nW\n71.3\nSTEGO [18]\nW\n77.0\nDepthG [46]\nW\n80.4\nULTra (Ours)\nW/O\n74.6\n6\n\nHow do I [ 2 0 M ] stop feeling bad about myself for having no relationship\nexperience at all ? POST : It just seems like everyone I know has at least\nhad a \" thing \" with someone by this point . I ' ve made out with a girl once\n( who later told me that was a mistake ) and I feel like girls always reject\nme or only see me as a friend . Which is perfectly acceptable , but I ' m\nstarting to get ups et that I ' ve never had any kind of relationship . I just\ngot rejected by a girl who I thought was into me and I ' ve been feeling\nbad ever since . I just don ' t know what ' s wrong with me . I guess I ' m a\nlittle bit skin ny ( I work out regularly though ), but I show er every day ,\ndress pretty well , all that stuff .\nI need help about those feelings POST : I am a 1 8 M , she ' s a 1 7 F . We '\nve got a troubles ome relationship which started as a pure friendship one\nyear ago . I ' ve made mist akers , she made hers too . O ur last situation\nscenario is explained in here : Now I feel like I hate her , I used to adm ire\nher a lot , but I ' m really disappoint ed with her and with her character .\nBut I just realized I still like her . So , well , yeah , I like her and hate her .\nAnd just after that bad situation happened I realized she also had that\nfeeling . Well , now we both hate and love each other . What to do ? What\nto think ? What to feel ? add itional info : today our friend asked me for\nhelp with some calculations and I made a jo ke about our physics teacher .\nShe laughed and smiled at me just like one year ago , but after she\nrealized that , she seemed kind a [ gr ouch y ](\n(a) TL;DR: I’ve had very bad luck with girls my whole\n(b) TL;DR: I still like her but my rational\nlife and I don’t know how to get my confidence up.\nside says ”no, she is a trash person”.\nFigure 4. Visualization of Token Contribution Scores (λ(l)\ni ) highlighting the relevance of context tokens in interpreting the summary. Each\ntoken is colored proportionally to its λ(l)\ni\nvalue. These visualizations demonstrate the model’s ability to identify key semantic elements in\nthe context for generating relevant summaries.\nOur results in Tables 1, 2, and 3 highlight the effective-\nness of the proposed method for unsupervised semantic seg-\nmentation. Unlike baseline approaches, which require ad-\nditional training specifically tailored to segmentation, our\nmethod leverages a pre-trained vision transformer without\nany further fine-tuning. Despite this lack of task-specific\ntraining, our approach consistently outperforms other meth-\nods, demonstrating adaptability across diverse datasets.\n4.3. Interpreting LLMs in Text Summarization\nIn this section, we examine how our interpretability frame-\nwork can be applied to text summarization tasks to uncover\nthe underlying mechanisms and intent of LLMs. By visual-\nizing the regions of the input context that an LLM prioritizes\nwhile interpreting a given TL;DR summary, we gain deeper\ninsights into the model’s behavior and decision-making pro-\ncesses. As shown in Figure 4, these visualizations allow us\nto pinpoint the most influential regions of a textual input\nprompt in generating concise and relevant summaries.\nFor the experiments, we used a Supervised Fine-Tuned\n(SFT) version of Llama-2-7B trained on the UltraFeedback\nBinarized (UFB) dataset [12]. Additionally, we aligned the\nmodel to the text summarization task on the TL;DR dataset\nusing the Direct Preference Optimization (DPO) method\n[38] for 1,000 iterations, with a learning rate of 5 × 10−6\nand β = 0.5. To validate our framework, we selected the\npreferred response (TL;DR) of each sample in the dataset,\ndenoted it as y, and used it as the summary of the context x.\nIn this task, we concatenate the context x and the sum-\nmary y with a separator token. After feeding this input to\nthe model, we compute the relevance scores of the TL;DR\ntokens with respect to the context tokens. We then average\nthese scores for each token in x to obtain a scalar value,\nreferred to as the Token Contribution Score, λ(l)\ni\n∈R+,\nwhich highlights the contribution of each context token in\ninterpreting the summary y with respect to the context. This\nprovides visual evidence of how the model processes the\ncontext text and identifies key semantic elements relevant\nto producing the summary y. Accordingly, λ(l)\ni\nis computed\nas:\nλ(l)\ni\n= 1\n|y|\n|y|\nX\nj=1\nS(l)\nj+|x|[i],\n∀i ∈{1, · · · , |x|},\n(12)\nwhere | · | denotes the number of tokens in the text.\nIn example (a):\nsemantically significant words such\nas ‘relationship’, ‘experience’, ‘rejection’, and ‘never’ are\nprominently highlighted, reflecting the model’s interpreta-\ntion of the person’s struggles with relationships and feelings\nof rejection. Additionally, the highlighting of the question\nat the beginning of the context ‘How do I stop feeling bad...’\nsuggests the model recognizes the presence of uncertainty\nand a request for guidance, which is encapsulated in the\nsummary as ‘I don’t know.’\nIn example (b): λ(l)\ni\nscores reveals the model’s focus on\nwords such as ‘feelings’, ‘hate’, ‘disappoint’, ‘love’, and\n‘like’, which correspond to the person’s mixed emotions to-\nward their girlfriend, as described in the summary. The ap-\nparent contradiction between ‘love’ and ‘trashness’ in the\nsummary seems to be derived from these highlighted terms,\nsuggesting the model understands the conflicting emotions\npresent in the text. Furthermore, the emphasis on ‘charac-\nter’ aligns with the summary’s judgmental tone, suggesting\nthat the model interprets this word as indicative of an as-\nsessment of personality traits and behaviors.\nThis token-level analysis can serve as a valuable tool\nfor effective supervision in future research, particularly for\ndeveloping interpretable fine-tuning and alignment meth-\nods like Reinforcement Learning from Human Feedback\n(RLHF) [10, 35, 49] and Preference Optimization [3, 14,\n38], where understanding model behavior and intent is es-\nsential. It may also provide useful insights in related areas,\nsuch as Chain-of-Thought [8, 24, 43] and Theory-of-Mind\n[23, 60, 61], which seek to make the reasoning processes\nand intentions of LLMs more transparent. Ultimately, our\nframework offers insights into fundamental questions in\n7\n\n(a) Original Image\n(b) Explanation Relevency Map\nFigure 5. An example illustrating the model’s decision-making process across layers. Moving from left to right corresponds to deeper\nlayers in the network. The first row corresponds to the CLS token, while the second, third, and fourth rows represent three different tokens,\nhighlighted by red squares.\n2\n4\n6\n8\n10\n12\nLayer\n45\n50\n55\n60\n65\nU. Accuracy (%)\nAccuracy\nMean IoU\n16\n20\n24\n28\n32\nU. Mean IoU (%)\n(a) COCO-Stuff\n2\n4\n6\n8\n10\n12\nLayer\n60\n63\n66\n69\n72\n75\nU. Accuracy (%)\nAccuracy\nMean IoU\n33\n36\n39\n42\n45\n48\n51\nU. Mean IoU (%)\n(b) PASCAL VOC\nFigure 6. Ablation study on two evaluation metrics across layers.\nThese plots demonstrate a progressive improvement in semantic\nsegmentation performance in the deeper layers of the transformer\nmodel. This enhancement is attributed to latent tokens capturing\nmore meaningful segment structures, resulting in increasingly ac-\ncurate and refined semantic representations.\nNLP, such as: ”How is the model thinking?” or ”What is the\nunderlying intent behind the model’s generated response?”.\n4.4. Ablation Study on the Effect of Layer Depth in\nViT Token Understanding\nIn this section, we analyze the impact of depth on our\nmodel’s interpretability and segmentation performance,\nproviding insights into the contribution of each layer. As an-\nticipated and observed in Figure 6, deeper layers generally\ncarry more semantic significance. However, the contribu-\ntion diminishes in the final layers, suggesting that a depth\nof around 13 layers might be more than sufficient for the\nViT to effectively comprehend image content. This find-\ning implies that even fewer layers might achieve compara-\nble results, potentially reducing computational costs with-\nout compromising performance.\nWe observe an intriguing behavior in the initial lay-\ners, where performance initially declines before improving.\nThis phenomenon is also visually evident in Figure 5, where\nthe attention maps in the first layer appear to focus on the\nentire image. This suggests that, initially, the token exam-\nines the image as a whole before selectively gathering in-\nformation from tokens with similar characteristics.\n5. Conclusion\nThis paper introduces a novel interpretability framework\nthat provides valuable insights into the decision-making\nprocesses of Transformer models. The framework is based\non the input interpretation provided by each token in an ar-\nbitrary layer. By aggregating the interpretation of all tokens\nin the same layer, the framework enhances understanding\nof the Transformer’s behavior at the layer level. The rich-\nness of this understanding enables zero-shot unsupervised\nsemantic segmentation, where our method achieves state-\nof-the-art performance with an accuracy of 67.2% and an\nmIoU of 32.9% on the COCO-Stuff dataset, and an mIoU\nof 51.9% on the PASCAL VOC dataset for the unsupervised\nzero-shot version. Beyond vision tasks, the framework is\nnot limited to vision models and can also be applied to in-\nterpret the behavior of large language models (LLMs) in\ntasks such as text summarization. This highlights the ver-\nsatility and broad applicability of the proposed framework\n8\n\nacross various domains, offering potential for future work\nto simultaneously demystify the multimodal understanding\nof Transformers.\nReferences\n[1] Samira Abnar and Willem Zuidema.\nQuantifying atten-\ntion flow in transformers. arXiv preprint arXiv:2005.00928,\n2020. 1, 2, 4\n[2] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\nple, Lucile Saulnier, L´elio Renard Lavaud, Marie-Anne\nLachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth´ee Lacroix, and William El Sayed.\nMistral 7B, 2023. arXiv:2310.06825 [cs]. 1\n[3] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot,\nDaniel Guo, Daniele Calandriello, Michal Valko, and R´emi\nMunos. A general theoretical paradigm to understand learn-\ning from human preferences, 2023. 7\n[4] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In Computer vision\nand pattern recognition (CVPR), 2018 IEEE conference on.\nIEEE, 2018. 5\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650–9660, 2021. 3, 6\n[6] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter-\npretability beyond attention visualization.\nProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 782–791, 2021. 1, 2, 4\n[7] Haozhe Chen, Junfeng Yang, Carl Vondrick, and Chengzhi\nMao. Invite: Interpret and control vision-language models\nwith text explanations. In The Twelfth International Confer-\nence on Learning Representations, 2024. 1, 3\n[8] Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guan-\nqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei\nLai, Zexuan Xiong, and Minlie Huang. Tombench: Bench-\nmarking theory of mind in large language models, 2024. 7\n[9] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath\nHariharan. Picie: Unsupervised semantic segmentation us-\ning invariance and equivariance in clustering. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 16794–16804, 2021. 3, 6\n[10] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic,\nShane Legg, and Dario Amodei. Deep reinforcement learn-\ning from human preferences, 2023. 7\n[11] Kevin Clark. What does bert look at? an analysis of bert’s\nattention. arXiv preprint arXiv:1906.04341, 2019. 3\n[12] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingx-\niang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie,\nYankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback:\nBoosting language models with scaled ai feedback, 2024. 7\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 1\n[14] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Ju-\nrafsky, and Douwe Kiela. Kto: Model alignment as prospect\ntheoretic optimization, 2024. 7\n[15] Mark Everingham and John Winn. The pascal visual object\nclasses challenge 2011 (voc2011) development kit. Pattern\nAnalysis, Statistical Modelling and Computational Learning,\nTech. Rep, 8, 2011. 5\n[16] Lei Gao and Ling Guan. Interpretability of machine learning:\nRecent advances and future prospects. IEEE MultiMedia, 30\n(4):105–118, 2023. 2\n[17] Shanghua Gao, Zhong-Yu Li, Ming-Hsuan Yang, Ming-\nMing Cheng, Junwei Han, and Philip Torr. Large-scale un-\nsupervised semantic segmentation.\nIEEE transactions on\npattern analysis and machine intelligence, 45(6):7457–7476,\n2022. 3\n[18] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah\nSnavely, and William T Freeman. Unsupervised semantic\nsegmentation by distilling feature correspondences. arXiv\npreprint arXiv:2203.08414, 2022. 2, 3, 6\n[19] ISPRS. 2d semantic labeling contest - potsdam. https:\n/ / www . isprs . org / education / benchmarks /\nUrbanSemLab / 2d - sem - label - potsdam . aspx,\n2018. 5\n[20] Sarthak Jain and Byron C Wallace. Attention is not explana-\ntion. arXiv preprint arXiv:1902.10186, 2019. 2\n[21] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant in-\nformation clustering for unsupervised image classification\nand segmentation. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 9865–9874,\n2019. 3, 6\n[22] Sangwon Kim, Jaeyeal Nam, and Byoung Chul Ko. Vit-net:\nInterpretable vision transformers with neural tree decoder. In\nInternational conference on machine learning, pages 11162–\n11172. PMLR, 2022. 3\n[23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka\nMatsuo, and Yusuke Iwasawa. Large language models are\nzero-shot reasoners, 2023. 7\n[24] Michal Kosinski. Evaluating large language models in the-\nory of mind tasks. Proceedings of the National Academy of\nSciences, 121(45), 2024. 7\n[25] Mengcheng Lan, Xinjiang Wang, Yiping Ke, Jiaxing Xu,\nLitong Feng, and Wayne Zhang. Smooseg: smoothness prior\nfor unsupervised semantic segmentation. Advances in Neu-\nral Information Processing Systems, 36:11353–11373, 2023.\n3\n[26] Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei,\nJinfeng Yi, and Bowen Zhou. Trustworthy ai: From prin-\nciples to practices. ACM Computing Surveys, 55(9):1–46,\n2023. 2\n[27] Kehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, Yian\nZhao, Guoli Song, Chang Liu, Li Yuan, and Jie Chen. Acseg:\nAdaptive conceptualization for unsupervised semantic seg-\nmentation. In Proceedings of the IEEE/CVF conference on\n9\n\ncomputer vision and pattern recognition, pages 7162–7172,\n2023. 2, 3, 6\n[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer, 2014. 5\n[29] David Lindner, J´anos Kram´ar, Sebastian Farquhar, Matthew\nRahtz, Tom McGrath, and Vladimir Mikulik. Tracr: Com-\npiled transformers as a laboratory for interpretability. Ad-\nvances in Neural Information Processing Systems, 36, 2024.\n3\n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012–10022, 2021. 1\n[31] Scott Lundberg. A unified approach to interpreting model\npredictions. arXiv preprint arXiv:1705.07874, 2017. 2\n[32] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen\nheads really better than one? Advances in neural information\nprocessing systems, 32, 2019. 3\n[33] Dantong Niu, Xudong Wang, Xinyang Han, Long Lian, Roei\nHerzig, and Trevor Darrell. Unsupervised universal image\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22744–\n22754, 2024. 6\n[34] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert.\nFeature visualization. Distill, 2(11):e7, 2017. 2\n[35] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan\nLowe. Training language models to follow instructions with\nhuman feedback, 2022. 7\n[36] Yao Qiang, Chengyin Li, Prashant Khanduri, and Dongx-\niao Zhu.\nInterpretability-aware vision transformer.\narXiv\npreprint arXiv:2309.08035, 2023. 3\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 1, 3, 5\n[38] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Er-\nmon, Christopher D. Manning, and Chelsea Finn.\nDirect\npreference optimization: Your language model is secretly a\nreward model, 2024. 7\n[39] Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and\nZiyu Yao. A practical review of mechanistic interpretabil-\nity for transformer-based language models. arXiv preprint\narXiv:2407.02646, 2024. 3\n[40] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.\n” why should i trust you?” explaining the predictions of any\nclassifier. In Proceedings of the 22nd ACM SIGKDD interna-\ntional conference on knowledge discovery and data mining,\npages 1135–1144, 2016. 2\n[41] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.\nAnchors: High-precision model-agnostic explanations.\nIn\nProceedings of the AAAI conference on artificial intelli-\ngence, 2018. 2\n[42] Mattia Rigotti, Christoph Miksovic, Ioana Giurgiu, Thomas\nGschwind, and Paolo Scotton. Attention-based interpretabil-\nity with concept transformers. In International conference\non learning representations, 2021. 3\n[43] Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin\nChoi, and Yulia Tsvetkov. Minding language models’ (lack\nof) theory of mind: A plug-and-play multi-character belief\ntracker, 2023. 7\n[44] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam:\nVisual explanations from deep networks via\ngradient-based localization.\nProceedings of the IEEE in-\nternational conference on computer vision, pages 618–626,\n2017. 1, 2\n[45] Hyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil\nHeo. Leveraging hidden positives for unsupervised semantic\nsegmentation. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 19540–\n19549, 2023. 3\n[46] Leon Sick, Dominik Engel, Pedro Hermosilla, and Timo\nRopinski.\nUnsupervised semantic segmentation through\ndepth-guided feature correlation and sampling. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3637–3646, 2024. 2, 3, 6\n[47] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.\nDeep inside convolutional networks:\nVisualising image\nclassification models and saliency maps.\narXiv preprint\narXiv:1312.6034, 2014. 1, 2\n[48] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas\nBrox, and Martin Riedmiller. Striving for simplicity: The\nall convolutional net. arXiv preprint arXiv:1412.6806, 2014.\n2\n[49] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,\nRyan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and\nPaul Christiano. Learning to summarize from human feed-\nback, 2022. 5, 7\n[50] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic\nattribution for deep networks. In International conference on\nmachine learning, pages 3319–3328. PMLR, 2017. 2\n[51] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert\nDadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\nMorgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, Pouya\nTafti, L´eonard Hussenot, Pier Giuseppe Sessa, Aakanksha\nChowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex\nCastro-Ros, Ambrose Slone, Am´elie H´eliou, Andrea Tac-\nchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak\nShahriari, Charline Le Lan, Christopher A. Choquette-Choo,\nCl´ement Crepy, Daniel Cer, Daphne Ippolito, David Reid,\nElena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George\nTucker, George-Christian Muraru, Grigory Rozhdestvenskiy,\nHenryk Michalewski, Ian Tenney, Ivan Grishchenko, Ja-\ncob Austin, James Keeling, Jane Labanowski, Jean-Baptiste\n10\n\nLespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Jo-\nhan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee,\nKathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee,\nLucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth,\nMichael Sharman, Nikolai Chinaev, Nithum Thain, Olivier\nBachem, Oscar Chang, Oscar Wahltinez, Paige Bailey,\nPaul Michel, Petko Yotov, Rahma Chaabouni, Ramona Co-\nmanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu,\nRyan Mullins, Samuel L Smith, Sebastian Borgeaud, Ser-\ntan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri,\nSoham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg,\nWojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao\nGong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl´ement\nFarabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu,\nDemis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle\nBarral, Fernando Pereira, Eli Collins, Armand Joulin, Noah\nFiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy.\nGemma: Open models based on gemini research and tech-\nnology, 2024. 1\n[52] Hans Thisanke, Chamli Deshan, Kavindu Chamith, Sa-\nchith Seneviratne, Rajith Vidanaarachchi, and Damayanthi\nHerath. Semantic segmentation using vision transformers:\nA survey. Engineering Applications of Artificial Intelligence,\n126:106669, 2023. 1\n[53] Bhavani Thuraisingham.\nTrustworthy machine learning.\nIEEE Intelligent Systems, 37(1):21–24, 2022. 2\n[54] Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,\nMoya Chen, Guillem Cucurull, David Esiobu, Jude Fer-\nnandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia\nGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Vik-\ntor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-\nrenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov,\nand Thomas Scialom. Llama 2: Open foundation and fine-\ntuned chat models, 2023. 1, 6\n[55] Svenja\nUhlemeyer,\nMatthias\nRottmann,\nand\nHanno\nGottschalk.\nTowards unsupervised open world semantic\nsegmentation. In Uncertainty in Artificial Intelligence, pages\n1981–1991. PMLR, 2022. 3\n[56] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\nGeorgoulis, and Luc Van Gool. Unsupervised semantic seg-\nmentation by contrasting object mask proposals. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 10052–10062, 2021. 3, 6\n[57] Wouter Van Gansbeke, Simon Vandenhende, and Luc\nVan Gool.\nDiscovering object masks with transformers\nfor unsupervised semantic segmentation.\narXiv preprint\narXiv:2206.06363, 2022. 6\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in Neural\nInformation Processing Systems (NeurIPS), 2017. 1\n[59] Jesse Vig and Yonatan Belinkov. Analyzing the structure of\nattention in a transformer language model. arXiv preprint\narXiv:1906.04284, 2019. 1, 2\n[60] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny\nZhou. Chain-of-thought prompting elicits reasoning in large\nlanguage models, 2023. 7\n[61] Yu Xia, Rui Wang, Xu Liu, Mingyan Li, Tong Yu, Xiang\nChen, Julian McAuley, and Shuai Li.\nBeyond chain-of-\nthought: A survey of chain-of-x paradigms for llms, 2024.\n7\n[62] Zhaoyuan Yin, Pichao Wang, Fan Wang, Xianzhe Xu, Han-\nling Zhang, Hao Li, and Rong Jin. Transfgu: a top-down ap-\nproach to fine-grained unsupervised semantic segmentation.\nIn European conference on computer vision, pages 73–89.\nSpringer, 2022. 3, 6\n[63] Matthew D Zeiler and Rob Fergus.\nVisualizing and\nunderstanding convolutional networks.\narXiv preprint\narXiv:1311.2901, 2014. 1, 2\n[64] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Ser-\ncan ¨O Arik, and Tomas Pfister. Nested hierarchical trans-\nformer: Towards accurate, data-efficient and interpretable vi-\nsual understanding. In Proceedings of the AAAI Conference\non Artificial Intelligence, pages 3417–3425, 2022. 3\n[65] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Learning deep features for discrimina-\ntive localization. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2921–2929,\n2016. 2\n[66] Adrian Ziegler and Yuki M Asano. Self-supervised learning\nof object parts for semantic segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14502–14511, 2022. 3, 6\n11",
    "pdf_filename": "ULTra_Unveiling_Latent_Token_Interpretability_in_Transformer_Based_Understanding.pdf"
}