{
    "title": "Different Horses for Different Courses Comparing Bias Mitigation Algorithms in ML",
    "context": "With fairness concerns gaining significant attention in Machine Learning (ML), several bias mitigation techniques have been proposed, often compared against each other to find the best method. These benchmarking efforts tend to use a common setup for evaluation under the assumption that providing a uniform environment ensures a fair comparison. However, bias mitigation techniques are sensitive to hyperparameter choices, random seeds, feature selection, etc., meaning that comparison on just one setting can unfairly favour certain algorithms. In this work, we show significant variance in fairness achieved by several algorithms and the influence of the learning pipeline on fairness scores. We highlight that most bias mitigation techniques can achieve comparable performance, given the freedom to perform hyperparameter optimization, suggesting that the choice of the evaluation parameters—rather than the mitigation technique itself—can sometimes create the perceived superiority of one method over another. We hope our work encourages future research on how various choices in the lifecycle of developing an algorithm impact fairness, and trends that guide the selection of appropriate algorithms. 1 Over the past decade, concerns about fairness and discrimination in Machine Learning (ML) systems have emerged as critical issues, driving extensive research into the development of fair ML practices, including mitigation algorithms and fairness criteria [Mehrabi et al., 2021, Gohar and Cheng, 2023, Barocas et al., 2023]. This has led to emerging global AI regulation focused on mitigating discrimina- tion in AI/ML systems, mandating the reporting of fairness metrics of algorithms in compliance with various anti-discrimination laws such as the disparate impact doctrine [Justice., 2023]. However, despite the regulatory efforts, recent research has increasingly shown that the fair ML pipeline suffers from instability and high variance in fairness measures, which can mask the underlying unfairness while creating an illusion of fairness [Black et al., 2023]. For instance, recent work has pointed out how fairness measures vary across different training runs or between training and deployment, challenging the effectiveness, reliability, and utility of existing methods [Baldini et al., 2021, Black and Fredrikson, 2021, Friedler et al., 2019, Ganesh et al., 2023]. Additionally, the multitude of mitigation techniques and fairness metrics further complicate accurate benchmarking. Therefore, from both a regulatory perspective and best practices, such variances must be taken into account to accurately represent the performance of these systems and fairness intervention methods. ∗Equal contribution Workshop on Algorithmic Fairness through the Lens of Metrics and Evaluation (AFME) at NeurIPS 2024. arXiv:2411.11101v2  [cs.LG]  19 Nov 2024",
    "body": "Different Horses for Different Courses: Comparing\nBias Mitigation Algorithms in ML\nPrakhar Ganesh∗\nMcGill University and Mila\nprakhar.ganesh@mila.quebec\nUsman Gohar∗\nIowa State University\nugohar@iastate.edu\nLu Cheng\nUniversity of Illinois Chicago\nlucheng@uic.edu\nGolnoosh Farnadi\nMcGill University and Mila\nfarnadig@mila.quebec\nAbstract\nWith fairness concerns gaining significant attention in Machine Learning (ML),\nseveral bias mitigation techniques have been proposed, often compared against each\nother to find the best method. These benchmarking efforts tend to use a common\nsetup for evaluation under the assumption that providing a uniform environment\nensures a fair comparison. However, bias mitigation techniques are sensitive\nto hyperparameter choices, random seeds, feature selection, etc., meaning that\ncomparison on just one setting can unfairly favour certain algorithms. In this work,\nwe show significant variance in fairness achieved by several algorithms and the\ninfluence of the learning pipeline on fairness scores. We highlight that most bias\nmitigation techniques can achieve comparable performance, given the freedom to\nperform hyperparameter optimization, suggesting that the choice of the evaluation\nparameters—rather than the mitigation technique itself—can sometimes create the\nperceived superiority of one method over another. We hope our work encourages\nfuture research on how various choices in the lifecycle of developing an algorithm\nimpact fairness, and trends that guide the selection of appropriate algorithms.\n1\nIntroduction\nOver the past decade, concerns about fairness and discrimination in Machine Learning (ML) systems\nhave emerged as critical issues, driving extensive research into the development of fair ML practices,\nincluding mitigation algorithms and fairness criteria [Mehrabi et al., 2021, Gohar and Cheng, 2023,\nBarocas et al., 2023]. This has led to emerging global AI regulation focused on mitigating discrimina-\ntion in AI/ML systems, mandating the reporting of fairness metrics of algorithms in compliance with\nvarious anti-discrimination laws such as the disparate impact doctrine [Justice., 2023].\nHowever, despite the regulatory efforts, recent research has increasingly shown that the fair ML\npipeline suffers from instability and high variance in fairness measures, which can mask the underlying\nunfairness while creating an illusion of fairness [Black et al., 2023]. For instance, recent work has\npointed out how fairness measures vary across different training runs or between training and\ndeployment, challenging the effectiveness, reliability, and utility of existing methods [Baldini et al.,\n2021, Black and Fredrikson, 2021, Friedler et al., 2019, Ganesh et al., 2023]. Additionally, the\nmultitude of mitigation techniques and fairness metrics further complicate accurate benchmarking.\nTherefore, from both a regulatory perspective and best practices, such variances must be taken into\naccount to accurately represent the performance of these systems and fairness intervention methods.\n∗Equal contribution\nWorkshop on Algorithmic Fairness through the Lens of Metrics and Evaluation (AFME) at NeurIPS 2024.\narXiv:2411.11101v2  [cs.LG]  19 Nov 2024\n\nLow bias\nHigh bias\nHyperparameter Setting 1\nA1\nA2\nHyperparameter Setting 2\nA1\nA3\nA2\nHyperparameter Setting 3\nA3\nA2\nHyperparameter Setting 4\nA1\nA3\nA2\n…….\nWhat is the best method if the \ndeveloper is constrained to \nhyperparameter setting 3?\nA1\nWhat is the best method if the \ndeveloper does not know the \nhyperparameter setting in \nadvance, and thus wants the \nmost robust technique?\nWhat is the best method if the \ndeveloper can use any \nhyperparameter?\nWhat is the best method if …\n?\nA3\nA3\nA1\nA2\nNo method is the “best” method in every context!\nFigure 1: Motivation behind a more nuanced and context-aware benchmarking of bias mitigation\ntechniques, instead of using a uniform evaluation setup or attempting to find the \"best\" technique.\nWhile recent works have highlighted the issue of variance in fairness, existing fairness benchmarks\npredominately operate under a single identical training environment (e.g., hyperparameters, random\nseed, etc.) to ensure more accurate and fair comparisons [Han et al., 2023]. However, this fails\nto consider the sensitivity of fairness to hyperparameter choices, which may mask the nuances of\nvarious bias mitigation techniques, favoring one over another [Dooley et al., 2024], as illustrated in\nFigure 1. We postulate that after accounting for the variance in fairness due to the hyperparameter\nchoice, most bias mitigation methods achieve comparable performance. This further raises important\nquestions about the one-dimensional nature of existing fairness evaluations. Is the \"best\" model\nsimply the one that performs optimally under specific hyperparameter configurations, or should\nfairness assessments take a more holistic approach? For instance, how do we account for trade-offs\nbetween fairness, interpretability, stability, and resource constraints? Should fairness evaluations\nprioritize consistency over best performance, or should context-specific factors like deployment\nenvironments and real-world implications dictate the criteria for success? These questions highlight\nthe need for more nuanced and multidimensional fairness benchmarks beyond traditional measures.\nContributions of our work. We show that bias mitigation algorithms are highly sensitive to several\nchoices made in the learning pipeline. Consequently, without incorporating the broader context, a\none-dimensional comparative analysis of these algorithms can create a false sense of fairness. We\nhighlight that most mitigation algorithms can achieve comparable performance under appropriate\nhyperparameter optimization and, therefore, advocate going beyond the narrow view of the fairness-\nutility tradeoff to explore other factors that impact model deployment. We conduct a large-scale\nempirical analysis to support our claims. We hope our work inspires future research that explores the\ninterplay between bias mitigation algorithms and the entire learning pipeline, rather than studying\nthem in isolation, as has often been the case in the literature.\n2\nRelated Work\nResearchers have developed a range of mitigation techniques and notions to address unfairness in\nML systems, targeting different stages of the ML pipeline, including pre-processing, in-processing,\nand post-processing methods [Mehrabi et al., 2021, Pessach and Shmueli, 2022, Gohar et al., 2024].\nPre-processing balances data distribution across protected groups, reducing variance, while post-\nprocessing modifies model outputs without accessing internal algorithms, ideal for black-box models.\nIn contrast, in-processing methods impose fairness constraints on the model or modify the objective\nfunction to mitigate bias. There is inherent randomness in the training process, which, while vital for\nconvergence and generalization [Noh et al., 2017], can be a source of high-fairness variance. Moreover,\nthese techniques also have control parameters (for example, regularization weight) that must be\noptimized for the training data, further introducing variance [Bottou, 2012]. In this work, we limit our\nfocus on in-processing techniques due to the high variance exhibited by these methods [Baldini et al.,\n2021, Black and Fredrikson, 2021, Friedler et al., 2019, Ganesh et al., 2023, Perrone et al., 2021].\n2\n\nThere is increasing evidence in the literature of the instability of fairness metrics associated with non-\ndeterminism in model training and decisions [Black et al., 2022, Baldini et al., 2021, Friedler et al.,\n2019]. This includes identical training environments with small changes such as different random\nseeds [Black and Fredrikson, 2021], sampling [Ganesh et al., 2023], hyperparameter choices [Ganesh,\n2024, Gohar et al., 2023], or even differences in train-test-split [Friedler et al., 2019], which have been\nshown to lead to meaningful differences in group fairness performance across different runs. The issue\narises when this instability is used to report higher fairness performance, which calls into question the\neffectiveness of fairness assessments [Black et al., 2024]. A few recent works have explored this issue\nfrom a regulatory fairness assessment standpoint where, inadvertently or deliberately, an actor can\nmisrepresent the fairness performance. For instance, a parallel work by Simson et al. [2024] proposes\nusing multiverse analysis to keep track of all possible decision combinations for data processing and\nits impact on model fairness. We build on a similar argument, focusing instead on decisions made\nduring the algorithm design, highlighting the instability of various fairness intervention methods.\nSeveral previous attempts have been made to benchmark bias mitigation algorithms [Bellamy et al.,\n2019, Bird et al., 2020, Han et al., 2023], including those conducted by papers proposing new\nmitigation techniques [Kamishima et al., 2012, Li et al., 2022, Madras et al., 2018, Adel et al., 2019,\nZhang et al., 2018]. As highlighted by Han et al. [2023], every paper that proposes a new bias\nmitigation algorithm often introduces its own experimental setup. This lack of standardization can\nmake it difficult to compare different algorithms effectively. To overcome this issue, Han et al. [2023]\ncreated the FFB benchmark, offering a comprehensive evaluation over a wide range of datasets\nand algorithms to allow fair comparison. While FFB provides an excellent foundation to compare\nand benchmark bias mitigation algorithms, its attempts to find an algorithm that provides the best\nfairness-utility tradeoff (which they claim is HSIC) comes with two important caveats, (a) FFB only\nconsiders a single hyperparameter setting, which, while useful for standardization, overlooks the\nsensitivity of fairness scores to hyperparameter choices [Perrone et al., 2021], and (b) FFB aggregates\nresults across multiple datasets, which can obscure the nuances of performance on individual datasets.\nIn our work, we demonstrate that the fairness variability due to hyperparameter choices can often\nmask unfairness and raise concerns about existing evaluation techniques.\n3\nVariance in Bias Mitigation\nIn this section, we argue that benchmarking under different settings can reveal trends that are lost\nwhen sticking to only a single standardized hyperparameter setting or aggregating results across\nmultiple datasets, as done by Han et al. [2023]. Thus, a more nuanced approach to benchmarking\nbias mitigation techniques is needed to capture the strengths and limitations of various algorithms.\n3.1\nExperiment setup\nFor our experiments, we borrow from Han et al. [2023], using their open-source code 2. We focus\non the seven tabular datasets and the seven bias mitigation algorithms used in their benchmark (not\nincluding the standard empirical risk minimization without fairness constraints). In addition to\nvarying the random seed for training and the control parameter for bias mitigation as done in the\nbenchmark, we also vary the batch size, learning rate, and model architecture to explore several\ndifferent hyperparameter settings. More details on the experiment setup are delegated to Appendix A.\n3.2\nCase Study: Adult Dataset\nWe start with a case study on the Adult dataset [Becker and Kohavi, 1996] and show the changing\ntrends across different hyperparameters. We plot fairness as demographic parity and utility as accuracy,\nstudying the fairness-utility tradeoff across various settings in Figure 2. Additional discussions on\nother datasets and fairness metrics are present in Appendix B.\nWe first observe the absence of a clear winner among bias mitigation algorithms; no single method\nconsistently and significantly outperforms the others. While HSIC achieves better tradeoffs under\nthe hyperparameter setting used by Han et al. [2023], other techniques such as PRemover and\nDiffDP perform equally well – or sometimes even better – under other hyperparameter settings.\n2https://github.com/ahxt/fair_fairness_benchmark\n3\n\nFigure 2: Fairness-utility (demographic parity-accuracy) tradeoff across various settings for the Adult\ndataset. Each graph represents a different combination of hyperparameters, and each dot in the graph\nrepresents a separate training run. Multiple dots for the same mitigation algorithm in the same graph\nrepresent runs with changing random seeds and control parameters.\nThus, a comparative analysis limited to just one combination of hyperparameters fails to capture the\ncompetitive performance of other algorithms.\nAnother interesting set of results comes from the hyperparameter setting with a large batch size and\na high learning rate. These settings are particularly relevant in scenarios where the emphasis is on\nrapid convergence and minimizing the number of training steps, even at the cost of performance, for\ninstance, during rapid prototyping, edge computing with limited compute cycles or federated learning.\nMost bias mitigation methods that performed well in other settings fail to even converge under these\nconditions. Instead, methods like AdvDebias and LAFTR, which did not stand out in other settings,\nprovide good fairness scores when trained under these constraints.\nOur findings highlight the importance of considering a diverse range of choices in the learning pipeline\nwhen evaluating bias mitigation techniques, as different algorithms excel under different settings.\nFurther interesting trends can be extracted from the comparative analysis of various algorithms\nacross different hyperparameter settings, left for future work. We now turn to a similarly nuanced\ncomparative analysis of bias mitigation techniques, focusing on changing trends across datasets.\n3.3\nChanging Trends Across Datasets\nIn the previous section, we observed different techniques perform better than others under changing\nsettings within a single dataset. We now extend this observation to multiple datasets to show that\neven the trends across different datasets can vary significantly, and the choice of combining results\nfrom all datasets, as done by Han et al. [2023], can obscure these trends. We record the fairness\n(demographic parity) and utility (accuracy) across different datasets and bias mitigation algorithms in\nFigure 3. Results for other fairness metrics are present in Appendix C.\nWe begin again with HSIC, which Han et al. [2023] identifies as the algorithm offering the best\ntradeoff overall. While HSIC does well on most datasets, it is noticeably not the unanimous top\nchoice for both the COMPAS and German datasets. Interestingly, both of these are small datasets\nwith smaller batch sizes used for training. Given that HSIC relies on pairwise similarity in a batch\nto estimate dependence, its superiority with larger batch sizes but its lackluster performance with\nsmaller batch sizes is not surprising. Most of the datasets analyzed by Han et al. [2023] were big\ndatasets with large batch sizes. Thus, combining results from multiple datasets overshadowed the\ntrends present only in smaller datasets, effectively hiding HSIC’s shortcomings.\n4\n\nFigure 3: Fairness-utility (demographic parity-accuracy) tradeoff across various datasets, under their\ndefault hyperparameters. Each dot in the graph represents a separate training run with changing\nrandom seeds and control parameters.\nAnother intriguing trend is present in the performance of LAFTR, the adversarial representation\nlearning-based bias mitigation technique. Generally, LAFTR underperforms across various datasets,\noffering poor tradeoffs. However, it performs surprisingly well on the COMPAS dataset, providing\ncompetitive tradeoffs against other techniques. This anomaly may be linked to the data preprocessing\nstep adopted by Han et al. [2023], where all categorical features are converted into one-hot encodings.\nSince LAFTR relies on representation learning at its core, this explosion in the number of features\ncan make the representation learning task more difficult, thus hurting the eventual mitigation attempts.\nNotably, the COMPAS dataset, where LAFTR excels, contains the least number of categorical\nfeatures and, consequently, the smallest input size compared to other datasets. Thus, LAFTR’s\nunderperformance might not be simply due to the algorithm itself but rather the choice of input\nfeature representation used.\nFinally, we also observe fairness metric-specific trends that affect the comparative analysis between\ndifferent algorithms. For many datasets like Adult, COMPAS, and KDD, there is a clear tradeoff\nbetween demographic parity and accuracy, which isn’t surprising since demographic parity is not\naligned with accuracy. In these datasets, we find different mitigation techniques occupy distinct\npositions in the tradeoff curves. Thus, the choice of the mitigation technique, therefore, depends on\nthe stakeholders’ level of willingness to trade utility for fairness. Conversely, many other datasets do\nnot have these apparent tradeoffs. Here, we find that the results are mixed, with highly overlapping\ntrends across different mitigation algorithms.\n4\nComparisons Beyond the Fairness-Utility Tradeoff\nIn the previous section, we discussed the limitations of a generic comparative analysis of bias\nmitigation algorithms that don’t take into account the nuances of the entire learning process. Building\non this, we now turn to a practical concern: choosing the appropriate algorithm.\nWe begin this section by first showing that given the opportunity to perform hyperparameter opti-\nmization, various mitigation algorithms can provide competitive models. We then discuss how, given\nthe lack of appropriate differentiation between these algorithms in their fairness-utility tradeoff, the\nselection of the appropriate algorithm can prioritize other factors, like algorithm runtime, complexity,\npotential robustness, theoretical guarantees, etc. Consequently, selecting the appropriate algorithm\nand the resulting model would involve balancing these additional considerations, rather than solely\nfocusing on just the best fairness-utility tradeoff.\n5\n\nFigure 4: Pareto front of the fairness-utility (demographic parity-accuracy) tradeoff across various\ndatasets. Each dot in the graph represents a separate training run on the pareto front with changing\nhyperparameters, random seeds and control parameters.\n4.1\nMost Mitigation Algorithms are Competitive\nAs we saw in Figure 2 (and Appendix B), different algorithms do well under varying settings. In\nseveral real-world applications, many of these choices are flexible, and hyperparameter optimization\nplays an important role in model selection. Thus, when comparing different algorithms, it is important\nto focus on evaluating the best-performing models from each algorithm, as these are the models that\nwould be deployed if those algorithms were used.\nTo perform this comparison, we only filter the models at the Pareto front for various algorithms after\nsearching through different hyperparameters and random seeds collected in Figure 4. Trends for other\nfairness metrics are present in Appendix D. We find that several algorithms can provide competitive\ntradeoffs for almost every dataset. For instance, DiffDP, PRemover, and HSIC demonstrate excellent\nfairness-utility tradeoffs for the Adult dataset, while all seven bias mitigation algorithms exhibit\ncompetitive tradeoffs on the German dataset. With multiple algorithms showing similar tradeoffs, it\nbecomes evident that simply evaluating fairness-utility tradeoffs is insufficient when choosing the\nmost suitable bias mitigation technique. We explore these considerations further in the next section.\n4.2\nChoosing the Right Mitigation Technique\nWhen several bias mitigation algorithms provide similar tradeoffs, selecting one can be challenging.\nIn such cases, additional factors must be considered, such as the specific requirements of the task, the\ndeployment environment, the stakeholders’ expectations, etc. Here, we provide some examples of\ncomparisons beyond the fairness-utility tradeoff that can help choose an appropriate algorithm.\nRuntime:\nAn algorithm’s runtime can be a crucial factor when comparing bias mitigation tech-\nniques. Even minor differences in runtime might become relevant when multiple runs of the same\nalgorithm are needed, for instance, to perform hyperparameter optimization. Our results, detailed\nin Table 1, reveal interesting trends in training runtime across various algorithms. We find the\nalgorithms HSIC, LAFTR, PRemover, DiffEOdd, and DiffEOpp to be quite expensive, while in\ncontrast, algorithms DiffDP and AdvDebias offer runtime comparable to the standard empirical risk\nminimization. Considering the competitive tradeoffs achieved by DiffDP, in addition to the lower\nruntime, it emerges as an appropriate choice for settings where computational efficiency is critical,\nsurpassing other well-performing but slower methods like HSIC and PRemover.\n6\n\nDataset\nRuntime (rounded to 5s intervals)\nERM\nDiffDP\nDiffEOdd\nDiffEOpp\nHSIC\nLAFTR\nPRemover\nAdvDebias\nBank\n1m 15s\n1m 15s\n1m 45s\n1m 45s\n1m 50s\n1m 46s\n1m 50s\n1m 25s\nGerman\n30s\n30s\n35s\n35s\n40s\n35s\n40s\n30s\nAdult\n1m 40s\n1m 40s\n1m 45s\n1m 45s\n2m 0s\n1m 50s\n2m 0s\n1m 40s\nCOMPAS\n30s\n30s\n30s\n30s\n30s\n30s\n30s\n30s\nKDDCensus\n6m 45s\n6m 50s\n10m 40s\n10m 40s\n10m 5s\n10m 0s\n9m 50s\n6m 50s\nACS-I\n7m 10s\n7m 10s\n9m 50s\n9m 50s\n9m 50s\n9m 30s\n10m 0s\n7m 50s\nACS-E\n13m 40s\n13m 45s\n15m 40s\n15m 40s\n16m 20s\n15m 50s\n16m 10s\n13m 40s\nACS-P\n7m 20s\n7m 35s\n9m 50s\n9m 50s\n9m 40s\n10m 5s\n10m 5s\n7m 40s\nACS-M\n4m 40s\n4m 45s\n6m 5s\n6m 5s\n6m 0s\n6m 10s\n6m 0s\n4m 50s\nACS-T\n7m 30s\n7m 30s\n10m 5s\n10m 5s\n10m 15s\n10m 0s\n10m 20s\n8m 0s\nTable 1: Training runtime of mitigation algorithms across datasets, under default hyperparameters.\nTheoretical Guarantees and Procedural Requirements:\nAnother important consideration when\nselecting the appropriate algorithm is the theoretical guarantees that some techniques can offer. For\ninstance, while adding regularizers to the training objective can be useful, it does not provide any\nform of guarantee for the model’s final fairness scores. In contrast, methods like HSIC and LAFTR\ncan provide theoretical bounds on the fairness of the final model, albeit limited to only simpler\nmodels [Li et al., 2022, Madras et al., 2018].\nFurthermore, the deployed models may need to comply with specific procedural requirements, which\ncan influence the choice of the mitigation algorithm. For instance, one might need to choose between\nalgorithms focusing on outcome fairness (such as DiffDP, DiffEOpp, DiffEOdd) versus those focusing\non process fairness (such as HSIC, LAFTR, PRemover, AdvDebias). The specific requirements of\nthe application can dictate the choice of the algorithm, looking beyond the tradeoffs it can provide.\nMultiplicity and Arbitrariness:\nModel multiplicity refers to the existence of a set of good models,\nwhich have similar performance but differ in their predictions for individuals [Marx et al., 2020, Black\net al., 2022]. Existing works have shown that bias mitigation can exacerbate multiplicity concerns,\nleading to arbitrariness in individual-level predictions [Long et al., 2024]. However, the degree of\nmultiplicity introduced can vary depending on the mitigation algorithm used. Following Long et al.\n[2024], we define the set of competing models as models with similar accuracy under ERM and\nrecord multiplicity using ambiguity [Marx et al., 2020], which is the fraction of data points whose\npredictions change across different models within the set of good models, in Table 2.\nDataset\nAmbiguity\nERM\nDiffDP\nDiffEOdd\nDiffEOpp\nHSIC\nLAFTR\nPRemover\nAdvDebias\nBank\n0.15\n0.16\n0.16\n0.18\n0.17\n0.19\n0.15\n0.26\nGerman\n0.55\n0.57\n0.57\n0.63\n0.55\n0.59\n0.60\n0.87\nAdult\n0.17\n0.30\n0.37\n0.42\n0.28\n0.32\n0.34\n0.47\nCOMPAS\n0.93\n0.99\n1.0\n1.0\n1.0\n1.0\n0.92\n0.99\nKDDCensus\n0.04\n0.06\n0.04\n0.06\n0.05\n0.06\n0.04\n0.09\nACS-I\n0.26\n0.35\n0.38\n0.35\n0.38\n0.32\n0.75\n0.49\nACS-E\n0.14\n0.20\n0.30\n0.21\n0.26\n0.20\n0.49\n0.37\nACS-P\n0.27\n0.33\n0.38\n0.39\n0.34\n0.45\n0.32\n0.69\nACS-M\n0.26\n0.29\n0.27\n0.29\n0.31\n0.38\n0.20\n0.61\nACS-T\n0.70\n0.81\n0.88\n0.80\n0.79\n0.91\n0.92\n0.90\nTable 2: Ambiguity scores of mitigation algorithms across datasets, under default hyperparameters.\nUnsurprisingly, most bias mitigation techniques exhibit higher ambiguity than ERM, which aligns\nwith the observations made by Long et al. [2024]. However, an interesting exception is the PRemover\nalgorithm, which achieves remarkably low ambiguity scores across many datasets, distinguishing\nit from other algorithms. Strikingly, at the same time, PRemover also shows significantly high\nambiguity in several other datasets, highlighting its behavior on both extremes. Thus, for certain\ndatasets, PRemover could be considered a superior choice compared to other methods like HSIC and\n7\n\nDiffDP, which, while offering similar trade-offs, tend to introduce more arbitrariness into the model.\nIn contrast to PRemover, the AdvDebias algorithm consistently results in very high ambiguity scores,\nmaking it a poor choice in contexts where minimizing arbitrariness is crucial.\nIn this section, we showed several examples of additional factors to consider when selecting an\nalgorithm for a specific use case. Naturally, this list is not exhaustive, as additional considerations\nmay arise depending on the specific application context. The objective of our study was to emphasize\nthe lack of distinction between mitigation algorithms that focus solely on the fairness-utility tradeoff\nand the importance of choosing algorithms that offer additional advantages beyond this tradeoff. With\nthese results, we hope to move away from the narrative of a single optimal bias mitigation technique\nand emphasize the need for context-dependent comparative analysis.\n5\nDiscussion\nIn this paper, we underscore the limitations of current fairness benchmarking practices that rely\non uniform evaluation setups. We demonstrate that hyperparameter optimization can yield similar\nperformance across different bias mitigation techniques, raising questions about the effectiveness of\nexisting benchmarks and the criteria for selecting appropriate fairness algorithms.\nContext-dependent evaluation. We argue that the current one-dimensional approach to fairness\nevaluation may be insufficient. Given the high variability in fairness scores, relying on a single run\nor, conversely, simply aggregating multiple training runs, both common practices across different\ndimensions, may not always provide an appropriate comparison of bias mitigation techniques.\nFor example, when models are too large and retraining is impractical, choosing fairness interventions\nthat prioritize stability and consistent scores may be more appropriate. On the other hand, if sufficient\ncomputational resources exist to explore hyperparameter options, selecting the best-performing\nmodel might be more valid. Additionally, explainability, runtime, and scalability constraints can\nsignificantly impact the choice of fairness assessments. Ultimately, the method of comparing\nalgorithms depends on the context. However, in all cases, it is crucial to consider the variability\nintroduced by hyperparameter tuning.\nFuture work. Our experiments were limited to in-processing techniques in bias mitigation. In the\nfuture, we plan to explore a broader range of methods, including pre and post-processing. Moreover,\nwe have not explored the potential presence of consistent fairness trends for different hyperparameter\nchoices covered in the experiments. It would be interesting to investigate whether we can identify\npatterns that guide our decisions to choose better hyperparameter settings for various bias mitigation\nalgorithms. Finally, while evidence in the literature would suggest similar trends exist even with\nhyperparameters in other parts of the pipeline, for instance, data processing [Simson et al., 2024], our\nempirical results are limited to hyperparameter choices during training. Further work on a large-scale\nstudy of the impact of various choices in the lifetime of an algorithm design is needed.\nAcknowledgments and Disclosure of Funding\nFunding support for project activities has been partially provided by the Canada CIFAR AI Chair,\nFRQNT scholarship, and NSERC discovery award. We also express our gratitude to Compute Canada\nand Mila clusters for their support in providing facilities for our evaluations. Lu Cheng is supported\nby the National Science Foundation (NSF) Grant #2312862, NIH #R01AG091762, and a Cisco gift\ngrant.\nReferences\nCensus-Income\n(KDD).\nUCI\nMachine\nLearning\nRepository,\n2000.\nDOI:\nhttps://doi.org/10.24432/C5N30T.\nT. Adel, I. Valera, Z. Ghahramani, and A. Weller. One-network adversarial fairness. In Proceedings\nof the AAAI Conference on Artificial Intelligence, volume 33, pages 2412–2420, 2019.\nS. Baharlouei, M. Nouiehed, A. Beirami, and M. Razaviyayn. Rényi fair inference. In International\nConference on Learning Representations.\n8\n\nI. Baldini, D. Wei, K. N. Ramamurthy, M. Yurochkin, and M. Singh. Your fairness may vary:\nPretrained language model fairness in toxic text classification. arXiv preprint arXiv:2108.01250,\n2021.\nS. Barocas, M. Hardt, and A. Narayanan. Fairness and machine learning: Limitations and opportuni-\nties. MIT Press, 2023.\nB. Becker and R. Kohavi.\nAdult.\nUCI Machine Learning Repository, 1996.\nDOI:\nhttps://doi.org/10.24432/C5XW20.\nR. K. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta,\nA. Mojsilovi´c, et al. Ai fairness 360: An extensible toolkit for detecting and mitigating algorithmic\nbias. IBM Journal of Research and Development, 63(4/5):4–1, 2019.\nA. Beutel, J. Chen, Z. Zhao, and E. H. Chi. Data decisions and theoretical implications when\nadversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.\nS. Bird, M. Dudík, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach, and K. Walker.\nFairlearn: A toolkit for assessing and improving fairness in ai. Microsoft, Tech. Rep. MSR-TR-\n2020-32, 2020.\nE. Black and M. Fredrikson. Leave-one-out unfairness. In Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency, pages 285–295, 2021.\nE. Black, M. Raghavan, and S. Barocas. Model multiplicity: Opportunities, concerns, and solutions.\nIn Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages\n850–863, 2022.\nE. Black, R. Naidu, R. Ghani, K. Rodolfa, D. Ho, and H. Heidari. Toward operationalizing pipeline-\naware ml fairness: A research agenda for developing practical guidelines and tools. In Proceedings\nof the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization,\npages 1–11, 2023.\nE. Black, T. Gillis, and Z. Y. Hall. D-hacking. In The 2024 ACM Conference on Fairness, Account-\nability, and Transparency, pages 602–615, 2024.\nL. Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade: Second\nEdition, pages 421–436. Springer, 2012.\nF. Ding, M. Hardt, J. Miller, and L. Schmidt. Retiring adult: New datasets for fair machine learning.\nAdvances in neural information processing systems, 34:6478–6490, 2021.\nS. Dooley, R. Sukthanker, J. Dickerson, C. White, F. Hutter, and M. Goldblum. Rethinking bias\nmitigation: Fairer architectures make for fairer face recognition. Advances in Neural Information\nProcessing Systems, 36, 2024.\nH. Edwards and A. Storkey. Censoring representations with an adversary. In 4th International\nConference on Learning Representations, pages 1–14, 2016.\nS. A. Friedler, C. Scheidegger, S. Venkatasubramanian, S. Choudhary, E. P. Hamilton, and D. Roth.\nA comparative study of fairness-enhancing interventions in machine learning. In Proceedings of\nthe conference on fairness, accountability, and transparency, pages 329–338, 2019.\nP. Ganesh. An empirical investigation into benchmarking model multiplicity for trustworthy machine\nlearning: A case study on image classification. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision, pages 4488–4497, 2024.\nP. Ganesh, H. Chang, M. Strobel, and R. Shokri. On the impact of machine learning randomness\non group fairness. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and\nTransparency, pages 1789–1800, 2023.\nU. Gohar and L. Cheng. A survey on intersectional fairness in machine learning: Notions, mitigation,\nand challenges. arXiv preprint arXiv:2305.06969, 2023.\n9\n\nU. Gohar, S. Biswas, and H. Rajan. Towards understanding fairness and its composition in ensemble\nmachine learning. In 2023 IEEE/ACM 45th International Conference on Software Engineering\n(ICSE), pages 1533–1545. IEEE, 2023.\nU. Gohar, Z. Tang, J. Wang, K. Zhang, P. L. Spirtes, Y. Liu, and L. Cheng. Long-term fairness\ninquiries and pursuits in machine learning: A survey of notions, methods, and challenges. arXiv\npreprint arXiv:2406.06736, 2024.\nA. Gretton, O. Bousquet, A. Smola, and B. Schölkopf. Measuring statistical dependence with\nhilbert-schmidt norms. In International conference on algorithmic learning theory, pages 63–77.\nSpringer, 2005.\nX. Han, J. Chi, Y. Chen, Q. Wang, H. Zhao, N. Zou, and X. Hu. Ffb: A fair fairness benchmark for\nin-processing group fairness methods. In International Conference on Learning Representations.\nICLR, 2023.\nH. Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI:\nhttps://doi.org/10.24432/C5NC77.\nU. S. D. O. Justice. Title vi legal manual, section vii: Proving discrimination – disparate impact., Oct\n2023. URL https://www.justice.gov/crt/fcs/T6Manual7.\nT. Kamishima, S. Akaho, H. Asoh, and J. Sakuma. Fairness-aware classifier with prejudice remover\nregularizer. In Machine Learning and Knowledge Discovery in Databases: European Conference,\nECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23, pages 35–50.\nSpringer, 2012.\nJ. Larson, S. Mattu, L. Kirchner, and J. Angwin. Propublica compas analysis—data and analysis for\n‘machine bias.’. https://github.com/propublica/compas-analysis, 2016.\nZ. Li, A. Pérez-Suay, G. Camps-Valls, and D. Sejdinovic. Kernel dependence regularizers and\ngaussian processes with applications to algorithmic fairness. Pattern Recognition, 132:108922,\n2022.\nC. Long, H. Hsu, W. Alghamdi, and F. Calmon. Individual arbitrariness and group fairness. Advances\nin Neural Information Processing Systems, 36, 2024.\nG. Louppe, M. Kagan, and K. Cranmer. Learning to pivot with adversarial networks. Advances in\nneural information processing systems, 30, 2017.\nD. Madras, E. Creager, T. Pitassi, and R. Zemel. Learning adversarially fair and transferable\nrepresentations. In International Conference on Machine Learning, pages 3384–3393. PMLR,\n2018.\nC. Marx, F. Calmon, and B. Ustun. Predictive multiplicity in classification. In International\nConference on Machine Learning, pages 6765–6774. PMLR, 2020.\nN. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan. A survey on bias and fairness in\nmachine learning. ACM computing surveys (CSUR), 54(6):1–35, 2021.\nS. Moro, P. Rita, and P. Cortez. Bank Marketing. UCI Machine Learning Repository, 2014. DOI:\nhttps://doi.org/10.24432/C5K306.\nH. Noh, T. You, J. Mun, and B. Han. Regularizing deep neural networks by noise: Its interpretation\nand optimization. Advances in neural information processing systems, 30, 2017.\nV. Perrone, M. Donini, M. B. Zafar, R. Schmucker, K. Kenthapadi, and C. Archambeau. Fair bayesian\noptimization. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages\n854–863, 2021.\nD. Pessach and E. Shmueli. A review on fairness in machine learning. ACM Computing Surveys\n(CSUR), 55(3):1–44, 2022.\n10\n\nJ. Simson, F. Pfisterer, and C. Kern. One model many scores: Using multiverse analysis to prevent\nfairness hacking and evaluate the influence of model design decisions. In The 2024 ACM Conference\non Fairness, Accountability, and Transparency, pages 1305–1320, 2024.\nB. H. Zhang, B. Lemoine, and M. Mitchell. Mitigating unwanted biases with adversarial learning. In\nProceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 335–340, 2018.\n11\n\nA\nAdditional Details on Experiment Setup\nAs we directly borrow the experiment setup from Han et al. [2023], we redirect the reader to their\nwork and the FFB benchmark code 3 for details on the underlying setup. In this section, we briefly\nmention the datasets and algorithms used in the benchmark, and the new additions and changes we\nmade to their setup.\nA.1\nDatasets\nWe use 7 different tabular datasets for our experiments. This includes the Adult dataset [Becker and\nKohavi, 1996], COMPAS dataset [Larson et al., 2016], German dataset [Hofmann, 1994], Bank\nMarketing dataset [Moro et al., 2014], KDD Census dataset [cen, 2000], and ACS dataset with tasks\nIncome and Employment [Ding et al., 2021]. We use the sensitive attribute Race for all datasets,\nexcept the Bank Marketing dataset and the German dataset, where we use Age as the sensitive\nattribute.\nA.2\nBias Mitigation Algorithms\nWe use 7 different bias mitigation algorithms in our setup. This includes DiffDP, DiffEOdd, DiffEOpp,\nPRemover [Kamishima et al., 2012], HSIC [Baharlouei et al., Gretton et al., 2005, Li et al., 2022],\nAdvDebias [Adel et al., 2019, Beutel et al., 2017, Edwards and Storkey, 2016, Louppe et al., 2017,\nZhang et al., 2018], and LAFTR [Madras et al., 2018].\nA.3\nHyperparameters\nWe use the Adam optimizer, with no weight decay and a step learning rate scheduler for training. We\ntrain the model for 150 epochs and record the fairness and accuracy scores at the final epoch.\nWe use three different values of the control parameter for each algorithm, as defined in Table 3.\nAlgorithm\nControl Hyperparameter\nDiffDP\n0.2, 1.0, 1.8\nDiffEOdd\n0.2, 1.0, 1.8\nDiffEOdd\n0.2, 1.0, 1.8\nPRemover\n0.05, 0.25, 0.45\nHSIC\n50, 250, 450\nAdvDebias\n0.2, 1.0, 1.8\nLAFTR\n0.1, 0.5, 4.0\nTable 3: Control hyperparameters.\nWe use seven different hyperparameter settings for each dataset, as defined in Table 4.\nB\nAdditional Results for Trends Under Changing Hyperparameters\nWe present additional results for comparing trends under different hyperparameters in the Adult\ndataset for fairness definitions of equalized odds (Figure 5) and equal opportunity (Figure 6). We\nalso present additional results for comparing trends in other datasets like Bank Marketing dataset\n(Figure 7), COMPAS dataset (Figure 8), German dataset (Figure 9), KDDCensus dataset (Figure 10),\nACS-Income dataset (Figure 11) and ACS-Employment dataset (Figure 12).\nC\nAdditional Results for Changing Trends Across Datasets\nWe present additional results for comparing trends across multiple datasets, under fairness definition\nas equalized odds (Figure 13) and equal opportunity (Figure 14). Similar to the observations in\nthe main paper, we find distinct trends across different datasets and no clear single bias mitigation\nalgorithm that excels across all datasets.\n3https://github.com/ahxt/fair_fairness_benchmark\n12\n\nFigure 5: Fairness-utility (equalized odds-accuracy) tradeoff across various settings for the Adult\ndataset. Each graph represents a different combination of hyperparameters, and each dot in the graph\nrepresents a separate training run. Multiple dots for the same mitigation algorithm in the same graph\nrepresent runs with changing random seeds and control parameters.\nFigure 6: Fairness-utility (equal opportunity-accuracy) tradeoff across various settings for the Adult\ndataset. Each graph represents a different combination of hyperparameters, and each dot in the graph\nrepresents a separate training run. Multiple dots for the same mitigation algorithm in the same graph\nrepresent runs with changing random seeds and control parameters.\n13\n\nFigure 7: Fairness-utility (demographic parity-accuracy) tradeoff across various settings for the Bank\nMarketing dataset. Each graph represents a different combination of hyperparameters, and each dot\nin the graph represents a separate training run. Multiple dots for the same mitigation algorithm in the\nsame graph represent runs with changing random seeds and control parameters.\nFigure 8: Fairness-utility (demographic parity-accuracy) tradeoff across various settings for the\nCOMPAS dataset. Each graph represents a different combination of hyperparameters, and each dot\nin the graph represents a separate training run. Multiple dots for the same mitigation algorithm in the\nsame graph represent runs with changing random seeds and control parameters.\n14\n\nFigure 9: Fairness-utility (demographic parity-accuracy) tradeoff across various settings for the\nGerman dataset. Each graph represents a different combination of hyperparameters, and each dot in\nthe graph represents a separate training run. Multiple dots for the same mitigation algorithm in the\nsame graph represent runs with changing random seeds and control parameters.\nFigure 10: Fairness-utility (demographic parity-accuracy) tradeoff across various settings for the\nKDDCensus dataset. Each graph represents a different combination of hyperparameters, and each\ndot in the graph represents a separate training run. Multiple dots for the same mitigation algorithm in\nthe same graph represent runs with changing random seeds and control parameters.\n15\n\nFigure 11: Fairness-utility (demographic parity-accuracy) tradeoff across various settings for the\nACS-Income dataset. Each graph represents a different combination of hyperparameters, and each\ndot in the graph represents a separate training run. Multiple dots for the same mitigation algorithm in\nthe same graph represent runs with changing random seeds and control parameters.\nFigure 12: Fairness-utility (demographic parity-accuracy) tradeoff across various settings for the\nACS-Employment dataset. Each graph represents a different combination of hyperparameters, and\neach dot in the graph represents a separate training run. Multiple dots for the same mitigation\nalgorithm in the same graph represent runs with changing random seeds and control parameters.\n16\n\nFigure 13: Fairness-utility (equalized odds-accuracy) tradeoff across various datasets, under their\ndefault hyperparameters. Each dot in the graph represents a separate training run with changing\nrandom seeds and control parameters.\nFigure 14: Fairness-utility (equal opportunity-accuracy) tradeoff across various datasets, under their\ndefault hyperparameters. Each dot in the graph represents a separate training run with changing\nrandom seeds and control parameters.\n17\n\nAdult and Bank Marketing\nCOMPAS and German\nBatch Size\nLearning Rate\nMLP Layers\nBatch Size\nLearning Rate\nMLP Layers\n1024\n0.01\n512,256\n32\n0.01\n512,256\n1024\n0.01\n64\n32\n0.01\n64\n1024\n0.01\n512,256,256,64\n32\n0.01\n512,256,256,64\n128\n0.01\n512,256\n8\n0.01\n512,256\n128\n0.001\n512,256\n8\n0.001\n512,256\n4096\n0.01\n512,256\n128\n0.01\n512,256\n4096\n0.1\n512,256\n128\n0.1\n512,256\nKDD and ACS\nBatch Size\nLearning Rate\nMLP Layers\n4096\n0.01\n512,256\n4096\n0.01\n64\n4096\n0.01\n512,256,256,64\n512\n0.01\n512,256\n512\n0.001\n512,256\n8192\n0.01\n512,256\n8192\n0.1\n512,256\nTable 4: Hyperparameters.\nFigure 15: Pareto front of the fairness-utility (equalized odds-accuracy) tradeoff across various\ndatasets. Each dot in the graph represents a separate training run on the pareto front with changing\nhyperparameters, random seeds and control parameters.\nD\nAdditional Results at Pareto Front\nWe present additional results on the pareto front for various algorithms and datasets, under fairness\ndefinition as equalized odds (Figure 15) and equal opportunity (Figure 16). Similar to the trends seen\nin the main paper, we find many different algorithms provide competitive tradeoffs when allowed to\nperform appropriate hyperparameter optimization.\n18\n\nFigure 16: Pareto front of the fairness-utility (equal opportunity-accuracy) tradeoff across various\ndatasets. Each dot in the graph represents a separate training run on the pareto front with changing\nhyperparameters, random seeds and control parameters.\n19",
    "pdf_filename": "Different_Horses_for_Different_Courses_Comparing_Bias_Mitigation_Algorithms_in_ML.pdf"
}