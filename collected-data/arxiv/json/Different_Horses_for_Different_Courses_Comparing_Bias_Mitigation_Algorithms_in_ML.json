{
    "title": "Different Horses for Different Courses: Comparing",
    "abstract": "WithfairnessconcernsgainingsignificantattentioninMachineLearning(ML), severalbiasmitigationtechniqueshavebeenproposed,oftencomparedagainsteach othertofindthebestmethod. Thesebenchmarkingeffortstendtouseacommon setupforevaluationundertheassumptionthatprovidingauniformenvironment ensures a fair comparison. However, bias mitigation techniques are sensitive to hyperparameter choices, random seeds, feature selection, etc., meaning that comparisononjustonesettingcanunfairlyfavourcertainalgorithms. Inthiswork, weshowsignificantvarianceinfairnessachievedbyseveralalgorithmsandthe influenceofthelearningpipelineonfairnessscores. Wehighlightthatmostbias mitigationtechniquescanachievecomparableperformance,giventhefreedomto performhyperparameteroptimization,suggestingthatthechoiceoftheevaluation parameters—ratherthanthemitigationtechniqueitself—cansometimescreatethe perceivedsuperiorityofonemethodoveranother. Wehopeourworkencourages futureresearchonhowvariouschoicesinthelifecycleofdevelopinganalgorithm impactfairness,andtrendsthatguidetheselectionofappropriatealgorithms. 1 Introduction Overthepastdecade,concernsaboutfairnessanddiscriminationinMachineLearning(ML)systems haveemergedascriticalissues,drivingextensiveresearchintothedevelopmentoffairMLpractices, includingmitigationalgorithmsandfairnesscriteria[Mehrabietal.,2021,GoharandCheng,2023, Barocasetal.,2023]. ThishasledtoemergingglobalAIregulationfocusedonmitigatingdiscrimina- tioninAI/MLsystems,mandatingthereportingoffairnessmetricsofalgorithmsincompliancewith variousanti-discriminationlawssuchasthedisparateimpactdoctrine[Justice.,2023]. However, despite the regulatory efforts, recent research has increasingly shown that the fair ML pipelinesuffersfrominstabilityandhighvarianceinfairnessmeasures,whichcanmasktheunderlying unfairnesswhilecreatinganillusionoffairness[Blacketal.,2023]. Forinstance,recentworkhas pointed out how fairness measures vary across different training runs or between training and deployment,challengingtheeffectiveness,reliability,andutilityofexistingmethods[Baldinietal., 2021, Black and Fredrikson, 2021, Friedler et al., 2019, Ganesh et al., 2023]. Additionally, the multitudeofmitigationtechniquesandfairnessmetricsfurthercomplicateaccuratebenchmarking. Therefore,frombotharegulatoryperspectiveandbestpractices,suchvariancesmustbetakeninto accounttoaccuratelyrepresenttheperformanceofthesesystemsandfairnessinterventionmethods. ∗Equalcontribution WorkshoponAlgorithmicFairnessthroughtheLensofMetricsandEvaluation(AFME)atNeurIPS2024. 4202 voN 91 ]GL.sc[ 2v10111.1142:viXra",
    "body": "Different Horses for Different Courses: Comparing\nBias Mitigation Algorithms in ML\nPrakharGanesh∗ UsmanGohar∗\nMcGillUniversityandMila IowaStateUniversity\nprakhar.ganesh@mila.quebec ugohar@iastate.edu\nLuCheng GolnooshFarnadi\nUniversityofIllinoisChicago McGillUniversityandMila\nlucheng@uic.edu farnadig@mila.quebec\nAbstract\nWithfairnessconcernsgainingsignificantattentioninMachineLearning(ML),\nseveralbiasmitigationtechniqueshavebeenproposed,oftencomparedagainsteach\nothertofindthebestmethod. Thesebenchmarkingeffortstendtouseacommon\nsetupforevaluationundertheassumptionthatprovidingauniformenvironment\nensures a fair comparison. However, bias mitigation techniques are sensitive\nto hyperparameter choices, random seeds, feature selection, etc., meaning that\ncomparisononjustonesettingcanunfairlyfavourcertainalgorithms. Inthiswork,\nweshowsignificantvarianceinfairnessachievedbyseveralalgorithmsandthe\ninfluenceofthelearningpipelineonfairnessscores. Wehighlightthatmostbias\nmitigationtechniquescanachievecomparableperformance,giventhefreedomto\nperformhyperparameteroptimization,suggestingthatthechoiceoftheevaluation\nparameters—ratherthanthemitigationtechniqueitself—cansometimescreatethe\nperceivedsuperiorityofonemethodoveranother. Wehopeourworkencourages\nfutureresearchonhowvariouschoicesinthelifecycleofdevelopinganalgorithm\nimpactfairness,andtrendsthatguidetheselectionofappropriatealgorithms.\n1 Introduction\nOverthepastdecade,concernsaboutfairnessanddiscriminationinMachineLearning(ML)systems\nhaveemergedascriticalissues,drivingextensiveresearchintothedevelopmentoffairMLpractices,\nincludingmitigationalgorithmsandfairnesscriteria[Mehrabietal.,2021,GoharandCheng,2023,\nBarocasetal.,2023]. ThishasledtoemergingglobalAIregulationfocusedonmitigatingdiscrimina-\ntioninAI/MLsystems,mandatingthereportingoffairnessmetricsofalgorithmsincompliancewith\nvariousanti-discriminationlawssuchasthedisparateimpactdoctrine[Justice.,2023].\nHowever, despite the regulatory efforts, recent research has increasingly shown that the fair ML\npipelinesuffersfrominstabilityandhighvarianceinfairnessmeasures,whichcanmasktheunderlying\nunfairnesswhilecreatinganillusionoffairness[Blacketal.,2023]. Forinstance,recentworkhas\npointed out how fairness measures vary across different training runs or between training and\ndeployment,challengingtheeffectiveness,reliability,andutilityofexistingmethods[Baldinietal.,\n2021, Black and Fredrikson, 2021, Friedler et al., 2019, Ganesh et al., 2023]. Additionally, the\nmultitudeofmitigationtechniquesandfairnessmetricsfurthercomplicateaccuratebenchmarking.\nTherefore,frombotharegulatoryperspectiveandbestpractices,suchvariancesmustbetakeninto\naccounttoaccuratelyrepresenttheperformanceofthesesystemsandfairnessinterventionmethods.\n∗Equalcontribution\nWorkshoponAlgorithmicFairnessthroughtheLensofMetricsandEvaluation(AFME)atNeurIPS2024.\n4202\nvoN\n91\n]GL.sc[\n2v10111.1142:viXra\nHyperparameter Setting 1\nWhat is the best method if the What is the best method if the\ndeveloper is constrained to developer does not know the\nA3 A2 A1\nhyperparameter setting 3? hyperparameter setting in\nadvance, and thus wants the\nHyperparameter Setting 2 most robust technique?\nA1\nA2 A1 A3\nA2\nHyperparameter Setting 3\nWhat is the best method if the\nA1 A2 A3 developer can use any\nhyperparameter? What is the best method if …\nHyperparameter Setting 4\nA3 ?\nA2 A1 A3\n…….\nNo method is the “best” method in every context!\nLow bias High bias\nFigure1: Motivationbehindamorenuancedandcontext-awarebenchmarkingofbiasmitigation\ntechniques,insteadofusingauniformevaluationsetuporattemptingtofindthe\"best\"technique.\nWhilerecentworkshavehighlightedtheissueofvarianceinfairness,existingfairnessbenchmarks\npredominatelyoperateunderasingleidenticaltrainingenvironment(e.g.,hyperparameters,random\nseed, etc.) to ensure more accurate and fair comparisons [Han et al., 2023]. However, this fails\ntoconsiderthesensitivityoffairnesstohyperparameterchoices,whichmaymaskthenuancesof\nvariousbiasmitigationtechniques,favoringoneoveranother[Dooleyetal.,2024],asillustratedin\nFigure1. Wepostulatethatafteraccountingforthevarianceinfairnessduetothehyperparameter\nchoice,mostbiasmitigationmethodsachievecomparableperformance. Thisfurtherraisesimportant\nquestions about the one-dimensional nature of existing fairness evaluations. Is the \"best\" model\nsimply the one that performs optimally under specific hyperparameter configurations, or should\nfairnessassessmentstakeamoreholisticapproach? Forinstance,howdoweaccountfortrade-offs\nbetweenfairness,interpretability,stability,andresourceconstraints? Shouldfairnessevaluations\nprioritize consistency over best performance, or should context-specific factors like deployment\nenvironmentsandreal-worldimplicationsdictatethecriteriaforsuccess? Thesequestionshighlight\ntheneedformorenuancedandmultidimensionalfairnessbenchmarksbeyondtraditionalmeasures.\nContributionsofourwork. Weshowthatbiasmitigationalgorithmsarehighlysensitivetoseveral\nchoicesmadeinthelearningpipeline. Consequently,withoutincorporatingthebroadercontext,a\none-dimensionalcomparativeanalysisofthesealgorithmscancreateafalsesenseoffairness. We\nhighlightthatmostmitigationalgorithmscanachievecomparableperformanceunderappropriate\nhyperparameteroptimizationand,therefore,advocategoingbeyondthenarrowviewofthefairness-\nutilitytradeofftoexploreotherfactorsthatimpactmodeldeployment. Weconductalarge-scale\nempiricalanalysistosupportourclaims. Wehopeourworkinspiresfutureresearchthatexploresthe\ninterplaybetweenbiasmitigationalgorithmsandtheentirelearningpipeline,ratherthanstudying\ntheminisolation,ashasoftenbeenthecaseintheliterature.\n2 RelatedWork\nResearchershavedevelopedarangeofmitigationtechniquesandnotionstoaddressunfairnessin\nMLsystems,targetingdifferentstagesoftheMLpipeline,includingpre-processing,in-processing,\nandpost-processingmethods[Mehrabietal.,2021,PessachandShmueli,2022,Goharetal.,2024].\nPre-processingbalancesdatadistributionacrossprotectedgroups, reducingvariance, whilepost-\nprocessingmodifiesmodeloutputswithoutaccessinginternalalgorithms,idealforblack-boxmodels.\nIncontrast,in-processingmethodsimposefairnessconstraintsonthemodelormodifytheobjective\nfunctiontomitigatebias. Thereisinherentrandomnessinthetrainingprocess,which,whilevitalfor\nconvergenceandgeneralization[Nohetal.,2017],canbeasourceofhigh-fairnessvariance.Moreover,\nthese techniques also have control parameters (for example, regularization weight) that must be\noptimizedforthetrainingdata,furtherintroducingvariance[Bottou,2012]. Inthiswork,welimitour\nfocusonin-processingtechniquesduetothehighvarianceexhibitedbythesemethods[Baldinietal.,\n2021,BlackandFredrikson,2021,Friedleretal.,2019,Ganeshetal.,2023,Perroneetal.,2021].\n2\nThereisincreasingevidenceintheliteratureoftheinstabilityoffairnessmetricsassociatedwithnon-\ndeterminisminmodeltraininganddecisions[Blacketal.,2022,Baldinietal.,2021,Friedleretal.,\n2019]. Thisincludesidenticaltrainingenvironmentswithsmallchangessuchasdifferentrandom\nseeds[BlackandFredrikson,2021],sampling[Ganeshetal.,2023],hyperparameterchoices[Ganesh,\n2024,Goharetal.,2023],orevendifferencesintrain-test-split[Friedleretal.,2019],whichhavebeen\nshowntoleadtomeaningfuldifferencesingroupfairnessperformanceacrossdifferentruns.Theissue\nariseswhenthisinstabilityisusedtoreporthigherfairnessperformance,whichcallsintoquestionthe\neffectivenessoffairnessassessments[Blacketal.,2024]. Afewrecentworkshaveexploredthisissue\nfromaregulatoryfairnessassessmentstandpointwhere,inadvertentlyordeliberately,anactorcan\nmisrepresentthefairnessperformance. Forinstance,aparallelworkby Simsonetal.[2024]proposes\nusingmultiverseanalysistokeeptrackofallpossibledecisioncombinationsfordataprocessingand\nitsimpactonmodelfairness. Webuildonasimilarargument,focusinginsteadondecisionsmade\nduringthealgorithmdesign,highlightingtheinstabilityofvariousfairnessinterventionmethods.\nSeveralpreviousattemptshavebeenmadetobenchmarkbiasmitigationalgorithms[Bellamyetal.,\n2019, Bird et al., 2020, Han et al., 2023], including those conducted by papers proposing new\nmitigationtechniques[Kamishimaetal.,2012,Lietal.,2022,Madrasetal.,2018,Adeletal.,2019,\nZhang et al., 2018]. As highlighted by Han et al. [2023], every paper that proposes a new bias\nmitigationalgorithmoftenintroducesitsownexperimentalsetup. Thislackofstandardizationcan\nmakeitdifficulttocomparedifferentalgorithmseffectively. Toovercomethisissue,Hanetal.[2023]\ncreated the FFB benchmark, offering a comprehensive evaluation over a wide range of datasets\nandalgorithmstoallowfaircomparison. WhileFFBprovidesanexcellentfoundationtocompare\nandbenchmarkbiasmitigationalgorithms,itsattemptstofindanalgorithmthatprovidesthebest\nfairness-utilitytradeoff(whichtheyclaimisHSIC)comeswithtwoimportantcaveats,(a)FFBonly\nconsidersasinglehyperparametersetting,which,whileusefulforstandardization,overlooksthe\nsensitivityoffairnessscorestohyperparameterchoices[Perroneetal.,2021],and(b)FFBaggregates\nresultsacrossmultipledatasets,whichcanobscurethenuancesofperformanceonindividualdatasets.\nInourwork,wedemonstratethatthefairnessvariabilityduetohyperparameterchoicescanoften\nmaskunfairnessandraiseconcernsaboutexistingevaluationtechniques.\n3 VarianceinBiasMitigation\nInthissection,wearguethatbenchmarkingunderdifferentsettingscanrevealtrendsthatarelost\nwhen sticking to only a single standardized hyperparameter setting or aggregating results across\nmultipledatasets,asdonebyHanetal.[2023]. Thus,amorenuancedapproachtobenchmarking\nbiasmitigationtechniquesisneededtocapturethestrengthsandlimitationsofvariousalgorithms.\n3.1 Experimentsetup\nForourexperiments,weborrowfromHanetal.[2023],usingtheiropen-sourcecode2. Wefocus\nontheseventabulardatasetsandthesevenbiasmitigationalgorithmsusedintheirbenchmark(not\nincluding the standard empirical risk minimization without fairness constraints). In addition to\nvaryingtherandomseedfortrainingandthecontrolparameterforbiasmitigationasdoneinthe\nbenchmark, we also vary the batch size, learning rate, and model architecture to explore several\ndifferenthyperparametersettings. MoredetailsontheexperimentsetuparedelegatedtoAppendixA.\n3.2 CaseStudy: AdultDataset\nWestartwithacasestudyontheAdultdataset[BeckerandKohavi,1996]andshowthechanging\ntrendsacrossdifferenthyperparameters.Weplotfairnessasdemographicparityandutilityasaccuracy,\nstudyingthefairness-utilitytradeoffacrossvarioussettingsinFigure2. Additionaldiscussionson\notherdatasetsandfairnessmetricsarepresentinAppendixB.\nWefirstobservetheabsenceofaclearwinneramongbiasmitigationalgorithms;nosinglemethod\nconsistentlyandsignificantlyoutperformstheothers. WhileHSICachievesbettertradeoffsunder\nthe hyperparameter setting used by Han et al. [2023], other techniques such as PRemover and\nDiffDP perform equally well – or sometimes even better – under other hyperparameter settings.\n2https://github.com/ahxt/fair_fairness_benchmark\n3\nFigure2: Fairness-utility(demographicparity-accuracy)tradeoffacrossvarioussettingsfortheAdult\ndataset. Eachgraphrepresentsadifferentcombinationofhyperparameters,andeachdotinthegraph\nrepresentsaseparatetrainingrun. Multipledotsforthesamemitigationalgorithminthesamegraph\nrepresentrunswithchangingrandomseedsandcontrolparameters.\nThus,acomparativeanalysislimitedtojustonecombinationofhyperparametersfailstocapturethe\ncompetitiveperformanceofotheralgorithms.\nAnotherinterestingsetofresultscomesfromthehyperparametersettingwithalargebatchsizeand\nahighlearningrate. Thesesettingsareparticularlyrelevantinscenarioswheretheemphasisison\nrapidconvergenceandminimizingthenumberoftrainingsteps,evenatthecostofperformance,for\ninstance,duringrapidprototyping,edgecomputingwithlimitedcomputecyclesorfederatedlearning.\nMostbiasmitigationmethodsthatperformedwellinothersettingsfailtoevenconvergeunderthese\nconditions. Instead,methodslikeAdvDebiasandLAFTR,whichdidnotstandoutinothersettings,\nprovidegoodfairnessscoreswhentrainedundertheseconstraints.\nOurfindingshighlighttheimportanceofconsideringadiverserangeofchoicesinthelearningpipeline\nwhenevaluatingbiasmitigationtechniques,asdifferentalgorithmsexcelunderdifferentsettings.\nFurther interesting trends can be extracted from the comparative analysis of various algorithms\nacrossdifferenthyperparametersettings,leftforfuturework. Wenowturntoasimilarlynuanced\ncomparativeanalysisofbiasmitigationtechniques,focusingonchangingtrendsacrossdatasets.\n3.3 ChangingTrendsAcrossDatasets\nIntheprevioussection,weobserveddifferenttechniquesperformbetterthanothersunderchanging\nsettingswithinasingledataset. Wenowextendthisobservationtomultipledatasetstoshowthat\neventhetrendsacrossdifferentdatasetscanvarysignificantly,andthechoiceofcombiningresults\nfromalldatasets, asdonebyHanetal.[2023], canobscurethesetrends. Werecordthefairness\n(demographicparity)andutility(accuracy)acrossdifferentdatasetsandbiasmitigationalgorithmsin\nFigure3. ResultsforotherfairnessmetricsarepresentinAppendixC.\nWe begin again with HSIC, which Han et al. [2023] identifies as the algorithm offering the best\ntradeoff overall. While HSIC does well on most datasets, it is noticeably not the unanimous top\nchoiceforboththeCOMPASandGermandatasets. Interestingly,bothofthesearesmalldatasets\nwithsmallerbatchsizesusedfortraining. GiventhatHSICreliesonpairwisesimilarityinabatch\ntoestimatedependence,itssuperioritywithlargerbatchsizesbutitslacklusterperformancewith\nsmallerbatchsizesisnotsurprising. MostofthedatasetsanalyzedbyHanetal.[2023]werebig\ndatasetswithlargebatchsizes. Thus,combiningresultsfrommultipledatasetsovershadowedthe\ntrendspresentonlyinsmallerdatasets,effectivelyhidingHSIC’sshortcomings.\n4\nFigure3: Fairness-utility(demographicparity-accuracy)tradeoffacrossvariousdatasets,undertheir\ndefault hyperparameters. Each dot in the graph represents a separate training run with changing\nrandomseedsandcontrolparameters.\nAnother intriguing trend is present in the performance of LAFTR, the adversarial representation\nlearning-basedbiasmitigationtechnique. Generally,LAFTRunderperformsacrossvariousdatasets,\nofferingpoortradeoffs. However,itperformssurprisinglywellontheCOMPASdataset,providing\ncompetitivetradeoffsagainstothertechniques. Thisanomalymaybelinkedtothedatapreprocessing\nstepadoptedbyHanetal.[2023],whereallcategoricalfeaturesareconvertedintoone-hotencodings.\nSinceLAFTRreliesonrepresentationlearningatitscore,thisexplosioninthenumberoffeatures\ncanmaketherepresentationlearningtaskmoredifficult,thushurtingtheeventualmitigationattempts.\nNotably, the COMPAS dataset, where LAFTR excels, contains the least number of categorical\nfeatures and, consequently, the smallest input size compared to other datasets. Thus, LAFTR’s\nunderperformance might not be simply due to the algorithm itself but rather the choice of input\nfeaturerepresentationused.\nFinally,wealsoobservefairnessmetric-specifictrendsthataffectthecomparativeanalysisbetween\ndifferentalgorithms. FormanydatasetslikeAdult,COMPAS,andKDD,thereisacleartradeoff\nbetweendemographicparityandaccuracy,whichisn’tsurprisingsincedemographicparityisnot\naligned with accuracy. In these datasets, we find different mitigation techniques occupy distinct\npositionsinthetradeoffcurves. Thus,thechoiceofthemitigationtechnique,therefore,dependson\nthestakeholders’levelofwillingnesstotradeutilityforfairness. Conversely,manyotherdatasetsdo\nnothavetheseapparenttradeoffs. Here,wefindthattheresultsaremixed,withhighlyoverlapping\ntrendsacrossdifferentmitigationalgorithms.\n4 ComparisonsBeyondtheFairness-UtilityTradeoff\nIn the previous section, we discussed the limitations of a generic comparative analysis of bias\nmitigationalgorithmsthatdon’ttakeintoaccountthenuancesoftheentirelearningprocess. Building\nonthis,wenowturntoapracticalconcern: choosingtheappropriatealgorithm.\nWebeginthissectionbyfirstshowingthatgiventheopportunitytoperformhyperparameteropti-\nmization,variousmitigationalgorithmscanprovidecompetitivemodels. Wethendiscusshow,given\nthelackofappropriatedifferentiationbetweenthesealgorithmsintheirfairness-utilitytradeoff,the\nselectionoftheappropriatealgorithmcanprioritizeotherfactors,likealgorithmruntime,complexity,\npotentialrobustness,theoreticalguarantees,etc. Consequently,selectingtheappropriatealgorithm\nandtheresultingmodelwouldinvolvebalancingtheseadditionalconsiderations,ratherthansolely\nfocusingonjustthebestfairness-utilitytradeoff.\n5\nFigure4: Paretofrontofthefairness-utility(demographicparity-accuracy)tradeoffacrossvarious\ndatasets. Eachdotinthegraphrepresentsaseparatetrainingrunontheparetofrontwithchanging\nhyperparameters,randomseedsandcontrolparameters.\n4.1 MostMitigationAlgorithmsareCompetitive\nAswesawinFigure2(andAppendixB),differentalgorithmsdowellundervaryingsettings. In\nseveralreal-worldapplications,manyofthesechoicesareflexible,andhyperparameteroptimization\nplaysanimportantroleinmodelselection.Thus,whencomparingdifferentalgorithms,itisimportant\ntofocusonevaluatingthebest-performingmodelsfromeachalgorithm,asthesearethemodelsthat\nwouldbedeployedifthosealgorithmswereused.\nToperformthiscomparison,weonlyfilterthemodelsattheParetofrontforvariousalgorithmsafter\nsearchingthroughdifferenthyperparametersandrandomseedscollectedinFigure4. Trendsforother\nfairnessmetricsarepresentinAppendixD.Wefindthatseveralalgorithmscanprovidecompetitive\ntradeoffsforalmosteverydataset. Forinstance,DiffDP,PRemover,andHSICdemonstrateexcellent\nfairness-utility tradeoffs for the Adult dataset, while all seven bias mitigation algorithms exhibit\ncompetitivetradeoffsontheGermandataset. Withmultiplealgorithmsshowingsimilartradeoffs,it\nbecomesevidentthatsimplyevaluatingfairness-utilitytradeoffsisinsufficientwhenchoosingthe\nmostsuitablebiasmitigationtechnique. Weexploretheseconsiderationsfurtherinthenextsection.\n4.2 ChoosingtheRightMitigationTechnique\nWhenseveralbiasmitigationalgorithmsprovidesimilartradeoffs,selectingonecanbechallenging.\nInsuchcases,additionalfactorsmustbeconsidered,suchasthespecificrequirementsofthetask,the\ndeploymentenvironment,thestakeholders’expectations,etc. Here,weprovidesomeexamplesof\ncomparisonsbeyondthefairness-utilitytradeoffthatcanhelpchooseanappropriatealgorithm.\nRuntime: Analgorithm’sruntimecanbeacrucialfactorwhencomparingbiasmitigationtech-\nniques. Evenminordifferencesinruntimemightbecomerelevantwhenmultiplerunsofthesame\nalgorithmareneeded,forinstance,toperformhyperparameteroptimization. Ourresults,detailed\nin Table 1, reveal interesting trends in training runtime across various algorithms. We find the\nalgorithms HSIC, LAFTR, PRemover, DiffEOdd, and DiffEOpp to be quite expensive, while in\ncontrast,algorithmsDiffDPandAdvDebiasofferruntimecomparabletothestandardempiricalrisk\nminimization. ConsideringthecompetitivetradeoffsachievedbyDiffDP,inadditiontothelower\nruntime,itemergesasanappropriatechoiceforsettingswherecomputationalefficiencyiscritical,\nsurpassingotherwell-performingbutslowermethodslikeHSICandPRemover.\n6\nRuntime(roundedto5sintervals)\nDataset\nERM DiffDP DiffEOdd DiffEOpp HSIC LAFTR PRemover AdvDebias\nBank 1m15s 1m15s 1m45s 1m45s 1m50s 1m46s 1m50s 1m25s\nGerman 30s 30s 35s 35s 40s 35s 40s 30s\nAdult 1m40s 1m40s 1m45s 1m45s 2m0s 1m50s 2m0s 1m40s\nCOMPAS 30s 30s 30s 30s 30s 30s 30s 30s\nKDDCensus 6m45s 6m50s 10m40s 10m40s 10m5s 10m0s 9m50s 6m50s\nACS-I 7m10s 7m10s 9m50s 9m50s 9m50s 9m30s 10m0s 7m50s\nACS-E 13m40s 13m45s 15m40s 15m40s 16m20s 15m50s 16m10s 13m40s\nACS-P 7m20s 7m35s 9m50s 9m50s 9m40s 10m5s 10m5s 7m40s\nACS-M 4m40s 4m45s 6m5s 6m5s 6m0s 6m10s 6m0s 4m50s\nACS-T 7m30s 7m30s 10m5s 10m5s 10m15s 10m0s 10m20s 8m0s\nTable1: Trainingruntimeofmitigationalgorithmsacrossdatasets,underdefaulthyperparameters.\nTheoreticalGuaranteesandProceduralRequirements: Anotherimportantconsiderationwhen\nselectingtheappropriatealgorithmisthetheoreticalguaranteesthatsometechniquescanoffer. For\ninstance,whileaddingregularizerstothetrainingobjectivecanbeuseful,itdoesnotprovideany\nformofguaranteeforthemodel’sfinalfairnessscores. Incontrast,methodslikeHSICandLAFTR\ncan provide theoretical bounds on the fairness of the final model, albeit limited to only simpler\nmodels[Lietal.,2022,Madrasetal.,2018].\nFurthermore,thedeployedmodelsmayneedtocomplywithspecificproceduralrequirements,which\ncaninfluencethechoiceofthemitigationalgorithm. Forinstance,onemightneedtochoosebetween\nalgorithmsfocusingonoutcomefairness(suchasDiffDP,DiffEOpp,DiffEOdd)versusthosefocusing\nonprocessfairness(suchasHSIC,LAFTR,PRemover,AdvDebias). Thespecificrequirementsof\ntheapplicationcandictatethechoiceofthealgorithm,lookingbeyondthetradeoffsitcanprovide.\nMultiplicityandArbitrariness: Modelmultiplicityreferstotheexistenceofasetofgoodmodels,\nwhichhavesimilarperformancebutdifferintheirpredictionsforindividuals[Marxetal.,2020,Black\netal.,2022]. Existingworkshaveshownthatbiasmitigationcanexacerbatemultiplicityconcerns,\nleadingtoarbitrarinessinindividual-levelpredictions[Longetal.,2024]. However,thedegreeof\nmultiplicityintroducedcanvarydependingonthemitigationalgorithmused. FollowingLongetal.\n[2024], we define the set of competing models as models with similar accuracy under ERM and\nrecordmultiplicityusingambiguity[Marxetal.,2020],whichisthefractionofdatapointswhose\npredictionschangeacrossdifferentmodelswithinthesetofgoodmodels,inTable2.\nAmbiguity\nDataset\nERM DiffDP DiffEOdd DiffEOpp HSIC LAFTR PRemover AdvDebias\nBank 0.15 0.16 0.16 0.18 0.17 0.19 0.15 0.26\nGerman 0.55 0.57 0.57 0.63 0.55 0.59 0.60 0.87\nAdult 0.17 0.30 0.37 0.42 0.28 0.32 0.34 0.47\nCOMPAS 0.93 0.99 1.0 1.0 1.0 1.0 0.92 0.99\nKDDCensus 0.04 0.06 0.04 0.06 0.05 0.06 0.04 0.09\nACS-I 0.26 0.35 0.38 0.35 0.38 0.32 0.75 0.49\nACS-E 0.14 0.20 0.30 0.21 0.26 0.20 0.49 0.37\nACS-P 0.27 0.33 0.38 0.39 0.34 0.45 0.32 0.69\nACS-M 0.26 0.29 0.27 0.29 0.31 0.38 0.20 0.61\nACS-T 0.70 0.81 0.88 0.80 0.79 0.91 0.92 0.90\nTable2: Ambiguityscoresofmitigationalgorithmsacrossdatasets,underdefaulthyperparameters.\nUnsurprisingly,mostbiasmitigationtechniquesexhibithigherambiguitythanERM,whichaligns\nwiththeobservationsmadebyLongetal.[2024]. However,aninterestingexceptionisthePRemover\nalgorithm,whichachievesremarkablylowambiguityscoresacrossmanydatasets,distinguishing\nit from other algorithms. Strikingly, at the same time, PRemover also shows significantly high\nambiguityinseveralotherdatasets,highlightingitsbehavioronbothextremes. Thus,forcertain\ndatasets,PRemovercouldbeconsideredasuperiorchoicecomparedtoothermethodslikeHSICand\n7\nDiffDP,which,whileofferingsimilartrade-offs,tendtointroducemorearbitrarinessintothemodel.\nIncontrasttoPRemover,theAdvDebiasalgorithmconsistentlyresultsinveryhighambiguityscores,\nmakingitapoorchoiceincontextswhereminimizingarbitrarinessiscrucial.\nIn this section, we showed several examples of additional factors to consider when selecting an\nalgorithmforaspecificusecase. Naturally,thislistisnotexhaustive,asadditionalconsiderations\nmayarisedependingonthespecificapplicationcontext. Theobjectiveofourstudywastoemphasize\nthelackofdistinctionbetweenmitigationalgorithmsthatfocussolelyonthefairness-utilitytradeoff\nandtheimportanceofchoosingalgorithmsthatofferadditionaladvantagesbeyondthistradeoff. With\ntheseresults,wehopetomoveawayfromthenarrativeofasingleoptimalbiasmitigationtechnique\nandemphasizetheneedforcontext-dependentcomparativeanalysis.\n5 Discussion\nIn this paper, we underscore the limitations of current fairness benchmarking practices that rely\nonuniformevaluationsetups. Wedemonstratethathyperparameteroptimizationcanyieldsimilar\nperformanceacrossdifferentbiasmitigationtechniques,raisingquestionsabouttheeffectivenessof\nexistingbenchmarksandthecriteriaforselectingappropriatefairnessalgorithms.\nContext-dependentevaluation. Wearguethatthecurrentone-dimensionalapproachtofairness\nevaluationmaybeinsufficient. Giventhehighvariabilityinfairnessscores,relyingonasinglerun\nor,conversely,simplyaggregatingmultipletrainingruns,bothcommonpracticesacrossdifferent\ndimensions,maynotalwaysprovideanappropriatecomparisonofbiasmitigationtechniques.\nForexample,whenmodelsaretoolargeandretrainingisimpractical,choosingfairnessinterventions\nthatprioritizestabilityandconsistentscoresmaybemoreappropriate. Ontheotherhand,ifsufficient\ncomputational resources exist to explore hyperparameter options, selecting the best-performing\nmodelmightbemorevalid. Additionally, explainability, runtime, andscalabilityconstraintscan\nsignificantly impact the choice of fairness assessments. Ultimately, the method of comparing\nalgorithms depends on the context. However, in all cases, it is crucial to consider the variability\nintroducedbyhyperparametertuning.\nFuturework. Ourexperimentswerelimitedtoin-processingtechniquesinbiasmitigation. Inthe\nfuture,weplantoexploreabroaderrangeofmethods,includingpreandpost-processing. Moreover,\nwehavenotexploredthepotentialpresenceofconsistentfairnesstrendsfordifferenthyperparameter\nchoicescoveredintheexperiments. Itwouldbeinterestingtoinvestigatewhetherwecanidentify\npatternsthatguideourdecisionstochoosebetterhyperparametersettingsforvariousbiasmitigation\nalgorithms. Finally,whileevidenceintheliteraturewouldsuggestsimilartrendsexistevenwith\nhyperparametersinotherpartsofthepipeline,forinstance,dataprocessing[Simsonetal.,2024],our\nempiricalresultsarelimitedtohyperparameterchoicesduringtraining. Furtherworkonalarge-scale\nstudyoftheimpactofvariouschoicesinthelifetimeofanalgorithmdesignisneeded.\nAcknowledgmentsandDisclosureofFunding\nFundingsupportforprojectactivitieshasbeenpartiallyprovidedbytheCanadaCIFARAIChair,\nFRQNTscholarship,andNSERCdiscoveryaward.WealsoexpressourgratitudetoComputeCanada\nandMilaclustersfortheirsupportinprovidingfacilitiesforourevaluations. LuChengissupported\nbytheNationalScienceFoundation(NSF)Grant#2312862,NIH#R01AG091762,andaCiscogift\ngrant.\nReferences\nCensus-Income (KDD). UCI Machine Learning Repository, 2000. DOI:\nhttps://doi.org/10.24432/C5N30T.\nT.Adel,I.Valera,Z.Ghahramani,andA.Weller. One-networkadversarialfairness. InProceedings\noftheAAAIConferenceonArtificialIntelligence,volume33,pages2412–2420,2019.\nS.Baharlouei,M.Nouiehed,A.Beirami,andM.Razaviyayn. Rényifairinference. InInternational\nConferenceonLearningRepresentations.\n8\nI. Baldini, D. Wei, K. N. Ramamurthy, M. Yurochkin, and M. Singh. Your fairness may vary:\nPretrainedlanguagemodelfairnessintoxictextclassification. arXivpreprintarXiv:2108.01250,\n2021.\nS.Barocas,M.Hardt,andA.Narayanan. Fairnessandmachinelearning: Limitationsandopportuni-\nties. MITPress,2023.\nB. Becker and R. Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI:\nhttps://doi.org/10.24432/C5XW20.\nR.K.Bellamy,K.Dey,M.Hind,S.C.Hoffman,S.Houde,K.Kannan,P.Lohia,J.Martino,S.Mehta,\nA.Mojsilovic´,etal. Aifairness360: Anextensibletoolkitfordetectingandmitigatingalgorithmic\nbias. IBMJournalofResearchandDevelopment,63(4/5):4–1,2019.\nA. Beutel, J. Chen, Z. Zhao, and E. H. Chi. Data decisions and theoretical implications when\nadversariallylearningfairrepresentations. arXivpreprintarXiv:1707.00075,2017.\nS.Bird,M.Dudík,R.Edgar,B.Horn,R.Lutz,V.Milan,M.Sameki,H.Wallach,andK.Walker.\nFairlearn: Atoolkitforassessingandimprovingfairnessinai. Microsoft,Tech.Rep.MSR-TR-\n2020-32,2020.\nE.BlackandM.Fredrikson. Leave-one-outunfairness. InProceedingsofthe2021ACMConference\nonFairness,Accountability,andTransparency,pages285–295,2021.\nE.Black,M.Raghavan,andS.Barocas. Modelmultiplicity: Opportunities,concerns,andsolutions.\nInProceedingsofthe2022ACMConferenceonFairness,Accountability,andTransparency,pages\n850–863,2022.\nE.Black,R.Naidu,R.Ghani,K.Rodolfa,D.Ho,andH.Heidari. Towardoperationalizingpipeline-\nawaremlfairness: Aresearchagendafordevelopingpracticalguidelinesandtools. InProceedings\nofthe3rdACMConferenceonEquityandAccessinAlgorithms,Mechanisms,andOptimization,\npages1–11,2023.\nE.Black,T.Gillis,andZ.Y.Hall. D-hacking. InThe2024ACMConferenceonFairness,Account-\nability,andTransparency,pages602–615,2024.\nL. Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade: Second\nEdition,pages421–436.Springer,2012.\nF.Ding,M.Hardt,J.Miller,andL.Schmidt. Retiringadult: Newdatasetsforfairmachinelearning.\nAdvancesinneuralinformationprocessingsystems,34:6478–6490,2021.\nS.Dooley, R.Sukthanker, J.Dickerson, C.White, F.Hutter, andM.Goldblum. Rethinkingbias\nmitigation: Fairerarchitecturesmakeforfairerfacerecognition. AdvancesinNeuralInformation\nProcessingSystems,36,2024.\nH. Edwards and A. Storkey. Censoring representations with an adversary. In 4th International\nConferenceonLearningRepresentations,pages1–14,2016.\nS.A.Friedler,C.Scheidegger,S.Venkatasubramanian,S.Choudhary,E.P.Hamilton,andD.Roth.\nAcomparativestudyoffairness-enhancinginterventionsinmachinelearning. InProceedingsof\ntheconferenceonfairness,accountability,andtransparency,pages329–338,2019.\nP.Ganesh. Anempiricalinvestigationintobenchmarkingmodelmultiplicityfortrustworthymachine\nlearning:Acasestudyonimageclassification.InProceedingsoftheIEEE/CVFWinterConference\nonApplicationsofComputerVision,pages4488–4497,2024.\nP.Ganesh,H.Chang,M.Strobel,andR.Shokri. Ontheimpactofmachinelearningrandomness\nongroupfairness. InProceedingsofthe2023ACMConferenceonFairness,Accountability,and\nTransparency,pages1789–1800,2023.\nU.GoharandL.Cheng. Asurveyonintersectionalfairnessinmachinelearning: Notions,mitigation,\nandchallenges. arXivpreprintarXiv:2305.06969,2023.\n9\nU.Gohar,S.Biswas,andH.Rajan. Towardsunderstandingfairnessanditscompositioninensemble\nmachinelearning. In2023IEEE/ACM45thInternationalConferenceonSoftwareEngineering\n(ICSE),pages1533–1545.IEEE,2023.\nU. Gohar, Z. Tang, J. Wang, K. Zhang, P. L. Spirtes, Y. Liu, and L. Cheng. Long-term fairness\ninquiriesandpursuitsinmachinelearning: Asurveyofnotions,methods,andchallenges. arXiv\npreprintarXiv:2406.06736,2024.\nA. Gretton, O. Bousquet, A. Smola, and B. Schölkopf. Measuring statistical dependence with\nhilbert-schmidtnorms. InInternationalconferenceonalgorithmiclearningtheory,pages63–77.\nSpringer,2005.\nX.Han,J.Chi,Y.Chen,Q.Wang,H.Zhao,N.Zou,andX.Hu. Ffb: Afairfairnessbenchmarkfor\nin-processinggroupfairnessmethods. InInternationalConferenceonLearningRepresentations.\nICLR,2023.\nH. Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI:\nhttps://doi.org/10.24432/C5NC77.\nU.S.D.O.Justice. Titlevilegalmanual,sectionvii: Provingdiscrimination–disparateimpact.,Oct\n2023. URLhttps://www.justice.gov/crt/fcs/T6Manual7.\nT.Kamishima,S.Akaho,H.Asoh,andJ.Sakuma. Fairness-awareclassifierwithprejudiceremover\nregularizer. InMachineLearningandKnowledgeDiscoveryinDatabases: EuropeanConference,\nECMLPKDD2012,Bristol,UK,September24-28,2012.Proceedings,PartII23,pages35–50.\nSpringer,2012.\nJ.Larson,S.Mattu,L.Kirchner,andJ.Angwin. Propublicacompasanalysis—dataandanalysisfor\n‘machinebias.’. https://github.com/propublica/compas-analysis,2016.\nZ. Li, A. Pérez-Suay, G. Camps-Valls, and D. Sejdinovic. Kernel dependence regularizers and\ngaussianprocesseswithapplicationstoalgorithmicfairness. PatternRecognition,132:108922,\n2022.\nC.Long,H.Hsu,W.Alghamdi,andF.Calmon. Individualarbitrarinessandgroupfairness. Advances\ninNeuralInformationProcessingSystems,36,2024.\nG.Louppe,M.Kagan,andK.Cranmer. Learningtopivotwithadversarialnetworks. Advancesin\nneuralinformationprocessingsystems,30,2017.\nD. Madras, E. Creager, T. Pitassi, and R. Zemel. Learning adversarially fair and transferable\nrepresentations. InInternationalConferenceonMachineLearning,pages3384–3393.PMLR,\n2018.\nC. Marx, F. Calmon, and B. Ustun. Predictive multiplicity in classification. In International\nConferenceonMachineLearning,pages6765–6774.PMLR,2020.\nN.Mehrabi,F.Morstatter,N.Saxena,K.Lerman,andA.Galstyan. Asurveyonbiasandfairnessin\nmachinelearning. ACMcomputingsurveys(CSUR),54(6):1–35,2021.\nS.Moro,P.Rita,andP.Cortez. BankMarketing. UCIMachineLearningRepository,2014. DOI:\nhttps://doi.org/10.24432/C5K306.\nH.Noh,T.You,J.Mun,andB.Han. Regularizingdeepneuralnetworksbynoise: Itsinterpretation\nandoptimization. Advancesinneuralinformationprocessingsystems,30,2017.\nV.Perrone,M.Donini,M.B.Zafar,R.Schmucker,K.Kenthapadi,andC.Archambeau. Fairbayesian\noptimization. InProceedingsofthe2021AAAI/ACMConferenceonAI,Ethics,andSociety,pages\n854–863,2021.\nD.PessachandE.Shmueli. Areviewonfairnessinmachinelearning. ACMComputingSurveys\n(CSUR),55(3):1–44,2022.\n10\nJ.Simson,F.Pfisterer,andC.Kern. Onemodelmanyscores: Usingmultiverseanalysistoprevent\nfairnesshackingandevaluatetheinfluenceofmodeldesigndecisions.InThe2024ACMConference\nonFairness,Accountability,andTransparency,pages1305–1320,2024.\nB.H.Zhang,B.Lemoine,andM.Mitchell. Mitigatingunwantedbiaseswithadversariallearning. In\nProceedingsofthe2018AAAI/ACMConferenceonAI,Ethics,andSociety,pages335–340,2018.\n11\nA AdditionalDetailsonExperimentSetup\nAswedirectlyborrowtheexperimentsetupfromHanetal.[2023],weredirectthereadertotheir\nworkandtheFFBbenchmarkcode3fordetailsontheunderlyingsetup. Inthissection,webriefly\nmentionthedatasetsandalgorithmsusedinthebenchmark,andthenewadditionsandchangeswe\nmadetotheirsetup.\nA.1 Datasets\nWeuse7differenttabulardatasetsforourexperiments. ThisincludestheAdultdataset[Beckerand\nKohavi,1996], COMPASdataset [Larsonetal.,2016], Germandataset[Hofmann,1994], Bank\nMarketingdataset[Moroetal.,2014],KDDCensusdataset[cen,2000],andACSdatasetwithtasks\nIncomeandEmployment[Dingetal.,2021]. WeusethesensitiveattributeRaceforalldatasets,\nexcept the Bank Marketing dataset and the German dataset, where we use Age as the sensitive\nattribute.\nA.2 BiasMitigationAlgorithms\nWeuse7differentbiasmitigationalgorithmsinoursetup.ThisincludesDiffDP,DiffEOdd,DiffEOpp,\nPRemover[Kamishimaetal.,2012],HSIC[Baharloueietal.,Grettonetal.,2005,Lietal.,2022],\nAdvDebias[Adeletal.,2019,Beuteletal.,2017,EdwardsandStorkey,2016,Louppeetal.,2017,\nZhangetal.,2018],andLAFTR[Madrasetal.,2018].\nA.3 Hyperparameters\nWeusetheAdamoptimizer,withnoweightdecayandasteplearningrateschedulerfortraining. We\ntrainthemodelfor150epochsandrecordthefairnessandaccuracyscoresatthefinalepoch.\nWeusethreedifferentvaluesofthecontrolparameterforeachalgorithm,asdefinedinTable3.\nAlgorithm ControlHyperparameter\nDiffDP 0.2,1.0,1.8\nDiffEOdd 0.2,1.0,1.8\nDiffEOdd 0.2,1.0,1.8\nPRemover 0.05,0.25,0.45\nHSIC 50,250,450\nAdvDebias 0.2,1.0,1.8\nLAFTR 0.1,0.5,4.0\nTable3: Controlhyperparameters.\nWeusesevendifferenthyperparametersettingsforeachdataset,asdefinedinTable4.\nB AdditionalResultsforTrendsUnderChangingHyperparameters\nWe present additional results for comparing trends under different hyperparameters in the Adult\ndatasetforfairnessdefinitionsofequalizedodds(Figure5)andequalopportunity(Figure6). We\nalsopresentadditionalresultsforcomparingtrendsinotherdatasetslikeBankMarketingdataset\n(Figure7),COMPASdataset(Figure8),Germandataset(Figure9),KDDCensusdataset(Figure10),\nACS-Incomedataset(Figure11)andACS-Employmentdataset(Figure12).\nC AdditionalResultsforChangingTrendsAcrossDatasets\nWepresentadditionalresultsforcomparingtrendsacrossmultipledatasets,underfairnessdefinition\nas equalized odds (Figure 13) and equal opportunity (Figure 14). Similar to the observations in\nthemainpaper,wefinddistincttrendsacrossdifferentdatasetsandnoclearsinglebiasmitigation\nalgorithmthatexcelsacrossalldatasets.\n3https://github.com/ahxt/fair_fairness_benchmark\n12\nFigure5: Fairness-utility(equalizedodds-accuracy)tradeoffacrossvarioussettingsfortheAdult\ndataset. Eachgraphrepresentsadifferentcombinationofhyperparameters,andeachdotinthegraph\nrepresentsaseparatetrainingrun. Multipledotsforthesamemitigationalgorithminthesamegraph\nrepresentrunswithchangingrandomseedsandcontrolparameters.\nFigure6: Fairness-utility(equalopportunity-accuracy)tradeoffacrossvarioussettingsfortheAdult\ndataset. Eachgraphrepresentsadifferentcombinationofhyperparameters,andeachdotinthegraph\nrepresentsaseparatetrainingrun. Multipledotsforthesamemitigationalgorithminthesamegraph\nrepresentrunswithchangingrandomseedsandcontrolparameters.\n13\nFigure7: Fairness-utility(demographicparity-accuracy)tradeoffacrossvarioussettingsfortheBank\nMarketingdataset. Eachgraphrepresentsadifferentcombinationofhyperparameters,andeachdot\ninthegraphrepresentsaseparatetrainingrun. Multipledotsforthesamemitigationalgorithminthe\nsamegraphrepresentrunswithchangingrandomseedsandcontrolparameters.\nFigure 8: Fairness-utility (demographic parity-accuracy) tradeoff across various settings for the\nCOMPASdataset. Eachgraphrepresentsadifferentcombinationofhyperparameters,andeachdot\ninthegraphrepresentsaseparatetrainingrun. Multipledotsforthesamemitigationalgorithminthe\nsamegraphrepresentrunswithchangingrandomseedsandcontrolparameters.\n14\nFigure 9: Fairness-utility (demographic parity-accuracy) tradeoff across various settings for the\nGermandataset. Eachgraphrepresentsadifferentcombinationofhyperparameters,andeachdotin\nthegraphrepresentsaseparatetrainingrun. Multipledotsforthesamemitigationalgorithminthe\nsamegraphrepresentrunswithchangingrandomseedsandcontrolparameters.\nFigure10: Fairness-utility(demographicparity-accuracy)tradeoffacrossvarioussettingsforthe\nKDDCensusdataset. Eachgraphrepresentsadifferentcombinationofhyperparameters,andeach\ndotinthegraphrepresentsaseparatetrainingrun. Multipledotsforthesamemitigationalgorithmin\nthesamegraphrepresentrunswithchangingrandomseedsandcontrolparameters.\n15\nFigure11: Fairness-utility(demographicparity-accuracy)tradeoffacrossvarioussettingsforthe\nACS-Incomedataset. Eachgraphrepresentsadifferentcombinationofhyperparameters,andeach\ndotinthegraphrepresentsaseparatetrainingrun. Multipledotsforthesamemitigationalgorithmin\nthesamegraphrepresentrunswithchangingrandomseedsandcontrolparameters.\nFigure12: Fairness-utility(demographicparity-accuracy)tradeoffacrossvarioussettingsforthe\nACS-Employmentdataset. Eachgraphrepresentsadifferentcombinationofhyperparameters,and\neach dot in the graph represents a separate training run. Multiple dots for the same mitigation\nalgorithminthesamegraphrepresentrunswithchangingrandomseedsandcontrolparameters.\n16\nFigure13: Fairness-utility(equalizedodds-accuracy)tradeoffacrossvariousdatasets,undertheir\ndefault hyperparameters. Each dot in the graph represents a separate training run with changing\nrandomseedsandcontrolparameters.\nFigure14: Fairness-utility(equalopportunity-accuracy)tradeoffacrossvariousdatasets,undertheir\ndefault hyperparameters. Each dot in the graph represents a separate training run with changing\nrandomseedsandcontrolparameters.\n17\nAdultandBankMarketing COMPASandGerman\nBatchSize LearningRate MLPLayers BatchSize LearningRate MLPLayers\n1024 0.01 512,256 32 0.01 512,256\n1024 0.01 64 32 0.01 64\n1024 0.01 512,256,256,64 32 0.01 512,256,256,64\n128 0.01 512,256 8 0.01 512,256\n128 0.001 512,256 8 0.001 512,256\n4096 0.01 512,256 128 0.01 512,256\n4096 0.1 512,256 128 0.1 512,256\nKDDandACS\nBatchSize LearningRate MLPLayers\n4096 0.01 512,256\n4096 0.01 64\n4096 0.01 512,256,256,64\n512 0.01 512,256\n512 0.001 512,256\n8192 0.01 512,256\n8192 0.1 512,256\nTable4: Hyperparameters.\nFigure 15: Pareto front of the fairness-utility (equalized odds-accuracy) tradeoff across various\ndatasets. Eachdotinthegraphrepresentsaseparatetrainingrunontheparetofrontwithchanging\nhyperparameters,randomseedsandcontrolparameters.\nD AdditionalResultsatParetoFront\nWepresentadditionalresultsontheparetofrontforvariousalgorithmsanddatasets,underfairness\ndefinitionasequalizedodds(Figure15)andequalopportunity(Figure16). Similartothetrendsseen\ninthemainpaper,wefindmanydifferentalgorithmsprovidecompetitivetradeoffswhenallowedto\nperformappropriatehyperparameteroptimization.\n18\nFigure16: Paretofrontofthefairness-utility(equalopportunity-accuracy)tradeoffacrossvarious\ndatasets. Eachdotinthegraphrepresentsaseparatetrainingrunontheparetofrontwithchanging\nhyperparameters,randomseedsandcontrolparameters.\n19",
    "pdf_filename": "Different_Horses_for_Different_Courses_Comparing_Bias_Mitigation_Algorithms_in_ML.pdf"
}