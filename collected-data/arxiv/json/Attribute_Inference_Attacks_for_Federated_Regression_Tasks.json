{
    "title": "Attribute Inference Attacks for Federated Regression Tasks",
    "context": "Federated Learning (FL) enables multiple clients, such as mo- bile phones and IoT devices, to collaboratively train a global machine learning model while keeping their data localized. However, recent studies have revealed that the training phase of FL is vulnerable to reconstruction attacks, such as attribute inference attacks (AIA), where adversaries exploit exchanged messages and auxiliary public information to uncover sensi- tive attributes of targeted clients. While these attacks have been extensively studied in the context of classification tasks, their impact on regression tasks remains largely unexplored. In this paper, we address this gap by proposing novel model- based AIAs specifically designed for regression tasks in FL environments. Our approach considers scenarios where ad- versaries can either eavesdrop on exchanged messages or directly interfere with the training process. We benchmark our proposed attacks against state-of-the-art methods using real-world datasets. The results demonstrate a significant in- crease in reconstruction accuracy, particularly in heteroge- neous client datasets, a common scenario in FL. The efficacy of our model-based AIAs makes them better candidates for empirically quantifying privacy leakage for federated regres- sion tasks. 1 Federated learning (FL) enables multiple clients to collab- oratively train a global model (McMahan et al. 2017; Lian et al. 2017; Li et al. 2020). Since clients’ data is not col- lected by a third party, FL naturally offers a certain level of privacy. Nevertheless, FL alone does not provide formal pri- vacy guarantees, and recent works have demonstrated that clients’ private information can still be easily leaked (Lyu et al. 2020; Liu, Xu, and Wang 2022). For instance, an ad- versary with access to the exchanged messages and knowl- edge of some public information (e.g., client’s provided rat- ings) (Lyu and Chen 2021; Chen et al. 2022; Feng et al. 2021) can reconstruct a client’s sensitive attributes (e.g., gender/religion) in an attack known as attribute inference attack (AIA). Additionally, the adversary can reconstruct client’s training samples such as images (Geiping et al. 2020; Yin et al. 2021). However, these reconstruction attacks for FL have pri- marily been tested on classification tasks and have not been Figure 1: Average performance of different AIAs when four clients train a neural network through FedAvg with 1 local epoch and batch size 32. Each client stores |Dc| data points randomly sampled from ACS Income dataset (Ding et al. 2024). The adversary infers the gender attribute of every data sample held by the client given access to the released (pub- lic) information. explored for regression tasks, which are, needless to say, equally important for practical applications. Quite surpris- ingly, our experiments, as shown in Fig. 1, demonstrate that the accuracy of state-of-the-art gradient-based AIA under a honest-but-curious adversary (Lyu and Chen 2021; Chen et al. 2022) (referred to as passive) drops significantly from 71% on a classification task to 50% (random guess) on a re- gression task once the targeted client holds more than 256 data points. Furthermore, even a more powerful (active) ad- versary capable of forging the messages to the targeted client to extract more information (Lyu and Chen 2021; Chen et al. 2022) offers only limited improvement to the AIA perfor- mance on regression tasks. Detailed information about this experiment is in Appendix B.1. In this paper, we show that federated training of regression tasks does not inherently enjoy higher privacy, but it is sim- ply more vulnerable to other forms of attacks. While existing FL AIA attacks for classification tasks are gradient-based (see Sec. 2.3), we show that model-based AIAs—initially proposed for centralized training (Fredrikson et al. 2014; arXiv:2411.12697v1  [cs.LG]  19 Nov 2024",
    "body": "Attribute Inference Attacks for Federated Regression Tasks\nFrancesco Diana1, Othmane Marfoq2, Chuan Xu1, 3, 4, 5, Giovanni Neglia1, 3, Fr´ed´eric Giroire1, 3, 4, 5,\nEoin Thomas6\n1INRIA\n2Meta\n3Universit´e Cˆote d’Azur\n4CNRS\n5I3S\n6Amadeus\nfrancesco.diana@inria.fr, omarfoq@meta.com, chuan.xu@inria.fr, giovanni.neglia@inria.fr, frederic.giroire@inria.fr,\neoin.thomas@amadeus.com\nAbstract\nFederated Learning (FL) enables multiple clients, such as mo-\nbile phones and IoT devices, to collaboratively train a global\nmachine learning model while keeping their data localized.\nHowever, recent studies have revealed that the training phase\nof FL is vulnerable to reconstruction attacks, such as attribute\ninference attacks (AIA), where adversaries exploit exchanged\nmessages and auxiliary public information to uncover sensi-\ntive attributes of targeted clients. While these attacks have\nbeen extensively studied in the context of classification tasks,\ntheir impact on regression tasks remains largely unexplored.\nIn this paper, we address this gap by proposing novel model-\nbased AIAs specifically designed for regression tasks in FL\nenvironments. Our approach considers scenarios where ad-\nversaries can either eavesdrop on exchanged messages or\ndirectly interfere with the training process. We benchmark\nour proposed attacks against state-of-the-art methods using\nreal-world datasets. The results demonstrate a significant in-\ncrease in reconstruction accuracy, particularly in heteroge-\nneous client datasets, a common scenario in FL. The efficacy\nof our model-based AIAs makes them better candidates for\nempirically quantifying privacy leakage for federated regres-\nsion tasks.\n1\nIntroduction\nFederated learning (FL) enables multiple clients to collab-\noratively train a global model (McMahan et al. 2017; Lian\net al. 2017; Li et al. 2020). Since clients’ data is not col-\nlected by a third party, FL naturally offers a certain level of\nprivacy. Nevertheless, FL alone does not provide formal pri-\nvacy guarantees, and recent works have demonstrated that\nclients’ private information can still be easily leaked (Lyu\net al. 2020; Liu, Xu, and Wang 2022). For instance, an ad-\nversary with access to the exchanged messages and knowl-\nedge of some public information (e.g., client’s provided rat-\nings) (Lyu and Chen 2021; Chen et al. 2022; Feng et al.\n2021) can reconstruct a client’s sensitive attributes (e.g.,\ngender/religion) in an attack known as attribute inference\nattack (AIA). Additionally, the adversary can reconstruct\nclient’s training samples such as images (Geiping et al.\n2020; Yin et al. 2021).\nHowever, these reconstruction attacks for FL have pri-\nmarily been tested on classification tasks and have not been\nFigure 1: Average performance of different AIAs when four\nclients train a neural network through FedAvg with 1 local\nepoch and batch size 32. Each client stores |Dc| data points\nrandomly sampled from ACS Income dataset (Ding et al.\n2024). The adversary infers the gender attribute of every data\nsample held by the client given access to the released (pub-\nlic) information.\nexplored for regression tasks, which are, needless to say,\nequally important for practical applications. Quite surpris-\ningly, our experiments, as shown in Fig. 1, demonstrate that\nthe accuracy of state-of-the-art gradient-based AIA under\na honest-but-curious adversary (Lyu and Chen 2021; Chen\net al. 2022) (referred to as passive) drops significantly from\n71% on a classification task to 50% (random guess) on a re-\ngression task once the targeted client holds more than 256\ndata points. Furthermore, even a more powerful (active) ad-\nversary capable of forging the messages to the targeted client\nto extract more information (Lyu and Chen 2021; Chen et al.\n2022) offers only limited improvement to the AIA perfor-\nmance on regression tasks. Detailed information about this\nexperiment is in Appendix B.1.\nIn this paper, we show that federated training of regression\ntasks does not inherently enjoy higher privacy, but it is sim-\nply more vulnerable to other forms of attacks. While existing\nFL AIA attacks for classification tasks are gradient-based\n(see Sec. 2.3), we show that model-based AIAs—initially\nproposed for centralized training (Fredrikson et al. 2014;\narXiv:2411.12697v1  [cs.LG]  19 Nov 2024\n\nKasiviswanathan, Rudelson, and Smith 2013; Yeom et al.\n2018)—may be more effective for regression tasks. Figure 1\nillustrates that a model-based attack on the server’s global\nmodel (i.e., the final model trained through FL) already per-\nforms at least as well as the SOTA gradient-based passive at-\ntack. Moreover, it highlights that even more powerful attacks\n(up to 30 p.p. more accurate) could be launched if the adver-\nsary had access to the optimal local model of the targeted\nclient (i.e., a model trained only on the client’s dataset).\nMotivated by these observations, we propose a new two-\nstep model-based AIA for federated regression tasks. In this\nattack, the adversary first (approximately) reconstructs the\nclient’s optimal local model and then applies an existing\nmodel-based AIA to that model.\nOur main contributions can be summarized as follows:\n• We provide an analytical lower bound for model-based\nAIA accuracy in the least squares regression problem.\nThis result motivates the adversary’s strategy to approxi-\nmate the client’s optimal local model in federated regres-\nsion tasks. (Sec. 3).\n• We propose methods for approximating optimal local\nmodels where adversaries can either eavesdrop on ex-\nchanged messages or directly interfere with the training\nprocess (Sec. 4).\n• Our experiments show that our model-based AIAs are\nbetter candidates for empirically quantifying privacy\nleakage for federated regression tasks (Sec. 5).\n2\nPreliminaries\n2.1\nFederated learning\nWe denote by C the set of all clients participating to FL.\nLet Dc = {(xc(i), yc(i)), i = 1, ..., Sc} denote the local\ndataset of client c ∈C with size Sc ≜|Dc|. Each data sam-\nple (xc(i), yc(i)) is a pair consisting of an input xc(i) and\nof an associated target value yc(i). In FL, clients cooperate\nto learn a global model, which minimizes the following em-\npirical risk over all the data owned by clients:\nmin\nθ∈Rd L(θ) =\nX\nc∈C\npcLc(θ, Dc)\n=\nX\nc∈C\npc\n \n1\nSc\nSc\nX\ni=1\nℓ(θ, xc(i), yc(i))\n!\n.\n(1)\nwhere ℓ(θ, xc(i), yc(i)) measures the loss of the model θ on\nthe sample (xc(i), yc(i)) ∈Dc and pc is the positive weight\nof client c, s.t., P\nc∈C pc = 1. Common choices of weights\nare pc =\n1\n|C| or pc =\nSc\nP\nc∈C Sc .\nAlgorithm 1: FL Framework\nOutput: θT\nServer s:\n1: for t ∈{0, ..., T −1} do\n2:\ns selects a subset of the clients Ct\ns ⊆C,\n3:\ns broadcasts the global model θt to Ct\ns,\n4:\ns waits for the updated models θt\nc from every client c ∈Ct\ns,\n5:\ns computes θt+1 by aggregating the received updated mod-\nels.\nClient c ∈C: Input θ, Output θc\n6: while FL training is not completed do\n7:\nc listens for the arrival of new global model θ,\n8:\nc updates its local model: θc ←Local Updatec(θ, Dc)\n9:\nc sends back θc to the server.\nAlgorithm\n2:\nClient\nc’s\nlocal\nupdate\nrule\n(Local Updatec(θt, Dc)) in FedAvg (McMahan et al.\n2017)\nInput: server model θt, local dataset Dc, batch size B, number of\nlocal epochs E, learning rate η.\n1: θt\nc(0) ←θt, B ←(split Dc into batches of size B), k ←0\n2: for each local epoch e from 1 to E do\n3:\nfor batch b ∈B do\n4:\nθt\nc(k + 1)\n←\nθt\nc(k) −η × g(θt\nc(k), b),\nwhere\ng(θt\nc(k), b) =\n1\nB\nP\nx∈b ∇ℓ(θt\nc(k), x)\n5:\nk ←k + 1\n6: Return θt\nc(K), where K = E⌈Sc\nB ⌉\nLet θ∗= arg minθ∈Rd L(θ) be a global optimal model,\ni.e., a minimizer of Problem (1). A general framework to\nlearn such a global model in a federated way is shown\nin Algorithm 1; it generalizes a large number of FL al-\ngorithms, including FedAvg (McMahan et al. 2017), Fed-\nProx (Li et al. 2020), and FL with different client sam-\npling techniques (Nishio and Yonetani 2019; Chen, Horvath,\nand Richtarik 2020; Jee Cho, Wang, and Joshi 2022). The\nmodel θT —the output of Algorithm 1—is the tentative so-\nlution of Problem (1). Its performance depends on the spe-\ncific FL algorithm, which precises how clients are selected\nin line 2, how the updated local models are aggregated in\nline 5, and how the local update rule works in line 8. For ex-\nample, in FedAvg, clients are selected uniformly at random\namong the available clients, the local models are averaged\nwith constant weights, and the clients perform locally mul-\ntiple stochastic gradient steps (Algorithm 2).\n2.2\nThreat Model\nHonest-but-curious\nadversary\nWe\ndescribe\nfirst\nan\nhonest-but-curious adversary,1 which is a standard threat\nmodel in existing literature (Melis et al. 2019; Geiping et al.\n2020; Yin et al. 2021; Nasr, Shokri, and Houmansadr 2019),\nincluding the FL one (Kairouz et al. 2021, Table 7), (Lyu\nand Chen 2021; Chen et al. 2022). This passive adversary,\nwho could be the server itself, is knowledgeable about the\n1In what follows, we refer to the client using female pronouns\nand the adversary using male pronouns, respectively.\n\ntrained model structure, the loss function, and the train-\ning algorithm, and may eavesdrop on communication be-\ntween the attacked client and the server but does not inter-\nfere with the training process. For instance, during training\nround t, the adversary can inspect the messages exchanged\nbetween the server and the attacked client (denoted by c),\nallowing him to recover the parameters of the global model\nθt and the updated client model θt\nc(K) (Algorithm 2). Let\nTc ⊆{t|c ∈Ct\ns, ∀t ∈0, . . . , T −1} denote the set of com-\nmunication rounds during which the adversary inspects mes-\nsages exchanged between the server and the attacked client\nand Mc = {(θt, θt\nc), ∀t ∈Tc} denote the corresponding set\nof messages.\nWhen it comes to defenses against such an adversary,\nit is crucial to understand that traditional measures like\nencrypted communications are ineffective if the attacker\nis the FL server. More sophisticated cryptographic tech-\nniques like secure aggregation protocols (Bonawitz et al.\n2017; Kadhe et al. 2020) allow the server to aggregate lo-\ncal updates without having access to each individual update\nand, then, do hide the client’s updated model θt\nc from the\nserver. Nevertheless, they come with a significant compu-\ntation overhead (Quoc et al. 2020) and are inefficient for\nsparse vector aggregation (Kairouz et al. 2021). Moreover,\nthey are vulnerable to poisoning attacks, as they hinder the\nserver from detecting (and removing) potentially harmful\nupdates from malicious clients (Blanchard et al. 2017; Yin\net al. 2018; El Mhamdi 2020). For instance, Tram`er et al.\n(2022, Sec. 4.4) introduce a new class of data poisoning at-\ntacks that succeed when training models with secure mul-\ntiparty computation. Alternatively, Trusted Execution Envi-\nronments (TEEs) (Sabt, Achemlal, and Bouabdallah 2015;\nSingh et al. 2021) provide an encrypted memory region to\nensure the code has been executed faithfully and privately.\nThey can then both conceal clients’ updated models and\ndefend against poisoning attacks. However, implementing\na reliable TEE platform in FL remains an open challenge\ndue to the infrastructure resource constraints and the re-\nquired communication processes needed to connect verified\ncodes (Kairouz et al. 2021).\nMalicious adversary\nWe also consider a stronger ac-\ntive adversary who can interfere with the training process.\nSpecifically, this adversary can modify the messages sent\nto the clients and have the clients update models θt con-\ncocted to reveal more private information. Let T a\nc be the\nset of rounds during which the adversary attacks client c by\nsending malicious model θt. As above, the adversary could\nbe the server itself. This adversary has been widely consid-\nered in the literature on reconstruction attacks (Wen et al.\n2022; Boenisch et al. 2023) and membership inference at-\ntacks (Nguyen et al. 2023; Nasr, Shokri, and Houmansadr\n2019). Some studies have also explored the possibility of a\nmalicious adversary modifying the model architecture dur-\ning training (Fowl et al. 2022; Zhao et al. 2023), even though\nsuch attacks appear to be easily detectable. In this paper, we\ndo not allow the adversary to modify the model architecture.\nFor simplicity, we will refer to these two adversaries as\npassive and active adversaries, respectively, throughout the\nrest of the paper.\n2.3\nAttribute inference attack (AIA) for FL\nAIA leverages public information to deduce private or sensi-\ntive attributes (Kasiviswanathan, Rudelson, and Smith 2013;\nLyu and Chen 2021; Chen et al. 2022; Fredrikson et al.\n2014; Yeom et al. 2018). For example, an AIA could recon-\nstruct a user’s gender from a recommender model by having\naccess to the user’s provided ratings. Formally, each input\nxc(i) of client c consists of public attributes xp\nc(i) and of\na sensitive attribute sc(i). The target value, assumed to be\npublic, is denoted by yp\nc(i). The adversary, having access to\n{(xp\nc(i), yp\nc(i)), i = 1, ..., Sc} and Mc, aims to recover the\nsensitive attributes sc(i).2\nExisting gradient-based AIA for FL\nChen et al. (2022)\npresent AIAs specifically designed for the FL context and\nboth passive and active adversaries. The central idea in-\nvolves identifying sensitive attribute values that yield virtual\ngradients closely resembling the client’s model updates—\nreferred to as pseudo-gradients—in terms of cosine similar-\nity. Formally, the adversary solves the following optimiza-\ntion problem:\nargmax\n{sc(i)}Sc\ni=1\nX\nt∈T\nCosSim\n\u0012∂ℓ(θt, {(xp\nc(i), sc(i), yp\nc(i))})\n∂θt\n, θt −θt\nc\n\u0013\n,\n(2)\nwhere T ⊆Tc for a passive adversary and T ⊆Tc ∪T a\nc for\nan active adversary. The active adversary simply sends back\nto the targeted client c her own model θ(t−1)\nc\nat each attack\nround in T a\nc .\nChen et al. (2022) assume that the sensitive attributes\nare categorical and discrete random variables. Neverthe-\nless, problem (2) can be solved efficiently using a gradient\nmethod with the reparameterization trick and the Gumbel\nsoftmax distribution (Jang, Gu, and Poole 2017). From (2),\nwe observe that, since gradients incorporate information\nfrom all samples, the attack performance deteriorates in the\npresence of a large local dataset. For example, the attack ac-\ncuracy almost halves on the Genome dataset for the clas-\nsification task when the client’s local dataset size increases\nfrom 50 to 1000 samples (Lyu and Chen 2021, Table 8). Our\nexperiment (Figure 1) on a regression task corroborates this\nfinding: when the local dataset size increases from 64 to 256,\nthe attack accuracy drops from 60% to the level of random\nguessing.\nModel-based AIA\nAs an alternative, the AIA can be ex-\necuted directly on the model (rather than on the model\npseudo-gradients), as initially proposed for centralized\ntraining in (Kasiviswanathan, Rudelson, and Smith 2013;\nFredrikson et al. 2014). Given a model θ, the adversary can\ninfer the sensitive attributes by solving the following opti-\nmization problems:\nargmin\nsc(i)\nℓ(θ, (xp\nc(i), sc(i), yp\nc(i))), ∀i ∈{1, ..., Sc},\n(3)\n2In\n(Fredrikson et al. 2014; Yeom et al. 2018), the adver-\nsary possesses additional information, including estimates of the\nmarginals or the joint distribution of the data samples. However, in\nthis paper, we do not consider such a more powerful adversary.\n\nBelow we provide theoretical guarantees (Prop. 1) for the\naccuracy of the model-based AIAs to least squares regres-\nsion problems. Our theoretical result supports that in an FL\nsetting, the adversary may advantageously start by estimat-\ning the client’s optimal local model, i.e., the model that min-\nimizes the client’s empirical loss.\n3\nModel-based AIA guarantees for least\nsquares regression.\nIn this section, we provide novel theoretical guarantees for\nthe accuracy of the model-based AIA (Problem (3)) in the\ncontext of least squares regression. In particular, we show\nthat the better the model θ fits the local data and the more\nthe sensitive attribute affects the final prediction, the higher\nthe AIA accuracy.\nProposition 1. Let Ec be the mean square error of a given\nleast squares regression model θ on the local dataset of\nclient c and θ[s] be the model parameter corresponding to a\nbinary sensitive attribute. The accuracy of the model-based\nAIA (3) is larger than or equal to 1 −4Ec\nθ[s]2 .\nProof. Let sc be the vector including all the unknown sensi-\ntive binary attributes {sc(i), ∀i ∈{1, ..., Sc}} of client c. Let\nxc ∈RSc×d be the design matrix with rank d and yc ∈RSc\nbe the labels in the local dataset Dc of the client c. Let\nθ[: p] ∈Rd−1 be the parameters corresponding to the public\nattributes. The adversary has access to partial data instances\nin Dc which consists of the public attributes P ∈RSc×(d−1)\nand the corresponding labels yc ∈RSc.\nThe goal for the adversary is to decode the values of the\nbinary sensitive attribute sc ∈{0, 1}Sc given (P, yc) by\nsolving (3), i.e., checking for each point, which value for\nthe sensitive attribute leads to a smaller loss.\nIt is easy to check that the problem can be equivalently\nsolved through the following two-step procedure. First, the\nadversary computes the vector of real values:\n˜sc = argmin\nsc∈RSc\n||Pθ[: p] + scθ[s] −yc||2\n2 = yc −Pθ[: p]\nθ[s]\n.\nThen, the adversary reconstruct the vector of sensitive fea-\ntures ˆsc ∈{0, 1}Sc as follows\nˆsc(i) =\n\u001a0\nif ˜sc(i) < 1\n2\n1\notherwise\n, ∀i ∈{1, ..., Sc}.\n(4)\nLet e be the vector of residuals for the local dataset, i.e.,\nec = yc −(Pθ[: p] + scθ[s]). We have then\nsc = yc −Pθ[: p] −ec\nθ[s]\n= ˜sc −ec\nθ[s].\n(5)\nLet us say that the sensitive feature of sample i has been\nerroneously reconstructed, i.e., sc(i) ̸= ˆsc(i), then (4), im-\nplies that |sc(i) −˜sc(i)| ≥1/2, and from (5) it follows\nthat |ec(i)|2 ≥θ[s]2/4. As EcSc = ∥ec∥2\n2, there can be at\nmost 4EcSc/θ[s]2 samples erroneously reconstructed, from\nwhich we can conclude the result.\nAlgorithm 3: Reconstruction of client-c local model by a\npassive adversary for federated least squares regression\nInput: the server models θti(0) = θti\nc (0) and the local up-\ndated models θti\nc (K) at all the inspected rounds ti ∈Tc =\n{t1, t2, . . . , tnc}.\n1: Let Θin = [θt1\nc (0) θt2\nc (0) ... θtnc\nc\n(0)]T ∈Rnc×d\n2: Let Θout =\n\n\n(θt1\nc (0) −θt1\nc (K))T\n1\n...\n(θtnc\nc\n(0) −θtnc\nc\n(K))T\n1\n\n∈Rnc×(d+1)\n3: (ˆθ∗\nc)T ←last row of\n\u0000(ΘT\noutΘout)†ΘT\noutΘin\n\u0001\n4: Return ˆθ∗\nc as the estimator for client c’s local model\nProposition 1 indicates that the model-based AIA per-\nforms better the more the model overfits the dataset. This ob-\nservation justifies the following two-step attack in a FL set-\nting: the adversary first reconstructs the optimal local model\nof the targeted client c, i.e., θ∗\nc = arg minθ∈Rd Lc(θ, Dc),\nand then execute the AIA in (3) on the reconstructed model.3\nIn the following, we will detail our approaches for recon-\nstructing the optimal local model by passive and active ad-\nversary, respectively.\n4\nReconstructing the local model in FL\nIn this section, we show how an adversary may recon-\nstruct the optimal local model of client c, i.e., θ∗\nc\n=\narg minθ∈Rd Lc(θ, Dc).\nFirst, we provide an efficient approach for least squares\nregression under a passive adversary. We prove that the ad-\nversary can exactly reconstruct the optimal local model un-\nder deterministic FL updates and provide probabilistic guar-\nantees on the reconstruction error under stochastic FL up-\ndates (Sec. 4.1).\nSecond, we show that an active adversary can potentially\nreconstruct any client’s local model (not just least square re-\ngression ones) in a federated setting (Sec. 4.2).\n4.1\nPassive approach for linear least squares\nWe consider that clients cooperatively train a linear regres-\nsion model with quadratic loss. We refer to this setting as a\nfederated least squares regression. The attack is detailed in\nAlg. 3 and involves a single matrix computation (line 3) after\nthe exchanged messages Mc have been collected. A† rep-\nresents the pseudo-inverse of matrix A. Theorem 1 provides\ntheoretical guarantees for the distance between the recon-\nstructed model and the optimal local one, when the model is\ntrained through FedAvg (McMahan et al. 2017) with batch\nsize B and local epochs E. The formal statement of the the-\norem and its proof are in Appendix A.1.\nTheorem 1 (Informal statement). Consider a federated\nleast squares regression with a large number of clients and\nassume that i) client c has d-rank design matrix xc ∈RSc×d,\n3Note that this model is optimal in terms of the training error,\nbut not in general of the final test error. On the contrary, it is likely\nto overfit the local dataset.\n\nii) she updates the global model through stochastic gradient\nsteps with sub-Gaussian noise with scale σ and small learn-\ning rate η, and iii) the local data distribution of the targeted\nclient is different from the global data distribution. By eaves-\ndropping on nc > d message exchanges between client c and\nthe server, the error of the reconstructed model ˆθ∗\nc of Algo-\nrithm 3 is upper bounded w.p. ≥1−δ when η ≤\nSc\n2λmax(xT\nc xc)\nand\n\r\r\rˆθ∗\nc −θ∗\nc\n\r\r\r\n2 = O\n\nησd\ns\nE\n\u0018Sc\nB\n\u0019 d + 1 + ln 2d\nδ\nnc · λ\n\n. (6)\nWhen the batch size is equal to the local dataset size,\nAlg. 3 exactly reconstructs the optimal local model (proof\nin Appendix A.2):\nProposition 2. Consider a federated least squares regres-\nsion and assume that client c has d-rank design matrix and\nupdates the global model through full-batch gradient up-\ndates (i.e., B = Sc) and an arbitrary number of local\nepochs. Once the adversary eavesdrops on d + 1 commu-\nnication exchanges between client c and the server, it can\nrecover the client’s optimal local model.\nFinally, the following proposition shows that our attack\nfor the full-batch gradient case is order-optimal in terms of\nthe number of messages the adversary needs to eavesdrop.\nThe proof is in Appendix A.3.\nProposition 3. Consider that the federated least squares\nregression is trained through FedAvg with one local step\n(E = 1) and full batch (B = Sc for each client c ∈C). At\nleast one client is required to communicate with the server\nΩ(d) times for the global model to be learned, and the ad-\nversary needs to eavesdrop at least Ω(d) messages from this\nclient to reconstruct her optimal local model.\nToy example illustration\nHere, we illustrate the perfor-\nmance of our Alg. 3 on a toy dataset detailed in Ap-\npendix B.2. Figure 2 (left) demonstrates that as the batch\nsize increases, the reconstructed model is closer to the op-\ntimal local model (as shown in our Theorem 1). Moreover,\nsince the model overfits more the dataset (with smaller loss),\nthe AIA accuracy increases (Figure 2 (right)).\nFigure 2: The performance of our passive approach for re-\nconstructing optimal local model (left) and the triggered\nAIA (right) on a toy dataset with two clients training a linear\nmodel with size d = 11 under batch size 64, 256 and 1024\nfor 5 seeds each, respectively. The passive adversary only\neavesdropped d + 1 messages.\nAlgorithm 4: Reconstruction of client-c local model by an\nactive adversary a\nInput: Let T a\nc\nbe set of rounds during which the adver-\nsary attacks client c and θa\nc be the corresponding malicious\nmodel.\n1: θa\nc ←latest model received from client c\n2: for t ∈T a\nc do\n3:\na sends the model θa\nc to client c,\n4:\na waits the updated model from θc from client c,\n5:\na computes the pseudo-gradient θa\nc −θc and updates\nθa\nc and the corresponding moment vectors following\nAdam (Kingma and Ba 2015, Alg. 1),\n6: Return θa\nc as the estimator for client c’s local model\n4.2\nActive approach\nHere we consider an active adversary (e.g., the server itself)\nthat can modify the global model weights θt to recover the\nclient’s optimal local model. To achieve this, the adversary\ncan simply send back to the targeted client c her own model\nθ(t−1)\nc\ninstead of the averaged model θt. In this way, client c\nis led to compute her own optimal local model.\nWe propose a slightly more sophisticated version of this\nactive attack. Specifically, we suggest that the adversary em-\nulates the Adam optimization algorithm (Kingma and Ba\n2015) for the model updated by client c by adjusting the\nlearning rate for each parameter based on the magnitude and\nhistory of the gradients, and incorporating momentum. The\nmotivation is twofold. First, the client does not receive back\nexactly the same model and thus cannot easily detect the at-\ntack. Second, Adam is known to minimize the training error\nfaster than stochastic gradient descent at the cost of overfit-\nting more to the training data (Zhou et al. 2020b; Zou et al.\n2023). We can then expect Adam to help the adversary re-\nconstruct the client’s optimal local model better for a given\nnumber of modified messages, and our experiments in Sec. 5\nconfirm that this is the case.\nThe details of this attack are outlined in Alg. 4. We ob-\nserve that the adversary does not need to systematically\nmodify all messages sent to the target client c but can modify\njust a handful of messages that are not necessarily consecu-\ntive. This contributes to the difficulty of detecting the attack.\n5\nExperiments\n5.1\nDatasets\nMedical (Lantz 2013).\nThis dataset includes 1,339\nrecords and 6 features: age, gender, BMI, number of chil-\ndren, smoking status, and region. The regression task is to\npredict each individual’s medical charges billed by health\ninsurance. The dataset is split i.i.d. between 2 clients.\nIncome (Ding et al. 2024).\nThis dataset contains census\ninformation from 50 U.S. states and Puerto Rico, spanning\nfrom 2014 to 2018. It includes 15 features related to demo-\ngraphic information such as age, occupation, and education\nlevel. The regression task is to predict an individual’s in-\ncome. We investigate two FL scenarios, named Income-L\n\nand Income-A, respectively. In Income-L, there are 10\nclients holding only records from the state of Louisiana\n(15,982 records in total). These clients can be viewed as the\nlocal entities working for the Louisiana State Census Data\nCenter. We examine various levels of statistical heterogene-\nity among these local entities, with the splitting strategy de-\ntailed in Appendix B.3. In Income-A, there are 51 clients,\neach representing a census region and collectively covering\nall regions. Every client randomly selects 20% of the data\npoints from the corresponding census region, resulting in a\ntotal of 332,900 records.\nFor all the datasets, each client keeps 90% of its data for\ntraining and uses 10% for validation.\n5.2\nFL training and attack setup\nIn all the experiments, each client follows FedAvg (Alg. 2)\nto train a neural network model with a single hidden layer of\n128 neurons, using ReLU as activation function. The num-\nber of communication rounds is fixed to T = ⌈100/E⌉\nwhere E is the number of local epochs. Each client partic-\nipates to all rounds, i.e., Tc = {0, ..., T −1}. The learning\nrate is tuned for each training scenario (different datasets\nand number of local epochs), with values provided in Ap-\npendix B.4. The passive adversary may eavesdrop all the\nexchanged messages until the end of the training. The ac-\ntive adversary launches the attack after T rounds4 for addi-\ntional ⌈10/E⌉and ⌈50/E⌉rounds. Every attack is evaluated\nover FL trainings from 3 different random seeds. For Med-\nical dataset, the adversary infers whether a person smokes\nor not. For Income-L and Income-A datasets, the adversary\ninfers the gender. The AIA accuracy is the fraction of the\ncorrect inferences over all the samples. We have also con-\nducted experiments for federated least squares regression on\nthe same datasets. The results can be found in Appendix C.1.\n5.3\nBaselines and our attack implementation\nGradient-based:\nWe compare our method with the\n(gradient-based) SOTA (Sec. 2.3). The baseline performance\nis affected by the set of inspected rounds T considered\nin (2). We select the inspected rounds T based on two crite-\nria: the highest cosine similarity and the best AIA accuracy.\nIn a real attack, the adversary is not expected to know the at-\ntack accuracy beforehand. Therefore, we refer to the attack\nbased on the highest cosine similarity as Grad and to the\nother as Grad-w-O (Gradient with Oracle), as it assumes\nthe existence of an oracle that provides the attack accuracy.\nThe details for the tuning of T and other hyper-parameter\nsettings can be found in Appendix B.4.\nOur Attacks:\nOur attacks consist of two steps: 1) re-\nconstructing the optimal local model, and 2) executing the\nmodel-based AIA (3). For the first step, a passive adversary\nuses the last-returned model from the targeted client, while\nan active adversary executes Alg. 4. The details of the hy-\nperparameter settings can be found in Appendix B.4 .\n4We also examine the impact of the attack’s starting round. The\nresults are presented in the Appendix C.2.\nModel-based with Oracle (Model-w-O):\nTo provide an\nupper bound on the performance of our approach, we as-\nsume the existence of an oracle that provides the optimal\nlocal model for the first step of our attack. In practice, the\noptimal local model is determined empirically by running\nAdam for a very large number of iterations.\n5.4\nExperimental results\nAIA (%)\nDatasets\nIncome-L Income-A\nMedical\nPassive\nGrad\n60.36±0.67 54.98±0.29 87.26±0.92\nGrad-w-O 71.44±0.33 56.10±1.12 91.06±0.55\nOurs\n75.27±0.32 55.75±0.17 95.90±0.04\nActive\n(10 Rnds)\nGrad\n60.24±0.60 54.98±0.29 87.26±0.92\nGrad-w-O 80.69±0.55 56.10±1.12 91.06±0.55\nOurs\n82.02±0.85 63.53±0.73 95.93±0.07\nActive\n(50 Rnds)\nGrad\n60.24±0.60 53.36±0.40 87.26±0.92\nGrad-w-O 80.69±0.55 56.12±0.12 91.06±0.55\nOurs\n94.31±0.11 78.09±0.25 96.79±0.79\nModel-w-O\n94.31±0.11 78.31±0.07 96.79±0.79\nTable 1: The AIA accuracy over all clients’ local datasets\nevaluated under both honest-but-curious (passive) and mali-\ncious (active) adversaries across Income-L, Income-A, and\nMedical FL datasets (Sec. 5.1). The standard deviation is\nevaluated over three FL training runs with different random\nseeds. All clients run FedAvg with 1 epoch and batch size\n32. For Income-L, we consider the dataset with 40% of data\nheterogeneity (Appendix B.3).\nFrom Table 1, we see that our attacks outperform\ngradient-based ones in both passive and active scenarios\nacross all three datasets. Notably, our passive attack achieves\nimprovements of over 15 and 8 percentage points (p.p.) for\nthe Income-L and Medical datasets, respectively. Even when\nthe gradient-based method has access to an oracle, our pas-\nsive attacks still achieves higher accuracy on two datasets\nand comes very close on Income-A. When shifting to ac-\ntive attacks, the gains are even more substantial. For in-\nstance, when the attack is active for 50 rounds, we achieve\ngains of 13, 22, and 5 percentage points (p.p.) in Income-L,\nIncome-A, and Medical, respectively, over Grad-w-O, and\neven larger gains over the more realistic Grad. Furthermore,\nthe attack accuracy reaches the performance expected from\nan adversary who knows the optimal local model. Interest-\ningly, while our attacks consistently improve as the adver-\nsary’s capacity increases (moving from a passive attacker to\nan active one and increasing the number of rounds of the ac-\ntive attack), this is not the case for gradient-based methods.\nImpact of data heterogeneity.\nWe simulate varying lev-\nels of heterogeneity in the Income-L dataset and illustrate\nhow the attack performance evolves in Figure 3 (left). First,\nwe observe that as the data is more heterogeneously split,\nthe accuracy of AIA improves for all approaches. Indeed,\nthe data splitting strategy (Appendix B.3) leads to a greater\ndegree of correlation between the clients’ sensitive attributes\nand the target variable at higher levels of heterogeneity. This\n\nFigure 3: The AIA accuracy over all clients’ local datasets under different heterogeneity levels (left) (0% represents i.i.d case),\nbatch sizes (center), and local epochs (right) for Income-L dataset. The default values for heterogeneity level, batch size B and\nlocal epochs E are set to 40%, 32, and 1, respectively. The malicious adversary attacks ⌈50/E⌉rounds after ⌈100/E⌉communi-\ncation rounds. Crosses represent passive attacks, while dots represent active attacks. Dashed lines correspond to gradient-based\nattacks (Grad), and solid lines correspond to model-based attacks (Ours and Model-w-O).\nintrinsic correlation facilitates the operation of all AIAs. We\nobserve in these experiments, as in previous ones, that ac-\ntive Grad does not necessarily offer advantages over passive\nGrad. In the more homogeneous case (which is less realis-\ntic in an FL setting), our passive attack performs slightly\nworse than Grad, but its accuracy increases more rapidly\nwith heterogeneity, resulting in an AIA accuracy advantage\nof over 20 p.p. in the most heterogeneous case. Our active\nattack over 50 additional rounds consistently outperforms\nGrad by at least 30 p.p. and is almost indistinguishable from\nModel-w-O.\nImpact of batch size.\nFigure 3 (center) shows that the per-\nformance of our attacks slightly decrease as the batch size\nincreases. A possible explanation for this is that a larger\nbatch size results in fewer local updates per epoch, leading\nthe client to return a less overfitted model. Despite this, our\napproach consistently outperforms Grad in all cases.\nImpact of the number of local epochs.\nFigure 3 (right)\nshows that under passive attacks, our approach is not sensi-\ntive to the number of local epochs, whereas Grad’s perfor-\nmance deteriorates to the level of random guessing as the\nnumber of local epochs increases to 5. Interestingly, active\nGrad significantly improves upon passive Grad as the num-\nber of local epochs increases. While our active approach pro-\ngressively performs slightly worse, it still maintains an ad-\nvantage of over 10 p.p. compared to Grad.\nImpact of the local dataset size.\nThe figure on the\nright\nshows\nhow\neach client’s dataset\nsize\nimpacts\nin-\ndividual\n(active)\nAIA\naccuracy\nin\nIncome-A\ndataset.\nThe\nexperiment\nsetting is the same\nas in Table 1 with 50\nactive rounds.\nWe observe that for all methods, clients with smaller\ndatasets are more vulnerable to the attacks, because the\nmodels overfit the dataset more easily. Moreover, for clients\nwith more than 20000 data points, Grad provides an attack\nperformance close to random guessing.\nImpact of defense mechanisms.\nTo mitigate privacy leak-\nage, we apply a federated version of DP-SGD (Abadi\net al. 2016), which provides sample-level differential privacy\nguarantees (Dwork et al. 2006; Choudhury et al. 2019). Our\nexperiments show that even with this defense mechanism in\nplace, our approach still outperforms the baselines in most\nof the scenarios. The results are in Appendix C.4.\n6\nDiscussion and conclusions\nIn our work, we have demonstrated the effectiveness of us-\ning model-based AIA for federated regression tasks, when\nthe attacker can approximate the optimal local model. For\nan honest-but-curious adversary, we proposed a computa-\ntionally efficient approach to reconstruct the optimal linear\nlocal model in least squares regression. In contrast, for neu-\nral network scenarios, our passive approach involves directly\nutilizing the last returned model (Sec. 5.3). We believe more\nsophisticated reconstruction techniques may exist, and we\nplan to investigate this aspect in future work.\nThe reader may wonder if the superiority of model-based\nattacks over gradient-based ones also holds for classification\ntasks. Some preliminary experiments we conducted suggest\nthat the relationship is inverted. We advance a partial expla-\nnation for this observation. For a linear model with binary\ncross-entropy loss, it can be shown that the model-based\nAIA (3) on a binary sensitive attribute is equivalent to a sim-\nple label-based attack, where the attribute is uniquely recon-\nstructed based on the label. This approach leads to poor per-\nformance because the attack relies only on general popula-\ntion characteristics and ignores individual specificities. This\nobservation also holds experimentally for neural networks\ntrained on the Income-L dataset, largely due to the inherent\nunfairness of the learned models. For example, for the same\n\nset of public features, being a man consistently results in a\nhigher probability of being classified as wealthy compared\nto being a woman. As a consequence, the AIA infers gender\nsolely based on the high or low income label. These prelim-\ninary remarks may prompt new interesting perspectives on\nthe relationship between fairness and privacy (see for exam-\nple the seminal paper (Chang and Shokri 2021)).\nLast but not least, to mitigate privacy leakage, beyond dif-\nferential privacy mechanisms, there are empirical defenses\nsuch as Mixup (Zhang et al. 2018), which trains a model\non composite data samples obtained through linear com-\nbinations of sample pairs, and TAPPFL (Arroyo Arevalo\net al. 2024), which learns low-dimensional representations\nthat minimize information about private attributes. However,\nthe effectiveness of these defenses has not yet been demon-\nstrated on regression tasks, and we leave this for future work.\n7\nAcknowledgements\nThis research was supported in part by the Groupe La\nPoste, sponsor of the Inria Foundation, in the framework\nof the FedMalin Inria Challenge, as well as by the France\n2030 program, managed by the French National Research\nAgency under grant agreements No. ANR-21-PRRD-0005-\n01, ANR-23-PECL-0003, and ANR-22-PEFT-0002. It was\nalso funded in part by the European Network of Excel-\nlence dAIEDGE under Grant Agreement Nr. 101120726,\nby SmartNet and LearnNet, and by the French govern-\nment National Research Agency (ANR) through the UCA\nJEDI (ANR-15-IDEX-01), EUR DS4H (ANR-17-EURE-\n004), and the 3IA Cˆote d’Azur Investments in the Future\nproject with the reference number ANR-19-P3IA-0002.\nThe authors are grateful to the OPAL infrastructure from\nUniversit´e Cˆote d’Azur for providing resources and support.\nReferences\nAbadi, M.; Chu, A.; Goodfellow, I.; McMahan, H. B.;\nMironov, I.; Talwar, K.; and Zhang, L. 2016. Deep Learning\nwith Differential Privacy. In Proceedings of the 2016 ACM\nSIGSAC Conference on Computer and Communications Se-\ncurity. ACM.\nAgarwal, A.; Negahban, S.; and Wainwright, M. J. 2012.\nStochastic optimization and sparse statistical recovery: Op-\ntimal algorithms for high dimensions. Advances in Neural\nInformation Processing Systems, 25.\nAkiba, T.; Sano, S.; Yanase, T.; Ohta, T.; and Koyama, M.\n2019.\nOptuna: A Next-generation Hyperparameter Opti-\nmization Framework.\nIn Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discov-\nery & Data Mining, KDD ’19, 2623–2631. New York,\nNY, USA: Association for Computing Machinery.\nISBN\n9781450362016.\nArroyo Arevalo, C.; Noorbakhsh, S. L.; Dong, Y.; Hong,\nY.; and Wang, B. 2024. Task-Agnostic Privacy-Preserving\nRepresentation Learning for Federated Learning against At-\ntribute Inference Attacks. Proceedings of the AAAI Confer-\nence on Artificial Intelligence, 38: 10909–10917.\nBlanchard, P.; El Mhamdi, E. M.; Guerraoui, R.; and Stainer,\nJ. 2017. Machine learning with adversaries: Byzantine tol-\nerant gradient descent. Advances in neural information pro-\ncessing systems, 30.\nBoenisch, F.; Dziedzic, A.; Schuster, R.; Shamsabadi, A. S.;\nShumailov, I.; and Papernot, N. 2023. When the Curious\nAbandon Honesty: Federated Learning Is Not Private. In\n2023 IEEE 8th European Symposium on Security and Pri-\nvacy (EuroS&P), 175–199.\nBonawitz, K.; Ivanov, V.; Kreuter, B.; Marcedone, A.;\nMcMahan, H. B.; Patel, S.; Ramage, D.; Segal, A.; and\nSeth, K. 2017. Practical Secure Aggregation for Privacy-\nPreserving Machine Learning.\nIn Proceedings of the\n2017 ACM SIGSAC Conference on Computer and Com-\nmunications Security, CCS ’17, 1175–1191. New York,\nNY, USA: Association for Computing Machinery.\nISBN\n9781450349468.\nChang, H.; and Shokri, R. 2021. On the privacy risks of\nalgorithmic fairness. In 2021 IEEE European Symposium\non Security and Privacy (EuroS&P), 292–303. IEEE.\nChen, C.; Lyu, L.; Yu, H.; and Chen, G. 2022. Practical At-\ntribute Reconstruction Attack Against Federated Learning.\nIEEE Transactions on Big Data, 1–1.\nChen, W.; Horvath, S.; and Richtarik, P. 2020.\nOptimal\nClient Sampling for Federated Learning.\nWorkshop in\nNeurIPS 2020: Privacy Preserving Machine Learning.\nChoudhury, O.; Gkoulalas-Divanis, A.; Salonidis, T.; Sylla,\nI.; Park, Y.; Hsu, G.; and Das, A. 2019.\nDifferential\nPrivacy-enabled Federated Learning for Sensitive Health\nData. Workshop on Machine learning for Health in NeurIPS.\nDing, F.; Hardt, M.; Miller, J.; and Schmidt, L. 2024. Re-\ntiring adult: new datasets for fair machine learning. In Pro-\nceedings of the 35th International Conference on Neural In-\nformation Processing Systems, NIPS ’21. Red Hook, NY,\nUSA: Curran Associates Inc. ISBN 9781713845393.\n\nDwork, C.; McSherry, F.; Nissim, K.; and Smith, A. 2006.\nCalibrating Noise to Sensitivity in Private Data Analysis.\nIn Halevi, S.; and Rabin, T., eds., Theory of Cryptography,\n265–284. Berlin, Heidelberg: Springer Berlin Heidelberg.\nISBN 978-3-540-32732-5.\nEl Mhamdi, E. M. 2020. Robust Distributed Learning. Ph.D.\nthesis, EPFL.\nFeng, T.; Hashemi, H.; Hebbar, R.; Annavaram, M.; and\nNarayanan, S. S. 2021. Attribute inference attack of speech\nemotion recognition in federated learning settings.\narXiv\npreprint arXiv:2112.13416.\nFowl, L. H.; Geiping, J.; Czaja, W.; Goldblum, M.; and\nGoldstein, T. 2022. Robbing the Fed: Directly Obtaining\nPrivate Data in Federated Learning with Modified Models.\nIn The Tenth International Conference on Learning Rep-\nresentations, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nFredrikson, M.; Lantz, E.; Jha, S.; Lin, S.; Page, D.; and Ris-\ntenpart, T. 2014. Privacy in pharmacogenetics: An end-to-\nend case study of personalized warfarin dosing.\nIn 23rd\n{USENIX} Security Symposium ({USENIX} Security 14),\n17–32.\nGeiping, J.; Bauermeister, H.; Dr¨oge, H.; and Moeller, M.\n2020. Inverting Gradients - How easy is it to break privacy\nin federated learning? In Larochelle, H.; Ranzato, M.; Had-\nsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neu-\nral Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual.\nJang, E.; Gu, S.; and Poole, B. 2017. Categorical Reparam-\neterization with Gumbel-Softmax. In International Confer-\nence on Learning Representations.\nJee Cho, Y.; Wang, J.; and Joshi, G. 2022. Towards Un-\nderstanding Biased Client Selection in Federated Learning.\nIn Camps-Valls, G.; Ruiz, F. J. R.; and Valera, I., eds., Pro-\nceedings of The 25th International Conference on Artificial\nIntelligence and Statistics, volume 151 of Proceedings of\nMachine Learning Research, 10351–10375. PMLR.\nJin, C.; Ge, R.; Netrapalli, P.; Kakade, S. M.; and Jordan,\nM. I. 2017. How to Escape Saddle Points Efficiently. In\nPrecup, D.; and Teh, Y. W., eds., Proceedings of the 34th\nInternational Conference on Machine Learning, volume 70\nof Proceedings of Machine Learning Research, 1724–1732.\nPMLR.\nKadhe, S.; Rajaraman, N.; Koyluoglu, O. O.; and Ramchan-\ndran, K. 2020. FastSecAgg: Scalable Secure Aggregation\nfor Privacy-Preserving Federated Learning. arXiv preprint\narXiv:2009.11248.\nKairouz, P.; McMahan, H. B.; Avent, B.; Bellet, A.; Bennis,\nM.; Bhagoji, A. N.; Bonawitz, K.; Charles, Z.; Cormode, G.;\nCummings, R.; et al. 2021. Advances and open problems in\nfederated learning. Foundations and Trends® in Machine\nLearning, 14(1–2): 1–210.\nKasiviswanathan, S. P.; Rudelson, M.; and Smith, A. 2013.\nThe power of linear reconstruction attacks. In Proceedings\nof the twenty-fourth annual ACM-SIAM symposium on Dis-\ncrete algorithms, 1415–1433. SIAM.\nKingma, D. P.; and Ba, J. 2015.\nAdam: A Method for\nStochastic Optimization. In Bengio, Y.; and LeCun, Y., eds.,\n3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Confer-\nence Track Proceedings.\nLantz, B. 2013. Machine Learning with R. Packt Publishing.\nISBN 1782162143.\nLi, T.; Sahu, A. K.; Zaheer, M.; Sanjabi, M.; Talwalkar, A.;\nand Smith, V. 2020.\nFederated optimization in heteroge-\nneous networks. MLSys.\nLi, X.; and Orabona, F. 2020. A High Probability Analysis\nof Adaptive SGD with Momentum. In Workshop on Beyond\nFirst Order Methods in ML Systems at ICML’20.\nLian, X.; Zhang, C.; Zhang, H.; Hsieh, C.; Zhang, W.; and\nLiu, J. 2017.\nCan Decentralized Algorithms Outperform\nCentralized Algorithms? A Case Study for Decentralized\nParallel Stochastic Gradient Descent. In Advances in Neural\nInformation Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, 5330–5340.\nLiu, P.; Xu, X.; and Wang, W. 2022. Threats, attacks and\ndefenses to federated learning: issues, taxonomy and per-\nspectives. Cybersecurity, 5(1): 4.\nLyu, L.; and Chen, C. 2021. A Novel Attribute Reconstruc-\ntion Attack in Federated Learning. arXiv:2108.06910.\nLyu, L.; Yu, H.; Ma, X.; Chen, C.; Sun, L.; Zhao, J.; Yang,\nQ.; and Yu, P. S. 2020. Privacy and Robustness in Federated\nLearning: Attacks and Defenses.\nMcMahan, B.; Moore, E.; Ramage, D.; Hampson, S.; and\ny Arcas, B. A. 2017. Communication-efficient learning of\ndeep networks from decentralized data. In Artificial Intelli-\ngence and Statistics, 1273–1282. PMLR.\nMelis, L.; Song, C.; De Cristofaro, E.; and Shmatikov, V.\n2019. Exploiting unintended feature leakage in collaborative\nlearning. In 2019 IEEE symposium on security and privacy\n(SP), 691–706. IEEE.\nNasr, M.; Shokri, R.; and Houmansadr, A. 2019. Compre-\nhensive privacy analysis of deep learning: Passive and active\nwhite-box inference attacks against centralized and feder-\nated learning. In 2019 IEEE Symposium on Security and\nPrivacy (SP), 739–753. IEEE.\nNesterov, Y. 2003. Introductory lectures on convex optimiza-\ntion: A basic course, volume 87. Springer Science & Busi-\nness Media.\nNguyen, T.; Lai, P.; Tran, K.; Phan, N. H.; and Thai, M.\n2023.\nActive Membership Inference Attack under Local\nDifferential Privacy in Federated Learning.\nNishio, T.; and Yonetani, R. 2019. Client selection for feder-\nated learning with heterogeneous resources in mobile edge.\nIn 2019 IEEE International Conference on Communications\n(ICC), 1–7. IEEE.\nQuoc, D. L.; Gregor, F.; Arnautov, S.; Kunkel, R.; Bhatotia,\nP.; and Fetzer, C. 2020. Securetf: A secure tensorflow frame-\nwork. In Proceedings of the 21st International Middleware\nConference, 44–59.\nRigollet, P.; and Hutter, J.-C. 2023. Lecture Notes: High-\nDimensional Statistics.\n\nSabt, M.; Achemlal, M.; and Bouabdallah, A. 2015. Trusted\nExecution Environment: What It is, and What It is Not. In\n2015 IEEE Trustcom/BigDataSE/ISPA, volume 1, 57–64.\nShao, J. 2003. Mathematical Statistics. Springer Texts in\nStatistics. Springer. ISBN 9780387953823.\nSingh, J.; Cobbe, J.; Quoc, D. L.; and Tarkhani, Z. 2021.\nEnclaves in the Clouds: Legal Considerations and Broader\nImplications. Commun. ACM, 64(5): 42–51.\nTram`er, F.; Shokri, R.; Joaquin, A. S.; Le, H.; Jagielski, M.;\nHong, S.; and Carlini, N. 2022.\nTruth Serum: Poisoning\nMachine Learning Models to Reveal Their Secrets. In ACM\nCCS.\nVlaski, S.; and Sayed, A. H. 2021. Distributed Learning in\nNon-Convex Environments—Part I: Agreement at a Linear\nRate. IEEE Transactions on Signal Processing, 69: 1242–\n1256.\nWen, Y.; Geiping, J.; Fowl, L.; Goldblum, M.; and Gold-\nstein, T. 2022. Fishing for User Data in Large-Batch Feder-\nated Learning via Gradient Magnification. In Chaudhuri, K.;\nJegelka, S.; Song, L.; Szepesv´ari, C.; Niu, G.; and Sabato, S.,\neds., International Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA, volume\n162 of Proceedings of Machine Learning Research, 23668–\n23684. PMLR.\nYeom, S.; Giacomelli, I.; Fredrikson, M.; and Jha, S. 2018.\nPrivacy risk in machine learning: Analyzing the connection\nto overfitting. In 2018 IEEE 31st Computer Security Foun-\ndations Symposium (CSF), 268–282. IEEE.\nYin, D.; Chen, Y.; Kannan, R.; and Bartlett, P. 2018.\nByzantine-robust distributed learning: Towards optimal sta-\ntistical rates.\nIn International Conference on Machine\nLearning, 5650–5659. PMLR.\nYin, H.; Mallya, A.; Vahdat, A.; Alvarez, J. M.; Kautz,\nJ.; and Molchanov, P. 2021.\nSee through Gradients: Im-\nage Batch Recovery via GradInversion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 16337–16346.\nYousefpour, A.; Shilov, I.; Sablayrolles, A.; Testuggine, D.;\nPrasad, K.; Malek, M.; Nguyen, J.; Ghosh, S.; Bharadwaj,\nA.; Zhao, J.; Cormode, G.; and Mironov, I. 2021. Opacus:\nUser-Friendly Differential Privacy Library in PyTorch.\nZhang, H.; Ciss´e, M.; Dauphin, Y. N.; and Lopez-Paz, D.\n2018.\nmixup: Beyond Empirical Risk Minimization.\nIn\n6th International Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings.\nZhao, J. C.; Sharma, A.; Elkordy, A. R.; Ezzeldin, Y. H.;\nAvestimehr, S.; and Bagchi, S. 2023.\nLOKI: Large-scale\nData Reconstruction Attack against Federated Learning\nthrough Model Manipulation. In 2024 IEEE Symposium on\nSecurity and Privacy (SP), 30–30. IEEE Computer Society.\nZhou, D.; Chen, J.; Cao, Y.; Tang, Y.; Yang, Z.; and Gu,\nQ. 2020a. On the Convergence of Adaptive Gradient Meth-\nods for Nonconvex Optimization. In OPT2020: 12th Annual\nWorkshop on Optimization for Machine Learning.\nZhou, P.; Feng, J.; Ma, C.; Xiong, C.; Hoi, S. C.; and\nE, W. 2020b. Towards Theoretically Understanding Why\nSgd Generalizes Better Than Adam in Deep Learning. In\nLarochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and\nLin, H., eds., Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nZou, D.; Cao, Y.; Li, Y.; and Gu, Q. 2023. Understanding\nthe Generalization of Adam in Learning Neural Networks\nwith Proper Regularization. In The Eleventh International\nConference on Learning Representations, ICLR 2023, Ki-\ngali, Rwanda, May 1-5, 2023. OpenReview.net.\nReproducibility Checklist\nThis paper:\n• Includes a conceptual outline and/or pseudocode descrip-\ntion of AI methods introduced. Yes\n• Clearly delineates statements that are opinions, hypothe-\nsis, and speculation from objective facts and results. Yes\n• Provides well marked pedagogical references for less-\nfamiliare readers to gain background necessary to repli-\ncate the paper. Yes\nDoes this paper make theoretical contributions? Yes\n• All assumptions and restrictions are stated clearly and\nformally. Yes\n• All novel claims are stated formally. Yes\n• Proofs of all novel claims are included. Yes\n• Proof sketches or intuitions are given for complex and/or\nnovel results. Yes\n• Appropriate citations to theoretical tools used are given.\nYes\n• All theoretical claims are demonstrated empirically to\nhold. Yes\n• All experimental code used to eliminate or disprove\nclaims is included. NA\nDoes this paper rely on one or more datasets? Yes\n• A motivation is given for why the experiments are con-\nducted on the selected datasets. No\n• All novel datasets introduced in this paper are included\nin a data appendix. NA\n• All novel datasets introduced in this paper will be made\npublicly available upon publication of the paper with a\nlicense that allows free usage for research purposes. NA\n• All datasets drawn from the existing literature (poten-\ntially including authors’ own previously published work)\nare accompanied by appropriate citations. Yes\n• All datasets drawn from the existing literature (poten-\ntially including authors’ own previously published work)\nare publicly available. Yes\n• All datasets that are not publicly available are described\nin detail, with explanation why publicly available alter-\nnatives are not scientifically satisficing. NA\n\nDoes this paper include computational experiments? Yes\n• Any code required for pre-processing data is included in\nthe appendix. Yes\n• All source code required for conducting and analyzing\nthe experiments is included in a code appendix. Yes\n• All source code required for conducting and analyzing\nthe experiments will be made publicly available upon\npublication of the paper with a license that allows free\nusage for research purposes. Yes\n• All source code implementing new methods have com-\nments detailing the implementation, with references to\nthe paper where each step comes from. Partial\n• If an algorithm depends on randomness, then the method\nused for setting seeds is described in a way sufficient to\nallow replication of results. Yes\n• This paper specifies the computing infrastructure used\nfor running experiments (hardware and software), includ-\ning GPU/CPU models; amount of memory; operating\nsystem; names and versions of relevant software libraries\nand frameworks. Partial\n• This paper formally describes evaluation metrics used\nand explains the motivation for choosing these metrics.\nYes\n• This paper states the number of algorithm runs used to\ncompute each reported result. Yes\n• Analysis of experiments goes beyond single-dimensional\nsummaries of performance (e.g., average; median) to in-\nclude measures of variation, confidence, or other distri-\nbutional information. Yes\n• The significance of any improvement or decrease in\nperformance is judged using appropriate statistical tests\n(e.g., Wilcoxon signed-rank). No\n• This paper lists all final (hyper-)parameters used for each\nmodel/algorithm in the paper’s experiments. Partial\n• This paper states the number and range of values tried\nper (hyper-) parameter during development of the paper,\nalong with the criterion used for selecting the final pa-\nrameter setting. Yes\n\nAppendices\nA\nTheoretical Results\nA.1\nFull statement and proof of Theorem 1\nTheorem 1. Consider training a least squares regression\nthrough FedAvg (Alg. 2) with batch size B and local epochs\nE. Assume that\n1. the client’s design matrix xc ∈Rm×d has rank d equal\nto the number of features plus one;5\n2. the components of the stochastic (mini-batch) gradi-\nent are distributed as sub-Gaussian random variables\nwith variance proxy σ2, i.e., E[exp(e(θ)[i]/σ2)]\n≤\nexp(1), ∀θ, ∀i ∈{1, 2, ..., d}, where e(θ) = g(θ) −\n∇Lc(θ);6\n3. the input model vectors at an observed round are inde-\npendent of the previous stochastic gradients computed by\nthe attacked client;\n4. there exists λ > 0 such that ∀nc ∈N, we can always\nselect nc observation rounds so that λmin\n\u0010\nΘT\noutΘout\nnc\n\u0011\n≥λ,\nwhere λmin(A) denotes the smallest eigenvalue of the\nmatrix A, and Θout is defined in Alg. 3.\nThe error of the reconstructed model ˆθ∗\nc of Algorithm 3 is\nupper bounded w.p. ≥1 −δ when η ≤\nm\n2λmax(xT\nc xc) and\n\r\r\rˆθ∗\nc −θ∗\nc\n\r\r\r\n2 = O\n\nησd\ns\ndE\nlm\nB\nm d + 1 + ln 2d\nδ\nnc · λ\n\n. (7)\nBefore presenting the proof, we discuss the assumptions.\nThe first assumption can be relaxed at the cost of replac-\ning in the proof the inverse H−1 of the matrix H = xT\nc xc\nby its pseudo-inverse H†. The second assumption is com-\nmon in the analysis of stochastic gradient methods (Agar-\nwal, Negahban, and Wainwright 2012; Jin et al. 2017; Vlaski\nand Sayed 2021; Li and Orabona 2020; Zhou et al. 2020a).\nThe third assumption is technically not satisfied in our sce-\nnario as the stochastic gradients computed by the attacked\nclient i before an observed round tn have contributed to de-\ntermine the models sent back by client i at rounds t < tn and\nthen the global model θtn(0) sent by the server to client i.\nNevertheless, we observe that θtn(0) is almost independent\non client-i’s stochastic gradients if the set of clients is very\nlarge. Moreover, the assumption is verified if we assume a\nmore powerful adversary who can act as a man in the middle\nand arbitrarily modifies the model sent to the client. Finally,\nthe fourth assumption corresponds to the “well-behaved\ndata assumption” for a generic linear regression problem,\nwhich is required to be able to prove the consistency of\nthe estimators, i.e., that they converge with probability 1 to\nthe correct value as the number of samples diverges (see\nfor example (Shao 2003, Thm. 3.11)). In this context, we\n5Remember that a client’s design matrix has a number of rows\nequal to the input features of the samples in the client’s local train-\ning dataset.\n6Note that, by Jensen’s inequality, this condition implies a\nbounded variance for each component of the stochastic gradients.\ncan observe that λmin\n\u0010\nΘT\noutΘout\nnc\n\u0011\n=\n1\nnc λmin\n\u0000ΘT\noutΘout\n\u0001\n=\n1\nnc σ2\nmin(Θout) =\n1\nnc minz,∥z∥=1∥Θoutz∥2\n2. Now, consider\nthat the components of the gradient noise e(θ) are indepen-\ndent, each with variance lower bounded by τ 2 > 0, it fol-\nlows that E[∥Θoutz∥2\n2] ≥ncτ 2\nd\nd+1∥z∥2\n2 = ncτ 2\nd\nd+1. This\nsuggests that σ2\nmin(Θout) grows linearly with nc and then it\nis possible to lower bound λmin\n\u0010\nΘT\noutΘout\nnc\n\u0011\nwith a positive\nconstant.\nProof. Let H = xT\nc xc which is positive definite. Let yc ∈\nRm be the labels in the local dataset Dc with size m = |Dc|.\nLc(θ) = ∥xcθ −yc∥2\nm\n(8)\nWe know that θ∗\nc = (xT\nc xc)−1xT\nc yc. When computing\nthe stochastic gradient, we have:\ng(θ) = ∇Lc(θ) + e(θ) = 2\nm (Hθ −Hθ∗\nc) + e(θ).\n(9)\nAt round t, if selected, client c receives the server model and\nexecutes Algorithm 2. Let θt\nc(k) be the model after the k-\nth local update. To simplify the notation, in the following,\nwe replace e(θt\nc(k)) by ek. Replacing (9) in line 4 of Algo-\nrithm 2, we have\nθt\nc(K) =\n\u0012\nI −2η\nm H\n\u0013K\nθt\nc(0) +\n\"\nI −\n\u0012\nI −2η\nm H\n\u0013K#\nθ∗\nc\n−\nK\nX\nk=1\n\u0012\nI −2η\nm H\n\u0013k−1\nηeK−k+1.\nwhere K = E⌈m\nB ⌉as in each epoch there are ⌈m\nB ⌉local\nsteps. Let W =\nh\nI −\n\u0000I −2η\nm H\n\u0001Ki\n. W is proven to be\ninvertible in Lemma 2, then we have:\nθt\nc(0) = W−1 \u0000θt\nc(0) −θt\nc(K)\n\u0001\n+ θ∗\nc\n−W−1\nK\nX\nk=1\n\u0012\nI −2η\nm H\n\u0013k−1\nηeK−k+1\n(10)\nLet γt = −ηW−1 PK\nk=1\n\u0000I −2η\nm H\n\u0001k−1 eK−k+1. Let\nx[i] be the ith element of vector x and X[i, :] and X[:, i] be\nthe ith row and column of the matrix X, respectively. Since\nevery element in e is sub-Gaussian and the weighted sum\nof independent sub-Gaussian variables is still sub-Gaussian,\nγ[i] is sub-Gaussian with E[γt[i]] = 0 and\nVar[γt[i]] ≤dη2σ2∥W−1∥2\n2\nK\nX\nk=1\n\r\r\r\r\r\n\u0012\nI −2η\nm H\n\u0013k−1\r\r\r\r\r\n2\n2\n=\ndη2σ2\nλ2\nmin(W)\nK\nX\nk=1\n\r\r\r\r(I −2η\nm H)k−1\n\r\r\r\r\n2\n2\n=\ndη2σ2\nλ2\nmin(W)\nK\nX\nk=1\nρ2\nmax((I −2η\nm H)k−1),\n\nwhere ρmax(A) is the spectral radius of matrix A.\nLet Λ be a diagonal matrix whose entries are the positive\neigenvalues of H and λmax(H) be the largest eigenvalue of\nH, i.e., λmax(H) = ∥Λ∥1. When η ≤\nm\n2λmax(H), the diago-\nnal values in 2η\nm Λ are positive and smaller than or equal to 1.\nMoreover, the matrix (I−2η\nm H)k−1 is positive semi-definite\nand we have ρmax[(I −2η\nm H)k−1] ≤1. Thus, we have\nVar[γ[i]] ≤dη2σ2K\nλ2\nmin(W).\n(11)\nEquation (10) for nc observation can be written in a com-\npact way as\nΘin = Θout(W−1, θ∗\nc)T + ΓT ,\n(12)\nwhere Γ = (γt1, . . . , γtc).\nLet us denote W−1 as V. We can determine estimators\n( ˆV, ˆθ∗\nc) for (V, θ∗\nc), through ordinary least squares (OLS)\nminimization. In particular, if we extract the relation involv-\ning the i-th row of V and θ∗\nc, we obtain:\nΘin[:, i] = Θout(V[i, :], θ∗\nc[i, :])T + ΓT [:, i].\n(13)\nThe OLS estimator is\n( ˆV[i, :], ˆθ∗\nc[i])T = argmin\nv∈Rd,θ∈R\n∥Θin[:, i] −Θout(v; θ)∥2\n2 ,\n(14)\nand can be expressed in closed form as\n( ˆV[i, :], ˆθ∗\nc[i])T =\n\u0000ΘT\noutΘout\n\u0001−1 ΘT\noutΘin[:, i].\n(15)\nA more compact representation is the following\n( ˆV, ˆθ∗\nc)T =\nargmin\nV∈Rd×d,θ∈Rd\n\r\r\rΘin −Θout (W′ θ)T \r\r\r\n2\nF ,\nand the corresponding closed form is\n( ˆV, ˆθ∗\nc)T =\n\u0000ΘT\noutΘout\n\u0001−1 ΘT\noutΘin ∈R(d+1)×d.\n(16)\nNote that this is how θ∗\nc is estimated in Algorithm 3 line 3.\nThe presentation of separate linear systems for each row\ni in (13) is also instrumental to the rest of the proof. We\nobserve that (13) describes a regression model with inde-\npendent sub-gaussian noise, and then from Theorem 2.2 and\nRemark 2.3 in (Rigollet and Hutter 2023), we have that for\na given row i ∈{1, . . . , d}, w.p. at least 1 −δ\nd,\n(ˆθ∗\nc[i] −θ∗\nc[i])2 ≤(ˆθ∗\nc[i] −θ∗\nc[i])2 + ∥ˆV[i, :] −V[i, :]∥2\n2\n≤Var[γ[i]] d + 1 + log 2d\nδ\nncλmin\n\u0010\nΘT\noutΘout\nnc\n\u0011.\nFrom (11) and the fourth assumption, it follows that w.p. at\nleast 1 −δ\nd\n(ˆθ∗\nc[i] −θ∗\nc[i])2 ≤dη2σ2K\nλ2\nmin(W) · d + 1 + log 2d\nδ\nncλ\n.\n(17)\nWe are going now to consider that each row is estimated\nthrough a disjoint set of nc inspected messages, so that the\nestimates for each row are independent. We can then apply\nthe union bound and obtain that the Euclidean distance be-\ntween the two vectors can be bounded as follows, which con-\ncludes the proof:\n\r\r\rˆθ∗\nc −θ∗\nc\n\r\r\r\n2\n2 ≤d2η2σ2K\nλ2\nmin(W) · d + 1 + ln 2d\nδ\nncλ\n, w.p. ≥1 −δ.\n(18)\nAbout the Eigenvalues of the Matrix W.\nH = xT\nc xc is\ndiagonalizable, i.e., H = UΛUT with U an orthonormal\nmatrix and Λ a diagonal one. After some calculations\nW = I −\n\u0012\nI −2η\nm H\n\u0013E⌈m\nb ⌉\n= U\n \nI −\n\u0012\nI −2η\nm Λ\n\u0013E⌈m\nb ⌉!\nUT .\nThen\nλmin(W) = 1 −\n\u0012\n1 −2η\nm λmin(H)\n\u0013E⌈m\nb ⌉\nλmax(W) = 1 −\n\u0012\n1 −2η\nm λmax(H)\n\u0013E⌈m\nb ⌉\nLemma 2. Let A be a symmetric positive definite matrix.\nI −(I −A)n is invertible for n ≥0.\nProof. Since A is positive definite and symmetric, it can be\ndecomposed to UΛUT , where UUT = I and Λ is a diago-\nnal matrix whose entries are the positive eigenvalues of A.\nThus we have\nI −(I −A)n = UIUT −(UIUT −UΛUT )n\n= UIUT −U(I −Λ)nUT\n= U(I −(I −Λ)n)UT .\nSince Λ is with only positive values on diagonal, eigenvalues\nof I −(I −A)n are non-zeros. So I −(I −A)n is invertible\nand its inverse is U(I −(I −Λ)n)−1UT .\nA.2\nProof of Proposition 2\nProof. When FedAvg is executed with full batch (i.e.,\ne(θ) = 0), according to Eqs. 9 and 10, we have\ng(θ) = ∇Lc(θ) = 2\nm (Hθ −Hθ∗\nc)\nθt\nc(0) = W−1 \u0000θt\nc(0) −θt\nc(K)\n\u0001\n+ θ∗\nc.\n(19)\nThen once nc > d, by solving d multivariate linear equa-\ntions (Eq. 19), we obtain the exact optimal local model\nθ∗\nc.\n\nA.3\nProof of Proposition 3\nProof. To prove the lower bound for the communications,\nwe construct a specific “hard” scenario. This scenario is\ninspired by the lower bound for the convergence of gradi-\nent methods minimizing smooth convex functions (Nesterov\n2003, Sect. 2.1.2).\nLet c be the client having the local dataset (xc, yc) such\nthat Hc = xT\nc xc is a tridiagonal matrix with Hc[i, i + 1] =\nHc[i + 1, i] = −1/2 and Hc[i, i] = 1, ∀i ∈{1, 2, ..., d},\nand xT\nc yc = [1/2, 0, 0, ..., 0]T .7 We have\nLc(θ)\n=\n1\nm\n\u0000θT Hcθ −2(xT\nc yc)T θ −yT\nc yc\n\u0001\n=\n1\nm\n d\nX\ni=1\nθ2\n[i] −\nd−1\nX\ni=1\nθ[i]θ[i+1] −θ[1] −C\n!\n(20)\nwhere C = yT\nc yc is a constant and θ∗\nc = arg min Lc(θ) =\n(Hc)−1xT\nc yc = [1−\n1\nd+1, 1−\n2\nd+1, ..., 1−\nd\nd+1]T (Nesterov\n2003). According to (20), we know that if θ[i] = θ[i+1] =\n. . . = θ[d] = 0, then ∇Lc(θ)[i+1] = . . . = ∇Lc(θ)[d] = 0.\nAt the same time, for any other client ∀¯c ∈C \\{c}, we as-\nsume their local optimum are zeros where their local dataset\nX¯c = I and y¯c = 0. In this setting, we know that if the ith\nelement of the global model is zero, i.e., θ(t)[i] = 0, then\nθt\n¯c(E)[i] = 0.\nLet n = |C| be the number of clients and assume that\nevery client has m local data samples. Then the global em-\npirical risk and its gradients have the following expressions:\nL(θ) =\n1\nnm\n d\nX\ni=1\nn × θ2\n[i] −\nd−1\nX\ni=1\nθ[i]θ[i+1] −θ[1] −C′\n!\n,\n(21)\n∇L(θ)[1] =\n1\nnm(2n × θ[1] −θ[2] −1),\n(22)\n∇L(θ)[i] =\n1\nnm\n\u00002n × θ[i] −θ[i−1] −θ[i+1]\n\u0001\n, ∀i ∈{2, ..., d −1},\n(23)\n∇L(θ)[d] =\n1\nnm\n\u00002n × θ[d] −θ[d−1]\n\u0001\n,\n(24)\nwhere C′ is a constant. According to (22), (23) and (24), the\nglobal optimum θ∗satisfies:\nθ∗\n[1]\n=\n(1 + θ∗\n[2])/2n,\n(25)\nθ∗\n[i]\n=\n2nθ∗\n[i+1] −θ∗\n[i+2], ∀i ∈{1, ..., d −2}, (26)\nθ∗\n[d−1]\n=\n2nθ∗\n[d].\n(27)\nEquations 26 and 27 show that θ∗\n[i] is proportional to θ∗\n[d] for\nevery i ∈{1, ..., d −1}, i.e., θ∗\n[i] = ki × θ∗\n[d] where ki > 0.\nSince θ∗\n[1] = k1 ×θ∗\n[d] and θ∗\n[2] = k2 ×θ∗\n[d] where k1 > 0 and\n7One example for the local data (xc, yc) satisfying the con-\ndition: xc\n∈Rd×d with xc[1, 1] = 1; xc[i + 1, i + 1] =\nq\n1 −\n1\n4xc[i,i]2 and xc[i + 1, i] =\n−1\n2xc[i,i], ∀i ∈{1, ..., d −1};\nxc[i, i + 1] = xc[i, j] = 0, ∀|i −j| ≥1. yc ∈Rd with yc[1] = 1\n2\nand yc[i + 1] = −xc[i+1,i]yc[i]\nxc[i+1,i+1] , ∀i ∈{1, ..., d −1}.\nk2 > 0, by substituting θ∗\n[1] and θ∗\n[2] into (25), we can see that\nθ∗\n[d] ̸= 0. Therefore, we can prove then that every element of\nthe global optimum is non-zero, i.e., θ∗\n[i] ̸= 0, ∀i ∈{1, .., d}.\nNow, suppose that we run the FedAvg with one local\nstep under the above scenario, with initial global model\nθ(0) = 0, i.e., θ0\nc(0) = 0, ∀c ∈C. According to the pre-\nvious analysis, we know that ∀t ∈{0, 1..., d −1}:\nθt\nc(E)[t+1] = θt\nc(E)[t+2] = . . . = θt\nc(E)[d] = 0,\n(28)\nand\nθ(t)[t+1] = θ(t)[t+2] = . . . = θ(t)[d] = 0.\n(29)\nTherefore, to reach the non-zeros global optimum, the client\nc needs to communicate with the server (be selected by the\nserver) at least d times.\nMoreover, to recover the local optimum of client c, the\nadversary must listen on the communication channel for at\nleast d times. In fact, suppose that the client c holds another\nlocal data set which gives H′\nc equals to Hc but for the last\nrow and the last column that are zeros. Under this case, the\nadversary will have the same observation under Hc and un-\nder H′\nc till round d −1.\nB\nThe details of experimental setups\nB.1\nExperiment in Figure 1\nWe evaluate different AIAs when four clients train a neural\nnetwork (a single hidden layer of 128 neurons using ReLU\nas activation function) through FedAvg with 1 local epoch\nand batch size 32. Each client stores |Dc| data points ran-\ndomly sampled from ACS Income dataset (Ding et al. 2024).\nThis dataset contains census information from 50 U.S. states\nand Puerto Rico, spanning from 2014 to 2018. It includes 15\nfeatures related to demographic information such as age, oc-\ncupation, and education level. The adversary infers the gen-\nder attribute of every data sample held by the client given\naccess to the released (public) information.\nThe regression task is to predict an individual’s income.\nThe classification task is to predict whether an individ-\nual has an income higher than the median income of the\nwhole dataset, i.e., 39K$. To train the classification task, for\neach sample i with target income yi, the label is given by\n1yi>39K.\nB.2\nToy dataset setting\nWe test our proposed passive LMRA (Alg. 3) on a toy fed-\nerated least squares regression with two clients. Each client\nc has 1024 samples with 10 features, where nine numeri-\ncal features are sampled from a uniform distribution over\n[0, 1) and one binary feature is sampled from a Bernoulli\ndistribution with p = 1/2. The prediction y is generated\nfrom the regression model y = xcθ∗+ ϵ where ϵ is drawn\nfrom N(0, 0.1)Sc and the optimal local model θ∗∈Rd is\ndrawn from the standard normal distribution where d = 11.\nThe training is run by FedAvg with 1 local epoch, batch size\n64, 256, and 1024, and 5 seeds each, respectively. The num-\nber of communication rounds are 300 and the honest-but-\ncurious adversary only eavesdropped d + 1 = 12 messages\nfrom rounds T = {i ∗20|i ∈{0, 1, ..., 11}}.\n\nAlgorithm 5: Splitting strategy for Income-L dataset with\nheterogeneity level h ∈[0, 0.5]\nInput:\nthe\ninitial\nIncome-L\ndataset\nD\n=\n{(xp(i), s(i), y(i)), i = 1, ..., |D|}\n1: med ←median value in {y(i), i = 1, ..., |D|}\n2: Let Dh = {((xp(i), s(i), y(i)) ∈D |(s(i) = man ∧\ny(i) > med) ∨(s(i) = woman ∧y(i) <= med)}\n3: Let Dl = D \\ Dh\n4: k ←min(|Dh|, |Dl|)\n5: D′\nh ←sample randomly k points from Dh\n6: D′\nl ←sample randomly k points from Dl\n7: D′\nhs ←sample randomly (0.5 −h)k points from D′\nh\n8: D′\nls ←sample randomly (0.5 −h)k points from D′\nl\n9: D′\nh ←(D′\nh \\ D′\nhs) ∪D′\nls\n10: D′\nl ←(D′\nl \\ D′\nls) ∪D′\nhs\n11: Split D′\nl equally among the first 5 clients\n12: Split D′\nh equally among the last 5 clients\nNote that, for the full batch scenario (i.e., B = Sc), nu-\nmerical inversion errors occurred in line 3 may prevent the\nexact computation of the local model, particularly when Θout\nis ill-conditioned. That explains why in Figure 2, we have\n||ˆθ∗−θ∗||2 close to but not exactly equal to zero when\nB = Sc.8\nB.3\nData splitting strategy for Income-L dataset\nFor Income-L dataset, we apply a splitting strategy to con-\ntrol statistical heterogeneity among the 10 clients, which is\ndetailed in Alg. 5.\nTo achieve this, we first partition the initial dataset into\ntwo clusters, Dh and Dl, which exhibit strong opposing cor-\nrelations between the sensitive attribute and the target in-\ncome. More precisely, Dh contains samples of rich men and\npoor women and Dl contains samples of poor men and rich\nwomen (lines 2-3). We then randomly select min(|Dh|, |Dl|)\nsamples from each cluster to have balanced size, denoted\nby D′\nh and D′\nh (lines 5-6). Initially, D′\nh and D′\nl have clearly\ndifferent distributions. By randomly swapping a fraction of\n0.5 −h of samples between the two clusters (lines 7-8), the\ndistributions of D′\nh and D′\nl become more similar as h de-\ncreases. Lastly, each cluster is divided equally into datasets\nfor five clients (lines 11-12).\nB.4\nHyperparameters\nLearning\nrates\nfor\nFL\ntraining\non\nneural\nnet-\nwork\nIn the experiments on Income-L, clients train\ntheir model using varying batch sizes from the set\nB = {32, 64, 128, 256, 512, 1024}. The learning rates are\ntuned in the range [1·10−7, 5·10−6], according to the batch\nsize: 5·10−7 for batch sizes of 32 and 64, 1·10−6 for a batch\nsize of 128, 2 · 10−6 for batch sizes of 256 and 512, and\n3 · 10−6 for a batch size of 1024. We keep the same learning\nrates when varying the number of local epochs and the\ndata heterogeneity level. For Income-A, the learning rate is\n8In real practice, the adversary can optimize the set of the mes-\nsages considered to minimize the condition number of Θout.\nset to 1·10−6. For Medical, the learning rate is set to 2·10−6.\nHyperparamters for Gradient-based attacks\nFor the\npassive adversary, the set of inspected messages T is se-\nlected in T p = {{first max{1, ⌊f|Tc|⌋} rounds in Tc},\nfor f ∈F}. For the active adversary, T is selected in\nT p ∪{{first max{1, ⌊f|T a\nc |⌋} rounds in T a\nc }, for f ∈F}.\nWe set F = {0.01, 0.05, 0.1, 0.2, 0.5, 1}\nTo solve (2) with the Gumbel-softmax trick, we set the\nGumbel-softmax temperature to 1.0 and use a SGD opti-\nmizer with a learning rate tuned from the set {10n, n =\n2, 3, . . . , 6} for every attack.\nHyperparameters for our active attack\nIn our attack\nwith an active adversary, detailed in Alg. 4, Adams’ hy-\nperparameters are selected following a tuning process per-\nformed using Optuna (Akiba et al. 2019) hyperparameter\noptimization framework. In all the experiments, values for\nthe learning rate, β1 and β2 are optimized to minimize\neach client’s training loss. Learning rate is tuned in range\n[0.1, 50], whereas β1 and β2 values are tested in range\n[0.6, 0.999]. The optimal hyperparameters’ set is determined\nafter 50 trials.\nC\nAdditional experimental results\nC.1\nFederated least squares regression\nHere, we show the results of AIAs for federated least\nsquares regression task, on Income-L, Income-A and Medi-\ncal datasets. In all the experiments, each client trains a linear\nmodel for 300 communication rounds with 1 local epoch and\nbatch size 32. The learning rate is set to 5·10−3. The passive\nadversary may eavesdrop all the exchanged messages until\nthe end of the training. The active adversary launches the at-\ntack after 300 rounds for additional 10 and 50 rounds. All\nattacks are targeted at a randomly chosen single client. To\napproximate the optimal local model of the targeted client,\nour passive adversary applies Alg. 3 and our active adversary\napplies Alg. 4.\nTo reduce the task difficulty by a linear model, we shrink\nthe feature space of Income-L and Income-A. More pre-\ncisely, we remove ‘Occupation’, ‘Relationship’, and ‘Place\nof Birth’ features, transform ‘Race Code’ and ‘Marital Sta-\ntus’ from categorical to binary features (i.e., White/Others\nand Married/Not married), and reduce the cardinality of\n‘Class of Worker’ to 3 groups (Public employess/Private em-\nployees/Others).\nHyperparameters\nWe optimize the set of messages for\nboth our method and gradient-based passive attacks. For\ngradient-based attacks, the set of inspected messages T\nis selected in T p\n=\n{{first t rounds in Tc}, for t\n∈\n{1, 5, 10, 20, 50, 100, 150, 300}}. For our attack, we explore\na much larger number of sets of messages as our attack is\ncomputationally-light, evolving only simple matrix compu-\ntation. In particular, we randomly sample 107 sets of size\nd + 1 and choose the one which minimizes the condition\nnumber of Θout, since an ill-conditioned matrix Θout leads to\na high numerical inversion error as mentioned in App. B.2.\n\nAll other hyperparameters are consistent with those used in\nthe neural network experiments.\nAIA (%)\nDatesets\nIncome-L Income-A\nMedical\nPassive\nGrad\n53.10±1.40 49.74±3.17 87.76±3.80\nGrad-w-O 58.19±0.41 55.97±0.38 94.68±0.23\nOurs\n59.05±0.40 56.56±0.41 94.13±0.16\nActive\n(10 Rnds)\nGrad\n53.10±1.40 49.74±3.17 87.76±3.80\nGrad-w-O 59.06±0.07 57.18±0.39 94.68±0.23\nOurs\n57.99±1.71 56.44±0.12 90.47±5.12\nActive\n(50 Rnds)\nGrad\n53.10±1.40 49.74±3.17 87.76±3.80\nGrad-w-O 60.69±0.30 57.18±0.39 94.68±0.23\nOurs\n59.10±0.16 56.22±0.06 93.91±0.08\nModel-w-O\n59.10±0.16 56.56±0.41 94.13±0.16\nTable 2: The AIA accuracy over one (random chosen) tar-\ngeted client local dataset, evaluated under both honest-but-\ncurious (passive) and malicious (active) adversaries across\nIncome L, Income-A and Medical FL datasets. The standard\ndeviation is evaluated over three FL training runs. All clients\nsolve a least squares regression problem running FedAvg\nwith 1 epoch and batch size 32. For income-L, we consider\nan i.i.d setting.\nResults in Table 2 show that our attacks outperform the\nbaseline Grad in both passive and active scenarios on across\nall three datasets. Notably, our passive attack achieves im-\nprovements of over 4, 5 and 7 percentage points (p.p.)\nfor Income-L, Income-A and Medical datasets, respectively.\nEven when the gradient-based method has access to an or-\nacle, our passive attacks still achieves higher accuracy on\nIncome-A and Income-L and comes very close on Medi-\ncal dataset. More importantly, our passive attack reaches al-\nready to the performance expected from an adversary who\nknows the optimal local model, demonstrating the effective-\nness of our passive approach in approximating the optimal\nlocal model (Alg. 3). When shifting to active attacks, the\nperformance gain over Grad remains largely consistent after\n50 active rounds.\nC.2\nImpact of the starting round for active attack\nIn our implementations, with a local epoch of 1, all ac-\ntive attacks were initiated from the 100th communication\nround. Here, we vary the starting round of the active at-\ntacks and run the attacks for additional 50 rounds. We il-\nlustrate the attacks’ performance in Figure 4. We observe\nthat our approach shows slight improvements when the at-\ntack is launched in the later phases of training. However,\nthe gradient-based method Grad is more effective during the\nearly training phase. Overall, our approach maintains an ad-\nvantage of over 12 percentage points.\nC.3\nImpact of the number of active rounds\nWe evaluate the performance of active attacks under differ-\nent numbers of active rounds |T a\nc | (Figure 5), during which\nan active adversary launches attacks. Our attack becomes\nmore powerful as the number of active rounds increases,\nFigure 4: The AIA accuracy over all clients’ local datasets\nunder different starting points of active attack for Income-L\ndataset (40% heterogeneity level). The clients train a neural\nnetwork through FedAvg with 1 local epoch and batch size\n32.\nwhereas Grad does not demonstrate the same level of ef-\nfectiveness.\nC.4\nDefense\nTo mitigate privacy leakage, we apply a federated version of\nDP-SGD (Abadi et al. 2016), which provides (ϵ, δ) sample-\nlevel differential privacy guarantees (Dwork et al. 2006;\nChoudhury et al. 2019). More precisely, the clients clip and\nadd Gaussian noises to their gradients in FL. We used Opa-\ncus (Yousefpour et al. 2021) to incorporate a (1, 1 · 10−5)-\ndifferentially private defense on Income-L and Medical\ndatasets, and (1, 1 · 10−6)-differentially private defense on\nIncome-A. All attacks are targeted at a randomly chosen sin-\ngle client. Other experimental settings are consistent with\nthose used in the neural network experiments.\nHyperparameters for defense\nWe adjust the clipping\nnorm for each dataset and select the one that yields the low-\nest validation loss in the final global model. For Income-L\nand Income-A datasets, the clipping norm is tuned over the\nset {1·106, 3·106, 5·106, 7·106, 9·106 1·107, 3·107, 5·107}.\nFor Medical dataset, the clipping norm is tuned over the set\n{5·105, 7·105, 9·105, 1·106, 3·106, 5·106, 7·106, 9·106}.\nTable 3 presents the results with both active and honest-\nbut-curious adversaries. Our passive attacks significantly\noutperform Grad baselines on Income-A and Medical\ndatasets, achieving over 30 and 8 percentage point improve-\nments on Income-A and Medical datasets, respectively. Fur-\nthermore, our attacks improve also Grad-w-O baseline by\n4 percentage points on Income-A and 3 percentage points\non Medical. Surprisingly, passive gradient-based attacks\ndemonstrate better performance than Ours on Income-L, de-\nspite having near-zero cosine similarity. This observation\n\nFigure 5: The AIA accuracy over all clients’ local datasets\nunder different numbers of active rounds for Income-L\ndataset (40% heterogeneity level). The clients train a neu-\nral network through FedAvg with 1 local epoch and batch\nsize 32.\nsuggest that cosine similarity may not be a suitable met-\nric for gradient-based attacks’ optimization. Moving to the\nactive adversary, our attacks exhibit minimal performance\nchanges between rounds 10 and 50 on Income-A and Med-\nical datasets, as they approach the empirical optimal perfor-\nmance achievable by Model-w-O. Conversely, on Income-\nL, where performance margins are larger, our attack surpass\nGrad by 3 and 12 percentage points after 10 and 50 active\nrounds, respectively, with potential for further optimization\nto match the optimal local model performance.\nAIA (%)\nDatasets\nIncome-L Income-A\nMedical\nPassive\nGrad\n59.34±3.58 50.52±2.49 63.51±6.63\nGrad-w-O 77.67±1.01 53.83±0.19 91.09±0.08\nOurs\n48.69±4.03 58.08±0.11 94.19±0.23\nActive\n(10 Rnds)\nGrad\n59.34±3.58 50.52±2.49 63.51±6.63\nGrad-w-O 77.67±1.01 54.42±2.77 91.09±0.08\nOurs\n62.04±3.16 57.60±0.85 93.96±0.41\nActive\n(50 Rnds)\nGrad\n59.34±3.58 50.52±2.49 63.51±6.63\nGrad-w-O 77.67±1.01 54.42±2.77 91.09±0.08\nOurs\n71.37±1.86 57.25±0.86 94.30±0.08\nModel-w-O\n79.33±1.11 58.53±0.61 94.30±0.08\nTable 3: The AIA accuracy over one (random chosen) tar-\ngeted client’s local dataset evaluated under both honest-but-\ncurious (passive) and malicious (active) adversaries across\nIncome L, Income-A and Medical FL datasets. The standard\ndeviation is evaluated over three FL training runs. All clients\ntrain a neural network through a federated version of DP-\nSGD with 1 local epoch and batch size 32, providing (ϵ = 1,\nδ = 1 · 10−5) sample level differential privacy for every\nclient on Medical and Income-L, and (ϵ = 1, δ = 1 · 10−6)\nsample level differential privacy on Income-A.",
    "pdf_filename": "Attribute_Inference_Attacks_for_Federated_Regression_Tasks.pdf"
}