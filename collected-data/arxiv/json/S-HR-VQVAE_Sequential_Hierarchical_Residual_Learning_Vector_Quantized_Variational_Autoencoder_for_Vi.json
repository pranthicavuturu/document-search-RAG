{
    "title": "1",
    "abstract": "forthanovelmodelthatcombines(i)anovelhierarchicalresidual is, therefore, a more challenging problem for video prediction learningvectorquantizedvariationalautoencoder(HR-VQVAE), thanforanyothervideotask.Mostmethodsusemeansquared and (ii) a novel autoregressive spatiotemporal predictive model error (MSE) objective that tends to average over possible (AST-PM).Werefertothisapproachasasequentialhierarchical residual learning vector quantized variational autoencoder (S- outcomes, resulting in blurred predictions. The challenge of HR-VQVAE). By leveraging the intrinsic capabilities of HR- physical characteristics pertains to object and scene attributes VQVAE at modeling still images with a parsimonious represen- affecting prediction. Proper modeling of these characteristics tation,combinedwiththeAST-PM‚Äôsabilitytohandlespatiotem- may potentially aid future frame predictions. Recent video poral information, S-HR-VQVAE can better deal with major prediction methods have made significant progress in tackling challengesinvideoprediction.Theseincludelearningspatiotem- poral information, handling high dimensional data, combating these challenges, yet they still face several limitations. We blurry prediction, and implicit modeling of physical character- will detail the state-of-the-art with respect to each of these istics. Extensive experimental results on four challenging tasks, challenges in Section II. namely KTH Human Action, TrafficBJ, Human3.6M, and Kitti, This paper introduces a sequential hierarchical residual demonstrate that our model compares favorably against state- learning vector quantized variational autoencoder (S-HR- of-the-art video prediction techniques both in quantitative and qualitativeevaluationsdespiteamuchsmallermodelsize.Finally, VQVAE), which is tailored for video prediction with the goal we boost S-HR-VQVAE by proposing a novel training method of tackling the above-mentioned challenges. To this end, S- to jointly estimate the HR-VQVAE and AST-PM parameters. HR-VQVAE implements a novel autoregressive spatiotempo- Index Terms‚ÄîVideo Prediction, Hierarchical Modeling, Au- ral predictive model (AST-PM) to capture distributions of toregressive Modeling dependencies between latent representations across time and space. The latent representations are generated through our I. INTRODUCTION novel encoding scheme, termed hierarchical vector quanti- Video prediction involves anticipating future video frames zation variational autoencoder (HR-VQVAE) that we have based on a sequence of preceding frames [1]. It is a challeng- recently used with success for still image reconstruction [4]. ingtask,requiringalgorithmstograspcomplexspatiotemporal Leveraging those two novel blocks, namely HR-VQVAE, relationships within the video, at the same time as handling and AST-PM, S-HR-VQVAE effectively tackles the video high dimensionality, addressing blurry predictions, and ac- prediction task in three steps: In the first step, the input counting for the physical characteristics of the scenes. Spa- video frames are encoded to a continuous latent space and tiotemporal modeling aims to capture dependencies in video then mapped to discrete representations through HR-VQVAE, framesequences,mirroringhumanperceptionofdynamicphe- with each latent vector, in each layer in the model, assigned nomena [2]. This is a general problem in video modeling, but to a codeword in a codebook. The key property of this becomes especially challenging when we need to recursively model is the strict hierarchy imposed between codebooks and accurately predict video frames for long temporal spans. belonging to different layers, producing extremely compact Current state-of-the-art methods often struggle with long-term and efficient discrete representations. In the second step, we dependenciesandcomplexmotionpatterns,leadingtoinaccu- predict future events in latent rather than image space. To raciesinthepredictedframes.Highdimensionalityisinherent perform this prediction, we use spatiotemporal modeling (the in video patterns, leading to the ‚Äúcurse of dimensionality‚Äù proposed AST-PM), where the distribution of the discrete in function approximation and optimization [3]. Autoencoder- latent representations for a particular location in the current basedmethodsattempttoreducedimensionality,butmaylose frame is conditioned on the representations for neighboring important fine-grained details necessary for accurate predic- locationsbothinspaceandtime.Inthethirdandfinalstep,the tion.Blurrypredictionsstemfromstatisticalmodelsproducing predicteddiscreterepresentationsareusedbytheHR-VQVAE 4202 voN 91 ]VC.sc[ 3v10760.7032:viXra",
    "body": "1\nS-HR-VQVAE: Sequential Hierarchical Residual\nLearning Vector Quantized Variational Autoencoder\nfor Video Prediction\nMohammad Adiban‚Ä†, Kalin Stefanov‚àó, Sabato Marco Siniscalchi‚Ä†+ Senior Member, IEEE, Giampiero\nSalvi‚Ä† Senior Member, IEEE\n‚Ä†Norwegian University of Science and Technology\n‚àóMonash University\n+Universita` degli Studi di Palermo\nE-mails: {mohammad.adiban,marco.siniscalchi,giampiero.salvi}@ntnu.no, kalin.stefanov@monash.edu\nAbstract‚ÄîWe address the video prediction task by putting fuzzier outputs when predicting uncertain future events. This\nforthanovelmodelthatcombines(i)anovelhierarchicalresidual is, therefore, a more challenging problem for video prediction\nlearningvectorquantizedvariationalautoencoder(HR-VQVAE),\nthanforanyothervideotask.Mostmethodsusemeansquared\nand (ii) a novel autoregressive spatiotemporal predictive model\nerror (MSE) objective that tends to average over possible\n(AST-PM).Werefertothisapproachasasequentialhierarchical\nresidual learning vector quantized variational autoencoder (S- outcomes, resulting in blurred predictions. The challenge of\nHR-VQVAE). By leveraging the intrinsic capabilities of HR- physical characteristics pertains to object and scene attributes\nVQVAE at modeling still images with a parsimonious represen- affecting prediction. Proper modeling of these characteristics\ntation,combinedwiththeAST-PM‚Äôsabilitytohandlespatiotem-\nmay potentially aid future frame predictions. Recent video\nporal information, S-HR-VQVAE can better deal with major\nprediction methods have made significant progress in tackling\nchallengesinvideoprediction.Theseincludelearningspatiotem-\nporal information, handling high dimensional data, combating these challenges, yet they still face several limitations. We\nblurry prediction, and implicit modeling of physical character- will detail the state-of-the-art with respect to each of these\nistics. Extensive experimental results on four challenging tasks, challenges in Section II.\nnamely KTH Human Action, TrafficBJ, Human3.6M, and Kitti,\nThis paper introduces a sequential hierarchical residual\ndemonstrate that our model compares favorably against state-\nlearning vector quantized variational autoencoder (S-HR-\nof-the-art video prediction techniques both in quantitative and\nqualitativeevaluationsdespiteamuchsmallermodelsize.Finally, VQVAE), which is tailored for video prediction with the goal\nwe boost S-HR-VQVAE by proposing a novel training method of tackling the above-mentioned challenges. To this end, S-\nto jointly estimate the HR-VQVAE and AST-PM parameters. HR-VQVAE implements a novel autoregressive spatiotempo-\nIndex Terms‚ÄîVideo Prediction, Hierarchical Modeling, Au- ral predictive model (AST-PM) to capture distributions of\ntoregressive Modeling dependencies between latent representations across time and\nspace. The latent representations are generated through our\nI. INTRODUCTION novel encoding scheme, termed hierarchical vector quanti-\nVideo prediction involves anticipating future video frames zation variational autoencoder (HR-VQVAE) that we have\nbased on a sequence of preceding frames [1]. It is a challeng- recently used with success for still image reconstruction [4].\ningtask,requiringalgorithmstograspcomplexspatiotemporal Leveraging those two novel blocks, namely HR-VQVAE,\nrelationships within the video, at the same time as handling and AST-PM, S-HR-VQVAE effectively tackles the video\nhigh dimensionality, addressing blurry predictions, and ac- prediction task in three steps: In the first step, the input\ncounting for the physical characteristics of the scenes. Spa- video frames are encoded to a continuous latent space and\ntiotemporal modeling aims to capture dependencies in video then mapped to discrete representations through HR-VQVAE,\nframesequences,mirroringhumanperceptionofdynamicphe- with each latent vector, in each layer in the model, assigned\nnomena [2]. This is a general problem in video modeling, but to a codeword in a codebook. The key property of this\nbecomes especially challenging when we need to recursively model is the strict hierarchy imposed between codebooks\nand accurately predict video frames for long temporal spans. belonging to different layers, producing extremely compact\nCurrent state-of-the-art methods often struggle with long-term and efficient discrete representations. In the second step, we\ndependenciesandcomplexmotionpatterns,leadingtoinaccu- predict future events in latent rather than image space. To\nraciesinthepredictedframes.Highdimensionalityisinherent perform this prediction, we use spatiotemporal modeling (the\nin video patterns, leading to the ‚Äúcurse of dimensionality‚Äù proposed AST-PM), where the distribution of the discrete\nin function approximation and optimization [3]. Autoencoder- latent representations for a particular location in the current\nbasedmethodsattempttoreducedimensionality,butmaylose frame is conditioned on the representations for neighboring\nimportant fine-grained details necessary for accurate predic- locationsbothinspaceandtime.Inthethirdandfinalstep,the\ntion.Blurrypredictionsstemfromstatisticalmodelsproducing predicteddiscreterepresentationsareusedbytheHR-VQVAE\n4202\nvoN\n91\n]VC.sc[\n3v10760.7032:viXra\n2\ndecoder to generate the corresponding frame. Normally, HR- inter-frame motion can oversimplify complex dynamics, and\nVQVAEandAST-PMmaybetrainedindependently.However, its implicit hierarchy may limit its ability to capture fine-\nwealsoproposeanoveljointtrainingschemetooptimizeHR- grained spatiotemporal details compared to models with ex-\nVQVAE and AST-PM together and show that this improves plicit multi-layered hierarchies.\nvideo prediction. We argue that the reason for the improved To address the spatiotemporal challenge, S-HR-VQVAE\nperformance is that AST-PM and the decoder of HR-VQVAE leverages our proposed AST-PM module. In this module,\nare trained in such a way as to optimize both the predicted causal convolutions in time and spatiotemporal self-attention\nquantizedlatentrepresentationforfutureframesaswellasthe are used to model the spatiotemporal correlations on the\nreconstruction of future frames in image space. quantized codes level. Moreover, AST-PM operates on the\nOur contributions can be summarized as follows: latent discrete representations produced by the HR-VQVAE\n‚Ä¢ S-HR-VQVAE,anoveltechniqueforvideoprediction,is module instead of using pixels directly.\nproposed. This includes a hierarchical vector quantized\nencoding scheme and a spatiotemporal autoregressive B. High Dimensionality\nmodelofthelatentrepresentations.Thismodelallowsthe\nThe above spatiotemporal methods rely on complex mod-\ncapture of different levels of abstraction in a sequence of\neling, which hampers scalability, especially with the high\nvideo frames, thus resulting in a compact but effective\ndimensionality of video data. Hsieh et al. [22] addressed this\nrepresentation of the task.\nby dividing frames into patches and predicting their evolution\n‚Ä¢ A novel loss function to jointly train the components of over time using a recurrent convolutional neural network\nS-HR-VQVAE (HR-VQVAE and AST-PM) with further\n(rCNN) [23]. Jun-Ting et al. [24] proposed the decompo-\nimprovements of the prediction performance.\nsitional disentangled predictive autoencoder (DDPAE) frame-\n‚Ä¢ State-of-the-art results on several challenging video pre- work, automatically breaking down high-dimensional videos\ndiction tasks, namely KTH Human Action [5], Traf-\ninto components with low-dimensional temporal dynamics.\nficBJ [6], Human3.6M [7] and Kitti [8].\nXueetal. [25]proposedavariationalautoencoder(VAE)[26]\nmodel to generate a distribution of next frame predictions.\nII. RELATEDWORK Oliu et al. [27] utilized a folded recurrent neural network\nA. Spatiotemporal Modeling (fRNN)withagatedrecurrentunit(GRU)forbidirectionalin-\nformationflow,enablingstatesharingbetweentheencoderand\nHuetal.[9]introduceddisentangledrepresentationnet(Dr-\ndecoder.Variational3DConvLSTM(V-3D-ConvLSTM) [28]\nNet) for spatial feature modeling in single video frames, ne-\ncombined variational encoder-decoder and 3D-ConvLSTM\nglecting temporal information. Motion-content network (Mc-\ntechniques. [29] developed thr video prediction Transformer\nNet) [10] and mutual suppression network (MsNet) [11]\n(VPTR), an attention-based encoder-decoder, to learn local\naddressed motion and content separately, overlooking joint\nspatiotemporal representations while simplifying the model.\ncorrelations. Convolutional long short-term memory (Con-\nCompared to the aforementioned methods, S-HR-VQVAE\nvLSTM) [12] aimed at capturing both spatial and temporal\ncan effectively manage the high dimensionality of video data,\ncorrelations but struggled with long-term dependencies and\nleveraging the hierarchical structure inside the vector quanti-\nscalability.ToovercomeConvLSTM‚Äôslimitations,Wangetal.\nzationmodule,whichefficientlycompresseseachvideoframe,\nproposedpredictiverecurrentneuralnetwork(PredRNN)[13],\nas demonstrated in the experimental section.\nwhich, despite improvements, still faced challenges in mod-\neling complex long-term dependencies. PredRNN++ [14] and\nC. Blurry Predictions\nPredRNN-V2 [15] aimed to enhance PredRNN‚Äôs performance\nby incorporating hierarchical recurrent structures. Eidetic 3D As reported in [30], video prediction solutions quite often\nLSTM (E3D-LSTM) [16] was introduced to jointly model rely on RNNs, VAEs, and their variants (e.g., variational\nspatial and temporal dynamics. Su et al. [17] improved ef- RNNs - VRNNs [31]) resulting in blurry predictions. Two\nficiency using low-rank tensor factorization, while robust spa- main strategies have emerged to address this issue: (i) Latent\ntiotemporalLSTM(R-ST-LSTM)[18]andmemoryinmemory variablemethodsthatexplicitlymodelunderlyingstochasticity\n(MIM)[19]demonstratedperformanceimprovementsinlong- and (ii) Adversarially-trained models that aim to produce\nterm frame prediction tasks. The simple video prediction more natural images. In [32], the authors instead aimed to\nmodel (SimVP) [20] showed significant improvement over investigate stochastic models for video prediction using the\nRNN-basedmodelsbutstruggledwithencodinglong-termdy- VAE framework. Given the recent advances in generative\nnamics, making accurate future prediction challenging. Chang adversarial networks (GANs), researchers have also explored\net al. [21] introduce hierarchical semantic separation in video alternative techniques, such as VAE-GANs [30], [33] for\nprediction using a spatiotemporal encoding-decoding scheme videoframeprediction.VAE-GANsallowcapturingstochastic\nand residual predictive memory called STRPM. This scheme posterior distributions of videos while making it feasible to\nseparates spatial and temporal information with independent modelthespatiotemporaljointdistributionofpixels.However,\nencoders, preserving distinct features and improving high- such methods often suffer from the problem of mode collapse\nresolution video predictions. The STRPM refines the separa- and unrealistic predictions [33], [34].\ntion by focusing on inter-frame residuals for more accurate S-HR-VQVAEcombatsimageblurringthankstothetempo-\nfuture predictions. However, STRPM‚Äôs reliance on residual ralmodelleveragingthehierarchicalcodebookrepresentation.\n3\nThis allows for an increase in the quantization granularity input image x to a continuous latent vector z ‚àà RH√óW√óD\nwithout resulting in blurry images. In fact, despite the lossy through a non-linear encoder: z = E(x). Next, each element\nnature of the compressed encoding, our experiments clearly z ‚ààRD,withh‚àà[1,H],andw ‚àà[1,W],inthecontinuous\nhw\ndemonstrate that the original video can be reconstructed with latentvectorzisquantizedtothenearestcodebookvector(i.e.,\na high degree of fidelity through the latent representations. a codeword) e ‚ààRD, k ‚àà1,...,m by\nk\nQuantize(z ):=e where k =argmin‚à•z ‚àíe ‚à• . (1)\nhw k hw j 2\nD. Physical Characteristics j\nTo leverage physical characteristics, some methods focus The quantized vectors corresponding to each element z hw\non pixel-level representations. For example, De Brabandere are then recombined into the continuous representation e ‚àà\net al. [35] introduced the dynamic filter network (DFN), RH√óW√óD to form the input of the decoder that reconstructs\nwhich learns local spatial transformations from flow informa- theinputimageusingatransformationD(¬∑).Thelossfunction\ntion. Finn et al. [36] proposed convolutional dynamic neural L(.) aims at minimizing the reconstruction error ‚à•x‚àíD(e)‚à• 2\nadvection (CDNA), a model that predicts object motion and whilst minimizing the quantization error ‚à•z‚àíe‚à• 2 as follows\npixel motion distributions from previous frames. In another L(x,D(e))=‚à•x‚àíD(e)‚à•2+‚à•sg[z]‚àíe‚à•2+Œ≤‚à•sg[e]‚àíz‚à•2, (2)\napproach [37], a system was developed to predict optical 2 2 2\nflows between future and past frames. Berg et al. [38] where sg(.) is a stop-gradient operator cutting gradient flow\nutilized backward content transformation via a 6-parameter during backpropagation, and Œ≤ is a hyperparameter governing\naffine model to learn future-to-past relationships. Villegas et the stability of encoder output latent vectors.\nal. [10]] employed LSTM to independently model pixel-level In [44] a multi-layer version of VQVAE was proposed.\nimages for spatial layout and temporal dynamics, simplifying However, the representations at different levels in the archi-\nprediction tasks. Guen et al. introduced the Physical dynam- tecture were not related hierarchically.\nics network (PhyDNet) [39] to separate physical dynamics\nfrom other factors, yielding notable improvements. A motion- IV. PROPOSEDMETHOD\nbased modeling technique (MotionRNN) [40] decomposes\nIn[4],weintroducedatrulyhierarchicalversionofVQVAE\nmotions into transient variations and trends, utilizing RNN-\n(HR-VQVAE) that is one of the building blocks of the video\nbased models like ConvLSTM, PredRNN, and E3D-LSTM\nprediction method proposed in this work. HR-VQVAE deals\nfor prediction. Lee et al. [41] proposed a long-term motion\nwith limitations in techniques such as VQVAE, e.g., code-\ncontext memory (LMC-memory) model for considering long-\nbook collapse and non-locality in codewords‚Äô indices. In HR-\ntermmotioncontextinfutureframeprediction.However,these\nVQVAE, each layer captures residual information that is not\nmethods primarily address physical characteristics, overlook-\nproperly modeled by the preceding layers, and the codebooks\ningchallengeslikehighdimensionality,blurrypredictions,and\nat different layers are constrained by a strict hierarchy.\nspatiotemporal modeling in video prediction.\nFig.1showstheproposedframework.GivenT inputframes\nS-HR-VQVAE does not explicitly model physical char-\n(x ,...,x ) in a video, the goal is to predict the following\nacteristics. Nonetheless, the modularity of the hierarchical 1 T\nS frames (x ,...,x ) in three steps. First, the input\nvector quantization block allows S-HR-VQVAE to implicitly T+1 T+S\nframes are encoded into a discrete latent representation using\nmodel physical characteristics. In fact, latent representations\nHR-VQVAE.Next,anovelautoregressivespatiotemporalpre-\nare decomposed into a hierarchy of discrete codes, separating\ndictive model (AST-PM) is proposed to predict new discrete\nhigh-level global information (e.g., static background) from\nlatent variables of future frames based on the latent variables\ndetails (e.g., fine texture or small motions). Since the latent\nfor previous frames. Finally, the HR-VQVAE decoder is used\nrepresentations are decomposed into different layers of hi-\nto generate the new frames from the latent variables obtained\nerarchical residual codes, the proposed AST-PM can exploit\nby AST-PM. The proposed approach is referred as sequen-\nspatiotemporal dependencies that are different for different\ntial hierarchical residual learning vector quantized variational\nlevels of detail. For example, the background evolves slowly\nautoencoder (S-HR-VQVAE).\nin time; whereas, the foreground object may move quickly.\nSimilarly, within the foreground object, some details, such\nas hands and arms, may exhibit different movement patterns A. Step 1: Frame Encoding\ncomparedtothebody.Insum,thecombinationofHR-VQVAE In the first step, each frame x ‚àà RHI√óWI√óDI is encoded\nand AST-PM allows the modeling of physical characteristics,\nusing HR-VQVAE into a discrete latent representation. HR-\nimproving accuracy while reducing complexity.\nVQVAE first encodes the frame into a continuous vector\nz = E(x) ‚àà RH√óW√óD. These vectors are then iteratively\nIII. THEORETICALBACKGROUND\nquantized into n hierarchical layers of discrete latent embed-\nVariationalautoencoders(VAEs)andvectorquantizedVAEs dings. Assuming that the first layer has a single codebook of\n(VQVAEs) have been used for many applications for their sizeM,thesecondlayerhasM independentcodebooksofsize\ninherent representation capabilities [42], [43]. Focusing on M (for a total of M2 codewords), and so on. A generic layer\nimage processing applications, our primary focus, an input i has thereby Mi‚àí1 codebooks of size M, for a total of Mi\nimage is represented as a tensor x ‚àà RHI√óWI√óDI of height codewords. However, only one of those codebooks is used in\nH , width W and D color channels. VQVAE first maps the each layer depending on which codewords were chosen in the\nI I I\n4\nHR-VQVAE\nùíõ\nùêÑ(ùíôùíä)=ùíõùíä ùíä\nVQ-Encoder ùë∏ùíäùüè VQ-Decoder\nùë∏ùíäùüê\nùêá√óùêñ√óùêÉ ùêá√óùêñ ùë∏ùíäùíé ùêá√óùêñ√óùêÉ ùíô ùíä\nùíô ùíä ùêû ùêÇ\nEncoder Quintized codes Decoder\nS-HR-VQVAE ùë∏ùüèùüè ùë∏ùê≠ùüè\n+ùüè\nùêÑ(ùíôùüè)=ùíõùüè\nùë∏ùíïùüè\nVQ-Encoder ùë∏ùüèùüè ùíõ\nùê≠+ùüè\nùíôùüèùíôùüê\nùíôùíï ùêá√óùêñ ‚Ä¶√óùêÉ ùêá√óùêñ ‚Ä¶ùë∏ùüèùíé\nùë∏ùüèùüê\nùë∏ùíïùüêùë∏ùüèùüê ùë∏ùê≠ùüê +ùüè VQ-Decoder\nùêÑùíôùíï =ùíõùíï ‚Ä¶ ùêá√óùêñ√óùêÉ ùíô ùê≠+ùüè\nEncoder\nVQ-Encoder ùë∏ùíïùüêùë∏ùíïùüè ùë∏ùíïùíéùë∏ùüèùíé ‚Ä¶ ùë∏ùê≠ùíé +ùüè\nPredicted\nDecoder (Predicted frame)\nùêá√óùêñ√óùêÉ\nùêá√óùêñùë∏ùíïùíé Qu Ca on dt eiz sed\nQuintized codes\nAST-PM\nFig. 1: Top: The HR-VQVAE module for hierarchical vector quantization, where each frame i is encoded into m hierarchical\nlayersofquantizedvalues(cid:0) Q1,...,Qm(cid:1)\n.Bottom:IllustrationoftheS-HR-VQVAEforvideoprediction,whichcombinesHR-\ni i\nVQVAE with the AST-PM model. AST-PM predicts the indices of quantized values in both spatial and temporal dimensions,\nwhere each index at time t+1 is predicted by accessing only its preceding indices‚Äîthose located above and to the left in a\nraster-scan spatial order and those before t+1 in the temporal domain.\nprevious layers. In each layer i, the codebook is optimized to maingoalofEqs.4and5istomakeahierarchicalmappingof\nminimizetheerrorbetweencodewordsei ‚ààRD andelements inputdatainwhicheachlayerofquantizationextractsresidual\nk\nŒæi‚àí1 ‚ààRD of the residual error from the previous layer1 information from its bottom layers.\nhw\nIn the proposed S-HR-VQVAE, we do not reconstruct\nQuantizei(Œæi‚àí1):=ei where k =argmin‚à•Œæi‚àí1‚àíei‚à• , (3)\nhw k hw j 2 images directly. The indices to the codewords ei are, instead,\nj\nused as latent representations for each input frame in the\nand ei belongs to one of the possible codebooks C (t)\nk i video and each layer in the system and are input to the\nfor layer i. Which codebook is used is determined by the\nvideo prediction steps described below. We call these indices\ncodeword ei‚àí1 selected at the previous layer. Within each\nt for layer i, Qi ‚àà [1,M]H√óW, with M the size of each\nlayer,thecodewordsei,foreachelementŒæi‚àí1 oftheresidual,\nk hw codebook. The difference is clarified in Figure 1, where the\nare combined to form the tensor ei ‚àà RH√óW√óD. Across the\nimage reconstruction case is represented in the top panel, and\ndifferent layers, the tensors ei are then summed to form the\nthe video prediction is depicted in the bottom panel.\n‚Äúcombined‚Äù discrete representation e . When HR-VQVAE is\nC\nused to reconstruct single images, e is fed into the decoder\nC\ntoreconstructtheimageasxÀÜ=D(e ),andthecorresponding B. Step 2: Spatiotemporal Latent Representation Prediction\nC\nobjective function is used to train the system In the second step, the indices (Qi,...,Qi ) of the code-\n1 T\nwords(ei,...,ei ),obtainedfromeachlayeriofHR-VQVAE\n1 T\nfromtheinputframes(x ,...,x ),areusedtopredictthein-\nL(x,D(e C))=‚à•x‚àíD(e C)‚à•2 2+‚à•sg[Œæ0]‚àíe C‚à•2 2 dices(Qi ,...,Qi )1 ofthecT odewords(ei ,...,ei )\nT+1 T+S T+1 T+S\n(cid:88)n for S future frames, with the goal of predicting the S next\n+Œ≤ ‚à•sg[e ]‚àíŒæ0‚à•2+ L(Œæi‚àí1,ei), (4)\n0 C 2 future frames (x T+1,...,x T+S).\ni=1 To this end, we propose a probabilistic autoregressive\nwith spatiotemporal predictive model (AST-PM). AST-PM takes\nL(Œæi‚àí1,ei)=‚à•sg[Œæi‚àí1]‚àíei‚à•2+Œ≤ ‚à•sg[ei]‚àíŒæi‚àí1‚à•2. (5) discrete indices of the latent representations as input and\n2 i 2 predictsfutureindices.BecauseHR-VQVAEishierarchical,it\nThe Œ≤ are hyperparameters that control the reluctance to greatly simplifies predicting spatial and temporal information,\ni\nchange the code corresponding to the encoder output. The which allows our model to focus on the most important\nparts of the frames in both space and time. Accordingly,\n1Forthefirstlayer,Œæ0\nhw\n‚â°z hw. the model predicts future codeword indices QÀÜt>T using\n5\nthe codeword indices from previous times (Qi,...,Qi ). To introduced by the encoder of HR-VQVAE, which means\n1 T\nexplain our probabilistic model, we first arrange the elements that the decoder is optimized solely for reconstructing the\nofQi ‚àà[1,M]H√óW fromlefttorightandtoptobottomusing originalinputframe.However,whenwereconstructtheframes\nt\na linear index v ‚àà [1,HW]. We use the notation vj <k to (x ,...,x ), we also need to deal with the uncertainty\nk T+1 T+S\nrefertoanyelementofQi thatistotheleftorabovev .Given introduced by the AST-PM predictions. In fact, the AST-PM\nt k\nthis notation, the probabilistic model can be written as: uncertaintyreferstothemismatchbetweenthepredictedlatent\nspaces of future frames and the actual latent space of future\nH√óW\np(QÀÜi (v ))= (cid:89) p(QÀÜi (v )|Qi(v ),...,Qi(v )), frames. This indicates that the HR-VQVAE decoder block\nt+1 k t+1 j<k 1 j<k t j<k\nand the AST-PM block operate independently, without being\nj=1\n(6) aware of the uncertainties introduced by the other block. In\nwhere QÀÜi represents the predicted quantized discrete codes of an attempt to address this issue, we propose to optimize the\nt\nlayer i obtained from the tth frame. This behavior is achieved AST-PM and the HR-VQVAE decoder jointly. Therefore, we\nby using convolutional masks to limit the information used proposed a joint training in Eq. 9 which includes two distinct\nduringprediction.Theconvolutionalmasksrestricttheconvo- objectives: the loss for the HR-VQVAE decoder, represented\nlutions to retrieve only spatial information from the left and by the first term of Eq. 4, and the AST-PM loss in Eq. 7. The\nabove each index. For the temporal dimension, convolutions corresponding multi-objective loss is:\nare limited to previous time steps by masking out present and\nfuture timesteps. This strategy is implemented using multi- L =L +Œª‚à•x ‚àíD(e )‚à•2, (9)\njoint p t C(t) 2\nhead attention layers similar to those in [44]. However, in our\nwhere Œª is a hyperparameter that controls the effect of\ncase, the attention is applied to 3D voxels. The loss function\nthe reconstruction loss on the joint training. In this case,\nof AST-PM is as follows\nduring training, HR-VQVAE only produces the latent repre-\nL p(p(Qi t>T),p(QÀÜi t>T))= s Te hn eta lt aio ten ns\nt\n( rQ epi 1 r, e. se. n., taQ tii T on) sfo (Qr ithe ,in ..p .u ,t Qfr iame )s f( ox r1 t, h. e.. fr, ax mT e) s.\nH√óW M T+1 T+S\n‚àí 1 (cid:88) (cid:88) p(Qi )[j,m]‚àólogp(QÀÜi )[j,m], (x T+1,...,x T+S)arepredictedbyAST-PMandthenusedto\nH √óW t>T t>T train the HR-VQVAE decoder.\nj=1 m=1\n(7)\nV. EXPERIMENTS\nC. Step 3: Frame Generation A. Datasets\nOnce the quantization indices QÀÜi t for each layer i and We conducted experiments using four different challenging\neach time step t ‚àà [T + 1,T + S] have been estimated datasets. Table I presents a summary of corresponding statis-\nby the AST-PM, the corresponding quantized representation tics, including the number of training samples (#Train), the\nÀÜz t ‚ààRH√óW√óD canbecomputedbycodebookaccesse C(t) = number of test samples (#Test), image resolution represented\nm\n(cid:80) ei (see Section IV-A). as (H,W,C), input sequence length indicated as T, and\ni=1 t predicted sequence length referred to as TÀÜ.\nFinally, the predicted quantized codes are decoded to se-\nThe KTH Human Action dataset [5] is a moving image\nquences of frames using the HR-VQVAE decoder D(.)\ndataset with a resolution of 160√ó120 pixels that contains six\n(xÀÜ ,...,xÀÜ )=(D(ÀÜz ),...,D(ÀÜz )), (8) types of human actions, including walking, jogging, running,\nT+1 T+S T+1 T+S\nboxing, hand waving, and hand clapping. The dataset com-\nwhere the and zÀÜ t>T and xÀÜ t>T represent the predicted latent prises 25 human subjects performing actions in four different\nrepresentations and frames, respectively. scenarios. For our experiments, we followed [15], resized the\nvideoframesdownto128√ó128,andsplitthedatasetintotwo\nD. Disjoint and Joint Training\nsubsets: (i) a training set, consisting of the first 16 subjects,\nHR-VQVAEandAST-PMinthecombinedmodeldescribed and (ii) a test set, containing the remaining subjects.\nabove can be trained independently. In this case, we first train The TrafficBJ is a collection of taxicab GPS data and\nHR-VQVAE according to Eq. 4 to predict each frame x in meteorological data recorded in Beijing [6]. Each frame in\ni\nthe video independently of the others. We obtain a sequence TrafficBJhas32√ó32pixels,includingthetrafficflowentering\nof latent representations (Qi,...,Qi ,Qi ,...,Qi ) for and leaving the same district. We normalized the data to [0,1]\n1 T T+1 T+S\neach layer in HR-VQVAE and for the complete sequence and follow the experimental settings as [45].\nof frames. The AST-PM can the been trained to predict The Human3.6M dataset [7] consists of 3.6 million sam-\nthe sequence (Qi ,...,Qi ) given the input sequence ples capturing diverse human activities. Similar to previous\nT+1 T+S\n(Qi,...,Qi ), by optimizing Eq. 7. In the test phase, we papers [19], [20], [39], we focus on the ‚Äúwalking‚Äù scenario.\n1 T\nuse the predictions of AST-PM in combination with the HR- The Kitti dataset [8] was created through real traffic\nVQVAE decoder to predict unseen video frames, making sure scenario collections by specially equipped vehicles, a joint\nthatthecombinedmodelonlyhasaccessto(x ,...,x )when effort by Germany‚Äôs Karlsruhe Institute of Technology and\n1 T\npredicting (x ,...,x ). the Toyota Institute of Technology in the United States. We\nT+1 T+S\nFollowing this training procedure, the decoder in HR- employ Kitti using three scenarios: road, city, and residential,\nVQVAE is exclusively optimized to deal with the uncertainty resulting in 57 videos for a training set and 4 for a test set.\n6\nTABLE I: Dataset statistics. #Train and #Test indicate the\nachieved by quantifying the feature distribution gap between\nnumber of samples for the training and test set, respectively.\nreal and generated videos, which effectively captures both\nEach input sequence consists of T, and the output sequence\ntemporal inconsistencies and motion-related artifacts. Further-\nconsists of TÀÜ frames with shape (H, W, C).\nmore, FVD evaluates both the temporal coherence of video\ncontentandthequalityofindividualframes,offeringaholistic\nDataset #Train #Test (H,W,C) TTT TTTÀÜÀÜÀÜ\nperspective on video realism and overall coherence. All those\nTrafficBJ[6] 19,627 1,334 (32,32,2) 4 4 metrics,however,havelimitations.Forexample,PSNR,MSE,\nKTH[5] 5,200 3,167 (128,128,3) 10 20\nHuman3.6M[7] 2,624 1,135 (128,128,3) 4 4 and MAE have been shown to have poor correlation with\nKitti[8] 40,783 1,963 (128,128,3) 4,5 5 human perception [51], [52] and may not take into account\nTABLE II: Configuration details for S-HR-VQVAE. higher-levelsemanticinformation,suchasinactionmodeling.\nSSIM and LPIPS are more effective in capturing perceptual\nKTH&Human3.6M&Kitti TrafficBJ differences, but they may not be sensitive to all types of\nInputsize 128√ó128 32√ó32 visual information: they may not be as effective at capturing\nBitrate 8 8 differences in color or texture as at capturing differences in\nLatentsize 32√ó32√ó8 16√ó16√ó4\nQuantizedsize 32√ó32 16√ó16 luminanceandcontrast[53].fFVDtendstoprioritizeavideo‚Äôs\n#Layers 1 3 9 1 3 6 spatial elements and may overlook the natural flow of its\nCodebooksize 512 8 2 64 4 2 temporal dynamics [54]. Therefore, several metrics must be\n#Codewords 512 {8,64,512} {2,4,...,512} 64 {4,16,64} {2,4,...,64}\nconsidered to better capture different aspects of the video\nprediction task and obtain a more comprehensive assessment\nFinally, it is important to note that 5% of the training set\nof the methods‚Äô performance.\nwas reserved as a validation set, which was used specifically\nWe report results according to all those metrics and include\nfor fine-tuning the hyperparameters.\nall available results for the related methods. Because of the\nlimitations of these metrics, we also provide a qualitative\nB. Experimental Setup assessment to verify whether the metrics have missed some\nimportant aspects of the video prediction task.\nTableIIlistssomedetailsaboutS-HR-VQVAEarchitecture\nfor tackling the datasets. Input size refers to the initial res-\nolution of the video frames. Latent size corresponds to the VI. RESULTS\ncontinuous latent representation in HR-VQVAE. Quantized Results of the quantitative evaluation of the proposed\nlatent size to the quantized representation in the model. We method followed by a qualitative assessment are now pre-\nalso provide additional information for the bit rate, number of sented. To better appreciate the effectiveness of the proposed\nhierarchy layers, codebook size, and number of codewords. technique,wehaveperformedasystematicreviewofreported\nThe proposed S-HR-VQVAE was trained on sequences quantitative results of recent, state-of-the-art solutions.\nconsistingof10consecutiveframestopredict20futureframes The qualitative analysis is performed by observing the\nfor KTH Human Action, and also trained on 4 consecutive behavior of the proposed method on several video sequences,\nframes to predict 4 future frames for both TrafficBJ and which is a common practice in the research field. However,\nHuman3.6M datasets, which is a common practice for the while reviewing the literature, we noticed that different meth-\ntasks. In addition, for the Kitti dataset, we focused on two ods use different video sequences to visually demonstrate the\nspecificsettings:(i)4inputframesand5predictedframesand quality of their approaches; furthermore, the source code is\n(ii) 5 input frames and 5 predicted frames. In all experiments, not available for all methods in the literature, which implies\nthe model is trained using the Adam optimizer [46], and the that different systems can not be compared on the same set of\nlearning rate is set to 0.0003 for both HR-VQVAE encoder- predefined video sequences. To overcome that issue, we first\ndecoder and AST-PM. Besides, Œª in Eq. 9 is set to 0.11. selectedvideosequencescommonamongdifferenttechniques\nin the literature. Then, we evaluated our S-HR-VQVAE on\nthose selected examples and grouped the results accordingly.\nC. Metrics\nTo the best of our knowledge, this is the first time that such\nWe report results adopting metrics that are commonly\na systematic comparison has been carried out.\nused in the literature, namely: peak signal-to-noise ratio\nWe also provide results for other aspects of the proposed\n(PSNR)[47],structuralsimilarityindexmeasure(SSIM)[48],\nS-HR-VQVAE, including reconstruction capability in (i) blur\nlearnedperceptualimagepatchsimilarity(LPIPS)[49],frechet\nmitigation, (ii) noise removal, and (iii) compression.\nvideo distance (FVD) [50], mean square error (MSE), and\nmean absolute error (MAE). PSNR, SSIM, LPIPS, MSE,\nA. Quantitative Analysis\nand MAE are all image quality metrics but differ in their\ncharacteristics. PSNR focuses on signal-to-noise ratio, SSIM In this study, we assess the performance of state-of-the-art\nconsiders structural similarity, MSE and MAE measure pixel- video prediction methods on different datasets, providing a\nwise differences, and LPIPS aims to capture perceptual sim- comprehensive overview of the advancements in the field. In\nilarity based on deep neural networks. FVD, on the other particular, Tables III, IV and V list state-of-the-art methods\nhand, is a comprehensive video quality metric employed to from2015to2023inachronologicallyascendingorder,high-\nassess the quality of generated videos. This evaluation is lightingtherebytheevolutionofthetechniquesovertheyears.\n7\nTABLE III: Results on KTH Human Action dataset. S-HR-\nin all evaluation metrics, especially in FVD (better temporal\nVQVAEwith3layerswasusedwithdisjointandjointtraining.\nmodeling) and FLOPs (computational efficiency).\nKTHHumanAction(10‚Üí20) The performance of the S-HR-VQVAE on the challenging\nMethod PSNR‚Üë SSIM‚Üë LPIPS‚Üì #Params FLOPs task of the Kitti dataset is detailed in Table V. Unlike\nConvLSTM(2015)[12] 23.01 0.704 0.156 16.60M 1,468G other tasks such as KTH Human Action, TrafficBJ, and\nDFN(2016)[35] 27.26 0.794 √ó √ó √ó Human3.6M, where the background is static, the Kitti dataset\nCDNA(2016)[36] 23.75 0.752 √ó √ó √ó\nDrNet(2017)[9] 25.56 0.764 √ó 23.30M √ó introduces a unique challenge with its dynamic and complex\nPredRNN(2017)[13] 27.55 0.839 0.167 23.85M 2,800G environments. This complexity comes from the challenging\nMcNet(2018)[10] 25.95 0.804 √ó 3.50M √ó\nMsNet(2018)[11] 27.08 0.876 √ó 3.20M √ó driving scenes, where both the foreground and background\nfRNN(2018)[27] 26.12 0.771 √ó √ó √ó are in motion. This requires the prediction model to accu-\nPredRNN++(2018)[14] 28.62 0.888 0.229 15.40M 4,162G\nE3D-LSTM(2019)[16] 27.92 0.893 √ó 41.94M 214.0G rately handle multiple moving elements and rapidly changing\nMIM(2019)[19] 27.78 0.902 0.188 37.37M 1,099G landscapes, a significant shift from tasks where movement is\nConv-TT-LSTM(2020)[17] 28.36 0.907 0.133 39.8M √ó\nPhyDNet(2020)[39] 28.69 √ó 0.188 3.10M 93.6G mainly due to a single object against a constant background.\nJinetal.(2020)[55] 29.85 0.893 0.118 √ó √ó\nLMC-Memory(2021)[41] 28.61 0.894 0.133 √ó √ó FortheKitti(4‚Üí5)task,S-HR-VQVAEhasdemonstrated\nV-3D-ConvLSTM(2021)[28] 28.31 0.866 √ó 12.90M √ó remarkable improvement over traditional models like Pre-\nR-ST-ConvLSTM(2022)[18] 28.99 0.854 √ó √ó √ó\nSimVP(2022)[20] 33.72 0.905 √ó 22.30M 125.6G dRNNandMcNetandmorerecentapproachessuchasNPVP.\nPredRNN-V2(2023)[15] 28.37 0.838 0.139 23.86M 2,815G It not only obtains better performance in SSIM, showing\nVPTR(2023)[29] 26.96 0.879 0.076 162.48M √ó\nNPVP(2023)[56]‚àó 27.66 0.909 0.066 √ó √ó the best perceptual quality of predictions but also achieves\nS-HR-VQVAE-disjoint(ours) 28.43 0.863 0.130 1.14M 94.1G the lowest LPIPS and a significantly better FVD, indicating\nS-HR-VQVAE-joint(ours) 28.49 0.910 0.093 1.14M 95.8G superior performance in capturing both spatial and temporal\n‚àóNPVPresizedKTHsamplesto64√ó64insteadofstandard128√ó128.\naspects of the scenes. Similarly, for the Kitti (5 ‚Üí 5) task,\n(‚Üë)meanshigherisbetterand(‚Üì)meanslowerisbetter.\nS-HR-VQVAE significantly outperforms other state-of-the-art\nmodels such as PhyDNet, LMC-Memory, MotionRNN, and\nOntheKTHHumanActiontask,PSNRandSSIMarereported\nMIMO. It achieves higher SSIM and PSNR values, which\nby all competing techniques; whereas, LPIPS is provided for\nindicates that it not only captures higher structural similarities\nonly a few methods. On TrafficBJ and Human3.6M tasks, we\nbetween the predicted and actual frames but also maintains\nreport MSE, MAE, and SSIM as in [19], [20], [39]. Finally,\nhigh-quality predictions across various frames. The lower\nontheKittidataset,wereportSSIM,LPIPS,FVD,andPSNR.\nLPIPS also shows further evidence of S-HR-VQVAE‚Äôs capa-\nHere we report our results in order of complexity of the task\nbility to preserve more accurate textural and detail-oriented\n(from KTH Human action to Kitti).\nfeatures that are critical in dynamic scenes.\nFor the KTH Human Action task, from Table III, it is\nReferring to Tables III, IV and V, we can observe that the\nevidentthattheproposedS-HR-VQVAEoutperformsallmeth-\ndifferentmetricsimproveovertheyears.Also,itcanbeargued\nods, up to fRNN, in all reported metrics. Among methods in-\nthat starting from 2018, all methods are quite competitive\ntroducedafterfRNN,S-HR-VQVAEoutperformsPredRNN++\nwith one another, and it is not possible to indicate a single\non two metrics out of three, E3D-LSTM on all, Conv-TT-\ntechnique that performs the best on the video prediction task\nLSTM on all, PhyDNet on one out of two, Jin et al. [55] on\nacross all metrics. Indeed, when we attempt to determine the\ntwo out of three, LMC-Memory on two out of three, V-3D-\nbest method, it can be seen that methods performing best\nConvLSTM across all, R-ST-ConvLSTM on one out of two,\nin one metric are usually outperformed by other methods\nSimVPononeoutoftwo,PredRNN-V2onall,VPTRontwo\nin other metrics, and therefore, it is essential to evaluate\noutofthree,andNPVPontwooutofthree.Itcanalsobeseen\nthe results using all available metrics. From this analysis,\nfrom Table III that SimVP has the overall best PSNR, but our\nwe can conclude that although some of the state-of-the-art\nmethod outperforms it and achieves the best result in terms of\nmethods outperform our method on a single metric, S-HR-\nSSIM. For LPIPS, NPVP has the best performance; however,\nVQVAE is more robust across all metrics for the considered\nthe method is outperformed by our method both in terms of\ntasks, especially for the challenging tasks of Human3.6M and\nPSNR and SSIM, despite NPVP downsampling video frames\nKitti, where S-HR-VQVAE outperforms the state-of-the-art\nto64√ó64ratherthanthetypical128√ó128.Itisnoteworthythat\nmethods across all metrics. Ultimately, Tables III, IV, and V\nS-HR-VQVAEachievesthoseresultswithasignificantlylower\nshow the positive impact of jointly training HR-VQVAE and\nnumber of parameters with respect to all other methods and\nAST-PM across all datasets. While joint training introduces a\nranks second in terms of computational efficiency (FLOPs).\nslight increase in FLOPs compared to disjoint training, this\nOn the TrafficBJ task, as detailed in Table IV, S-HR-\nmarginal rise is outweighed by the significant benefits of joint\nVQVAE exhibits exceptional performance, outperforming ex-\ntraining, particularly in improving spatiotemporal modeling,\nisting state-of-the-art methods on all evaluation metrics. Our\nwhich contributes to the overall performance of the model.\nmodel particularly stands out by significantly outperforming\nmethods such as PhyDNet and SimVP, achieving the highest Theeffectivenessofourapproachcanbefurtherappreciated\nscores across all metrics. A similar trend is observed on the by considering the following qualitative analysis since objec-\nchallenging task of Human3.6M, where S-HR-VQVAE again tive metrics might not capture all aspects of the actual quality\noutperforms the current state-of-the-art approaches, leading of the predicted sequences.\n8\nTABLE IV: Results on TrafficBJ and Human 3.6M. S-HR-VQVAE with 3 layers was used with disjoint and joint training.\nTrafficBJ(4‚Üí4) Human3.6M(4‚Üí4)\nMethod MSE√ó√ó√ó100‚Üì MAE‚Üì SSIM‚Üë FLOPs MSE/10‚Üì MAE/100‚Üì SSIM‚Üë FVD‚Üì FLOPs\nConvLSTM(2015)[12] 48.5 17.7 0.978 20.74G 50.4 18.9 0.776 28.4 347.0G\nPredRNN(2017)[13] 46.4 17.1 0.971 42.40G 48.4 18.9 0.781 24.7 704.0G\nPredRNN++(2018)[14] 44.8 16.9 0.977 62.95G √ó √ó √ó √ó 1,033G\nE3D-LSTM(2019)[16] 43.2 16.9 0.979 98.19G 46.4 16.6 0.869 23.7 542.0G\nMIM(2019)[19] 42.9 16.6 0.971 64.10G 42.9 17.8 0.790 21.8 1,051G\nPhyDNet(2020)[39] 41.9 16.2 0.982 5.60G 36.9 16.2 0.901 18.3 19.1G\nMotionRNN(2021)[40] √ó √ó √ó √ó 34.2 14.8 0.846 18.3 49.5G\nSimVP(2022)[20] 41.4 16.2 0.982 3.61G 31.6 15.1 0.904 √ó 197.0G\nPredRNN-V2(2023)[15] 45.6 16.8 0.980 42.63G 36.3 17.7 0.863 √ó 708.0G\nS-HR-VQVAE-disjoint(ours) 41.5 16.2 0.985 3.11G 30.9 14.4 0.916 16.5 16.7G\nS-HR-VQVAE-joint(ours) 40.3 15.2 0.993 4.07G 30.4 12.4 0.939 15.2 17.4G\n(‚Üë)meanshigherisbetterand(‚Üì)meanslowerisbetter.\nInputs Prediction Ground Truth Inputs Prediction Ground Truth\nConvLSTM\nConvLSTM\nPredRNN\nDFN\nR-ST-ConvLSTM\nPredRNN\nPredRNN-V2\nPredRNN++\nS-HR-VQVAE-disjoint\nfRNN\nS-HR-VQVAE-joint\nMcNet (b) Jogging.\nInputs Prediction Ground Truth\n8 10 12 14 16 18 20 22 24 26 28 30\nConv-TT-LSTM\nPredRNN-V2\nLMC-Memory\nVPTR\nS-HR-VQVAE-disjoint\nS-HR-VQVAE-disjoint\nS-HR-VQVAE-joint\nS-HR-VQVAE-joint\n(a) Walking. (c) Handwaving.\nFig.2:ComparisonofS-HR-VQVAEwithstate-of-the-artmethodsonKTHHumanMovingActiondatasetoverthreesequences\n(a, b, and c) that are commonly reported in the literature. It should be noted that 10 frames (1-10 in the figures) are given as\ninput, and the next 20 frames (11-30 in the figures) are predicted.\nB. Qualitative Analysis of the jogger is observed along with sharper images.\nFigure 2 shows the predictions for different state-of-the- Figure 3 presents a qualitative analysis of the results\nart methods and S-HR-VQVAE on the KTH Human Action obtained for TrafficBJ samples. To enhance the clarity of\ndatasetforthreedifferentactivities:walking(panela),jogging our comparisons, we include visualizations of the differences\n(panelb),andhandwaving(panelc).Inthehandwaveactivity, between the predictions and the corresponding ground truth\nfor example, hand movements are relatively fast, but S-HR- images. S-HR-VQVAE demonstrates impressive performance\nVQVAE can better predict the ground truth whilst avoiding in generating predicted frames when compared to the other\nblurry outputs, as shown in frames 28 and 30. In the walking models, as evidenced by the minimal intensity of differences\ntask,mostmethodsdonotpredictwellthepositionofthebody observed.ItisnoteworthythatS-HR-VQVAEobtainsthebest\nand the legs, except for our method, PredRNN, PredRNN++, result on all metrics for this task.\nand PredRNN-V2 (see frames 27 and 30, for example). The qualitative analysis presented in Figure 4 reveals that\nHowever, our method produces sharper images and correctly S-HR-VQVAE generates more precise predictions for motion\npredictsthelocationofbothlegsfortheseframes.Finally,for positions and object sizes. This observation underscores the\nthe jogging task, an overall better estimation of the location efficacyofS-HR-VQVAEwhenappliedtointricatereal-world\n9\nTABLE V: Results on Kitti dataset. S-HR-VQVAE with 3\nlayers was used with disjoint and joint training.\nInputs Prediction Ground Truth\nKitti(4‚Üí5)\nMethod SSIM‚Üë LPIPS‚Üì FVD‚Üì\nPredRNN(2017)[13] 0.475 0.629 √ó\nMcNet(2018)[10] 0.554 0.373 √ó\nNPVP(2023)[56] 0.661 0.279 134.69\nPredRNN\nS-HR-VQVAE-disjoint(ours) 0.673 0.188 127.04\nS-HR-VQVAE-joint(ours) 0.692 0.164 121.84\nKitti(5‚Üí5)\nSSIM‚Üë LPIPS‚Üì PSNR‚Üë\nMIM\nPhyDNet(2020)[39] 0.674 0.403 19.159\nLMC-Memory(2021)[41] 0.660 0.410 18.692\nMotionRNN(2021)[40] 0.652 0.384 18.931\nMIMO(2023)[57] 0.703 0.308 19.616\nS-HR-VQVAE-disjoint(ours) 0.845 0.187 19.774\nPhyDNet S-HR-VQVAE-joint(ours) 0.861 0.114 21.877\n(‚Üë)meanshigherisbetterand(‚Üì)meanslowerisbetter.\nInputs Prediction Ground Truth\nt = 4 t = 5 t = 6 t = 7 t = 8 t = 9\nS-HR-VQVAE (joint)\n‚Ä¶\nFig. 3: Comparison of S-HR-VQVAE with state-of-the-art-\nmethodsonTrafficBJdataset.Itshouldbenotedthat4frames\nNPVP\n(1-4 in the figure) are given as input, and the next 4 frames\n(5-8 in the figure) are predicted.\nS-HR-VQVAE\n(joint)\nInputs Prediction Ground Truth Fig. 5: Comparison of S-HR-VQVAE with the state-of-the-art\nmethodontheKittidataset,where4framesaregivenasinput,\nand the next 5 frames are predicted.\ndatasets. S-HR-VQVAE better performance in predicting ob-\nject positions and sizes can be attributed to the collaborative\nConvLSTM interaction between our spatiotemporal predictive model and\nthe decoder, as stated in the objective function in Eq. 9.\nThe qualitative assessment on the Kitti dataset is depicted\nMIM\nin Figure 5. From the visual analysis, it is evident that S-\nHR-VQVAE exhibits finer details, such as intricate shadow\npatterns, leaf textures on trees, and more precise car features,\nE3D-LSTM\ncompared to NPVP. Moreover, S-HR-VQVAE significantly\nreduced blurry predictions compared to NPVP. These obser-\nvations align well with the quantitative findings presented in\nPredRNN\nTable V, where S-HR-VQVAE outperforms NPVP across all\nevaluationmetrics:SSIM,LPIPS,andFVD.ThehigherSSIM\nPredRNN score of S-HR-VQVAE indicates better structural similarity\n+MotionRNN between predicted and ground truth frames, while the lower\nLPIPS value suggests reduced perceptual differences, under-\nscoring the model‚Äôs ability to generate more visually faithful\nS-HR-VQVAE\n(joint) predictions.Furthermore,thesignificantlylowerFVDscoreof\nS-HR-VQVAEcomparedtoNPVPhighlightsitssuperiorityin\nFig. 4: Comparison of S-HR-VQVAE with state-of-the-art-\ncapturing temporal consistencies and minimizing artifacts.\nmethods on Human3.6M dataset. 4 frames (1-4 in the figure)\nWe can summarise the outcome of the qualitative analy-\nare given as input, and the next 4 frames (5-8 in the figure)\nsis as follows: Although quantitative analysis is useful for\nare predicted.\nunderstanding whether a sequence prediction technique is\nviable or not, objective measures by themselves may not\n10\nFig. 6: Heatmap of reconstructions obtained from different\nlayers of a 3-layer HR-VQVAE.\nreveal the actual capability of a technique. State-of-the-art\nmethods exhibit a varying sequence prediction quality across\ntasks, as observed, for example, in Figure 2 despite the\nFig. 7: Reconstructions by 3-layer HR-VQVAE. a) Gaussian\ngood numerical results reported in Table III, whereas S-\nBlur b) Fragment Blur c) Noise d) Compression. Zoom in to\nHR-VQVAE performs consistently across tasks. Moreover,\nsee more details.\nthe figure suggests joint training within our methodology\nleadstoasignificantenhancementinlocationprediction.This\nimprovement is evident across the majority of the frames. applications where the quality of the predicted video frames\nNevertheless, it is important to acknowledge that while this is critical, such as autonomous driving.\nimprovementinlocationpredictionisevident,itappearstobe\naccompanied by a minor reduction in image sharpness in the\nVIII. CONCLUSION\nreconstructed frames. This observation may provide insights\ninto the relatively modest quantitative improvements observed In this study, we proposed a video prediction framework\nin our results following the incorporation of joint training. that combines the hierarchical vector quantization codebooks\nof the previously proposed HR-VQVAE with the novel au-\nVII. DISCUSSION toregressive spatiotemporal predictive model (AST-PM). We\ncall this method sequential HR-VQVAE (S-HR-VQVAE). We\nA. Model Interpretability\nshow how the proposed S-HR-VQVAE takes advantage of\nTo facilitate the interpretation of latent representations pro-\nhierarchical frame modeling to model different levels of ab-\nduced by the model, we present heatmaps over various layers\nstraction, enabling the system to capture both context and\nof HR-VQVAE in Fig. 6. Each heatmap highlights regions\nmovements (details) in video frames with a fraction of the\nof significance within the reconstructed latent representation.\nparameters used by competing models. We show by extensive\nGeneral information, i.e., background, is mainly captured in\nexperimental evidence on the KTH Human Action, TrafficBJ,\nthe first layer; the second layer focuses on the position of the\nHuman3.6M,andKittitasksthatthemodelisverycompetitive\nforeground object, whereas the third layer is concerned with\nwiththestate-of-the-artinvideoprediction,outperformingthe\ndetails of the moving objects.\nbest methods, at least in a subset of the available metrics\n(PSNR, SSIM, LPIPS, FVD, MSE, and MAE) with signif-\nB. Blur, Noise, and Compression icantly lower number of parameters. We provide a detailed\nanalysisofthepropertiesofthemodel,includingananalysisof\nTo gain more insights into the effectiveness of S-HR-\nits internal representations and its behavior concerning blurry\nVQVAE against blurriness, we artificially corrupt some video\nand noisy input frames. The proposed method is competitive\nsequencesbyinjectingGaussianBlur(Fig.7-a)andFragment\nfor the video prediction task, in terms of performance, low\nBlur(Fig.7-b).Thepredictionresultsreportedinthosefigures\ncomplexity, and interpretability.\ndemonstrate that HR-VQVAE can successfully reduce blurri-\nness while being able to reconstruct details in the images that\nwere lost due to the blur effect. In addition to blur mitigation,\nREFERENCES\nHR-VQVAE is also robust to noise, as shown in Fig. 7-c,\nwhere accurate sequence prediction is attained although the [1] V.Vukotic¬¥,S.-L.Pintea,C.Raymond,G.Gravier,andJ.C.VanGemert,\ninput frames were artificially corrupted with additive noise ‚ÄúOne-steptime-dependentfuturevideoframepredictionwithaconvolu-\ntionalencoder-decoderneuralnetwork,‚ÄùinInternationalconferenceon\nat different SNR levels. Finally, we show the reconstruction\nimageanalysisandprocessing. Springer,2017,pp.140‚Äì151.\nof compressed images with two levels of compression ratio [2] C.Lu,M.Hirsch,andB.Scho¬®lkopf,‚ÄúFlexiblespatio-temporalnetworks\nin Fig. 7-d, showcasing the HR-VQVAE‚Äôs robustness against forvideoprediction,‚Äùin2017IEEEConferenceonComputerVisionand\nPatternRecognition(CVPR),2017.\ncompression. HR-VQVAE robustness against blur, noise, and\n[3] R. Bellman, Adaptive Control Processes: A Guided Tour. Princeton\ncompression in sequence prediction is especially valuable in UniversityPress,1961.\n11\n[4] M.Adiban,K.Stefanov,S.M.Siniscalchi,andG.Salvi,‚ÄúHierarchical [24] J.-T. Hsieh, B. Liu, D.-A. Huang, L. F. Fei-Fei, and J. C. Niebles,\nresidual learning based vector quantized variational autoencoder for ‚ÄúLearning to decompose and disentangle representations for video\nimage reconstruction and generation,‚Äù in 33rd British Machine Vision prediction,‚Äù in Advances in Neural Information Processing Systems,\nConference 2022, BMVC 2022, London, UK, November 21-24, 2022. 2018,pp.517‚Äì526.\nBMVAPress,2022. [25] T. Xue, J. Wu, K. Bouman, and B. Freeman, ‚ÄúVisual dynamics:\n[5] C. Schuldt, I. Laptev, and B. Caputo, ‚ÄúRecognizing human actions: Probabilistic future frame synthesis via cross convolutional networks,‚Äù\na local SVM approach,‚Äù in Proceedings of the 17th International inAdvancesinneuralinformationprocessingsystems,2016,pp.91‚Äì99.\nConferenceonPatternRecognition,2004.ICPR2004.,vol.3. IEEE, [26] D. P. Kingma and M. Welling, ‚ÄúStochastic gradient vb and the varia-\n2004,pp.32‚Äì36. tional auto-encoder,‚Äù in Second International Conference on Learning\n[6] J.Zhang,Y.Zheng,D.Qi,R.Li,X.Yi,andT.Li,‚ÄúPredictingcitywide Representations,ICLR,vol.19,2014.\ncrowd flows using deep spatio-temporal residual networks,‚Äù Artificial [27] M. Oliu, J. Selva, and S. Escalera, ‚ÄúFolded recurrent neural networks\nIntelligence,vol.259,pp.147‚Äì166,2018. forfuturevideoprediction,‚ÄùinProceedingsoftheEuropeanConference\n[7] C.Ionescu,D.Papava,V.Olaru,andC.Sminchisescu,‚ÄúHuman3.6m: onComputerVision(ECCV),2018,pp.716‚Äì731.\nLarge scale datasets and predictive methods for 3d human sensing [28] H. Razali and B. Fernando, ‚ÄúA log-likelihood regularized kl diver-\nin natural environments,‚Äù IEEE transactions on pattern analysis and genceforvideopredictionwitha3dconvolutionalvariationalrecurrent\nmachineintelligence,vol.36,no.7,pp.1325‚Äì1339,2013. network,‚Äù in Proceedings of the IEEE/CVF Winter Conference on\n[8] A.Geiger,P.Lenz,C.Stiller,andR.Urtasun,‚ÄúVisionmeetsrobotics: ApplicationsofComputerVision,2021,pp.209‚Äì217.\nThe kitti dataset,‚Äù The International Journal of Robotics Research, [29] X.YeandG.-A.Bilodeau,‚ÄúVideopredictionbyefficienttransformers,‚Äù\nvol.32,no.11,pp.1231‚Äì1237,2013. ImageandVisionComputing,vol.130,p.104612,2023.\n[9] E. Denton and V. Birodkar, ‚ÄúUnsupervised learning of disentangled [30] A. B. L. Larsen, S. K. S√∏nderby, H. Larochelle, and O. Winther,\nrepresentations from video,‚Äù in Proceedings of the 31st International ‚ÄúAutoencoding beyond pixels using a learned similarity metric,‚Äù in\nConferenceonNeuralInformationProcessingSystems,2017,pp.4417‚Äì ProceedingsofThe33rdInternationalConferenceonMachineLearning,\n4426. ser.ProceedingsofMachineLearningResearch,2016,pp.1558‚Äì1566.\n[10] R.Villegas,J.Yang,S.Hong,X.Lin,andH.Lee,‚ÄúDecomposingmotion [31] L.Castrejon,N.Ballas,andA.Courville,‚ÄúImprovedconditionalvrnns\nandcontentfornaturalvideosequenceprediction,‚Äùin5thInternational forvideoprediction,‚ÄùinProceedingsoftheIEEEInternationalConfer-\nConference on Learning Representations, ICLR 2017. International enceonComputerVision,2019,pp.7608‚Äì7617.\nConferenceonLearningRepresentations,ICLR,2017.\n[32] E. Denton and R. Fergus, ‚ÄúStochastic video generation with a learned\n[11] J. Lee, J. Lee, S. Lee, and S. Yoon, ‚ÄúMutual suppression network prior,‚ÄùinInternationalconferenceonmachinelearning. PMLR,2018,\nfor video prediction using disentangled features,‚Äù in British Machine pp.1174‚Äì1183.\nVisionConference,2018.\n[33] A. X. Lee, R. Zhang, F. Ebert, P. Abbeel, C. Finn, and\n[12] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c.\nS. Levine, ‚ÄúStochastic adversarial video prediction,‚Äù arXiv preprint\nWoo, ‚ÄúConvolutional lstm network: A machine learning approach for\narXiv:1804.01523,2018.\nprecipitation nowcasting,‚Äù Advances in neural information processing\n[34] M. Babaeizadeh, C. Finn, D. Erhan, R. Campbell, and S. Levine,\nsystems,vol.28,2015.\n‚ÄúStochastic variational video prediction,‚Äù in 6th International Confer-\n[13] Y.Wang,M.Long,J.Wang,Z.Gao,andP.S.Yu,‚ÄúPredRNN:Recurrent\nenceonLearningRepresentations,ICLR2018,2018.\nneural networks for predictive learning using spatiotemporal lstms,‚Äù\n[35] X.Jia,B.DeBrabandere,T.Tuytelaars,andL.V.Gool,‚ÄúDynamicfilter\nAdvancesinneuralinformationprocessingsystems,vol.30,2017.\nnetworks,‚ÄùAdvancesinneuralinformationprocessingsystems,vol.29,\n[14] Y. Wang, Z. Gao, M. Long, J. Wang, and S. Y. Philip, ‚ÄúPredRNN++:\n2016.\nTowards a resolution of the deep-in-time dilemma in spatiotemporal\n[36] C. Finn, I. Goodfellow, and S. Levine, ‚ÄúUnsupervised learning for\npredictivelearning,‚ÄùinInternationalConferenceonMachineLearning.\nphysical interaction through video prediction,‚Äù in Advances in neural\nPMLR,2018,pp.5123‚Äì5132.\ninformationprocessingsystems,2016,pp.64‚Äì72.\n[15] Y.Wang,H.Wu,J.Zhang,Z.Gao,J.Wang,S.Y.Philip,andM.Long,\n[37] Z.Liu,R.A.Yeh,X.Tang,Y.Liu,andA.Agarwala,‚ÄúVideoframesyn-\n‚ÄúPredrnn: A recurrent neural network for spatiotemporal predictive\nthesisusingdeepvoxelflow,‚ÄùinProceedingsoftheIEEEInternational\nlearning,‚Äù IEEE Transactions on Pattern Analysis and Machine Intel-\nConferenceonComputerVision,2017,pp.4463‚Äì4471.\nligence,vol.45,no.2,pp.2208‚Äì2225,2023.\n[38] M. Jaderberg, K. Simonyan, A. Zisserman et al., ‚ÄúSpatial transformer\n[16] Y. Wang, L. Jiang, M.-H. Yang, L.-J. Li, M. Long, and L. Fei-\nnetworks,‚ÄùinAdvancesinneuralinformationprocessingsystems,2015,\nFei, ‚ÄúEidetic 3d lstm: A model for video prediction and beyond,‚Äù in\npp.2017‚Äì2025.\nInternationalconferenceonlearningrepresentations,2019.\n[39] V. L. Guen and N. Thome, ‚ÄúDisentangling physical dynamics from\n[17] J.Su,W.Byeon,J.Kossaifi,F.Huang,J.Kautz,andA.Anandkumar,\nunknownfactorsforunsupervisedvideoprediction,‚ÄùinProceedingsof\n‚ÄúConvolutionaltensor-trainlstmforspatio-temporallearning,‚ÄùAdvances\ntheIEEE/CVFConferenceonComputerVisionandPatternRecognition,\ninNeuralInformationProcessingSystems,vol.33,pp.13714‚Äì13726,\n2020,pp.11474‚Äì11484.\n2020.\n[40] H. Wu, Z. Yao, J. Wang, and M. Long, ‚ÄúMotionrnn: A flexible model\n[18] W.Saideni,D.Helbert,F.Courreges,andJ.P.Cances,‚ÄúAnovelvideo\nforvideopredictionwithspacetime-varyingmotions,‚ÄùinProceedingsof\npredictionalgorithmbasedonrobustspatiotemporalconvolutionallong\nshort-term memory (robust-st-convlstm),‚Äù in Proceedings of Seventh\ntheIEEE/CVFconferenceoncomputervisionandpatternrecognition,\nInternationalCongressonInformationandCommunicationTechnology: 2021,pp.15435‚Äì15444.\nICICT2022,London,Volume2. Springer,2022,pp.193‚Äì204. [41] S. Lee, H. G. Kim, D. H. Choi, H.-I. Kim, and Y. M. Ro, ‚ÄúVideo\n[19] Y.Wang,J.Zhang,H.Zhu,M.Long,J.Wang,andP.S.Yu,‚ÄúMemory prediction recalling long-term motion context via memory alignment\nin memory: A predictive neural network for learning higher-order learning,‚Äù in Proceedings of the IEEE/CVF Conference on Computer\nnon-stationarity from spatiotemporal dynamics,‚Äù in Proceedings of the VisionandPatternRecognition,2021,pp.3054‚Äì3063.\nIEEE/CVFconferenceoncomputervisionandpatternrecognition,2019, [42] C. Doersch, ‚ÄúTutorial on variational autoencoders,‚Äù arXiv preprint\npp.9154‚Äì9162. arXiv:1606.05908,2016.\n[20] Z.Gao,C.Tan,L.Wu,andS.Z.Li,‚ÄúSimVP:Simpleryetbettervideo [43] A. van den Oord, O. Vinyals et al., ‚ÄúNeural discrete representation\nprediction,‚ÄùinProceedingsoftheIEEE/CVFConferenceonComputer learning,‚ÄùinAdvancesinNeuralInformationProcessingSystems,2017,\nVisionandPatternRecognition,2022,pp.3170‚Äì3180. pp.6306‚Äì6315.\n[21] Z.Chang,X.Zhang,S.Wang,S.Ma,andW.Gao,‚ÄúStrpm:Aspatiotem- [44] A. Razavi, A. van den Oord, and O. Vinyals, ‚ÄúGenerating diverse\nporal residual predictive model for high-resolution video prediction,‚Äù high-fidelityimageswithvq-vae-2,‚ÄùinAdvancesinNeuralInformation\nin Proceedings of the IEEE/CVF conference on computer vision and ProcessingSystems,2019,pp.14837‚Äì14847.\npatternrecognition,2022,pp.13946‚Äì13955. [45] J.Zhang,Y.Zheng,andD.Qi,‚ÄúDeepspatio-temporalresidualnetworks\n[22] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert, and for citywide crowd flows prediction,‚Äù in Proceedings of the AAAI\nS.Chopra,‚ÄúVideo(language)modeling:abaselineforgenerativemodels conferenceonartificialintelligence,vol.31,no.1,2017.\nofnaturalvideos,‚ÄùarXivpreprintarXiv:1412.6604,2014. [46] D.P.KingmaandJ.Ba,‚ÄúAdam:Amethodforstochasticoptimization,‚Äù\n[23] C. Xu, P. Zhao, Y. Liu, J. Xu, V. S. S. S. Sheng, Z. Cui, X. Zhou, arXivpreprintarXiv:1412.6980,2014.\nand H. Xiong, ‚ÄúRecurrent convolutional neural network for sequential [47] S.WinklerandP.Mohandas,‚ÄúTheevolutionofvideoqualitymeasure-\nrecommendation,‚ÄùinTheworldwidewebconference,2019,pp.3398‚Äì ment:Frompsnrtohybridmetrics,‚ÄùIEEEtransactionsonBroadcasting,\n3404. vol.54,no.3,pp.660‚Äì668,2008.\n12\n[48] V. Sitzmann, M. Zollho¬®fer, and G. Wetzstein, ‚ÄúScene representation\nnetworks:Continuous3d-structure-awareneuralscenerepresentations,‚Äù\nAdvancesinNeuralInformationProcessingSystems,vol.32,2019.\n[49] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, ‚ÄúThe\nunreasonable effectiveness of deep features as a perceptual metric,‚Äù in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition,2018,pp.586‚Äì595.\n[50] T.Unterthiner,S.vanSteenkiste,K.Kurach,R.Marinier,M.Michalski,\nandS.Gelly,‚ÄúFvd:Anewmetricforvideogeneration,‚ÄùICLR,2019.\n[51] S. Mrak et al., ‚ÄúReliability of objective picture quality measures,‚Äù\nJournalofElectricalEngineering,vol.55,no.1-2,pp.3‚Äì10,2004.\n[52] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‚ÄúImage\nquality assessment: from error visibility to structural similarity,‚Äù IEEE\ntransactionsonimageprocessing,vol.13,no.4,pp.600‚Äì612,2004.\n[53] X. Fei, L. Xiao, Y. Sun, and Z. Wei, ‚ÄúPerceptual image quality\nassessment based on structural similarity and visual masking,‚Äù Signal\nProcessing:ImageCommunication,vol.27,no.7,pp.772‚Äì783,2012.\n[54] P. J. Kim, S. Kim, and J. Yoo, ‚ÄúStream: Spatio-temporal evaluation\nand analysis metric for video generative models,‚Äù in The Twelfth\nInternationalConferenceonLearningRepresentations(ICLR),2024.\n[55] B. Jin, Y. Hu, Q. Tang, J. Niu, Z. Shi, Y. Han, and X. Li, ‚ÄúExploring\nspatial-temporalmulti-frequencyanalysisforhigh-fidelityandtemporal-\nconsistencyvideoprediction,‚ÄùinProceedingsoftheIEEE/CVFConfer-\nenceonComputerVisionandPatternRecognition,2020,pp.4554‚Äì4563.\n[56] X.YeandG.-A.Bilodeau,‚ÄúAunifiedmodelforcontinuousconditional\nvideo prediction,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputerVisionandPatternRecognition,2023,pp.3603‚Äì3612.\n[57] S. Ning, M. Lan, Y. Li, C. Chen, Q. Chen, X. Chen, X. Han, and\nS.Cui,‚ÄúMimoisallyouneed:Astrongmulti-in-multi-outbaselinefor\nvideoprediction,‚ÄùinProceedingsoftheAAAIConferenceonArtificial\nIntelligence,vol.37,no.2,2023,pp.1975‚Äì1983.\n13\nMohammadAdibanisaPhDcandidateinMachine\nLearningattheNorwegianUniversityofScienceand\nTechnology(NTNU).HeholdsaBachelor‚Äôsdegree\nin Computer Engineering and a Master‚Äôs degree in\nArtificialIntelligencefromtheSharifUniversityof\nTechnology,awardedin2017.In2022,heconducted\nresearch as a visiting scholar at Monash Univer-\nsityinAustralia.Additionally,Mohammadisaco-\nfounder of the company Connect Me and Senior\nData Scientist at Bluware company. His research\nfocuses on statistical machine learning, signal pro-\ncessing, computer vision, speech processing, biomedical applications, and\ncybersecurity.\nKalin Stefanov is an ARC DECRA Fellow at the\nFaculty of Information Technology, Monash Uni-\nversity,Melbourne,Australia.HereceivedtheMSc\ndegreeinArtificialIntelligencefromtheUniversity\nofAmsterdam,Amsterdam,NetherlandsandaPhD\ndegree in Computer Science from KTH Royal In-\nstituteofTechnology,Stockholm,Sweden.Priorto\nhis current role, he was a Research Associate and\nPostdoctoral Research Scholar at the University of\nSouthern California, Los Angeles, USA. His main\nresearch interests are machine learning, computer\nvision,andaffectivecomputing.\nSabato Marco Siniscalchi (SeniorMember,IEEE)\nis a FULL Professor with the University of\nPalermo,Palermo, Italy, an Adjunct Professor with\nthe Norwegian University of Science and Technol-\nogy (NTNU), and an Affiliate Faculty with the\nGeorgia Institute of Technology. He received his\ndoctorate degree in computer engineering from the\nUniversity of Palermo, Palermo, Italy, in 2006. In\n2006, he was a Postdoctoral Fellow with Ga Tech.\nFrom2007to2010,hejoinedNTNU,Norway,asa\nResearch Scientist. From 2010 to 2023, he was an\nAssistantProfessor,first,anAssociateProfessor,second,andaFullProfessor,\nafter, at Kore University. From 2017 to 2018, he was a Senior Speech\nResearcherwithSiriSpeechGroup,AppleInc.,CupertinoCA,USA.Heacted\nasanAssociateEditoroftheIEEE/ACMTransactionsonAudio,Speechand\nLanguage Processing, from 2015 to 2019. Prof. Siniscalchi was an Elected\nMemberoftheIEEESLTCommitteefrom2019to2022andwasre-elected\nin2024.\nGiampiero Salvi (Senior Member, IEEE) is a Full\nProfessor at the Department of Electronic Systems\nat the Norwegian University of Science and Tech-\nnology(NTNU),Trondheim,Norway,andAssociate\nProfessor at KTH Royal Institute of Technology,\nDepartmentofElectricalEngineeringandComputer\nScience, Stockholm, Sweden. Prof. Salvi received\nthe MSc degree in Electronic Engineering from\nUniversita` la Sapienza, Rome, Italy and the PhD\ndegree in Computer Science from KTH. He was a\npost-doctoralfellowattheInstituteofSystemsand\nRobotics, Lisbon, Portugal. He was a co-founder of the company SynFace\nAB,activebetween2006and2016.Hismaininterestsaremachinelearning,\nspeechtechnology,andcognitivesystems.",
    "pdf_filename": "S-HR-VQVAE_Sequential_Hierarchical_Residual_Learning_Vector_Quantized_Variational_Autoencoder_for_Vi.pdf"
}