{
    "title": "S-HR-VQVAE Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Vi",
    "context": "forth a novel model that combines (i) a novel hierarchical residual learning vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel autoregressive spatiotemporal predictive model (AST-PM). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S- HR-VQVAE). By leveraging the intrinsic capabilities of HR- VQVAE at modeling still images with a parsimonious represen- tation, combined with the AST-PM’s ability to handle spatiotem- poral information, S-HR-VQVAE can better deal with major challenges in video prediction. These include learning spatiotem- poral information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical character- istics. Extensive experimental results on four challenging tasks, namely KTH Human Action, TrafficBJ, Human3.6M, and Kitti, demonstrate that our model compares favorably against state- of-the-art video prediction techniques both in quantitative and qualitative evaluations despite a much smaller model size. Finally, we boost S-HR-VQVAE by proposing a novel training method to jointly estimate the HR-VQVAE and AST-PM parameters. Index Terms—Video Prediction, Hierarchical Modeling, Au- toregressive Modeling Video prediction involves anticipating future video frames based on a sequence of preceding frames [1]. It is a challeng- ing task, requiring algorithms to grasp complex spatiotemporal relationships within the video, at the same time as handling high dimensionality, addressing blurry predictions, and ac- counting for the physical characteristics of the scenes. Spa- tiotemporal modeling aims to capture dependencies in video frame sequences, mirroring human perception of dynamic phe- nomena [2]. This is a general problem in video modeling, but becomes especially challenging when we need to recursively and accurately predict video frames for long temporal spans. Current state-of-the-art methods often struggle with long-term dependencies and complex motion patterns, leading to inaccu- racies in the predicted frames. High dimensionality is inherent in video patterns, leading to the “curse of dimensionality” in function approximation and optimization [3]. Autoencoder- based methods attempt to reduce dimensionality, but may lose important fine-grained details necessary for accurate predic- tion. Blurry predictions stem from statistical models producing fuzzier outputs when predicting uncertain future events. This is, therefore, a more challenging problem for video prediction than for any other video task. Most methods use mean squared error (MSE) objective that tends to average over possible outcomes, resulting in blurred predictions. The challenge of physical characteristics pertains to object and scene attributes affecting prediction. Proper modeling of these characteristics may potentially aid future frame predictions. Recent video prediction methods have made significant progress in tackling these challenges, yet they still face several limitations. We will detail the state-of-the-art with respect to each of these challenges in Section II. This paper introduces a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR- VQVAE), which is tailored for video prediction with the goal of tackling the above-mentioned challenges. To this end, S- HR-VQVAE implements a novel autoregressive spatiotempo- ral predictive model (AST-PM) to capture distributions of dependencies between latent representations across time and space. The latent representations are generated through our novel encoding scheme, termed hierarchical vector quanti- zation variational autoencoder (HR-VQVAE) that we have recently used with success for still image reconstruction [4]. Leveraging those two novel blocks, namely HR-VQVAE, and AST-PM, S-HR-VQVAE effectively tackles the video prediction task in three steps: In the first step, the input video frames are encoded to a continuous latent space and then mapped to discrete representations through HR-VQVAE, with each latent vector, in each layer in the model, assigned to a codeword in a codebook. The key property of this model is the strict hierarchy imposed between codebooks belonging to different layers, producing extremely compact and efficient discrete representations. In the second step, we predict future events in latent rather than image space. To perform this prediction, we use spatiotemporal modeling (the proposed AST-PM), where the distribution of the discrete latent representations for a particular location in the current frame is conditioned on the representations for neighboring locations both in space and time. In the third and final step, the predicted discrete representations are used by the HR-VQVAE arXiv:2307.06701v3  [cs.CV]  19 Nov 2024",
    "body": "1\nS-HR-VQVAE: Sequential Hierarchical Residual\nLearning Vector Quantized Variational Autoencoder\nfor Video Prediction\nMohammad Adiban†, Kalin Stefanov∗, Sabato Marco Siniscalchi†+ Senior Member, IEEE, Giampiero\nSalvi† Senior Member, IEEE\n†Norwegian University of Science and Technology\n∗Monash University\n+Universit`a degli Studi di Palermo\nE-mails: {mohammad.adiban,marco.siniscalchi,giampiero.salvi}@ntnu.no, kalin.stefanov@monash.edu\nAbstract—We address the video prediction task by putting\nforth a novel model that combines (i) a novel hierarchical residual\nlearning vector quantized variational autoencoder (HR-VQVAE),\nand (ii) a novel autoregressive spatiotemporal predictive model\n(AST-PM). We refer to this approach as a sequential hierarchical\nresidual learning vector quantized variational autoencoder (S-\nHR-VQVAE). By leveraging the intrinsic capabilities of HR-\nVQVAE at modeling still images with a parsimonious represen-\ntation, combined with the AST-PM’s ability to handle spatiotem-\nporal information, S-HR-VQVAE can better deal with major\nchallenges in video prediction. These include learning spatiotem-\nporal information, handling high dimensional data, combating\nblurry prediction, and implicit modeling of physical character-\nistics. Extensive experimental results on four challenging tasks,\nnamely KTH Human Action, TrafficBJ, Human3.6M, and Kitti,\ndemonstrate that our model compares favorably against state-\nof-the-art video prediction techniques both in quantitative and\nqualitative evaluations despite a much smaller model size. Finally,\nwe boost S-HR-VQVAE by proposing a novel training method\nto jointly estimate the HR-VQVAE and AST-PM parameters.\nIndex Terms—Video Prediction, Hierarchical Modeling, Au-\ntoregressive Modeling\nI. INTRODUCTION\nVideo prediction involves anticipating future video frames\nbased on a sequence of preceding frames [1]. It is a challeng-\ning task, requiring algorithms to grasp complex spatiotemporal\nrelationships within the video, at the same time as handling\nhigh dimensionality, addressing blurry predictions, and ac-\ncounting for the physical characteristics of the scenes. Spa-\ntiotemporal modeling aims to capture dependencies in video\nframe sequences, mirroring human perception of dynamic phe-\nnomena [2]. This is a general problem in video modeling, but\nbecomes especially challenging when we need to recursively\nand accurately predict video frames for long temporal spans.\nCurrent state-of-the-art methods often struggle with long-term\ndependencies and complex motion patterns, leading to inaccu-\nracies in the predicted frames. High dimensionality is inherent\nin video patterns, leading to the “curse of dimensionality”\nin function approximation and optimization [3]. Autoencoder-\nbased methods attempt to reduce dimensionality, but may lose\nimportant fine-grained details necessary for accurate predic-\ntion. Blurry predictions stem from statistical models producing\nfuzzier outputs when predicting uncertain future events. This\nis, therefore, a more challenging problem for video prediction\nthan for any other video task. Most methods use mean squared\nerror (MSE) objective that tends to average over possible\noutcomes, resulting in blurred predictions. The challenge of\nphysical characteristics pertains to object and scene attributes\naffecting prediction. Proper modeling of these characteristics\nmay potentially aid future frame predictions. Recent video\nprediction methods have made significant progress in tackling\nthese challenges, yet they still face several limitations. We\nwill detail the state-of-the-art with respect to each of these\nchallenges in Section II.\nThis paper introduces a sequential hierarchical residual\nlearning vector quantized variational autoencoder (S-HR-\nVQVAE), which is tailored for video prediction with the goal\nof tackling the above-mentioned challenges. To this end, S-\nHR-VQVAE implements a novel autoregressive spatiotempo-\nral predictive model (AST-PM) to capture distributions of\ndependencies between latent representations across time and\nspace. The latent representations are generated through our\nnovel encoding scheme, termed hierarchical vector quanti-\nzation variational autoencoder (HR-VQVAE) that we have\nrecently used with success for still image reconstruction [4].\nLeveraging those two novel blocks, namely HR-VQVAE,\nand AST-PM, S-HR-VQVAE effectively tackles the video\nprediction task in three steps: In the first step, the input\nvideo frames are encoded to a continuous latent space and\nthen mapped to discrete representations through HR-VQVAE,\nwith each latent vector, in each layer in the model, assigned\nto a codeword in a codebook. The key property of this\nmodel is the strict hierarchy imposed between codebooks\nbelonging to different layers, producing extremely compact\nand efficient discrete representations. In the second step, we\npredict future events in latent rather than image space. To\nperform this prediction, we use spatiotemporal modeling (the\nproposed AST-PM), where the distribution of the discrete\nlatent representations for a particular location in the current\nframe is conditioned on the representations for neighboring\nlocations both in space and time. In the third and final step, the\npredicted discrete representations are used by the HR-VQVAE\narXiv:2307.06701v3  [cs.CV]  19 Nov 2024\n\n2\ndecoder to generate the corresponding frame. Normally, HR-\nVQVAE and AST-PM may be trained independently. However,\nwe also propose a novel joint training scheme to optimize HR-\nVQVAE and AST-PM together and show that this improves\nvideo prediction. We argue that the reason for the improved\nperformance is that AST-PM and the decoder of HR-VQVAE\nare trained in such a way as to optimize both the predicted\nquantized latent representation for future frames as well as the\nreconstruction of future frames in image space.\nOur contributions can be summarized as follows:\n• S-HR-VQVAE, a novel technique for video prediction, is\nproposed. This includes a hierarchical vector quantized\nencoding scheme and a spatiotemporal autoregressive\nmodel of the latent representations. This model allows the\ncapture of different levels of abstraction in a sequence of\nvideo frames, thus resulting in a compact but effective\nrepresentation of the task.\n• A novel loss function to jointly train the components of\nS-HR-VQVAE (HR-VQVAE and AST-PM) with further\nimprovements of the prediction performance.\n• State-of-the-art results on several challenging video pre-\ndiction tasks, namely KTH Human Action [5], Traf-\nficBJ [6], Human3.6M [7] and Kitti [8].\nII. RELATED WORK\nA. Spatiotemporal Modeling\nHu et al. [9] introduced disentangled representation net (Dr-\nNet) for spatial feature modeling in single video frames, ne-\nglecting temporal information. Motion-content network (Mc-\nNet) [10] and mutual suppression network (MsNet) [11]\naddressed motion and content separately, overlooking joint\ncorrelations. Convolutional long short-term memory (Con-\nvLSTM) [12] aimed at capturing both spatial and temporal\ncorrelations but struggled with long-term dependencies and\nscalability. To overcome ConvLSTM’s limitations, Wang et al.\nproposed predictive recurrent neural network (PredRNN) [13],\nwhich, despite improvements, still faced challenges in mod-\neling complex long-term dependencies. PredRNN++ [14] and\nPredRNN-V2 [15] aimed to enhance PredRNN’s performance\nby incorporating hierarchical recurrent structures. Eidetic 3D\nLSTM (E3D-LSTM) [16] was introduced to jointly model\nspatial and temporal dynamics. Su et al. [17] improved ef-\nficiency using low-rank tensor factorization, while robust spa-\ntiotemporal LSTM (R-ST-LSTM) [18] and memory in memory\n(MIM) [19] demonstrated performance improvements in long-\nterm frame prediction tasks. The simple video prediction\nmodel (SimVP) [20] showed significant improvement over\nRNN-based models but struggled with encoding long-term dy-\nnamics, making accurate future prediction challenging. Chang\net al. [21] introduce hierarchical semantic separation in video\nprediction using a spatiotemporal encoding-decoding scheme\nand residual predictive memory called STRPM. This scheme\nseparates spatial and temporal information with independent\nencoders, preserving distinct features and improving high-\nresolution video predictions. The STRPM refines the separa-\ntion by focusing on inter-frame residuals for more accurate\nfuture predictions. However, STRPM’s reliance on residual\ninter-frame motion can oversimplify complex dynamics, and\nits implicit hierarchy may limit its ability to capture fine-\ngrained spatiotemporal details compared to models with ex-\nplicit multi-layered hierarchies.\nTo address the spatiotemporal challenge, S-HR-VQVAE\nleverages our proposed AST-PM module. In this module,\ncausal convolutions in time and spatiotemporal self-attention\nare used to model the spatiotemporal correlations on the\nquantized codes level. Moreover, AST-PM operates on the\nlatent discrete representations produced by the HR-VQVAE\nmodule instead of using pixels directly.\nB. High Dimensionality\nThe above spatiotemporal methods rely on complex mod-\neling, which hampers scalability, especially with the high\ndimensionality of video data. Hsieh et al. [22] addressed this\nby dividing frames into patches and predicting their evolution\nover time using a recurrent convolutional neural network\n(rCNN)\n[23]. Jun-Ting et al. [24] proposed the decompo-\nsitional disentangled predictive autoencoder (DDPAE) frame-\nwork, automatically breaking down high-dimensional videos\ninto components with low-dimensional temporal dynamics.\nXue et al. [25] proposed a variational autoencoder (VAE) [26]\nmodel to generate a distribution of next frame predictions.\nOliu et al.\n[27] utilized a folded recurrent neural network\n(fRNN) with a gated recurrent unit (GRU) for bidirectional in-\nformation flow, enabling state sharing between the encoder and\ndecoder. Variational 3D ConvLSTM (V-3D-ConvLSTM) [28]\ncombined variational encoder-decoder and 3D-ConvLSTM\ntechniques. [29] developed thr video prediction Transformer\n(VPTR), an attention-based encoder-decoder, to learn local\nspatiotemporal representations while simplifying the model.\nCompared to the aforementioned methods, S-HR-VQVAE\ncan effectively manage the high dimensionality of video data,\nleveraging the hierarchical structure inside the vector quanti-\nzation module, which efficiently compresses each video frame,\nas demonstrated in the experimental section.\nC. Blurry Predictions\nAs reported in [30], video prediction solutions quite often\nrely on RNNs, VAEs, and their variants (e.g., variational\nRNNs - VRNNs [31]) resulting in blurry predictions. Two\nmain strategies have emerged to address this issue: (i) Latent\nvariable methods that explicitly model underlying stochasticity\nand (ii) Adversarially-trained models that aim to produce\nmore natural images. In [32], the authors instead aimed to\ninvestigate stochastic models for video prediction using the\nVAE framework. Given the recent advances in generative\nadversarial networks (GANs), researchers have also explored\nalternative techniques, such as VAE-GANs [30], [33] for\nvideo frame prediction. VAE-GANs allow capturing stochastic\nposterior distributions of videos while making it feasible to\nmodel the spatiotemporal joint distribution of pixels. However,\nsuch methods often suffer from the problem of mode collapse\nand unrealistic predictions [33], [34].\nS-HR-VQVAE combats image blurring thanks to the tempo-\nral model leveraging the hierarchical codebook representation.\n\n3\nThis allows for an increase in the quantization granularity\nwithout resulting in blurry images. In fact, despite the lossy\nnature of the compressed encoding, our experiments clearly\ndemonstrate that the original video can be reconstructed with\na high degree of fidelity through the latent representations.\nD. Physical Characteristics\nTo leverage physical characteristics, some methods focus\non pixel-level representations. For example, De Brabandere\net al.\n[35] introduced the dynamic filter network (DFN),\nwhich learns local spatial transformations from flow informa-\ntion. Finn et al. [36] proposed convolutional dynamic neural\nadvection (CDNA), a model that predicts object motion and\npixel motion distributions from previous frames. In another\napproach\n[37], a system was developed to predict optical\nflows between future and past frames. Berg et al.\n[38]\nutilized backward content transformation via a 6-parameter\naffine model to learn future-to-past relationships. Villegas et\nal. [10]] employed LSTM to independently model pixel-level\nimages for spatial layout and temporal dynamics, simplifying\nprediction tasks. Guen et al. introduced the Physical dynam-\nics network (PhyDNet) [39] to separate physical dynamics\nfrom other factors, yielding notable improvements. A motion-\nbased modeling technique (MotionRNN) [40] decomposes\nmotions into transient variations and trends, utilizing RNN-\nbased models like ConvLSTM, PredRNN, and E3D-LSTM\nfor prediction. Lee et al. [41] proposed a long-term motion\ncontext memory (LMC-memory) model for considering long-\nterm motion context in future frame prediction. However, these\nmethods primarily address physical characteristics, overlook-\ning challenges like high dimensionality, blurry predictions, and\nspatiotemporal modeling in video prediction.\nS-HR-VQVAE does not explicitly model physical char-\nacteristics. Nonetheless, the modularity of the hierarchical\nvector quantization block allows S-HR-VQVAE to implicitly\nmodel physical characteristics. In fact, latent representations\nare decomposed into a hierarchy of discrete codes, separating\nhigh-level global information (e.g., static background) from\ndetails (e.g., fine texture or small motions). Since the latent\nrepresentations are decomposed into different layers of hi-\nerarchical residual codes, the proposed AST-PM can exploit\nspatiotemporal dependencies that are different for different\nlevels of detail. For example, the background evolves slowly\nin time; whereas, the foreground object may move quickly.\nSimilarly, within the foreground object, some details, such\nas hands and arms, may exhibit different movement patterns\ncompared to the body. In sum, the combination of HR-VQVAE\nand AST-PM allows the modeling of physical characteristics,\nimproving accuracy while reducing complexity.\nIII. THEORETICAL BACKGROUND\nVariational autoencoders (VAEs) and vector quantized VAEs\n(VQVAEs) have been used for many applications for their\ninherent representation capabilities [42], [43]. Focusing on\nimage processing applications, our primary focus, an input\nimage is represented as a tensor x ∈RHI ×WI ×DI of height\nHI, width WI and DI color channels. VQVAE first maps the\ninput image x to a continuous latent vector z ∈RH×W ×D\nthrough a non-linear encoder: z = E(x). Next, each element\nzhw ∈RD, with h ∈[1, H], and w ∈[1, W], in the continuous\nlatent vector z is quantized to the nearest codebook vector (i.e.,\na codeword) ek ∈RD, k ∈1, ..., m by\nQuantize(zhw) := ek where k = arg min\nj\n∥zhw −ej∥2. (1)\nThe quantized vectors corresponding to each element zhw\nare then recombined into the continuous representation e ∈\nRH×W ×D to form the input of the decoder that reconstructs\nthe input image using a transformation D(·). The loss function\nL(.) aims at minimizing the reconstruction error ∥x−D(e)∥2\nwhilst minimizing the quantization error ∥z −e∥2 as follows\nL(x, D(e)) = ∥x−D(e)∥2\n2+∥sg[z]−e∥2\n2+β∥sg[e]−z∥2\n2, (2)\nwhere sg(.) is a stop-gradient operator cutting gradient flow\nduring backpropagation, and β is a hyperparameter governing\nthe stability of encoder output latent vectors.\nIn [44] a multi-layer version of VQVAE was proposed.\nHowever, the representations at different levels in the archi-\ntecture were not related hierarchically.\nIV. PROPOSED METHOD\nIn [4], we introduced a truly hierarchical version of VQVAE\n(HR-VQVAE) that is one of the building blocks of the video\nprediction method proposed in this work. HR-VQVAE deals\nwith limitations in techniques such as VQVAE, e.g., code-\nbook collapse and non-locality in codewords’ indices. In HR-\nVQVAE, each layer captures residual information that is not\nproperly modeled by the preceding layers, and the codebooks\nat different layers are constrained by a strict hierarchy.\nFig. 1 shows the proposed framework. Given T input frames\n(x1, . . . , xT ) in a video, the goal is to predict the following\nS frames (xT +1, . . . , xT +S) in three steps. First, the input\nframes are encoded into a discrete latent representation using\nHR-VQVAE. Next, a novel autoregressive spatiotemporal pre-\ndictive model (AST-PM) is proposed to predict new discrete\nlatent variables of future frames based on the latent variables\nfor previous frames. Finally, the HR-VQVAE decoder is used\nto generate the new frames from the latent variables obtained\nby AST-PM. The proposed approach is referred as sequen-\ntial hierarchical residual learning vector quantized variational\nautoencoder (S-HR-VQVAE).\nA. Step 1: Frame Encoding\nIn the first step, each frame x ∈RHI×WI×DI is encoded\nusing HR-VQVAE into a discrete latent representation. HR-\nVQVAE first encodes the frame into a continuous vector\nz = E(x) ∈RH×W ×D. These vectors are then iteratively\nquantized into n hierarchical layers of discrete latent embed-\ndings. Assuming that the first layer has a single codebook of\nsize M, the second layer has M independent codebooks of size\nM (for a total of M 2 codewords), and so on. A generic layer\ni has thereby M i−1 codebooks of size M, for a total of M i\ncodewords. However, only one of those codebooks is used in\neach layer depending on which codewords were chosen in the\n\n4\nDecoder\nEncoder\nVQ-Encoder\nQuintized codes\n𝐄(𝒙𝒊) = 𝒛𝒊\n𝐇× 𝐖× 𝐃\n𝑸𝒊\n𝟏\n𝑸𝒊\n𝒎\n𝑸𝒊\n𝟐\n𝐇× 𝐖\n 𝒛𝒊\n𝐇× 𝐖× 𝐃\nVQ-Decoder\n 𝒙𝒊\n𝒙𝒊\n…\n𝐇× 𝐖× 𝐃\nVQ-Decoder\n 𝒛𝐭+𝟏\n 𝒙𝐭+𝟏\n𝑸𝒕\n𝟏\n𝑸𝟏\n𝟏\n𝑸𝒕\n𝟐\n𝑸𝟏\n𝟐\n𝑸𝒕\n𝒎\n𝑸𝟏\n𝒎\n𝑸𝐭+𝟏\n𝟏\n𝑸𝐭+𝟏\n𝟐\n𝑸𝐭+𝟏\n𝒎\nPredicted \nQuantized \nCodes\n…\n(Predicted frame)\n𝐄𝒙𝒕= 𝒛𝒕\nEncoder\n𝐇× 𝐖× 𝐃\nVQ-Encoder\n𝐇× 𝐖\n𝑸𝒕\n𝟏\n𝑸𝒕\n𝒎\n𝑸𝒕\n𝟐\nQuintized codes\n𝐄(𝒙𝟏) = 𝒛𝟏\n𝐇× 𝐖× 𝐃\nVQ-Encoder\n𝑸𝟏\n𝟏\n𝑸𝟏\n𝒎\n𝑸𝟏\n𝟐\n𝐇× 𝐖\n…\n𝒙𝟐\n𝒙𝒕\n…\nDecoder\nAST-PM\n𝒙𝟏\nHR-VQVAE \nS-HR-VQVAE\n𝐞𝐂\nFig. 1: Top: The HR-VQVAE module for hierarchical vector quantization, where each frame i is encoded into m hierarchical\nlayers of quantized values\n\u0000Q1\ni , . . . , Qm\ni\n\u0001\n. Bottom: Illustration of the S-HR-VQVAE for video prediction, which combines HR-\nVQVAE with the AST-PM model. AST-PM predicts the indices of quantized values in both spatial and temporal dimensions,\nwhere each index at time t + 1 is predicted by accessing only its preceding indices—those located above and to the left in a\nraster-scan spatial order and those before t + 1 in the temporal domain.\nprevious layers. In each layer i, the codebook is optimized to\nminimize the error between codewords ei\nk ∈RD and elements\nξi−1\nhw ∈RD of the residual error from the previous layer1\nQuantizei(ξi−1\nhw ) := ei\nk where k = arg min\nj ∥ξi−1\nhw −ei\nj∥2, (3)\nand ei\nk belongs to one of the possible codebooks Ci(t)\nfor layer i. Which codebook is used is determined by the\ncodeword ei−1\nt\nselected at the previous layer. Within each\nlayer, the codewords ei\nk, for each element ξi−1\nhw of the residual,\nare combined to form the tensor ei ∈RH×W ×D. Across the\ndifferent layers, the tensors ei are then summed to form the\n“combined” discrete representation eC. When HR-VQVAE is\nused to reconstruct single images, eC is fed into the decoder\nto reconstruct the image as ˆx = D(eC), and the corresponding\nobjective function is used to train the system\nL(x, D(eC)) = ∥x −D(eC)∥2\n2 + ∥sg[ξ0] −eC∥2\n2\n+ β0∥sg[eC] −ξ0∥2\n2 +\nn\nX\ni=1\nL(ξi−1, ei),\n(4)\nwith\nL(ξi−1, ei) = ∥sg[ξi−1] −ei∥2\n2 + βi∥sg[ei] −ξi−1∥2\n2.\n(5)\nThe βi are hyperparameters that control the reluctance to\nchange the code corresponding to the encoder output. The\n1For the first layer, ξ0\nhw ≡zhw.\nmain goal of Eqs. 4 and 5 is to make a hierarchical mapping of\ninput data in which each layer of quantization extracts residual\ninformation from its bottom layers.\nIn the proposed S-HR-VQVAE, we do not reconstruct\nimages directly. The indices to the codewords ei are, instead,\nused as latent representations for each input frame in the\nvideo and each layer in the system and are input to the\nvideo prediction steps described below. We call these indices\nfor layer i, Qi ∈[1, M]H×W , with M the size of each\ncodebook. The difference is clarified in Figure 1, where the\nimage reconstruction case is represented in the top panel, and\nthe video prediction is depicted in the bottom panel.\nB. Step 2: Spatiotemporal Latent Representation Prediction\nIn the second step, the indices (Qi\n1, . . . , Qi\nT ) of the code-\nwords (ei\n1, . . . , ei\nT ), obtained from each layer i of HR-VQVAE\nfrom the input frames (x1, . . . , xT ), are used to predict the in-\ndices (Qi\nT +1, . . . , Qi\nT +S) of the codewords (ei\nT +1, . . . , ei\nT +S)\nfor S future frames, with the goal of predicting the S next\nfuture frames (xT +1, . . . , xT +S).\nTo this end, we propose a probabilistic autoregressive\nspatiotemporal predictive model (AST-PM). AST-PM takes\ndiscrete indices of the latent representations as input and\npredicts future indices. Because HR-VQVAE is hierarchical, it\ngreatly simplifies predicting spatial and temporal information,\nwhich allows our model to focus on the most important\nparts of the frames in both space and time. Accordingly,\nthe model predicts future codeword indices ˆQt > T using\n\n5\nthe codeword indices from previous times (Qi\n1, . . . , Qi\nT ). To\nexplain our probabilistic model, we first arrange the elements\nof Qi\nt ∈[1, M]H×W from left to right and top to bottom using\na linear index vk ∈[1, HW]. We use the notation vj < k to\nrefer to any element of Qi\nt that is to the left or above vk. Given\nthis notation, the probabilistic model can be written as:\np( ˆQi\nt+1(vk)) =\nH×W\nY\nj=1\np( ˆQi\nt+1(vj<k)|Qi\n1(vj<k), . . . , Qi\nt(vj<k)),\n(6)\nwhere ˆQi\nt represents the predicted quantized discrete codes of\nlayer i obtained from the tth frame. This behavior is achieved\nby using convolutional masks to limit the information used\nduring prediction. The convolutional masks restrict the convo-\nlutions to retrieve only spatial information from the left and\nabove each index. For the temporal dimension, convolutions\nare limited to previous time steps by masking out present and\nfuture timesteps. This strategy is implemented using multi-\nhead attention layers similar to those in [44]. However, in our\ncase, the attention is applied to 3D voxels. The loss function\nof AST-PM is as follows\nLp(p(Qi\nt>T ), p( ˆQi\nt>T )) =\n−\n1\nH × W\nH×W\nX\nj=1\nM\nX\nm=1\np(Qi\nt>T )[j, m] ∗log p( ˆQi\nt>T )[j, m],\n(7)\nC. Step 3: Frame Generation\nOnce the quantization indices\nˆQi\nt for each layer i and\neach time step t ∈[T + 1, T + S] have been estimated\nby the AST-PM, the corresponding quantized representation\nˆzt ∈RH×W ×D can be computed by codebook access eC(t) =\nm\nP\ni=1\nei\nt (see Section IV-A).\nFinally, the predicted quantized codes are decoded to se-\nquences of frames using the HR-VQVAE decoder D(.)\n(ˆxT +1, ..., ˆxT +S) = (D(ˆzT +1), ..., D(ˆzT +S)),\n(8)\nwhere the and ˆzt>T and ˆxt>T represent the predicted latent\nrepresentations and frames, respectively.\nD. Disjoint and Joint Training\nHR-VQVAE and AST-PM in the combined model described\nabove can be trained independently. In this case, we first train\nHR-VQVAE according to Eq. 4 to predict each frame xi in\nthe video independently of the others. We obtain a sequence\nof latent representations (Qi\n1, . . . , Qi\nT , Qi\nT +1, . . . , Qi\nT +S) for\neach layer in HR-VQVAE and for the complete sequence\nof frames. The AST-PM can the been trained to predict\nthe sequence (Qi\nT +1, . . . , Qi\nT +S) given the input sequence\n(Qi\n1, . . . , Qi\nT ), by optimizing Eq. 7. In the test phase, we\nuse the predictions of AST-PM in combination with the HR-\nVQVAE decoder to predict unseen video frames, making sure\nthat the combined model only has access to (x1, . . . , xT ) when\npredicting (xT +1, . . . , xT +S).\nFollowing this training procedure, the decoder in HR-\nVQVAE is exclusively optimized to deal with the uncertainty\nintroduced by the encoder of HR-VQVAE, which means\nthat the decoder is optimized solely for reconstructing the\noriginal input frame. However, when we reconstruct the frames\n(xT +1, . . . , xT +S), we also need to deal with the uncertainty\nintroduced by the AST-PM predictions. In fact, the AST-PM\nuncertainty refers to the mismatch between the predicted latent\nspaces of future frames and the actual latent space of future\nframes. This indicates that the HR-VQVAE decoder block\nand the AST-PM block operate independently, without being\naware of the uncertainties introduced by the other block. In\nan attempt to address this issue, we propose to optimize the\nAST-PM and the HR-VQVAE decoder jointly. Therefore, we\nproposed a joint training in Eq. 9 which includes two distinct\nobjectives: the loss for the HR-VQVAE decoder, represented\nby the first term of Eq. 4, and the AST-PM loss in Eq. 7. The\ncorresponding multi-objective loss is:\nLjoint = Lp + λ∥xt −D(eC(t))∥2\n2,\n(9)\nwhere λ is a hyperparameter that controls the effect of\nthe reconstruction loss on the joint training. In this case,\nduring training, HR-VQVAE only produces the latent repre-\nsentations (Qi\n1, . . . , Qi\nT ) for the input frames (x1, . . . , xT ).\nThe latent representations (Qi\nT +1, . . . , Qi\nT +S) for the frames\n(xT +1, . . . , xT +S) are predicted by AST-PM and then used to\ntrain the HR-VQVAE decoder.\nV. EXPERIMENTS\nA. Datasets\nWe conducted experiments using four different challenging\ndatasets. Table I presents a summary of corresponding statis-\ntics, including the number of training samples (#Train), the\nnumber of test samples (#Test), image resolution represented\nas (H, W, C), input sequence length indicated as T, and\npredicted sequence length referred to as ˆT.\nThe KTH Human Action dataset [5] is a moving image\ndataset with a resolution of 160 × 120 pixels that contains six\ntypes of human actions, including walking, jogging, running,\nboxing, hand waving, and hand clapping. The dataset com-\nprises 25 human subjects performing actions in four different\nscenarios. For our experiments, we followed [15], resized the\nvideo frames down to 128×128, and split the dataset into two\nsubsets: (i) a training set, consisting of the first 16 subjects,\nand (ii) a test set, containing the remaining subjects.\nThe TrafficBJ is a collection of taxicab GPS data and\nmeteorological data recorded in Beijing [6]. Each frame in\nTrafficBJ has 32×32 pixels, including the traffic flow entering\nand leaving the same district. We normalized the data to [0, 1]\nand follow the experimental settings as [45].\nThe Human3.6M dataset [7] consists of 3.6 million sam-\nples capturing diverse human activities. Similar to previous\npapers [19], [20], [39], we focus on the “walking” scenario.\nThe Kitti dataset [8] was created through real traffic\nscenario collections by specially equipped vehicles, a joint\neffort by Germany’s Karlsruhe Institute of Technology and\nthe Toyota Institute of Technology in the United States. We\nemploy Kitti using three scenarios: road, city, and residential,\nresulting in 57 videos for a training set and 4 for a test set.\n\n6\nTABLE I: Dataset statistics. #Train and #Test indicate the\nnumber of samples for the training and test set, respectively.\nEach input sequence consists of T, and the output sequence\nconsists of ˆT frames with shape (H, W, C).\nDataset\n#Train\n#Test\n(H, W, C)\nTTT\nˆTˆTˆT\nTrafficBJ [6]\n19,627\n1,334\n(32, 32, 2)\n4\n4\nKTH [5]\n5,200\n3,167\n(128,128, 3)\n10\n20\nHuman3.6M [7]\n2,624\n1,135\n(128, 128, 3)\n4\n4\nKitti [8]\n40,783\n1,963\n(128, 128, 3)\n4, 5\n5\nTABLE II: Configuration details for S-HR-VQVAE.\nKTH & Human3.6M & Kitti\nTrafficBJ\nInput size\n128 × 128\n32 × 32\nBit rate\n8\n8\nLatent size\n32 × 32 × 8\n16 × 16 × 4\nQuantized size\n32 × 32\n16 × 16\n#Layers\n1\n3\n9\n1\n3\n6\nCodebook size 512\n8\n2\n64\n4\n2\n#Codewords\n512 {8, 64, 512} {2, 4,..., 512} 64 {4, 16, 64} {2, 4,..., 64}\nFinally, it is important to note that 5% of the training set\nwas reserved as a validation set, which was used specifically\nfor fine-tuning the hyperparameters.\nB. Experimental Setup\nTable II lists some details about S-HR-VQVAE architecture\nfor tackling the datasets. Input size refers to the initial res-\nolution of the video frames. Latent size corresponds to the\ncontinuous latent representation in HR-VQVAE. Quantized\nlatent size to the quantized representation in the model. We\nalso provide additional information for the bit rate, number of\nhierarchy layers, codebook size, and number of codewords.\nThe proposed S-HR-VQVAE was trained on sequences\nconsisting of 10 consecutive frames to predict 20 future frames\nfor KTH Human Action, and also trained on 4 consecutive\nframes to predict 4 future frames for both TrafficBJ and\nHuman3.6M datasets, which is a common practice for the\ntasks. In addition, for the Kitti dataset, we focused on two\nspecific settings: (i) 4 input frames and 5 predicted frames and\n(ii) 5 input frames and 5 predicted frames. In all experiments,\nthe model is trained using the Adam optimizer [46], and the\nlearning rate is set to 0.0003 for both HR-VQVAE encoder-\ndecoder and AST-PM. Besides, λ in Eq. 9 is set to 0.11.\nC. Metrics\nWe report results adopting metrics that are commonly\nused in the literature, namely: peak signal-to-noise ratio\n(PSNR) [47], structural similarity index measure (SSIM) [48],\nlearned perceptual image patch similarity (LPIPS) [49], frechet\nvideo distance (FVD) [50], mean square error (MSE), and\nmean absolute error (MAE). PSNR, SSIM, LPIPS, MSE,\nand MAE are all image quality metrics but differ in their\ncharacteristics. PSNR focuses on signal-to-noise ratio, SSIM\nconsiders structural similarity, MSE and MAE measure pixel-\nwise differences, and LPIPS aims to capture perceptual sim-\nilarity based on deep neural networks. FVD, on the other\nhand, is a comprehensive video quality metric employed to\nassess the quality of generated videos. This evaluation is\nachieved by quantifying the feature distribution gap between\nreal and generated videos, which effectively captures both\ntemporal inconsistencies and motion-related artifacts. Further-\nmore, FVD evaluates both the temporal coherence of video\ncontent and the quality of individual frames, offering a holistic\nperspective on video realism and overall coherence. All those\nmetrics, however, have limitations. For example, PSNR, MSE,\nand MAE have been shown to have poor correlation with\nhuman perception [51], [52] and may not take into account\nhigher-level semantic information, such as in action modeling.\nSSIM and LPIPS are more effective in capturing perceptual\ndifferences, but they may not be sensitive to all types of\nvisual information: they may not be as effective at capturing\ndifferences in color or texture as at capturing differences in\nluminance and contrast [53]. fFVD tends to prioritize a video’s\nspatial elements and may overlook the natural flow of its\ntemporal dynamics [54]. Therefore, several metrics must be\nconsidered to better capture different aspects of the video\nprediction task and obtain a more comprehensive assessment\nof the methods’ performance.\nWe report results according to all those metrics and include\nall available results for the related methods. Because of the\nlimitations of these metrics, we also provide a qualitative\nassessment to verify whether the metrics have missed some\nimportant aspects of the video prediction task.\nVI. RESULTS\nResults of the quantitative evaluation of the proposed\nmethod followed by a qualitative assessment are now pre-\nsented. To better appreciate the effectiveness of the proposed\ntechnique, we have performed a systematic review of reported\nquantitative results of recent, state-of-the-art solutions.\nThe qualitative analysis is performed by observing the\nbehavior of the proposed method on several video sequences,\nwhich is a common practice in the research field. However,\nwhile reviewing the literature, we noticed that different meth-\nods use different video sequences to visually demonstrate the\nquality of their approaches; furthermore, the source code is\nnot available for all methods in the literature, which implies\nthat different systems can not be compared on the same set of\npredefined video sequences. To overcome that issue, we first\nselected video sequences common among different techniques\nin the literature. Then, we evaluated our S-HR-VQVAE on\nthose selected examples and grouped the results accordingly.\nTo the best of our knowledge, this is the first time that such\na systematic comparison has been carried out.\nWe also provide results for other aspects of the proposed\nS-HR-VQVAE, including reconstruction capability in (i) blur\nmitigation, (ii) noise removal, and (iii) compression.\nA. Quantitative Analysis\nIn this study, we assess the performance of state-of-the-art\nvideo prediction methods on different datasets, providing a\ncomprehensive overview of the advancements in the field. In\nparticular, Tables III, IV and V list state-of-the-art methods\nfrom 2015 to 2023 in a chronologically ascending order, high-\nlighting thereby the evolution of the techniques over the years.\n\n7\nTABLE III: Results on KTH Human Action dataset. S-HR-\nVQVAE with 3 layers was used with disjoint and joint training.\nKTH Human Action (10 →20)\nMethod\nPSNR↑SSIM↑LPIPS↓\n#Params\nFLOPs\nConvLSTM (2015) [12]\n23.01\n0.704\n0.156\n16.60M\n1,468G\nDFN (2016) [35]\n27.26\n0.794\n×\n×\n×\nCDNA (2016) [36]\n23.75\n0.752\n×\n×\n×\nDrNet(2017) [9]\n25.56\n0.764\n×\n23.30M\n×\nPredRNN (2017) [13]\n27.55\n0.839\n0.167\n23.85M\n2,800G\nMcNet (2018) [10]\n25.95\n0.804\n×\n3.50M\n×\nMsNet (2018) [11]\n27.08\n0.876\n×\n3.20M\n×\nfRNN (2018) [27]\n26.12\n0.771\n×\n×\n×\nPredRNN++ (2018) [14]\n28.62\n0.888\n0.229\n15.40M\n4,162G\nE3D-LSTM (2019) [16]\n27.92\n0.893\n×\n41.94M\n214.0G\nMIM (2019) [19]\n27.78\n0.902\n0.188\n37.37M\n1,099G\nConv-TT-LSTM (2020) [17]\n28.36\n0.907\n0.133\n39.8M\n×\nPhyDNet (2020) [39]\n28.69\n×\n0.188\n3.10M\n93.6G\nJin et al. (2020) [55]\n29.85\n0.893\n0.118\n×\n×\nLMC-Memory (2021) [41]\n28.61\n0.894\n0.133\n×\n×\nV-3D-ConvLSTM (2021) [28]\n28.31\n0.866\n×\n12.90M\n×\nR-ST-ConvLSTM (2022) [18]\n28.99\n0.854\n×\n×\n×\nSimVP (2022) [20]\n33.72\n0.905\n×\n22.30M\n125.6G\nPredRNN-V2 (2023) [15]\n28.37\n0.838\n0.139\n23.86M\n2,815G\nVPTR (2023) [29]\n26.96\n0.879\n0.076\n162.48M\n×\nNPVP (2023) [56]∗\n27.66\n0.909\n0.066\n×\n×\nS-HR-VQVAE-disjoint (ours)\n28.43\n0.863\n0.130\n1.14M\n94.1G\nS-HR-VQVAE-joint (ours)\n28.49\n0.910\n0.093\n1.14M\n95.8G\n∗NPVP resized KTH samples to 64 × 64 instead of standard 128 × 128.\n(↑) means higher is better and (↓) means lower is better.\nOn the KTH Human Action task, PSNR and SSIM are reported\nby all competing techniques; whereas, LPIPS is provided for\nonly a few methods. On TrafficBJ and Human3.6M tasks, we\nreport MSE, MAE, and SSIM as in [19], [20], [39]. Finally,\non the Kitti dataset, we report SSIM, LPIPS, FVD, and PSNR.\nHere we report our results in order of complexity of the task\n(from KTH Human action to Kitti).\nFor the KTH Human Action task, from Table III, it is\nevident that the proposed S-HR-VQVAE outperforms all meth-\nods, up to fRNN, in all reported metrics. Among methods in-\ntroduced after fRNN, S-HR-VQVAE outperforms PredRNN++\non two metrics out of three, E3D-LSTM on all, Conv-TT-\nLSTM on all, PhyDNet on one out of two, Jin et al. [55] on\ntwo out of three, LMC-Memory on two out of three, V-3D-\nConvLSTM across all, R-ST-ConvLSTM on one out of two,\nSimVP on one out of two, PredRNN-V2 on all, VPTR on two\nout of three, and NPVP on two out of three. It can also be seen\nfrom Table III that SimVP has the overall best PSNR, but our\nmethod outperforms it and achieves the best result in terms of\nSSIM. For LPIPS, NPVP has the best performance; however,\nthe method is outperformed by our method both in terms of\nPSNR and SSIM, despite NPVP downsampling video frames\nto 64×64 rather than the typical 128×128. It is noteworthy that\nS-HR-VQVAE achieves those results with a significantly lower\nnumber of parameters with respect to all other methods and\nranks second in terms of computational efficiency (FLOPs).\nOn the TrafficBJ task, as detailed in Table IV, S-HR-\nVQVAE exhibits exceptional performance, outperforming ex-\nisting state-of-the-art methods on all evaluation metrics. Our\nmodel particularly stands out by significantly outperforming\nmethods such as PhyDNet and SimVP, achieving the highest\nscores across all metrics. A similar trend is observed on the\nchallenging task of Human3.6M, where S-HR-VQVAE again\noutperforms the current state-of-the-art approaches, leading\nin all evaluation metrics, especially in FVD (better temporal\nmodeling) and FLOPs (computational efficiency).\nThe performance of the S-HR-VQVAE on the challenging\ntask of the Kitti dataset is detailed in Table V. Unlike\nother tasks such as KTH Human Action, TrafficBJ, and\nHuman3.6M, where the background is static, the Kitti dataset\nintroduces a unique challenge with its dynamic and complex\nenvironments. This complexity comes from the challenging\ndriving scenes, where both the foreground and background\nare in motion. This requires the prediction model to accu-\nrately handle multiple moving elements and rapidly changing\nlandscapes, a significant shift from tasks where movement is\nmainly due to a single object against a constant background.\nFor the Kitti (4 →5) task, S-HR-VQVAE has demonstrated\nremarkable improvement over traditional models like Pre-\ndRNN and McNet and more recent approaches such as NPVP.\nIt not only obtains better performance in SSIM, showing\nthe best perceptual quality of predictions but also achieves\nthe lowest LPIPS and a significantly better FVD, indicating\nsuperior performance in capturing both spatial and temporal\naspects of the scenes. Similarly, for the Kitti (5 →5) task,\nS-HR-VQVAE significantly outperforms other state-of-the-art\nmodels such as PhyDNet, LMC-Memory, MotionRNN, and\nMIMO. It achieves higher SSIM and PSNR values, which\nindicates that it not only captures higher structural similarities\nbetween the predicted and actual frames but also maintains\nhigh-quality predictions across various frames. The lower\nLPIPS also shows further evidence of S-HR-VQVAE’s capa-\nbility to preserve more accurate textural and detail-oriented\nfeatures that are critical in dynamic scenes.\nReferring to Tables III, IV and V, we can observe that the\ndifferent metrics improve over the years. Also, it can be argued\nthat starting from 2018, all methods are quite competitive\nwith one another, and it is not possible to indicate a single\ntechnique that performs the best on the video prediction task\nacross all metrics. Indeed, when we attempt to determine the\nbest method, it can be seen that methods performing best\nin one metric are usually outperformed by other methods\nin other metrics, and therefore, it is essential to evaluate\nthe results using all available metrics. From this analysis,\nwe can conclude that although some of the state-of-the-art\nmethods outperform our method on a single metric, S-HR-\nVQVAE is more robust across all metrics for the considered\ntasks, especially for the challenging tasks of Human3.6M and\nKitti, where S-HR-VQVAE outperforms the state-of-the-art\nmethods across all metrics. Ultimately, Tables III, IV, and V\nshow the positive impact of jointly training HR-VQVAE and\nAST-PM across all datasets. While joint training introduces a\nslight increase in FLOPs compared to disjoint training, this\nmarginal rise is outweighed by the significant benefits of joint\ntraining, particularly in improving spatiotemporal modeling,\nwhich contributes to the overall performance of the model.\nThe effectiveness of our approach can be further appreciated\nby considering the following qualitative analysis since objec-\ntive metrics might not capture all aspects of the actual quality\nof the predicted sequences.\n\n8\nTABLE IV: Results on TrafficBJ and Human 3.6M. S-HR-VQVAE with 3 layers was used with disjoint and joint training.\nTrafficBJ (4 →4)\nHuman3.6M (4 →4)\nMethod\nMSE×\n×\n×100↓\nMAE↓\nSSIM↑\nFLOPs\nMSE/10↓\nMAE/100↓\nSSIM↑\nFVD↓\nFLOPs\nConvLSTM (2015) [12]\n48.5\n17.7\n0.978\n20.74G\n50.4\n18.9\n0.776\n28.4\n347.0G\nPredRNN (2017) [13]\n46.4\n17.1\n0.971\n42.40G\n48.4\n18.9\n0.781\n24.7\n704.0G\nPredRNN++ (2018) [14]\n44.8\n16.9\n0.977\n62.95G\n×\n×\n×\n×\n1,033G\nE3D-LSTM (2019) [16]\n43.2\n16.9\n0.979\n98.19G\n46.4\n16.6\n0.869\n23.7\n542.0G\nMIM (2019) [19]\n42.9\n16.6\n0.971\n64.10G\n42.9\n17.8\n0.790\n21.8\n1,051G\nPhyDNet (2020) [39]\n41.9\n16.2\n0.982\n5.60G\n36.9\n16.2\n0.901\n18.3\n19.1G\nMotionRNN (2021) [40]\n×\n×\n×\n×\n34.2\n14.8\n0.846\n18.3\n49.5G\nSimVP (2022) [20]\n41.4\n16.2\n0.982\n3.61G\n31.6\n15.1\n0.904\n×\n197.0G\nPredRNN-V2 (2023) [15]\n45.6\n16.8\n0.980\n42.63G\n36.3\n17.7\n0.863\n×\n708.0G\nS-HR-VQVAE-disjoint (ours)\n41.5\n16.2\n0.985\n3.11G\n30.9\n14.4\n0.916\n16.5\n16.7G\nS-HR-VQVAE-joint (ours)\n40.3\n15.2\n0.993\n4.07G\n30.4\n12.4\n0.939\n15.2\n17.4G\n(↑) means higher is better and (↓) means lower is better.\n(a) Walking.\n(b) Jogging.\n(c) Handwaving.\n8      10      12      14      16     18      20      22      24     26      28     30    \nConvLSTM\nDFN\nPredRNN\nPredRNN++\nfRNN\nMcNet\nConv-TT-LSTM\nPredRNN-V2\nS-HR-VQVAE-disjoint\nS-HR-VQVAE-joint\nConvLSTM\nPredRNN\nR-ST-ConvLSTM\nPredRNN-V2\nS-HR-VQVAE-disjoint\nS-HR-VQVAE-joint\nLMC-Memory\nVPTR\nS-HR-VQVAE-disjoint\nS-HR-VQVAE-joint\nInputs\nPrediction Ground Truth\nInputs\nPrediction Ground Truth\nInputs\nPrediction Ground Truth\nFig. 2: Comparison of S-HR-VQVAE with state-of-the-art methods on KTH Human Moving Action dataset over three sequences\n(a, b, and c) that are commonly reported in the literature. It should be noted that 10 frames (1-10 in the figures) are given as\ninput, and the next 20 frames (11-30 in the figures) are predicted.\nB. Qualitative Analysis\nFigure 2 shows the predictions for different state-of-the-\nart methods and S-HR-VQVAE on the KTH Human Action\ndataset for three different activities: walking (panel a), jogging\n(panel b), and handwaving (panel c). In the hand wave activity,\nfor example, hand movements are relatively fast, but S-HR-\nVQVAE can better predict the ground truth whilst avoiding\nblurry outputs, as shown in frames 28 and 30. In the walking\ntask, most methods do not predict well the position of the body\nand the legs, except for our method, PredRNN, PredRNN++,\nand PredRNN-V2 (see frames 27 and 30, for example).\nHowever, our method produces sharper images and correctly\npredicts the location of both legs for these frames. Finally, for\nthe jogging task, an overall better estimation of the location\nof the jogger is observed along with sharper images.\nFigure 3 presents a qualitative analysis of the results\nobtained for TrafficBJ samples. To enhance the clarity of\nour comparisons, we include visualizations of the differences\nbetween the predictions and the corresponding ground truth\nimages. S-HR-VQVAE demonstrates impressive performance\nin generating predicted frames when compared to the other\nmodels, as evidenced by the minimal intensity of differences\nobserved. It is noteworthy that S-HR-VQVAE obtains the best\nresult on all metrics for this task.\nThe qualitative analysis presented in Figure 4 reveals that\nS-HR-VQVAE generates more precise predictions for motion\npositions and object sizes. This observation underscores the\nefficacy of S-HR-VQVAE when applied to intricate real-world\n\n9\nMIM\nPhyDNet\nS-HR-VQVAE (joint)\nPredRNN\nInputs\nPrediction Ground Truth\nFig. 3: Comparison of S-HR-VQVAE with state-of-the-art-\nmethods on TrafficBJ dataset. It should be noted that 4 frames\n(1-4 in the figure) are given as input, and the next 4 frames\n(5-8 in the figure) are predicted.\nConvLSTM\nMIM\nE3D-LSTM\nPredRNN\nPredRNN\n+MotionRNN\nS-HR-VQVAE\n(joint)\nInputs\nPrediction Ground Truth\nFig. 4: Comparison of S-HR-VQVAE with state-of-the-art-\nmethods on Human3.6M dataset. 4 frames (1-4 in the figure)\nare given as input, and the next 4 frames (5-8 in the figure)\nare predicted.\nTABLE V: Results on Kitti dataset. S-HR-VQVAE with 3\nlayers was used with disjoint and joint training.\nKitti (4 →5)\nMethod\nSSIM↑\nLPIPS↓\nFVD↓\nPredRNN (2017) [13]\n0.475\n0.629\n×\nMcNet (2018) [10]\n0.554\n0.373\n×\nNPVP (2023) [56]\n0.661\n0.279\n134.69\nS-HR-VQVAE-disjoint (ours)\n0.673\n0.188\n127.04\nS-HR-VQVAE-joint (ours)\n0.692\n0.164\n121.84\nKitti (5 →5)\nSSIM↑\nLPIPS↓\nPSNR↑\nPhyDNet (2020) [39]\n0.674\n0.403\n19.159\nLMC-Memory (2021) [41]\n0.660\n0.410\n18.692\nMotionRNN (2021) [40]\n0.652\n0.384\n18.931\nMIMO (2023) [57]\n0.703\n0.308\n19.616\nS-HR-VQVAE-disjoint (ours)\n0.845\n0.187\n19.774\nS-HR-VQVAE-joint (ours)\n0.861\n0.114\n21.877\n(↑) means higher is better and (↓) means lower is better.\nt = 4          t = 5         t = 6         t = 7        t = 8       t = 9\nNPVP\nS-HR-VQVAE\n(joint)\n…\nInputs\nPrediction Ground Truth\nFig. 5: Comparison of S-HR-VQVAE with the state-of-the-art\nmethod on the Kitti dataset, where 4 frames are given as input,\nand the next 5 frames are predicted.\ndatasets. S-HR-VQVAE better performance in predicting ob-\nject positions and sizes can be attributed to the collaborative\ninteraction between our spatiotemporal predictive model and\nthe decoder, as stated in the objective function in Eq. 9.\nThe qualitative assessment on the Kitti dataset is depicted\nin Figure 5. From the visual analysis, it is evident that S-\nHR-VQVAE exhibits finer details, such as intricate shadow\npatterns, leaf textures on trees, and more precise car features,\ncompared to NPVP. Moreover, S-HR-VQVAE significantly\nreduced blurry predictions compared to NPVP. These obser-\nvations align well with the quantitative findings presented in\nTable V, where S-HR-VQVAE outperforms NPVP across all\nevaluation metrics: SSIM, LPIPS, and FVD. The higher SSIM\nscore of S-HR-VQVAE indicates better structural similarity\nbetween predicted and ground truth frames, while the lower\nLPIPS value suggests reduced perceptual differences, under-\nscoring the model’s ability to generate more visually faithful\npredictions. Furthermore, the significantly lower FVD score of\nS-HR-VQVAE compared to NPVP highlights its superiority in\ncapturing temporal consistencies and minimizing artifacts.\nWe can summarise the outcome of the qualitative analy-\nsis as follows: Although quantitative analysis is useful for\nunderstanding whether a sequence prediction technique is\nviable or not, objective measures by themselves may not\n\n10\nFig. 6: Heatmap of reconstructions obtained from different\nlayers of a 3-layer HR-VQVAE.\nreveal the actual capability of a technique. State-of-the-art\nmethods exhibit a varying sequence prediction quality across\ntasks, as observed, for example, in Figure 2 despite the\ngood numerical results reported in Table III, whereas S-\nHR-VQVAE performs consistently across tasks. Moreover,\nthe figure suggests joint training within our methodology\nleads to a significant enhancement in location prediction. This\nimprovement is evident across the majority of the frames.\nNevertheless, it is important to acknowledge that while this\nimprovement in location prediction is evident, it appears to be\naccompanied by a minor reduction in image sharpness in the\nreconstructed frames. This observation may provide insights\ninto the relatively modest quantitative improvements observed\nin our results following the incorporation of joint training.\nVII. DISCUSSION\nA. Model Interpretability\nTo facilitate the interpretation of latent representations pro-\nduced by the model, we present heatmaps over various layers\nof HR-VQVAE in Fig. 6. Each heatmap highlights regions\nof significance within the reconstructed latent representation.\nGeneral information, i.e., background, is mainly captured in\nthe first layer; the second layer focuses on the position of the\nforeground object, whereas the third layer is concerned with\ndetails of the moving objects.\nB. Blur, Noise, and Compression\nTo gain more insights into the effectiveness of S-HR-\nVQVAE against blurriness, we artificially corrupt some video\nsequences by injecting Gaussian Blur (Fig. 7-a) and Fragment\nBlur (Fig. 7-b). The prediction results reported in those figures\ndemonstrate that HR-VQVAE can successfully reduce blurri-\nness while being able to reconstruct details in the images that\nwere lost due to the blur effect. In addition to blur mitigation,\nHR-VQVAE is also robust to noise, as shown in Fig. 7-c,\nwhere accurate sequence prediction is attained although the\ninput frames were artificially corrupted with additive noise\nat different SNR levels. Finally, we show the reconstruction\nof compressed images with two levels of compression ratio\nin Fig. 7-d, showcasing the HR-VQVAE’s robustness against\ncompression. HR-VQVAE robustness against blur, noise, and\ncompression in sequence prediction is especially valuable in\nFig. 7: Reconstructions by 3-layer HR-VQVAE. a) Gaussian\nBlur b) Fragment Blur c) Noise d) Compression. Zoom in to\nsee more details.\napplications where the quality of the predicted video frames\nis critical, such as autonomous driving.\nVIII. CONCLUSION\nIn this study, we proposed a video prediction framework\nthat combines the hierarchical vector quantization codebooks\nof the previously proposed HR-VQVAE with the novel au-\ntoregressive spatiotemporal predictive model (AST-PM). We\ncall this method sequential HR-VQVAE (S-HR-VQVAE). We\nshow how the proposed S-HR-VQVAE takes advantage of\nhierarchical frame modeling to model different levels of ab-\nstraction, enabling the system to capture both context and\nmovements (details) in video frames with a fraction of the\nparameters used by competing models. We show by extensive\nexperimental evidence on the KTH Human Action, TrafficBJ,\nHuman3.6M, and Kitti tasks that the model is very competitive\nwith the state-of-the-art in video prediction, outperforming the\nbest methods, at least in a subset of the available metrics\n(PSNR, SSIM, LPIPS, FVD, MSE, and MAE) with signif-\nicantly lower number of parameters. We provide a detailed\nanalysis of the properties of the model, including an analysis of\nits internal representations and its behavior concerning blurry\nand noisy input frames. The proposed method is competitive\nfor the video prediction task, in terms of performance, low\ncomplexity, and interpretability.\nREFERENCES\n[1] V. Vukoti´c, S.-L. Pintea, C. Raymond, G. Gravier, and J. C. Van Gemert,\n“One-step time-dependent future video frame prediction with a convolu-\ntional encoder-decoder neural network,” in International conference on\nimage analysis and processing.\nSpringer, 2017, pp. 140–151.\n[2] C. Lu, M. Hirsch, and B. Sch¨olkopf, “Flexible spatio-temporal networks\nfor video prediction,” in 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017.\n[3] R. Bellman, Adaptive Control Processes: A Guided Tour.\nPrinceton\nUniversity Press, 1961.\n\n11\n[4] M. Adiban, K. Stefanov, S. M. Siniscalchi, and G. Salvi, “Hierarchical\nresidual learning based vector quantized variational autoencoder for\nimage reconstruction and generation,” in 33rd British Machine Vision\nConference 2022, BMVC 2022, London, UK, November 21-24, 2022.\nBMVA Press, 2022.\n[5] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions:\na local SVM approach,” in Proceedings of the 17th International\nConference on Pattern Recognition, 2004. ICPR 2004., vol. 3.\nIEEE,\n2004, pp. 32–36.\n[6] J. Zhang, Y. Zheng, D. Qi, R. Li, X. Yi, and T. Li, “Predicting citywide\ncrowd flows using deep spatio-temporal residual networks,” Artificial\nIntelligence, vol. 259, pp. 147–166, 2018.\n[7] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3. 6m:\nLarge scale datasets and predictive methods for 3d human sensing\nin natural environments,” IEEE transactions on pattern analysis and\nmachine intelligence, vol. 36, no. 7, pp. 1325–1339, 2013.\n[8] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\nThe kitti dataset,” The International Journal of Robotics Research,\nvol. 32, no. 11, pp. 1231–1237, 2013.\n[9] E. Denton and V. Birodkar, “Unsupervised learning of disentangled\nrepresentations from video,” in Proceedings of the 31st International\nConference on Neural Information Processing Systems, 2017, pp. 4417–\n4426.\n[10] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee, “Decomposing motion\nand content for natural video sequence prediction,” in 5th International\nConference on Learning Representations, ICLR 2017.\nInternational\nConference on Learning Representations, ICLR, 2017.\n[11] J. Lee, J. Lee, S. Lee, and S. Yoon, “Mutual suppression network\nfor video prediction using disentangled features,” in British Machine\nVision Conference, 2018.\n[12] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c.\nWoo, “Convolutional lstm network: A machine learning approach for\nprecipitation nowcasting,” Advances in neural information processing\nsystems, vol. 28, 2015.\n[13] Y. Wang, M. Long, J. Wang, Z. Gao, and P. S. Yu, “PredRNN: Recurrent\nneural networks for predictive learning using spatiotemporal lstms,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[14] Y. Wang, Z. Gao, M. Long, J. Wang, and S. Y. Philip, “PredRNN++:\nTowards a resolution of the deep-in-time dilemma in spatiotemporal\npredictive learning,” in International Conference on Machine Learning.\nPMLR, 2018, pp. 5123–5132.\n[15] Y. Wang, H. Wu, J. Zhang, Z. Gao, J. Wang, S. Y. Philip, and M. Long,\n“Predrnn: A recurrent neural network for spatiotemporal predictive\nlearning,” IEEE Transactions on Pattern Analysis and Machine Intel-\nligence, vol. 45, no. 2, pp. 2208–2225, 2023.\n[16] Y. Wang, L. Jiang, M.-H. Yang, L.-J. Li, M. Long, and L. Fei-\nFei, “Eidetic 3d lstm: A model for video prediction and beyond,” in\nInternational conference on learning representations, 2019.\n[17] J. Su, W. Byeon, J. Kossaifi, F. Huang, J. Kautz, and A. Anandkumar,\n“Convolutional tensor-train lstm for spatio-temporal learning,” Advances\nin Neural Information Processing Systems, vol. 33, pp. 13 714–13 726,\n2020.\n[18] W. Saideni, D. Helbert, F. Courreges, and J. P. Cances, “A novel video\nprediction algorithm based on robust spatiotemporal convolutional long\nshort-term memory (robust-st-convlstm),” in Proceedings of Seventh\nInternational Congress on Information and Communication Technology:\nICICT 2022, London, Volume 2.\nSpringer, 2022, pp. 193–204.\n[19] Y. Wang, J. Zhang, H. Zhu, M. Long, J. Wang, and P. S. Yu, “Memory\nin memory: A predictive neural network for learning higher-order\nnon-stationarity from spatiotemporal dynamics,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, 2019,\npp. 9154–9162.\n[20] Z. Gao, C. Tan, L. Wu, and S. Z. Li, “SimVP: Simpler yet better video\nprediction,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 3170–3180.\n[21] Z. Chang, X. Zhang, S. Wang, S. Ma, and W. Gao, “Strpm: A spatiotem-\nporal residual predictive model for high-resolution video prediction,”\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2022, pp. 13 946–13 955.\n[22] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert, and\nS. Chopra, “Video (language) modeling: a baseline for generative models\nof natural videos,” arXiv preprint arXiv:1412.6604, 2014.\n[23] C. Xu, P. Zhao, Y. Liu, J. Xu, V. S. S. S. Sheng, Z. Cui, X. Zhou,\nand H. Xiong, “Recurrent convolutional neural network for sequential\nrecommendation,” in The world wide web conference, 2019, pp. 3398–\n3404.\n[24] J.-T. Hsieh, B. Liu, D.-A. Huang, L. F. Fei-Fei, and J. C. Niebles,\n“Learning to decompose and disentangle representations for video\nprediction,” in Advances in Neural Information Processing Systems,\n2018, pp. 517–526.\n[25] T. Xue, J. Wu, K. Bouman, and B. Freeman, “Visual dynamics:\nProbabilistic future frame synthesis via cross convolutional networks,”\nin Advances in neural information processing systems, 2016, pp. 91–99.\n[26] D. P. Kingma and M. Welling, “Stochastic gradient vb and the varia-\ntional auto-encoder,” in Second International Conference on Learning\nRepresentations, ICLR, vol. 19, 2014.\n[27] M. Oliu, J. Selva, and S. Escalera, “Folded recurrent neural networks\nfor future video prediction,” in Proceedings of the European Conference\non Computer Vision (ECCV), 2018, pp. 716–731.\n[28] H. Razali and B. Fernando, “A log-likelihood regularized kl diver-\ngence for video prediction with a 3d convolutional variational recurrent\nnetwork,” in Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, 2021, pp. 209–217.\n[29] X. Ye and G.-A. Bilodeau, “Video prediction by efficient transformers,”\nImage and Vision Computing, vol. 130, p. 104612, 2023.\n[30] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther,\n“Autoencoding beyond pixels using a learned similarity metric,” in\nProceedings of The 33rd International Conference on Machine Learning,\nser. Proceedings of Machine Learning Research, 2016, pp. 1558–1566.\n[31] L. Castrejon, N. Ballas, and A. Courville, “Improved conditional vrnns\nfor video prediction,” in Proceedings of the IEEE International Confer-\nence on Computer Vision, 2019, pp. 7608–7617.\n[32] E. Denton and R. Fergus, “Stochastic video generation with a learned\nprior,” in International conference on machine learning.\nPMLR, 2018,\npp. 1174–1183.\n[33] A.\nX.\nLee,\nR.\nZhang,\nF.\nEbert,\nP.\nAbbeel,\nC.\nFinn,\nand\nS. Levine, “Stochastic adversarial video prediction,” arXiv preprint\narXiv:1804.01523, 2018.\n[34] M. Babaeizadeh, C. Finn, D. Erhan, R. Campbell, and S. Levine,\n“Stochastic variational video prediction,” in 6th International Confer-\nence on Learning Representations, ICLR 2018, 2018.\n[35] X. Jia, B. De Brabandere, T. Tuytelaars, and L. V. Gool, “Dynamic filter\nnetworks,” Advances in neural information processing systems, vol. 29,\n2016.\n[36] C. Finn, I. Goodfellow, and S. Levine, “Unsupervised learning for\nphysical interaction through video prediction,” in Advances in neural\ninformation processing systems, 2016, pp. 64–72.\n[37] Z. Liu, R. A. Yeh, X. Tang, Y. Liu, and A. Agarwala, “Video frame syn-\nthesis using deep voxel flow,” in Proceedings of the IEEE International\nConference on Computer Vision, 2017, pp. 4463–4471.\n[38] M. Jaderberg, K. Simonyan, A. Zisserman et al., “Spatial transformer\nnetworks,” in Advances in neural information processing systems, 2015,\npp. 2017–2025.\n[39] V. L. Guen and N. Thome, “Disentangling physical dynamics from\nunknown factors for unsupervised video prediction,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 11 474–11 484.\n[40] H. Wu, Z. Yao, J. Wang, and M. Long, “Motionrnn: A flexible model\nfor video prediction with spacetime-varying motions,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\n2021, pp. 15 435–15 444.\n[41] S. Lee, H. G. Kim, D. H. Choi, H.-I. Kim, and Y. M. Ro, “Video\nprediction recalling long-term motion context via memory alignment\nlearning,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 3054–3063.\n[42] C. Doersch, “Tutorial on variational autoencoders,” arXiv preprint\narXiv:1606.05908, 2016.\n[43] A. van den Oord, O. Vinyals et al., “Neural discrete representation\nlearning,” in Advances in Neural Information Processing Systems, 2017,\npp. 6306–6315.\n[44] A. Razavi, A. van den Oord, and O. Vinyals, “Generating diverse\nhigh-fidelity images with vq-vae-2,” in Advances in Neural Information\nProcessing Systems, 2019, pp. 14 837–14 847.\n[45] J. Zhang, Y. Zheng, and D. Qi, “Deep spatio-temporal residual networks\nfor citywide crowd flows prediction,” in Proceedings of the AAAI\nconference on artificial intelligence, vol. 31, no. 1, 2017.\n[46] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[47] S. Winkler and P. Mohandas, “The evolution of video quality measure-\nment: From psnr to hybrid metrics,” IEEE transactions on Broadcasting,\nvol. 54, no. 3, pp. 660–668, 2008.\n\n12\n[48] V. Sitzmann, M. Zollh¨ofer, and G. Wetzstein, “Scene representation\nnetworks: Continuous 3d-structure-aware neural scene representations,”\nAdvances in Neural Information Processing Systems, vol. 32, 2019.\n[49] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The\nunreasonable effectiveness of deep features as a perceptual metric,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 586–595.\n[50] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski,\nand S. Gelly, “Fvd: A new metric for video generation,” ICLR, 2019.\n[51] S. Mrak et al., “Reliability of objective picture quality measures,”\nJournal of Electrical Engineering, vol. 55, no. 1-2, pp. 3–10, 2004.\n[52] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image\nquality assessment: from error visibility to structural similarity,” IEEE\ntransactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.\n[53] X. Fei, L. Xiao, Y. Sun, and Z. Wei, “Perceptual image quality\nassessment based on structural similarity and visual masking,” Signal\nProcessing: Image Communication, vol. 27, no. 7, pp. 772–783, 2012.\n[54] P. J. Kim, S. Kim, and J. Yoo, “Stream: Spatio-temporal evaluation\nand analysis metric for video generative models,” in The Twelfth\nInternational Conference on Learning Representations (ICLR), 2024.\n[55] B. Jin, Y. Hu, Q. Tang, J. Niu, Z. Shi, Y. Han, and X. Li, “Exploring\nspatial-temporal multi-frequency analysis for high-fidelity and temporal-\nconsistency video prediction,” in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 2020, pp. 4554–4563.\n[56] X. Ye and G.-A. Bilodeau, “A unified model for continuous conditional\nvideo prediction,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 3603–3612.\n[57] S. Ning, M. Lan, Y. Li, C. Chen, Q. Chen, X. Chen, X. Han, and\nS. Cui, “Mimo is all you need: A strong multi-in-multi-out baseline for\nvideo prediction,” in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 37, no. 2, 2023, pp. 1975–1983.\n\n13\nMohammad Adiban is a PhD candidate in Machine\nLearning at the Norwegian University of Science and\nTechnology (NTNU). He holds a Bachelor’s degree\nin Computer Engineering and a Master’s degree in\nArtificial Intelligence from the Sharif University of\nTechnology, awarded in 2017. In 2022, he conducted\nresearch as a visiting scholar at Monash Univer-\nsity in Australia. Additionally, Mohammad is a co-\nfounder of the company Connect Me and Senior\nData Scientist at Bluware company. His research\nfocuses on statistical machine learning, signal pro-\ncessing, computer vision, speech processing, biomedical applications, and\ncyber security.\nKalin Stefanov is an ARC DECRA Fellow at the\nFaculty of Information Technology, Monash Uni-\nversity, Melbourne, Australia. He received the MSc\ndegree in Artificial Intelligence from the University\nof Amsterdam, Amsterdam, Netherlands and a PhD\ndegree in Computer Science from KTH Royal In-\nstitute of Technology, Stockholm, Sweden. Prior to\nhis current role, he was a Research Associate and\nPostdoctoral Research Scholar at the University of\nSouthern California, Los Angeles, USA. His main\nresearch interests are machine learning, computer\nvision, and affective computing.\nSabato Marco Siniscalchi (Senior Member, IEEE)\nis\na\nFULL\nProfessor\nwith\nthe\nUniversity\nof\nPalermo,Palermo, Italy, an Adjunct Professor with\nthe Norwegian University of Science and Technol-\nogy (NTNU), and an Affiliate Faculty with the\nGeorgia Institute of Technology. He received his\ndoctorate degree in computer engineering from the\nUniversity of Palermo, Palermo, Italy, in 2006. In\n2006, he was a Postdoctoral Fellow with Ga Tech.\nFrom 2007 to 2010, he joined NTNU, Norway, as a\nResearch Scientist. From 2010 to 2023, he was an\nAssistant Professor, first, an Associate Professor, second, and a Full Professor,\nafter, at Kore University. From 2017 to 2018, he was a Senior Speech\nResearcher with Siri Speech Group, Apple Inc., Cupertino CA, USA. He acted\nas an Associate Editor of the IEEE/ACM Transactions on Audio, Speech and\nLanguage Processing, from 2015 to 2019. Prof. Siniscalchi was an Elected\nMember of the IEEE SLT Committee from 2019 to 2022 and was re-elected\nin 2024.\nGiampiero Salvi (Senior Member, IEEE) is a Full\nProfessor at the Department of Electronic Systems\nat the Norwegian University of Science and Tech-\nnology (NTNU), Trondheim, Norway, and Associate\nProfessor at KTH Royal Institute of Technology,\nDepartment of Electrical Engineering and Computer\nScience, Stockholm, Sweden. Prof. Salvi received\nthe MSc degree in Electronic Engineering from\nUniversit`a la Sapienza, Rome, Italy and the PhD\ndegree in Computer Science from KTH. He was a\npost-doctoral fellow at the Institute of Systems and\nRobotics, Lisbon, Portugal. He was a co-founder of the company SynFace\nAB, active between 2006 and 2016. His main interests are machine learning,\nspeech technology, and cognitive systems.",
    "pdf_filename": "S-HR-VQVAE_Sequential_Hierarchical_Residual_Learning_Vector_Quantized_Variational_Autoencoder_for_Vi.pdf"
}