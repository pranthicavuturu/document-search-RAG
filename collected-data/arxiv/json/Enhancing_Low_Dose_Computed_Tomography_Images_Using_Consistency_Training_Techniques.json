{
    "title": "Enhancing Low Dose Computed Tomography Images Using Consistency Training Techniques",
    "context": "Diffusion models have significant impact on wide range of generative tasks, especially on image inpainting and restoration. Although the improvements on aiming for decreasing number of function evaluations (NFE), the iterative results are still computationally expensive. Consistency models are as a new family of generative models, enable single-step sampling of high quality data without the need for adversarial training. In this paper, we introduce the beta noise distribution, which provides flexibility in adjusting noise levels. This is combined with a sinusoidal curriculum that enhances the learning of the trajectory between the noise distribution and the posterior distribution of interest, allowing High Noise Improved Consistency Training (HN-iCT) to be trained in a supervised fashion. Additionally, High Noise Improved Consistency Training with Image Condition (HN- iCT-CN) architecture is introduced, enables to take Low Dose images as a condition for extracting significant features by Weighted Attention Gates (WAG).Our results indicate that unconditional image generation using HN-iCT significantly outperforms basic CT and iCT training techniques with NFE=1 on the CIFAR10 and CelebA datasets. Moreover, our image-conditioned model demonstrates exceptional performance in enhancing low-dose (LD) CT scans. Keywords Deep Learning · Consistency · Diffusion 1 X-ray computed tomography (CT) is essential in both diagnosis and treatment, with applications ranging from detecting internal injuries and tumors to surgical planning.To minimize the harmful effects of low-dose ionizing radiation, many studies focus on achieving high-quality denoising while keeping the dose as low as reasonably possible. Recent studies reveals that generative tasks has a remarkable success to increase quality of low dose CT scans. The most commonly used techniques due to their ease of application are Non-local Means (NLM) and Block-Matching 3D (BM3D), both of which can enhance low-dose CT (LDCT) performance. However, despite their utility, these post-processing methods often fall short of meeting clinical requirements [9, 3]. Recent advancements in deep learning showed that generative models are highly capable of meeting clinical requirements for LDCT denoising [19, 5, 1, 21]. Especially, Diffusion Probabilistic Models (DDPM) models outperform other techniques, especially GANs, upon the task of image denoising by iteratively recovering data [2]. The core working arXiv:2411.12181v1  [eess.IV]  19 Nov 2024",
    "body": "ENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES\nUSING CONSISTENCY TRAINING TECHNIQUES\nA PREPRINT\nMahmut S. Gokmen\nDepartment of Computer Science\nUniversity of Kentucky\nmselmangokmen@uky.edu\nCody Bumgardner\nDepartment of Internal Medicine\nInstitute for Biomedical Informatics\nUniversity of Kentucky\ncody@uky.edu\nJie Zhang\nDepartment of Radiology\nUniversity of Kentucky\njie.zhang1@uky.edu\nGe Wang\nDepartment of Biomedical Engineering\nRensselaer Polytechnic Institute\nwangg6@rpi.edu\nJin Chen\nDepartment of Medicine\nDepartment of Biomedical Informatics and Data Science\nUniversity of Alabama at Birmingham\njinchen@uab.edu\nNovember 20, 2024\nABSTRACT\nDiffusion models have significant impact on wide range of generative tasks, especially on image\ninpainting and restoration. Although the improvements on aiming for decreasing number of function\nevaluations (NFE), the iterative results are still computationally expensive. Consistency models\nare as a new family of generative models, enable single-step sampling of high quality data without\nthe need for adversarial training. In this paper, we introduce the beta noise distribution, which\nprovides flexibility in adjusting noise levels. This is combined with a sinusoidal curriculum that\nenhances the learning of the trajectory between the noise distribution and the posterior distribution of\ninterest, allowing High Noise Improved Consistency Training (HN-iCT) to be trained in a supervised\nfashion. Additionally, High Noise Improved Consistency Training with Image Condition (HN-\niCT-CN) architecture is introduced, enables to take Low Dose images as a condition for extracting\nsignificant features by Weighted Attention Gates (WAG).Our results indicate that unconditional\nimage generation using HN-iCT significantly outperforms basic CT and iCT training techniques with\nNFE=1 on the CIFAR10 and CelebA datasets. Moreover, our image-conditioned model demonstrates\nexceptional performance in enhancing low-dose (LD) CT scans.\nKeywords Deep Learning · Consistency · Diffusion\n1\nIntroduction\nX-ray computed tomography (CT) is essential in both diagnosis and treatment, with applications ranging from detecting\ninternal injuries and tumors to surgical planning.To minimize the harmful effects of low-dose ionizing radiation, many\nstudies focus on achieving high-quality denoising while keeping the dose as low as reasonably possible. Recent studies\nreveals that generative tasks has a remarkable success to increase quality of low dose CT scans. The most commonly\nused techniques due to their ease of application are Non-local Means (NLM) and Block-Matching 3D (BM3D), both of\nwhich can enhance low-dose CT (LDCT) performance. However, despite their utility, these post-processing methods\noften fall short of meeting clinical requirements [9, 3].\nRecent advancements in deep learning showed that generative models are highly capable of meeting clinical requirements\nfor LDCT denoising [19, 5, 1, 21]. Especially, Diffusion Probabilistic Models (DDPM) models outperform other\ntechniques, especially GANs, upon the task of image denoising by iteratively recovering data [2]. The core working\narXiv:2411.12181v1  [eess.IV]  19 Nov 2024\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\nprinciple of DDPM relies on recovering gradually perturbed data from a Gaussian distribution to the original data\ndistribution. This iterative process has a strong capacity to predict and restore missing or noisy pixels by replacing\nthem with the most likely candidates according to the data distribution. This iterative denoising technique ensures that\nDDPM models remain generative throughout the process, gradually converging to the original data distribution with\nhigh accuracy.\nThe primary challenge with DDPM models lies in the iterative time-consuming denoising process. While recent studies\nhave reduced the number of function evaluations (NFE) to as few as 20 steps, generating the highest quality samples\ntypically requires at least 80 NFEs. To address this issue, consistency models (CM) have recently emerged as a family\nof generative models, utilizing rapidly evolving knowledge distillation training techniques. Unlike DDPM models, such\nas score-based diffusion models [17], CM does not require multiple sampling steps to produce high-quality samples; it\ncan generate results in a single step. Also, CMs preserves flexibility and computation theory for generating samples in\ndifferent number of inference steps.\nConsistency models uses two different training techniques, which are represented as consistency distillation (CD)\nand consistency training (CT) [16]. The CD training requires a pre-trained model for distilling the knowledge into\nconsistency model. In contrast, the CT training allows the consistency model to learn directly from data. Although recent\nstudies shows high-performance consistency models successfully built with the CD training, CD relies on pre-trained\nmodels and requires high computational resource for distilling the data from pre-trained models. Additionally, model\nperformance is limited by the pre-trained models. Song et al [15] demonstrated that the CT training has the potential to\noutperform the CD training by eliminating Exponential Moving Average (EMA) for the teacher network and improving\ncurriculum with a step-wise increase relied on the current training steps. Furthermore, noise scheduling could also be\nimproved by adopting the log-normal distribution to sample noise levels.\nWhile full of potential, the central problem is the CT training is its sensitivity to noise distribution during training.\nAlthough the log-normal noise distribution heavily weights low noise levels in mini-batches, our experiments on various\nnoise distributions reveal that a more balanced noise distribution, spanning from the lowest to the highest noise levels,\nproduces better results. Another issue with consistency models is the performance loss that arises from zero-shot editing.\nUnlike multi-step denoising, consistency models use ODE solvers to reach the original data distribution in a single step.\nThis approach leaves no room for trade-offs between computation and image quality, resulting in zero tolerance for\nany degradation. Additionally, the high computational demands of processing medical images, which often have high\nresolutions, necessitate the design of a new model. It is also important to provide high quality denoised samples in a\nshort of training period of the model.\nTo overcome the problems arise from log-normal distribution, it is introduced beta scheduling which has two parameters\nα and β provides flexibility for adjusting the weights of noise levels in a mini-batch. This beta noise scheduling\nsignificantly enhances unconditional image generation while requiring only half the number of parameters compared to\nprevious models.\nAnother challenge lies in the curriculum, which determines the variety of noise levels encountered during model\ntraining. The latest iCT approach [15] utilizes an improved curriculum technique that doubles the variety of noise levels\npresented to the model every 50k training steps. While this improved curriculum is effective in representing various\nnoise levels and preventing overfitting, it can reduce the traceability of trajectory points due to marginal changes in the\ncurriculum. To address this, we propose a curriculum based on a sinusoidal function, which gradually decreases the rate\nat which the number of noise varieties increases across levels, thereby maintaining the traceability of trajectory points.\nAdditionally, the architecture proposed in this study for medical image denoising can effectively extract common\nfeatures between an unconditionally generated image and a conditionally represented image, as illustrated in Figure\n1. With this technique, instead of adding noise to an LDCT image, we allow the model to generate an unconditional\nmedical image while matching common features with the LDCT image, which is used as a condition. Thus, the model\nhas a able to generate an unconditional image within under supervision of LDCT image. To mitigate the dominance of\nthe LDCT-conditioned image and prevent additional computational costs, the weighted attention gate modules have\nbeen introduced in this study.\nOur contributions address key challenges in consistency models by enhancing training techniques and proposing a novel\narchitecture for medical image denoising. These improvements offer more efficient and effective solutions compared to\nexisting methods.\n• Introduced beta noise scheduling for improved unconditional image generation with reduced parameters.\n• Proposed a sinusoidal curriculum to maintain trajectory point traceability by gradually adjusting noise level\nincrements.\n2\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\n• Developed a new architecture for medical image denoising using weighted attention gate modules for balanced\nsupervision.\nFigure 1: The illustration of image conditioned consistency model\n2\nBackground\n2.1\nConsistency Models\nConsistency models [16], relies on continuous-time definition of diffusion models which generate data by gradually\ntransforming the data distribution,pdata, into a noise distribution using stochastic differential equation (SDE). These\nmodels then learn to reverse this process to generate data from noise. Notably, the SDE has a related ordinary differential\nequation (ODE) known as the probability flow (PF) ODE [17].According to definition of score based generative models,\nPF ODE shapes into\ndx = −σ∇x log pσ(x) dt, σ ∈[σmin, σmax]\n(1)\nfor ∇x log pσ(x) is score function for perturbed data distribution pσ(x) ≈pσmin(x). σmin is defined as 0.002 to\nmaintain stability and σmax = 80 is chosen as reasonable by [6] to provide pσ(x) ≈N(0, σ2\nmaxI). Solving the\nprobability flow ODE from σ(t + 1) to σ(t) allows the model to transform a sample xσt+1 ≈pσt+1 to xσt ≈pσt.\nThe provided bijective mapping between xσ ≈pσ(x) and xσmin ∼pσmin(x) ≈pdata(x) maintains consistency and\nwhich is denoted as f ∗: (xσ, σ) 7→xσmin.\nA consistency model which denoted as fθ(x, σ) is parameterized to meet boundary condition and transforming it into a\ndifferentiable form, it is parameterized as it is defined in [16].\nfθ(x, σ) = cskip(σ)x + cout(σ)Fθ(x, σ),\n(2)\nWhere Fθ(x, σ) is a free-from network. To train consistency models, probability flow ODE is discretized using\nnoise sequences are ranging from σmin to σmax = σN. Discretization of these noise sequences is denoted as\nσi =\n\u0010\nσ1/ρ\nmin + i−1\nN−1\n\u0010\nσ1/ρ\nmax −σ1/ρ\nmin\n\u0011\u0011ρ\nfor i ∈J1, NK, ρ = 7 in [6].\nConsistency models are trained using the consistency matching loss\nE\n\u0002\nλ(σi)d(fθ(˜xσi+1, σi+1), fθ−(˜xσi, σi))\n\u0003\n,\n(5)\n3\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\nwhere\n˜xσi = xσi+1 −(σi −σi+1)σi+1∇x log pσi+1(x)\n\f\f\nx=xσi+1\n(6)\nis a single-step in the reverse direction solving the PF ODE in Eq. (1). Here, λ(·) is a positive weighting function, and\nd(·, ·) is determined as 1 in [16]. It is suggested using the Learned Perceptual Image Patch Similarity (LPIPS) as the\nmetric function [16]. The expectation in Eq. (5) is taken with respect to i ∼U[1, N −1], a uniform distribution over\nintegers i = 1, . . . , N −1, and xσi+1 ∼pσi+1(x). The objective in Eq. (5) is minimized via stochastic gradient descent\non parameters θ while θ−is updated using the exponential moving average (EMA)\nθ−←stopgrad(µθ−+ (1 −µ)θ),\n(7)\nwhere 0 ≤µ ≤1 is the decay rate. fθ and fθ−are referred to as the student and teacher networks, respectively.\nThe training technique for consistency models can be divided into two categories: consistency distillation (CD) and\nconsistency training (CT). The consistency distillation technique relies on utilizing a pre-trained EDM model as a\nteacher model, denoted as fθ−(˜xσi, σi).\nConsistency distillation (CD) training technique surpasses consistency training (CT) in FID results when compared. It\nis crucial that the best results for a model trained with CD can only match the best results of the teacher model trained\nwith EDM. To overcome this obstacle, [15] suggests an improved training technique for CT, which generates better FID\nresults.\n2.2\nImproved Training Techniques for Consistency Models\nImproved training techniques for CT moves one step forward isolation training for consistency models [15]. The\nmodifications are utilized for improved consistency models (iCM) consist of curriculum, elimination of EMA and\nteacher model, replacing LPIPS loss function with pseudo huber loss and the noise distribution changed from uniform\nto lognormal noise distribution. The first modification, setting N to 1281 in N(k), provides a good balance between\nbias and variance compared to CM training. Experimental results show that changing s0 from 2 to 10 and s1 from 150\nto 1280 yields the best generative performance for iCT.\nN(k) = min(s02⌊k\nK′ ⌋, s1) + 1,\nK′ =\n\u0016\nK\nlog2 [s1/s0] + 1\n\u0017\n, where K = 1280\n(3)\niCT utilizes the same noise scheduling as it is described in [6] and Equation 4 which emphasizes high weighted low\nnoise levels and corresponds to p(log σ) = σ\nσ1/ρ−1\nρ(σ1/ρ\nmax−σ1/ρ\nmin)\nas N →∞..\nσi =\n\u0012\nσ1/ρ\nmin + i −1\nN −1\n\u0010\nσ1/ρ\nmax −σ1/ρ\nmin\n\u0011\u0013ρ\nfor i ∈J1, NK, and ρ = 7,\n(4)\nBesides the exponential curriculum and Karras noise scheduling, to emphasize lower noise levels in the noise distribution\nduring training, the modification employed for iCM includes a log-normal noise distribution on image batches, which\nsignificantly assigns low weights to high noise levels.\np(σi) ∝erf\n\u0012log(σi+1) −Pmean\n√\n2Pstd\n\u0013\n−erf\n\u0012log(σi) −Pmean\n√\n2Pstd\n\u0013\n,\n(5)\nwhere Pmean = −1.1 and Pstd = 2.0. This log-normal noise schedule leverages sampling quality and significantly\ndecrease FID scores. Addition to lognormal noise distribution, to increase the emphasize on lower noise levels which is\nprovided by lognormal noise distribution, the loss weighting is adjusted as\nλ(σi) =\n1\nσi+1 −σi\n.\n(6)\nThe refined weighting function notably improves sample quality in consistency training by assigning smaller weights to\nhigher noise levels. This approach addresses the issue of the default uniform weighting function, which assigns equal\nweights to all noise levels and is found to be suboptimal. By reducing the weighting as noise levels increase, the new\nmethod ensures that smaller noise levels, which can influence larger ones, are weighted more heavily.\nImproved training techniques for CT eliminates teacher model and EMA update during training. The underlying reason\nfor elimination of EMA and teacher model is unbiased trajectory mapping between student and teacher model and the\ndecay rate is updated as µ(k) = 0. The loss function LPIPS employed for CT is replaced with the pseudo-Huber loss\ndue to undesirable bias in evaluation.\nd(x, y) =\nq\n∥x −y∥2\n2 + c2 −c\n(7)\n4\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\n3\nMethods\n3.1\nArchitecture\nThe proposed approach, High Noise Improved Consistency Training (HN-iCT), shares the same backbone as the\nconsistency models used for image generation. We categorized this approach into two sections: the first focuses on\nunconditional image generation, which involves modifications to the curriculum and noise distribution, while the second\nis conditional image generation (HN-iCT-CN), which requires a different architecture in addition to the proposed\ncurriculum and beta noise distribution, as shown in Figure 1.\nFor unconditional image generation, HN-iCT relies on the same U-Net architecture as implemented for consistency\nmodels [16]. For image conditional generation, the most well-known technique is label embedding or class embedding.\nGenerally, this type of embedding is implemented by summing encoded features irrespective of the dimensions of the\ninput size. In other cases, such as when an image is given as condition to a model, it is important to choose what type of\nembedding will be employed while training. In this study, AG (Attention Gate) modules are utilized to extract common\nspatial features between the image chosen as condition and input image [13]. Further more, AG modules are modified\nas Weigted Attention Gate (WAG) to prevent incompatibility between other U-NET components and adapted according\nto HN-iCT model. Figure 1 represents general architecture used for image conditional training, comprehends iAG\nmodules.\nWeighted Attention Gate (WAG): During the encoding process, gradually downsampled features retain essential\nstructural information about the image that will be denoised in the decoding process. While it is possible to reconstruct\nthe denoised image from the latent space, the output may include noisy pixels due to the loss of structural details. To\nachieve a clean image at the end of the decoding process and preserve structural information, it is necessary to evaluate\nthe skip connections and decoded features. The attention gates are designed to assess both global structural information\nand pixel-wise details to reconstruct denoised images effectively. In our implementation, the Weighted Attention Gate\n(WAG) incorporates a learnable weighting mechanism, where the attention map is squared to sharpen common features\nbetween the skip connection and the conditioned input. The gate ensures that the skip connections do not dominate the\nreconstruction while preserving the spatial-temporal features. Additionally, extracted features from the conditioned\ninput are scaled by a weight parameter (defaulting to 0.8) and combined with the skip connection output to achieve a\nbalanced reconstruction. The architecture of WAG is represented in Figure 2.\nFigure 2: The illustration of WAG unit used in proposed architecture. Weight(w) parameter is determined as 0.8.\n3.2\nTraining Technique\nHigh Noise Level Effect on Performance. Noise distribution in mini-batches plays a crucial role in teaching the model\nthe trajectories between noise levels. To provide a high-confidence trajectory, it is important to have a noise distribution\nthat includes a wide variety of noise levels. A noise distribution with a high variety of noise levels increases the number\nof high noise levels in the distribution, in contrast to the log-normal noise distribution used in iCT training techniques\n[15].\nIn this section, we evaluate the experimental results conducted within the scope of adding extra high noise levels\nmanually to scheduled noises with a log-normal distribution. As it is claimed ’High Noises is a Must’, the experiments\nbegin by adding high-noise levels gradually on the mini-batches based on percentage ratio of the mini-batch size. This\n5\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\nexperiments proves that necessities of a noise scheduling comprising high-level noises beside high weighted low-level\nnoises in a noise scheduling. High noise levels ranging between 40 and 80 are randomly added to mini-batches to\nincrease the weight of these high noise levels. The experiments conducted with additional high noise levels reveal\nthat denoising performance significantly increases when the high noise levels constitute up to 4% of the mini-batches.\nAdditional experimental details are provided in appendix A.\nBeta Distribution. The beta distribution is a continuous probability distribution defined on the interval [0, 1], commonly\nused to model the behavior of random variables that are constrained within this range. It is parameterized by two\npositive shape parameters, denoted as α (alpha) and β (beta), which determine the distribution’s shape. The probability\ndensity function (PDF) of the beta distribution is given by:\nf(x; α, β) = xα−1(1 −x)β−1\nB(α, β)\nwhere\nB(α, β) =\nZ 1\n0\ntα−1(1 −t)β−1 dt\n(8)\nfor 0 ≤x ≤1. The parameters α and β influence the skewness and kurtosis of the distribution, allowing for a wide\nrange of shapes including uniform, U-shaped, and J-shaped distributions. This flexibility makes the beta distribution\nparticularly useful in Bayesian statistics, where it is often employed as a prior distribution for probabilities and\nproportions.\nThe flexibility of the beta distribution makes it highly adaptable for various noise distributions, particularly for those\naiming to adjust the weight of noise levels within a mini-batch [22]. Particularly, it is possible increasing the weight of\nhigh level noises in distribution up to 4% by adjusting α = 0.5 and β = 5. CLAIM and modify: The set of parameters\nwere empirically determined by using CIFAR10.\nSinusoidal Curriculum. The proposed sinusoidal curriculum, inspired by the sinus function, offers a significant\nadvancement in the training of consistency models by introducing a smooth, continuous progression of the number\nof timesteps for each noise distribution on mini-batch. Unlike improved curriculum that rely on abrupt or stepwise\nchanges, the sinusoidal curriculum leverages the natural oscillation of the sine function to modulate the variety of\nnoise levels encountered during training. This gradual adjustment ensures that the model experiences a consistent and\nwell-distributed range of noise levels, enhancing the stability and robustness of the learning process.\nN(k) = min\n\u0012\u0018\f\f\f\f\n\u0012\ns1 · sin\n\u0012π · 3 · k\n2 · K\n\u0013\u0013\n+ s0\n\f\f\f\f\n\u0019\n+ 1, s1 + 1\n\u0013\n(9)\nσ(i) = sin\n\u0012 π · i\n2 · N\n\u0013\n· ∆t + t0\n(10)\nThe sinusoidal curriculum is governed by key parameters. The initial timestep s0 sets the starting point, while the\ndifference between final number of time steps and initial the number of time steps ∆t = s1 −s0 controls the amplitude\nof the sinusoidal curve. The total number of time steps N defines the schedule length, with the sine function scaled by\nπ to ensure a smooth, gradual adjustment across timesteps. To prevent marginal increases, s0 and s1 are set to 20 and\n250, respectively, providing broad yet stable coverage of noise levels and enhancing training stability.\n4\nExperiments\n4.1\nExperimental Setup\nDatasets. For unconditional image generation, we consider CIFAR10 [8], which includes 50K training images; CelebA\n64x64 [11], which contains 162,770 training images of individual human faces; and Butterflies [18] 256x256 dataset,\nwhich provides 1K images with different species of butterflies.\nFor image conditional learning, the dataset sourced from the Mayo Clinic, as used in the AAPM low-dose CT grand\nchallenge, was utilized [12]. The data is reconstructed on a 512x512 pixel grid with a slice thickness of 1 mm and a\nmedium (B30) reconstruction kernel. The first eight patients provided training data, resulting in a total of 4800 slices,\nwhile the remaining two patients were used for validation, contributing a total of 1136 slices. This demonstrates the\nmodel’s effectiveness on medical data and validates that the proposed training technique yields good results across\ndifferent datasets.\nImage Conditioning. The image conditioning technique employed in our model takes Low dose 512x512 reconstructed\nCT slices as a condition for denoising process. No additional pre-processing is applied on low dose slices before\nsubmitting it as a condition. It is a similar approach such as label embedding.\n6\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\nModel Configurations. It is identified several configurations for HN-iCT model. This configurations depends on\ncapacity of the hardware for training the model. Additionally, the configurations for other models utilized in experiments\nare provided in Table 1. To make a fair comparison, all hyper parameters such as optimizer type, learning rate and\ndropout kept as same with HN-iCT model except the number of residual blocks which depends on hardware limitations.\nThe smallest model configuration of HN-iCT is utilized in our experiments to prove stability and robustness of our\napproach. Other options of the HN-iCT model were not used due to hardware limitations and extended training time,\nbut these options are available in our public repository for researchers who wish to experiment further.\nModel Name\n#Number of Res. Blocks\nBase Channel Size\n#Params\nUnconditional Model\nHN-iCT-Small\n2\n128\n234M\nHN-iCT-Small/H (High Res.)\n2\n192\n527M\nHN-iCT-Mid\n4\n128\n373M\nHN-iCT-Large\n6\n128\n512M\nCT\n4\n192\n897M\niCT\n4\n192\n897M\nImage Conditioned Model\nHN-iCT-CN-Small\n2\n32\n14M\nHN-iCT-CN-Mid\n4\n32\n22M\nTable 1: Model Configurations for unconditional image generation and image conditioned generation\nTraining. We use the RAdam optimizer and learning rate is adjusted as 1-e4 for all datasets. For unconditional image\ngeneration, we train 800K iterations on CIFAR10 and CelebA 64x64 with a batch size of 512. For image conditional\ngeneration, we train 400K iterations on reconstructed CT slices with 512x512 patches with a batch size of 32.\n4.2\nUnconditional Image Generation\nWe compare HN-iCT model with prior models are trained with CT, iCT technique and diffusion models. Considering\nlack of studies based on CT and iCT technique, we evaluate publicly available sources on iCT and CT technique for\ncomparing with our approach. The models are gathered from public resources are referred as number of versions from\nv1 to v3 in table 2. Additionally, to recover fairness of comparison between models, all models compared in table 2 are\ntrained locally. As shown in Table 2, HN-iCT performs better FID results when it is compared the other models which\nare publicly available and official models. We employed 2 number of residual blocks instead of 4 to prove robustness of\nour proposed training technique.\nCIFAR10 32x32\nCelebA 64x64\nModel\n#Residual Blocks\nNFE\nFID ↓\nDDPM [5]\n4\n1000\n3.17\n3.26\nDDIM [14]\n4\n10\n13.36\n17.33\nEDM [7]\n4\n35\n2.04\n3.32\nCT [16]\n4\n1\n14.32\n18.72\nCT-v12\n4\n1\n15.54\n19.66\niCT [15]\n4\n1\n13.50\n15.60\niCT-v11\n4\n1\n14.72\n18.54\niCT-v23\n4\n1\n13.03\n16.03\nHN-iCT-Small\n2\n1\n10.50\n12.31\nTable 2: Model Performance on CIFAR10 32x32 and CelebA 64x64, α = 1.5, β = 5\n1iCT-v1: https://github.com/Kinyugo/consistency_models\n2CT-v1: https://github.com/cloneofsimo/consistency_models\n3CT-v2: https://github.com/junhsss/consistency-models\n7\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\n4.3\nImage Conditioned Generation\nIn this section, our proposed image conditioned consistency model (HN-iCT-CN Small) which employs Weighted\nAttention Gate (WAG) modules is utilized for enhancing low-dose CT scans. Experimental results are shown in Table 3\nand compared with related studies utilizing same LDCT dataset [12].\nModel Name\nLPIPS (↓)\nSSIM (↑)\nPSNR (↑)\nNFE (↓)\nLDCT\n0.152 ± 0.02\n0.94 ± 0.02\n41.50 ± 1.5\nDDPM [19]\n0.032 ± 0.02\n0.97 ± 0.02\n43.11 ± 1.6\n50\nEDM [7]\n0.053 ± 0.01\n0.97 ± 0.01\n43.80 ± 1.2\n79\nDDIM [10]\n0.053 ± 0.01\n0.81 ± 0.01\n35.35 ± 1.2\n10\nCD [16]\n0.065 ± 0.01\n0.94 ± 0.01\n42.00 ± 0.8\n1\nPFGM++ [20]\n0.055 ± 0.01\n0.96 ± 0.01\n43.6 ± 0.8\n79\nPS-PFCM [4]\n0.061 ± 0.01\n0.96 ± 0.01\n43.00 ± 0.8\n1\nHN iCT CN Small\nα=0.5,β=5\n0.016 ± 0.01\n0.96 ± 0.01\n43.68 ± 1.2\n1\nHN iCT CN Medium\nα=0.5,β=5\n0.017 ± 0.01\n0.96 ± 0.01\n44.07 ± 1.4\n1\nTable 3: Quantitative Results, copied from other paper.\nWe explored that, effectiveness of the noise distribution on denoising performance depends and highly related on α\nparameter, to adjust the balance between low and high noise levels in generated noise levels. This reveals a correlation\nbetween batch size and the α parameter, where a reduction in batch size leads to a decrease in α, directly impacting the\noverall performance. Based on the results from different tests, the optimal α value for achieving the best denoising\nperformance is set to 0.5. Additonally, experimental denoising samples from Low Dose CT validation set are represented\nin Figure 3.\n4.4\nAblation Study\nIn this section, we present an ablation study conducted on the improved curriculum, sinusoidal curriculum, log-normal\nnoise distribution, and beta noise distribution. The model configuration used for the ablations is designated as HN-\niCT-Small, and the training steps are kept the same at 400K for each configuration, as shown in Table 4. Each training\nconfiguration for the ablations is represented by version numbers from HN −iCT −Sv1 to HN −iCT −Sv4, with\ntwo different options configured within the maximum possible transformation configurations.\nModel Versions\nCifar10\nComponents\nFID\nCurriculum Type\nNoise Distribution\nHN −iCT −Sv1\n21.19\nImproved\nLog-Normal\nHN −iCT −Sv2\n17.31\nImproved\nBeta\nHN −iCT −Sv3\n17.85\nSinusoidal\nLog-Normal\nHN −iCT −Small\n10.51\nSinusoidal\nBeta\nTable 4: Ablation study on curriculum and noise distribution , evaluated at 400K training steps. β=1.5 , α=5 for Beta\ndistribution\n5\nDiscussion and Conclusion\nOur enhancements to noise scheduling and curriculum address the need for a balanced noise distribution and controlled\nprogression of noise steps within the curriculum. We examined the impact of the high noise levels in a noise distribution\n8\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\nFigure 3: Representation of denoised slice samples from the low-dose CT validation set. The first column features\nfull-dose slice samples, the middle column includes low-dose slices, and the last column presents single-step denoising\nsamples processed by the HN iCT CN model. 1 mm slices, Window setting [-160,240] HU\ncomprising the high-weighted low noise levels by beta distribution. Also, proposed curriculum which is based on\nsinusoidal function also provides a stability with minor changes on the number of noise steps for each training steps.\nAlthough the sinusoidal curriculum and beta distribution have a significant effect on the denoising performance of\nunconditional image generation, they need to be extended with different values for the parameters β, α, s0, and s1. The\nbiggest obstacle is widely known to be the training time.\nIn aspect of medical image processing, image conditioned architecture has made it possible to process medical images\nwithout resizing, by decreasing the number of learnable parameters around 80%. Also, it should be regarded that\nweighted Attention Gate modules have valuable impact on leveraging denoising performance for medical images. This\nexperimental approach conducted on consistency models has a potential to combine different images given as condition\nand input, and may lead various applications by tuning parameters on WAG module.\n9\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\nIn conclusion, it is presented a new curriculum schedule and noise distribution for consistency models, which provides\nflexibility on distribution of noise levels in a mini-batch by adjusting α and β parameters. The suggested approach\nfor unconditional image generation makes it possible to generate better quality images with lower model size and the\nmodel can learn easily trajectory from the prior noise distribution to posterior distribution of noise. The results indicate\nthat tuning the weighting parameter in the WAG module, as well as the β and α parameters in the curriculum and the\nbeta noise distribution algorithm, allows us to outperform the corresponding consistency models.\n6\nAcknowledgements\nThe project described was supported by the NIH National Center for Advancing Translational Sciences through grant\nnumber UL1TR001998. The content is solely the responsibility of the authors and does not necessarily represent the\nofficial views of the NIH.\nReferences\n[1] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A\nsurvey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9):10850–10869, September 2023.\n[2] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021.\n[3] P Fumene Feruglio, C Vinegoni, J Gros, A Sbarbati, and R Weissleder. Block matching 3d random noise filtering\nfor absorption optical projection tomography. Physics in Medicine and Biology, 55(18):5401–5415, August 2010.\n[4] Dennis Hein, Adam Wang, and Ge Wang. Poisson flow consistency models for low-dose ct image denoising,\n2024.\n[5] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.\n[6] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\ngenerative models. 2022.\n[7] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\ngenerative models, 2022.\n[8] Alex Krizhevsky. Learning multiple layers of features from tiny images. pages 32–33, 2009.\n[9] Zhoubo Li, Lifeng Yu, Joshua D. Trzasko, David S. Lake, Daniel J. Blezek, Joel G. Fletcher, Cynthia H.\nMcCollough, and Armando Manduca. Adaptive nonlocal means filtering based on local noise level for ct\ndenoising: Adaptive nonlocal means filtering for ct denoising. Medical Physics, 41(1):011908, December 2013.\n[10] Xuan Liu, Yaoqin Xie, Jun Cheng, Songhui Diao, Shan Tan, and Xiaokun Liang. Diffusion probabilistic priors for\nzero-shot low-dose ct image denoising, 2023.\n[11] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings\nof International Conference on Computer Vision (ICCV), December 2015.\n[12] Cynthia H. McCollough, Adam C. Bartley, Rickey E. Carter, Baiyu Chen, Tammy A. Drees, Phillip Edwards,\nDavid R. Holmes, Alice E. Huang, Farhana Khan, Shuai Leng, Kyle L. McMillan, Gregory J. Michalak, Kristina M.\nNunez, Lifeng Yu, and Joel G. Fletcher. Low-dose <scp>ct</scp> for the detection and classification of metastatic\nliver lesions: Results of the 2016 low dose <scp>ct</scp> grand challenge. Medical Physics, 44(10), October\n2017.\n[13] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori,\nSteven McDonagh, Nils Y Hammerla, Bernhard Kainz, Ben Glocker, and Daniel Rueckert. Attention u-net:\nLearning where to look for the pancreas, 2018.\n[14] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2020.\n[15] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. 2023.\n[16] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023.\n[17] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. 2020.\n[18] Josiah Wang, Katja Markert, and Mark Everingham. Learning models for object recognition from natural language\ndescriptions. In Proceedings of the British Machine Vision Conference, 2009.\n[19] Wenjun Xia, Qing Lyu, and Ge Wang. Low-dose ct using denoising diffusion probabilistic model for 20× speedup,\n2022.\n10\n\nENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES USING CONSISTENCY TRAINING\nTECHNIQUES\nA PREPRINT\n[20] Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, and Tommi Jaakkola. Pfgm++: Unlocking\nthe potential of physics-inspired generative models, 2023.\n[21] Xin Yi, Ekta Walia, and Paul Babyn. Generative adversarial network in medical imaging: A review. Medical\nImage Analysis, 58:101552, December 2019.\n[22] Mingyuan Zhou, Tianqi Chen, Zhendong Wang, and Huangjie Zheng. Beta diffusion, 2023.\nAppendices\nA\nHigh Noise Level Experimental Details\nThe experiments reveals that adding minor weighted high noise levels on mini-batches increase denoising performance.\nAs it is represented in table 5, while adding high level noise levels with lower percentage ratios can enhance denoising\nperformance, adding high level noise at 10% of the mini-batch length has effects on denoising performance conversely.\nTable 5: Evaluation of denoising performance of models trained with noise schedules, including high noise levels\nproportional to the size of mini-batches.\nModel Versions\nBatch Size\nTraining Steps\nNumber of Res. Blocks\nHigh Level Noise Ratio\nN\nNoise Scheduling\nAttention Resolutions\nFID Score\niCT v1\n1024\n100000\n2\n0%\n100\nLog-Normal\n[16,8]\n66.51\niCT v2\n1024\n100000\n2\n2%\n100\nLog-Normal\n[16,8]\n75.34\niCT v3\n1024\n100000\n2\n3%\n100\nLog-Normal\n[16,8]\n43.81\niCT v4\n1024\n100000\n2\n4%\n100\nLog-Normal\n[16,8]\n40.32\niCT v5\n1024\n100000\n2\n5%\n100\nLog-Normal\n[16,8]\n73.24\nThe results represented in Table 5 demonstrate that implementing a log-normal noise distribution with high noise levels\nenhances denoising performance when the high-level noises reach 5% of the total number of mini-batch size. Based on\nthese results, we propose beta noise distribution which represents high-weighted low noise levels and low-weighted\nhigh noise levels. Additionally, beta disribution provides a flexibility to adjust on the ratio of noise levels by adjusting β\nand α parameters as it desired.\n11",
    "pdf_filename": "Enhancing_Low_Dose_Computed_Tomography_Images_Using_Consistency_Training_Techniques.pdf"
}