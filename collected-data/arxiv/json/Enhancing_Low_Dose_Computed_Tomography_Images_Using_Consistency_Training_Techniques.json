{
    "title": "ENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES",
    "abstract": "Diffusionmodelshavesignificantimpactonwiderangeofgenerativetasks, especiallyonimage inpaintingandrestoration. Althoughtheimprovementsonaimingfordecreasingnumberoffunction evaluations (NFE), the iterative results are still computationally expensive. Consistency models areasanewfamilyofgenerativemodels,enablesingle-stepsamplingofhighqualitydatawithout the need for adversarial training. In this paper, we introduce the beta noise distribution, which provides flexibility in adjusting noise levels. This is combined with a sinusoidal curriculum that enhancesthelearningofthetrajectorybetweenthenoisedistributionandtheposteriordistributionof interest,allowingHighNoiseImprovedConsistencyTraining(HN-iCT)tobetrainedinasupervised fashion. Additionally, High Noise Improved Consistency Training with Image Condition (HN- iCT-CN)architectureisintroduced,enablestotakeLowDoseimagesasaconditionforextracting significant features by Weighted Attention Gates (WAG).Our results indicate that unconditional imagegenerationusingHN-iCTsignificantlyoutperformsbasicCTandiCTtrainingtechniqueswith NFE=1ontheCIFAR10andCelebAdatasets. Moreover,ourimage-conditionedmodeldemonstrates exceptionalperformanceinenhancinglow-dose(LD)CTscans.",
    "body": "ENHANCING LOW DOSE COMPUTED TOMOGRAPHY IMAGES\nUSING CONSISTENCY TRAINING TECHNIQUES\nAPREPRINT\nMahmutS.Gokmen CodyBumgardner JieZhang\nDepartmentofComputerScience DepartmentofInternalMedicine DepartmentofRadiology\nUniversityofKentucky InstituteforBiomedicalInformatics UniversityofKentucky\nmselmangokmen@uky.edu UniversityofKentucky jie.zhang1@uky.edu\ncody@uky.edu\nGeWang JinChen\nDepartmentofBiomedicalEngineering DepartmentofMedicine\nRensselaerPolytechnicInstitute DepartmentofBiomedicalInformaticsandDataScience\nwangg6@rpi.edu UniversityofAlabamaatBirmingham\njinchen@uab.edu\nNovember20,2024\nABSTRACT\nDiffusionmodelshavesignificantimpactonwiderangeofgenerativetasks, especiallyonimage\ninpaintingandrestoration. Althoughtheimprovementsonaimingfordecreasingnumberoffunction\nevaluations (NFE), the iterative results are still computationally expensive. Consistency models\nareasanewfamilyofgenerativemodels,enablesingle-stepsamplingofhighqualitydatawithout\nthe need for adversarial training. In this paper, we introduce the beta noise distribution, which\nprovides flexibility in adjusting noise levels. This is combined with a sinusoidal curriculum that\nenhancesthelearningofthetrajectorybetweenthenoisedistributionandtheposteriordistributionof\ninterest,allowingHighNoiseImprovedConsistencyTraining(HN-iCT)tobetrainedinasupervised\nfashion. Additionally, High Noise Improved Consistency Training with Image Condition (HN-\niCT-CN)architectureisintroduced,enablestotakeLowDoseimagesasaconditionforextracting\nsignificant features by Weighted Attention Gates (WAG).Our results indicate that unconditional\nimagegenerationusingHN-iCTsignificantlyoutperformsbasicCTandiCTtrainingtechniqueswith\nNFE=1ontheCIFAR10andCelebAdatasets. Moreover,ourimage-conditionedmodeldemonstrates\nexceptionalperformanceinenhancinglow-dose(LD)CTscans.\nKeywords DeepLearning·Consistency·Diffusion\n1 Introduction\nX-raycomputedtomography(CT)isessentialinbothdiagnosisandtreatment,withapplicationsrangingfromdetecting\ninternalinjuriesandtumorstosurgicalplanning.Tominimizetheharmfuleffectsoflow-doseionizingradiation,many\nstudiesfocusonachievinghigh-qualitydenoisingwhilekeepingthedoseaslowasreasonablypossible. Recentstudies\nrevealsthatgenerativetaskshasaremarkablesuccesstoincreasequalityoflowdoseCTscans. Themostcommonly\nusedtechniquesduetotheireaseofapplicationareNon-localMeans(NLM)andBlock-Matching3D(BM3D),bothof\nwhichcanenhancelow-doseCT(LDCT)performance. However,despitetheirutility,thesepost-processingmethods\noftenfallshortofmeetingclinicalrequirements[9,3].\nRecentadvancementsindeeplearningshowedthatgenerativemodelsarehighlycapableofmeetingclinicalrequirements\nfor LDCT denoising [19, 5, 1, 21]. Especially, Diffusion Probabilistic Models (DDPM) models outperform other\ntechniques,especiallyGANs,uponthetaskofimagedenoisingbyiterativelyrecoveringdata[2]. Thecoreworking\n4202\nvoN\n91\n]VI.ssee[\n1v18121.1142:viXra\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\nprincipleofDDPMreliesonrecoveringgraduallyperturbeddatafromaGaussiandistributiontotheoriginaldata\ndistribution. Thisiterativeprocesshasastrongcapacitytopredictandrestoremissingornoisypixelsbyreplacing\nthemwiththemostlikelycandidatesaccordingtothedatadistribution. Thisiterativedenoisingtechniqueensuresthat\nDDPMmodelsremaingenerativethroughouttheprocess,graduallyconvergingtotheoriginaldatadistributionwith\nhighaccuracy.\nTheprimarychallengewithDDPMmodelsliesintheiterativetime-consumingdenoisingprocess. Whilerecentstudies\nhavereducedthenumberoffunctionevaluations(NFE)toasfewas20steps,generatingthehighestqualitysamples\ntypicallyrequiresatleast80NFEs. Toaddressthisissue,consistencymodels(CM)haverecentlyemergedasafamily\nofgenerativemodels,utilizingrapidlyevolvingknowledgedistillationtrainingtechniques. UnlikeDDPMmodels,such\nasscore-baseddiffusionmodels[17],CMdoesnotrequiremultiplesamplingstepstoproducehigh-qualitysamples;it\ncangenerateresultsinasinglestep. Also,CMspreservesflexibilityandcomputationtheoryforgeneratingsamplesin\ndifferentnumberofinferencesteps.\nConsistency models uses two different training techniques, which are represented as consistency distillation (CD)\nandconsistencytraining(CT)[16]. TheCDtrainingrequiresapre-trainedmodelfordistillingtheknowledgeinto\nconsistencymodel.Incontrast,theCTtrainingallowstheconsistencymodeltolearndirectlyfromdata.Althoughrecent\nstudiesshowshigh-performanceconsistencymodelssuccessfullybuiltwiththeCDtraining,CDreliesonpre-trained\nmodelsandrequireshighcomputationalresourcefordistillingthedatafrompre-trainedmodels. Additionally,model\nperformanceislimitedbythepre-trainedmodels. Songetal[15]demonstratedthattheCTtraininghasthepotentialto\noutperformtheCDtrainingbyeliminatingExponentialMovingAverage(EMA)fortheteachernetworkandimproving\ncurriculumwithastep-wiseincreasereliedonthecurrenttrainingsteps. Furthermore,noiseschedulingcouldalsobe\nimprovedbyadoptingthelog-normaldistributiontosamplenoiselevels.\nWhilefullofpotential, thecentralproblemistheCTtrainingisitssensitivitytonoisedistributionduringtraining.\nAlthoughthelog-normalnoisedistributionheavilyweightslownoiselevelsinmini-batches,ourexperimentsonvarious\nnoisedistributionsrevealthatamorebalancednoisedistribution,spanningfromthelowesttothehighestnoiselevels,\nproducesbetterresults. Anotherissuewithconsistencymodelsistheperformancelossthatarisesfromzero-shotediting.\nUnlikemulti-stepdenoising,consistencymodelsuseODEsolverstoreachtheoriginaldatadistributioninasinglestep.\nThisapproachleavesnoroomfortrade-offsbetweencomputationandimagequality,resultinginzerotolerancefor\nanydegradation. Additionally,thehighcomputationaldemandsofprocessingmedicalimages,whichoftenhavehigh\nresolutions,necessitatethedesignofanewmodel. Itisalsoimportanttoprovidehighqualitydenoisedsamplesina\nshortoftrainingperiodofthemodel.\nToovercometheproblemsarisefromlog-normaldistribution,itisintroducedbetaschedulingwhichhastwoparameters\nα and β provides flexibility for adjusting the weights of noise levels in a mini-batch. This beta noise scheduling\nsignificantlyenhancesunconditionalimagegenerationwhilerequiringonlyhalfthenumberofparameterscomparedto\npreviousmodels.\nAnother challenge lies in the curriculum, which determines the variety of noise levels encountered during model\ntraining. ThelatestiCTapproach[15]utilizesanimprovedcurriculumtechniquethatdoublesthevarietyofnoiselevels\npresentedtothemodelevery50ktrainingsteps. Whilethisimprovedcurriculumiseffectiveinrepresentingvarious\nnoiselevelsandpreventingoverfitting,itcanreducethetraceabilityoftrajectorypointsduetomarginalchangesinthe\ncurriculum. Toaddressthis,weproposeacurriculumbasedonasinusoidalfunction,whichgraduallydecreasestherate\natwhichthenumberofnoisevarietiesincreasesacrosslevels,therebymaintainingthetraceabilityoftrajectorypoints.\nAdditionally, the architecture proposed in this study for medical image denoising can effectively extract common\nfeaturesbetweenanunconditionallygeneratedimageandaconditionallyrepresentedimage,asillustratedinFigure\n1. Withthistechnique,insteadofaddingnoisetoanLDCTimage,weallowthemodeltogenerateanunconditional\nmedicalimagewhilematchingcommonfeatureswiththeLDCTimage,whichisusedasacondition. Thus,themodel\nhasaabletogenerateanunconditionalimagewithinundersupervisionofLDCTimage. Tomitigatethedominanceof\ntheLDCT-conditionedimageandpreventadditionalcomputationalcosts,theweightedattentiongatemoduleshave\nbeenintroducedinthisstudy.\nOurcontributionsaddresskeychallengesinconsistencymodelsbyenhancingtrainingtechniquesandproposinganovel\narchitectureformedicalimagedenoising. Theseimprovementsoffermoreefficientandeffectivesolutionscomparedto\nexistingmethods.\n• Introducedbetanoiseschedulingforimprovedunconditionalimagegenerationwithreducedparameters.\n• Proposedasinusoidalcurriculumtomaintaintrajectorypointtraceabilitybygraduallyadjustingnoiselevel\nincrements.\n2\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\n• Developedanewarchitectureformedicalimagedenoisingusingweightedattentiongatemodulesforbalanced\nsupervision.\nFigure1: Theillustrationofimageconditionedconsistencymodel\n2 Background\n2.1 ConsistencyModels\nConsistencymodels[16],reliesoncontinuous-timedefinitionofdiffusionmodelswhichgeneratedatabygradually\ntransformingthedatadistribution,p ,intoanoisedistributionusingstochasticdifferentialequation(SDE).These\ndata\nmodelsthenlearntoreversethisprocesstogeneratedatafromnoise.Notably,theSDEhasarelatedordinarydifferential\nequation(ODE)knownastheprobabilityflow(PF)ODE[17].Accordingtodefinitionofscorebasedgenerativemodels,\nPFODEshapesinto\ndx=−σ∇ logp (x)dt,σ ∈[σ ,σ ] (1)\nx σ min max\nfor ∇ logp (x) is score function for perturbed data distribution p (x) ≈ p (x). σ is defined as 0.002 to\nx σ σ σmin min\nmaintain stability and σ = 80 is chosen as reasonable by [6] to provide p (x) ≈ N(0,σ2 I). Solving the\nmax σ max\nprobability flow ODE from σ(t+1) to σ(t) allows the model to transform a sample x ≈ p to x ≈ p .\nσt+1 σt+1 σt σt\nTheprovidedbijectivemappingbetweenx ≈p (x)andx ∼p (x)≈p (x)maintainsconsistencyand\nσ σ σmin σmin data\nwhichisdenotedasf∗ :(x ,σ)(cid:55)→x .\nσ σmin\nAconsistencymodelwhichdenotedasf (x,σ)isparameterizedtomeetboundaryconditionandtransformingitintoa\nθ\ndifferentiableform,itisparameterizedasitisdefinedin[16].\nf (x,σ)=c (σ)x+c (σ)F (x,σ), (2)\nθ skip out θ\nWhere F (x,σ) is a free-from network. To train consistency models, probability flow ODE is discretized using\nθ\nnoise sequences are ranging from σ to σ = σ . Discretization of these noise sequences is denoted as\nmin max N\n(cid:16) (cid:16) (cid:17)(cid:17)ρ\nσ = σ1/ρ+ i−1 σ1/ρ−σ1/ρ fori∈ 1,N ,ρ=7in[6].\ni min N−1 max min\n(cid:74) (cid:75)\nConsistencymodelsaretrainedusingtheconsistencymatchingloss\nE(cid:2)\nλ(σ )d(f (x˜ ,σ ),f (x˜ ,σ\n))(cid:3)\n, (5)\ni θ σi+1 i+1 θ− σi i\n3\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\nwhere\n(cid:12)\nx˜\nσi\n=x\nσi+1\n−(σ i−σ i+1)σ i+1∇ xlogp σi+1(x)(cid:12)\nx=xσi+1\n(6)\nisasingle-stepinthereversedirectionsolvingthePFODEinEq. (1). Here,λ(·)isapositiveweightingfunction,and\nd(·,·)isdeterminedas1in[16]. ItissuggestedusingtheLearnedPerceptualImagePatchSimilarity(LPIPS)asthe\nmetricfunction[16]. TheexpectationinEq. (5)istakenwithrespecttoi∼U[1,N −1],auniformdistributionover\nintegersi=1,...,N−1,andx ∼p (x). TheobjectiveinEq. (5)isminimizedviastochasticgradientdescent\nσi+1 σi+1\nonparametersθwhileθ−isupdatedusingtheexponentialmovingaverage(EMA)\nθ− ←stopgrad(µθ−+(1−µ)θ), (7)\nwhere0≤µ≤1isthedecayrate. f andf arereferredtoasthestudentandteachernetworks,respectively.\nθ θ−\nThetrainingtechniqueforconsistencymodelscanbedividedintotwocategories: consistencydistillation(CD)and\nconsistencytraining(CT).Theconsistencydistillationtechniquereliesonutilizingapre-trainedEDMmodelasa\nteachermodel,denotedasf (x˜ ,σ ).\nθ− σi i\nConsistencydistillation(CD)trainingtechniquesurpassesconsistencytraining(CT)inFIDresultswhencompared. It\niscrucialthatthebestresultsforamodeltrainedwithCDcanonlymatchthebestresultsoftheteachermodeltrained\nwithEDM.Toovercomethisobstacle,[15]suggestsanimprovedtrainingtechniqueforCT,whichgeneratesbetterFID\nresults.\n2.2 ImprovedTrainingTechniquesforConsistencyModels\nImproved training techniques for CT moves one step forward isolation training for consistency models [15]. The\nmodificationsareutilizedforimprovedconsistencymodels(iCM)consistofcurriculum, eliminationofEMAand\nteachermodel,replacingLPIPSlossfunctionwithpseudohuberlossandthenoisedistributionchangedfromuniform\ntolognormalnoisedistribution. Thefirstmodification,settingN to1281inN(k),providesagoodbalancebetween\nbiasandvariancecomparedtoCMtraining. Experimentalresultsshowthatchangings from2to10ands from150\n0 1\nto1280yieldsthebestgenerativeperformanceforiCT.\n(cid:22) (cid:23)\nK\nN(k)=min(s 02⌊ Kk ′⌋ ,s 1)+1, K′ =\nlog [s /s ]+1\n,whereK =1280 (3)\n2 1 0\niCT utilizesthesamenoiseschedulingasitisdescribedin[6]andEquation4whichemphasizeshighweightedlow\nnoiselevelsandcorrespondstop(logσ)=σ σ1/ρ−1 asN →∞..\nρ(σm1/ aρ x−σ m1/ iρ n)\n(cid:18) i−1 (cid:16) (cid:17)(cid:19)ρ\nσ = σ1/ρ + σ1/ρ −σ1/ρ fori∈ 1,N , andρ=7, (4)\ni min N −1 max min (cid:74) (cid:75)\nBesidestheexponentialcurriculumandKarrasnoisescheduling,toemphasizelowernoiselevelsinthenoisedistribution\nduringtraining,themodificationemployedforiCMincludesalog-normalnoisedistributiononimagebatches,which\nsignificantlyassignslowweightstohighnoiselevels.\n(cid:18) (cid:19) (cid:18) (cid:19)\nlog(σ )−P log(σ )−P\np(σ )∝erf i√+1 mean −erf √i mean , (5)\ni\n2P 2P\nstd std\nwhereP = −1.1andP = 2.0. Thislog-normalnoisescheduleleveragessamplingqualityandsignificantly\nmean std\ndecreaseFIDscores. Additiontolognormalnoisedistribution,toincreasetheemphasizeonlowernoiselevelswhichis\nprovidedbylognormalnoisedistribution,thelossweightingisadjustedas\n1\nλ(σ )= . (6)\ni σ −σ\ni+1 i\nTherefinedweightingfunctionnotablyimprovessamplequalityinconsistencytrainingbyassigningsmallerweightsto\nhighernoiselevels. Thisapproachaddressestheissueofthedefaultuniformweightingfunction,whichassignsequal\nweightstoallnoiselevelsandisfoundtobesuboptimal. Byreducingtheweightingasnoiselevelsincrease,thenew\nmethodensuresthatsmallernoiselevels,whichcaninfluencelargerones,areweightedmoreheavily.\nImprovedtrainingtechniquesforCTeliminatesteachermodelandEMAupdateduringtraining. Theunderlyingreason\nforeliminationofEMAandteachermodelisunbiasedtrajectorymappingbetweenstudentandteachermodelandthe\ndecayrateisupdatedasµ(k)=0. ThelossfunctionLPIPSemployedforCTisreplacedwiththepseudo-Huberloss\nduetoundesirablebiasinevaluation.\n(cid:113)\nd(x,y)= ∥x−y∥2+c2−c (7)\n2\n4\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\n3 Methods\n3.1 Architecture\nThe proposed approach, High Noise Improved Consistency Training (HN-iCT), shares the same backbone as the\nconsistencymodelsusedforimagegeneration. Wecategorizedthisapproachintotwosections: thefirstfocuseson\nunconditionalimagegeneration,whichinvolvesmodificationstothecurriculumandnoisedistribution,whilethesecond\nis conditional image generation (HN-iCT-CN), which requires a different architecture in addition to the proposed\ncurriculumandbetanoisedistribution,asshowninFigure1.\nForunconditionalimagegeneration,HN-iCTreliesonthesameU-Netarchitectureasimplementedforconsistency\nmodels[16]. Forimageconditionalgeneration,themostwell-knowntechniqueislabelembeddingorclassembedding.\nGenerally,thistypeofembeddingisimplementedbysummingencodedfeaturesirrespectiveofthedimensionsofthe\ninputsize. Inothercases,suchaswhenanimageisgivenasconditiontoamodel,itisimportanttochoosewhattypeof\nembeddingwillbeemployedwhiletraining. Inthisstudy,AG(AttentionGate)modulesareutilizedtoextractcommon\nspatialfeaturesbetweentheimagechosenasconditionandinputimage[13]. Furthermore,AGmodulesaremodified\nasWeigtedAttentionGate(WAG)topreventincompatibilitybetweenotherU-NETcomponentsandadaptedaccording\ntoHN-iCTmodel. Figure1representsgeneralarchitectureusedforimageconditionaltraining,comprehendsiAG\nmodules.\nWeightedAttentionGate(WAG):Duringtheencodingprocess, graduallydownsampledfeaturesretainessential\nstructuralinformationabouttheimagethatwillbedenoisedinthedecodingprocess. Whileitispossibletoreconstruct\nthedenoisedimagefromthelatentspace,theoutputmayincludenoisypixelsduetothelossofstructuraldetails. To\nachieveacleanimageattheendofthedecodingprocessandpreservestructuralinformation,itisnecessarytoevaluate\ntheskipconnectionsanddecodedfeatures. Theattentiongatesaredesignedtoassessbothglobalstructuralinformation\nandpixel-wisedetailstoreconstructdenoisedimageseffectively. Inourimplementation,theWeightedAttentionGate\n(WAG)incorporatesalearnableweightingmechanism,wheretheattentionmapissquaredtosharpencommonfeatures\nbetweentheskipconnectionandtheconditionedinput. Thegateensuresthattheskipconnectionsdonotdominatethe\nreconstructionwhilepreservingthespatial-temporalfeatures. Additionally,extractedfeaturesfromtheconditioned\ninputarescaledbyaweightparameter(defaultingto0.8)andcombinedwiththeskipconnectionoutputtoachievea\nbalancedreconstruction. ThearchitectureofWAGisrepresentedinFigure2.\nFigure2: TheillustrationofWAGunitusedinproposedarchitecture. Weight(w)parameterisdeterminedas0.8.\n3.2 TrainingTechnique\nHighNoiseLevelEffectonPerformance. Noisedistributioninmini-batchesplaysacrucialroleinteachingthemodel\nthetrajectoriesbetweennoiselevels. Toprovideahigh-confidencetrajectory,itisimportanttohaveanoisedistribution\nthatincludesawidevarietyofnoiselevels. Anoisedistributionwithahighvarietyofnoiselevelsincreasesthenumber\nofhighnoiselevelsinthedistribution,incontrasttothelog-normalnoisedistributionusediniCTtrainingtechniques\n[15].\nIn this section, we evaluate the experimental results conducted within the scope of adding extra high noise levels\nmanuallytoschedulednoiseswithalog-normaldistribution. Asitisclaimed’HighNoisesisaMust’,theexperiments\nbeginbyaddinghigh-noiselevelsgraduallyonthemini-batchesbasedonpercentageratioofthemini-batchsize. This\n5\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\nexperimentsprovesthatnecessitiesofanoiseschedulingcomprisinghigh-levelnoisesbesidehighweightedlow-level\nnoisesinanoisescheduling. Highnoiselevelsrangingbetween40and80arerandomlyaddedtomini-batchesto\nincreasetheweightofthesehighnoiselevels. Theexperimentsconductedwithadditionalhighnoiselevelsreveal\nthatdenoisingperformancesignificantlyincreaseswhenthehighnoiselevelsconstituteupto4%ofthemini-batches.\nAdditionalexperimentaldetailsareprovidedinappendixA.\nBetaDistribution.Thebetadistributionisacontinuousprobabilitydistributiondefinedontheinterval[0,1],commonly\nused to model the behavior of random variables that are constrained within this range. It is parameterized by two\npositiveshapeparameters,denotedasα(alpha)andβ (beta),whichdeterminethedistribution’sshape. Theprobability\ndensityfunction(PDF)ofthebetadistributionisgivenby:\nxα−1(1−x)β−1 (cid:90) 1\nf(x;α,β)= where B(α,β)= tα−1(1−t)β−1dt (8)\nB(α,β)\n0\nfor0≤x≤1. Theparametersαandβ influencetheskewnessandkurtosisofthedistribution,allowingforawide\nrangeofshapesincludinguniform,U-shaped,andJ-shapeddistributions. Thisflexibilitymakesthebetadistribution\nparticularly useful in Bayesian statistics, where it is often employed as a prior distribution for probabilities and\nproportions.\nTheflexibilityofthebetadistributionmakesithighlyadaptableforvariousnoisedistributions,particularlyforthose\naimingtoadjusttheweightofnoiselevelswithinamini-batch[22]. Particularly,itispossibleincreasingtheweightof\nhighlevelnoisesindistributionupto4%byadjustingα=0.5andβ =5. CLAIMandmodify: Thesetofparameters\nwereempiricallydeterminedbyusingCIFAR10.\nSinusoidal Curriculum. The proposed sinusoidal curriculum, inspired by the sinus function, offers a significant\nadvancementinthetrainingofconsistencymodelsbyintroducingasmooth,continuousprogressionofthenumber\noftimestepsforeachnoisedistributiononmini-batch. Unlikeimprovedcurriculumthatrelyonabruptorstepwise\nchanges, thesinusoidalcurriculumleveragesthenaturaloscillationofthesinefunctiontomodulatethevarietyof\nnoiselevelsencounteredduringtraining. Thisgradualadjustmentensuresthatthemodelexperiencesaconsistentand\nwell-distributedrangeofnoiselevels,enhancingthestabilityandrobustnessofthelearningprocess.\n(cid:18)(cid:24)(cid:12)(cid:18) (cid:18) (cid:19)(cid:19) (cid:12)(cid:25) (cid:19)\n(cid:12) π·3·k (cid:12)\nN(k)=min (cid:12)\n(cid:12)\ns 1·sin\n2·K\n+s 0(cid:12)\n(cid:12)\n+1,s 1+1 (9)\n(cid:18) (cid:19)\nπ·i\nσ(i)=sin ·∆t+t (10)\n2·N 0\nThesinusoidalcurriculumisgovernedbykeyparameters. Theinitialtimesteps setsthestartingpoint, whilethe\n0\ndifferencebetweenfinalnumberoftimestepsandinitialthenumberoftimesteps∆t=s −s controlstheamplitude\n1 0\nofthesinusoidalcurve. ThetotalnumberoftimestepsN definestheschedulelength,withthesinefunctionscaledby\nπtoensureasmooth,gradualadjustmentacrosstimesteps. Topreventmarginalincreases,s ands aresetto20and\n0 1\n250,respectively,providingbroadyetstablecoverageofnoiselevelsandenhancingtrainingstability.\n4 Experiments\n4.1 ExperimentalSetup\nDatasets. Forunconditionalimagegeneration,weconsiderCIFAR10[8],whichincludes50Ktrainingimages;CelebA\n64x64[11],whichcontains162,770trainingimagesofindividualhumanfaces;andButterflies[18]256x256dataset,\nwhichprovides1Kimageswithdifferentspeciesofbutterflies.\nForimageconditionallearning,thedatasetsourcedfromtheMayoClinic,asusedintheAAPMlow-doseCTgrand\nchallenge,wasutilized[12]. Thedataisreconstructedona512x512pixelgridwithaslicethicknessof1mmanda\nmedium(B30)reconstructionkernel. Thefirsteightpatientsprovidedtrainingdata,resultinginatotalof4800slices,\nwhiletheremainingtwopatientswereusedforvalidation,contributingatotalof1136slices. Thisdemonstratesthe\nmodel’seffectivenessonmedicaldataandvalidatesthattheproposedtrainingtechniqueyieldsgoodresultsacross\ndifferentdatasets.\nImageConditioning. TheimageconditioningtechniqueemployedinourmodeltakesLowdose512x512reconstructed\nCT slices as a condition for denoising process. No additional pre-processing is applied on low dose slices before\nsubmittingitasacondition. Itisasimilarapproachsuchaslabelembedding.\n6\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\nModelConfigurations. ItisidentifiedseveralconfigurationsforHN-iCTmodel. Thisconfigurationsdependson\ncapacityofthehardwarefortrainingthemodel.Additionally,theconfigurationsforothermodelsutilizedinexperiments\nareprovidedinTable1. Tomakeafaircomparison,allhyperparameterssuchasoptimizertype,learningrateand\ndropoutkeptassamewithHN-iCTmodelexceptthenumberofresidualblockswhichdependsonhardwarelimitations.\nThesmallestmodelconfigurationofHN-iCTisutilizedinourexperimentstoprovestabilityandrobustnessofour\napproach. OtheroptionsoftheHN-iCTmodelwerenotusedduetohardwarelimitationsandextendedtrainingtime,\nbuttheseoptionsareavailableinourpublicrepositoryforresearcherswhowishtoexperimentfurther.\nModelName #NumberofRes. Blocks BaseChannelSize #Params\nUnconditionalModel\nHN-iCT-Small 2 128 234M\nHN-iCT-Small/H(HighRes.) 2 192 527M\nHN-iCT-Mid 4 128 373M\nHN-iCT-Large 6 128 512M\nCT 4 192 897M\niCT 4 192 897M\nImageConditionedModel\nHN-iCT-CN-Small 2 32 14M\nHN-iCT-CN-Mid 4 32 22M\nTable1: ModelConfigurationsforunconditionalimagegenerationandimageconditionedgeneration\nTraining. WeusetheRAdamoptimizerandlearningrateisadjustedas1-e4foralldatasets. Forunconditionalimage\ngeneration,wetrain800KiterationsonCIFAR10andCelebA64x64withabatchsizeof512. Forimageconditional\ngeneration,wetrain400KiterationsonreconstructedCTsliceswith512x512patcheswithabatchsizeof32.\n4.2 UnconditionalImageGeneration\nWecompareHN-iCTmodelwithpriormodelsaretrainedwithCT,iCTtechniqueanddiffusionmodels. Considering\nlackofstudiesbasedonCTandiCTtechnique,weevaluatepubliclyavailablesourcesoniCTandCTtechniquefor\ncomparingwithourapproach. Themodelsaregatheredfrompublicresourcesarereferredasnumberofversionsfrom\nv1tov3intable2. Additionally,torecoverfairnessofcomparisonbetweenmodels,allmodelscomparedintable2are\ntrainedlocally. AsshowninTable2,HN-iCTperformsbetterFIDresultswhenitiscomparedtheothermodelswhich\narepubliclyavailableandofficialmodels. Weemployed2numberofresidualblocksinsteadof4toproverobustnessof\nourproposedtrainingtechnique.\nCIFAR1032x32 CelebA64x64\nModel #ResidualBlocks NFE FID↓\nDDPM[5] 4 1000 3.17 3.26\nDDIM[14] 4 10 13.36 17.33\nEDM[7] 4 35 2.04 3.32\nCT[16] 4 1 14.32 18.72\nCT-v12 4 1 15.54 19.66\niCT[15] 4 1 13.50 15.60\niCT-v11 4 1 14.72 18.54\niCT-v23 4 1 13.03 16.03\nHN-iCT-Small 2 1 10.50 12.31\nTable2: ModelPerformanceonCIFAR1032x32andCelebA64x64,α=1.5,β =5\n1iCT-v1:https://github.com/Kinyugo/consistency_models\n2CT-v1:https://github.com/cloneofsimo/consistency_models\n3CT-v2:https://github.com/junhsss/consistency-models\n7\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\n4.3 ImageConditionedGeneration\nInthissection, ourproposedimageconditionedconsistencymodel(HN-iCT-CNSmall)whichemploysWeighted\nAttentionGate(WAG)modulesisutilizedforenhancinglow-doseCTscans. ExperimentalresultsareshowninTable3\nandcomparedwithrelatedstudiesutilizingsameLDCTdataset[12].\nModelName LPIPS(↓) SSIM(↑) PSNR(↑) NFE(↓)\nLDCT 0.152±0.02 0.94±0.02 41.50±1.5\nDDPM[19] 0.032±0.02 0.97±0.02 43.11±1.6 50\nEDM[7] 0.053±0.01 0.97±0.01 43.80±1.2 79\nDDIM[10] 0.053±0.01 0.81±0.01 35.35±1.2 10\nCD[16] 0.065±0.01 0.94±0.01 42.00±0.8 1\nPFGM++[20] 0.055±0.01 0.96±0.01 43.6±0.8 79\nPS-PFCM[4] 0.061±0.01 0.96±0.01 43.00±0.8 1\nHNiCTCNSmall\nα=0.5,β=5 0.016±0.01 0.96±0.01 43.68±1.2 1\nHNiCTCNMedium\nα=0.5,β=5 0.017±0.01 0.96±0.01 44.07±1.4 1\nTable3: QuantitativeResults,copiedfromotherpaper.\nWeexploredthat,effectivenessofthenoisedistributionondenoisingperformancedependsandhighlyrelatedonα\nparameter,toadjustthebalancebetweenlowandhighnoiselevelsingeneratednoiselevels. Thisrevealsacorrelation\nbetweenbatchsizeandtheαparameter,whereareductioninbatchsizeleadstoadecreaseinα,directlyimpactingthe\noverallperformance. Basedontheresultsfromdifferenttests,theoptimalαvalueforachievingthebestdenoising\nperformanceissetto0.5.Additonally,experimentaldenoisingsamplesfromLowDoseCTvalidationsetarerepresented\ninFigure3.\n4.4 AblationStudy\nInthissection,wepresentanablationstudyconductedontheimprovedcurriculum,sinusoidalcurriculum,log-normal\nnoisedistribution,andbetanoisedistribution. ThemodelconfigurationusedfortheablationsisdesignatedasHN-\niCT-Small,andthetrainingstepsarekeptthesameat400Kforeachconfiguration,asshowninTable4. Eachtraining\nconfigurationfortheablationsisrepresentedbyversionnumbersfromHN −iCT −S toHN −iCT −S ,with\nv1 v4\ntwodifferentoptionsconfiguredwithinthemaximumpossibletransformationconfigurations.\nModelVersions Cifar10 Components\nFID CurriculumType NoiseDistribution\nHN−iCT −S 21.19 Improved Log-Normal\nv1\nHN−iCT −S 17.31 Improved Beta\nv2\nHN−iCT −S 17.85 Sinusoidal Log-Normal\nv3\nHN−iCT−Small 10.51 Sinusoidal Beta\nTable4: Ablationstudyoncurriculumandnoisedistribution,evaluatedat400Ktrainingsteps. β=1.5,α=5forBeta\ndistribution\n5 DiscussionandConclusion\nOurenhancementstonoiseschedulingandcurriculumaddresstheneedforabalancednoisedistributionandcontrolled\nprogressionofnoisestepswithinthecurriculum. Weexaminedtheimpactofthehighnoiselevelsinanoisedistribution\n8\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\nFigure3: Representationofdenoisedslicesamplesfromthelow-doseCTvalidationset. Thefirstcolumnfeatures\nfull-doseslicesamples,themiddlecolumnincludeslow-doseslices,andthelastcolumnpresentssingle-stepdenoising\nsamplesprocessedbytheHNiCTCNmodel. 1mmslices,Windowsetting[-160,240]HU\ncomprisingthehigh-weightedlownoiselevelsbybetadistribution. Also, proposedcurriculumwhichisbasedon\nsinusoidalfunctionalsoprovidesastabilitywithminorchangesonthenumberofnoisestepsforeachtrainingsteps.\nAlthoughthesinusoidalcurriculumandbetadistributionhaveasignificanteffectonthedenoisingperformanceof\nunconditionalimagegeneration,theyneedtobeextendedwithdifferentvaluesfortheparametersβ,α,s ,ands . The\n0 1\nbiggestobstacleiswidelyknowntobethetrainingtime.\nInaspectofmedicalimageprocessing,imageconditionedarchitecturehasmadeitpossibletoprocessmedicalimages\nwithout resizing, by decreasing the number of learnable parameters around 80%. Also, it should be regarded that\nweightedAttentionGatemoduleshavevaluableimpactonleveragingdenoisingperformanceformedicalimages. This\nexperimentalapproachconductedonconsistencymodelshasapotentialtocombinedifferentimagesgivenascondition\nandinput,andmayleadvariousapplicationsbytuningparametersonWAGmodule.\n9\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\nInconclusion,itispresentedanewcurriculumscheduleandnoisedistributionforconsistencymodels,whichprovides\nflexibilityondistributionofnoiselevelsinamini-batchbyadjustingαandβ parameters. Thesuggestedapproach\nforunconditionalimagegenerationmakesitpossibletogeneratebetterqualityimageswithlowermodelsizeandthe\nmodelcanlearneasilytrajectoryfromthepriornoisedistributiontoposteriordistributionofnoise. Theresultsindicate\nthattuningtheweightingparameterintheWAGmodule,aswellastheβ andαparametersinthecurriculumandthe\nbetanoisedistributionalgorithm,allowsustooutperformthecorrespondingconsistencymodels.\n6 Acknowledgements\nTheprojectdescribedwassupportedbytheNIHNationalCenterforAdvancingTranslationalSciencesthroughgrant\nnumberUL1TR001998. Thecontentissolelytheresponsibilityoftheauthorsanddoesnotnecessarilyrepresentthe\nofficialviewsoftheNIH.\nReferences\n[1] Florinel-AlinCroitoru,VladHondru,RaduTudorIonescu,andMubarakShah. Diffusionmodelsinvision: A\nsurvey. IEEETransactionsonPatternAnalysisandMachineIntelligence,45(9):10850–10869,September2023.\n[2] PrafullaDhariwalandAlexNichol. Diffusionmodelsbeatgansonimagesynthesis,2021.\n[3] PFumeneFeruglio,CVinegoni,JGros,ASbarbati,andRWeissleder. Blockmatching3drandomnoisefiltering\nforabsorptionopticalprojectiontomography. PhysicsinMedicineandBiology,55(18):5401–5415,August2010.\n[4] DennisHein,AdamWang,andGeWang. Poissonflowconsistencymodelsforlow-dosectimagedenoising,\n2024.\n[5] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels,2020.\n[6] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\ngenerativemodels. 2022.\n[7] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\ngenerativemodels,2022.\n[8] AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. pages32–33,2009.\n[9] Zhoubo Li, Lifeng Yu, Joshua D. Trzasko, David S. Lake, Daniel J. Blezek, Joel G. Fletcher, Cynthia H.\nMcCollough, and Armando Manduca. Adaptive nonlocal means filtering based on local noise level for ct\ndenoising: Adaptivenonlocalmeansfilteringforctdenoising. MedicalPhysics,41(1):011908,December2013.\n[10] XuanLiu,YaoqinXie,JunCheng,SonghuiDiao,ShanTan,andXiaokunLiang. Diffusionprobabilisticpriorsfor\nzero-shotlow-dosectimagedenoising,2023.\n[11] ZiweiLiu,PingLuo,XiaogangWang,andXiaoouTang. Deeplearningfaceattributesinthewild. InProceedings\nofInternationalConferenceonComputerVision(ICCV),December2015.\n[12] CynthiaH.McCollough,AdamC.Bartley,RickeyE.Carter,BaiyuChen,TammyA.Drees,PhillipEdwards,\nDavidR.Holmes,AliceE.Huang,FarhanaKhan,ShuaiLeng,KyleL.McMillan,GregoryJ.Michalak,KristinaM.\nNunez,LifengYu,andJoelG.Fletcher. Low-dose<scp>ct</scp>forthedetectionandclassificationofmetastatic\nliverlesions: Resultsofthe2016lowdose<scp>ct</scp>grandchallenge. MedicalPhysics,44(10),October\n2017.\n[13] OzanOktay,JoSchlemper,LoicLeFolgoc,MatthewLee,MattiasHeinrich,KazunariMisawa,KensakuMori,\nSteven McDonagh, Nils Y Hammerla, Bernhard Kainz, Ben Glocker, and Daniel Rueckert. Attention u-net:\nLearningwheretolookforthepancreas,2018.\n[14] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels,2020.\n[15] YangSongandPrafullaDhariwal. Improvedtechniquesfortrainingconsistencymodels. 2023.\n[16] YangSong,PrafullaDhariwal,MarkChen,andIlyaSutskever. Consistencymodels. 2023.\n[17] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-basedgenerativemodelingthroughstochasticdifferentialequations. 2020.\n[18] JosiahWang,KatjaMarkert,andMarkEveringham. Learningmodelsforobjectrecognitionfromnaturallanguage\ndescriptions. InProceedingsoftheBritishMachineVisionConference,2009.\n[19] WenjunXia,QingLyu,andGeWang. Low-dosectusingdenoisingdiffusionprobabilisticmodelfor20×speedup,\n2022.\n10\nENHANCINGLOWDOSECOMPUTEDTOMOGRAPHYIMAGESUSINGCONSISTENCYTRAINING\nTECHNIQUES APREPRINT\n[20] YilunXu,ZimingLiu,YonglongTian,ShangyuanTong,MaxTegmark,andTommiJaakkola.Pfgm++:Unlocking\nthepotentialofphysics-inspiredgenerativemodels,2023.\n[21] XinYi,EktaWalia,andPaulBabyn. Generativeadversarialnetworkinmedicalimaging: Areview. Medical\nImageAnalysis,58:101552,December2019.\n[22] MingyuanZhou,TianqiChen,ZhendongWang,andHuangjieZheng. Betadiffusion,2023.\nAppendices\nA HighNoiseLevelExperimentalDetails\nTheexperimentsrevealsthataddingminorweightedhighnoiselevelsonmini-batchesincreasedenoisingperformance.\nAsitisrepresentedintable5,whileaddinghighlevelnoiselevelswithlowerpercentageratioscanenhancedenoising\nperformance,addinghighlevelnoiseat10%ofthemini-batchlengthhaseffectsondenoisingperformanceconversely.\nTable5: Evaluationofdenoisingperformanceofmodelstrainedwithnoiseschedules, includinghighnoiselevels\nproportionaltothesizeofmini-batches.\nModelVersions BatchSize TrainingSteps NumberofRes.Blocks HighLevelNoiseRatio N NoiseScheduling AttentionResolutions FIDScore\niCTv1 1024 100000 2 0% 100 Log-Normal [16,8] 66.51\niCTv2 1024 100000 2 2% 100 Log-Normal [16,8] 75.34\niCTv3 1024 100000 2 3% 100 Log-Normal [16,8] 43.81\niCTv4 1024 100000 2 4% 100 Log-Normal [16,8] 40.32\niCTv5 1024 100000 2 5% 100 Log-Normal [16,8] 73.24\nTheresultsrepresentedinTable5demonstratethatimplementingalog-normalnoisedistributionwithhighnoiselevels\nenhancesdenoisingperformancewhenthehigh-levelnoisesreach5%ofthetotalnumberofmini-batchsize. Basedon\ntheseresults,weproposebetanoisedistributionwhichrepresentshigh-weightedlownoiselevelsandlow-weighted\nhighnoiselevels. Additionally,betadisributionprovidesaflexibilitytoadjustontheratioofnoiselevelsbyadjustingβ\nandαparametersasitdesired.\n11",
    "pdf_filename": "Enhancing_Low_Dose_Computed_Tomography_Images_Using_Consistency_Training_Techniques.pdf"
}