{
    "title": "Provable unlearning in topic modeling and downstream tasks",
    "abstract": "Machine unlearning algorithms are increasingly important as legal concerns arise around the provenance of training data, but verifying the success of unlearning is often difﬁcult. Provable guarantees for unlearning are often limited to supervised learning settings. In this paper, we provide the ﬁrst theoretical guarantees for un- learning in the pre-training and ﬁne-tuning paradigm by studying topic models, simple bag-of-words language models that can be adapted to solve downstream tasks like retrieval and classiﬁcation. First, we design a provably effective unlearn- ing algorithm for topic models that incurs a computational overhead independent of the size of the original dataset. Our analysis additionally quantiﬁes the dele- tion capacity of the model – i.e., the number of examples that can be unlearned without incurring a signiﬁcant cost in model performance. Finally, we formally extend our analyses to account for adaptation to a given downstream task. In par- ticular, we design an efﬁcient algorithm to perform unlearning after ﬁne-tuning the topic model via a linear head. Notably, we show that it is easier to unlearn pre-training data from models that have been ﬁne-tuned to a particular task, and one can unlearn this data without modifying the base model. 1 INTRODUCTION Modern-day machine learning has shifted from single-stage supervised learning on manually constructed datasets to a paradigm in which models are pre-trained and subsequently ﬁne- tuned (Bommasani et al., 2022). In this setting, a model initially learns a good representation of the data using a self-supervised objective on a large unstructured corpus. The resulting pre-trained model is later adapted to solve speciﬁc tasks for which it is difﬁcult or costly to curate a large dataset. This blueprint has yielded strong performance in text (e.g., Devlin et al., 2019; Brown et al., 2020), vision (e.g., Oquab et al., 2024; He et al., 2022), and multimodal (e.g., Radford et al., 2021; Zhai et al., 2023) settings. It is well-known that the scale of the pre-training data is strongly corre- lated with the ﬁnal performance of the model (Hoffmann et al., 2022), leading to the construction of larger datasets via broad internet scrapes (Gao et al., 2020; Schuhmann et al., 2022; Soldaini et al., 2024; Penedo et al., 2023). Such datasets have been found to often inadvertently include private, sensitive, and unsafe data (Birhane et al., 2021; Longpre et al., 2024; He et al., 2024). Unsafe data can generally degrade model performance and introduce biases, making the model less useful for various applications (McKenna et al., 2023; Birhane & Prabhu, 2021; Choenni et al., 2021; Naous et al., 2024). Using private and sensitive data, even unknowingly, poses legal risks (Bommasani et al., 2022; Henderson et al., 2023). In particular, recent works have shown that models can memorize and thus permit the extraction of training data (Somepalli et al., 2023; Carlini et al., 2021; 2023). Moreover, one may be requested to remove data in accordance with GDPR’s right to be forgotten (European Parliament & Council of the European Union), or as part of a copyright-related lawsuit (Tremblay v. OpenAI, Inc.,, 2023; DOE 1 v. GitHub, Inc., N.D. Cal. 2022). Therefore, there is great empirical interest in developing machine unlearning algorithms that can surgically remove portions of the training data from an already learned model without harming per- formance. The gold standard for machine unlearning is for the model to behave as though it had never been trained on that datapoint (Cao & Yang, 2015). As it is often undesirable to completely 1",
    "body": "arXiv:2411.12600v1  [cs.LG]  19 Nov 2024\nPreprint. Under review.\nPROVABLE UNLEARNING IN TOPIC MODELING AND\nDOWNSTREAM TASKS\nStanley Wei, Sadhika Malladi, Sanjeev Arora\nPrinceton University\n{stanley.wei, smalladi, arora}@princeton.edu\nAmartya Sanyal\nUniversity of Copenhagen\namsa@di.ku.dk\nABSTRACT\nMachine unlearning algorithms are increasingly important as legal concerns arise\naround the provenance of training data, but verifying the success of unlearning is\noften difﬁcult. Provable guarantees for unlearning are often limited to supervised\nlearning settings. In this paper, we provide the ﬁrst theoretical guarantees for un-\nlearning in the pre-training and ﬁne-tuning paradigm by studying topic models,\nsimple bag-of-words language models that can be adapted to solve downstream\ntasks like retrieval and classiﬁcation. First, we design a provably effective unlearn-\ning algorithm for topic models that incurs a computational overhead independent\nof the size of the original dataset. Our analysis additionally quantiﬁes the dele-\ntion capacity of the model – i.e., the number of examples that can be unlearned\nwithout incurring a signiﬁcant cost in model performance. Finally, we formally\nextend our analyses to account for adaptation to a given downstream task. In par-\nticular, we design an efﬁcient algorithm to perform unlearning after ﬁne-tuning\nthe topic model via a linear head. Notably, we show that it is easier to unlearn\npre-training data from models that have been ﬁne-tuned to a particular task, and\none can unlearn this data without modifying the base model.\n1\nINTRODUCTION\nModern-day machine learning has shifted from single-stage supervised learning on manually\nconstructed datasets to a paradigm in which models are pre-trained and subsequently ﬁne-\ntuned (Bommasani et al., 2022). In this setting, a model initially learns a good representation of\nthe data using a self-supervised objective on a large unstructured corpus. The resulting pre-trained\nmodel is later adapted to solve speciﬁc tasks for which it is difﬁcult or costly to curate a large\ndataset. This blueprint has yielded strong performance in text (e.g., Devlin et al., 2019; Brown et al.,\n2020), vision (e.g., Oquab et al., 2024; He et al., 2022), and multimodal (e.g., Radford et al., 2021;\nZhai et al., 2023) settings. It is well-known that the scale of the pre-training data is strongly corre-\nlated with the ﬁnal performance of the model (Hoffmann et al., 2022), leading to the construction of\nlarger datasets via broad internet scrapes (Gao et al., 2020; Schuhmann et al., 2022; Soldaini et al.,\n2024; Penedo et al., 2023). Such datasets have been found to often inadvertently include private,\nsensitive, and unsafe data (Birhane et al., 2021; Longpre et al., 2024; He et al., 2024).\nUnsafe data can generally degrade model performance and introduce biases, making the model\nless useful for various applications (McKenna et al., 2023; Birhane & Prabhu, 2021; Choenni et al.,\n2021; Naous et al., 2024).\nUsing private and sensitive data, even unknowingly, poses legal\nrisks (Bommasani et al., 2022; Henderson et al., 2023). In particular, recent works have shown\nthat models can memorize and thus permit the extraction of training data (Somepalli et al., 2023;\nCarlini et al., 2021; 2023).\nMoreover, one may be requested to remove data in accordance\nwith GDPR’s right to be forgotten (European Parliament & Council of the European Union), or\nas part of a copyright-related lawsuit (Tremblay v. OpenAI, Inc.,, 2023; DOE 1 v. GitHub, Inc.,\nN.D. Cal. 2022).\nTherefore, there is great empirical interest in developing machine unlearning algorithms that can\nsurgically remove portions of the training data from an already learned model without harming per-\nformance. The gold standard for machine unlearning is for the model to behave as though it had\nnever been trained on that datapoint (Cao & Yang, 2015). As it is often undesirable to completely\n1\n\nPreprint. Under review.\nretrain models, especially as they grow larger, many works have proposed computationally cheaper\nheuristics for solving this problem (e.g., Jang et al., 2023; Foster et al., 2024; Kurmanji et al., 2023;\nZhang et al., 2024b; Eldan & Russinovich, 2023; Gandikota et al., 2023). In the absence of theoret-\nical guarantees, it is common to use empirics to measure the success of these algorithms. However,\nrecent works have shown that such evaluations often overestimate the success of these unlearning\nmethods (Hayes et al., 2024; Shi et al., 2024; Maini et al., 2024) and thus it has proven difﬁcult to\nconﬁdently ascertain whether the proposed methods meet the necessary compliance standards. In\nthis context, it is highly desirable to design efﬁcient unlearning algorithms with well-motivated\nguarantees that are salient to the pre-training and ﬁnetuning paradigm (Thudi et al., 2022; Lee et al.,\n2024).\nWhile there are some instances of such algorithms for linear models (Guo et al., 2020; Izzo et al.,\n2021; Mahadevan & Mathioudakis, 2023), general convex models (Ullah et al., 2021; Sekhari et al.,\n2021; Neel et al., 2021), Bayesian models (Nguyen et al., 2020), and GANs (Liu et al., 2024), there\nare no works on the paradigm of pre-training and ﬁne-tuning algorithms. One of the most classical\nsuch algorithms is topic modeling (Hofmann et al., 1999; Blei et al., 2003; Blei & Lafferty, 2006;\nLi & McCallum, 2006), which can also be thought of as the simplest language model. In this paper,\nwe present the ﬁrst provably effective and efﬁcient unlearning algorithms for topic models.\nTopic models are generally pre-trained to extract latent structure (i.e., a small set of underlying top-\nics) from a large corpus of documents. This feature extractor is then used for a variety of down-\nstream applications, including retrieval, classiﬁcation, and recommendation (Boyd-Graber et al.,\n2017). Despite their simplicity, topic models can be used to effectively solve many real-world\nnatural language problems — see a survey in Churchill & Singh (2022).\n1.1\nOVERVIEW OF RESULTS\nWe focus on the setting in Arora et al. (2012b), because it admits an efﬁcient learning algorithm with\nprovable guarantees (Arora et al., 2012a). The corpus is assumed to contain r underlying topics,\nwhere each topic deﬁnes a distribution over words. Let D be a distribution over topic distributions.\nThen, each document d is generated by sampling a topic distribution Wd ∼D over topics, and then\nsampling words according to Wd.\nThe dataset of m documents is a matrix M ∈Rn×m, where M permits a non-negative matrix\nfactorization M = A∗X. Here, A∗∈Rn×r is the distribution of words in each of the r unknown\nunderlying topics, and X ∈Rr×m is the sampled distribution of topics in each document. In\nparticular, A⋆, X have columns on the probability simplex. We seek to learn the embedding function\nA∗and the topic-topic covariance R⋆= ED[XX⊤].\nTo derive provable guarantees on the success of unlearning, we adapt the notion of (ǫ, δ)-unlearning\nintroduced in Sekhari et al. (2021) to the topic modeling setting. The unlearned model is required\nto behave indistinguishably from a model that was retrained on the modiﬁed dataset. We deﬁne a\nnotion of utility-preserving unlearning that combines this condition with an analysis on the deletion\ncapacity – i.e., the number of datapoints that can be unlearned without performance degradation\n(Deﬁnition 4). We now state our main result on utility-preserving unlearning in topic models.\nMain Result 1 (Informal version of Theorem 2). Suppose we trained a topic model AS, XS on a\ntraining set S containing m documents. Algorithm 1 can perform utility-preserving unlearning of\nmU = ˜O\n\u0012\nm\nr2√nr\n\u0013\ndocuments from the pre-trained topic model, where ˜O(·) hides constants depending on the learning\nand unlearning algorithm.\nTo adapt a topic model to a downstream topic classiﬁcation task, we learn a head w ∈Rr on top\nof A to minimize a strongly convex loss function (Deﬁnition 2). When A and w are both released,\none would necessarily have to ﬁrst unlearn from A, which makes unlearning just as hard as it was\nin pre-training (Theorem 3). This setting is rather unrealistic, because there is no obvious case in\nwhich one would want to use w without A or vice versa. We thus advocate for viewing ﬁne-tuned\nmodel B = Aw as a whole i.e. it is not allowed to access outputs of A solely, and we show that it\nis easier to perform utility-preserving unlearning of pre-training data in this case.\n2\n\nPreprint. Under review.\nMain Result 2 (Informal version of Theorem 4). After adapting the model to a downstream task\n(Deﬁnitions 1 and 2), Algorithm 2 can perform utility-preserving unlearning of ˜Ω\n\u0010\nmq\nr√nr\n\u0011\ndoc-\numents, where q ∈[1/r, 1] is a task-dependent quantity, without modifying the base model A.\nSimpler downstream tasks have a larger q, increasing the separation from the pre-training result.\nWe demonstrate that our unlearning algorithms run substantially faster than retraining the model\n(Table 1). Overall, our results imply the following takeaways in the context of topic models —\n(1) It is possible to effectively and efﬁciently unlearn datapoints from a pre-trained model without\nretraining it (Algorithm 1 and Theorem 2), (2) One can effectively unlearn more pre-training data\nfrom a model that has been adapted to a downstream task without harming the utility of the base and\nﬁne-tuned models (Theorem 4), and (3) One can unlearn pre-training data from a ﬁne-tuned model\nwithout modifying the base model (Algorithm 2 and Theorem 4).\n2\nTOPIC MODELS\nAs we previously discussed, topic models can be considered as one of the simplest language models\nthat one can pre-train in a self-supervised fashion and later ﬁne-tune for other language-related\ntasks. This pipeline mirrors the modern-day paradigm of pre-training large language models to\nbuild a general understanding of natural language and later ﬁne-tuning them to solve a variety of\ntasks ranging from classiﬁcation to code generation.\n2.1\nPROBLEM DESCRIPTION\nTopic modeling is a classical, bag-of-words method to discover structure in a corpus of docu-\nments (Hofmann et al., 1999). One assumes that each document contains a convex combination\nof topics, each of which can be described in terms of a distribution over the vocabulary. Different\nassumptions on the structure of this distribution and the topics have yielded a variety of topic mod-\neling methodologies (Blei & Lafferty, 2006; Li & McCallum, 2006) – perhaps most famous among\nthese is the latent Dirichlet allocation (LDA, Blei et al. (2003)). Many early works established the\nstatistical learnability of topic models under such assumptions, but the learning algorithms generally\nwere not efﬁcient in real-world settings (Arora et al., 2012b; Recht et al., 2012).\nOur paper focuses on the setting in Arora et al. (2012b), for which Arora et al. (2012a) provided an\nempirically efﬁcient learning algorithm. The dataset consists of a set of m documents d1, ..., dm,\nwhere each document contains L words from a vocabulary V with |V| = n.1 The corpus contains r\ndifferent underlying topics, each of which deﬁnes a distribution over words. Each word in document\nd is generated by: (1) sampling a distribution over topics Wd ∼D, and then (2) sampling L words\nindependently according to Wd.\nWe represent the corpus as a matrix M ∈Rn×m, where M permits a non-negative matrix factoriza-\ntion M = A⋆X. Here, A⋆∈Rn×r is the distribution of words in each of the r topics, X ∈Rr×m\nis the distribution of topics in each document, and hence M is the distribution of words in each\ndocument. While there are several algorithms for learning the feature extractor A⋆, it is well-known\nthat it is hard to recover X exactly (Arora et al., 2012b). Instead, it is desirable to learn how the\ntopics co-occur together, denoted as R⋆= ED[XX⊤]. This quantity is termed the topic-topic\ncovariance. Further discussion of this has been included in Appendix A.\nThe topic modeling setting generally determines D (e.g., in LDA, D is a Dirichlet distribution). In\norder to recover A∗and R∗efﬁciently and accurately from an observed corpus M ∼D, we need\nto make the following assumption on the underlying data distribution.\nAssumption 1 (p-separability, Arora et al. (2012b)). The topic matrix A⋆is p-separable for p > 0\nif for every topic k ∈[r], there exists a word i ∈[n] such that A∗\ni,k ≥p and A∗\ni,k′ = 0 for all\nk′ ̸= k. Such words are called anchor words.\nWithout this separability assumption, maximum likelihood estimation of a topic model is NP-\nhard (Arora et al., 2012b). Assumption 1 requires that A⋆contains a diagonal matrix, up to row\npermutations; intuitively, the appearance of an anchor word in a document perfectly indicates the\n1Without loss of generality, we assume L = 2.\n3\n\nPreprint. Under review.\ndocument has nonzero probability of the corresponding topic. As we will detail in Section 4, this ob-\nservation inspires a two-phase learning algorithm, whereby one ﬁrst approximates the anchor words\nfor each topic and then leverages them to identify patterns among the topics.\n2.2\nDOWNSTREAM ADAPTATION\nTopic models are frequently trained on a general corpus, and the embeddings can be later used\nto classify documents. The classiﬁcation problem usually involves only a subset of topics. For\nexample, after training a topic model on a large corpus of news articles with diverse topics (e.g.,\nsports, politics, technology, ﬁnance, etc.), one relevant downstream task is to classify the subject of\na given news article as sports or politics. We formalize the topic classiﬁcation task below.\nDeﬁnition 1 (Topic Classiﬁcation Task). A topic classiﬁcation task T = (Tclf, w⋆) is deﬁned by a\nsubset of topics Tclf ⊂[r] on which the task is deﬁned and a ground-truth labelling vector w⋆∈Rr\nwith bounded norm. Importantly, w⋆only has non-zero coordinates in the positions corresponding\nto Tclf.\nThe classiﬁcation task is deﬁned on the latent features of a given document, so it is necessary to ﬁrst\nidentify the salient topics as they occur in the text. Fitting a topic model to the corpus yields such a\nfeature extractor A that embeds a document into the r-dimensional topic space. In order to adapt a\ntopic model to a particular classiﬁcation task, we perform head tuning on the feature extractor A.\nDeﬁnition 2 (Head Tuning). For a given labelled document classiﬁcation dataset Dclf = {(di, yi)}\nrepresenting a topic classiﬁcation task T , embed each document di as a vector xi ∈Rn containing\nthe word counts in the document. To perform head tuning on a pre-trained topic model A, we learn\nw ∈Rr to minimize\nℓT (w; A) =\n1\n|Dclf|\nX\n(x,y)∈Dclf\nf(x⊤Aw, y)\nwhere ℓT is strongly convex in w.\nOne example of f is the logistic loss with ℓ2 regularization. For ease of exposition, we primarily\nconsider binary classiﬁcation tasks, but we point out that the deﬁnition can extend to multi-class\ntasks solved via the one-vs-all scheme (Rifkin & Klautau, 2004).\nWe note that head tuning, also referred to as linear probing, is a simpler adaptation technique than\nﬁne-tuning A alongside w. Nonetheless, recent works on popular language models have demon-\nstrated that head tuning can substantially improve the ability of general pre-trained language models\nto solve complex classiﬁcation tasks (Malladi et al., 2023a;b). Head tuning thus serves as a con-\nvenient yet effective adaptation method that avoids updating the pre-trained model, which is often\ndesirable. For example, if a single pre-trained model needs to be separately adapted to solve many\ndifferent tasks, then it is desirable to minimize the number of parameters that are ﬁne-tuned to min-\nimize the memory needed to store all of the adapted models.2\n3\nUNLEARNING\nAs mentioned previously, there is increased interest in machine unlearning due to the growing scale\nof modern datasets and the difﬁculty of manually inspecting each datapoint. Theoretically, the\ngold standard for unlearning is that the model should behave identically to one that was trained\nwithout the datapoint in its corpus (Cao & Yang, 2015). We ﬁrst deﬁne what it means for two models\nθ1, θ2 ∈Θ to behave almost identically, where Θ denotes the parameter space of a hypothesis class.\nDue to randomness in learning, θ1 and θ2 are random variables.\nDeﬁnition 3 ((ǫ, δ)-indistinguishable models, Dwork et al. (2014)). Two models denoted by random\nvariables θ1, θ2 ∈Θ are (ǫ, δ)-indistinguishable if for all possible subsets of models T ⊆Θ,\nPr (θ1 ∈T ) ≤eǫ Pr (θ2 ∈T ) + δ\nPr (θ2 ∈T ) ≤eǫ Pr (θ1 ∈T ) + δ\nWe denote this as θ1\nǫ,δ\n≈θ2.\n2This motivation has driven widespread development and adoption of parameter-efﬁcient ﬁne-tuning meth-\nods for large language models. Liu et al. (2021) contains a survey of such techniques.\n4\n\nPreprint. Under review.\nWe adapt the deﬁnitions from Sekhari et al. (2021) to the topic modeling setting. A learning al-\ngorithm A takes in a set of m documents, denoted as S, and returns a topic model θ = (A, R).\nAnalogously, an unlearning algorithm U takes in the learned topic model θ, a set of documents to\nunlearn Sf ⊆S, some statistics on the training set T (S), and outputs a model. The set of datapoints\nto unlearn Sf is often referred to as the forget set. With this in mind, we now deﬁne a notion of\nutility-preserving unlearning, whereby the unlearning algorithm needs to not only effectively simu-\nlate retraining the model from scratch but also maintain the model’s performance.\nDeﬁnition 4 (Utility-preserving (ǫ, δ)-Unlearning with Deletion Capacity). Let m0 ∈N be a con-\nstant that depends on the topic modeling distribution D satisfying Assumption 1. For any training\ndataset S\ni.i.d.\n∼D of size at least m0, and ǫ, δ > 0, we say that a pair of learning and unlearning\nalgorithms (A, U) performs utility-preserving unlearning with deletion capacity T A,U\nǫ,δ (m) if\n1. With probability at least 0.9 over draws from D, for any forget set Sf ⊆S of size at most\nT A,U\nǫ,δ (m), model trained on S \\Sf is indistinguishable from that resulting from unlearning\nwith U.\nU(Sf, A(S), T (S))\nǫ,δ\n≈U(∅, A(S \\ Sf), T (S \\ Sf))\n2. Even for an adversarially chosen Sf, the unlearned model does not suffer a large perfor-\nmance degradation. Formally,\nEA,U\n\"\nmax\n|Sf|≤T A,U\nǫ,δ\n(m)\nh(U(Sf, A(S), T (S))) −h⋆\n#\n≤0.01\nwhere h : Θ →R is the loss of the topic model, and h⋆= minw∈W h(w) is the irreducible loss.\nThe above deﬁnition can be applied to both the pre-training and the downstream adaptation stages\nof training a topic model. Of particular note is that (1) does not guarantee (2), since the former\nonly concerns indistinguishability between the unlearned and retrained models, while the latter is\na statement about utility preservation. Moreover, unless T (S) contains the entire dataset, we note\nthat the unlearning algorithm U cannot be as simple as retraining the model. In this paper, we will\ndesign an unlearning algorithm for topic models that satisﬁes this deﬁnition of provable unlearning,\nand the number of statistics T (S) will not depend on the initial dataset size m.\nTo show (ǫ, δ)-indistinguishability, we utilize the Gaussian mechanism, a classic tool from differen-\ntial privacy. Given a particular function, the Gaussian mechanism essentially prescribes how much\nnoise one must add to the output in order for the input to be indistinguishable from a similar one.\nThe guarantee of the Gaussian mechanism is described in the following lemma.\nLemma 1 (Gaussian Mechanism, Dwork et al. (2014)). Let f be an arbitrary d-dimensional func-\ntion, and deﬁne its ℓ2-sensitivity to be ∆2f :=\nmax\nadjacent x,y ∥f(x) −f(y)∥2. Then, for c2 > 2 log 1.25\nδ ,\nthe Gaussian mechanism with parameter σ ≥c∆2f/ǫ is (ǫ, δ)-differentially private.\nIn our case, we deﬁne adjacent inputs (i.e., training datasets) as the case where y is a superset of x.\n4\nLEARNING AND UNLEARNING TOPIC MODELS\nIn this section, we present the learning and unlearning algorithms and guarantees for topic models.\nNotation.\nWe use A⋆to refer to the ground-truth topic model, AS to refer to a topic model trained\non S, and AF to denote a topic model retrained with the forget set removed S \\ Sf. We also use ¯\nA\nto denote the unlearned topic model before applying the Gaussian mechanism and ˜\nA to denote the\nmodel after the mechanism is applied. Analogous notations are used for R.\n4.1\nLEARNING ALGORITHM AND GUARANTEES\nPer Arora et al. (2012a), the learning algorithm Abase takes in a corpus of documents S\n=\n{d1, ..., dm} and consists of the following three phases to learn a topic model θ = (AS, RS).\n5\n\nPreprint. Under review.\n1. Measure the word co-occurrences. Compute the word co-occurrence matrix Q ∈Rn×n,\nwhere Qij is the number of times word i appears in the same document as word j. We also\ncompute ¯Q, which normalizes the rows of Q to sum to 1. A detailed discussion of the con-\nstruction of Q and its relationship to the factorization M = A⋆X is included in Appendix A.\n2. Identify the anchor words P. Recall that in order to be able to learn topic models efﬁciently,\nthere must exist a set of anchor words P with |P| = r, and each anchor word must appear\nexclusively in a single topic (Assumption 1). This subroutine uses ¯Q to approximately identify\nthe r anchor words P.\n3. Learn the feature extractor AS and the topic-topic covariance RS. The algorithm uses the\nanchor words P and the word co-occurrences ¯Q to learn AS and RS. Each word is expressed\nas a convex combination of anchor words, and thus, topics. With appropriate normalization\nand by cross-referencing information with the co-occurrence matrix, one can recover A⋆, R⋆\nin the inﬁnite data limit.\nWe sketch how this algorithm recovers the ground truth A⋆, R⋆when one has inﬁnitely many doc-\numents in Appendix A. Arora et al. (2012a) gives the following ﬁnite-document guarantee.\nTheorem 1 (Learning Guarantee). Running Abase on a dataset S of size m, where m is at least\nmax\n\u001a\nO\n\u0012 ar3 log n\nL(γp)6ǫ0\n\u0013\n, O\n\u0012a3r3 log n\nLǫ3\n0(γp)4\n\u0013\n, O\n\u0012r2 log r\nLǫ2\n0\n\u0013\u001b\nrecovers AS and RS with entrywise additive error up to ǫ0 from the ground truth A⋆, R⋆, respec-\ntively. Here, a is the topic imbalance parameter, and γ is the condition number of the ground truth\nR⋆. Formally, we have a = maxi,j∈[r] PrD[z = i]/ PrD[z = j].\nApproximating the anchor words. We defer a precise description of the anchor word identiﬁcation\nalgorithm to Appendix A and instead focus here on the intuitions driving its design and the guar-\nantees we will use throughout the paper. First, we note the relationship between ¯Q and the set of\nanchor words. If we had inﬁnitely many documents, then the convex hull of the rows in ¯Q will be a\nsimplex with vertices corresponding to the anchor words, because each anchor word corresponds to\na topic, and each topic prescribes a distribution over words. However, in the ﬁnite document setting,\neach row of ¯Q only approximates their expected value, and so one must approximate the vertices of\na convex hull when given access to a perturbation of the points that deﬁne it.\nWe start by requiring that each topic is distinctly different from any mixture on the other topics.\nFormally, this requires that the simplex is robust, in that each vertex (i.e., anchor word) is sufﬁciently\nfar from any combination of the other topics. Most topic modeling settings deﬁne lower bounds on\nthe robustness of the simplex. By a result in Arora et al. (2012b), the simplex deﬁned by the r\nanchor word rows of the population ¯Q is γp-robust. We can now deﬁne exactly the sense in which\na ¯Q computed on a ﬁnite dataset approximates the population co-occurrence matrix.\nDeﬁnition 5. Let {ai}n\ni=1 be a set of points whose convex hull P is a simplex with vertices {vi}r\ni=1.\nWe say a set of r points is ǫ-close the vertex set {vi}r\ni=1 if each of the r points is ǫ-close in ℓ2\ndistance to a different vertex in P. Moreover, we say that a simplex P is β-robust if for every vertex\nv of P, the ℓ2 distance between v and the convex hull of the rest of the vertices as at least β.\nIn the context of this deﬁnition, P corresponds to the ground truth convex hull, and the ﬁnite sample\n¯Q can be seen as a perturbation to it. In particular, Arora et al. (2012a) used this to established a\nguarantee on the accuracy of anchor word recovery.\nLemma 2 (Approximation Guarantee on Anchor Words). Suppose each row of ¯Q is at most δ\ndistance away from the ground truth γp-robust simplex ¯Q⋆in ℓ2 norm. If 20rδ/(γp)2 < γp, then\nthe set of anchor words found by the algorithm is O(δ/γp)-close to the ground truth anchor words.\nWe now describe how to use the recovered approximate anchor words to learn the topic model.\nLearning the topic model from anchor words. We are given the set of anchor words P, the word\nco-occurrence matrix Q ∈Rn×n, and the normalized co-occurrence matrix ¯Q. Our goal is to use\nthese quantities to learn A ∈Rn×r and R ∈Rr×r. We will do so by ﬁrst expressing each word\ni ∈[n] as a convex combination of the anchor words (and thus, the topics). In particular, for each\nword i, we learn the coefﬁcients Ci ∈∆r as\nCi = arg min\nv∈∆r\n∥¯Qi −v⊤¯QP ∥2\n(1)\n6\n\nPreprint. Under review.\nAlgorithm 1 Unlearning algorithm (Ubase)\nInput: Forget set Sf ⊆S, statistics T (S) which include {CS\ni }n\ni=1, QS, P, normalization con-\nstants pS\nOutput: Unlearned model ˜\nA, ˜R\nCompute the updated co-occurrence matrix QF by subtracting documents in Sf\nStore the updated normalization constants pF = QF 1\nfor i in 1, . . . , n do\nNewton step update on Ci’s:\n¯CF\ni ←CS\ni −H−1\nCS\ni ∇L(CS\ni , S \\ Sf)\n(2)\n¯CF\ni ←proj∆r( ¯CF\ni )\n(3)\nwhere L(v, S \\ Sf) := ∥¯QF\ni −v⊤¯QF\nP ∥2 and HCS\ni = ∇2L(CS\ni , S \\ Sf)\nend for\n¯\nA′ = diag(pF ) ¯C\n¯\nA = column normalized ¯\nA′\n¯R = ¯\nA†QF ¯\nA†⊤where ¯\nA† is the pseudoinverse of ¯\nA\nSample νA, νR from normal distribution deﬁned by Gaussian mechanism guarantee\n˜\nA = Project each column of ¯\nA + νA to ∆n.\n˜R = Project ¯R + νR onto the set of PSD matrices.\nreturn The unlearned topic model ˜\nA, ˜R\nwhere ¯QP is the P rows of ¯Q corresponding to the anchor words. Arora et al. (2012a) showed the\nfollowing approximation guarantee for Ci compared to the ground-truth coefﬁcients.\nLemma 3. When 20rδ/(γp)2 < γp, for every word i, Ci has entrywise error O(δ/(γp)2) from C⋆\ni .\nWe then normalize this Ci by the total number of co-occurrences that word i is involved in. Note that\nthe Ci can be assembled into a matrix C ∈Rn×r. We set A to be C after normalizing the columns\nsum to 1, since the columns represent the topic-conditioned distribution over the vocabulary. We\nﬁnally compute R = A†QA†⊤, where A† denotes the pseudoinverse of A.\n4.2\nUNLEARNING ALGORITHM AND GUARANTEES\nLearning Phase\nRetrain Time\nUnlearning Update\nUnlearning Time\nCo-occurrence matrix computation\nO(m)\nUpdating frequencies\nO(mU)\nIdentify anchor words\nO(n2 + nr/ǫ2\n0)\nUse learned anchor words\nO(1)\nRecover topics from anchors\nO(n2r + nr2/ǫ2\n0)\nProjected Newton step\nO(nr2)\nHead tuning w (Deﬁnition 2)\nERM\nNewton step\nO(r3)\nTable 1: Our unlearning algorithms generally have a runtime shorter than the retraining procedure.\nERM denotes empirical risk minimization, and we note the training time relies on the error tolerance.\nWe describe our unlearning algorithm Ubase to forget a set Sf from a trained model (Algorithm 1),\nwhich crucially updates Ci with a Newton step. We then compute ¯\nA from the modiﬁed Ci and\napply the Gaussian mechanism to ensure indistinguishability. We describe our formal guarantee on\nthe unlearning algorithm below, sketching out our utility preserving guarantees with respect to A⋆.\nThe arguments for R⋆follow analogously; we defer the discussion to the appendix.\nTheorem 2 (Utility-Preserving Unlearning on the Base Model). Let Abase be the learning algo-\nrithm described in the prior sections and Ubase be the unlearning algorithm in Algorithm 1. Then,\n(Abase, Ubase) performs utility-preserving unlearning with deletion capacity\nT Abase,Ubase\nǫ,δ\n(m) ≥c ·\nm\nr2√rn\n(4)\n7\n\nPreprint. Under review.\nwhere m is the number of training documents, r is the number of topics, and c is a constant de-\npendent on ǫ, δ, and D. The loss function h used in the utility-preserving deﬁnition is the maximum\nentrywise error from the ground truth topic model A⋆.\nProof sketch. The full proof can be found in Appendix B.2. We delete mU ≤0.001mǫ0(γp)3\na2r2\npoints.\nThis upper bound ensures that the anchor words are likely unchanged per Lemma 2. Recall that\nutility-preserving unlearning requires: (1) that the unlearned model is indistinguishable from the\nretrained model, and (2) that the unlearned model is not too far from the ground-truth model.\nIndistinguishability. The Gaussian mechanism introduced in Lemma 1 allows us to make two models\nwith a given ℓ2-sensitivity (ǫ, δ)-indistinguishable from each other. We bound the ℓ2-sensitivity of\nthe feature extractor A by noting that ¯\nA is a rescaled version of ¯C.\nLemma 4. For ǫ, δ > 0, the following holds for the ¯C and the topic matrix ¯\nA:\n∥¯C −CF ∥∞≤c · armU\nmǫ0γp\n∥¯\nA −AF ∥∞≤(ar) · ∥¯C −CF ∥∞\n(5)\nApplying the Gaussian mechanism with noise σ = ∆\nǫ\np\n2 log(1.25/δ), where ∆= c√nr · (ar)2mU\nmǫ0γp\nand followed by projecting the columns of ¯\nA + νA back to ∆n yields the desired result.\nUtility Preservation. We ﬁrst apply Lemma 2 to show that, with high probability, the anchor words\ndo not change when unlearning mU documents. Then, we use Lemma 8 to bound the distance\nbetween the unlearned ¯Ci and the ground truth C⋆\ni . Accounting for the noise added via the Gaussian\nmechanism completes the proof.\nLemma 5. For ǫ, δ > 0, denote the unlearned model after the Gaussian mechanism described above\nas ˜\nA. Then, ˜\nA satisﬁes:\nE\nh\n∥˜\nA −A⋆∥∞\ni\n≤c · (ar)2mU\nmǫ0γp\n·\n \n√nr ·\np\nlog(nr) ·\np\nlog(1/δ)\nǫ\n+ 1\n!\n(6)\nEach of the two terms in the above equation yield a constraint on mU.\nIn particular, mU ≤\nmin\nn\n˜O\n\u0010\nm\nr2√nr\n\u0011\n, O\n\u0000 m\nr2\n\u0001o\n, so setting mU ≤˜O\n\u0010\nm\nr2√nr\n\u0011\ncompletes the proof.\n5\nUNLEARNING WITH RESPECT TO A DOWNSTREAM TASK\nWe are interested in unlearning a set of pre-training documents Sf ⊆S. A topic classiﬁcation task\nis usually deﬁned on a subset of the topics in the dataset — for example, if the pre-training corpus\ncontained diverse news articles, one plausible downstream task is to classify the content of a given\ndocument as containing politics or sports. Deﬁnition 1 formalizes this: a topic classiﬁcation task\nT = (Tclf, w∗) is deﬁned on a subset of the topics Tclf and a r-length ground-truth labelling vector\nw∗∈Whead, where w∗only has non-zero values in positions corresponding to Tclf. We describe\ntwo possible settings under which we can show utility-preserving unlearning.\n5.1\nNAIVE SETTING\nIn the ﬁrst setting, the learning algorithm Ahead, naive returns the pre-trained feature extractor A and\nthe head w separately. So, we must ensure that the forget set Sf ⊆S cannot be recovered from either\nA or w. As such, we must necessarily perform unlearning on A as described in Algorithm 1, which\nmeans that unlearning the ﬁne-tuned model is exactly as difﬁcult as unlearning the base model.\nTheorem 3 (Unlearning when releasing A and w). For a downstream task T with loss func-\ntion ℓT , consider the unlearning algorithm Uhead, naive that ﬁrst runs Algorithm 1 to compute\n˜\nA = Ubase(Sf, Abase(S), T (S)), where (Abase, Ubase) performs utility-preserving unlearning (Theo-\nrem 2). Then, it ﬁts a head w = arg minw∈Whead ℓT (w; ˜\nA) and returns ˜\nA and w. We assert that\n(Ahead, naive, Uhead, naive) performs utility-preserving unlearning (Deﬁnition 4).\n8\n\nPreprint. Under review.\nAlgorithm 2 Unlearning algorithm for task T (Uhead)\nInput: Document deletion requests Sf ⊆S, statistics T (S) which include AS, {CS\ni }n\ni=1, QS, P,\ndiag(pS), wS = arg minw∈Whead ℓT (w; AS)\n¯\nA, ¯R = Run Algorithm 1 (Ubase) up to the Gaussian mechanism\n¯w = wS −H−1\nwS∇wℓT (wS; ¯\nA) where HwS = ∇2\nwℓT (wS; ¯\nA)\nreturn (AS)† ¯\nA ¯w + ξ, in accordance with the Gaussian mechanism\nGiven the guarantee on ˜\nA from Theorem 2, we show that this result extends to w by the well-\nknown fact: for ǫ, δ > 0, post-processing indistinguishable quantities (Deﬁnition 3) preserves (ǫ, δ)-\nindistinguishability (Dwork et al., 2014). The full proof of utility preservation can be found in Ap-\npendix C, which essentially boils down to a Lipschitz condition. However, there are some downsides\nto this algorithm. First, it requires retraining the head w for each unlearning request, but we want\nto perform unlearning without access to Dclf. Second, repeatedly noising the base model via the\nGaussian mechanism will erode its utility. We address these issues in the realistic setting.\n5.2\nREALISTIC SETTING\nThere is little reason to release A and w separately after ﬁne-tuning the model, because it is unclear\nwhy one would want to use A without w or vice versa. One can obtain A directly after pre-training\ninstead of relying on a ﬁne-tuned model, and there is little use for w alone, because it is highly\nsensitive to the speciﬁc topics extracted by A and their ordering. As such, we argue for releasing\nthe ﬁne-tuned model as a single matrix3 B = Aw, where B ∈Rn×1.\nTheorem 4 (Utility-Preserving Unlearning on the Downstream Task). Suppose that the downstream\ntask T only depends on a subset of topics Tclf ⊆[r]; that is, w⋆= arg minv∈Wbase ℓT (v; A⋆) has\nnon-zero entries only in the index set Tclf. Denote q := mink∈Tclf PrD[z = k], and let Ahead be\nthe head tuning algorithm (Deﬁnition 2) and Uhead be Algorithm 2. Then, (Ahead, Uhead) performs\nutility-preserving unlearning with deletion capacity\nT Ahead,Uhead\nǫ,δ\n(m) ≥c′ ·\nmq\nr√nr\n(7)\nwhere c′ is a constant dependent on ǫ, δ, D, and T .\nThe full proof is in Appendix C, including the worst case of Tclf = [r]. When the task relies heavily\non every single topic (i.e., q = 1/ar), the above guarantee is equivalent to the one in the pre-training\nphase. However, in most realistic settings, the downstream task will only depend on a subset of\nthe latent topics in the corpus. In this case, q > 1/ar, and we can unlearn more points without\ndegrading the utility of the model. Intuitively this makes sense too; the more reliance T has on a\nrare topic, the less adversarial deletion it can tolerate.\nProof sketch.\nWe again assume that we are deleting mU ≤0.001mǫ0(γp)3\na2r2\npoints. For any mod-\niﬁcation made to A, there is an equivalent modiﬁcation that can be made to w instead such that\nB = Aw is preserved, so we do not need to update A.\nWe look for v ∈Whead such that\nASv = AF wF , where wF is the head learned on AF . It can be shown that ¯AS has a unique\npseudoinverse since it is full rank; naturally, we set v = AS†AF wF , thereby ensuring privacy even\nif one recovers a part of A from B = Aw. We furthermore deﬁne ¯v that is ﬁt to the unlearned\nmodel before the Gaussian mechanism, ¯v = AS† ¯\nA ¯w. We now need to show v and ¯v satisfy both\nthe indistinguishability and utility preservation conditions in Deﬁnition 4.\nIndistinguishability. Let ¯w⋆= arg minv∈Whead ℓT (v; ¯\nA) denote the result of head tuning ¯\nA, and let\n¯w be the result of taking a Newton step on w (see Algorithm 2). Then by triangle inequality,\n∥¯\nA ¯w −AF wF ∥2 ≤∥¯\nA ¯w −¯\nA ¯w⋆∥2 + ∥¯\nA ¯w⋆−AF ¯w⋆∥2 + ∥AF ¯w⋆−AF wF ∥2\n(8)\n3One can generalize this to the case where the downstream task is a C-way classiﬁcation, in which case\nB ∈Rn×C.\n9\n\nPreprint. Under review.\nInformally, the ﬁrst term is controlled by the error in the Newton step approximation, and the third\nterm is bounded by the error to the retrained wF . The remaining term can be rewritten as ∥( ¯\nA −\nAF )( ¯w⋆−w⋆) + ( ¯\nA −AF )w⋆∥, where the ﬁrst term can be bounded using the same technique\nuse to prove Lemmas 4 and 5. The second term can be bounded by noting that w⋆is sparse, which\nyields the below lemma that plays a crucial role in establishing the improved deletion capacity.\nLemma 6 (Modiﬁcation of Lemma 4 for downstream task). For ǫ, δ > 0,\n∥¯\nA −AF ∥∞≤1\nq · ∥¯C −CF∥∞= c · 1\nq · armU\nmǫ0γp\nAs in the pre-training case, we can now set the noise scale in the Gaussian mechanism and complete\nthe proof. In the worst case, when the downstream task depends on every topic, then q = 1/ar, and\nwe recover Lemma 4; however, this is unlikely to happen in practice.\nUtility Preservation. We compare the value of v after the Gaussian mechanism ˜v = ¯v + ν¯v to what\nit would be for the ground-truth model v⋆= (AS)†A⋆w⋆. We again rely the sparsity of w⋆and\nbound E[∥¯v −v⋆∥∞] in Lemma 31.\n6\nRELATED WORKS\nProvable unlearning.\nOne ideally wants the unlearned model to behave identically to one that\nwas retrained from scratch with the forget set removed from the training data (Cao & Yang, 2015;\nBourtoule et al., 2021; Gupta et al., 2021). This is difﬁcult to achieve in many settings, so there are\nseveral notions of approximate unlearning (Ginart et al., 2019; Guo et al., 2020; Neel et al., 2021)\nreminiscent of differential privacy (Dwork et al., 2014). Most relevant to our work is the notion\nof (ǫ, δ)-unlearning introduced in Sekhari et al. (2021), which we adapt to construct Deﬁnition 4.\nOur work focuses on deriving unlearning guarantees in the pre-training and ﬁne-tuning pipeline.\nGolatkar et al. (2020) is closest to our work. They show considerably weaker guarantees on un-\nlearning information with respect to probes ﬁt to the weights. In contrast, our work is focused\non realistic topic classiﬁcation tasks and demonstrates strong guarantees (Deﬁnition 4). Recent\nworks have extended notions of certiﬁed unlearning to nonconvex settings. Zhang et al. (2024a);\nMu & Klabjan (2024); Chien et al. (2024) provide unlearning algorithms without deletion capac-\nity guarantees. Qiao et al. (2024) also proposes an unlearning method for non-convex settings but\nanalyzes its deletion capacity in a convex setting. Our work extends beyond the convex setting to\nprovide provable unlearning methods and corresponding deletion capacity analysis for non-convex\nmodels.\nTheoretical analysis of pre-training and ﬁne-tuning.\nOur downstream task deﬁnition (Sec-\ntion 2.2) is inspired by works on transfer learning in language models (Saunshi et al., 2021;\nWei et al., 2021; Wu et al., 2023; Kumar et al., 2022), contrastive learning (Lee et al., 2021;\nHaoChen & Ma, 2023), and meta-learning (Chua et al., 2021; Collins et al., 2022; Y¨uksel et al.,\n2024).\n7\nCONCLUSION\nThis work uses topic models to develop the ﬁrst provable guarantees on unlearning in the modern-\nday pre-training and ﬁne-tuning paradigm. We propose two unlearning algorithms that can effec-\ntively and efﬁciently unlearn from both the pre-trained model (Algorithm 1 and Theorem 2) and\nthe ﬁne-tuned model (Algorithm 2 and Theorem 4). Notably, we ﬁnd that it is easier, in terms of\nthe deletion capacity (Deﬁnition 4), to unlearn pre-training data from the ﬁne-tuned model, and we\ncan do so without modifying the pre-trained base model. Our ﬁndings suggest that task-speciﬁc un-\nlearning is easier than full model unlearning, providing a promising path forward to design efﬁcient\nalgorithms for large-scale models.\nThe most notable limitation of our work is that our usage of topic models, which permit a tractable\nanalysis but cannot capture interesting features of modern-day language models (e.g., their autore-\ngressive nature). Moreover, with the growing popularity of foundation models, there is scholarly\ndiscussion around meaningful deﬁnitions of unlearning and how they can be measured (Thudi et al.,\n10\n\nPreprint. Under review.\n2022; Lee et al., 2024). Our work focuses on traditional notions of unlearning centered on differen-\ntial privacy (see Deﬁnition 4), but we hope to extend these deﬁnitions to capture additional features\nof generative models that are salient to their real-world uses.\nREFERENCES\nSanjeev Arora, Rong Ge, Yoni Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu,\nand Michael Zhu. A practical algorithm for topic modeling with provable guarantees, 2012a.\nURL https://arxiv.org/abs/1212.4777.\nSanjeev Arora, Rong Ge, and Ankur Moitra. Learning topic models - going beyond svd, 2012b.\nAbeba Birhane and Vinay Uday Prabhu. Large image datasets: A pyrrhic win for computer vision?\nIn 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 1536–1546.\nIEEE, 2021.\nAbeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny,\npornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.\nDavid M. Blei and John D. Lafferty. Dynamic topic models. In Proceedings of the 23rd International\nConference on Machine Learning, ICML ’06, pp. 113–120, New York, NY, USA, 2006. Asso-\nciation for Computing Machinery. ISBN 1595933832. doi: 10.1145/1143844.1143859. URL\nhttps://doi.org/10.1145/1143844.1143859.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993–1022, 2003.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolf-\nsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen\nCreel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Dur-\nmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori\nHashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang,\nThomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keel-\ning, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Ku-\nditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,\nXiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani,\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben New-\nman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel\nOrr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi\nRaghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan,\nChristopher R´e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srini-\nvasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram`er, Rose E. Wang, William\nWang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You,\nMatei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kait-\nlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models, 2022. URL\nhttps://arxiv.org/abs/2108.07258.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin\nTravers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE\nSymposium on Security and Privacy (SP), pp. 141–159. IEEE, 2021.\nJordan Boyd-Graber, Yuening Hu, David Mimno, et al. Applications of topic models. Foundations\nand Trends® in Information Retrieval, 11(2-3):143–296, 2017.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\n11\n\nPreprint. Under review.\nRadford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural\nInformation Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8a\nYinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In 2015\nIEEE symposium on security and privacy, pp. 463–480. IEEE, 2015.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\nfrom large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp.\n2633–2650, 2021.\nNicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja\nBalle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd\nUSENIX Security Symposium (USENIX Security 23), pp. 5253–5270, 2023.\nEli\nChien,\nHaoyu\nWang,\nZiang\nChen,\nand\nPan\nLi.\nLangevin\nunlearning:\nA\nnew\nperspective\nof\nnoisy\ngradient\ndescent\nfor\nmachine\nunlearning,\n2024.\nURL\nhttps://arxiv.org/abs/2401.10371.\nRochelle Choenni, Ekaterina Shutova, and Robert van Rooij.\nStepmothers are mean and\nacademics are pretentious:\nWhat do pretrained language models learn about you?\nIn\nMarie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Pro-\nceedings of the 2021 Conference on Empirical Methods in Natural Language Process-\ning, pp. 1477–1491, Online and Punta Cana, Dominican Republic, November 2021. As-\nsociation for Computational Linguistics.\ndoi:\n10.18653/v1/2021.emnlp-main.111.\nURL\nhttps://aclanthology.org/2021.emnlp-main.111.\nKurtland Chua, Qi Lei, and Jason D Lee. How ﬁne-tuning allows for effective meta-learning. Ad-\nvances in Neural Information Processing Systems, 34:8871–8884, 2021.\nRob Churchill and Lisa Singh. The evolution of topic modeling. ACM Comput. Surv., 2022.\nLiam Collins, Aryan Mokhtari, Sewoong Oh, and Sanjay Shakkottai. Maml and anil provably learn\nrepresentations. In International Conference on Machine Learning, pp. 4238–4310.PMLR, 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of\ndeep bidirectional transformers for language understanding.\nIn Jill Burstein, Christy Do-\nran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics:\nHuman Language Tech-\nnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota,\nJune 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/N19-1423.\nURL\nhttps://aclanthology.org/N19-1423.\nCynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations\nand Trends® in Theoretical Computer Science, 9(3–4):211–407, 2014.\nRonen Eldan and Mark Russinovich. Who’s harry potter? approximate unlearning in llms, 2023.\nURL https://arxiv.org/abs/2310.02238.\nDOE 1 v. GitHub, Inc. 4:22-cv-06823, N.D. Cal. 2022.\nTremblay v. OpenAI, Inc.,. 23-cv-03416-AMO, (N.D. Cal.), 2023.\nEuropean\nParliament\nand\nCouncil\nof\nthe\nEuropean\nUnion.\nRegulation\n(EU)\n2016/679\nof\nthe\nEuropean\nParliament\nand\nof\nthe\nCouncil.\nURL\nhttps://data.europa.eu/eli/reg/2016/679/oj.\nJack Foster, Stefan Schoepf, and Alexandra Brintrup. Fast machine unlearning without retraining\nthrough selective synaptic dampening. In Proceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 38, pp. 12043–12051, 2024.\n12\n\nPreprint. Under review.\nRohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts\nfrom diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 2426–2436, 2023.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor\nLeahy.\nThe pile: An 800gb dataset of diverse text for language modeling, 2020.\nURL\nhttps://arxiv.org/abs/2101.00027.\nAntonio Ginart,\nMelody Guan,\nGregory Valiant,\nand James Y Zou.\nMaking ai for-\nget\nyou:\nData\ndeletion\nin\nmachine\nlearning.\nIn\nH.\nWallach,\nH.\nLarochelle,\nA. Beygelzimer,\nF. d'Alch´e-Buc,\nE. Fox,\nand R. Garnett (eds.),\nAdvances in Neu-\nral Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\nURL\nhttps://proceedings.neurips.cc/paper_files/paper/2019/file/cb79f8fa58b91d3af6c9c991\nAditya Golatkar, Alessandro Achille, and Stefano Soatto.\nEternal sunshine of the spotless net:\nSelective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 9304–9312, 2020.\nChuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certiﬁed data removal\nfrom machine learning models. In International Conference on Machine Learning, pp. 3832–\n3842. PMLR, 2020.\nVarun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Shariﬁ-Malvajerdi, and Chris Waites.\nAdaptive machine unlearning. Advances in Neural Information Processing Systems, 34:16319–\n16330, 2021.\nJeff Z. HaoChen and Tengyu Ma.\nA theoretical study of inductive biases in contrastive learn-\ning.\nIn The Eleventh International Conference on Learning Representations, 2023.\nURL\nhttps://openreview.net/forum?id=AuEgNlEAmed.\nJamie Hayes, Ilia Shumailov, Eleni Triantaﬁllou, Amr Khalifa, and Nicolas Papernot.\nInex-\nact unlearning needs more careful evaluations to avoid a false sense of privacy, 2024.\nURL\nhttps://arxiv.org/abs/2403.01218.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked au-\ntoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 16000–16009, 2022.\nLuxi He, Yangsibo Huang, Weijia Shi, Tinghao Xie, Haotian Liu, Yue Wang, Luke Zettlemoyer,\nChiyuan Zhang, Danqi Chen, and Peter Henderson. Fantastic copyrighted beasts and how (not)\nto generate them. arXiv preprint arXiv:2406.14526, 2024.\nPeter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A Lemley, and Percy\nLiang. Foundation models and fair use. Journal of Machine Learning Research, 24(400):1–79,\n2023.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-\ning compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nThomas Hofmann et al. Probabilistic latent semantic analysis. In UAI, volume 99, pp. 289–296,\n1999.\nZachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion\nfrom machine learning models, 2021.\nJoel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and\nMinjoon Seo. Knowledge unlearning for mitigating privacy risks in language models, 2023. URL\nhttps://openreview.net/forum?id=zAxuIJLb38.\n13\n\nPreprint. Under review.\nAnanya\nKumar,\nAditi\nRaghunathan,\nRobbie\nMatthew\nJones,\nTengyu\nMa,\nand\nPercy\nLiang.\nFine-tuning\ncan\ndistort\npretrained\nfeatures\nand\nunderperform\nout-of-\ndistribution.\nIn International Conference on Learning Representations, 2022.\nURL\nhttps://openreview.net/forum?id=UYneFzXSJWh.\nMeghdad Kurmanji, Peter Triantaﬁllou, Jamie Hayes, and Eleni Triantaﬁllou. Towards unbounded\nmachine unlearning. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023. URL https://openreview.net/forum?id=OveBaTtUAT.\nJason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:\nProvable self-supervised learning. Advances in Neural Information Processing Systems, 34:309–\n323, 2021.\nKatherine Lee, A. Cooper, Christopher Choquette-Choo, Ken Liu, Matthew Jagielski, Niloofar\nMireshghallah, Lama Ahmed, James Grimmelmann, David Bau, Christopher De Sa, Fernando\nDelgado, Vitaly Shmatikov, Katja Filippova, Seth Neel, Miranda Bogen, Amy Cyphert, Mark\nLemley, and Nicolas Papernot. Extended abstract: Machine unlearning doesn’t do what you\nthink, 04 2024.\nWei Li and Andrew McCallum. Pachinko allocation: Dag-structured mixture models of topic corre-\nlations. In Proceedings of the 23rd international conference on Machine learning, pp. 577–584,\n2006.\nJiaqi Liu, Jian Lou, Zhan Qin, and Kui Ren. Certiﬁed minimax unlearning with generalization rates\nand deletion capacity. Advances in Neural Information Processing Systems, 36, 2024.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods in natural language pro-\ncessing, 2021. URL https://arxiv.org/abs/2107.13586.\nShayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William\nBrannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. A large-\nscale audit of dataset licensing and attribution in ai. Nature Machine Intelligence, 6(8):975–987,\n2024.\nAnanth Mahadevan and Michael Mathioudakis. Cost-effective retraining of machine learning mod-\nels. arXiv preprint arXiv:2310.04216, 2023.\nPratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. Tofu: A task\nof ﬁctitious unlearning for llms, 2024. URL https://arxiv.org/abs/2401.06121.\nSadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen,\nand Sanjeev Arora.\nFine-tuning language models with just forward passes.\nIn\nThirty-seventh Conference on Neural Information Processing Systems,\n2023a.\nURL\nhttps://openreview.net/forum?id=Vota6rFhBQ.\nSadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based\nview of language model ﬁne-tuning.\nIn International Conference on Machine Learning, pp.\n23610–23641. PMLR, 2023b.\nNick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark\nSteedman. Sources of hallucination by large language models on inference tasks. arXiv preprint\narXiv:2305.14552, 2023.\nSiqiao Mu and Diego Klabjan. Rewind-to-delete: Certiﬁed machine unlearning for nonconvex func-\ntions, 2024. URL https://arxiv.org/abs/2409.09778.\nTarek Naous, Michael Ryan, Alan Ritter, and Wei Xu.\nHaving beer after prayer?\nmeasur-\ning cultural bias in large language models.\nIn Lun-Wei Ku, Andre Martins, and Vivek\nSrikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp. 16366–16393, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.862. URL\nhttps://aclanthology.org/2024.acl-long.862.\n14\n\nPreprint. Under review.\nSeth Neel, Aaron Roth, and Saeed Shariﬁ-Malvajerdi. Descent-to-delete: Gradient-based methods\nfor machine unlearning. In Algorithmic Learning Theory, pp. 931–962. PMLR, 2021.\nQuoc Phong Nguyen, Bryan Kian Hsiang Low, and Patrick Jaillet. Variational bayesian unlearning.\nAdvances in Neural Information Processing Systems, 33:16025–16036, 2020.\nMaxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khali-\ndov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran,\nNicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,\nMichael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick\nLabatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features with-\nout supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL\nhttps://openreview.net/forum?id=a68SUt6zFt.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The reﬁnedweb\ndataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv\npreprint arXiv:2306.01116, 2023.\nXinbao\nQiao,\nMeng\nZhang,\nMing\nTang,\nand\nErmin\nWei.\nEfﬁcient\nand\ngener-\nalizable\ncertiﬁed\nunlearning:\nA\nhessian-free\nrecollection\napproach,\n2024.\nURL\nhttps://arxiv.org/abs/2404.01712.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748–8763. PMLR, 2021.\nBen Recht, Christopher Re, Joel Tropp, and Victor Bittorf. Factoring nonnegative matrices with\nlinear programs. Advances in neural information processing systems, 25, 2012.\nRyan Rifkin and Aldebaro Klautau. In defense of one-vs-all classiﬁcation. The Journal of Machine\nLearning Research, 5:101–141, 2004.\nNikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why language\nmodels help solve downstream tasks. In International Conference on Learning Representations,\n2021. URL https://openreview.net/forum?id=vVjIW3sEc1s.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems, 35:25278–25294, 2022.\nAyush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh.\nRe-\nmember what you want to forget:\nAlgorithms for machine unlearning, 2021.\nURL\nhttps://arxiv.org/abs/2103.03279.\nWeijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao\nLiu, Luke Zettlemoyer, Noah A. Smith, and Chiyuan Zhang. Muse: Machine unlearning six-way\nevaluation for language models, 2024. URL https://arxiv.org/abs/2407.06460.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Au-\nthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya\nJha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison,\nNiklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichan-\nder, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord,\nEvan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groen-\neveld, Jesse Dodge, and Kyle Lo.\nDolma:\nan open corpus of three trillion tokens for\nlanguage model pretraining research.\nIn Lun-Wei Ku, Andre Martins, and Vivek Sriku-\nmar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 15725–15788, Bangkok, Thailand, August 2024.\nAssociation for Computational Linguistics.\ndoi:\n10.18653/v1/2024.acl-long.840.\nURL\nhttps://aclanthology.org/2024.acl-long.840.\n15\n\nPreprint. Under review.\nGowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion\nart or digital forgery? investigating data replication in diffusion models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6048–6058, 2023.\nAnvith\nThudi,\nHengrui\nJia,\nIlia\nShumailov,\nand\nNicolas\nPapernot.\nOn\nthe\nne-\ncessity\nof\nauditable\nalgorithmic\ndeﬁnitions\nfor\nmachine\nunlearning.\nIn\n31st\nUSENIX\nSecurity\nSymposium\n(USENIX\nSecurity\n22),\npp.\n4007–4022,\nBoston,\nMA,\nAugust\n2022.\nUSENIX\nAssociation.\nISBN\n978-1-939133-31-1.\nURL\nhttps://www.usenix.org/conference/usenixsecurity22/presentation/thudi.\nEnayat Ullah, Tung Mai, Anup Rao, Ryan A Rossi, and Raman Arora. Machine unlearning via\nalgorithmic stability. In Conference on Learning Theory, pp. 4126–4142. PMLR, 2021.\nColin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in down-\nstream tasks? an analysis of head and prompt tuning. Advances in Neural Information Processing\nSystems, 34:16158–16170, 2021.\nChenwei Wu, Holden Lee, and Rong Ge. Connecting pre-trained language model and downstream\ntask via properties of representation. In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023. URL https://openreview.net/forum?id=YLOJ4aKAka.\nO˘guz Kaan Y¨uksel, Etienne Boursier, and Nicolas Flammarion. First-order ANIL provably learns\nrepresentations despite overparametrisation. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=if2vRbS8Ew.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language\nimage pre-training. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 11975–11986, 2023.\nBinchi Zhang, Yushun Dong, Tianhao Wang, and Jundong Li. Towards certiﬁed unlearning for deep\nneural networks, 2024a. URL https://arxiv.org/abs/2408.00920.\nRuiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catas-\ntrophic collapse to effective unlearning. In First Conference on Language Modeling, 2024b. URL\nhttps://openreview.net/forum?id=MXLBXjQkmb.\n16\n\nPreprint. Under review.\nA\nPRECISE DESCRIPTION OF ABASE\nA.1\nCOMPLETE DESCRIPTION\nAlgorithm 3 High level learning algorithm (A)\nInput: document corpus S = {di}m\ni=1, anchor word tolerance ǫ0\nOutput: matrices A,R\nQ = word co-occurrences\n¯Q = row-normalized Q\nP = RecoverAnchors({ ¯Q1, . . . , ¯Qn})\nA, R = RecoverTopics(Q, S)\nreturn A, R\nAlgorithm 4 RecoverAnchors, same as Arora et al. (2012a)\nInput: Row-normalized co-ocurrence matrix ¯Q and ǫ0 tolerance parameter\nOutput: r points of this perturbed simplex close to the vertices of the actual simplex\nProject the rows to a randomly chosen 4 log n/ǫ2\n0 dimensional subspace\nS ←{ ¯Qi} where ¯Qi is the furthest point from the origin\nfor i in 1, . . . , r −1 do\nLet ¯Qj be the row of Q with largest distance to span(S)\nS ←S ∪{ ¯Qj}\nend forS = { ¯Qs1, . . . , ¯Qsr}\nfor i in 1, . . . , r do\nLet ¯Qj be the point that has largest distance to span(S \\ { ¯Qsi})\nRemove ¯Qsi from S and insert ¯Qj into S\nend for\nreturn S\nAlgorithm 5 Recover Topics, from Arora et al. (2012a)\nInput: Co-ocurrence matrix Q, anchor words P = {s1, . . . , sk}, tolerance parameter ǫ0\nOutput: Matrices A, R\n¯Q = row normalized Q\nStore the normalization constants p = Q1\nfor i in 1, . . . , n do\nSolve Ci = arg min\nv∈∆r\n∥¯Qi −v⊤¯QP )∥2\nup to ǫ0 accuracy\nend for\nA′ = diag(p)C\nA = column-sum-one normalized A′\nR = A†QA†⊤where A† is the pseudoinverse of A\nreturn A, R\nMore formally, the co-occurrence matrix is constructed as follows. For each document, let Hd ∈Rn\nbe the frequency vector of each word in the document; the sum of its entries should be L. Then, for\na document d, consider the matrix\nGd := ˜\nHd ˜\nH⊤\nd −ˆ\nHd\n(9)\nwhere\n˜\nHd :=\nHd\np\nL(L −1)\n(10)\nˆ\nHd := diag(Hd)\nL(L −1)\n(11)\n17\n\nPreprint. Under review.\nIn particular, the denominator term L(L −1) is precisely the number of co-occurences in each\ndocument, by simple combinatorics, and it can be seen that the sum of the entries of Gd is always\n1. Our co-ocurrence matrix Q is deﬁned to be\nQ := 1\nm\nm\nX\ni=1\nGd\n(12)\nso that Q also has entries that sum to 1. By linearity of expectation, we have\nE[Q] = E[Gd] = A⋆E[XdX⊤\nd ]A⋆⊤\n(13)\nwhich implies that as the number of documents increases, Q concentrates around AE[XX⊤]A⊤=\nE[MM ⊤]. Therefore, we should expect A†QA†⊤to concentrate around E[XX⊤] = R⋆.\nA.2\nSKETCH: POPULATION ANALYSIS\nTo understand this algorithm, consider the setting where we have inﬁnitely many documents. Specif-\nically, consider two words w1, w2 in a document and their respective topics z1, z2. Then, this\npopulation co-occurrence matrix Q will have elements Qi,j = Pr[w1 = i, w2 = j], and the row-\nnormalized co-occurrence matrix ¯Q will have entries ¯Qi,j = Pr[w2 = j|w1 = i]. Moreover, we\nhave that Ai,k = Pr[w1 = i|z1 = k] = Pr[w2 = i|z2 = k].\nConsider the set of anchor words P = {s1, . . . , sr} ⊆[n], where sk is the anchor word for topic k.\nThen, observe that for an anchor word row sk of ¯Q, it holds that\n¯Qsk,j = Pr[w2 = j|w1 = sk] =\nX\nk′\nPr[z1 = k′|w1 = sk] Pr[w2 = j|w1 = sk, z1 = k′]\n(14)\n= Pr[w2 = j|w1 = sk, z1 = k]\n(15)\n= Pr[w2 = j|z1 = k]\n(16)\nwhere the second line follows from only Pr[z1 = k|w1 = sk] = 1 in the summation, and the last\nline follows from w2, w1 are conditionally independent given z1. Furthermore, for non-anchor word\nrows i of ¯Q, it holds that\n¯Qi,j =\nX\nk\nPr[z1 = k|w1 = i] Pr[w2 = j|z1 = k]\n(17)\nwhere again we use that w2, w1 are conditionally independent z1. For a word i, let Ci ∈Rr be\nthe vector such that Ci,k := Pr[z1 = k|w1 = i]. Then, it holds that ¯Qi = c⊤\ni ¯QS, where ¯QS is\nthe submatrix of ¯Q constrained to the anchor word rows. In other words, for every word i, ¯Qi is a\nconvex combination of rows of ¯QS.\nIn the algorithm, one can see that A′\ni,k = Ci,kpi. Normalizing this along each column, we obtain\nAi,k =\nCi,kpi\nP\ni′ Ci′,kpi′ =\nPr[z1 = k|w1 = i] Pr[w1 = i]\nP\ni′ Pr[z1 = k|w1 = i′] Pr[w1 = i′] = Pr[w1 = i|z1 = k]\n(18)\nHence, in the inﬁnite document limit, this algorithm recovers the ground truth A⋆, R⋆.\nB\nFROM PROPERTIES OF THE LEARNING ALGORITHM TO THE PROOF OF\nTHEOREM 2\nWe ﬁrst give the formal statement of Theorem 2.\nTheorem 5 (Formal statement of Theorem 2). Let Abase be the learning algorithm described in the\nprior sections and Ubase be the unlearning algorithm in Algorithm 1. Then, (Abase, Ubase) performs\nutility-preserving unlearning with deletion capacity\nT Abase,Ubase\nǫ,δ\n(m) ≥c · min\n(\nmǫ\nr2p\nrn log 1/δ\n, 0.001m\nr2\n)\n(19)\nwhere m is the number of training documents, r is the number of topics, and c is a constant depen-\ndent on D. The loss function h used in the utility-preserving deﬁnition is the maximum entrywise\nerror from the ground truth topic model A⋆.\n18\n\nPreprint. Under review.\nB.1\nPRELIMINARIES\nWhen the norm is not speciﬁed, we assume that it is the Euclidean norm ∥· ∥2. We now start off\nwith a technical assumption on the precision of the learning algorithm.\nAssumption 2. ǫ0 ≤O(1/√nr).\nAssumption 3. Every word appears with probability ǫ0/4ar without loss of generality; see discus-\nsion in Arora et al. (2012b). Essentially, less probable words can be combined in a sense to form a\nsingle category of ”rare” words.\nWe recall the deﬁnitions from Arora et al. (2012a).\nDeﬁnition 6 (β-robust simplex). A simplex P is β-robust if for every vertex v of P, the ℓ2 distance\nbetween v and the convex hull of the rest of the vertices as at least β.\nDeﬁnition 7. Let {ai}n\ni=1 be a set of points whose convex hull is a simplex with vertices {vi}r\ni=1.\nWe say a set of r points is ǫ-close the vertex set {vi}r\ni=1 if each of the r points is ǫ-close in ℓ2\ndistance to a different vertex in this vertex set.\nThe following result will be used throughout our proof.\nProposition 1 (Arora et al. (2012b)). ¯Q⋆\nP in population is γp-robust.\nWe now list the high probability events we condition on throughout our proof. These follow from\nprevious results in Arora et al. (2012a); they concern the properties of the output of the learning\nalgorithm.\nProposition 2. With high probability, in our regime of m, the following hold:\n• The correct anchor words are selected.\n• Each word appears at least O\n\u0000 mǫ0\n4ar\n\u0001\ntimes.\n• The error in the empirical matrix ˆQ is entrywise at most ˜O(1/√m) from the population\nQ⋆.\nWe also utilize the following two key lemmas from Arora et al. (2012a) that we touched upon in the\nmain paper.\nLemma 7 (Approximation Guarantee on Anchor Words). Suppose each row of ¯Q is at most δ\ndistance away from the ground truth γp-robust simplex ¯Q⋆in ℓ2 norm. If 20rδ/(γp)2 < γp, then\nthe set of anchor words found by the algorithm is O(δ/γp)-close to the ground truth anchor words.\nLemma 8. When 20rδ/(γp)2 < γp, it holds for every word i that Ci has entrywise error\nO(δ/(γp)2) from C⋆\ni .\nB.2\nPROOF OF THEOREM 2\nThe following are lemmas bounding the relation between ¯QS\ni , ¯QF\ni , ¯Q⋆\ni .\nLemma 9. After training, the error of each row of ¯QS is at most δ2 := O\n\u0010q\n4ar\nmǫ0\n\u0011\n. That is,\n∥¯QS\ni −¯Q⋆\ni ∥≤δ2 for all words i.\nImportantly, note that\n20rδ2/(γp)2 < γp\n(20)\nThis implies that the anchor words of ¯QS\ni are O(δ2/(γp)) close to the anchor words of ¯Q⋆\ni .\nConsequently, it holds that\n∥CS −C⋆∥∞≤O(δ2/(γp)2)\n(21)\nProof. The ﬁrst part follows directly from the fact that if the number of documents m = ˜Ω(1/ǫ2\nQ),\nthen ∥¯QS\ni −¯Q⋆\ni ∥≤δ2 for each row i. To show that\n20rδ2/(γp)2 < γp\n(22)\n19\n\nPreprint. Under review.\nwe note that by the sample complexity guarantee,\nmǫ0 ≥˜O\n\u0012 ar3\n(γp)6\n\u0013\n(23)\nwhich implies that\nδ2 ≤˜O\n\u0012(γp)3\nr\n\u0013\n(24)\nas desired.\nLemma 10. When we delete mU ≤0.001mǫ0(γp)3\na2r2\n, it holds that\n∥¯QF\ni −¯QS\ni ∥≤\nmU\nmǫ0/4ar = 4armU\nmǫ0\n(25)\nIn particular, this is smaller than\n0.001mǫ0(γp)3\na2r2\n·\n1\nmǫ0/4ar = 0.004(γp)3\nar\n(26)\nProof. For a word i, consider the change in ¯Qi after deletion requests. Let F be the initial sum of\nthe the ith row of Q. Each coordinate j ∈[n] will change as follows:\nδj = fj −tj\nF −mU\n−fj\nF = mUfj −Ftj\nF(F −mU)\n(27)\nwhere fj is the initial number of coocurrences of words i, j and tj is the number of documents\nremoved that have this cooccurrence. Moreover, F is the number of initial occurrences of word i, and\nT is the number of deletions of the word i. From the previous lemma, it holds that F ≥mǫ0/4ar,\nand that mU ≥Pn\nj=1 tj Hence, it follows that the squared Euclidean norm of the change is:\nn\nX\nj=1\nδ2\nj =\n1\nF 2(F −T )2\nn\nX\nj=1\n(mUfj −Ftj)2 ≤\n2F 2m2\nU\nF 2(F −mU)2 ≤2\n\u0012\nmU\nF −mU\n\u00132\n(28)\nHence, for the regime where mU ≤0.001mǫ0(γp)3\na2r2\n, we have\n∥¯QS\ni −¯QF\ni ∥≤\n√\n2\nmU\nF −mU\n≲mU\nF\n≲4armU\nmǫ0\n(29)\nOf particular notice is that when mU is taken as large as possible, this is at most\n0.001mǫ0(γp)3/a2r2\nmǫ0/4ar\n= 0.004(γp)3/ar\n(30)\nWe now combine the above two with triangle inequality.\nLemma 11. Hence, it holds that\n∥¯QF\ni −¯Q⋆\ni ∥≤4armU\nmǫ0\n+ δ2 = 4armU\nmǫ0\n+ O\n\u0012r\n4ar\nmǫ0\n\u0013\n=: δ′\n2\n(31)\nImportantly, note that\n20rδ′\n2/(γp)2 < γp\n(32)\nThis implies that the anchor words of ¯QF\ni are O(δ′\n2/(γp)) close to the anchor words of ¯Q⋆\ni .\nConsequently, it holds that\n∥CF −C⋆∥∞≤O(δ′\n2/(γp)2)\n(33)\n20\n\nPreprint. Under review.\nProof. The ﬁrst part follows from triangle inequality, and the second part follows from lemma B.1\nfrom Arora et al. (2012a).\nWe now bound what happens to ∥CF −CS∥∞. First, we have that the perturbed simplex ¯QS\nP is\nγp/2-robust.\nLemma 12. The perturbed simplex ¯QS\nP is γp/2-robust.\nProof. This is because of Lemma A.1 in Arora et al. (2012a). Since 10√rδ2 < γp, the result of that\nlemma applies.\nHence, we will apply Lemma B.1 from Arora et al. (2012a) on CS to say something about ∥CF −\nCS∥∞.\nLemma 13. Recall that when we delete mU ≤0.001mǫ0(γp)3\na2r2\n, it holds that\n∥¯QF\ni −¯QS\ni ∥≤\nmU\nmǫ0/4ar = 4armU\nmǫ0\n(34)\nImportantly, note that\n20r\n\u00124armU\nmǫ0\n\u0013\n/(γp/2)2 < γp/2\n(35)\nThis implies that the anchor words of ¯QF\ni are 4armU/mǫ0\nγp/2\nclose to the anchor words of ¯QS\ni . By\nlemma B.1 from Arora et al. (2012a), it holds that\n∥CF −CS∥∞≤O\n\u00124armU\nmǫ0\n/(γp/2)2\n\u0013\n(36)\nObserve that this is smaller than O((γp)/ar).\nWe now deal with the Hessian step that we had took to prevent retraining the Ci’s. In particular, we\nwill denote ¯C to be our estimated new C.\nFirst, a lemma to say that our Hessian step is full rank and has a lower bound on its minimum\nsingular value.\nLemma 14. When we delete mU ≤0.001mǫ0(γp)3\na2r2\nsamples, it holds that the minimum eigenvalue of\n¯QF\nP ¯QF\nP is at least γp/2.\nProof. Follows from Lemma A.3 in Arora et al. (2012a).\nLemma 15. When we delete mU ≤0.001mǫ0(γp)3\na2r2\nsamples, it holds for all i,\n∥CF\ni −¯CF\ni ∥≤4\nγp\n\u0012\nδ2 + 4armU\nmǫ0\n\u0013\n(37)\nProof. For the case of d(·, ·) being the squared loss, we will denote the following:\nCi,uncon := arg min\nC\n∥¯QF ⊤\nP C −¯QF ⊤\ni\n∥2 = ( ¯QF\nP ¯QF ⊤\nP )−1 ¯QF\nP ¯QF ⊤\ni\n(38)\n¯CF\ni := proj∆r(Ci,uncon)\n(39)\nCF\ni := arg min\nC∈∆r\n∥¯QF ⊤\nP C −¯QF ⊤\ni\n∥2\n(40)\nIn particular, the Newton step plus projection outputs Ci,proj. First, observe that by one of the\nanchor word lemmas,\nmin\nC ∥¯QF ⊤\nP C −¯QF ⊤\ni\n∥= ∥¯QF ⊤\nP Ci,uncon −¯QF ⊤\ni\n∥≤∥¯QF ⊤\nP CF\ni −¯QF ⊤\ni\n∥≤δ2 + 4armU\nmǫ0\n(41)\n21\n\nPreprint. Under review.\nThe last inequality follows from the fact that ¯QF\nP is a perturbed version of ¯QS\nP , and ¯QS\nP is a perturbed\nversion of ¯Q⋆\nP . Hence, we will bound\n∥¯CF\ni −CF\ni ∥= ∥proj∆r(Ci,uncon) −proj∆r(CF\ni )∥\n(42)\n≤∥Ci,uncon −CF\ni ∥\n(43)\n≤\n1\nσmin\n∥¯QF ⊤\nP (Ci,uncon −CF\ni )∥\n(44)\n≤\n1\nσmin\n\u0000∥¯QF ⊤\ni\n−¯QF ⊤\nP CF\ni ∥+ ∥¯QF ⊤\nP Ci,uncon −¯QF ⊤\ni\n∥\n\u0001\n(45)\n≤\n2\nσmin\n\u0012\nδ2 + 4armU\nmǫ0\n\u0013\n(46)\nwhere σmin is the smallest singular value of ¯QF ⊤\ni\n, which is guaranteed to be full rank per the\nprevious lemma. Due to a result in Arora et al. (2012a), this σmin ≥(γp)/2. This gives us that the\nwhole thing is at most\n4\nγp\n\u0012\nδ2 + 4armU\nmǫ0\n\u0013\n(47)\nCorollary 1. We have that\n∥CF −¯CF ∥∞≤4\nγp\n\u0012\nδ2 + 4armU\nmǫ0\n\u0013\n(48)\nsince the ℓ∞norm is upper bounded by the ℓ2 norm.\nLemma 16. The following are true.\n• ∥CF −¯CF∥∞≤\n4\nγp\n\u0010\nδ2 + 4armU\nmǫ0\n\u0011\n• ∥¯CF −C⋆∥∞≤∥¯CF −CF ∥∞+ ∥CF −C⋆∥∞≤\n4\nγp\n\u0010\nδ2 + 4armU\nmǫ0\n\u0011\n+ O(δ′\n2/(γp)2)\nFrom this, we can bound the errors on the topic matrix.\nLemma 17. The following are true.\n• ∥AF −¯\nA∥∞≤O(ar∥CF −¯CF ∥∞)\n• ∥¯\nA −A⋆∥∞≤O(ar∥¯\nCF −C⋆∥∞)\n• ∥AS −AF ∥∞≤O(ar∥CF −CS∥∞)\nProof. Note that entries Ai,k are\nAi,k = Ci,k Pr[w = i]\nPr[z = k]\n(49)\nTherefore, the perturbation in A will be the perturbation in C multiplied by ar, since the denomi-\nnator is lower bounded by 1/ar due to the topic imbalance constant.\nNow, we give a new lemma.\nProposition 3. When mU ≥Ω(p mǫ0\n4ar ), we have that\nδ′\n2 = δ2 + 4armU\nmǫ0\n=\nr\n4ar\nmǫ0\n+ 4armU\nmǫ0\n≤O\n\u0012armU\nmǫ0\n\u0013\n(50)\nNow, we analyze what happens given that Ω\n\u0000p mǫ0\n4ar\n\u0001\n≤mU ≤0.001mǫ0(γp)3\na2r2\n.\n22\n\nPreprint. Under review.\nLemma 18. For ǫ, δ > 0, the deletion capacity satisﬁes\nT A,U\nǫ,δ (m) ≥˜Ω\n\u0012\nm\nr2√nr\n\u0013\n(51)\nProof. Recall that\n∥¯\nA −A⋆∥∞≤O(arδ′\n2(1/γp + 1/(γp)2)) ≤O\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n(52)\nMoreover, we also have that\n∥¯\nA −AF ∥∞≤O(ar∥CF −¯CF ∥∞)\n(53)\n≤O\n\u00124arδ′\n2\nγp\n\u0013\n(54)\n≤O\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n(55)\nNote that A has ℓ2 sensitivity O\n\u0010√nr (ar)2mU\nmǫ0γp\n\u0011\n. We now apply the Gaussian mechanism to the\nmatrix A entrywise with noise\nσ =\nO\n\u0010√nr (ar)2mU\nmǫ0γp\n\u0011\nǫ\np\n2 log(1.25/δ)\n(56)\nFrom this, we obtain that\nE\nh\n∥˜\nA −A⋆∥∞\ni\n≤E\n\u0014\nmax\ni,k |νi,k|\n\u0015\n+ E\n\u0002\n∥¯\nA −A⋆∥∞\n\u0003\n(57)\n≤O\n \n√nr · (ar)2mU\nmǫ0γp\n·\np\nlog(nr) ·\np\nlog(1/δ)\nǫ\n!\n+ O\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n(58)\nFinally, this says that when\nmU ≤˜Ω\n\u0012\nm\nr2√nr\n\u0013\n(59)\nwe have that the utility is preserved up to constant amount, say 0.01.\nThis proves Theorem 2. It is straightforward to continue the perturbation analysis for the topic-topic\ncovariance matrix R⋆and prove similar deletion capacity rates.\nC\nDOWNSTREAM TASK PROOFS\nRecall the algorithm for learning the downstream task head.\nAlgorithm 6 Learning algorithm for task T (Ahead)\nInput: document corpus S = {di}m\ni=1, anchor word tolerance ǫ0\nA, R = Abase(S)\nreturn arg minw∈Whead ℓT (w; A)\nAssumption 4. For any A, ℓT is λ-strongly convex with respect to w.\nSince our topic matrix A, can only take on a bounded support (i.e. the set of matrices where each\nrow is on the probability simplex), it is natural to say that the set of values w⋆(A) takes on over all\ntopic matrices A is bounded in a certain sense. As such, we also assume the following:\n23\n\nPreprint. Under review.\nAssumption 5. For any base model A, the vector v such that v = arg minw ℓT (w; A) satisﬁes\n∥v∥2 ≤B.\nAssumption 6. For any A, ℓT is L-Lipschitz with respect to w and the ℓ2 norm, and is L2-Hessian\nLipschitz with respect to w and the ℓ2 norm. In other words,\n∥ℓT (A, w1) −ℓT (A, w2)∥2 ≤L∥w1 −w2∥2\n(60)\n∥∇2\nwℓT (A, w1) −∇2\nwℓT (A, w2)∥2 ≤L2∥w1 −w2∥2\n(61)\nAssumption 7. For any w, ∇wℓT is L∞-Lipschitz with respect to A and the ℓ∞norm; that is,\n∥∇wℓT (A, w) −∇wℓT ( ˜\nA, w)∥2 ≤L∞∥A −˜\nA∥∞\n(62)\n(63)\nWe give a helper lemma that (ǫ, δ)-indistinguishability is immune to post processing.\nLemma 19 (Post-processing immunity). Consider two random variables θ1, θ2 ∈Θ that are (ǫ, δ)-\nindistinguishable. Then, for any arbitrary mapping f : Θ →Θ′, it holds that f(θ1), f(θ2) ∈Θ′ are\n(ǫ, δ)-indistinguishable.\nProof. Consider an arbitrary set T ′ ⊆Θ′; let T = {r ∈Θ : f(r) ∈T ′}. Then, it holds that\nPr[f(θ1) ∈T ′] = Pr[θ1 ∈T]\n(64)\n≤eǫ Pr[θ2 ∈T] + δ\n(65)\n= eǫ Pr[f(θ2) ∈T ′] + δ\n(66)\nas desired.\nWe now give a certiﬁable unlearning guarantee for the most naive retraining algorithm for the down-\nstream task, which we mentioned in the main text as Theorem 3.\nTheorem 6 (Unlearning when releasing A and w). For a downstream task T with loss func-\ntion ℓT , consider the unlearning algorithm Uhead, naive that ﬁrst runs Algorithm 1 to compute\n˜\nA = Ubase(Sf, Abase(S), T (S)), where (Abase, Ubase) perform utility-preserving unlearning (The-\norem 2). Then, it ﬁts a head w = arg minw∈Whead ℓT (w; ˜\nA) and returns ˜\nA and w. We assert that\n(Ahead, naive, Uhead, naive) performs utility-preserving unlearning (Deﬁnition 4).\nProof. Intuitively, this is a result of post processing.\nMore precisely, consider the (ǫ, δ)-\nindistinguishable base models ˜\nA := Ubase(Sf, Abase(S), T (S)) and ˜\nA′ := Ubase(∅, Abase(S \\\nSf), T (S \\ Sf)). Then, since the head ﬁtting is a deterministic post-processing of the original\nmodel, this proves the (ǫ, δ)-indistinguishability between the two.\nTo prove the utility preservation, observe that in this setting\nE[∥˜\nA −A⋆∥∞] ≤0.01\n(67)\n(68)\nWe thus obtain by Lemma 20\nE[∥w⋆( ˜\nA) −w⋆(A⋆)∥∞] ≤E[∥w⋆( ˜\nA) −w⋆(A⋆)∥2]\n(69)\n≤L∞\nλ E[∥˜\nA −A⋆∥∞]\n(70)\nwhich is at most 0.01, up to constant rescaling.\nThe above result is nice, and it follows from the fact that the training algorithm of the downstream\ntask head is just a post-processing. However, a downside is that it still requires retraining of the\ndownstream task head. We can show something stronger: even without provable unlearning of the\nbase model (A and R), we can achieve provable unlearning of the downstream task head weights\nwhen the downstream task loss is convex in the trainable weights w.\nWe will now consider an arbitrary task T . We ﬁrst give the following notation.\n24\n\nPreprint. Under review.\nDeﬁnition 8. For a base model A, let w⋆(A) := arg minw ℓT (w; A).\nFirst, we give the following helper lemma that will be useful later on.\nLemma 20. Consider two base models A1 and A2. Then, it holds that\n∥w⋆(A1) −w⋆(A2)∥2 ≤L∞\nλ ∥A1 −A2∥∞\n(71)\nProof. Observe that\nλ∥w⋆(A1) −w⋆(A2)∥2 ≤∥∇wℓT (w⋆(A1); A2) −∇wℓT (w⋆(A2); A2)∥2\n(72)\n= ∥∇wℓT (w⋆(A1); A2) −∇wℓT (w⋆(A1); A1)∥2\n(73)\n≤L∞∥A1 −A2∥∞\n(74)\nwhere the ﬁrst line follows from strong convexity, the second line from the gradients being zero,\nand the third line from the deﬁnition of L∞Lipschitz constant. Dividing both sides by λ gives the\ndesired result.\nWe now deﬁne the following notations for clarity.\n• wS := w⋆(AS)\n• wF := w⋆(AF )\n• ¯w⋆:= w⋆( ¯\nA)\n• ¯w := wS −H−1\nwS∇wℓT (wS; ¯A), which is the Newton step we take from wS to approxi-\nmate ¯w⋆\nFirst, we give a bound on the approximation error of the Newton step.\nLemma 21. It holds that\n∥¯w −¯w⋆∥≤L2L2\n∞\n2λ3 ∥AS −¯\nA∥2\n∞\n(75)\nProof. We aim to bound the distance of the Newton step from ¯w⋆:\n¯w −¯w⋆=\n\u0000wS −H−1\nwS∇wℓT ( ¯\nA, wS)\n\u0001\n−¯w⋆\n(76)\nwhere HwS = ∇2\nwℓT ( ¯\nA, wS). Then, it holds that\nwS −H−1\nwS∇wℓT ( ¯\nA, wS) −¯w⋆\n(77)\n= wS −¯w⋆−H−1\nwS\n\u0000∇wℓT ( ¯\nA, wS) −∇wℓT ( ¯\nA, ¯w⋆)\n\u0001\n(78)\n= H−1\nwS\n\u0012\nHwS(wS −¯w⋆) −\nZ 1\n0\nH ¯\nw⋆+t(wS−¯\nw⋆)(wS −¯w⋆)dt\n\u0013\n(79)\n= H−1\nwS\nZ 1\n0\n\u0000HwS −H ¯\nw⋆+t(wS−¯\nw⋆)\n\u0001\ndt · (wS −¯w⋆)\n(80)\nThe norm of this quantity is therefore bounded by\n∥H−1\nwS∥2 · L2\n2 ∥wS −¯w⋆∥· ∥wS −¯w⋆∥\n(81)\n≤L2\n2λ∥wS −¯w⋆∥2\n2\n(82)\n≤L2\n2λ\n\u0012 1\nλ∥∇ℓT ( ¯\nA, wS) −∇ℓT (AS, wS)∥2\n\u00132\n(83)\n≤L2\n2λ\n\u0012L∞\nλ ∥¯\nA −AS∥∞\n\u00132\n(84)\nHence, we have that\n∥¯w −¯w⋆∥2 ≤L2L2\n∞\n2λ3 ∥AS −¯\nA∥2\n∞\n(85)\n25\n\nPreprint. Under review.\nC.1\nINSTANTIATING FOR TCLF = [r]\nWe ﬁrst instantiate Theorem 4 for the case where Tclf = [r], or equivalently when q = 1/ar.\nLemma 22. Recall our retrained model for the downstream task is AF wF . Then, it holds that\n∥¯\nA ¯w −AF wF ∥2 ≤O\n \n√r\n\u0012(ar)2mU\nmǫ0γp\n\u00132\n+ B√nr (ar)2mU\nmǫ0γp\n!\n(86)\nProof. We rewrite as follows.\n¯\nA ¯w −AF wF =\n\u0000 ¯\nA ¯w −¯\nA ¯w⋆\u0001\n+\n\u0000 ¯\nA ¯w⋆−AF ¯w⋆\u0001\n+\n\u0000AF ¯w⋆−AF wF \u0001\n(87)\nNow, we proceed to bound the ℓ2 norm of each of these individual terms separately. For the ﬁrst\nterm, we have that\n∥¯\nA ¯w −¯\nA ¯w⋆∥2 = ∥¯\nA( ¯w −¯w⋆)∥2\n(88)\n≤∥¯w −¯w⋆∥1\n(89)\n≤√r∥¯w −¯w⋆∥2\n(90)\n≤√rL2L2\n∞\n2λ3 ∥AS −¯\nA∥2\n∞\n(91)\n≤√rL2L2\n∞\n2λ3\n\u0012(ar)2mU\nmǫ0γp\n\u00132\n(92)\nwhere second line follows from ¯A having column sum 1, and the fourth line follows from Lemma 20\nFor the third term, we have a similar analysis.\n∥AF ¯w⋆−AF wF ∥2 = ∥AF ( ¯w⋆−wF )∥2\n(93)\n≤∥¯w⋆−wF ∥1\n(94)\n≤√r∥¯w⋆−wF ∥2\n(95)\n≤√r L∞\nλ ∥¯\nA −AF ∥∞\n(96)\n≤√r L∞\nλ\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n(97)\nFinally, for the second term, we have that\n∥¯\nA ¯w⋆−AF ¯w⋆∥2 ≤∥¯\nA −AF ∥2∥¯w⋆∥2\n(98)\n≤∥¯\nA −AF ∥∞\n√nr∥¯w⋆∥2\n(99)\n≤O\n\u0012(ar)2mU\nmǫ0γp\n√nrB\n\u0013\n(100)\nBy triangle inequality, we obtain the desired result.\nFirst, we note show the following property of the learned topic model AS.\nLemma 23. The minimum singular value of the ground truth topic matrix AS is at least Θ(p), since\nthe perturbations in entries of A⋆are at most ǫ0 ≤O(1/√nr). Hence, the singular values cannot\nchange by more than a constant factor relative to p.\nProof. We know that A⋆is a p-separable topic model, and hence has smallest singular value at\nleast p. For the given sample complexity of learning, AS will have smallest singular value at least\nΘ(p).\nThe above result says that AS has a unique pseudoinverse, and has largest singular value at most\nO(1/p).\n26\n\nPreprint. Under review.\nRecall that our goal for the downstream task is to approximate the v such that\nASv = AF wF\n(101)\nin order to say we have approximated the unlearned ﬁne-tuned model. Therefore, it sufﬁces to obtain\nindistinguishability of our unlearning algorithm output ˜w with (AS)†AF wF . Our following claim\nis that we can use (AS)† ¯\nA ¯w as the approximation for this.\nProposition 4. It holds that\n∥(AS)† ¯\nA ¯w −(AS)†AF wF ∥2 ≤O\n\u00121\np∥¯\nA ¯w −AF wF ∥2\n\u0013\n(102)\n≤O\n \n1\np ·\n\"\n√r\n\u0012(ar)2mU\nmǫ0γp\n\u00132\n+ B√nr (ar)2mU\nmǫ0γp\n#!\n(103)\nLet ¯v := (AS)† ¯\nA ¯w and v = (AS)†AF wF . We claim the following.\nLemma 24. The unlearning algorithm Uhead that outputs\n˜v := ¯v + νv\n(104)\nwhere νv is the noise deﬁned by the Gaussian mechanism using the above sensitivity satisﬁes prov-\nable (ǫ, δ) unlearning. In particular, we use\nσ =\nO\n\u0012\n1\np ·\n\u0014√r\n\u0010\n(ar)2mU\nmǫ0γp\n\u00112\n+ B√nr (ar)2mU\nmǫ0γp\n\u0015\u0013\nǫ\np\n2 log(1.25/δ)\n(105)\nwhere the numerator of the fraction is from the previous proposition.\nProof. This follows from Gaussian mechanism.\nWe now proceed to bound the deletion capacity. In this case, the utility is deﬁned by the closeness\nof ˜v to (AS)†A⋆w⋆in ℓ∞norm, similar the way we deﬁned this for the base model unlearning\nalgorithm Ubase earlier.\nFirst, the following lemma to bound AF wF −A⋆w⋆.\nLemma 25. We have that\n∥AF wF −A⋆w⋆∥2 ≤O\n\u0012\nB√nr (ar)2mU\nmǫ0γp\n\u0013\n(106)\nProof. We decompose as follows.\nAF wF −A⋆w⋆= (AF wF −AF w⋆) + (AF w⋆−A⋆w⋆)\n(107)\nThe ﬁrst term is bounded by\n∥AF wF −AF w⋆∥2 ≤√r∥wF −w⋆∥2 ≤O(√r∥AF −A⋆∥∞) ≤O\n\u0012√r (ar)2mU\nmǫ0γp\n\u0013\n(108)\nThe second term is bounded by\n∥AFw⋆−A⋆w⋆∥2 ≤O\n\u0012(ar)2mU\nmǫ0γp\n√nrB\n\u0013\n(109)\nby considering the spectral norm ∥AF −A⋆∥2. This gives the desired result.\nAs a result, the following holds.\nProposition 5. It holds that\n∥(AS)†AF wF −(AS)†A⋆w⋆∥2 ≤O\n\u00121\np\n\u0014√r (ar)2mU\nmǫ0γp\n+ B√nr(ar)2mU\nmǫ0γp\n\u0015\u0013\n(110)\n27\n\nPreprint. Under review.\nThis is once again from the bounded operator norm property of (AS)†.\nFinally, we can apply triangle inequality to get the following.\nLemma 26. It holds that\n∥(AS)† ¯\nA ¯w −(AS)†A⋆w⋆∥2 ≤\n \n1\np ·\n\"\n√r\n\u0012(ar)2mU\nmǫ0γp\n\u00132\n+ B√nr(ar)2mU\nmǫ0γp\n#!\n(111)\nThen, we can get the following bound on deletion capacity.\nLemma 27. For ǫ, δ > 0, the deletion capacity satisﬁes\nT Ahead,Uhead\nǫ,δ\n(m) ≥˜Ω\n\u0012\nm\nr2√nr\n\u0013\n(112)\nProof. The calculation is as follows.\nE\n\u0002\n∥˜v −(AS)†A⋆w⋆∥∞\n\u0003\n≤E[∥νv∥∞] + E\n\u0002\n∥(AS)† ¯\nA ¯w −(AS)†A⋆w⋆∥∞\n\u0003\n(113)\n≤\n \n1\np ·\n\"\n√r\n\u0012(ar)2mU\nmǫ0γp\n\u00132\n+ B√nr (ar)2mU\nmǫ0γp\n#! p\nlog r log 1/δ\nǫ\n+ 1\n!\n(114)\nFor this to be a small constant, we require\n(ar)2mU\nmǫ0γp\n≤˜O\n\u0012\nmin\n\u001a 1\nr1/4 ,\n1\n√nr\n\u001b\u0013\n(115)\nTherefore, we should have\nmU ≤˜Ω\n\u0012\nm\nr2√nr\n\u0013\n(116)\nC.2\nPROOF FOR GENERAL q\nThe following is the formal statement of Theorem 4.\nTheorem 7 (Formal version of Theorem 4). Suppose that the downstream task T only depends on\na subset of topics Tclf ⊆[r]; that is, w⋆= arg minv∈Wbase ℓT (v; A⋆) has non-zero entries only in\nthe index set Tclf. Denote q := mink∈Tclf PrD[z = k], and let Ahead be the head tuning algorithm\n(Deﬁnition 2) and Uhead be Algorithm 2. Then, (Ahead, Uhead) performs utility-preserving unlearning\nwith deletion capacity\nT Ahead,Uhead\nǫ,δ\n(m) ≥c′ · min\n(\nmqǫ\nr\np\nnr log 1/δ\n, 0.001m\nr2\n)\n(117)\nwhere c′ is a constant dependent on D, and T .\nLemma 28. Recall our retrained model for the downstream task is AF wF . Then, it holds that\n∥¯\nA ¯w −AF wF ∥2 ≤O\n\u0012√r\n\u0012(ar)2mU\nmǫ0γp\n\u0013\u0013\n+ O\n\u0012\nB√nr (1/q)armU\nmǫ0γp\n\u0013\n+ O\n \u0012(ar)2mU\nmǫ0γp\n\u00132√nr\n!\n(118)\nProof. Consider this decomposition again.\n¯\nA ¯w −AF wF =\n\u0000 ¯\nA ¯w −¯\nA ¯w⋆\u0001\n+\n\u0000 ¯\nA ¯w⋆−AF ¯w⋆\u0001\n+\n\u0000AF ¯w⋆−AF wF \u0001\n(119)\nThe ﬁrst term is the same as old analysis; the second term is from considering q; the third is the\nsame as the old analysis. In particular, when q = 1/ar, we recover the old bound. We have that the\nﬁrst term is\n∥¯\nA ¯w −¯\nA ¯w⋆∥≤√r L2L2\n∞\n2λ3\n\u0012(ar)2mU\nmǫ0γp\n\u00132\n(120)\n28\n\nPreprint. Under review.\nThe third term is\n∥AF ¯w⋆−AF wF ∥≤√r L∞\nλ\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n(121)\nThe second term is\n∥¯\nA ¯w⋆−AF ¯w⋆∥≤∥( ¯\nA −AF ) ¯w⋆∥+ ∥( ¯\nA −AF )(w⋆−¯w⋆)∥\n(122)\n≤O\n\u0012\nB√nr(1/q)armU\nmǫ0γp\n\u0013\n+ O\n \u0012(ar)2mU\nmǫ0γp\n\u00132√nr\n!\n(123)\nThis gives the desired result using triangle inequality.\nContinuing, we have the following.\nProposition 6. It holds that\n∥(AS)† ¯\nA ¯w −(AS)†AF wF ∥2\n(124)\n≤O\n\u00121\np∥¯\nA ¯w −AF wF ∥2\n\u0013\n(125)\n≤O\n \n1\np ·\n\"\n√r\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n+ B√nr (1/q)armU\nmǫ0γp\n+\n\u0012(ar)2mU\nmǫ0γp\n\u00132√nr\n#!\n(126)\nThis gives us the following.\nLemma 29. The unlearning algorithm Uhead that outputs\n˜v := ¯v + νv\n(127)\nwhere νv is the noise deﬁned by the Gaussian mechanism using the above sensitivity satisﬁes prov-\nable (ǫ, δ) unlearning. In particular, we use\nσ =\nO\n\u0012\n1\np ·\n\u0014√r\n\u0010\n(ar)2mU\nmǫ0γp\n\u0011\n+ B√nr (1/q)armU\nmǫ0γp\n+\n\u0010\n(ar)2mU\nmǫ0γp\n\u00112√nr\n\u0015\u0013\nǫ\np\n2 log(1.25/δ)\n(128)\nwhere the numerator of the fraction is from the previous proposition.\nProof. This follows from Gaussian mechanism.\nWe now proceed to bound the deletion capacity. In this case, the utility is deﬁned by the closeness\nof ˜v to (AS)†A⋆w⋆in ℓ∞norm, similar the way we deﬁned this for the base model unlearning\nalgorithm Ubase earlier.\nFirst, the following lemma to bound AF wF −A⋆w⋆.\nLemma 30. We have that\n∥AF wF −A⋆w⋆∥2 ≤O\n \n√r\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n+ B√nr(1/q)armU\nmǫ0γp\n+\n\u0012(ar)2mU\nmǫ0γp\n\u00132√nr\n!\n(129)\nProof. We decompose as follows.\nAF wF −A⋆w⋆= (AF wF −AF w⋆) + (AF w⋆−A⋆w⋆)\n(130)\nThe ﬁrst term is bounded by\n∥AF wF −AF w⋆∥2 ≤√r∥wF −w⋆∥2 ≤O(√r∥AF −A⋆∥∞) ≤O\n\u0012√r\n\u0012(ar)2mU\nmǫ0γp\n\u0013\u0013\n(131)\n29\n\nPreprint. Under review.\nThe second term is bounded by\n∥AF w⋆−A⋆w⋆∥2 ≤B√nr (1/q)armU\nmǫ0γp\n+\n\u0012(ar)2mU\nmǫ0γp\n\u00132√nr\n(132)\nTriangle inequality gives us the desired result.\nAs a result, the following holds.\nProposition 7. It holds that\n∥(AS)†AF wF −(AS)†A⋆w⋆∥2 ≤O\n \n1\np ·\n\"\n√r\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n+ B√nr (1/q)armU\nmǫ0γp\n+\n\u0012(ar)2mU\nmǫ0γp\n\u00132√nr\n#!\n(133)\nThis is once again from the bounded operator norm property.\nFinally, we can apply triangle inequality to get the following.\nLemma 31. It holds that\n∥(AS)† ¯\nA ¯w −(AS)†A⋆w⋆∥2 ≤O\n \n1\np ·\n\"\n√r\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n+ B√nr (1/q)armU\nmǫ0γp\n+\n\u0012(ar)2mU\nmǫ0γp\n\u00132√nr\n#!\n(134)\nThen, we can get the following bound on deletion capacity.\nLemma 32. For ǫ, δ > 0, the deletion capacity satisﬁes\nT Ahead,Uhead\nǫ,δ\n(m) ≥˜Ω\n\u0012\nm\nr2√nr\n\u0013\n(135)\nProof. The calculation is as follows.\nE\n\u0002\n∥˜v −(AS)†A⋆w⋆∥∞\n\u0003\n≤E[∥νv∥∞] + E\n\u0002\n∥(AS)† ¯\nA ¯w −(AS)†A⋆w⋆∥∞\n\u0003\n(136)\n≤\n \n1\np ·\n\"\n√r\n\u0012(ar)2mU\nmǫ0γp\n\u0013\n+ B√nr (1/q)armU\nmǫ0γp\n+\n\u0012(ar)2mU\nmǫ0γp\n\u00132√nr\n#!\n(137)\n·\n p\nlog r log 1/δ\nǫ\n+ 1\n!\n(138)\nFor this to be a small constant, we require\n(ar)2mU\nmǫ0γp\n≤˜O\n\u0012\nmin\n\u001a 1\nr1/2 ,\n1\n(nr)1/4 , arq\n√nr\n\u001b\u0013\n(139)\nWhen n is at least r3, the last of these terms will be the smallest. Therefore, we have that\nmU ≤˜Ω\n\u0010\nmq\nr1.5n0.5\n\u0011\n(140)\n30",
    "pdf_filename": "Provable_unlearning_in_topic_modeling_and_downstream_tasks.pdf"
}