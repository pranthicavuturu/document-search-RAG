{
    "title": "Preprint.Underreview.",
    "abstract": "Machineunlearningalgorithmsareincreasinglyimportantaslegalconcernsarise aroundtheprovenanceoftrainingdata,butverifyingthesuccessofunlearningis oftendifficult. Provableguaranteesforunlearningareoftenlimitedtosupervised learningsettings. Inthispaper,weprovidethefirsttheoreticalguaranteesforun- learning in the pre-trainingand fine-tuning paradigm by studying topic models, simple bag-of-wordslanguage models that can be adapted to solve downstream taskslikeretrievalandclassification.First,wedesignaprovablyeffectiveunlearn- ingalgorithmfortopicmodelsthatincursacomputationaloverheadindependent of the size of the originaldataset. Our analysis additionallyquantifies the dele- tion capacity of the model– i.e., the numberof examplesthat can be unlearned withoutincurringa significantcost in modelperformance. Finally, we formally extendouranalysestoaccountforadaptationtoagivendownstreamtask. Inpar- ticular, we design an efficient algorithm to perform unlearning after fine-tuning the topic model via a linear head. Notably, we show that it is easier to unlearn pre-trainingdata frommodelsthathavebeenfine-tunedto a particulartask, and onecanunlearnthisdatawithoutmodifyingthebasemodel. 1 INTRODUCTION Modern-day machine learning has shifted from single-stage supervised learning on manually constructed datasets to a paradigm in which models are pre-trained and subsequently fine- tuned (Bommasanietal., 2022). In this setting, a model initially learns a good representation of thedatausingaself-supervisedobjectiveonalargeunstructuredcorpus. Theresultingpre-trained model is later adapted to solve specific tasks for which it is difficult or costly to curate a large dataset.Thisblueprinthasyieldedstrongperformanceintext(e.g.,Devlinetal.,2019;Brownetal., 2020),vision(e.g.,Oquabetal.,2024;Heetal.,2022),andmultimodal(e.g.,Radfordetal.,2021; Zhaietal.,2023)settings. Itiswell-knownthatthescaleofthepre-trainingdataisstronglycorre- latedwiththefinalperformanceofthemodel(Hoffmannetal.,2022),leadingtotheconstructionof largerdatasetsviabroadinternetscrapes(Gaoetal.,2020;Schuhmannetal.,2022;Soldainietal., 2024; Penedoetal., 2023). Such datasets have been found to often inadvertently include private, sensitive,andunsafedata(Birhaneetal.,2021;Longpreetal.,2024;Heetal.,2024). Unsafe data can generally degrade model performance and introduce biases, making the model lessusefulforvariousapplications(McKennaetal.,2023;Birhane&Prabhu,2021;Choennietal., 2021; Naousetal., 2024). Using private and sensitive data, even unknowingly, poses legal risks (Bommasanietal., 2022; Hendersonetal., 2023). In particular, recent works have shown that models can memorize and thus permit the extraction of training data (Somepallietal., 2023; Carlinietal., 2021; 2023). Moreover, one may be requested to remove data in accordance with GDPR’s right to be forgotten (EuropeanParliament&CounciloftheEuropeanUnion), or as part of a copyright-related lawsuit (Tremblayv. OpenAI,Inc.,, 2023; DOE1v. GitHub,Inc., N.D.Cal. 2022). Therefore, there is great empirical interest in developing machine unlearning algorithms that can surgicallyremoveportionsofthetrainingdatafromanalreadylearnedmodelwithoutharmingper- formance. The gold standard for machine unlearning is for the model to behave as though it had neverbeentrainedonthatdatapoint(Cao&Yang,2015). Asitisoftenundesirabletocompletely 1 4202 voN 91 ]GL.sc[ 1v00621.1142:viXra",
    "body": "Preprint.Underreview.\nPROVABLE UNLEARNING IN TOPIC MODELING AND\nDOWNSTREAM TASKS\nStanleyWei,SadhikaMalladi,SanjeevArora AmartyaSanyal\nPrincetonUniversity UniversityofCopenhagen\nstanley.wei, smalladi, arora @princeton.edu amsa@di.ku.dk\n{ }\nABSTRACT\nMachineunlearningalgorithmsareincreasinglyimportantaslegalconcernsarise\naroundtheprovenanceoftrainingdata,butverifyingthesuccessofunlearningis\noftendifficult. Provableguaranteesforunlearningareoftenlimitedtosupervised\nlearningsettings. Inthispaper,weprovidethefirsttheoreticalguaranteesforun-\nlearning in the pre-trainingand fine-tuning paradigm by studying topic models,\nsimple bag-of-wordslanguage models that can be adapted to solve downstream\ntaskslikeretrievalandclassification.First,wedesignaprovablyeffectiveunlearn-\ningalgorithmfortopicmodelsthatincursacomputationaloverheadindependent\nof the size of the originaldataset. Our analysis additionallyquantifies the dele-\ntion capacity of the model– i.e., the numberof examplesthat can be unlearned\nwithoutincurringa significantcost in modelperformance. Finally, we formally\nextendouranalysestoaccountforadaptationtoagivendownstreamtask. Inpar-\nticular, we design an efficient algorithm to perform unlearning after fine-tuning\nthe topic model via a linear head. Notably, we show that it is easier to unlearn\npre-trainingdata frommodelsthathavebeenfine-tunedto a particulartask, and\nonecanunlearnthisdatawithoutmodifyingthebasemodel.\n1 INTRODUCTION\nModern-day machine learning has shifted from single-stage supervised learning on manually\nconstructed datasets to a paradigm in which models are pre-trained and subsequently fine-\ntuned (Bommasanietal., 2022). In this setting, a model initially learns a good representation of\nthedatausingaself-supervisedobjectiveonalargeunstructuredcorpus. Theresultingpre-trained\nmodel is later adapted to solve specific tasks for which it is difficult or costly to curate a large\ndataset.Thisblueprinthasyieldedstrongperformanceintext(e.g.,Devlinetal.,2019;Brownetal.,\n2020),vision(e.g.,Oquabetal.,2024;Heetal.,2022),andmultimodal(e.g.,Radfordetal.,2021;\nZhaietal.,2023)settings. Itiswell-knownthatthescaleofthepre-trainingdataisstronglycorre-\nlatedwiththefinalperformanceofthemodel(Hoffmannetal.,2022),leadingtotheconstructionof\nlargerdatasetsviabroadinternetscrapes(Gaoetal.,2020;Schuhmannetal.,2022;Soldainietal.,\n2024; Penedoetal., 2023). Such datasets have been found to often inadvertently include private,\nsensitive,andunsafedata(Birhaneetal.,2021;Longpreetal.,2024;Heetal.,2024).\nUnsafe data can generally degrade model performance and introduce biases, making the model\nlessusefulforvariousapplications(McKennaetal.,2023;Birhane&Prabhu,2021;Choennietal.,\n2021; Naousetal., 2024). Using private and sensitive data, even unknowingly, poses legal\nrisks (Bommasanietal., 2022; Hendersonetal., 2023). In particular, recent works have shown\nthat models can memorize and thus permit the extraction of training data (Somepallietal., 2023;\nCarlinietal., 2021; 2023). Moreover, one may be requested to remove data in accordance\nwith GDPR’s right to be forgotten (EuropeanParliament&CounciloftheEuropeanUnion), or\nas part of a copyright-related lawsuit (Tremblayv. OpenAI,Inc.,, 2023; DOE1v. GitHub,Inc.,\nN.D.Cal. 2022).\nTherefore, there is great empirical interest in developing machine unlearning algorithms that can\nsurgicallyremoveportionsofthetrainingdatafromanalreadylearnedmodelwithoutharmingper-\nformance. The gold standard for machine unlearning is for the model to behave as though it had\nneverbeentrainedonthatdatapoint(Cao&Yang,2015). Asitisoftenundesirabletocompletely\n1\n4202\nvoN\n91\n]GL.sc[\n1v00621.1142:viXra\nPreprint.Underreview.\nretrainmodels,especiallyastheygrowlarger,manyworkshaveproposedcomputationallycheaper\nheuristicsforsolvingthisproblem(e.g.,Jangetal.,2023;Fosteretal.,2024;Kurmanjietal.,2023;\nZhangetal.,2024b;Eldan&Russinovich,2023;Gandikotaetal.,2023). Intheabsenceoftheoret-\nicalguarantees,itiscommontouseempiricstomeasurethesuccessofthesealgorithms. However,\nrecentworkshave shownthat such evaluationsoften overestimatethe success of these unlearning\nmethods(Hayesetal.,2024; Shietal., 2024;Mainietal., 2024)andthusithasprovendifficultto\nconfidentlyascertain whether the proposedmethodsmeet the necessary compliancestandards. In\nthis context, it is highly desirable to design efficient unlearning algorithms with well-motivated\nguaranteesthataresalienttothepre-trainingandfinetuningparadigm(Thudietal.,2022;Leeetal.,\n2024).\nWhile there are some instancesof such algorithmsfor linear models(Guoetal., 2020; Izzoetal.,\n2021;Mahadevan&Mathioudakis,2023),generalconvexmodels(Ullahetal.,2021;Sekharietal.,\n2021;Neeletal.,2021),Bayesianmodels(Nguyenetal.,2020),andGANs(Liuetal.,2024),there\narenoworksontheparadigmofpre-trainingandfine-tuningalgorithms. Oneofthemostclassical\nsuch algorithmsis topic modeling(Hofmannetal., 1999; Bleietal., 2003; Blei&Lafferty, 2006;\nLi&McCallum,2006),whichcanalsobethoughtofasthesimplestlanguagemodel. Inthispaper,\nwepresentthefirstprovablyeffectiveandefficientunlearningalgorithmsfortopicmodels.\nTopicmodelsaregenerallypre-trainedtoextractlatentstructure(i.e.,asmallsetofunderlyingtop-\nics) from a large corpus of documents. This feature extractor is then used for a variety of down-\nstream applications, including retrieval, classification, and recommendation (Boyd-Graberetal.,\n2017). Despite their simplicity, topic models can be used to effectively solve many real-world\nnaturallanguageproblems—seeasurveyinChurchill&Singh(2022).\n1.1 OVERVIEW OF RESULTS\nWefocusonthesettinginAroraetal.(2012b),becauseitadmitsanefficientlearningalgorithmwith\nprovable guarantees (Aroraetal., 2012a). The corpus is assumed to contain r underlying topics,\nwhereeachtopicdefinesadistributionoverwords. Let beadistributionovertopicdistributions.\nD\nThen,eachdocumentdisgeneratedbysamplingatopicdistributionW overtopics,andthen\nd\n∼D\nsamplingwordsaccordingtoW .\nd\nThe dataset of m documents is a matrix M Rn m, where M permits a non-negative matrix\n×\nfactorizationM = A X. Here,A Rn r i∈ sthedistributionofwordsineachoftherunknown\n∗ ∗ ×\nunderlying topics, and X Rr m∈ is the sampled distribution of topics in each document. In\n×\nparticular,A⋆,Xhavecolum∈\nnsontheprobabilitysimplex.Weseektolearntheembeddingfunction\nA andthetopic-topiccovarianceR⋆ =E [XX ].\n∗ ⊤\nD\nToderiveprovableguaranteesonthesuccessofunlearning,weadaptthenotionof(ǫ,δ)-unlearning\nintroducedin Sekharietal. (2021) to thetopicmodelingsetting. Theunlearnedmodelisrequired\nto behaveindistinguishablyfroma modelthatwas retrainedonthe modifieddataset. We definea\nnotionofutility-preservingunlearningthatcombinesthisconditionwithananalysisonthedeletion\ncapacity – i.e., the number of datapoints that can be unlearned without performance degradation\n(Definition4). Wenowstateourmainresultonutility-preservingunlearningintopicmodels.\nMainResult1(InformalversionofTheorem2). SupposewetrainedatopicmodelAS,XS ona\ntrainingsetS containingmdocuments.Algorithm1canperformutility-preservingunlearningof\nm\nm = ˜\nU O r2√nr\n(cid:18) (cid:19)\ndocumentsfromthepre-trainedtopicmodel,where ˜()hidesconstantsdependingonthelearning\nO ·\nandunlearningalgorithm.\nTo adapta topic modelto a downstreamtopic classificationtask, we learna headw Rr on top\n∈\nofAtominimizeastronglyconvexlossfunction(Definition2). WhenAandwarebothreleased,\nonewouldnecessarilyhavetofirstunlearnfromA, whichmakesunlearningjustashardasitwas\nin pre-training(Theorem3). Thissetting is ratherunrealistic, because there is no obviouscase in\nwhichonewouldwanttousew withoutAorviceversa. Wethusadvocateforviewingfine-tuned\nmodelB = Awasawholei.e. itisnotallowedtoaccessoutputsofAsolely,andweshowthatit\niseasiertoperformutility-preservingunlearningofpre-trainingdatainthiscase.\n2\nPreprint.Underreview.\nMain Result 2 (Informalversion of Theorem4). After adaptingthe modelto a downstreamtask\n(Definitions 1 and 2), Algorithm 2 can perform utility-preserving unlearning of Ω˜ mq doc-\nr√nr\numents, where q [1/r,1] is a task-dependent quantity, without modifying the ba(cid:16)se mo(cid:17)del A.\n∈\nSimplerdownstreamtaskshavealargerq,increasingtheseparationfromthepre-trainingresult.\nWe demonstrate that our unlearning algorithms run substantially faster than retraining the model\n(Table 1). Overall, our results imply the following takeaways in the context of topic models —\n(1)Itispossibletoeffectivelyandefficientlyunlearndatapointsfroma pre-trainedmodelwithout\nretrainingit (Algorithm1 andTheorem2), (2) Onecan effectivelyunlearnmorepre-trainingdata\nfromamodelthathasbeenadaptedtoadownstreamtaskwithoutharmingtheutilityofthebaseand\nfine-tunedmodels(Theorem4),and(3)Onecanunlearnpre-trainingdatafromafine-tunedmodel\nwithoutmodifyingthebasemodel(Algorithm2andTheorem4).\n2 TOPIC MODELS\nAswepreviouslydiscussed,topicmodelscanbeconsideredasoneofthesimplestlanguagemodels\nthat one can pre-train in a self-supervised fashion and later fine-tune for other language-related\ntasks. This pipeline mirrors the modern-day paradigm of pre-training large language models to\nbuild a general understandingof natural language and later fine-tuning them to solve a variety of\ntasksrangingfromclassificationtocodegeneration.\n2.1 PROBLEM DESCRIPTION\nTopic modeling is a classical, bag-of-words method to discover structure in a corpus of docu-\nments (Hofmannetal., 1999). One assumes that each document contains a convex combination\noftopics, eachofwhichcanbedescribedintermsofadistributionoverthevocabulary. Different\nassumptionsonthestructureofthisdistributionandthetopicshaveyieldedavarietyoftopicmod-\nelingmethodologies(Blei&Lafferty,2006;Li&McCallum,2006)–perhapsmostfamousamong\ntheseisthelatentDirichletallocation(LDA,Bleietal.(2003)). Manyearlyworksestablishedthe\nstatisticallearnabilityoftopicmodelsundersuchassumptions,butthelearningalgorithmsgenerally\nwerenotefficientinreal-worldsettings(Aroraetal.,2012b;Rechtetal.,2012).\nOurpaperfocusesonthesettinginAroraetal.(2012b),forwhichAroraetal.(2012a)providedan\nempirically efficientlearning algorithm. The dataset consists of a set of m documentsd ,...,d ,\n1 m\nwhereeachdocumentcontainsLwordsfromavocabulary with =n.1 Thecorpuscontainsr\nV |V|\ndifferentunderlyingtopics,eachofwhichdefinesadistributionoverwords.Eachwordindocument\ndisgeneratedby: (1)samplingadistributionovertopicsW ,andthen(2)samplingLwords\nd\n∼ D\nindependentlyaccordingtoW .\nd\nWerepresentthecorpusasamatrixM Rn m,whereM permitsanon-negativematrixfactoriza-\n×\ntionM =A⋆X. Here,A⋆ Rn r ist∈ hedistributionofwordsineachofthertopics,X Rr m\n× ×\n∈ ∈\nis the distribution of topics in each document, and hence M is the distribution of words in each\ndocument.WhilethereareseveralalgorithmsforlearningthefeatureextractorA⋆,itiswell-known\nthat it is hard to recoverX exactly (Aroraetal., 2012b). Instead, it is desirable to learn how the\ntopics co-occur together, denoted as R⋆ = E [XX ]. This quantity is termed the topic-topic\n⊤\ncovariance.FurtherdiscussionofthishasbeenDincludedinAppendixA.\nThetopicmodelingsettinggenerallydetermines (e.g.,inLDA, isaDirichletdistribution). In\nD D\nordertorecoverA andR efficientlyandaccuratelyfromanobservedcorpusM ,weneed\n∗ ∗\n∼ D\ntomakethefollowingassumptionontheunderlyingdatadistribution.\nAssumption1(p-separability,Aroraetal.(2012b)). ThetopicmatrixA⋆ isp-separableforp >0\nif for every topic k [r], there exists a word i [n] such that A p and A = 0 for all\n∈ ∈\n∗i,k\n≥\n∗i,k′\nk =k. Suchwordsarecalledanchorwords.\n′\n6\nWithout this separability assumption, maximum likelihood estimation of a topic model is NP-\nhard (Aroraetal., 2012b). Assumption 1 requires that A⋆ contains a diagonal matrix, up to row\npermutations; intuitively, the appearanceof an anchor word in a documentperfectly indicates the\n1Withoutlossofgenerality,weassumeL=2.\n3\nPreprint.Underreview.\ndocumenthasnonzeroprobabilityofthecorrespondingtopic.AswewilldetailinSection4,thisob-\nservationinspiresatwo-phaselearningalgorithm,wherebyonefirstapproximatestheanchorwords\nforeachtopicandthenleveragesthemtoidentifypatternsamongthetopics.\n2.2 DOWNSTREAMADAPTATION\nTopic models are frequently trained on a general corpus, and the embeddings can be later used\nto classify documents. The classification problem usually involves only a subset of topics. For\nexample, after training a topic model on a large corpus of news articles with diverse topics (e.g.,\nsports,politics,technology,finance,etc.),onerelevantdownstreamtaskistoclassifythesubjectof\nagivennewsarticleassportsorpolitics. Weformalizethetopicclassificationtaskbelow.\nDefinition1(TopicClassificationTask). Atopicclassificationtask = (T ,w⋆)isdefinedbya\nclf\nsubsetoftopicsT [r]onwhichthetaskisdefinedandaground-T truthlabellingvectorw⋆ Rr\nclf\nwithboundednorm.⊂ Importantly,w⋆ onlyhasnon-zerocoordinatesinthepositionscorrespon∈ ding\ntoT .\nclf\nTheclassificationtaskisdefinedonthelatentfeaturesofagivendocument,soitisnecessarytofirst\nidentifythesalienttopicsastheyoccurinthetext. Fittingatopicmodeltothecorpusyieldssucha\nfeatureextractorAthatembedsadocumentintother-dimensionaltopicspace. Inordertoadapta\ntopicmodeltoaparticularclassificationtask,weperformheadtuningonthefeatureextractorA.\nDefinition2(HeadTuning). ForagivenlabelleddocumentclassificationdatasetD = (d ,y )\nclf i i\nrepresentingatopicclassificationtask ,embedeachdocumentd asavectorx\nRnc{ ontainin}\ng\ni i\nT ∈\nthewordcountsinthedocument.Toperformheadtuningonapre-trainedtopicmodelA,welearn\nw Rr tominimize\n∈ 1\nℓ (w;A)=\nD\nf(x ⊤Aw,y)\nT | clf |(x,Xy) ∈D clf\nwhereℓ isstronglyconvexinw.\nT\nOne example of f is the logistic loss with ℓ regularization. For ease of exposition, we primarily\n2\nconsider binary classification tasks, but we point out that the definition can extend to multi-class\ntaskssolvedviatheone-vs-allscheme(Rifkin&Klautau,2004).\nWenotethatheadtuning,alsoreferredtoaslinearprobing,isasimpleradaptationtechniquethan\nfine-tuningA alongside w. Nonetheless, recentworks on popularlanguagemodels have demon-\nstratedthatheadtuningcansubstantiallyimprovetheabilityofgeneralpre-trainedlanguagemodels\nto solve complex classification tasks (Malladietal., 2023a;b). Head tuning thus serves as a con-\nvenientyeteffectiveadaptationmethodthatavoidsupdatingthepre-trainedmodel,whichisoften\ndesirable. Forexample,ifasinglepre-trainedmodelneedstobeseparatelyadaptedtosolvemany\ndifferenttasks,thenitisdesirabletominimizethenumberofparametersthatarefine-tunedtomin-\nimizethememoryneededtostorealloftheadaptedmodels.2\n3 UNLEARNING\nAsmentionedpreviously,thereisincreasedinterestinmachineunlearningduetothegrowingscale\nof modern datasets and the difficulty of manually inspecting each datapoint. Theoretically, the\ngold standard for unlearning is that the model should behave identically to one that was trained\nwithoutthedatapointinitscorpus(Cao&Yang,2015).Wefirstdefinewhatitmeansfortwomodels\nθ ,θ Θtobehavealmostidentically,whereΘdenotestheparameterspaceofahypothesisclass.\n1 2\n∈\nDuetorandomnessinlearning,θ andθ arerandomvariables.\n1 2\nDefinition3((ǫ,δ)-indistinguishablemodels,Dworketal.(2014)). Twomodelsdenotedbyrandom\nvariablesθ ,θ Θare(ǫ,δ)-indistinguishableifforallpossiblesubsetsofmodelsT Θ,\n1 2\n∈ ⊆\nPr(θ T) eǫPr(θ T)+δ\n1 2\n∈ ≤ ∈\nPr(θ T) eǫPr(θ T)+δ\n2 1\n∈ ≤ ∈\nǫ,δ\nWedenotethisasθ θ .\n1 2\n≈\n2Thismotivationhasdrivenwidespreaddevelopmentandadoptionofparameter-efficientfine-tuningmeth-\nodsforlargelanguagemodels.Liuetal.(2021)containsasurveyofsuchtechniques.\n4\nPreprint.Underreview.\nWe adapt the definitions from Sekharietal. (2021) to the topic modeling setting. A learning al-\ngorithm takes in a set of m documents, denoted as S, and returns a topic model θ = (A,R).\nA\nAnalogously,an unlearningalgorithm takesin the learnedtopic modelθ, a setofdocumentsto\nU\nunlearnS S,somestatisticsonthetrainingsetT(S),andoutputsamodel.Thesetofdatapoints\nf\n⊆\nto unlearn S is often referredto as the forget set. With this in mind, we now define a notion of\nf\nutility-preservingunlearning,wherebytheunlearningalgorithmneedstonotonlyeffectivelysimu-\nlateretrainingthemodelfromscratchbutalsomaintainthemodel’sperformance.\nDefinition4(Utility-preserving(ǫ,δ)-UnlearningwithDeletionCapacity). Letm Nbeacon-\n0\n∈\nstantthatdependsonthetopicmodelingdistribution satisfyingAssumption1. Foranytraining\nD\ni.i.d.\ndataset S of size at least m , and ǫ,δ > 0, we say that a pair of learning and unlearning\n0\n∼ D\nalgorithms( A, U)performsutility-preservingunlearningwithdeletioncapacityT ǫA,δ, U(m)if\n1. Withprobabilityatleast0.9overdrawsfrom ,foranyforgetsetS S ofsizeatmost\nf\nD ⊆\nT ǫA,δ, U(m),modeltrainedonS \\S\nf\nisindistinguishablefromthatresultingfromunlearning\nwith .\nU ǫ,δ\n(S , (S),T(S)) ( , (S S ),T(S S ))\nf f f\nU A ≈ U ∅ A \\ \\\n2. Evenfor an adversariallychosenS , the unlearnedmodeldoes notsuffer a largeperfor-\nf\nmancedegradation.Formally,\nE max h( (S , (S),T(S))) h⋆ 0.01\n, f\nAU\" |Sf|≤T ǫA ,δ,U(m) U A − #≤\nwhereh:Θ Risthelossofthetopicmodel,andh⋆ =min h(w)istheirreducibleloss.\nw\n→ ∈W\nTheabovedefinitioncanbeappliedtoboththepre-trainingandthedownstreamadaptationstages\nof training a topic model. Of particular note is that (1) does not guarantee (2), since the former\nonly concernsindistinguishabilitybetween the unlearnedand retrained models, while the latter is\na statementaboututility preservation. Moreover,unlessT(S)containsthe entiredataset, we note\nthattheunlearningalgorithm cannotbeassimpleasretrainingthemodel. Inthispaper,wewill\nU\ndesignanunlearningalgorithmfortopicmodelsthatsatisfiesthisdefinitionofprovableunlearning,\nandthenumberofstatisticsT(S)willnotdependontheinitialdatasetsizem.\nToshow(ǫ,δ)-indistinguishability,weutilizetheGaussianmechanism,aclassictoolfromdifferen-\ntialprivacy. Givenaparticularfunction,theGaussianmechanismessentiallyprescribeshowmuch\nnoise onemustaddto the outputin orderforthe inputto be indistinguishablefroma similar one.\nTheguaranteeoftheGaussianmechanismisdescribedinthefollowinglemma.\nLemma1(GaussianMechanism,Dworketal.(2014)). Letf beanarbitraryd-dimensionalfunc-\ntion,anddefineitsℓ -sensitivitytobe∆ f := max f(x) f(y) . Then,forc2 >2log1.25,\n2 2 adjacentx,yk − k2 δ\ntheGaussianmechanismwithparameterσ c∆ f/ǫis(ǫ,δ)-differentiallyprivate.\n2\n≥\nInourcase,wedefineadjacentinputs(i.e.,trainingdatasets)asthecasewhereyisasupersetofx.\n4 LEARNING AND UNLEARNING TOPIC MODELS\nInthissection,wepresentthelearningandunlearningalgorithmsandguaranteesfortopicmodels.\nNotation. WeuseA⋆torefertotheground-truthtopicmodel,AS torefertoatopicmodeltrained\nonS,andAF todenoteatopicmodelretrainedwiththeforgetsetremovedS S . WealsouseA¯\nf\n\\\ntodenotetheunlearnedtopicmodelbeforeapplyingtheGaussianmechanismandA˜ todenotethe\nmodelafterthemechanismisapplied.AnalogousnotationsareusedforR.\n4.1 LEARNING ALGORITHM ANDGUARANTEES\nPer Aroraetal. (2012a), the learning algorithm takes in a corpus of documents S =\nbase\nd ,...,d andconsistsofthefollowingthreephasA estolearnatopicmodelθ =(AS,RS).\n1 m\n{ }\n5\nPreprint.Underreview.\n1. Measure the word co-occurrences. Compute the word co-occurrence matrix Q Rn n,\n×\n∈\nwhereQ is the numberoftimes wordi appearsin the same documentasword j. We also\nij\ncomputeQ¯, whichnormalizesthe rowsof Q to sum to 1. A detaileddiscussion of the con-\nstructionofQanditsrelationshiptothefactorizationM =A⋆X isincludedinAppendixA.\n2. IdentifytheanchorwordsP. Recallthatinordertobeabletolearntopicmodelsefficiently,\nthere must exist a set of anchor words P with P = r, and each anchor word must appear\nexclusivelyinasingletopic(Assumption1). Thi| ss| ubroutineusesQ¯ toapproximatelyidentify\ntheranchorwordsP.\n3. LearnthefeatureextractorAS andthetopic-topiccovarianceRS. Thealgorithmusesthe\nanchorwordsP andthewordco-occurrencesQ¯ tolearnAS andRS. Eachwordisexpressed\nas a convexcombination of anchor words, and thus, topics. With appropriatenormalization\nandbycross-referencinginformationwiththeco-occurrencematrix,onecanrecoverA⋆,R⋆\nintheinfinitedatalimit.\nWesketchhowthisalgorithmrecoversthegroundtruthA⋆,R⋆ whenonehasinfinitelymanydoc-\numentsinAppendixA.Aroraetal.(2012a)givesthefollowingfinite-documentguarantee.\nTheorem1(LearningGuarantee). Running onadatasetS ofsizem,wheremisatleast\nbase\nA\nar3logn a3r3logn r2logr\nmax , ,\nO L(γp)6ǫ O Lǫ3(γp)4 O Lǫ2\n(cid:26) (cid:18) 0(cid:19) (cid:18) 0 (cid:19) (cid:18) 0 (cid:19)(cid:27)\nrecoversAS andRS withentrywiseadditiveerroruptoǫ fromthegroundtruthA⋆,R⋆,respec-\n0\ntively. Here,aisthetopicimbalanceparameter,andγ istheconditionnumberofthegroundtruth\nR⋆. Formally,wehavea=max Pr [z =i]/Pr [z =j].\ni,j [r]\n∈ D D\nApproximatingtheanchorwords.Wedeferaprecisedescriptionoftheanchorwordidentification\nalgorithmto AppendixA and instead focushere on the intuitionsdriving its design and the guar-\nanteeswe willuse throughoutthe paper. First, we note the relationshipbetween Q¯ and the set of\nanchorwords.Ifwehadinfinitelymanydocuments,thentheconvexhulloftherowsinQ¯ willbea\nsimplexwithverticescorrespondingtotheanchorwords,becauseeachanchorwordcorrespondsto\natopic,andeachtopicprescribesadistributionoverwords.However,inthefinitedocumentsetting,\neachrowofQ¯ onlyapproximatestheirexpectedvalue,andsoonemustapproximatetheverticesof\naconvexhullwhengivenaccesstoaperturbationofthepointsthatdefineit.\nWe start by requiring that each topic is distinctly different from any mixture on the other topics.\nFormally,thisrequiresthatthesimplexisrobust,inthateachvertex(i.e.,anchorword)issufficiently\nfarfromanycombinationoftheothertopics. Mosttopicmodelingsettingsdefinelowerboundson\nthe robustness of the simplex. By a result in Aroraetal. (2012b), the simplex defined by the r\nanchorwordrowsofthepopulationQ¯ isγp-robust. Wecannowdefineexactlythesenseinwhich\naQ¯ computedonafinitedatasetapproximatesthepopulationco-occurrencematrix.\nDefinition5. Let a n beasetofpointswhoseconvexhullP isasimplexwithvertices v r .\nWe say a set of r{ poi } ini= ts1 is ǫ-close the vertex set v r if each of the r points is ǫ-clo{ sei } ini= ℓ1\n{ i }i=1 2\ndistancetoadifferentvertexinP. Moreover,wesaythatasimplexP isβ-robustifforeveryvertex\nvofP,theℓ distancebetweenvandtheconvexhulloftherestoftheverticesasatleastβ.\n2\nInthecontextofthisdefinition,P correspondstothegroundtruthconvexhull,andthefinitesample\nQ¯ can be seen as a perturbationto it. In particular, Aroraetal. (2012a) used this to established a\nguaranteeontheaccuracyofanchorwordrecovery.\nLemma 2 (Approximation Guarantee on Anchor Words). Suppose each row of Q¯ is at most δ\ndistanceawayfromthegroundtruthγp-robustsimplex Q¯⋆ inℓ norm. If20rδ/(γp)2 < γp,then\n2\nthesetofanchorwordsfoundbythealgorithmisO(δ/γp)-closetothegroundtruthanchorwords.\nWenowdescribehowtousetherecoveredapproximateanchorwordstolearnthetopicmodel.\nLearningthetopicmodelfromanchorwords. WearegiventhesetofanchorwordsP,theword\nco-occurrencematrixQ Rn n, andthenormalizedco-occurrencematrixQ¯. Ourgoalistouse\n×\nthese quantitiesto learnA∈ Rn r and R Rr r. We will doso byfirst expressingeachword\n× ×\n∈ ∈\ni [n]asaconvexcombinationoftheanchorwords(andthus,thetopics). Inparticular,foreach\n∈\nwordi,welearnthecoefficientsC ∆ as\ni r\n∈\nC =argmin Q¯ v Q¯ 2 (1)\ni i ⊤ P\nk − k\nv ∈∆r\n6\nPreprint.Underreview.\nAlgorithm1Unlearningalgorithm( )\nbase\nU\nInput: ForgetsetS S, statistics T(S)whichinclude CS n , QS,P, normalizationcon-\nf ⊆ { i }i=1\nstantspS\nOutput: UnlearnedmodelA˜,R˜\nComputetheupdatedco-occurrencematrixQF bysubtractingdocumentsinS\nf\nStoretheupdatednormalizationconstantspF =QF1\nforiin1,...,ndo\nNewtonstepupdateonC ’s:\ni\nC¯ iF ←C iS −H C− iS1 ∇L(C iS,S \\S f) (2)\nC¯F proj (C¯F) (3)\ni ← ∆r i\nwhere (v,S S ):= Q¯F v Q¯F 2andH = 2 (CS,S S )\nL \\ f k i − ⊤ Pk C iS ∇ L i \\ f\nendfor\nA¯ =diag(pF)C¯\n′\nA¯=columnnormalizedA¯\n′\nR¯ =A¯ QFA¯ whereA¯ isthepseudoinverseofA¯\n† †⊤ †\nSampleνA,νRfromnormaldistributiondefinedbyGaussianmechanismguarantee\nA˜=ProjecteachcolumnofA¯+νAto∆ n.\nR˜ =ProjectR¯ +νR ontothesetofPSDmatrices.\nreturn TheunlearnedtopicmodelA˜,R˜\nwhereQ¯ istheP rowsofQ¯ correspondingtotheanchorwords. Aroraetal.(2012a)showedthe\nP\nfollowingapproximationguaranteeforC comparedtotheground-truthcoefficients.\ni\nLemma3. When20rδ/(γp)2 <γp,foreverywordi,C hasentrywiseerrorO(δ/(γp)2)fromC⋆.\ni i\nWethennormalizethisC bythetotalnumberofco-occurrencesthatwordiisinvolvedin.Notethat\ni\ntheC canbeassembledintoamatrixC Rn r. WesetAtobeC afternormalizingthecolumns\ni ×\n∈\nsum to 1, since the columnsrepresent the topic-conditioneddistribution over the vocabulary. We\nfinallycomputeR=A †QA †⊤,whereA †denotesthepseudoinverseofA.\n4.2 UNLEARNING ALGORITHMAND GUARANTEES\nLearningPhase RetrainTime UnlearningUpdate UnlearningTime\nCo-occurrencematrixcomputation O(m) Updatingfrequencies O(mU)\nIdentifyanchorwords O(n2+nr/ǫ2 0) Uselearnedanchorwords O(1)\nRecovertopicsfromanchors O(n2r+nr2/ǫ2 0) ProjectedNewtonstep O(nr2)\nHeadtuningw(Definition2) ERM Newtonstep O(r3)\nTable1: Ourunlearningalgorithmsgenerallyhavearuntimeshorterthantheretrainingprocedure.\nERMdenotesempiricalriskminimization,andwenotethetrainingtimereliesontheerrortolerance.\nWe describeourunlearningalgorithm toforgetasetS fromatrainedmodel(Algorithm1),\nbase f\nwhich crucially updates C with a NewU ton step. We then compute A¯ from the modified C and\ni i\napplytheGaussianmechanismtoensureindistinguishability. Wedescribeourformalguaranteeon\ntheunlearningalgorithmbelow,sketchingoutourutilitypreservingguaranteeswithrespecttoA⋆.\nTheargumentsforR⋆ followanalogously;wedeferthediscussiontotheappendix.\nTheorem 2 (Utility-Preserving Unlearning on the Base Model). Let be the learning algo-\nbase\nA\nrithm describedinthe priorsectionsand betheunlearningalgorithminAlgorithm1. Then,\nbase\nU\n( , )performsutility-preservingunlearningwithdeletioncapacity\nbase base\nA U\nm\nT ǫA,δbase, Ubase(m) ≥c\n· r2√rn\n(4)\n7\nPreprint.Underreview.\nwhere m is the number of training documents, r is the number of topics, and c is a constant de-\npendentonǫ,δ,and . Thelossfunctionhusedintheutility-preservingdefinitionisthemaximum\nentrywiseerrorfromD thegroundtruthtopicmodelA⋆.\nProofsketch. ThefullproofcanbefoundinAppendixB.2. Wedeletem\n0.001mǫ0(γp)3\npoints.\nU\n≤\na2r2\nThis upper bound ensures that the anchor words are likely unchanged per Lemma 2. Recall that\nutility-preserving unlearning requires: (1) that the unlearned model is indistinguishable from the\nretrainedmodel,and(2)thattheunlearnedmodelisnottoofarfromtheground-truthmodel.\nIndistinguishability.TheGaussianmechanismintroducedinLemma1allowsustomaketwomodels\nwithagivenℓ -sensitivity(ǫ,δ)-indistinguishablefromeachother. We boundtheℓ -sensitivityof\n2 2\nthefeatureextractorAbynotingthatA¯isarescaledversionofC¯.\nLemma4. Forǫ,δ >0,thefollowingholdsfortheC¯ andthetopicmatrixA¯:\narm\nC¯ CF c U A¯ AF (ar) C¯ CF (5)\nk − k∞ ≤ · mǫ 0γp k − k∞ ≤ ·k − k∞\nApplyingtheGaussianmechanismwithnoiseσ = ∆ 2log(1.25/δ),where∆=c√nr\n(ar)2mU\nǫ · mǫ0γp\nandfollowedbyprojectingthecolumnsofA¯+νAba pckto∆ nyieldsthedesiredresult.\nUtilityPreservation. WefirstapplyLemma2toshowthat,withhighprobability,theanchorwords\ndo not change when unlearning m documents. Then, we use Lemma 8 to bound the distance\nU\nbetweentheunlearnedC¯ andthegroundtruthC⋆.AccountingforthenoiseaddedviatheGaussian\ni i\nmechanismcompletestheproof.\nLemma5. Forǫ,δ >0,denotetheunlearnedmodelaftertheGaussianmechanismdescribedabove\nasA˜. Then,A˜satisfies:\n(ar)2m log(1/δ)\nE A˜ A⋆ c U √nr log(nr) +1 (6)\nk − k∞ ≤ · mǫ 0γp · · · p ǫ !\nh i p\nEach of the two terms in the above equation yield a constraint on m . In particular, m\nU U\n≤\nmin ˜ m , m ,sosettingm ˜ m completestheproof.\nO r2√nr O r2 U ≤O r2√nr\nn (cid:16) (cid:17) (cid:0) (cid:1)o (cid:16) (cid:17)\n5 UNLEARNING WITH RESPECT TO A DOWNSTREAM TASK\nWeareinterestedinunlearningasetofpre-trainingdocumentsS S. Atopicclassificationtask\nf\n⊆\nisusuallydefinedonasubsetofthetopicsinthedataset—forexample,ifthepre-trainingcorpus\ncontaineddiversenewsarticles,oneplausibledownstreamtaskistoclassifythecontentofagiven\ndocumentas containing politics or sports. Definition 1 formalizesthis: a topic classification task\n=(T ,w )isdefinedonasubsetofthetopicsT andar-lengthground-truthlabellingvector\nclf ∗ clf\nwT , where w onlyhasnon-zerovaluesin positionscorrespondingto T . We describe\n∗ head ∗ clf\n∈ W\ntwopossiblesettingsunderwhichwecanshowutility-preservingunlearning.\n5.1 NAIVE SETTING\nInthefirstsetting,thelearningalgorithm returnsthepre-trainedfeatureextractorAand\nhead,naive\nA\ntheheadwseparately.So,wemustensurethattheforgetsetS Scannotberecoveredfromeither\nf\n⊆\nAorw. Assuch,wemustnecessarilyperformunlearningonAasdescribedinAlgorithm1,which\nmeansthatunlearningthefine-tunedmodelisexactlyasdifficultasunlearningthebasemodel.\nTheorem 3 (Unlearning when releasing A and w). For a downstream task with loss func-\nT\ntion ℓ , consider the unlearning algorithm that first runs Algorithm 1 to compute\nhead,naive\nA˜= T (S , (S),T(S)),where( ,U )performsutility-preservingunlearning(Theo-\nbase f base base base\nU A A U\nrem 2). Then, itfitsaheadw = argmin ℓ (w;A˜)andreturnsA˜ andw. We assertthat\nw\n( ,\n)performsutility-pres∈erWvh ie nad guT\nnlearning(Definition4).\nhead,naive head,naive\nA U\n8\nPreprint.Underreview.\nAlgorithm2Unlearningalgorithmfortask ( )\nhead\nT U\nInput:DocumentdeletionrequestsS S,statisticsT(S)whichincludeAS, CS n ,QS,P,\nf ⊆ { i }i=1\ndiag(pS),wS =argmin ℓ (w;AS)\nw\nA¯,R¯ =RunAlgorithm1( ∈Whe )ad upT totheGaussianmechanism\nbase\nrw¯ et= urw nS (A− SH\n)\nw A− ¯S1 w¯∇ +wℓ ξT ,( iw nU aS c; cA o¯ r) dw anh ce ere wH ithw tS he= G∇ au2 w ssℓ iT an(w mS e; cA h¯ a)\nnism\n†\nGiven the guarantee on A˜ from Theorem 2, we show that this result extends to w by the well-\nknownfact:forǫ,δ >0,post-processingindistinguishablequantities(Definition3)preserves(ǫ,δ)-\nindistinguishability(Dworketal.,2014). ThefullproofofutilitypreservationcanbefoundinAp-\npendixC,whichessentiallyboilsdowntoaLipschitzcondition.However,therearesomedownsides\ntothisalgorithm. First, itrequiresretrainingtheheadw foreachunlearningrequest,butwewant\nto perform unlearning without access to D . Second, repeatedly noising the base model via the\nclf\nGaussianmechanismwillerodeitsutility. Weaddresstheseissuesintherealisticsetting.\n5.2 REALISTICSETTING\nThereislittlereasontoreleaseAandwseparatelyafterfine-tuningthemodel,becauseitisunclear\nwhyonewouldwanttouseAwithoutworviceversa.OnecanobtainAdirectlyafterpre-training\ninstead of relying on a fine-tuned model, and there is little use for w alone, because it is highly\nsensitive to thespecific topicsextractedby Aand theirordering. Assuch, we argueforreleasing\nthefine-tunedmodelasasinglematrix3B =Aw,whereB Rn 1.\n×\n∈\nTheorem4(Utility-PreservingUnlearningontheDownstreamTask). Supposethatthedownstream\ntask onlydependsonasubsetoftopicsT [r]; thatis, w⋆ = argmin ℓ (v;A⋆)has\nclf v\nnon-zT ero entries only in the index set T clf. Den⊆ ote q := min k ∈T clfPr D[z =∈kW],ba ase ndT let Ahead be\nthe headtuningalgorithm (Definition2) and be Algorithm 2. Then, ( , ) performs\nhead head head\nU A U\nutility-preservingunlearningwithdeletioncapacity\nmq\nT ǫA,δhead, Uhead(m) ≥c\n′\n· r√nr\n(7)\nwherec isaconstantdependentonǫ,δ, ,and .\n′\nD T\nThefullproofisinAppendixC,includingtheworstcaseofT =[r]. Whenthetaskreliesheavily\nclf\noneverysingletopic(i.e.,q =1/ar),theaboveguaranteeisequivalenttotheoneinthepre-training\nphase. However, in most realistic settings, the downstream task will only depend on a subset of\nthe latent topics in the corpus. In this case, q > 1/ar, and we can unlearn more points without\ndegradingthe utility of the model. Intuitivelythismakes sense too; the morereliance hason a\nT\nraretopic,thelessadversarialdeletionitcantolerate.\nProofsketch. We againassume thatwe aredeletingm\n0.001mǫ0(γp)3\npoints. Foranymod-\nU\n≤\na2r2\nification made to A, there is an equivalent modification that can be made to w instead such that\nB = Aw is preserved, so we do not need to update A. We look for v such that\nhead\nASv = AFwF, where wF is the head learned on AF. It can be shown tha∈ t A¯WS has a unique\npseudoinversesinceitisfullrank;naturally,wesetv =AS†AFwF,therebyensuringprivacyeven\nif one recoversa part of A from B = Aw. We furthermoredefine v¯ that is fit to the unlearned\nmodelbeforetheGaussianmechanism,v¯ = AS†A¯w¯. Wenowneedtoshowv andv¯satisfyboth\ntheindistinguishabilityandutilitypreservationconditionsinDefinition4.\nIndistinguishability.Letw¯⋆ =argmin ℓ (v;A¯)denotetheresultofheadtuningA¯,andlet\nv\nw¯ betheresultoftakingaNewtonstepo∈nWwhead (seTeAlgorithm2). Thenbytriangleinequality,\nA¯w¯ AFwF A¯w¯ A¯w¯⋆ + A¯w¯⋆ AFw¯⋆ + AFw¯⋆ AFwF (8)\n2 2 2 2\nk − k ≤k − k k − k k − k\n3Onecan generalizethistothecasewherethedownstream taskisaC-wayclassification, inwhichcase\nB∈Rn×C.\n9\nPreprint.Underreview.\nInformally,thefirsttermiscontrolledbytheerrorintheNewtonstepapproximation,andthethird\ntermis boundedbythe errorto the retrainedwF. Theremainingterm canbe rewrittenas (A¯\nAF)(w¯⋆ w⋆)+(A¯ AF)w⋆ ,wherethefirsttermcanbeboundedusingthesameteck hniqu− e\nusetopro− veLemmas4− and5. Thek secondtermcanbeboundedbynotingthatw⋆ issparse,which\nyieldsthebelowlemmathatplaysacrucialroleinestablishingtheimproveddeletioncapacity.\nLemma6(ModificationofLemma4fordownstreamtask). Forǫ,δ >0,\n1 1 arm\nA¯ AF C¯ CF =c U\nk − k∞ ≤ q ·k − k∞ · q · mǫ 0γp\nAsinthepre-trainingcase,wecannowsetthenoisescaleintheGaussianmechanismandcomplete\ntheproof.Intheworstcase,whenthedownstreamtaskdependsoneverytopic,thenq =1/ar,and\nwerecoverLemma4;however,thisisunlikelytohappeninpractice.\nUtilityPreservation.WecomparethevalueofvaftertheGaussianmechanismv˜=v¯+νv¯ towhat\nitwouldbe forthe ground-truthmodelv⋆ = (AS) A⋆w⋆. We againrelythe sparsityofw⋆ and\n†\nboundE[ v¯ v⋆ ]inLemma31.\nk − k∞\n6 RELATED WORKS\nProvable unlearning. One ideally wants the unlearned model to behave identically to one that\nwasretrainedfromscratchwiththeforgetsetremovedfromthetrainingdata(Cao&Yang, 2015;\nBourtouleetal.,2021;Guptaetal.,2021). Thisisdifficulttoachieveinmanysettings,sothereare\nseveralnotionsof approximateunlearning(Ginartetal., 2019; Guoetal., 2020; Neeletal., 2021)\nreminiscent of differential privacy (Dworketal., 2014). Most relevant to our work is the notion\nof (ǫ,δ)-unlearningintroduced in Sekharietal. (2021), which we adapt to construct Definition 4.\nOur work focuses on deriving unlearning guarantees in the pre-training and fine-tuning pipeline.\nGolatkaretal. (2020) is closest to our work. They show considerably weaker guarantees on un-\nlearning information with respect to probes fit to the weights. In contrast, our work is focused\non realistic topic classification tasks and demonstrates strong guarantees (Definition 4). Recent\nworks have extended notions of certified unlearning to nonconvexsettings. Zhangetal. (2024a);\nMu&Klabjan (2024); Chienetal. (2024) provide unlearning algorithms without deletion capac-\nity guarantees. Qiaoetal. (2024) also proposesan unlearningmethodfornon-convexsettings but\nanalyzesits deletioncapacity in a convexsetting. Our workextendsbeyondthe convexsetting to\nprovideprovableunlearningmethodsandcorrespondingdeletioncapacityanalysisfornon-convex\nmodels.\nTheoretical analysis of pre-training and fine-tuning. Our downstream task definition (Sec-\ntion 2.2) is inspired by works on transfer learning in language models (Saunshietal., 2021;\nWeietal., 2021; Wuetal., 2023; Kumaretal., 2022), contrastive learning (Leeetal., 2021;\nHaoChen&Ma, 2023), and meta-learning (Chuaetal., 2021; Collinsetal., 2022; Yu¨kseletal.,\n2024).\n7 CONCLUSION\nThisworkusestopicmodelstodevelopthefirstprovableguaranteesonunlearninginthemodern-\nday pre-trainingand fine-tuningparadigm. We proposetwo unlearningalgorithmsthat can effec-\ntively and efficiently unlearn from both the pre-trained model (Algorithm 1 and Theorem 2) and\nthe fine-tunedmodel(Algorithm2 and Theorem4). Notably, we find thatit is easier, in terms of\nthedeletioncapacity(Definition4),tounlearnpre-trainingdatafromthefine-tunedmodel,andwe\ncandosowithoutmodifyingthepre-trainedbasemodel.Ourfindingssuggestthattask-specificun-\nlearningiseasierthanfullmodelunlearning,providingapromisingpathforwardtodesignefficient\nalgorithmsforlarge-scalemodels.\nThemostnotablelimitationofourworkisthatourusageoftopicmodels,whichpermitatractable\nanalysisbutcannotcaptureinterestingfeaturesofmodern-daylanguagemodels(e.g.,theirautore-\ngressive nature). Moreover, with the growing popularity of foundationmodels, there is scholarly\ndiscussionaroundmeaningfuldefinitionsofunlearningandhowtheycanbemeasured(Thudietal.,\n10\nPreprint.Underreview.\n2022;Leeetal.,2024). Ourworkfocusesontraditionalnotionsofunlearningcenteredondifferen-\ntialprivacy(seeDefinition4),butwehopetoextendthesedefinitionstocaptureadditionalfeatures\nofgenerativemodelsthataresalienttotheirreal-worlduses.\nREFERENCES\nSanjeevArora,RongGe, YoniHalpern,David Mimno,AnkurMoitra, David Sontag,Yichen Wu,\nand Michael Zhu. A practical algorithm for topic modeling with provable guarantees, 2012a.\nURLhttps://arxiv.org/abs/1212.4777.\nSanjeevArora,RongGe,andAnkurMoitra. Learningtopicmodels-goingbeyondsvd,2012b.\nAbebaBirhaneandVinayUdayPrabhu. Largeimagedatasets: Apyrrhicwinforcomputervision?\nIn2021IEEEWinterConferenceonApplicationsofComputerVision(WACV),pp.1536–1546.\nIEEE,2021.\nAbebaBirhane,VinayUdayPrabhu,andEmmanuelKahembwe. Multimodaldatasets: misogyny,\npornography,andmalignantstereotypes. arXivpreprintarXiv:2110.01963,2021.\nDavidM.BleiandJohnD.Lafferty.Dynamictopicmodels.InProceedingsofthe23rdInternational\nConference on Machine Learning, ICML ’06, pp. 113–120,New York, NY, USA, 2006.Asso-\nciation for Computing Machinery. ISBN 1595933832. doi: 10.1145/1143844.1143859. URL\nhttps://doi.org/10.1145/1143844.1143859.\nDavidMBlei,AndrewYNg,andMichaelIJordan. Latentdirichletallocation. Journalofmachine\nLearningresearch,3(Jan):993–1022,2003.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolf-\nsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen\nCreel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Dur-\nmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori\nHashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang,\nThomasIcard,SaahilJain, DanJurafsky,PratyushaKalluri, SiddharthKaramcheti,GeoffKeel-\ning, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Ku-\nditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,\nXiangLisaLi,XuechenLi,TengyuMa,AliMalik,ChristopherD.Manning,SuvirMirchandani,\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben New-\nman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel\nOrr,IsabelPapadimitriou,JoonSungPark,ChrisPiech,EvaPortelance,ChristopherPotts,Aditi\nRaghunathan,Rob Reich, HongyuRen, FriedaRong, YusufRoohani, CamiloRuiz, Jack Ryan,\nChristopherRe´, Dorsa Sadigh, ShioriSagawa, Keshav Santhanam, AndyShih, Krishnan Srini-\nvasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Trame`r, Rose E. Wang, William\nWang,BohanWu,JiajunWu,YuhuaiWu,SangMichaelXie,MichihiroYasunaga,JiaxuanYou,\nMatei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, YuhuiZhang, Lucia Zheng, Kait-\nlyn Zhou, and Percy Liang. On the opportunitiesand risks of foundationmodels, 2022. URL\nhttps://arxiv.org/abs/2108.07258.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin\nTravers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE\nSymposiumonSecurityandPrivacy(SP),pp.141–159.IEEE,2021.\nJordanBoyd-Graber,YueningHu,DavidMimno,etal. Applicationsoftopicmodels. Foundations\nandTrends®inInformationRetrieval,11(2-3):143–296,2017.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\n11\nPreprint.Underreview.\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural\nInformationProcessingSystems,volume33,pp.1877–1901.CurranAssociates,Inc.,2020.URL\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac\nYinzhiCaoandJunfengYang. Towardsmakingsystemsforgetwithmachineunlearning. In2015\nIEEEsymposiumonsecurityandprivacy,pp.463–480.IEEE,2015.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extractingtraining data\nfrom large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp.\n2633–2650,2021.\nNicolasCarlini,JamieHayes,MiladNasr,MatthewJagielski,VikashSehwag,FlorianTramer,Borja\nBalle,DaphneIppolito,andEricWallace.Extractingtrainingdatafromdiffusionmodels.In32nd\nUSENIXSecuritySymposium(USENIXSecurity23),pp.5253–5270,2023.\nEli Chien, Haoyu Wang, Ziang Chen, and Pan Li. Langevin unlearning: A\nnew perspective of noisy gradient descent for machine unlearning, 2024. URL\nhttps://arxiv.org/abs/2401.10371.\nRochelle Choenni, Ekaterina Shutova, and Robert van Rooij. Stepmothers are mean and\nacademics are pretentious: What do pretrained language models learn about you? In\nMarie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Pro-\nceedings of the 2021 Conference on Empirical Methods in Natural Language Process-\ning, pp. 1477–1491, Online and Punta Cana, Dominican Republic, November 2021. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.111. URL\nhttps://aclanthology.org/2021.emnlp-main.111.\nKurtlandChua,QiLei,andJasonDLee. Howfine-tuningallowsforeffectivemeta-learning. Ad-\nvancesinNeuralInformationProcessingSystems,34:8871–8884,2021.\nRobChurchillandLisaSingh. Theevolutionoftopicmodeling. ACMComput.Surv.,2022.\nLiamCollins,AryanMokhtari,SewoongOh,andSanjayShakkottai. Mamlandanilprovablylearn\nrepresentations.InInternationalConferenceonMachineLearning,pp.4238–4310.PMLR,2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Jill Burstein, Christy Do-\nran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota,\nJune 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL\nhttps://aclanthology.org/N19-1423.\nCynthiaDwork,AaronRoth,etal. Thealgorithmicfoundationsofdifferentialprivacy.Foundations\nandTrends®inTheoreticalComputerScience,9(3–4):211–407,2014.\nRonen Eldan and Mark Russinovich. Who’s harrypotter? approximateunlearningin llms, 2023.\nURLhttps://arxiv.org/abs/2310.02238.\nDOE1v.GitHub,Inc. 4:22-cv-06823,N.D.Cal.2022.\nTremblayv.OpenAI,Inc.,. 23-cv-03416-AMO,(N.D.Cal.),2023.\nEuropean Parliament and Council of the European Union. Regulation\n(EU) 2016/679 of the European Parliament and of the Council. URL\nhttps://data.europa.eu/eli/reg/2016/679/oj.\nJack Foster, Stefan Schoepf, andAlexandraBrintrup. Fast machine unlearningwithoutretraining\nthroughselectivesynapticdampening.InProceedingsoftheAAAIConferenceonArtificialIntel-\nligence,volume38,pp.12043–12051,2024.\n12\nPreprint.Underreview.\nRohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts\nfromdiffusionmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer\nVision,pp.2426–2436,2023.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor\nLeahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. URL\nhttps://arxiv.org/abs/2101.00027.\nAntonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making ai for-\nget you: Data deletion in machine learning. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alche´-Buc, E. Fox, and R. Garnett (eds.), Advances in Neu-\nral Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL\nhttps://proceedings.neurips.cc/paper_files/paper/2019/file/cb79f8fa58b91d3af6c9c991f\nAditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net:\nSelectiveforgettingindeepnetworks. InProceedingsoftheIEEE/CVFConferenceonComputer\nVisionandPatternRecognition,pp.9304–9312,2020.\nChuanGuo, Tom Goldstein, Awni Hannun,and LaurensVan Der Maaten. Certified data removal\nfrom machine learning models. In International Conference on Machine Learning, pp. 3832–\n3842.PMLR,2020.\nVarunGupta,ChristopherJung,SethNeel,AaronRoth,SaeedSharifi-Malvajerdi,andChrisWaites.\nAdaptivemachineunlearning. AdvancesinNeuralInformationProcessingSystems, 34:16319–\n16330,2021.\nJeff Z. HaoChen and Tengyu Ma. A theoretical study of inductive biases in contrastive learn-\ning. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=AuEgNlEAmed.\nJamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, and Nicolas Papernot. Inex-\nact unlearning needs more careful evaluations to avoid a false sense of privacy, 2024. URL\nhttps://arxiv.org/abs/2403.01218.\nKaimingHe, XinleiChen,SainingXie, YanghaoLi, PiotrDolla´r,andRossGirshick. Maskedau-\ntoencodersarescalablevisionlearners. InProceedingsoftheIEEE/CVFconferenceoncomputer\nvisionandpatternrecognition,pp.16000–16009,2022.\nLuxi He, Yangsibo Huang, Weijia Shi, Tinghao Xie, Haotian Liu, Yue Wang, Luke Zettlemoyer,\nChiyuanZhang,DanqiChen,andPeterHenderson. Fantasticcopyrightedbeastsandhow(not)\ntogeneratethem. arXivpreprintarXiv:2406.14526,2024.\nPeter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A Lemley, and Percy\nLiang. Foundationmodelsandfairuse. JournalofMachineLearningResearch,24(400):1–79,\n2023.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.Train-\ningcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556,2022.\nThomas Hofmannet al. Probabilistic latent semantic analysis. In UAI, volume 99, pp. 289–296,\n1999.\nZacharyIzzo,MaryAnneSmart,KamalikaChaudhuri,andJamesZou. Approximatedatadeletion\nfrommachinelearningmodels,2021.\nJoelJang,DongkeunYoon,SoheeYang,SungminCha,MoontaeLee,LajanugenLogeswaran,and\nMinjoonSeo.Knowledgeunlearningformitigatingprivacyrisksinlanguagemodels,2023.URL\nhttps://openreview.net/forum?id=zAxuIJLb38.\n13\nPreprint.Underreview.\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy\nLiang. Fine-tuning can distort pretrained features and underperform out-of-\ndistribution. In International Conference on Learning Representations, 2022. URL\nhttps://openreview.net/forum?id=UYneFzXSJWh.\nMeghdadKurmanji,PeterTriantafillou,JamieHayes,andEleniTriantafillou. Towardsunbounded\nmachine unlearning. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023. URLhttps://openreview.net/forum?id=OveBaTtUAT.\nJasonDLee,QiLei,NikunjSaunshi,andJiachengZhuo. Predictingwhatyoualreadyknowhelps:\nProvableself-supervisedlearning. AdvancesinNeuralInformationProcessingSystems,34:309–\n323,2021.\nKatherine Lee, A. Cooper, Christopher Choquette-Choo, Ken Liu, Matthew Jagielski, Niloofar\nMireshghallah, Lama Ahmed, James Grimmelmann, David Bau, Christopher De Sa, Fernando\nDelgado, Vitaly Shmatikov, Katja Filippova, Seth Neel, Miranda Bogen, Amy Cyphert, Mark\nLemley, and Nicolas Papernot. Extended abstract: Machine unlearning doesn’t do what you\nthink,042024.\nWeiLiandAndrewMcCallum. Pachinkoallocation:Dag-structuredmixturemodelsoftopiccorre-\nlations. InProceedingsofthe23rdinternationalconferenceonMachinelearning,pp.577–584,\n2006.\nJiaqiLiu,JianLou,ZhanQin,andKuiRen. Certifiedminimaxunlearningwithgeneralizationrates\nanddeletioncapacity. AdvancesinNeuralInformationProcessingSystems,36,2024.\nPengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig. Pre-\ntrain, prompt, and predict: A systematic surveyof promptingmethodsin naturallanguagepro-\ncessing,2021. URLhttps://arxiv.org/abs/2107.13586.\nShayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William\nBrannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. A large-\nscaleauditofdatasetlicensingandattributioninai. NatureMachineIntelligence,6(8):975–987,\n2024.\nAnanthMahadevanandMichaelMathioudakis. Cost-effectiveretrainingofmachinelearningmod-\nels. arXivpreprintarXiv:2310.04216,2023.\nPratyushMaini,ZhiliFeng,AviSchwarzschild,ZacharyC.Lipton,andJ.ZicoKolter. Tofu:Atask\noffictitiousunlearningforllms,2024. URLhttps://arxiv.org/abs/2401.06121.\nSadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen,\nand Sanjeev Arora. Fine-tuning language models with just forward passes. In\nThirty-seventh Conference on Neural Information Processing Systems, 2023a. URL\nhttps://openreview.net/forum?id=Vota6rFhBQ.\nSadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based\nview of language model fine-tuning. In International Conference on Machine Learning, pp.\n23610–23641.PMLR,2023b.\nNick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark\nSteedman. Sourcesofhallucinationbylargelanguagemodelsoninferencetasks. arXivpreprint\narXiv:2305.14552,2023.\nSiqiaoMuandDiegoKlabjan.Rewind-to-delete:Certifiedmachineunlearningfornonconvexfunc-\ntions,2024. URLhttps://arxiv.org/abs/2409.09778.\nTarek Naous, Michael Ryan, Alan Ritter, and Wei Xu. Having beer after prayer? measur-\ning cultural bias in large language models. In Lun-Wei Ku, Andre Martins, and Vivek\nSrikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp. 16366–16393, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.862. URL\nhttps://aclanthology.org/2024.acl-long.862.\n14\nPreprint.Underreview.\nSethNeel,AaronRoth,andSaeedSharifi-Malvajerdi. Descent-to-delete: Gradient-basedmethods\nformachineunlearning. InAlgorithmicLearningTheory,pp.931–962.PMLR,2021.\nQuocPhongNguyen,BryanKianHsiangLow,andPatrickJaillet. Variationalbayesianunlearning.\nAdvancesinNeuralInformationProcessingSystems,33:16025–16036,2020.\nMaxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khali-\ndov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran,\nNicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,\nMichael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick\nLabatut,ArmandJoulin,andPiotrBojanowski. DINOv2: Learningrobustvisualfeatureswith-\nout supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL\nhttps://openreview.net/forum?id=a68SUt6zFt.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndatasetforfalconllm: outperformingcuratedcorporawith web data, andweb dataonly. arXiv\npreprintarXiv:2306.01116,2023.\nXinbao Qiao, Meng Zhang, Ming Tang, and Ermin Wei. Efficient and gener-\nalizable certified unlearning: A hessian-free recollection approach, 2024. URL\nhttps://arxiv.org/abs/2404.01712.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, AmandaAskell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual\nmodelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.\n8748–8763.PMLR,2021.\nBen Recht, Christopher Re, Joel Tropp, and Victor Bittorf. Factoring nonnegativematrices with\nlinearprograms. Advancesinneuralinformationprocessingsystems,25,2012.\nRyanRifkinandAldebaroKlautau. Indefenseofone-vs-allclassification. TheJournalofMachine\nLearningResearch,5:101–141,2004.\nNikunjSaunshi,SadhikaMalladi,andSanjeevArora. Amathematicalexplorationofwhylanguage\nmodelshelpsolvedownstreamtasks. InInternationalConferenceonLearningRepresentations,\n2021. URLhttps://openreview.net/forum?id=vVjIW3sEc1s.\nChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,Ross Wightman,Mehdi\nCherti, TheoCoombes,AarushKatta, ClaytonMullis, MitchellWortsman, etal. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformationProcessingSystems,35:25278–25294,2022.\nAyush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Re-\nmember what you want to forget: Algorithms for machine unlearning, 2021. URL\nhttps://arxiv.org/abs/2103.03279.\nWeijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao\nLiu,LukeZettlemoyer,NoahA.Smith,andChiyuanZhang. Muse:Machineunlearningsix-way\nevaluationforlanguagemodels,2024. URLhttps://arxiv.org/abs/2407.06460.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Au-\nthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya\nJha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison,\nNiklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichan-\nder, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord,\nEvan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groen-\neveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for\nlanguage model pretraining research. In Lun-Wei Ku, Andre Martins, and Vivek Sriku-\nmar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 15725–15788, Bangkok, Thailand, August 2024.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.840. URL\nhttps://aclanthology.org/2024.acl-long.840.\n15\nPreprint.Underreview.\nGowthamiSomepalli,VasuSingla,MicahGoldblum,JonasGeiping,andTomGoldstein. Diffusion\nart or digitalforgery? investigatingdata replicationin diffusion models. In Proceedingsof the\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.6048–6058,2023.\nAnvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot. On the ne-\ncessity of auditable algorithmic definitions for machine unlearning. In 31st\nUSENIX Security Symposium (USENIX Security 22), pp. 4007–4022, Boston,\nMA, August 2022. USENIX Association. ISBN 978-1-939133-31-1. URL\nhttps://www.usenix.org/conference/usenixsecurity22/presentation/thudi.\nEnayat Ullah, Tung Mai, Anup Rao, Ryan A Rossi, and Raman Arora. Machine unlearning via\nalgorithmicstability. InConferenceonLearningTheory,pp.4126–4142.PMLR,2021.\nColinWei,SangMichaelXie,andTengyuMa. Whydopretrainedlanguagemodelshelpindown-\nstreamtasks? ananalysisofheadandprompttuning.AdvancesinNeuralInformationProcessing\nSystems,34:16158–16170,2021.\nChenweiWu,HoldenLee,andRongGe. Connectingpre-trainedlanguagemodelanddownstream\ntask via propertiesof representation. In Thirty-seventhConference on NeuralInformationPro-\ncessingSystems,2023. URLhttps://openreview.net/forum?id=YLOJ4aKAka.\nOg˘uzKaanYu¨ksel,EtienneBoursier,andNicolasFlammarion. First-orderANILprovablylearns\nrepresentationsdespiteoverparametrisation.InTheTwelfthInternationalConferenceonLearning\nRepresentations,2024. URLhttps://openreview.net/forum?id=if2vRbS8Ew.\nXiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguage\nimage pre-training. In Proceedings of the IEEE/CVF International Conference on Computer\nVision,pp.11975–11986,2023.\nBinchiZhang,YushunDong,TianhaoWang,andJundongLi. Towardscertifiedunlearningfordeep\nneuralnetworks,2024a. URLhttps://arxiv.org/abs/2408.00920.\nRuiqi Zhang, Licong Lin, Yu Bai, and SongMei. Negativepreferenceoptimization: From catas-\ntrophiccollapsetoeffectiveunlearning.InFirstConferenceonLanguageModeling,2024b.URL\nhttps://openreview.net/forum?id=MXLBXjQkmb.\n16\nPreprint.Underreview.\nA PRECISE DESCRIPTION OF\nABASE\nA.1 COMPLETEDESCRIPTION\nAlgorithm3Highlevellearningalgorithm( )\nA\nInput: documentcorpusS = d m ,anchorwordtoleranceǫ\n{ i }i=1 0\nOutput: matricesA,R\nQ=wordco-occurrences\nQ¯ =row-normalizedQ\nP =RecoverAnchors( Q¯ ,...,Q¯ )\n1 n\n{ }\nA,R=RecoverTopics(Q,S)\nreturn A,R\nAlgorithm4RecoverAnchors,sameasAroraetal.(2012a)\nInput: Row-normalizedco-ocurrencematrixQ¯ andǫ toleranceparameter\n0\nOutput: rpointsofthisperturbedsimplexclosetotheverticesoftheactualsimplex\nProjecttherowstoarandomlychosen4logn/ǫ2dimensionalsubspace\n0\nS Q¯ whereQ¯ isthefurthestpointfromtheorigin\ni i\n←{ }\nforiin1,...,r 1do\nLetQ¯ bethe− rowofQwithlargestdistancetospan(S)\nj\nS S Q¯\nj\nendf← orS =∪{ Q¯ } ,...,Q¯\n{\ns1 sr}\nforiin1,...,rdo\nLetQ¯ bethepointthathaslargestdistancetospan(S Q¯ )\nRemovj\neQ¯ fromS andinsertQ¯ intoS \\{\nsi}\nsi j\nendfor\nreturn S\nAlgorithm5RecoverTopics,fromAroraetal.(2012a)\nInput: Co-ocurrencematrixQ,anchorwordsP = s ,...,s ,toleranceparameterǫ\n1 k 0\n{ }\nOutput: MatricesA,R\nQ¯ =rownormalizedQ\nStorethenormalizationconstantsp=Q1\nforiin1,...,ndo\nSolveC =argmin Q¯ v Q¯ ) 2\ni i ⊤ P\nuptoǫ\naccurav c∈y∆r k − k\n0\nendfor\nA =diag(p)C\n′\nA=column-sum-onenormalizedA\n′\nR=A QA whereA isthepseudoinverseofA\n† †⊤ †\nreturn A,R\nMoreformally,theco-occurrencematrixisconstructedasfollows.Foreachdocument,letH Rn\nd\n∈\nbethefrequencyvectorofeachwordinthedocument;thesumofitsentriesshouldbeL. Then,for\nadocumentd,considerthematrix\nG :=H˜ H˜ Hˆ (9)\nd d d⊤\n−\nd\nwhere\nH\nH˜ := d (10)\nd\nL(L 1)\n−\ndiag(H )\nHˆ := p d (11)\nd\nL(L 1)\n−\n17\nPreprint.Underreview.\nIn particular, the denominator term L(L 1) is precisely the number of co-occurences in each\n−\ndocument,bysimplecombinatorics,anditcanbeseenthatthesumoftheentriesofG isalways\nd\n1. Ourco-ocurrencematrixQisdefinedtobe\nm\n1\nQ:= G (12)\nd\nm\ni=1\nX\nsothatQalsohasentriesthatsumto1. Bylinearityofexpectation,wehave\nE[Q]=E[G ]=A⋆E[X X ]A⋆ (13)\nd d d⊤ ⊤\nwhichimpliesthatasthenumberofdocumentsincreases,QconcentratesaroundAE[XX ]A =\n⊤ ⊤\nE[MM ]. Therefore,weshouldexpectA QA toconcentratearoundE[XX ]=R⋆.\n⊤ † †⊤ ⊤\nA.2 SKETCH: POPULATIONANALYSIS\nTounderstandthisalgorithm,considerthesettingwherewehaveinfinitelymanydocuments.Specif-\nically, consider two words w ,w in a document and their respective topics z ,z . Then, this\n1 2 1 2\npopulationco-occurrencematrix Q will have elements Q = Pr[w =i,w =j], and the row-\ni,j 1 2\nnormalized co-occurrencematrix Q¯ will have entries Q¯ = Pr[w =j w =i]. Moreover, we\ni,j 2 1\n|\nhavethatA =Pr[w =iz =k]=Pr[w =iz =k].\ni,k 1 1 2 2\n| |\nConsiderthesetofanchorwordsP = s ,...,s [n],wheres istheanchorwordfortopick.\n1 r k\nThen,observethatforananchorwordr{\nows\nofQ¯} ,⊆\nitholdsthat\nk\nQ¯ =Pr[w =j w =s ]= Pr[z =k w =s ]Pr[w =j w =s ,z =k ] (14)\nsk,j 2\n|\n1 k 1 ′\n|\n1 k 2\n|\n1 k 1 ′\nk′\nX\n=Pr[w =j w =s ,z =k] (15)\n2 1 k 1\n|\n=Pr[w =j z =k] (16)\n2 1\n|\nwhere the secondline followsfrom only Pr[z =k w =s ] = 1 in the summation, and the last\n1 1 k\n|\nlinefollowsfromw ,w areconditionallyindependentgivenz . Furthermore,fornon-anchorword\n2 1 1\nrowsiofQ¯,itholdsthat\nQ¯ = Pr[z =k w =i]Pr[w =j z =k] (17)\ni,j 1 1 2 1\n| |\nk\nX\nwhere again we use that w ,w are conditionally independentz . For a word i, let C Rr be\n2 1 1 i\nthe vector such that C := Pr[z =k w =i]. Then, it holds that Q¯ = c Q¯ , wher∈ e Q¯ is\nthesubmatrixofQ¯\nconi, sk trainedto1\nthean|\nch1\norwordrows.\nInotherwords,i forev⊤i eryS\nwordi,Q¯\nS\nisa\ni\nconvexcombinationofrowsofQ¯ .\nS\nInthealgorithm,onecanseethatA =C p . Normalizingthisalongeachcolumn,weobtain\n′i,k i,k i\nC p Pr[z =k w =i]Pr[w =i]\nA = i,k i = 1 | 1 1 =Pr[w =iz =k] (18)\ni,k i′C i′,kp i′ i′Pr[z 1 =k |w 1 =i ′]Pr[w 1 =i ′] 1 | 1\nHence,intheiPnfinitedocumenPtlimit,thisalgorithmrecoversthegroundtruthA⋆,R⋆.\nB FROM PROPERTIES OF THE LEARNING ALGORITHM TO THE PROOF OF\nTHEOREM 2\nWefirstgivetheformalstatementofTheorem2.\nTheorem5(FormalstatementofTheorem2). Let bethelearningalgorithmdescribedinthe\nbase\nA\npriorsectionsand betheunlearningalgorithminAlgorithm1. Then,( , )performs\nbase base base\nU A U\nutility-preservingunlearningwithdeletioncapacity\nmǫ 0.001m\nT ǫA,δbase, Ubase(m) ≥c ·min\n(r2\nrnlog1/δ,\nr2 )\n(19)\nwheremisthenumberoftrainingdocuments,risthpenumberoftopics,andcisaconstantdepen-\ndenton . The loss function h used in the utility-preservingdefinitionis the maximum entrywise\nerrorfroD mthegroundtruthtopicmodelA⋆.\n18\nPreprint.Underreview.\nB.1 PRELIMINARIES\nWhen the normis notspecified, we assume that itis the Euclideannorm . We now startoff\n2\nk·k\nwithatechnicalassumptionontheprecisionofthelearningalgorithm.\nAssumption2. ǫ O(1/√nr).\n0\n≤\nAssumption3. Everywordappearswithprobabilityǫ /4arwithoutlossofgenerality;seediscus-\n0\nsioninAroraetal.(2012b). Essentially,lessprobablewordscanbecombinedinasensetoforma\nsinglecategoryof”rare”words.\nWerecallthedefinitionsfromAroraetal.(2012a).\nDefinition6(β-robustsimplex). AsimplexP isβ-robustifforeveryvertexvofP,theℓ distance\n2\nbetweenvandtheconvexhulloftherestoftheverticesasatleastβ.\nDefinition7. Let a n beasetofpointswhoseconvexhullisasimplexwithvertices v r .\nWe say a set of r{ poi i} ni t= s1 is ǫ-close the vertex set v r if each of the r points is ǫ-clo{ sei } ini= ℓ1\n{ i }i=1 2\ndistancetoadifferentvertexinthisvertexset.\nThefollowingresultwillbeusedthroughoutourproof.\nProposition1(Aroraetal.(2012b)). Q¯⋆ inpopulationisγp-robust.\nP\nWe nowlistthehighprobabilityeventswe conditiononthroughoutourproof. Thesefollowfrom\nprevious results in Aroraetal. (2012a); they concern the properties of the output of the learning\nalgorithm.\nProposition2. Withhighprobability,inourregimeofm,thefollowinghold:\n• Thecorrectanchorwordsareselected.\n• EachwordappearsatleastO mǫ0 times.\n4ar\n• The error in the empiricalma(cid:0)trix Q(cid:1)ˆ is entrywise at most O˜(1/√m) from the population\nQ⋆.\nWealsoutilizethefollowingtwokeylemmasfromAroraetal.(2012a)thatwetoucheduponinthe\nmainpaper.\nLemma 7 (Approximation Guarantee on Anchor Words). Suppose each row of Q¯ is at most δ\ndistanceawayfromthegroundtruthγp-robustsimplex Q¯⋆ inℓ norm. If20rδ/(γp)2 < γp,then\n2\nthesetofanchorwordsfoundbythealgorithmisO(δ/γp)-closetothegroundtruthanchorwords.\nLemma 8. When 20rδ/(γp)2 < γp, it holds for every word i that C has entrywise error\ni\nO(δ/(γp)2)fromC⋆.\ni\nB.2 PROOF OF THEOREM2\nThefollowingarelemmasboundingtherelationbetweenQ¯S,Q¯F,Q¯⋆.\ni i i\nLemma 9. After training, the error of each row of Q¯S is at most δ := O 4ar . That is,\n2 mǫ0\nQ¯S Q¯⋆ δ forallwordsi. (cid:16)q (cid:17)\nk i − ik≤ 2\nImportantly,notethat\n20rδ /(γp)2 <γp (20)\n2\nThisimpliesthattheanchorwordsofQ¯S areO(δ /(γp))closetotheanchorwordsofQ¯⋆.\ni 2 i\nConsequently,itholdsthat\nCS C⋆ O(δ /(γp)2) (21)\n2\nk − k∞ ≤\nProof. Thefirstpartfollowsdirectlyfromthefactthatifthenumberofdocumentsm = Ω˜(1/ǫ2),\nQ\nthen Q¯S Q¯⋆ δ foreachrowi. Toshowthat\nk i − ik≤ 2\n20rδ /(γp)2 <γp (22)\n2\n19\nPreprint.Underreview.\nwenotethatbythesamplecomplexityguarantee,\nar3\nmǫ O˜ (23)\n0 ≥ (γp)6\n(cid:18) (cid:19)\nwhichimpliesthat\n(γp)3\nδ O˜ (24)\n2\n≤ r\n(cid:18) (cid:19)\nasdesired.\nLemma10. Whenwedeletem\n0.001mǫ0(γp)3\n,itholdsthat\nU\n≤\na2r2\nm 4arm\nQ¯F Q¯S U = U (25)\nk i − ik≤ mǫ /4ar mǫ\n0 0\nInparticular,thisissmallerthan\n0.001mǫ (γp)3 1 0.004(γp)3\n0 = (26)\na2r2 · mǫ /4ar ar\n0\nProof. Forawordi,considerthechangeinQ¯ afterdeletionrequests. LetF betheinitialsumof\ni\nthetheithrowofQ. Eachcoordinatej [n]willchangeasfollows:\n∈\nf t f m f Ft\nδ = j − j j = U j − j (27)\nj\nF m − F F(F m )\nU U\n− −\nwhere f is the initial number of coocurrences of words i,j and t is the number of documents\nj j\nremovedthathavethiscooccurrence.Moreover,F isthenumberofinitialoccurrencesofwordi,and\nT isthenumberofdeletionsofthewordi. Fromthepreviouslemma,itholdsthatF mǫ /4ar,\n0\nandthatm n t Hence,itfollowsthatthesquaredEuclideannormofthechan≥ geis:\nU ≥ j=1 j\nn P 1 n 2F2m2 m 2\nδ2 = (m f Ft )2 U 2 U (28)\nj F2(F T)2 U j − j ≤ F2(F m )2 ≤ F m\nj=1 − j=1 − U (cid:18) − U(cid:19)\nX X\nHence,fortheregimewherem\n0.001mǫ0(γp)3\n,wehave\nU ≤ a2r2\nm m 4arm\nQ¯S Q¯F √2 U . U . U (29)\nk i − i k≤ F m F mǫ\nU 0\n−\nOfparticularnoticeisthatwhenm istakenaslargeaspossible,thisisatmost\nU\n0.001mǫ (γp)3/a2r2\n0 =0.004(γp)3/ar (30)\nmǫ /4ar\n0\nWenowcombinetheabovetwowithtriangleinequality.\nLemma11. Hence,itholdsthat\n4arm 4arm 4ar\nQ¯F Q¯⋆ U +δ = U +O =:δ (31)\nk i − ik≤ mǫ 2 mǫ mǫ 2′\n0 0 (cid:18)r 0(cid:19)\nImportantly,notethat\n20rδ /(γp)2 <γp (32)\n2′\nThisimpliesthattheanchorwordsofQ¯F areO(δ /(γp))closetotheanchorwordsofQ¯⋆.\ni 2′ i\nConsequently,itholdsthat\nCF C⋆ O(δ /(γp)2) (33)\nk − k∞ ≤\n2′\n20\nPreprint.Underreview.\nProof. Thefirstpartfollowsfromtriangleinequality,andthesecondpartfollowsfromlemmaB.1\nfromAroraetal.(2012a).\nWe now boundwhathappensto CF CS . First, we have thatthe perturbedsimplex Q¯S is\nγp/2-robust.\nk − k∞ P\nLemma12. TheperturbedsimplexQ¯S isγp/2-robust.\nP\nProof. ThisisbecauseofLemmaA.1inAroraetal.(2012a).Since10√rδ <γp,theresultofthat\n2\nlemmaapplies.\nHence,wewillapplyLemmaB.1fromAroraetal.(2012a)onCS tosaysomethingabout CF\nCS . k −\nk∞\nLemma13. Recallthatwhenwedeletem\n0.001mǫ0(γp)3\n,itholdsthat\nU\n≤\na2r2\nm 4arm\nQ¯F Q¯S U = U (34)\nk i − ik≤ mǫ /4ar mǫ\n0 0\nImportantly,notethat\n4arm\n20r U /(γp/2)2 <γp/2 (35)\nmǫ\n(cid:18) 0 (cid:19)\nThis implies that the anchor words of Q¯F are 4armU/mǫ0 close to the anchor words of Q¯S. By\ni γp/2 i\nlemmaB.1fromAroraetal.(2012a),itholdsthat\n4arm\nCF CS O U /(γp/2)2 (36)\nk − k∞ ≤\n(cid:18)\nmǫ\n0 (cid:19)\nObservethatthisissmallerthanO((γp)/ar).\nWenowdealwiththeHessianstepthatwehadtooktopreventretrainingtheC ’s. Inparticular,we\ni\nwilldenoteC¯ tobeourestimatednewC.\nFirst, a lemma to say that our Hessian step is full rank and has a lower bound on its minimum\nsingularvalue.\nLemma14. Whenwedeletem\n0.001mǫ0(γp)3\nsamples,itholdsthattheminimumeigenvalueof\nU\n≤\na2r2\nQ¯FQ¯F isatleastγp/2.\nP P\nProof. FollowsfromLemmaA.3inAroraetal.(2012a).\nLemma15. Whenwedeletem\n0.001mǫ0(γp)3\nsamples,itholdsforalli,\nU\n≤\na2r2\n4 4arm\nCF C¯F δ + U (37)\nk i − i k≤ γp 2 mǫ\n(cid:18) 0 (cid:19)\nProof. Forthecaseofd(, )beingthesquaredloss,wewilldenotethefollowing:\n· ·\nC i,uncon :=arg Cmin kQ¯F P⊤C −Q¯F i ⊤ k2 =(Q¯F PQ¯F P⊤)−1Q¯F PQ¯F i ⊤ (38)\nC¯F :=proj (C ) (39)\ni ∆r i,uncon\nCF :=argmin Q¯F C Q¯F 2 (40)\ni C ∈∆r k P⊤ − i ⊤ k\nIn particular, the Newton step plus projection outputs C . First, observe that by one of the\ni,proj\nanchorwordlemmas,\n4arm\nm Cin kQ¯F P⊤C −Q¯F i ⊤ k= kQ¯F P⊤C i,uncon −Q¯F i ⊤ k≤kQ¯F P⊤C iF −Q¯F i ⊤ k≤δ 2+ mǫ U (41)\n0\n21\nPreprint.Underreview.\nThelastinequalityfollowsfromthefactthatQ¯F isaperturbedversionofQ¯S,andQ¯S isaperturbed\nP P P\nversionofQ¯⋆. Hence,wewillbound\nP\nC¯F CF = proj (C ) proj (CF) (42)\nk i − i k k ∆r i,uncon − ∆r i k\nC CF (43)\n≤k i,uncon − i k\n1\nQ¯F (C CF) (44)\n≤ σ k P⊤ i,uncon − i k\nmin\n1\nQ¯F Q¯F CF + Q¯F C Q¯F (45)\n≤ σ k i ⊤ − P⊤ i k k P⊤ i,uncon − i ⊤ k\nmin\n2 (cid:0) 4arm (cid:1)\nδ + U (46)\n2\n≤ σ mǫ\nmin(cid:18) 0 (cid:19)\nwhere σ is the smallest singular value of Q¯F , which is guaranteed to be full rank per the\nmin i ⊤\npreviouslemma. DuetoaresultinAroraetal.(2012a),thisσ (γp)/2. Thisgivesusthatthe\nmin\n≥\nwholethingisatmost\n4 4arm\nδ + U (47)\n2\nγp mǫ\n(cid:18) 0 (cid:19)\nCorollary1. Wehavethat\n4 4arm\nCF C¯F δ + U (48)\n2\nk − k∞ ≤ γp\n(cid:18)\nmǫ\n0 (cid:19)\nsincetheℓ normisupperboundedbytheℓ norm.\n2\n∞\nLemma16. Thefollowingaretrue.\n• CF C¯F 4 δ + 4armU\nk − k∞ ≤ γp 2 mǫ0\n(cid:16) (cid:17)\n• C¯F C⋆ C¯F CF + CF C⋆ 4 δ + 4armU +O(δ /(γp)2)\nk − k∞ ≤k − k∞ k − k∞ ≤ γp 2 mǫ0 2′\n(cid:16) (cid:17)\nFromthis,wecanboundtheerrorsonthetopicmatrix.\nLemma17. Thefollowingaretrue.\n• AF A¯ O(ar CF C¯F )\nk − k∞ ≤ k − k∞\n• A¯ A⋆ O(ar C¯F C⋆ )\nk − k∞ ≤ k − k∞\n• AS AF O(ar CF CS )\nk − k∞ ≤ k − k∞\nProof. NotethatentriesA are\ni,k\nC Pr[w =i]\nA = i,k (49)\ni,k\nPr[z =k]\nTherefore,theperturbationinAwillbetheperturbationinC multipliedbyar,sincethedenomi-\nnatorislowerboundedby1/arduetothetopicimbalanceconstant.\nNow,wegiveanewlemma.\nProposition3. Whenm Ω( mǫ0),wehavethat\nU ≥ 4ar\n4parm 4ar 4arm arm\nδ =δ + U = + U O U (50)\n2′ 2\nmǫ mǫ mǫ ≤ mǫ\n0 r 0 0 (cid:18) 0 (cid:19)\nNow,weanalyzewhathappensgiventhatΩ mǫ0 m\n0.001mǫ0(γp)3\n.\n4ar ≤ U ≤ a2r2\n(cid:0)p (cid:1)\n22\nPreprint.Underreview.\nLemma18. Forǫ,δ >0,thedeletioncapacitysatisfies\nm\nT ǫA,δ, U(m) ≥Ω˜\nr2√nr\n(51)\n(cid:18) (cid:19)\nProof. Recallthat\n(ar)2m\nA¯ A⋆ O(arδ (1/γp+1/(γp)2)) O U (52)\nk − k∞ ≤\n2′\n≤\n(cid:18)\nmǫ 0γp\n(cid:19)\nMoreover,wealsohavethat\nA¯ AF O(ar CF C¯F ) (53)\nk − k∞ ≤ k − k∞\n4arδ\nO 2′ (54)\n≤ γp\n(cid:18) (cid:19)\n(ar)2m\nO U (55)\n≤ mǫ γp\n(cid:18) 0 (cid:19)\nNote that A has ℓ sensitivity O\n√nr(ar)2mU\n. We now apply the Gaussian mechanism to the\n2 mǫ0γp\nmatrixAentrywisewithnoise (cid:16) (cid:17)\nO\n√nr(ar)2mU\nmǫ0γp\nσ = 2log(1.25/δ) (56)\n(cid:16) ǫ (cid:17)\np\nFromthis,weobtainthat\nE A˜ A⋆ E max ν +E A¯ A⋆ (57)\ni,k\nk − k∞ ≤ i,k | | k − k∞\nh i (cid:20) (cid:21) (cid:2) (cid:3)\n(ar)2m log(1/δ) (ar)2m\nO √nr U log(nr) +O U (58)\n≤ · mǫ 0γp · · p ǫ ! (cid:18) mǫ 0γp (cid:19)\np\nFinally,thissaysthatwhen\nm\nm Ω˜ (59)\nU ≤ r2√nr\n(cid:18) (cid:19)\nwehavethattheutilityispreserveduptoconstantamount,say0.01.\nThisprovesTheorem2. Itisstraightforwardtocontinuetheperturbationanalysisforthetopic-topic\ncovariancematrixR⋆ andprovesimilardeletioncapacityrates.\nC DOWNSTREAM TASK PROOFS\nRecallthealgorithmforlearningthedownstreamtaskhead.\nAlgorithm6Learningalgorithmfortask ( )\nhead\nT A\nInput: documentcorpusS = d m ,anchorwordtoleranceǫ\n{ i }i=1 0\nA,R= (S)\nbase\nA\nreturn argmin ℓ (w;A)\nw\n∈Whead T\nAssumption4. ForanyA,ℓ isλ-stronglyconvexwithrespecttow.\nT\nSinceourtopicmatrixA, canonlytakeona boundedsupport(i.e. thesetofmatriceswhereeach\nrowisontheprobabilitysimplex),itisnaturaltosaythatthesetofvaluesw⋆(A)takesonoverall\ntopicmatricesAisboundedinacertainsense. Assuch,wealsoassumethefollowing:\n23\nPreprint.Underreview.\nAssumption 5. For any base modelA, the vector v such thatv = argmin ℓ (w;A) satisfies\nw\nv B. T\n2\nk k ≤\nAssumption6. ForanyA,ℓ isL-Lipschitzwithrespecttowandtheℓ norm,andisL -Hessian\n2 2\nLipschitzwithrespecttowanTdtheℓ norm. Inotherwords,\n2\nℓ (A,w ) ℓ (A,w ) L w w (60)\n1 2 2 1 2 2\nk T − T k ≤ k − k\n2 ℓ (A,w ) 2 ℓ (A,w ) L w w (61)\nw 1 w 2 2 2 1 2 2\nk∇ T −∇ T k ≤ k − k\nAssumption7. Foranyw, ℓ isL -LipschitzwithrespecttoAandtheℓ norm;thatis,\nw\n∇ T ∞ ∞\nwℓ (A,w) wℓ (A˜,w)\n2\nL A A˜ (62)\nk∇ T −∇ T k ≤ ∞k − k∞\n(63)\nWegiveahelperlemmathat(ǫ,δ)-indistinguishabilityisimmunetopostprocessing.\nLemma19(Post-processingimmunity). Considertworandomvariablesθ ,θ Θthatare(ǫ,δ)-\n1 2\n∈\nindistinguishable.Then,foranyarbitrarymappingf :Θ Θ,itholdsthatf(θ ),f(θ ) Θ are\n′ 1 2 ′\n→ ∈\n(ǫ,δ)-indistinguishable.\nProof. ConsideranarbitrarysetT Θ;letT = r Θ:f(r) T . Then,itholdsthat\n′ ′ ′\n⊆ { ∈ ∈ }\nPr[f(θ ) T ]=Pr[θ T] (64)\n1 ′ 1\n∈ ∈\neǫPr[θ T]+δ (65)\n2\n≤ ∈\n=eǫPr[f(θ ) T ]+δ (66)\n2 ′\n∈\nasdesired.\nWenowgiveacertifiableunlearningguaranteeforthemostnaiveretrainingalgorithmforthedown-\nstreamtask,whichwementionedinthemaintextasTheorem3.\nTheorem 6 (Unlearning when releasing A and w). For a downstream task with loss func-\nT\ntion ℓ , consider the unlearning algorithm that first runs Algorithm 1 to compute\nhead,naive\nA˜ = T (S , (S),T(S)), where ( U , ) perform utility-preserving unlearning (The-\nbase f base base base\nU A A U\norem2). Then,itfitsaheadw = argmin ℓ (w;A˜)andreturnsA˜ andw. Weassertthat\nw\n( ,\n)performsutility-prese∈rWvih nea gd uTnlearning(Definition4).\nhead,naive head,naive\nA U\nProof. Intuitively, this is a result of post processing. More precisely, consider the (ǫ,δ)-\nindistinguishable base models A˜ := (S , (S),T(S)) and A˜ := ( , (S\nbase f base ′ base base\nU A U ∅ A \\\nS ),T(S S )). Then, since the head fitting is a deterministic post-processing of the original\nf f\n\\\nmodel,thisprovesthe(ǫ,δ)-indistinguishabilitybetweenthetwo.\nToprovetheutilitypreservation,observethatinthissetting\nE[ A˜ A⋆ ] 0.01 (67)\nk − k∞ ≤\n(68)\nWethusobtainbyLemma20\nE[ w⋆(A˜) w⋆(A⋆) ] E[ w⋆(A˜) w⋆(A⋆) ] (69)\n2\nk − k∞ ≤ k − k\nL\n∞E[ A˜ A⋆ ] (70)\n≤ λ k − k∞\nwhichisatmost0.01,uptoconstantrescaling.\nTheaboveresultisnice,anditfollowsfromthefactthatthetrainingalgorithmofthedownstream\ntask head is just a post-processing. However, a downside is that it still requires retraining of the\ndownstreamtaskhead. Wecanshowsomethingstronger: evenwithoutprovableunlearningofthe\nbase model(AandR), we canachieveprovableunlearningofthedownstreamtask headweights\nwhenthedownstreamtasklossisconvexinthetrainableweightsw.\nWewillnowconsideranarbitrarytask . Wefirstgivethefollowingnotation.\nT\n24\nPreprint.Underreview.\nDefinition8. ForabasemodelA,letw⋆(A):=argmin ℓ (w;A).\nw\nT\nFirst,wegivethefollowinghelperlemmathatwillbeusefullateron.\nLemma20. ConsidertwobasemodelsA andA . Then,itholdsthat\n1 2\nL\nw⋆(A 1) w⋆(A 2) 2 ∞ A 1 A 2 (71)\nk − k ≤ λ k − k∞\nProof. Observethat\nλ w⋆(A 1) w⋆(A 2)\n2\nwℓ (w⋆(A 1);A 2) wℓ (w⋆(A 2);A 2)\n2\n(72)\nk − k ≤k∇ T −∇ T k\n= wℓ (w⋆(A 1);A 2) wℓ (w⋆(A 1);A 1)\n2\n(73)\nk∇ T −∇ T k\nL A A (74)\n1 2\n≤ ∞k − k∞\nwhere the first line follows from strong convexity, the second line from the gradients being zero,\nandthethirdlinefromthedefinitionofL Lipschitzconstant. Dividingbothsidesbyλgivesthe\ndesiredresult. ∞\nWenowdefinethefollowingnotationsforclarity.\n• wS :=w⋆(AS)\n• wF :=w⋆(AF)\n• w¯⋆ :=w⋆(A¯)\n• mw¯ a: t= ew¯w ⋆S −H w−S1 ∇wℓ T(wS;A¯),whichistheNewtonstepwetakefromwS toapproxi-\nFirst,wegiveaboundontheapproximationerroroftheNewtonstep.\nLemma21. Itholdsthat\nL L2\nw¯ w¯⋆ 2\n∞\nAS A¯ 2 (75)\nk − k≤ 2λ3 k − k∞\nProof. WeaimtoboundthedistanceoftheNewtonstepfromw¯⋆:\nw¯ −w¯⋆ = wS −H w−S1 ∇wℓ T(A¯,wS) −w¯⋆ (76)\nwhereH wS = 2 wℓ (A¯,wS). Then,(cid:0)itholdsthat (cid:1)\n∇ T\nwS −H w−S1 ∇wℓ T(A¯,wS) −w¯⋆ (77)\n=wS −w¯⋆ −H w−S1 ∇wℓ T(A¯,wS) −∇wℓ T(A¯,w¯⋆) (78)\n1\n=H w−S1 (cid:18)HwS(wS −(cid:0) w¯⋆)\n−\nZ0\nHw¯⋆+t(wS −w¯⋆)(wS −(cid:1) w¯⋆)dt\n(cid:19)\n(79)\n1\n=H w−S1\nZ0\nHwS −Hw¯⋆+t(wS −w¯⋆) dt ·(wS −w¯⋆) (80)\nThenormofthisquantityisth(cid:0)ereforeboundedby (cid:1)\nL\nkH w−S1\nk2\n·\n22 kwS −w¯⋆ k·kwS −w¯⋆\nk\n(81)\nL\n2 wS w¯⋆ 2 (82)\n≤ 2λk − k2\n2\nL 1\n2 ℓ (A¯,wS) ℓ (AS,wS) (83)\n2\n≤ 2λ λk∇ T −∇ T k\n(cid:18) (cid:19)\n2\nL L\n2 ∞ A¯ AS (84)\n≤ 2λ λ k − k∞\n(cid:18) (cid:19)\nHence,wehavethat\nL L2\nkw¯ −w¯⋆\nk2\n≤\n22\nλ3∞\nkAS −A¯ k2\n∞\n(85)\n25\nPreprint.Underreview.\nC.1 INSTANTIATINGFORT =[r]\nCLF\nWefirstinstantiateTheorem4forthecasewhereT =[r],orequivalentlywhenq =1/ar.\nclf\nLemma22. RecallourretrainedmodelforthedownstreamtaskisAFwF. Then,itholdsthat\n(ar)2m 2 (ar)2m\nA¯w¯ AFwF O √r U +B√nr U (86)\n2\nk − k ≤ (cid:18) mǫ 0γp (cid:19) mǫ 0γp !\nProof. Werewriteasfollows.\nA¯w¯ AFwF = A¯w¯ A¯w¯⋆ + A¯w¯⋆ AFw¯⋆ + AFw¯⋆ AFwF (87)\n− − − −\nNow, we proceedto boundth(cid:0)e ℓ normof(cid:1)eac(cid:0)h of these individ(cid:1)ual(cid:0)terms separately. (cid:1)For the first\n2\nterm,wehavethat\nA¯w¯ A¯w¯⋆ = A¯(w¯ w¯⋆) (88)\n2 2\nk − k k − k\nw¯ w¯⋆ (89)\n1\n≤k − k\n√r w¯ w¯⋆ (90)\n2\n≤ k − k\nL L2\n√r 2\n∞\nAS A¯ 2 (91)\n≤ 2λ3 k − k∞\nL L2 (ar)2m 2\n√r 2\n∞\nU (92)\n≤ 2λ3 mǫ γp\n(cid:18) 0 (cid:19)\nwheresecondlinefollowsfromA¯havingcolumnsum1,andthefourthlinefollowsfromLemma20\nForthethirdterm,wehaveasimilaranalysis.\nAFw¯⋆ AFwF = AF(w¯⋆ wF) (93)\n2 2\nk − k k − k\nw¯⋆ wF (94)\n1\n≤k − k\n√r w¯⋆ wF (95)\n2\n≤ k − k\nL\n√r ∞ A¯ AF (96)\n≤ λ k − k∞\nL (ar)2m\n√r ∞ U (97)\n≤ λ mǫ γp\n(cid:18) 0 (cid:19)\nFinally,forthesecondterm,wehavethat\nA¯w¯⋆ AFw¯⋆ A¯ AF w¯⋆ (98)\n2 2 2\nk − k ≤k − k k k\nA¯ AF √nr w¯⋆ (99)\n2\n≤k − k∞ k k\n(ar)2m\nO U√nrB (100)\n≤ mǫ γp\n(cid:18) 0 (cid:19)\nBytriangleinequality,weobtainthedesiredresult.\nFirst,wenoteshowthefollowingpropertyofthelearnedtopicmodelAS.\nLemma23. TheminimumsingularvalueofthegroundtruthtopicmatrixAS isatleastΘ(p),since\ntheperturbationsinentriesofA⋆ areatmostǫ O(1/√nr). Hence,thesingularvaluescannot\n0\n≤\nchangebymorethanaconstantfactorrelativetop.\nProof. We know that A⋆ is a p-separable topic model, and hence has smallest singular value at\nleastp. Forthegivensamplecomplexityoflearning,AS willhavesmallestsingularvalueatleast\nΘ(p).\nThe aboveresult says that AS hasa uniquepseudoinverse,and has largestsingularvalue at most\nO(1/p).\n26\nPreprint.Underreview.\nRecallthatourgoalforthedownstreamtaskistoapproximatethevsuchthat\nASv =AFwF (101)\ninordertosaywehaveapproximatedtheunlearnedfine-tunedmodel.Therefore,itsufficestoobtain\nindistinguishabilityofourunlearningalgorithmoutputw˜ with(AS) AFwF. Ourfollowingclaim\n†\nisthatwecanuse(AS) A¯w¯ astheapproximationforthis.\n†\nProposition4. Itholdsthat\n1\n(AS)†A¯w¯ (AS)†AFwF\n2\nO A¯w¯ AFwF\n2\n(102)\nk − k ≤ pk − k\n(cid:18) (cid:19)\n1 (ar)2m 2 (ar)2m\nO √r U +B√nr U (103)\n≤ p ·\" (cid:18) mǫ 0γp (cid:19) mǫ 0γp #!\nLetv¯ :=(AS) A¯w¯ andv =(AS) AFwF. Weclaimthefollowing.\n† †\nLemma24. Theunlearningalgorithm thatoutputs\nhead\nU\nv˜:=v¯+ν (104)\nv\nwhereν isthenoisedefinedbytheGaussianmechanismusingtheabovesensitivitysatisfiesprov-\nv\nable(ǫ,δ)unlearning.Inparticular,weuse\nO 1 √r\n(ar)2mU 2 +B√nr(ar)2mU\np · mǫ0γp mǫ0γp\nσ = (cid:18) (cid:20) (cid:16) (cid:17) (cid:21)(cid:19) 2log(1.25/δ) (105)\nǫ\nwherethenumeratorofthefractionisfromthepreviouspropositionp.\nProof. ThisfollowsfromGaussianmechanism.\nWenowproceedtoboundthedeletioncapacity. Inthiscase,theutilityisdefinedbythecloseness\nof v˜ to (AS) A⋆w⋆ in ℓ norm, similar the way we defined this for the base model unlearning\n†\nalgorithm earlier. ∞\nbase\nU\nFirst,thefollowinglemmatoboundAFwF A⋆w⋆.\n−\nLemma25. Wehavethat\n(ar)2m\nAFwF A⋆w⋆ O B√nr U (106)\n2\nk − k ≤ mǫ γp\n(cid:18) 0 (cid:19)\nProof. Wedecomposeasfollows.\nAFwF A⋆w⋆ =(AFwF AFw⋆)+(AFw⋆ A⋆w⋆) (107)\n− − −\nThefirsttermisboundedby\n(ar)2m\nAFwF AFw⋆ √r wF w⋆ O(√r AF A⋆ ) O √r U (108)\n2 2\nk − k ≤ k − k ≤ k − k∞ ≤\n(cid:18)\nmǫ 0γp\n(cid:19)\nThesecondtermisboundedby\n(ar)2m\nAFw⋆ A⋆w⋆ O U√nrB (109)\n2\nk − k ≤ mǫ γp\n(cid:18) 0 (cid:19)\nbyconsideringthespectralnorm AF A⋆ . Thisgivesthedesiredresult.\n2\nk − k\nAsaresult,thefollowingholds.\nProposition5. Itholdsthat\n1 (ar)2m (ar)2m\n(AS) AFwF (AS) A⋆w⋆ O √r U +B√nr U (110)\n† † 2\nk − k ≤ p mǫ γp mǫ γp\n(cid:18) (cid:20) 0 0 (cid:21)(cid:19)\n27\nPreprint.Underreview.\nThisisonceagainfromtheboundedoperatornormpropertyof(AS) .\n†\nFinally,wecanapplytriangleinequalitytogetthefollowing.\nLemma26. Itholdsthat\n1 (ar)2m 2 (ar)2m\n(AS) A¯w¯ (AS) A⋆w⋆ √r U +B√nr U (111)\n† † 2\nk − k ≤ p ·\" (cid:18) mǫ 0γp (cid:19) mǫ 0γp #!\nThen,wecangetthefollowingboundondeletioncapacity.\nLemma27. Forǫ,δ >0,thedeletioncapacitysatisfies\nm\nT ǫA,δhead, Uhead(m) ≥Ω˜\nr2√nr\n(112)\n(cid:18) (cid:19)\nProof. Thecalculationisasfollows.\nE v˜ (AS)†A⋆w⋆ E[ νv ]+E (AS)†A¯w¯ (AS)†A⋆w⋆ (113)\nk − k∞ ≤ k k∞ k − k∞\n(cid:2) (cid:3) 1 √r (ar(cid:2))2m U 2 +B√nr(ar)2m U (cid:3) logrlog1/δ +1\n≤ p ·\" (cid:18) mǫ 0γp (cid:19) mǫ 0γp #! p ǫ !\n(114)\nForthistobeasmallconstant,werequire\n(ar)2m 1 1\nU O˜ min , (115)\nmǫ γp ≤ r1/4 √nr\n0 (cid:18) (cid:26) (cid:27)(cid:19)\nTherefore,weshouldhave\nm\nm Ω˜ (116)\nU ≤ r2√nr\n(cid:18) (cid:19)\nC.2 PROOF FORGENERALq\nThefollowingistheformalstatementofTheorem4.\nTheorem7(FormalversionofTheorem4). Supposethatthedownstreamtask onlydependson\nasubsetoftopicsT [r]; thatis,w⋆ = argmin ℓ (v;A⋆)hasnon-zT eroentriesonlyin\nclf v\nthe index setT clf. Deno⊆ te q := min k ∈T clfPr D[z = k]∈,Wab nas de lT et Ahead be the headtuningalgorithm\n(Definition2)and beAlgorithm2. Then,( , )performsutility-preservingunlearning\nhead head head\nU A U\nwithdeletioncapacity\nmqǫ 0.001m\nT ǫA,δhead, Uhead(m) ≥c\n′\n·min\n(r\nnrlog1/δ,\nr2 )\n(117)\nwherec ′isaconstantdependenton ,and . p\nD T\nLemma28. RecallourretrainedmodelforthedownstreamtaskisAFwF. Then,itholdsthat\n(ar)2m (1/q)arm (ar)2m 2\nA¯w¯ AFwF O √r U +O B√nr U +O U √nr\n2\nk − k ≤ (cid:18) (cid:18) mǫ 0γp (cid:19)(cid:19) (cid:18) mǫ 0γp (cid:19) (cid:18) mǫ 0γp (cid:19) !\n(118)\nProof. Considerthisdecompositionagain.\nA¯w¯ AFwF = A¯w¯ A¯w¯⋆ + A¯w¯⋆ AFw¯⋆ + AFw¯⋆ AFwF (119)\n− − − −\nThe first term is the same as old analysis; the second term is from considering q; the third is the\n(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)\nsameastheoldanalysis. Inparticular,whenq =1/ar,werecovertheoldbound.Wehavethatthe\nfirsttermis\nL L2 (ar)2m 2\nA¯w¯ A¯w¯⋆ √r 2\n∞\nU (120)\nk − k≤ 2λ3 mǫ γp\n(cid:18) 0 (cid:19)\n28\nPreprint.Underreview.\nThethirdtermis\nL (ar)2m\nAFw¯⋆ AFwF √r ∞ U (121)\nk − k≤ λ mǫ γp\n(cid:18) 0 (cid:19)\nThesecondtermis\nA¯w¯⋆ AFw¯⋆ (A¯ AF)w¯⋆ + (A¯ AF)(w⋆ w¯⋆) (122)\nk − k≤k − k k − − k\n(1/q)arm (ar)2m 2\nO B√nr U +O U √nr (123)\n≤\n(cid:18)\nmǫ 0γp\n(cid:19) (cid:18)\nmǫ 0γp\n(cid:19)\n!\nThisgivesthedesiredresultusingtriangleinequality.\nContinuing,wehavethefollowing.\nProposition6. Itholdsthat\n(AS)†A¯w¯ (AS)†AFwF\n2\n(124)\nk − k\n1\nO A¯w¯ AFwF (125)\n2\n≤ pk − k\n(cid:18) (cid:19)\n1 (ar)2m (1/q)arm (ar)2m 2\nO √r U +B√nr U + U √nr (126)\n≤ p ·\" (cid:18) mǫ 0γp (cid:19) mǫ 0γp (cid:18) mǫ 0γp (cid:19) #!\nThisgivesusthefollowing.\nLemma29. Theunlearningalgorithm thatoutputs\nhead\nU\nv˜:=v¯+ν (127)\nv\nwhereν isthenoisedefinedbytheGaussianmechanismusingtheabovesensitivitysatisfiesprov-\nv\nable(ǫ,δ)unlearning.Inparticular,weuse\nO 1 √r (ar)2mU +B√nr(1/q)armU + (ar)2mU 2 √nr\np · mǫ0γp mǫ0γp mǫ0γp\nσ = (cid:18) (cid:20) (cid:16) (cid:17) (cid:16) (cid:17) (cid:21)(cid:19) 2log(1.25/δ) (128)\nǫ\nwherethenumeratorofthefractionisfromthepreviousproposition. p\nProof. ThisfollowsfromGaussianmechanism.\nWenowproceedtoboundthedeletioncapacity. Inthiscase,theutilityisdefinedbythecloseness\nof v˜ to (AS) A⋆w⋆ in ℓ norm, similar the way we defined this for the base model unlearning\n†\nalgorithm earlier. ∞\nbase\nU\nFirst,thefollowinglemmatoboundAFwF A⋆w⋆.\n−\nLemma30. Wehavethat\n(ar)2m (1/q)arm (ar)2m 2\nAFwF A⋆w⋆ O √r U +B√nr U + U √nr\n2\nk − k ≤ (cid:18) mǫ 0γp (cid:19) mǫ 0γp (cid:18) mǫ 0γp (cid:19) !\n(129)\nProof. Wedecomposeasfollows.\nAFwF A⋆w⋆ =(AFwF AFw⋆)+(AFw⋆ A⋆w⋆) (130)\n− − −\nThefirsttermisboundedby\n(ar)2m\nAFwF AFw⋆ √r wF w⋆ O(√r AF A⋆ ) O √r U\n2 2\nk − k ≤ k − k ≤ k − k∞ ≤\n(cid:18) (cid:18)\nmǫ 0γp\n(cid:19)(cid:19)\n(131)\n29\nPreprint.Underreview.\nThesecondtermisboundedby\n(1/q)arm (ar)2m 2\nAFw⋆ A⋆w⋆ B√nr U + U √nr (132)\n2\nk − k ≤ mǫ γp mǫ γp\n0 (cid:18) 0 (cid:19)\nTriangleinequalitygivesusthedesiredresult.\nAsaresult,thefollowingholds.\nProposition7. Itholdsthat\n1 (ar)2m (1/q)arm (ar)2m 2\n(AS) AFwF (AS) A⋆w⋆ O √r U +B√nr U + U √nr\n† † 2\nk − k ≤ p ·\" (cid:18) mǫ 0γp (cid:19) mǫ 0γp (cid:18) mǫ 0γp (cid:19) #!\n(133)\nThisisonceagainfromtheboundedoperatornormproperty.\nFinally,wecanapplytriangleinequalitytogetthefollowing.\nLemma31. Itholdsthat\n1 (ar)2m (1/q)arm (ar)2m 2\n(AS) A¯w¯ (AS) A⋆w⋆ O √r U +B√nr U + U √nr\n† † 2\nk − k ≤ p ·\" (cid:18) mǫ 0γp (cid:19) mǫ 0γp (cid:18) mǫ 0γp (cid:19) #!\n(134)\nThen,wecangetthefollowingboundondeletioncapacity.\nLemma32. Forǫ,δ >0,thedeletioncapacitysatisfies\nm\nT ǫA,δhead, Uhead(m) ≥Ω˜\nr2√nr\n(135)\n(cid:18) (cid:19)\nProof. Thecalculationisasfollows.\nE v˜ (AS) †A⋆w⋆ E[ νv ]+E (AS) †A¯w¯ (AS) †A⋆w⋆ (136)\nk − k∞ ≤ k k∞ k − k∞\n(cid:2) (cid:3) 1 √r (ar(cid:2))2m U +B√nr(1/q)arm U +(cid:3) (ar)2m U 2 √nr\n≤ p ·\" (cid:18) mǫ 0γp (cid:19) mǫ 0γp (cid:18) mǫ 0γp (cid:19) #!\n(137)\nlogrlog1/δ\n+1 (138)\n· p ǫ !\nForthistobeasmallconstant,werequire\n(ar)2m 1 1 arq\nU O˜ min , , (139)\nmǫ γp ≤ r1/2 (nr)1/4 √nr\n0 (cid:18) (cid:26) (cid:27)(cid:19)\nWhennisatleastr3,thelastofthesetermswillbethesmallest. Therefore,wehavethat\nmq\nm Ω˜ (140)\nU ≤ r1.5n0.5\n(cid:16) (cid:17)\n30",
    "pdf_filename": "Provable_unlearning_in_topic_modeling_and_downstream_tasks.pdf"
}