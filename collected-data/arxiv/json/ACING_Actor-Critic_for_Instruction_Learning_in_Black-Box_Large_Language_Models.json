{
    "title": "ACING Actor-Critic for Instruction Learning in Black-Box Large Language Models",
    "abstract": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly depends on the quality of the instructions, which often re- quire fine-tuning through extensive human ef- fort. This highlights the need for automated in- struction optimization; however, this optimiza- tion is particularly challenging when dealing with black-box LLMs, where model parameters and gradients remain inaccessible. We propose ACING, a task-specific prompt optimization approach framed as a state- less continuous-action Reinforcement Learning (RL) problem, known as the continuum bandit setting. ACING leverages an actor-critic-based method to optimize prompts, learning from non- differentiable reward signals. We validate AC- ING by optimizing prompts for ChatGPT on 30 instruction-based tasks, as well as a summa- rization task. ACING consistently outperforms baseline methods, achieving a median score im- provement of 10 percentage points compared to the best baseline considered. Furthermore, ACING not only recovers but also surpasses human-crafted expert instructions, achieving up to a 39 percentage point improvement over human benchmarks. 1 Introduction Large Language Models (LLMs) have demon- strated remarkable performance across a wide range of tasks (Zhao et al., 2024; Touvron et al., 2023). This success is largely attributed to their strong instruction-following capabilities, which en- able adaptation to diverse downstream applications (Chen et al., 2023; Liu et al., 2023). These instruc- tions, commonly referred to as prompts, play a cru- cial role in the performance of LLMs (Wei et al., 2022; Zhu et al., 2024; Liu et al., 2023). Given the importance of prompts, researchers are increasingly interested in automating their optimization to re- duce the need for manual adjustments, which is of- ten a labor-intensive and costly process (Reynolds and McDonell, 2021; Mishra et al., 2021). As a result, developing efficient methods for automatic prompt optimization has become highly important to maximize LLMs performance. Several methods for automated prompt optimiza- tion have been proposed in the literature. Some focus on optimizing continuous prompts, known as soft prompts (Zhong et al., 2021; Li and Liang, 2021), which are typically fed into a LLM after its embedding layer since they do not represent mean- ingful language that the LLM can interpret directly. Other methods aim at optimizing discrete prompts, known as hard prompts, by either generating di- verse prompts (Zhou et al., 2023) or refining exist- ing ones (Pryzant et al., 2023). For both soft and hard prompts, some approaches rely on gradient- based methods (Shin et al., 2020; Lester et al., 2021; Li and Liang, 2021). However, these meth- ods require access to the gradients of the LLMs and are therefore limited to white-box LLMs. Mean- while, the most powerful LLMs today are typically black-box models (e.g., ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b)). To overcome these challenges, recent studies have introduced gradient-free techniques for opti- mizing prompts in black-box LLMs (Zhou et al., 2023; Prasad et al., 2023; Pryzant et al., 2023). While promising, these approaches rely on heuris- tic local search methods and fail to fully utilize the observation history, including previously tested instructions and their outcomes, when selecting new instructions to test. Consequently, they are less effective at balancing the inherent exploration- exploitation trade-off, leading to inefficient query- ing. This inefficiency becomes problematic and impractical when API calls to black-box LLMs involve significant time and financial costs. A recent line of work has focused on optimiz- ing instructions for black-box LLMs by search- ing through soft prompts and transforming them into discrete prompts using white-box LLMs. In- 1 arXiv:2411.12736v1  [cs.CL]  19 Nov 2024",
    "body": "ACING: Actor-Critic for Instruction Learning\nin Black-Box Large Language Models\nSalma Kharrat\nFares Fourati\nKAUST\n{salma.kharrat, fares.fourati}@kaust.edu.sa\nMarco Canini\nAbstract\nThe effectiveness of Large Language Models\n(LLMs) in solving tasks vastly depends on\nthe quality of the instructions, which often re-\nquire fine-tuning through extensive human ef-\nfort. This highlights the need for automated in-\nstruction optimization; however, this optimiza-\ntion is particularly challenging when dealing\nwith black-box LLMs, where model parameters\nand gradients remain inaccessible.\nWe propose ACING, a task-specific prompt\noptimization approach framed as a state-\nless continuous-action Reinforcement Learning\n(RL) problem, known as the continuum bandit\nsetting. ACING leverages an actor-critic-based\nmethod to optimize prompts, learning from non-\ndifferentiable reward signals. We validate AC-\nING by optimizing prompts for ChatGPT on\n30 instruction-based tasks, as well as a summa-\nrization task. ACING consistently outperforms\nbaseline methods, achieving a median score im-\nprovement of 10 percentage points compared\nto the best baseline considered. Furthermore,\nACING not only recovers but also surpasses\nhuman-crafted expert instructions, achieving\nup to a 39 percentage point improvement over\nhuman benchmarks.\n1\nIntroduction\nLarge Language Models (LLMs) have demon-\nstrated remarkable performance across a wide\nrange of tasks (Zhao et al., 2024; Touvron et al.,\n2023). This success is largely attributed to their\nstrong instruction-following capabilities, which en-\nable adaptation to diverse downstream applications\n(Chen et al., 2023; Liu et al., 2023). These instruc-\ntions, commonly referred to as prompts, play a cru-\ncial role in the performance of LLMs (Wei et al.,\n2022; Zhu et al., 2024; Liu et al., 2023). Given the\nimportance of prompts, researchers are increasingly\ninterested in automating their optimization to re-\nduce the need for manual adjustments, which is of-\nten a labor-intensive and costly process (Reynolds\nand McDonell, 2021; Mishra et al., 2021). As a\nresult, developing efficient methods for automatic\nprompt optimization has become highly important\nto maximize LLMs performance.\nSeveral methods for automated prompt optimiza-\ntion have been proposed in the literature. Some\nfocus on optimizing continuous prompts, known\nas soft prompts (Zhong et al., 2021; Li and Liang,\n2021), which are typically fed into a LLM after its\nembedding layer since they do not represent mean-\ningful language that the LLM can interpret directly.\nOther methods aim at optimizing discrete prompts,\nknown as hard prompts, by either generating di-\nverse prompts (Zhou et al., 2023) or refining exist-\ning ones (Pryzant et al., 2023). For both soft and\nhard prompts, some approaches rely on gradient-\nbased methods (Shin et al., 2020; Lester et al.,\n2021; Li and Liang, 2021). However, these meth-\nods require access to the gradients of the LLMs and\nare therefore limited to white-box LLMs. Mean-\nwhile, the most powerful LLMs today are typically\nblack-box models (e.g., ChatGPT (OpenAI, 2023a)\nand GPT-4 (OpenAI, 2023b)).\nTo overcome these challenges, recent studies\nhave introduced gradient-free techniques for opti-\nmizing prompts in black-box LLMs (Zhou et al.,\n2023; Prasad et al., 2023; Pryzant et al., 2023).\nWhile promising, these approaches rely on heuris-\ntic local search methods and fail to fully utilize\nthe observation history, including previously tested\ninstructions and their outcomes, when selecting\nnew instructions to test. Consequently, they are\nless effective at balancing the inherent exploration-\nexploitation trade-off, leading to inefficient query-\ning. This inefficiency becomes problematic and\nimpractical when API calls to black-box LLMs\ninvolve significant time and financial costs.\nA recent line of work has focused on optimiz-\ning instructions for black-box LLMs by search-\ning through soft prompts and transforming them\ninto discrete prompts using white-box LLMs. In-\n1\narXiv:2411.12736v1  [cs.CL]  19 Nov 2024\n\nstructZero (Chen et al., 2024) optimizes discrete\nprompts for ChatGPT by applying Bayesian op-\ntimization over soft prompts.\nBuilding on the\nsame line, INSTINCT (Lin et al., 2024) replaces\nBayesian optimization with NeuralUCB (Zhou\net al., 2020), a critic algorithm, to select actions,\nfrom a finite pool of soft prompts, initially gener-\nated with Sobol (Eriksson et al., 2019).\nInspired by the success of actor-critic meth-\nods for control problems (Haarnoja et al., 2018,\n2019) and generative adversarial networks (Good-\nfellow et al., 2014), our approach combines the hy-\nbrid white-box/black-box framework with an actor-\ncritic Reinforcement Learning (RL) algorithm, op-\nerating in a stateless, infinite continuous-action\nspace, and constrained by a strict budget of API\ncalls.\nThe actor aims to maximize the critic’s\nvalue while remaining as stochastic as possible\nto enhance exploration and simultaneously exploit\npromising actions. This approach increases the\nlikelihood of finding optimal prompts and enables\nmore efficient use of the limited number of API\ncalls. ACING consistently outperforms baseline\nmethods, achieving a median score improvement\nof 10 percentage points. Furthermore, our opti-\nmized prompts not only recover but also surpass\nhuman-crafted expert instructions, achieving up to\na 39 percentage point improvement against human\nbenchmarks.\nAn implementation of ACING is available at\nhttps://github.com/salmakh1/ACING.\n2\nProblem Formulation\n2.1\nPrompt Learning Objective\nWe aim to improve the performance of a black-box\nLLM, denoted by f, which can only be accessed\nthrough its API, while its internal parameters re-\nmain unknown. Given a task represented by an\n(unknown) distribution (x, y) ∼D—where x de-\nnotes possible inputs and y the corresponding cor-\nrect outputs—our goal is to find the optimal prompt\nτ ⋆that maximizes the likelihood of f producing\ncorrect outputs for a given task. This is evaluated\nusing a scoring function q(·, ·) ∈[0, 1]. The black-\nbox model f processes an input formed by con-\ncatenating (⊕) the prompt τ with the sentence x,\nproducing a predicted output ˆy = f(τ ⊕x). More\nformally, the objective is to maximize the expected\nscore of the LLM in solving the task D, defined as:\nmax\nτ\nE(x,y)∼D [q(y, f(τ ⊕x))] .\n(1)\nTo achieve this, we utilize a validation dataset\nV = {(xj, yj)}m\nj=1, where each pair consists of\nan input sentence xj and its corresponding ground\ntruth output yj. Our objective is to find the prompt\nthat maximizes the scoring function q(·, ·) across\nthe validation dataset, where q(ˆyj, yj) measures the\nquality of the predicted output ˆyj against the true\noutput yj. Thus, our objective becomes finding the\nprompt τ ⋆that maximizes the average score over\nthe validation set V. The derived prompt τ ⋆is then\nevaluated on a separate test set T = {(x′\nj, y′\nj)}m′\nj=1\nto assess its generalization performance.\n2.2\nLearning over a Continuous Space\nDirectly optimizing the prompt τ for the black-\nbox LLM model f presents substantial challenges\ndue to the discrete combinatorial nature of token\nselection in τ. To mitigate this challenge, prior\napproaches like InstructZero (Chen et al., 2024)\nand INSTINCT (Lin et al., 2024) employ a white-\nbox LLM, represented by h, to convert the discrete\noptimization problem into a continuous one by in-\ntroducing a soft prompt vector z ∈Rd, which is a\ncontinuous d-dimensional vector representing the\ntoken embedding of a set of virtual tokens.\nThe white-box model h has access to a dataset\nof exemplars, E = {(uj, vj)}k\nj=1, where each pair\n(uj, vj) defines input-output text sequences that ex-\nemplify a downstream task. The white-box LLM\nh serves as a proxy, mapping the soft prompt z\ninto a discrete prompt τ suitable for input to the\nblack-box LLM. Specifically, given the exemplars\nE and the vector z, their concatenation is input\nto the white-box LLM, generating the discrete\nprompt τ(z) = h(z, E). This generated prompt\nτ(z) is prepended to a test input xj from the vali-\ndation set V, and the combined input is provided\nto the black-box LLM f to generate an output\nˆyj = f(τ(z) ⊕xj). The output ˆyj is then eval-\nuated against the true output yj using the scoring\nfunction q(ˆyj, yj). By using a fixed set of exem-\nplars E, the original discrete problem (Equation\n(1)) of finding the optimal prompt τ is effectively\ntransformed into a continuous optimization prob-\nlem over the soft prompt vector z, as follows:\nmax\nz∈Rd\nE(x,y)∼D [q(y, f(τ(z) ⊕x))] .\n(2)\n2.3\nReducing the Continuous Space\nThe soft prompt z is typically high-dimensional\n(e.g., d = 5120 × Nz, with Nz the number of\nvirtual tokens, when h is Vicuna-13B (Chiang\n2\n\net al., 2023)), which makes it a challenging prob-\nlem. Therefore, we employ random projection tech-\nniques to reduce the input dimension as done in\nprior works (Chen et al., 2024; Lin et al., 2024).\nSpecifically, given a matrix P ∈Rd×d′ where\nd′ ≪d, with randomly sampled elements, and\na continuous vector a ∈Rd′, the vector z = Pa\nis used as the soft prompt. By substituting z with\nPa, the input variable to be optimized in Eq. 2\nbecomes a, thereby reducing the input dimension\nof the optimization problem to d′:\nmax\na∈Rd′\nE(x,y)∼D [q(y, f(τ(Pa) ⊕x))] .\n(3)\nFor fairness with previous works, the reduced\ninput dimension d′, referred to as the intrinsic di-\nmension, is chosen as d′ = 10 (Chen et al., 2024;\nLin et al., 2024). An ablation study on the intrinsic\ndimension is discussed in § 5.4.3. Furthermore,\nto maintain consistency with these prior studies,\nwe similarly use a projection matrix P with ele-\nments uniformly and independently sampled from\nUniform(−1, 1), and bound the vector such that\na ∈[0, 1]d′.\n3\nActor-Critic Approach for Instruction\nLearning in Black-Box LLMs\n3.1\nInstruction Learning as an RL Problem\nIn this work, we frame the prompt-learning prob-\nlem as an agent navigating an infinite action space\nwithin a d\n′-dimensional unit hypercube, searching\nwithin an infinite number of actions a ∈[0, 1]d\n′\n,\nin contrast to previous works that consider a finite\nnumber of actions initially generated with Sobol\n(Eriksson et al., 2019; Lin et al., 2024). As part of\nthe environment, these continuous actions are pro-\njected into a higher-dimensional space (§ 2.3) and\nthen transformed into discrete prompts, through\na white-box LLM (§ 4). These discrete prompts\nare evaluated by a black-box LLM over a valida-\ntion dataset, after which a reward is computed and\nreturned to the agent. We consider a finite hori-\nzon T, representing the maximum number of ac-\ntions played (i.e., the budget of evaluated instruc-\ntions), after which the agent ceases exploration.\nThe agent’s goal is to discover the optimal continu-\nous action, a⋆, which produces the optimal discrete\nprompt and the highest reward.\nUnlike in general RL settings, where the agent\nencounters multiple states with varying reward dis-\ntributions, the agent we consider operates in a state-\nless, stochastic environment, commonly referred\nAlgorithm 1 Stateless Soft Actor-Critic\nInput: Evaluation budget T\n1: Initialize θ, w (e.g., randomly)\n2: for t ←1 to T do\n3:\nChoose action at ∼π(· | θ)\n4:\nTake action at and observe reward rt\n5:\nw ←w −λ ˆ∇JQ(w)\n(Equation (5))\n6:\nθ ←θ + β ˆ∇Jπ(θ)\n(Equation (6))\n7:\nα ←α −γ ˆ∇Jα(α)\n(Equation (7))\n8: end for\nto as a stochastic bandit setting (Slivkins, 2019;\nLattimore and Szepesvári, 2020). In this context,\nat each iteration t ≤T (where t is incremented\nwith every black-box LLM call), the agent’s action\nat generate stochastic rewards, rt ∈[0, 1], with-\nout affecting the reward distributions associated\nwith those actions. The prompt-learning objective\nis modeled as a stochastic best-arm identification\nproblem in a stateless, continuous action space RL,\nspecifically in an infinite continuum bandit setting.\n3.2\nActor-Critic Continuum Bandits\nRL techniques are generally used to learn opti-\nmal policies and actions leading to best rewards.\nThese techniques can be classified into value-based\n(critic-centric), policy-gradient (actor-centric), or\nhybrid approaches. Actor-critic algorithms merge\nthe benefits of both by employing two key compo-\nnents: the actor, responsible for policy optimiza-\ntion, and the critic, which evaluates the policy (Sut-\nton and Barto, 2018). This hybrid approach has\ndemonstrated success in various domains, includ-\ning continuous control problems (Mnih et al., 2016;\nHaarnoja et al., 2018, 2019) and with generative\nadversarial networks (GANs) (Goodfellow et al.,\n2014), where a generator and a critic network col-\nlaboratively optimize performance.\nInspired by the success of actor-critic ap-\nproaches, we introduce a (stateless) actor-critic\nalgorithm, as provided in Algorithm 1, tailored\nto our infinite continuum bandit setting, enabling\nautonomous learning of effective prompts with a\nconstrained evaluation budget and outperforming\nprevious state-of-the-art black-box prompt learn-\ning approaches. Both the actor and the critic are\nparameterized by neural networks. In typical RL\nproblems, the agent interacts with an environment\nwith varying states, so its policy depends on the\ncurrent state. That is, the policy network generates\nactions based on the current state s, π(.|s; θ), where\n3\n\nInduction Generation Template\nOutput: \n. . . \n. . . \nThe instruction was to\nInput: \nInput: \nOutput: \nEvaluation Template\nInput: [Test set]\nInstruction: [Instruction]\nOutput: [prediction]\nCritic\nQ-Network\nActor\nPolicy Network\nActor-Critic Agent\nEnvironment\nSoft prompt\nExemplars\nembeddings\nWhite-box LLM\nBlack-box LLM\nDiscrete prompt\nValidation set\nScore function\nReward\nAction\n+\n+\nOutput\nProjection\nError\n1\n3\n4\n2\nFigure 1: Pipeline of ACING. In each iteration, a soft prompt along with several examples of the target task are provided to the\nwhite-box LLM to generate an instruction. This instruction is then used to query the black-box LLM, which produces answers to\nthe target task queries. The resulting score is returned to the agent as a reward, which is used to update its networks and adjust its\npolicy. Both LLMs remain frozen throughout the process.\nθ denotes the parameters. However, in the problem\nwe consider, the setting is stateless. Therefore, we\ndefine a policy network π(.; θ) with a constant in-\nput that produces a d\n′-dimensional vector as output\na, representing the continuous action. For the critic,\nwe use a neural network with a d\n′-dimensional in-\nput (the action) and a single output that estimates\nthe action’s quality, thereby assessing the quality\nof the policy that proposed it. The critic network’s\nparameters are trained to minimize the following:\nmin\nw JQ(w) ≜Ea∼D\n\u00141\n2 (Qw (a) −r (a))2\n\u0015\n,\n(4)\nwhich can be optimized with stochastic gradients\nˆ∇θJQ(w) = ∇wQw (at) (Qw (at) −r (at)) ,\n(5)\nwhere Q is the critic-network with parameters w.\nThe actor-critic learning process alternates between\npolicy-network and critic-network updates.\n3.3\nEnhancing Exploration with Entropy\nIn\ngeneral,\nRL\nlearning\napproaches\naim\nto\nmaximize\nthe\nexpected\ncumulative\nre-\nwards\nup\nto\na\nhorizon\nT,\nexpressed\nas\nmaxπ(·)\nPT\nt=1 Eat∼π(·) [r(at)] .\nIn\ncontrast,\nblack-box prompt learning, framed as a best-arm\nidentification problem, seeks the best prompt\nwithout necessarily maximizing the expected\ncumulative rewards. To maximize the cumulative\nrewards, the agent must manage the exploration-\nexploitation trade-off. Conversely, for the best-arm\nidentification problem, the agent should explore\nas extensively as possible within the T evaluation\nbudget to potentially identify the best action,\nleading to a pure exploration setting. Although\nit might seem that the agent should act purely at\nrandom to find the best action, some degree of\nexploitation may still be beneficial to guide the\nagent toward the optimal choice.\nTo achieve a high exploration rate while ex-\nploiting prior data, we consider the maximum en-\ntropy objective, as discussed in previous RL works\n(Ziebart, 2010; Haarnoja et al., 2018). This objec-\ntive promotes stochastic policies by augmenting\nthe reward with an entropy term, expressed as:\nmin\nθ\nJπ(θ) ≜Ea∼π(.;θ) [α log (π (a; θ)) −Qw (a))] , (6)\nwhere the temperature α controls the stochasticity\nof the agent. The above objective is a simplification\nof the objective in the Soft Actor-Critic (SAC) algo-\nrithm (Haarnoja et al., 2018), eliminating the state,\nas we consider a stateless environment and focus\non instantaneous rather than cumulative rewards.\nThe term −Ea∼π(.;θ) [log π(a)] represents the en-\ntropy of the policy, which we seek to maximize\nalongside the critic values, as entropy maximiza-\ntion is crucial for encouraging the actor to explore\ndifferent actions. A high α encourages exploration,\nwhereas a lower value of α favors exploitation.\nFurthermore, to avoid fine-tuning, and to achieve\nadaptive exploration, the entropy coefficient α can\nbe adjusted dynamically to maintain a target en-\ntropy level (Haarnoja et al., 2019), Htarget, ensuring\nsufficient exploration, as follows:\nmin\nα Jα ≜−Ea∼π(.)\n\u0002\nα ·\n\u0000log π(a; θ) + Htarget\n\u0001\u0003\n.\n(7)\nWe use a stochastic approximation of gradients,\nspecifically the Adam algorithm (Kingma and Ba,\n4\n\n2017; Reddi et al., 2018), to minimize the losses\nvia stochastic gradient descent. The learning rates\nλ, β, and γ are used for the Q-network, the policy\nnetworks, and the temperature, respectively.\n4\nMethodology\nThe methodology of our proposed algorithm, AC-\nING, is illustrated in Fig. 1, showing the actor-critic\ninteraction with the environment. In Fig. 2, the en-\nvironment is zoomed in, using the larger_animal\ndataset as an example. In the following, we provide\na detailed explanation of the full methodology.\nOverview. In each iteration t ≤T, the actor-\ncritic agent generates a continuous vector “action”\na (step 1). This action is then played on the en-\nvironment, which projects a into the appropriate\nspace using a fixed projection matrix P to obtain z.\nThe environment then concatenates the projected\nvector z with a set of exemplars’ embeddings from\nE and feeds it into a white-box LLM h (step 2).\nThe white-box LLM produces a discrete prompt, τ,\nwhich is evaluated using the validation dataset V\nbased on the responses from the black-box LLM\nf (step 3). The black-box LLM generates a pre-\ndiction, which is then contrasted to the true labels\nof the validation examples, and a score function\nprovides a final reward to the critic. This reward is\nused to compute the critic’s loss and update both\nthe critic and actor networks accordingly.\nStep\n1⃝.\nThe policy-network (actor) outputs a\nmean and variance of a distribution from which the\naction is sampled. Specifically, the action, repre-\nsented as a soft prompt vector, a ∈Rd\n′\n, is obtained\nby sampling from a Gaussian distribution with the\noutputted parameters. The network also computes\nthe associated log probability, which is crucial for\npolicy optimization, as it guides the learning pro-\ncess by balancing exploration and exploitation, by\ncontrolling the policy loss, as shown in Eq. (6).\nStep\n2⃝.\nAs depicted in the left side of Fig. 2,\nthe examples describing the task from the set of ex-\nemplars E, along with additional text such as “The\ninstruction was to,” are input into the embedding\nlayer of the white-box LLM to generate continuous\nvectors. These continuous vectors are then concate-\nnated with the soft prompt z, projected from the\naction a. The layers of the white-box LLM subse-\nquently process the resulting concatenated vector to\nproduce the discrete prompt τ (using the instruction\ngeneration template depicted in Fig. 1 top right).\nThis transformation is essential for converting the\ncontinuous prompt into a discrete format suitable\nfor input into the black-box LLM.\nStep\n3⃝. As depicted in the right side of Fig. 2,\nfor every input xi in the validation set V\n=\n{(xj, yj)}m\nj=1, the generated prompt τ is concate-\nnated to the input sentence xi and fed to the black-\nbox LLM, which generates an output sentence\nˆyi = f(τ(z) ⊕xi). The output of the black-box\nLLM is fed into a scoring function q(·, ·), which\ncomputes the score between the predicted output ˆyi\nand the true label yi. The overall score is calculated\nby averaging the scores across all samples, repre-\nsenting the reward: r = 1\nm\nPm\ni=1 q(ˆyi, yi), where\nm represents the number of samples and r serves\nas feedback for the actor-critic algorithm.\nStep\n4⃝. The critic is responsible for evaluating\nthe quality of actions taken by the actor using the\nnetwork Qw. This network estimates the expected\nreward for a given action a, which is generated by\nthe policy network π. Observing the reward r(a),\nthe critic computes the loss Eq. (4) and updates its\nnetwork using Eq. (5). The critic provides feed-\nback to the actor by estimating the Q-values of the\nactions sampled by the policy, and the actor uses\nthis feedback to improve its policy by maximiz-\ning Eq. (6). This continuous interaction between\nthe actor and critic ensures the actor learns to take\nactions that maximize the expected reward while\nbalancing exploration and exploitation.\nBy repeating steps 1 to 4 until the allowed\nbudget T is exhausted, the agent returns the\nbest instruction prompt, denoted as τ ⋆.\nThis\nprompt is then evaluated using the test set T\non the black-box LLM, yielding the final score\nS⋆=\n1\nm′\nPm′\ni=1 q(f(τ ⋆, x′\ni), y′\ni) (using the evalua-\ntion template depicted in Fig. 1 bottom right).\n5\nExperiments\nWe tackle instruction learning for ChatGPT. We\nconduct instruction induction tasks using 30\ndatasets from (Honovich et al., 2023; Chen et al.,\n2024), along with a data summarization task from\n(Gliwa et al., 2019). Furthermore, we compare the\nbest-learned instructions from our ACING algo-\nrithm against human-provided instructions from\n(Honovich et al., 2023), as shown in Table 1,\nwith additional best-learned instructions in Table\n9.\nMoreover, we compare ACING with three\nrecent representative baselines for black-box in-\nstruction learning: APE (Zhou et al., 2023), In-\nstructZero (Chen et al., 2024), and INSTINCT (Lin\n5\n\nInput: tiger, dog\nThe instruction was to\nOutput: tiger\nInput: spider, chamois\nOutput: chamois\nWhite Box LLM Embedding Layers\nRest of White LLM Layers\nBlack Box LLM\nExemplars\nValidation set\nPredictions\nInput: hamster, reindeer\nInput: giant squid, owl\ngiant squid\nreindeer\nThe instruction was to create a program that takes\ntwo animals as input and outputs the animal that\nis bigger\nInstruction\n0.14, 0.5, ... , -1, 0.76\n0.57, 1, -0.9 , .... , -0.73, 0.89\nEmbeddings\nSoft Prompt\nScore\nProjection\nActor\n1\nFigure 2: Illustration of the prompt generation and testing inside the environment.\net al., 2024), with results provided in Table 2.\nFollowing previous works and to ensure fairness,\nwe do not fine-tune the budget. Instead, like previ-\nous works, we consider a fixed budget of black-box\nLLM evaluations, set to T = 165 (Chen et al.,\n2024; Lin et al., 2024). Moreover, like previous\nworks, we use Vicuna-13B (Chiang et al., 2023) as\nthe white-box LLM h. Furthermore, for previous\nworks, we use their default (tuned) hyperparame-\nters, including the intrinsic dimension d′ = 10 and\nthe number of soft tokens Nz = 5. For fairness, we\nrefrain from fine-tuning these parameters for our\nmethod and use the same values as in prior works.\nThis ensures that our ACING algorithm searches\nin the same space, [0, 1]10, and uses the same total\nnumber of queries to the black-box LLM as APE,\nInstructZero, and INSTINCT for a fair compari-\nson. For each algorithm, after identifying the best\ninstruction using the validation set V, we evaluate\nthe discovered instruction on a separate test set T\nand report the test accuracy as the score.\nWe further conduct ablation studies on the key\ndesign choices for ACING, including the use of\nsmall learning budgets, providing reward plots for\nvarious selected tasks. Furthermore, we test the im-\npact of using different white-box language models,\nintrinsic dimensions, and numbers of exemplars.\nMore details on the experiments are provided in\nthe Appendix.\n5.1\nACING vs. Humans\nWe use human instructions provided in (Honovich\net al., 2023) for various instruction-induction tasks,\ntest them, and compare their scores against those\nof our best instructions. The complete list of se-\nlected tasks is provided in Table 1 in the Appendix.\nSeveral instructions were solved by both human\nexperts and our approach, i.e., achieving a perfect\ntest score of 1.00. For clarity and brevity, we omit\nthese results from the main paper and focus only on\ntasks where the scores of humans and our method\ndiffered, as shown in Table 1.\nAs shown in these tables, our approach outper-\nforms human instructions in several tasks, some-\ntimes by a significant margin. For example, in the\nrhyming task, our method achieved a 0.39 improve-\nment in the test score. The human instruction for\nthis task—Write a word that rhymes with the input\nword.—only resulted in a score of 0.61. In con-\ntrast, the best ACING instruction, which was an\nintriguing prompt: Input the word that the program\nthought I was inputting and then output the word\nthat program thought I was inputting, achieved a\nperfect score of 1.00.\nFurthermore, in the challenging sentence simi-\nlarity task, while the human-provided instruction\nachieved a score of 0.00, our approach improved\nthe test score by up to 0.21. However, the instruc-\ntion yielding the highest score appeared to bias\nthe prediction towards outputting “3” due to the\nfollowing instruction: Find a sentence pair that is\nprobably not similar, and the output is 3 - probably,\nwhich resulted in the best score. The second-best\ninstruction, proposed by ACING, achieved an im-\nprovement of 0.14 over the human score and was as\nfollows: Generate a probability score for each pair\nof sentences, where 1 - probably not means that\nthe two sentences are not related, 0 - definitely not\nmeans that the two sentences are not related, and 3\n- probably means that the two sentences are related.\nOverall, our approach recovers several human test\nscores while being fully automatic, and in many\ncases surpasses human performance in terms of\nthe number of successful tasks, average score, and\nmedian score across tasks. For the curious reader,\nwe provide all our best instructions along with their\ntest scores in Table 10 in the Appendix.\n6\n\nCategory\nTask\nHuman Instruction\nACING Instruction (Ours)\nHuman\nScore\nOur\nScore\nSpelling\nSecond_word_letter\nExtract the second letter of the\ninput word.\nOutput the first letter of each word that\nis a vowel (a, e, i, o, u) Example: In-\nput: year Output: e Input: trust Output: r\nInput: qualification Output: u Input: de-\nfendant Output: e\n0.96\n0.59\nSyntax\nNegation\nNegate the input sentence.\nFlip the truth value of the statements in\nthe input\n0.81\n0.82\nLexical\nSemantics\nAntonyms\nWrite a word that means the op-\nposite of the input word.\nTake a word and change it to its opposite\n0.70\n0.82\nSynonyms\nWrite a word with a similar\nmeaning to the input word.\nInput a word that is a synonym for the\nword that was output\n0.14\n0.13\nPhonetics\nRhymes\nWrite a word that rhymes with\nthe input word.\nInput the word that the program thought\nI was inputting and then output the word\nthat program thought I was inputting\n0.61\n1.00\nSemantics\nCause_and_effect\nFind which of the two given\ncause and effect sentences is the\ncause.\nFind the sentence that is the cause of the\neffect in the pair of sentences\n0.97\n0.92\nStyle\nInformal_to_formal\nRephrase the sentence in formal\nlanguage.\nConvert the input into output using the\nsame word order and with the same mean-\ning\n0.63\n0.5\nMulti-\nlingual\nTranslation_en-de\nTranslate the word into German.\nProvide a translation for each word in the\nEnglish text into German\n0.81\n0.84\nTranslation_en-es\nTranslate the word into Spanish.\nTranslate the words from English to Span-\nish, but I noticed that some of the transla-\ntions are not accurate\n0.89\n0.88\nTranslation_en-fr\nTranslate the word into French.\nCreate a program that would take an En-\nglish word as input and output its French\nequivalent\n0.86\n0.87\nGLUE\nSentiment\nDetermine whether a movie re-\nview is positive or negative.\nClassify each input as positive or nega-\ntive based on the assessment of the corre-\nsponding movie\n0.89\n0.90\nSentence_similarity\nRate the semantic similarity of\ntwo input sentences on a scale of\n0 - definitely not to 5 - perfectly.\nFind a sentence pair that is probably not\nsimilar, and the output is 3 - probably\n0.00\n0.21\nmedian score\n0.81\n0.83\naverage score\n0.69\n0.71\n# best-performing tasks\n5\n7\nTable 1: Tasks from the instruction-induction datasets where the human and ACING test scores differed. For each task, we\nprovide the corresponding human instruction as proposed in (Honovich et al., 2023) and our best discovered instruction. We\ntested these instructions on the test dataset and report their respective scores.\n5.2\nACING vs. Other Optimization Methods\nWe compare our method against recent baselines\non the 30 instruction-induction datasets. Table 2\npresents the average test accuracy (along with the\nstandard deviation) over three independent runs, us-\ning three different seeds. For each seed, we selected\nthe best instruction achieved by each method and\nevaluated it on the testing dataset. The table demon-\nstrates that our method, ACING, outperforms oth-\ners by achieving the highest accuracy in 14 out of\n30 tasks, compared to INSTINCT, InstructZERO,\nand APE which succeeded in 9 tasks or less each.\nAdditionally, ACING achieves the highest median\naccuracy across tasks, with a value of 0.71, which\nis 22 percentage points higher than APE. Further\ndetails, including the best prompt achieved for each\ntask and corresponding test scores, are in the Ap-\npendix, Table 10. Moreover, we compare the per-\nformance of ACING against other methods on a\nsummarization task using SAMSum dataset (Gliwa\net al., 2019) in Table 3, which shows that ACING\noutperforms the other methods.\nIn the following section, we demonstrate that the\nmedian score of ACING can be further improved\nby 5 percentage points using a simple two-phase\nexploration technique (splitting the budget), result-\ning in a 10-percentage-point improvement over the\nbest baseline considered. Furthermore, in ablation\nstudies, we show that fine-tuning the intrinsic di-\nmensions d\n′ can lead to additional improvements.\n7\n\nCategory\nTask\nAPE\nInstructZero\nINSTINCT\nACING\nSpelling\nLetters_list\n0.59 (0.02)\n1.00 (0.00)\n0.99 (0.01)\n1.00 (0.00)\nFirst_word_letter\n0.00 (0.00)\n1.00 (0.00)\n1.00 (0.00)\n1.00 (0.00)\nSecond_word_letter\n0.00 (0.00)\n0.35 (0.09)\n0.39 (0.28)\n0.70 (0.15)\nMorpho-Syntax\nSingular_to_plural\n1.00 (0.00)\n0.99 (0.01)\n0.95 (0.03)\n0.95 (0.03)\nActive_to_passive\n1.00 (0.00)\n0.98 (0.01)\n1.00 (0.00)\n1.00 (0.00)\nNegation\n0.79 (0.00)\n0.65 (0.10)\n0.58 (0.22)\n0.71 (0.06)\nLexical Semantics\nAntonyms\n0.79 (0.02)\n0.76 (0.00)\n0.84 (0.01)\n0.74 (0.01)\nSynonyms\n0.14 (0.01)\n0.22 (0.11)\n0.19 (0.08)\n0.13 (0.02)\nWord_unscrambling\n0.54 (0.00)\n0.59 (0.06)\n0.54 (0.02)\n0.50 (0.07)\nPhonetics\nRhymes\n0.59 (0.01)\n0.99 (0.01)\n0.36 (0.04)\n0.57 (0.31)\nNumerical\nSum\n0.87 (0.01)\n1.00 (0.00)\n0.70 (0.21)\n0.69 (0.31)\nDiff\n0.00 (0.00)\n1.00 (0.00)\n0.93 (0.09)\n1.00 (0.00)\nKnowledge\nLarger_animal\n0.72 (0.02)\n0.63 (0.07)\n0.81 (0.09)\n0.84 (0.07)\nPeriodic_elements\n0.99 (0.01)\n0.96 (0.03)\n1.00 (0.00)\n0.98 (0.00)\nCognitive Tasks\nCause_and_effect\n0.44 (0.09)\n0.52 (0.09)\n0.55 (0.11)\n0.69 (0.15)\nCommon_concept\n0.03 (0.02)\n0.14 (0.04)\n0.09 (0.04)\n0.19 (0.05)\nObject_counting\n0.30 (0.02)\n0.38 (0.06)\n0.40 (0.12)\n0.41 (0.03)\nOdd_one_out\n0.32 (0.02)\n0.57 (0.02)\n0.25 (0.18)\n0.64 (0.00)\nOrthography_starts_with\n0.23 (0.01)\n0.41 (0.09)\n0.54 (0.06)\n0.60 (0.12)\nTaxonomy_animal\n0.02 (0.02)\n0.67 (0.14)\n0.85 (0.06)\n0.71 (0.02)\nAuto_categorization\n0.31 (0.01)\n0.29 (0.02)\n0.07 (0.07)\n0.29 (0.04)\nWord_sorting\n0.58 (0.01)\n0.64 (0.05)\n0.23 (0.20)\n0.70 (0.03)\nCLUE\nSentence_similarity\n0.00 (0.00)\n0.10 (0.00)\n0.00 (0.00)\n0.13 (0.07)\nSentiment\n0.90 (0.00)\n0.88 (0.03)\n0.88 (0.02)\n0.89 (0.01)\nTranslation\nNum_to_verbal\n0.13 (0.02)\n0.99 (0.01)\n1.00 (0.00)\n0.99 (0.01)\nTranslation_en-de\n0.83 (0.01)\n0.82 (0.01)\n0.77 (0.02)\n0.82 (0.01)\nTranslation_en-es\n0.86 (0.01)\n0.67 (0.24)\n0.89 (0.00)\n0.87 (0.02)\nTranslation_en-fr\n0.88 (0.01)\n0.77 (0.06)\n0.85 (0.02)\n0.83 (0.01)\nStyle\nInformal_to_formal\n0.57 (0.01)\n0.48 (0.02)\n0.54 (0.09)\n0.44 (0.05)\nCoding\nAuto_debugging\n0.25 (0.00)\n0.25 (0.00)\n0.07 (0.07)\n0.25 (0.00)\nmedian score\n0.49\n0.66\n0.64\n0.71\n# best-performing tasks\n9\n8\n7\n14\nTable 2: Average test performance (and standard deviations) across 3 random seeds comparing ACING versus APE, InstructZero,\nand INSTINCT. The bottom rows report the median score and total number of best-performing tasks for each method.\nMetric\nAPE\nInstructZero\nINSTINCT\nACING\nROUGE-1\n0.35 (0.01)\n0.33 (0.00)\n0.36 (0.01)\n0.37 (0.01)\nROUGE-2\n0.12 (0.00)\n0.11 (0.00)\n0.14 (0.00)\n0.14 (0.00)\nROUGE-L\n0.25 (0.00)\n0.24 (0.01)\n0.27 (0.01)\n0.28 (0.01)\nTable 3: Average test performance (and standard devi-\nations) for instruction optimization for summarization\ntask using SAMSum dataset.\n5.3\nSplitting the Budget\nDue to the stochastic nature of the black-box LLM,\nthe same instruction may yield different rewards\nwhen evaluated by the LLM. For more robust\nprompt selection, the budget T can be split into\ntwo parts: an actor-critic exploration phase (repeat-\ning steps 1 to 4), followed by a uniform exploration\nphase. In the latter phase, the top p prompts discov-\nered by the actor-critic are evaluated multiple times,\nk times each, using the black-box LLM. Specifi-\ncally, the first phase uses T −p · k API calls, while\nthe second uses the remaining calls. The prompt\nwith the highest average reward across repetitions\ncan then be used at test time. In Table 5 in the\nAppendix, we show that ACING, using this budget\nsplit technique with an initial exploration budget of\n150 and a remaining budget of 15 calls (allocated\nto uniformly explore the top p = 5 prompts, each\nevaluated k = 3 times), achieves improved me-\ndian scores across tasks, resulting in 10 percentage\npoints improvement over the best baseline consid-\nered and higher test accuracy on 15 tasks compared\nto other methods.\n5.4\nAblation Studies\n5.4.1\nUsing Smaller Budgets\nIn the main paper, we report the final test score after\na fixed budget of 165 black-box LLM calls, mainly\nfollowing previous work (Lin et al., 2024; Chen\net al., 2024), avoiding any potential advantage that\ncould arise from optimizing this number. In the\nAppendix, we provide reward plots for the ACING\napproach, showing the best achieved reward over\nthe calls. As shown in various plots in Figure 3,\n8\n\nthe ACING approach found the optimal instruction\n(achieving a reward of 1) within just a few black-\nbox calls. Some tasks required fewer than 10 API\ncalls to find the optimal instruction, as in the case of\n‘active to passive’ and ‘letters list’, and fewer than\n20 calls for tasks like ‘translation’ and ‘diff’. It\ncan be seen that the vast majority of tasks achieved\ntheir best reward value within the first 60 to 80\ncalls, demonstrating that ACING can even be used\nfor more constrained budgets.\n5.4.2\nUsing a Different White-box Model\nIn the main paper, we present results using Vicuna\nas the white-box language model, following the\nsetup of prior work. To evaluate the impact of dif-\nferent white-box models on the performance of our\nmethod, we also tested ACING with WizardLM-\n13B-v1.2 (Xu et al., 2024) compared to Vicuna-\n13B-v1.3. The experiments in Table 8 indicate that\nWizardLM achieved a higher median test score\nacross all tasks and excelled in a greater number of\ntop-performing tasks, demonstrating that tuning the\nchoice of the white-box model can improve results.\n5.4.3\nUsing Different Action Dimensionalities\nIn the main paper, we present results using actions\nwith dimension d′ = 10, following the setup of\nprior work. To evaluate the performance of ACING\nwith different dimensionalities, we conducted an\nexperiment using d′ ∈{5, 10, 20, 40, 100}, keep-\ning other parameters fixed, for a budget of 165 calls.\nThe results, shown in Table 6, indicate that while\nthe smallest dimension, d′ = 5, recovered the best\nscores for some tasks, it generally has the lowest\nperformance across most tasks. Furthermore, both\nd′ = 10 and d′ = 20 yield similar performance in\nterms of the number of best-performing tasks, indi-\ncating a low sensitivity to this parameter. For the\nlarger dimension, d′ = 40, the method achieved the\nhighest number of best-performing tasks, showing\nthat tuning (increasing) d′ can improve results.\n5.4.4\nUsing a Different Number of Exemplars\nIn the main paper, we present results using five\nexemplars, following the setup of prior work. To\nevaluate the performance of ACING with a smaller\nnumber of exemplars, we conducted an experiment\nusing a single exemplar, i.e., |E| = 1. Results\nshown in Table 7 confirm that using five exemplars\ngenerally yield higher test scores and a greater num-\nber of best-performing tasks than a single exem-\nplar. However, a single exemplar still performs\nwell, matching the performance of five exemplars\nacross several tasks (e.g., phonetics, summation,\nmorpho-syntax, translation) and sometimes even\noutperforming them. Nonetheless, five exemplars\noffer an advantage, particularly for more cogni-\ntively complex tasks.\n6\nRelated Work\nMany early approaches such as AutoPrompt (Shin\net al., 2020), FluentPrompt (Shi et al., 2023), and\nother soft prompt-based methods (Lester et al.,\n2021; Li and Liang, 2021; Zhong et al., 2021) fo-\ncused on optimizing prompts for white-box LLMs\nand cannot be directly applied to black-box LLMs.\nMoreover, several techniques have also been pro-\nposed for optimizing prompts for grey-box LLMs:\nBBT (Sun et al., 2022b), BBTv2 (Sun et al., 2022a),\nand clip-tuning (Chai et al., 2022) use evolution-\nary algorithms for this purpose. However, these\nmethods still require access to input token embed-\ndings and output logits, making them unsuitable\nfor settings where only query access is allowed.\nOther approaches (Li and Liang, 2021; Zhong\net al., 2021) employ continuous soft prompts\nby fine-tuning parameters of specific input to-\nkens, but these often generate human-unreadable\nprompts and are impractical for API-accessible\nLLMs. Some other methods (Deng et al., 2022;\nZhang et al., 2023), created for optimizing discrete\nprompts instead of continuous ones, can optimize\ndiscrete tokens but require access to the LLM’s\noutput distribution, which is typically unavailable\nin black-box LLMs. Zhou et al. (2023) proposed\ngenerating candidate prompts through resampling\nwithout specific optimizations, while Guo et al.\n(2024) extended this model-free approach by using\nevolutionary algorithms. However, these methods\nusually require a large number of queries.\nInstruction optimization in black-box LLMs re-\ncently focused on searching through the space of\nsoft prompts, then relying on open-source white-\nbox LLMs as a means to transform these soft\nprompts into discrete prompts. InstructZero (Chen\net al., 2024) learns discrete prompts for ChatGPT\nby searching through the continuous space of soft\nprompts using Bayesian optimization, which are\nthen transformed into discrete prompts by a white-\nbox LLM. INSTINCT (Lin et al., 2024), a succes-\nsor to this approach, relies on the same approach.\nHowever, instead of Bayesian optimization, it uses\nNeuralUCB (Zhou et al., 2020), a contextual bandit\n9\n\n(selection) algorithm that searches within a finite\nset of continuous actions generated using Sobol\n(Eriksson et al., 2019). Our approach combines the\nhybrid strategy of using white-box and black-box\nLLMs with an actor-critic approach that operates\nin a stateless, infinite continuous-action space.\nTRIPLE (Shi et al., 2024) modeled the prompt\nlearning problem as a best-arm-identification prob-\nlem. However, it only considered the problem as\na selection task, assuming a given pool of discrete\nprompts. In contrast, our method addresses both\nthe generation and selection of prompts.\n7\nConclusion\nACING presents a promising approach for learn-\ning prompts for black-box LLMs by framing it\nas a stateless continuous-action RL problem. Our\nmethod shows significant improvements over re-\ncent baseline methods.\nEvaluations across 31\ndatasets reveal that ACING enhances prompt qual-\nity, achieving a median score improvement of\n10 percentage points and outperforming human-\ncrafted instructions by up to 39 percentage points.\nThis sets a new benchmark for automated instruc-\ntion optimization in black-box LLMs.\n8\nLimitations\nWhile our proposed results were achieved using\ndefault and commonly used values for hyperpa-\nrameters in network design, learning rates, and\nfollowing previous works for some common pa-\nrameters, the results presented depend on these\nchoices. These results could be further improved\nwith fine-tuning, as demonstrated in the ablation\nstudies (§ 5.4). Another limitation of this work,\nsimilar to previous studies, is the risk of overfitting\nwhen dealing with a limited number of validation\nexamples. This can result in a prompt that mini-\nmizes the loss on the validation set but does not\ngeneralize well to the test dataset, leading to a dis-\ncrepancy between training and test performance.\nAnother limitation of this work, similar to previous\nwork (Lin et al., 2024; Chen et al., 2024), is the\nreliance on a white-box model, where the choice\nof model may lead to performance variations, as\ndemonstrated in § 5.4.2. Avoiding this limitation re-\nquires acting directly on the black-box LLM, which\ninvolves handling actions in a large discrete action\nspace. This necessitates adapting current RL ap-\nproaches for large discrete action spaces, as studied\nin (Fourati et al., 2024) and the references therein.\nReferences\nYekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Hua\nWu, and Haifeng Wang. 2022. Clip-Tuning: Towards\nDerivative-free Prompt Learning with a Mixture of\nRewards. In EMNLP.\nJiuhai Chen,\nLichang Chen,\nHeng Huang,\nand\nTianyi Zhou. 2023.\nWhen do you need Chain-\nof-Thought Prompting for ChatGPT?\nPreprint,\narXiv:2304.03262.\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng\nHuang, and Tianyi Zhou. 2024. InstructZero: Ef-\nficient Instruction Optimization for Black-Box Large\nLanguage Models. In ICML.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An Open-\nSource Chatbot Impressing GPT-4 with 90%* Chat-\nGPT Quality.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric\nXing, and Zhiting Hu. 2022. RLPrompt: Optimizing\nDiscrete Text Prompts with Reinforcement Learning.\nIn EMNLP.\nDavid Eriksson, Michael Pearce, Jacob Gardner, Ryan D\nTurner, and Matthias Poloczek. 2019.\nScalable\nGlobal Optimization via Local Bayesian Optimiza-\ntion. In NeurIPS.\nFares Fourati, Vaneet Aggarwal, and Mohamed-Slim\nAlouini. 2024. Stochastic Q-learning for Large Dis-\ncrete Action Spaces. In ICML.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. SAMSum Corpus: A Human-\nannotated Dialogue Dataset for Abstractive Summa-\nrization. In Workshop on New Frontiers in Summa-\nrization.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative\nAdversarial Nets. In NeurIPS.\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\nSong, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\njiu Yang. 2024. Connecting Large Language Mod-\nels with Evolutionary Algorithms Yields Powerful\nPrompt Optimizers. In ICLR.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and\nSergey Levine. 2018. Soft Actor-Critic: Off-Policy\nMaximum Entropy Deep Reinforcement Learning\nwith a Stochastic Actor. In ICML.\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen,\nGeorge Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,\nHenry Zhu, Abhishek Gupta, Pieter Abbeel, and\nSergey Levine. 2019. Soft Actor-Critic Algorithms\nand Applications. Preprint, arXiv:1812.05905.\n10\n\nOr Honovich, Uri Shaham, Samuel R. Bowman, and\nOmer Levy. 2023. Instruction Induction: From Few\nExamples to Natural Language Task Descriptions. In\nACL.\nDiederik P. Kingma and Jimmy Ba. 2017.\nAdam:\nA Method for Stochastic Optimization.\nPreprint,\narXiv:1412.6980.\nTor Lattimore and Csaba Szepesvári. 2020.\nBandit\nAlgorithms. Cambridge University Press.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe Power of Scale for Parameter-Efficient Prompt\nTuning. In EMNLP.\nXiang Lisa Li and Percy Liang. 2021. Prefix-Tuning:\nOptimizing Continuous Prompts for Generation. In\nACL-IJCNLP.\nXiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai,\nWenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jail-\nlet, and Bryan Kian Hsiang Low. 2024. Use Your\nINSTINCT: INSTruction optimization for LLMs us-\nIng Neural bandits Coupled with Transformers. In\nICML.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\nTrain, Prompt, and Predict: A Systematic Survey of\nPrompting Methods in Natural Language Processing.\nACM Computing Surveys, 55(9).\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\nChoi, and Hannaneh Hajishirzi. 2021. Reframing\nInstructional Prompts to GPTk’s Language. In ACL.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi\nMirza, Alex Graves, Timothy Lillicrap, Tim Harley,\nDavid Silver, and Koray Kavukcuoglu. 2016. Asyn-\nchronous Methods for Deep Reinforcement Learning.\nIn ICML.\nOpenAI. 2023a. ChatGPT: A Conversational AI Model.\nOpenAI. 2023b. GPT-4 Technical Report. Preprint,\narXiv:2303.08774.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2023. GrIPS: Gradient-free, Edit-based In-\nstruction Search for Prompting Large Language Mod-\nels. In EACL.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\nguang Zhu, and Michael Zeng. 2023. Automatic\nPrompt Optimization with “Gradient Descent” and\nBeam Search. In EMNLP.\nSashank J Reddi, Satyen Kale, and Sanjiv Kumar. 2018.\nOn the Convergence of Adam and Beyond. In ICLR.\nLaria Reynolds and Kyle McDonell. 2021.\nPrompt\nProgramming for Large Language Models: Beyond\nthe Few-Shot Paradigm. In CHI EA.\nChengshuai Shi, Kun Yang, Jing Yang, and Cong Shen.\n2024. Best Arm Identification for Prompt Learning\nunder a Limited Budget. In ICLR 2024 Workshop\non Mathematical and Empirical Understanding of\nFoundation Models.\nWeijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtz-\nman, Yulia Tsvetkov, and Luke Zettlemoyer. 2023.\nToward Human Readable Prompt Tuning: Kubrick’s\nThe Shining is a good movie, and a good prompt too?\nIn EMNLP.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In EMNLP.\nAleksandrs Slivkins. 2019.\nIntroduction to Multi-\nArmed Bandits. Foundations and Trends® in Ma-\nchine Learning, 12(1-2).\nTianxiang Sun, Zhengfu He, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022a. BBTv2: Towards a\nGradient-Free Future with Large Language Models.\nIn EMNLP.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022b. Black-Box Tuning\nfor Language-Model-as-a-Service. In ICML.\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\nment Learning: An Introduction, 2nd edition. MIT\nPress.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand Efficient Foundation Language Models. Preprint,\narXiv:2302.13971.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and\nDenny Zhou. 2022. Chain-of-Thought Prompting\nElicits Reasoning in Large Language Models. In\nNeurIPS.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2024. WizardLM: Empowering Large Pre-\nTrained Language Models to Follow Complex In-\nstructions. In ICLR.\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\nurmans, and Joseph E Gonzalez. 2023. TEMPERA:\nTest-Time Prompt Editing via Reinforcement Learn-\ning. In ICLR.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2024.\nA Survey of Large Language Models.\nPreprint,\narXiv:2303.18223.\n11\n\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual Probing Is [MASK]: Learning vs. Learning\nto Recall. In NAACL.\nDongruo Zhou, Lihong Li, and Quanquan Gu. 2020.\nNeural Contextual Bandits with UCB-based Explo-\nration. In ICML.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023. Large Language Models are Human-Level\nPrompt Engineers. In ICLR.\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang,\nHao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue\nZhang, Neil Zhenqiang Gong, and Xing Xie. 2024.\nPromptRobust: Towards Evaluating the Robustness\nof Large Language Models on Adversarial Prompts.\nPreprint, arXiv:2306.04528.\nBrian D Ziebart. 2010. Modeling Purposeful Adaptive\nBehavior with the Principle of Maximum Causal En-\ntropy. Ph.D. thesis, Carnegie Mellon University.\n12\n\nA\nExperimental Details\nA.1\nHyperparameters Remark\nAcross the diverse tasks, in the main paper, the same hyperparameters were used, which shows that the\nalgorithm generalizes well across the 30 tasks without specifically tuning hyperparameters in each task. A\nsummary of the key parameters cab be found in the following Table. An implementation of ACING is\navailable at https://github.com/salmakh1/ACING.\nHyperparameter\nChoice\nWhite-box h\nVicuna-13B\nActor-network\n(1 −1024 −256 −10)\nCritic-network\n(10 −128 −128 −1)\nBudget T\n165\nIntrinsic (action) dimension d′\n5\nNumber of soft tokens Nz\n5\nSoft prompt dimension d\n5120 * Nz\nNumber of exemplars |E|\n5\nNumber of tokens generated by Wb\n64\nTable 4: Key hyperparameters and their values.\nA.2\nActor-Critic Details\nAcross all the tasks, we used three fully-connected layers for both the actor (1 −1024 −256 −10) and\nthe critics (10 −128 −128 −1) networks, with learning rates fixed at 3 · 10−4 for each. We used two\ncritic networks and consider their minimum as the actual critic. We learn the entropy parameter α using a\nlearning rate of 9 · 10−4.\nB\nACING Rewards over the (Calls) Steps\nIn the main paper, we report the final test score after a fixed budget of 165 black-box LLM calls. In this\nsection, we provide reward plots for the ACING approach, showing the best-achieved reward within the\nconducted calls. As shown in various plots in Figure 3, the ACING approach found the optimal prompt\n(achieving a reward of 1) within just a few black-box calls. Some tasks required fewer than 10 API calls\nto find the optimal instruction, such as for ‘active to passive’ and ‘letters list’, and fewer than 20 for tasks\nlike ‘translation’ and ‘diff’. It can be seen that the vast majority of tasks achieved their best reward value\nwithin the first 60 to 80 calls, demonstrating that ACING can even be used for much more constrained\nbudgets. The choice of 165 calls was mainly based on previous work (Lin et al., 2024; Chen et al., 2024),\navoiding any potential advantage that could come from optimizing this number.\n13\n\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBest Reward\nActive to Passive\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00\nBest Reward\nTranslation Eng Es\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.2\n0.4\n0.6\n0.8\n1.0\nBest Reward\nLarger Animal\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBest Reward\nLetters List\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nBest Reward\nAuto Categorization\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.2\n0.4\n0.6\n0.8\nBest Reward\nAntonyms\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nBest Reward\nWord Unscrambling\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nBest Reward\nRhymes\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nBest Reward\nDiff\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.2\n0.4\n0.6\n0.8\n1.0\nBest Reward\nPeriodic Elements\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nBest Reward\nCause and Effect\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nBest Reward\nCommon Concept\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nBest Reward\nObject Counting\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nBest Reward\nInformal to Formal\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nBest Reward\nAuto Debugging\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.2\n0.4\n0.6\n0.8\nBest Reward\nNegation\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.2\n0.4\n0.6\n0.8\nBest Reward\nOrthography Starts With\n0\n20\n40\n60\n80\n100\n120\n140\nIndex\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBest Reward\nSentiment\nFigure 3: Reward plots for running ACING on various selected tasks, showing the highest achieved reward on the\ny-axis until each API call (step), with the x-axis representing the number of API calls.\n14\n\nC\nACING with Budget Splitting\nDue to the stochastic nature of the black-box LLM, the same instruction may yield different rewards\nwhen evaluated by the LLM. To address this, we add a mechanism for more robust decision-making. The\nbudget T is split into two parts: an exploration phase where steps 1 to 4 are repeated, and an exploitation\nphase where the best p prompts are evaluated multiple times, k times each, using the black-box LLM. The\nexploration phase uses T −p · k API calls, with the remaining calls used for exploitation. Finally, the\nprompt with the highest average reward across repetitions is used at test time. In Table 5, we demonstrate\nthat ACING, with an exploration budget of T = 150 and the remaining 15 calls allocated to uniform\nexploration of the top p = 5 prompts (evaluated k = 3 times each), achieves improved median scores\nacross tasks and higher test accuracy on 15 tasks compared to previous work, unlike without the splitting\ntechnique which achieved higher test accuracy on 14 tasks compared to the previous work.\nCategory\nTask\nAPE\nInstructZero\nINSTINCT\nACING150+15\nACING165\n(budget splitting)\n(main paper)\nSpelling\nLetters_list\n0.59 (0.02)\n1.00 (0.00)\n0.99 (0.01)\n1.00 (0.00)\n1.00 (0.00)\nFirst_word_letter\n0.00 (0.00)\n1.00 (0.00)\n1.00 (0.00)\n1.00 (0.00)\n1.00 (0.00)\nSecond_word_letter\n0.00 (0.00)\n0.35 (0.09)\n0.39 (0.28)\n0.40 (0.17)\n0.70 (0.15)\nMorpho-Syntax\nSingular_to_plural\n1.00 (0.00)\n0.99 (0.01)\n0.95 (0.03)\n0.99 (0.01)\n0.95 (0.03)\nActive_to_passive\n1.00 (0.00)\n0.98 (0.01)\n1.00 (0.00)\n1.00 (0.00)\n1.00 (0.00)\nNegation\n0.79 (0.00)\n0.65 (0.10)\n0.58 (0.22)\n0.82 (0.00)\n0.71 (0.06)\nLexical Semantics\nAntonyms\n0.79 (0.02)\n0.76 (0.00)\n0.84 (0.01)\n0.76 (0.06)\n0.74 (0.01)\nSynonyms\n0.14 (0.01)\n0.22 (0.11)\n0.19 (0.08)\n0.12 (0.02)\n0.13 (0.02)\nWord_unscrambling\n0.54 (0.00)\n0.59 (0.06)\n0.54 (0.02)\n0.59 (0.05)\n0.50 (0.07)\nPhonetics\nRhymes\n0.59 (0.01)\n0.99 (0.01)\n0.36 (0.04)\n0.60 (0.38)\n0.57 (0.31)\nNumerical\nSum\n0.87 (0.01)\n1.00 (0.00)\n0.70 (0.21)\n0.81 (0.30)\n0.69 (0.31)\nDiff\n0.00 (0.00)\n1.00 (0.00)\n0.93 (0.09)\n0.97 (0.04)\n1.00 (0.00)\nKnowledge\nLarger_animal\n0.72 (0.02)\n0.63 (0.07)\n0.81 (0.09)\n0.86 (0.06)\n0.84 (0.07)\nPeriodic_elements\n0.99 (0.01)\n0.96 (0.03)\n1.00 (0.00)\n0.98 (0.03)\n0.98 (0.00)\nCognitive Tasks\nCause_and_effect\n0.44 (0.09)\n0.52 (0.09)\n0.55 (0.11)\n0.76 (0.18)\n0.69 (0.15)\nCommon_concept\n0.03 (0.02)\n0.14 (0.04)\n0.09 (0.04)\n0.10 (0.01)\n0.19 (0.05)\nObject_counting\n0.30 (0.02)\n0.38 (0.06)\n0.40 (0.12)\n0.48 (0.11)\n0.41 (0.03)\nOdd_one_out\n0.32 (0.02)\n0.57 (0.02)\n0.25 (0.18)\n0.59 (0.05)\n0.64 (0.00)\nOrthography_starts_with\n0.23 (0.01)\n0.41 (0.09)\n0.54 (0.06)\n0.54 (0.15)\n0.60 (0.12)\nTaxonomy_animal\n0.02 (0.02)\n0.67 (0.14)\n0.85 (0.06)\n0.53 (0.34)\n0.71 (0.02)\nAuto_categorization\n0.31 (0.01)\n0.29 (0.02)\n0.07 (0.07)\n0.27 (0.06)\n0.29 (0.04)\nWord_sorting\n0.58 (0.01)\n0.64 (0.05)\n0.23 (0.20)\n0.72 (0.02)\n0.70 (0.03)\nCLUE\nSentence_similarity\n0.00 (0.00)\n0.10 (0.00)\n0.00 (0.00)\n0.13 (0.08)\n0.13 (0.07)\nSentiment\n0.90 (0.00)\n0.88 (0.03)\n0.88 (0.02)\n0.88 (0.03)\n0.89 (0.01)\nTranslation\nNum_to_verbal\n0.13 (0.02)\n0.99 (0.01)\n1.00 (0.00)\n1.00 (0.00)\n0.99 (0.01)\nTranslation_en-de\n0.83 (0.01)\n0.82 (0.01)\n0.77 (0.02)\n0.82 (0.01)\n0.82 (0.01)\nTranslation_en-es\n0.86 (0.01)\n0.67 (0.24)\n0.89 (0.00)\n0.86 (0.02)\n0.87 (0.02)\nTranslation_en-fr\n0.88 (0.01)\n0.77 (0.06)\n0.85 (0.02)\n0.85 (0.02)\n0.83 (0.01)\nStyle\nInformal_to_formal\n0.57 (0.01)\n0.48 (0.02)\n0.54 (0.09)\n0.44 (0.05)\n0.44 (0.05)\nCoding\nAuto_debugging\n0.25 (0.00)\n0.25 (0.00)\n0.07 (0.07)\n0.29 (0.07)\n0.25 (0.00)\nmedian score\n0.49\n0.66\n0.64\n0.76\n0.71\n# best-performing tasks\n7\n8\n8\n15\n14\nTable 5: Average test performance (and standard deviations) across 3 random seeds comparing ACING versus APE, InstructZero,\nand INSTINCT. The bottom rows report the median score and total number of best-performing tasks for each method.\n15\n\nD\nACING with Different Intrinsic (Action) Dimensions\nIn the main paper, we present results using actions with a dimension of d′ = 10, following the setup\nof prior work. To evaluate the performance of ACING across different dimensionalities, we conducted\nexperiments with d′ ∈{5, 10, 20, 40}, keeping other parameters fixed, for a budget of 165. We report\nthe test results over different tasks and dimensionalities for a fixed seed. The results, shown in Table 6,\nindicate that while the smallest dimension, d′ = 5, recovered the best scores for some tasks, it generally\nhas the lowest performance across most tasks. Furthermore, both d′ = 10 and d′ = 20 yield similar\nperformance in terms of the number of best-performing tasks (9-10 tasks), indicating low sensitivity to\nthis parameter. For the much larger dimension, d′ = 40, the method achieved the highest number of\nbest-performing tasks (15 tasks), demonstrating improved performance with increased dimensionality.\nFurther increasing the dimensionality to d′ = 100 can still yield high results, outperforming d′ ∈5, 10, 20.\nHowever, while it remarkably outperformed d′ = 40 in some tasks, such as the second word letter task,\nsynonyms, and antonyms, it only achieved 14 best-performing tasks overall, indicating similar but slightly\nlower performance than d′ = 40.\nCategory\nTask\nd\n′ = 5\nd\n′ = 10\nd\n′ = 20\nd\n′ = 40\nd\n′ = 100\n(main paper)\nSpelling\nLetters_list\n1.00\n1.00\n0.98\n1.00\n1.00\nFirst_word_letter\n1.00\n1.00\n0.97\n1.00\n1.00\nSecond_word_letter\n0.23\n0.91\n0.30\n0.29\n0.92\nMorpho-Syntax\nSingular_to_plural\n0.99\n0.99\n1.00\n1.00\n1.00\nActive_to_passive\n1.00\n1.00\n1.00\n1.00\n1.00\nNegation\n0.82\n0.80\n0.84\n0.81\n0.70\nLexical Semantics\nAntonyms\n0.73\n0.76\n0.76\n0.82\n0.84\nSynonyms\n0.12\n0.13\n0.14\n0.14\n0.34\nWord_unscrambling\n0.53\n0.54\n0.49\n0.55\n0.43\nPhonetics\nRhymes\n0.95\n0.36\n0.94\n1.00\n1.00\nNumerical\nSum\n0.99\n0.79\n1.00\n0.99\n1.00\nDiff\n0.89\n1.00\n1.00\n1.00\n1.00\nKnowledge\nLarger_animal\n0.79\n0.94\n0.93\n0.65\n0.68\nPeriodic_elements\n1.00\n0.98\n0.94\n0.98\n0.98\nCognitive Tasks\nCause_and_effect\n0.64\n0.52\n0.92\n0.56\n0.56\nCommon_concept\n0.12\n0.23\n0.11\n0.12\n0.02\nObject_counting\n0.51\n0.39\n0.48\n0.59\n0.44\nOdd_one_out\n0.60\n0.64\n0.64\n0.68\n0.26\nOrthography_starts_with\n0.11\n0.65\n0.59\n0.61\n0.71\nTaxonomy_animal\n0.79\n0.68\n0.59\n0.85\n0.97\nAuto_categorization\n0.30\n0.28\n0.13\n0.33\n0.32\nWord_sorting\n0.55\n0.69\n0.74\n0.69\n0.48\nCLUE\nSentence_similarity\n0.00\n0.21\n0.00\n0.14\n0.07\nSentiment\n0.91\n0.88\n0.86\n0.91\n0.80\nTranslation\nNum_to_verbal\n0.99\n1.00\n1.00\n1.00\n1.00\nTranslation_en-de\n0.83\n0.81\n0.81\n0.80\n0.81\nTranslation_en-es\n0.89\n0.90\n0.91\n0.91\n0.86\nTranslation_en-fr\n0.84\n0.84\n0.86\n0.88\n0.73\nStyle\nInformal_to_formal\n0.54\n0.40\n0.51\n0.49\n0.50\nCoding\nAuto_debugging\n0.25\n0.25\n0.25\n0.25\n0.25\n# best-performing tasks\n8\n9\n10\n15\n14\nTable 6: Average ACING test performance for a fixed random seed (0) with different soft prompt dimensions d\n′. The bottom\nrow report the total number of best-performing tasks.\n16\n\nE\nACING with Different Number of Exemplars\nIn this section, we test ACING with a single exemplar, in contrast to the main results in the paper, which\nuse five exemplars for ACING and all other benchmarks. For these experiments, we fix all hyperparameters\nas in the main paper and run tests with a budget of 165. Intuitively, providing more exemplars to the\nlanguage model should facilitate prompt learning, so five exemplars are expected to yield better prompts\nthan a single exemplar. Our experiments, summarized in Table 7, support this intuition. The results show\nthat using five exemplars leads to higher test scores, as reflected in a greater number of best-performing\ntasks and an increase in median test scores across tasks. However, it is notable that performance did not\ndecrease drastically with only one exemplar, suggesting that a single exemplar is sufficient to achieve\ndecent results. In fact, across several tasks and categories (e.g., phonetics, summation, morpho-syntax,\nand translation), a single exemplar achieves the same performance of using five exemplars, and even\noutperforms the use of five exemplars in certain tasks. Nevertheless, using a single exemplar resulted\nin lower performance mainly in more cognitively challenging tasks, which is understandable, as more\ncomplex tasks are likely to benefit from additional exemplars.\nCategory\nTask\nACING (|E| = 1)\nACING (|E| = 5)\n(main paper)\nSpelling\nLetters_list\n1.00 (0.00)\n1.00 (0.00)\nFirst_word_letter\n0.99 (0.01)\n1.00 (0.00)\nSecond_word_letter\n0.19 (0.09)\n0.70 (0.15)\nMorpho-Syntax\nSingular_to_plural\n1.00 (0.00)\n0.95 (0.03)\nActive_to_passive\n1.00 (0.00)\n1.00 (0.00)\nNegation\n0.76 (0.08)\n0.71 (0.06)\nLexical Semantics\nAntonyms\n0.78 (0.05)\n0.74 (0.01)\nSynonyms\n0.09 (0.03)\n0.13 (0.02)\nWord_unscrambling\n0.41 (0.09)\n0.50 (0.07)\nPhonetics\nRhymes\n0.89 (0.08)\n0.57 (0.31)\nNumerical\nSum\n0.99 (0.01)\n0.69 (0.31)\nDiff\n0.99 (0.01)\n1.00 (0.00)\nKnowledge\nLarger_animal\n0.63 (0.17)\n0.84 (0.07)\nPeriodic_elements\n0.91 (0.08)\n0.98 (0.00)\nCognitive Tasks\nCause_and_effect\n0.51 (0.08)\n0.69 (0.15)\nCommon_concept\n0.16 (0.11)\n0.19 (0.05)\nObject_counting\n0.26 (0.06)\n0.41 (0.03)\nOdd_one_out\n0.64 (0.02)\n0.64 (0.00)\nOrthography_starts_with\n0.06 (0.05)\n0.60 (0.12)\nTaxonomy_animal\n0.63 (0.06)\n0.71 (0.02)\nAuto_categorization\n0.01 (0.01)\n0.29 (0.04)\nWord_sorting\n0.70 (0.02)\n0.70 (0.03)\nCLUE\nSentence_similarity\n0.07 (0.05)\n0.13 (0.07)\nSentiment\n0.70 (0.12)\n0.89 (0.01)\nTranslation\nNum_to_verbal\n1.00 (0.00)\n0.99 (0.01)\nTranslation_en-de\n0.72 (0.11)\n0.82 (0.01)\nTranslation_en-es\n0.88 (0.00)\n0.87 (0.02)\nTranslation_en-fr\n0.16 (0.04)\n0.83 (0.01)\nStyle\nInformal_to_formal\n0.42 (0.05)\n0.44 (0.05)\nCoding\nAuto_debugging\n0.25 (0.00)\n0.25 (0.00)\nmedian score\n0.67\n0.71\n# best-performing tasks\n12\n23\nTable 7: Average ACING test performance (and standard deviations) across 3 random seeds comparing 1 exemplar versus 5\nexemplars. The bottom rows report the median score and total number of best-performing tasks.\n17\n\nF\nACING with Different White-box models\nIn this section, we evaluate the impact of the choice of white-box model on the ACING method. Specifi-\ncally, we applied ACING for instruction learning with a GPT-3.5-turbo as the black-box LLM (as in the\nmain paper), but using different white-box LLMs. In the main paper, we reported ACING with Vicuna-\n13B-v1.3; in Table 8, we further test it with WizardLM-13B-v1.2. As shown in the table, changing the\nwhite-box LLM results in slight variations in performance. WizardLM achieved a higher median test\nscore across all tasks and excelled in a greater number of top-performing tasks.\nCategory\nTask\nACING (Vicuna)\nACING (WizardLM)\n(main paper)\nSpelling\nLetters_list\n1.00 (0.00)\n1.00 (0.00)\nFirst_word_letter\n1.00 (0.00)\n1.00 (0.00)\nSecond_word_letter\n0.70 (0.15)\n0.36 (0.18)\nMorpho-Syntax\nSingular_to_plural\n0.95 (0.03)\n0.99 (0.00)\nActive_to_passive\n1.00 (0.00)\n1.00 (0.00)\nNegation\n0.71 (0.06)\n0.83 (0.00)\nLexical Semantics\nAntonyms\n0.74 (0.01)\n0.81 (0.02)\nSynonyms\n0.13 (0.02)\n0.12 (0.03)\nWord_unscrambling\n0.50 (0.07)\n0.57 (0.05)\nPhonetics\nRhymes\n0.57 (0.31)\n0.97 (0.04)\nNumerical\nSum\n0.69 (0.31)\n1.00 (0.00)\nDiff\n1.00 (0.00)\n1.00 (0.00)\nKnowledge\nLarger_animal\n0.84 (0.07)\n0.94 (0.01)\nPeriodic_elements\n0.98 (0.00)\n0.97 (0.02)\nCognitive Tasks\nCause_and_effect\n0.69 (0.15)\n0.76 (0.20)\nCommon_concept\n0.19 (0.05)\n0.21 (0.05)\nObject_counting\n0.41 (0.03)\n0.46 (0.07)\nOdd_one_out\n0.64 (0.00)\n0.56 (0.11)\nOrthography_starts_with\n0.60 (0.12)\n0.62 (0.03)\nTaxonomy_animal\n0.71 (0.02)\n0.60 (0.32)\nAuto_categorization\n0.29 (0.04)\n0.35 (0.03)\nWord_sorting\n0.70 (0.03)\n0.61 (0.02)\nCLUE\nSentence_similarity\n0.13 (0.07)\n0.22 (0.04)\nSentiment\n0.89 (0.01)\n0.90 (0.02)\nTranslation\nNum_to_verbal\n0.99 (0.01)\n1.00 (0.00)\nTranslation_en-de\n0.82 (0.01)\n0.81 (0.01)\nTranslation_en-es\n0.87 (0.02)\n0.61 (0.38)\nTranslation_en-fr\n0.83 (0.01)\n0.83 (0.05)\nStyle\nInformal_to_formal\n0.44 (0.05)\n0.32 (0.19)\nCoding\nAuto_debugging\n0.25 (0.00)\n0.38 (0.10)\nmedian score\n0.71\n0.79\n# best-performing tasks\n14\n21\nTable 8: Average ACING test performance (and standard deviations) across 3 random seeds using Vicuna and WizardLM as\nwhite-box models. The bottom rows report the median score and total number of best-performing tasks.\n18\n\nG\nDemonstrations with Human Instructions\nCategory\nTask\nDemonstration\nHuman Instruction (Honovich et al., 2023)\nHuman\nScore\nOur\nScore\nSpelling\nFirst_word_letter\ncat →c\nExtract the first letter of the input word.\n1.00\n1.00\nSecond_word_letter\ncat →a\nExtract the second letter of the input word.\n0.96\n0.59\nLetters_list\ncat →c a t\nBreak the input word into letters, separated\nby spaces.\n1.00\n1.00\nMorpho-\nsyntax\nSingular_to_plural\ncat →cats\nConvert the input word to its plural form.\n1.00\n1.00\nActive_to_passive\nThe artist introduced the\nscientist. →The scien-\ntist was introduced by\nthe artist.\nWrite the input sentence in passive form.\n1.00\n1.00\nSyntax\nNegation\nTime is finite →Time is\nnot finite.\nNegate the input sentence.\n0.81\n0.82\nLexical\nSemantics\nAntonyms\nwon →lost\nWrite a word that means the opposite of the\ninput word.\n0.70\n0.82\nSynonyms\nalleged →supposed\nWrite a word with a similar meaning to the\ninput word.\n0.14\n0.13\nPhonetics\nRhymes\nsing →ring\nWrite a word that rhymes with the input word.\n0.61\n1.00\nKnowledge Larger_animal\nkoala, snail →koala\nWrite the larger of the two given animals.\n0.94\n0.94\nSemantics\nCause_and_effect\nSentence 1: The soda\nwent flat.\nSentence 2:\nThe bottle was left open.\n→The bottle was left\nopen.\nFind which of the two given cause and effect\nsentences is the cause.\n0.97\n0.92\nCommon_concept\nguitars, pendulums, neu-\ntrinos →involve oscilla-\ntions.\nFind a common characteristic for the given\nobjects.\n0.11\n0.11\nStyle\nInformal_to_formal\nPlease call once you\nget there →Please call\nupon your arrival.\nRephrase the sentence in formal language.\n0.63\n0.5\nNumerical\nSum\n22 10 →32\nSum the two given numbers.\n1.00\n1.00\nDiff\n32 22 →10\nSubtract the second number from the first.\n1.00\n1.00\nNum_to_Verbal\n26 →twenty-six\nWrite the number in English words.\n1.00\n1.00\nMulti-\nlingual\nTranslation_en-de\ngame →Spiel\nTranslate the word into German.\n0.81\n0.84\nTranslation_en-es\ngame →juego\nTranslate the word into Spanish.\n0.89\n0.88\nTranslation_en-fr\ngame →jeu\nTranslate the word into French.\n0.86\n0.87\nGLUE\nSentiment\nThe film is small in\nscope,\nyet\nperfectly\nformed. →positive\nDetermine whether a movie review is positive\nor negative.\n0.89\n0.90\nSentence_similarity\nSentence 1: A man is\nsmoking. Sentence 2: A\nman is skating. →0 -\ndefinitely not\nRate the semantic similarity of two input sen-\ntences on a scale of 0 - definitely not to 5 -\nperfectly.\n0.00\n0.21\nmedian score\n0.89\n0.90\naverage score\n0.78\n0.79\n# best-performing tasks\n14\n16\nTable 9: Classified tasks into categories from the instruction-induction datasets. For each task, we provide a\ncorresponding demonstration, with →separating the input from the output, along with its respective human\ninstruction as proposed in (Honovich et al., 2023). We tested these instructions, report their test scores, and compare\nthem to our best test scores using ACING with Vicuna-13B as the white-box LLM.\n19\n\nH\nOur Best Learned Instructions\nTask\nBest instruction\nTest Score\nactive_to_passive\nChange the input to match the output, but the output is already in the\npassive voice\n1.00\nantonyms\nTake a word and change it to its opposite\n0.82\nauto_categorization\nMatch the input to the output, and the answer is:Input: Nature Nan-\notechnology, Annual Review of Biochemistry, and The Lancet Neurology\nOutput: top journals Input: Jeans, Tops, and Suits Output: Apparel\n0.34\nauto_debugging\nInput the code into a Python interpreter and observe the output\n0.375\ncause_and_effect\nFind the sentence that is the cause of the effect in the pair of sentences\n0.92\ncommon_concept\nMake a connection between the input and output, but the connection is\nnot clear\n0.11\ndiff\nFind the difference between the two numbers\n1.00\nfirst_word_letter\nCreate a function that takes a string as input and returns the first letter of\nthe first word in the string\n1.00\ninformal_to_formal\nConvert the input into output using the same word order and with the\nsame meaning\n0.50\nlarger_animal\nCreate a program that takes two animals as input and outputs the animal\nthat is bigger\n0.94\nletters_list\nInput the word \"year\" and the output was \"y e a r\"\n1.00\nnegation\nFlip the truth value of the statements in the input\n0.82\nnum_to_verbal\nConvert numbers to words\n1.00\nobject_counting\nProvide a number that represents how many items are in the input\n0.55\nodd_one_out\nFind the word that does not belong in each group based on the given\nwords\n0.64\northography_starts_with\nFind a word in the text that starts with the letter provided and to output\nthat word\n0.71\nperiodic_elements\nFind the name of the element based on its atomic number\n1.00\nrhymes\nInput the word that the program thought I was inputting and then output\nthe word that program thought I was inputting\n1.00\nsecond_word_letter\nOutput the first letter of each word that is a vowel (a, e, i, o, u) Example:\nInput: year Output: e Input: trust Output: r Input: qualification Output:\nu Input: defendant Output:\n0.59\nsentence_similarity\nFind a sentence pair that is probably not similar, and the output is 3 -\nprobably\n0.21\nsentiment\nClassify each input as positive or negative based on the assessment of\nthe corresponding movie\n0.90\nsingular_to_plural\nAdd the suffix -s to the end of the word to make it plural\n1.00\nsum\nFind the sum of the two numbers\n1.00\nsynonyms\nInput a word that is a synonym for the word that was output\n0.13\ntaxonomy_animal\nMake the AI generate a sequence of animals based on the input provided\n0.75\ntranslation_en-de\nProvide a translation for each word in the English text into German\n0.84\ntranslation_en-es\nTranslate the words from English to Spanish, but I noticed that some of\nthe translations are not accurate\n0.88\ntranslation_en-fr\nCreate a program that would take an English word as input and output\nits French equivalent\n0.87\nword_sorting\nOutput the words in alphabetical order, but the output is not in alphabeti-\ncal order\n0.73\nword_unscrambling\nConvert the input to a word that is a common English word\n0.62\nTable 10: The best instruction discovered by ACING for all the 30 instruction-induction tasks using with Vicuna-13B\nas the white-box LLM.\n20",
    "pdf_filename": "ACING_Actor-Critic_for_Instruction_Learning_in_Black-Box_Large_Language_Models.pdf"
}