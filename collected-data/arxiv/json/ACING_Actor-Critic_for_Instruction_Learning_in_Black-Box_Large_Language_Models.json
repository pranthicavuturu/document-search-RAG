{
    "title": "ACING: Actor-Critic for Instruction Learning",
    "abstract": "result,developingefficientmethodsforautomatic TheeffectivenessofLargeLanguageModels promptoptimizationhasbecomehighlyimportant (LLMs) in solving tasks vastly depends on tomaximizeLLMsperformance. thequalityoftheinstructions,whichoftenre- quirefine-tuningthroughextensivehumanef- Severalmethodsforautomatedpromptoptimiza- fort. Thishighlightstheneedforautomatedin- tion have been proposed in the literature. Some structionoptimization;however,thisoptimiza- focus on optimizing continuous prompts, known tion is particularly challenging when dealing assoftprompts(Zhongetal.,2021;LiandLiang, withblack-boxLLMs,wheremodelparameters 2021),whicharetypicallyfedintoaLLMafterits andgradientsremaininaccessible. embeddinglayersincetheydonotrepresentmean- We propose ACING, a task-specific prompt ingfullanguagethattheLLMcaninterpretdirectly. optimization approach framed as a state- Othermethodsaimatoptimizingdiscreteprompts, lesscontinuous-actionReinforcementLearning known as hard prompts, by either generating di- (RL)problem,knownasthecontinuumbandit verseprompts(Zhouetal.,2023)orrefiningexist- setting. ACINGleveragesanactor-critic-based methodtooptimizeprompts,learningfromnon- ingones(Pryzantetal.,2023). Forbothsoftand differentiablerewardsignals. WevalidateAC- hard prompts, some approaches rely on gradient- ING by optimizing prompts for ChatGPT on based methods (Shin et al., 2020; Lester et al., 30instruction-basedtasks,aswellasasumma- 2021;LiandLiang,2021). However,thesemeth- rizationtask. ACINGconsistentlyoutperforms odsrequireaccesstothegradientsoftheLLMsand baselinemethods,achievingamedianscoreim- are therefore limited to white-box LLMs. Mean- provementof10percentagepointscompared while,themostpowerfulLLMstodayaretypically tothebestbaselineconsidered. Furthermore, black-boxmodels(e.g.,ChatGPT(OpenAI,2023a) ACING not only recovers but also surpasses human-crafted expert instructions, achieving andGPT-4(OpenAI,2023b)). uptoa39percentagepointimprovementover To overcome these challenges, recent studies humanbenchmarks. haveintroducedgradient-freetechniquesforopti- mizing prompts in black-box LLMs (Zhou et al., 1 Introduction 2023; Prasad et al., 2023; Pryzant et al., 2023). Large Language Models (LLMs) have demon- Whilepromising,theseapproachesrelyonheuris- strated remarkable performance across a wide tic local search methods and fail to fully utilize range of tasks (Zhao et al., 2024; Touvron et al., theobservationhistory,includingpreviouslytested 2023). This success is largely attributed to their instructions and their outcomes, when selecting stronginstruction-followingcapabilities,whichen- new instructions to test. Consequently, they are ableadaptationtodiversedownstreamapplications lesseffectiveatbalancingtheinherentexploration- (Chenetal.,2023;Liuetal.,2023). Theseinstruc- exploitationtrade-off,leadingtoinefficientquery- tions,commonlyreferredtoasprompts,playacru- ing. This inefficiency becomes problematic and cial role in the performance of LLMs (Wei et al., impractical when API calls to black-box LLMs 2022;Zhuetal.,2024;Liuetal.,2023). Giventhe involvesignificanttimeandfinancialcosts. importanceofprompts,researchersareincreasingly A recent line of work has focused on optimiz- interested in automating their optimization to re- ing instructions for black-box LLMs by search- ducetheneedformanualadjustments,whichisof- ing through soft prompts and transforming them tenalabor-intensiveandcostlyprocess(Reynolds intodiscretepromptsusingwhite-boxLLMs. In- 1 4202 voN 91 ]LC.sc[ 1v63721.1142:viXra",
    "body": "ACING: Actor-Critic for Instruction Learning\nin Black-Box Large Language Models\nSalmaKharrat FaresFourati MarcoCanini\nKAUST\n{salma.kharrat, fares.fourati}@kaust.edu.sa\nAbstract and McDonell, 2021; Mishra et al., 2021). As a\nresult,developingefficientmethodsforautomatic\nTheeffectivenessofLargeLanguageModels\npromptoptimizationhasbecomehighlyimportant\n(LLMs) in solving tasks vastly depends on\ntomaximizeLLMsperformance.\nthequalityoftheinstructions,whichoftenre-\nquirefine-tuningthroughextensivehumanef- Severalmethodsforautomatedpromptoptimiza-\nfort. Thishighlightstheneedforautomatedin- tion have been proposed in the literature. Some\nstructionoptimization;however,thisoptimiza- focus on optimizing continuous prompts, known\ntion is particularly challenging when dealing assoftprompts(Zhongetal.,2021;LiandLiang,\nwithblack-boxLLMs,wheremodelparameters\n2021),whicharetypicallyfedintoaLLMafterits\nandgradientsremaininaccessible.\nembeddinglayersincetheydonotrepresentmean-\nWe propose ACING, a task-specific prompt ingfullanguagethattheLLMcaninterpretdirectly.\noptimization approach framed as a state-\nOthermethodsaimatoptimizingdiscreteprompts,\nlesscontinuous-actionReinforcementLearning\nknown as hard prompts, by either generating di-\n(RL)problem,knownasthecontinuumbandit\nverseprompts(Zhouetal.,2023)orrefiningexist-\nsetting. ACINGleveragesanactor-critic-based\nmethodtooptimizeprompts,learningfromnon- ingones(Pryzantetal.,2023). Forbothsoftand\ndifferentiablerewardsignals. WevalidateAC- hard prompts, some approaches rely on gradient-\nING by optimizing prompts for ChatGPT on based methods (Shin et al., 2020; Lester et al.,\n30instruction-basedtasks,aswellasasumma- 2021;LiandLiang,2021). However,thesemeth-\nrizationtask. ACINGconsistentlyoutperforms\nodsrequireaccesstothegradientsoftheLLMsand\nbaselinemethods,achievingamedianscoreim-\nare therefore limited to white-box LLMs. Mean-\nprovementof10percentagepointscompared\nwhile,themostpowerfulLLMstodayaretypically\ntothebestbaselineconsidered. Furthermore,\nblack-boxmodels(e.g.,ChatGPT(OpenAI,2023a)\nACING not only recovers but also surpasses\nhuman-crafted expert instructions, achieving andGPT-4(OpenAI,2023b)).\nuptoa39percentagepointimprovementover To overcome these challenges, recent studies\nhumanbenchmarks. haveintroducedgradient-freetechniquesforopti-\nmizing prompts in black-box LLMs (Zhou et al.,\n1 Introduction\n2023; Prasad et al., 2023; Pryzant et al., 2023).\nLarge Language Models (LLMs) have demon- Whilepromising,theseapproachesrelyonheuris-\nstrated remarkable performance across a wide tic local search methods and fail to fully utilize\nrange of tasks (Zhao et al., 2024; Touvron et al., theobservationhistory,includingpreviouslytested\n2023). This success is largely attributed to their instructions and their outcomes, when selecting\nstronginstruction-followingcapabilities,whichen- new instructions to test. Consequently, they are\nableadaptationtodiversedownstreamapplications lesseffectiveatbalancingtheinherentexploration-\n(Chenetal.,2023;Liuetal.,2023). Theseinstruc- exploitationtrade-off,leadingtoinefficientquery-\ntions,commonlyreferredtoasprompts,playacru- ing. This inefficiency becomes problematic and\ncial role in the performance of LLMs (Wei et al., impractical when API calls to black-box LLMs\n2022;Zhuetal.,2024;Liuetal.,2023). Giventhe involvesignificanttimeandfinancialcosts.\nimportanceofprompts,researchersareincreasingly A recent line of work has focused on optimiz-\ninterested in automating their optimization to re- ing instructions for black-box LLMs by search-\nducetheneedformanualadjustments,whichisof- ing through soft prompts and transforming them\ntenalabor-intensiveandcostlyprocess(Reynolds intodiscretepromptsusingwhite-boxLLMs. In-\n1\n4202\nvoN\n91\n]LC.sc[\n1v63721.1142:viXra\nstructZero (Chen et al., 2024) optimizes discrete To achieve this, we utilize a validation dataset\nprompts for ChatGPT by applying Bayesian op- V = {(x ,y )}m , where each pair consists of\nj j j=1\ntimization over soft prompts. Building on the aninputsentencex anditscorrespondingground\nj\nsame line, INSTINCT (Lin et al., 2024) replaces truthoutputy . Ourobjectiveistofindtheprompt\nj\nBayesian optimization with NeuralUCB (Zhou thatmaximizesthescoringfunctionq(·,·)across\net al., 2020), a critic algorithm, to select actions, thevalidationdataset,whereq(yˆ ,y )measuresthe\nj j\nfromafinitepoolofsoftprompts,initiallygener- qualityofthepredictedoutputyˆ againstthetrue\nj\natedwithSobol(Erikssonetal.,2019). outputy . Thus,ourobjectivebecomesfindingthe\nj\nInspired by the success of actor-critic meth- promptτ⋆ thatmaximizestheaveragescoreover\nods for control problems (Haarnoja et al., 2018, thevalidationsetV. Thederivedpromptτ⋆ isthen\n2019)andgenerativeadversarialnetworks(Good- evaluatedonaseparatetestsetT = {(x′,y′)}m′\nj j j=1\nfellowetal.,2014),ourapproachcombinesthehy- toassessitsgeneralizationperformance.\nbridwhite-box/black-boxframeworkwithanactor-\n2.2 LearningoveraContinuousSpace\ncriticReinforcementLearning(RL)algorithm,op-\nerating in a stateless, infinite continuous-action Directly optimizing the prompt τ for the black-\nspace, and constrained by a strict budget of API boxLLMmodelf presentssubstantialchallenges\ncalls. The actor aims to maximize the critic’s due to the discrete combinatorial nature of token\nvalue while remaining as stochastic as possible selection in τ. To mitigate this challenge, prior\ntoenhanceexplorationandsimultaneouslyexploit approaches like InstructZero (Chen et al., 2024)\npromising actions. This approach increases the andINSTINCT(Linetal.,2024)employawhite-\nlikelihoodoffindingoptimalpromptsandenables boxLLM,representedbyh,toconvertthediscrete\nmore efficient use of the limited number of API optimizationproblemintoacontinuousonebyin-\ncalls. ACING consistently outperforms baseline troducingasoftpromptvectorz ∈ Rd,whichisa\nmethods,achievingamedianscoreimprovement continuousd-dimensionalvectorrepresentingthe\nof 10 percentage points. Furthermore, our opti- tokenembeddingofasetofvirtualtokens.\nmized prompts not only recover but also surpass Thewhite-boxmodelhhasaccesstoadataset\nhuman-craftedexpertinstructions,achievingupto ofexemplars,E = {(u j,v j)}k j=1,whereeachpair\na39percentagepointimprovementagainsthuman (u j,v j)definesinput-outputtextsequencesthatex-\nbenchmarks. emplifyadownstreamtask. Thewhite-boxLLM\nAn implementation of ACING is available at h serves as a proxy, mapping the soft prompt z\nhttps://github.com/salmakh1/ACING. into a discrete prompt τ suitable for input to the\nblack-boxLLM.Specifically,giventheexemplars\n2 ProblemFormulation E and the vector z, their concatenation is input\nto the white-box LLM, generating the discrete\n2.1 PromptLearningObjective\nprompt τ(z) = h(z,E). This generated prompt\nWeaimtoimprovetheperformanceofablack-box τ(z)isprependedtoatestinputx fromthevali-\nj\nLLM, denoted by f, which can only be accessed dation set V, and the combined input is provided\nthrough its API, while its internal parameters re- to the black-box LLM f to generate an output\nmain unknown. Given a task represented by an yˆ = f(τ(z) ⊕ x ). The output yˆ is then eval-\nj j j\n(unknown) distribution (x,y) ∼ D—where x de- uatedagainstthetrueoutputy usingthescoring\nj\nnotespossibleinputsandy thecorrespondingcor- function q(yˆ ,y ). By using a fixed set of exem-\nj j\nrectoutputs—ourgoalistofindtheoptimalprompt plars E, the original discrete problem (Equation\nτ⋆ that maximizes the likelihood of f producing (1))offindingtheoptimalpromptτ iseffectively\ncorrectoutputsforagiventask. Thisisevaluated transformedintoacontinuousoptimizationprob-\nusingascoringfunctionq(·,·) ∈ [0,1]. Theblack- lemoverthesoftpromptvectorz,asfollows:\nbox model f processes an input formed by con-\nmax E [q(y,f(τ(z)⊕x))]. (2)\ncatenating (⊕) the prompt τ with the sentence x, (x,y)∼D\nz∈Rd\nproducingapredictedoutputyˆ= f(τ ⊕x). More\n2.3 ReducingtheContinuousSpace\nformally,theobjectiveistomaximizetheexpected\nscoreoftheLLMinsolvingthetaskD,definedas: The soft prompt z is typically high-dimensional\n(e.g., d = 5120 × N , with N the number of\nz z\nmax E (x,y)∼D[q(y,f(τ ⊕x))]. (1) virtual tokens, when h is Vicuna-13B (Chiang\nτ\n2\netal.,2023)),whichmakesitachallengingprob- Algorithm1StatelessSoftActor-Critic\nlem. Therefore,weemployrandomprojectiontech- Input: EvaluationbudgetT\nniques to reduce the input dimension as done in 1: Initializeθ,w(e.g.,randomly)\nprior works (Chen et al., 2024; Lin et al., 2024). 2: fort ← 1toT do\nSpecifically, given a matrix P ∈ Rd×d′ where 3: Chooseactiona t ∼ π(· | θ)\nd′ ≪ d, with randomly sampled elements, and 4: Takeactiona t andobserverewardr t\na continuous vector a ∈ Rd′ , the vector z = Pa 5: w ← w−λ∇ˆJ Q(w) (Equation(5))\nisusedasthesoftprompt. Bysubstitutingzwith 6: θ ← θ+β∇ˆJ π(θ) (Equation(6))\nPa, the input variable to be optimized in Eq. 2 7: α ← α−γ∇ˆJ α(α) (Equation(7))\nbecomesa,therebyreducingtheinputdimension\n8: endfor\noftheoptimizationproblemtod′:\nmax E [q(y,f(τ(Pa)⊕x))]. (3)\na∈Rd′ (x,y)∼D to as a stochastic bandit setting (Slivkins, 2019;\nLattimore and Szepesvári, 2020). In this context,\nFor fairness with previous works, the reduced\nat each iteration t ≤ T (where t is incremented\ninputdimensiond′, referredtoastheintrinsicdi-\nwitheveryblack-boxLLMcall),theagent’saction\nmension,ischosenasd′ = 10(Chenetal.,2024;\na generate stochastic rewards, r ∈ [0,1], with-\nLinetal.,2024). Anablationstudyontheintrinsic t t\nout affecting the reward distributions associated\ndimension is discussed in § 5.4.3. Furthermore,\nwiththoseactions. Theprompt-learningobjective\nto maintain consistency with these prior studies,\nismodeledasastochasticbest-armidentification\nwe similarly use a projection matrix P with ele-\nprobleminastateless,continuousactionspaceRL,\nmentsuniformlyandindependentlysampledfrom\nspecificallyinaninfinitecontinuumbanditsetting.\nUniform(−1,1), and bound the vector such that\na ∈ [0,1]d′ .\n3.2 Actor-CriticContinuumBandits\n3 Actor-CriticApproachforInstruction RL techniques are generally used to learn opti-\nLearninginBlack-BoxLLMs mal policies and actions leading to best rewards.\nThesetechniquescanbeclassifiedintovalue-based\n3.1 InstructionLearningasanRLProblem\n(critic-centric),policy-gradient(actor-centric),or\nInthiswork,weframetheprompt-learningprob- hybridapproaches. Actor-criticalgorithmsmerge\nlemasanagentnavigatinganinfiniteactionspace thebenefitsofbothbyemployingtwokeycompo-\nwithinad′\n-dimensionalunithypercube,searching nents: the actor, responsible for policy optimiza-\n′\nwithin an infinite number of actions a ∈ [0,1]d , tion,andthecritic,whichevaluatesthepolicy(Sut-\nincontrasttopreviousworksthatconsiderafinite ton and Barto, 2018). This hybrid approach has\nnumber of actions initially generated with Sobol demonstratedsuccessinvariousdomains,includ-\n(Erikssonetal.,2019;Linetal.,2024). Aspartof ingcontinuouscontrolproblems(Mnihetal.,2016;\ntheenvironment,thesecontinuousactionsarepro- Haarnoja et al., 2018, 2019) and with generative\njectedintoahigher-dimensionalspace(§2.3)and adversarial networks (GANs) (Goodfellow et al.,\nthen transformed into discrete prompts, through 2014),whereageneratorandacriticnetworkcol-\na white-box LLM (§ 4). These discrete prompts laborativelyoptimizeperformance.\nare evaluated by a black-box LLM over a valida- Inspired by the success of actor-critic ap-\ntiondataset,afterwhicharewardiscomputedand proaches, we introduce a (stateless) actor-critic\nreturned to the agent. We consider a finite hori- algorithm, as provided in Algorithm 1, tailored\nzon T, representing the maximum number of ac- toourinfinitecontinuumbanditsetting,enabling\ntionsplayed(i.e.,thebudgetofevaluatedinstruc- autonomous learning of effective prompts with a\ntions), after which the agent ceases exploration. constrainedevaluationbudgetandoutperforming\nTheagent’sgoalistodiscovertheoptimalcontinu- previous state-of-the-art black-box prompt learn-\nousaction,a⋆,whichproducestheoptimaldiscrete ing approaches. Both the actor and the critic are\npromptandthehighestreward. parameterized by neural networks. In typical RL\nUnlikeingeneralRLsettings,wheretheagent problems,theagentinteractswithanenvironment\nencountersmultiplestateswithvaryingrewarddis- with varying states, so its policy depends on the\ntributions,theagentweconsideroperatesinastate- currentstate. Thatis,thepolicynetworkgenerates\nless, stochastic environment, commonly referred actionsbasedonthecurrentstates,π(.|s;θ),where\n3\nActor-Critic Agent Environment Induction Generation Template\nActor Projection Input: Output:\n1\nPolicy Network . . . . . .\nSoft prompt + Exemplars Input: Output:\nembeddings The instruction was to\nAction\n2 White-box LLM\nError\nCritic Discrete prompt + Validation set Evaluation Template\nQ-Network 4\nInput: [Test set]\nReward 3 Black-box LLM Instruction: [Instruction]\nOutput\nOutput: [prediction]\nScore function\nFigure1: PipelineofACING.Ineachiteration,asoftpromptalongwithseveralexamplesofthetargettaskareprovidedtothe\nwhite-boxLLMtogenerateaninstruction.Thisinstructionisthenusedtoquerytheblack-boxLLM,whichproducesanswersto\nthetargettaskqueries.Theresultingscoreisreturnedtotheagentasareward,whichisusedtoupdateitsnetworksandadjustits\npolicy.BothLLMsremainfrozenthroughouttheprocess.\nθ denotestheparameters. However,intheproblem leading to a pure exploration setting. Although\nweconsider,thesettingisstateless. Therefore,we it might seem that the agent should act purely at\ndefineapolicynetworkπ(.;θ)withaconstantin- random to find the best action, some degree of\nputthatproducesad′\n-dimensionalvectorasoutput exploitation may still be beneficial to guide the\na,representingthecontinuousaction. Forthecritic, agenttowardtheoptimalchoice.\nweuseaneuralnetworkwithad′\n-dimensionalin- To achieve a high exploration rate while ex-\nput(theaction)andasingleoutputthatestimates ploitingpriordata,weconsiderthemaximumen-\nthe action’s quality, thereby assessing the quality tropyobjective,asdiscussedinpreviousRLworks\nofthepolicythatproposedit. Thecriticnetwork’s (Ziebart,2010;Haarnojaetal.,2018). Thisobjec-\nparametersaretrainedtominimizethefollowing: tive promotes stochastic policies by augmenting\ntherewardwithanentropyterm,expressedas:\n(cid:20) (cid:21)\n1\nminJ (w)≜E (Q (a)−r(a))2 , (4)\nw Q a∼D 2 w minJ π(θ)≜E a∼π(.;θ)[αlog(π(a;θ))−Q w(a))], (6)\nθ\nwhichcanbeoptimizedwithstochasticgradients wherethetemperatureαcontrolsthestochasticity\noftheagent. Theaboveobjectiveisasimplification\n∇ˆ J (w)=∇ Q (a )(Q (a )−r(a )), (5)\nθ Q w w t w t t\noftheobjectiveintheSoftActor-Critic(SAC)algo-\nwhereQisthecritic-networkwithparametersw. rithm(Haarnojaetal.,2018),eliminatingthestate,\nTheactor-criticlearningprocessalternatesbetween asweconsiderastatelessenvironmentandfocus\npolicy-networkandcritic-networkupdates. on instantaneous rather than cumulative rewards.\nThe term −E [logπ(a)] represents the en-\na∼π(.;θ)\n3.3 EnhancingExplorationwithEntropy\ntropy of the policy, which we seek to maximize\nIn general, RL learning approaches aim alongside the critic values, as entropy maximiza-\nto maximize the expected cumulative re- tioniscrucialforencouragingtheactortoexplore\nwards up to a horizon T, expressed as differentactions. Ahighαencouragesexploration,\nmax (cid:80)T E [r(a )]. In contrast, whereasalowervalueofαfavorsexploitation.\nπ(·) t=1 at∼π(·) t\nblack-boxpromptlearning, framedasabest-arm Furthermore,toavoidfine-tuning,andtoachieve\nidentification problem, seeks the best prompt adaptiveexploration,theentropycoefficientαcan\nwithout necessarily maximizing the expected be adjusted dynamically to maintain a target en-\ncumulativerewards. Tomaximizethecumulative tropylevel(Haarnojaetal.,2019),H ,ensuring\ntarget\nrewards, the agent must manage the exploration- sufficientexploration,asfollows:\nexploitationtrade-off. Conversely,forthebest-arm minJ\nα\n≜−E a∼π(.)(cid:2) α·(cid:0) logπ(a;θ)+Htarget(cid:1)(cid:3) . (7)\nidentification problem, the agent should explore α\nasextensivelyaspossiblewithintheT evaluation We use a stochastic approximation of gradients,\nbudget to potentially identify the best action, specificallytheAdamalgorithm(KingmaandBa,\n4\n2017; Reddi et al., 2018), to minimize the losses continuouspromptintoadiscreteformatsuitable\nviastochasticgradientdescent. Thelearningrates forinputintotheblack-boxLLM.\nλ,β,andγ areusedfortheQ-network,thepolicy Step ⃝3 . As depicted in the right side of Fig. 2,\nnetworks,andthetemperature,respectively. for every input x in the validation set V =\ni\n{(x ,y )}m ,thegeneratedpromptτ isconcate-\nj j j=1\n4 Methodology natedtotheinputsentencex andfedtotheblack-\ni\nbox LLM, which generates an output sentence\nThemethodologyofourproposedalgorithm,AC-\nyˆ = f(τ(z)⊕x ). The output of the black-box\ni i\nING,isillustratedinFig.1,showingtheactor-critic\nLLM is fed into a scoring function q(·,·), which\ninteractionwiththeenvironment. InFig.2,theen-\ncomputesthescorebetweenthepredictedoutputyˆ\ni\nvironmentiszoomedin,usingthelarger_animal\nandthetruelabely . Theoverallscoreiscalculated\ni\ndatasetasanexample. Inthefollowing,weprovide\nbyaveragingthescoresacrossallsamples,repre-\nadetailedexplanationofthefullmethodology. sentingthereward: r = 1 (cid:80)m q(yˆ,y ), where\nm i=1 i i\nOverview. In each iteration t ≤ T, the actor- mrepresentsthenumberofsamplesandr serves\ncriticagentgeneratesacontinuousvector“action” asfeedbackfortheactor-criticalgorithm.\na (step 1). This action is then played on the en- Step⃝4 . Thecriticisresponsibleforevaluating\nvironment, which projects a into the appropriate thequalityofactionstakenbytheactorusingthe\nspaceusingafixedprojectionmatrixP toobtainz. networkQ . Thisnetworkestimatestheexpected\nw\nTheenvironmentthenconcatenatestheprojected rewardforagivenactiona,whichisgeneratedby\nvectorzwithasetofexemplars’embeddingsfrom thepolicynetworkπ. Observingtherewardr(a),\nE and feeds it into a white-box LLM h (step 2). thecriticcomputesthelossEq.(4)andupdatesits\nThewhite-boxLLMproducesadiscreteprompt,τ, network using Eq. (5). The critic provides feed-\nwhich is evaluated using the validation dataset V backtotheactorbyestimatingtheQ-valuesofthe\nbased on the responses from the black-box LLM actions sampled by the policy, and the actor uses\nf (step 3). The black-box LLM generates a pre- this feedback to improve its policy by maximiz-\ndiction,whichisthencontrastedtothetruelabels ing Eq. (6). This continuous interaction between\nof the validation examples, and a score function theactorandcriticensurestheactorlearnstotake\nprovidesafinalrewardtothecritic. Thisrewardis actionsthatmaximizetheexpectedrewardwhile\nused to compute the critic’s loss and update both balancingexplorationandexploitation.\nthecriticandactornetworksaccordingly.\nBy repeating steps 1 to 4 until the allowed\nStep ⃝1 . The policy-network (actor) outputs a\nbudget T is exhausted, the agent returns the\nmeanandvarianceofadistributionfromwhichthe best instruction prompt, denoted as τ⋆. This\naction is sampled. Specifically, the action, repre-\nprompt is then evaluated using the test set T\n′\nsentedasasoftpromptvector,a∈Rd ,isobtained on the black-box LLM, yielding the final score\nbysamplingfromaGaussiandistributionwiththe S⋆ = 1 (cid:80)m′ q(f(τ⋆,x′),y′) (using the evalua-\nm′ i=1 i i\noutputtedparameters. Thenetworkalsocomputes tiontemplatedepictedinFig.1bottomright).\ntheassociatedlogprobability,whichiscrucialfor\npolicyoptimization,asitguidesthelearningpro- 5 Experiments\ncessbybalancingexplorationandexploitation,by\ncontrollingthepolicyloss,asshowninEq.(6). We tackle instruction learning for ChatGPT. We\nStep ⃝2 . As depicted in the left side of Fig. 2, conduct instruction induction tasks using 30\ntheexamplesdescribingthetaskfromthesetofex- datasetsfrom(Honovichetal.,2023;Chenetal.,\nemplarsE,alongwithadditionaltextsuchas“The 2024),alongwithadatasummarizationtaskfrom\ninstructionwasto,” areinputintothe embedding (Gliwaetal.,2019). Furthermore,wecomparethe\nlayerofthewhite-boxLLMtogeneratecontinuous best-learned instructions from our ACING algo-\nvectors. Thesecontinuousvectorsarethenconcate- rithm against human-provided instructions from\nnated with the soft prompt z, projected from the (Honovich et al., 2023), as shown in Table 1,\nactiona. Thelayersofthewhite-boxLLMsubse- withadditionalbest-learnedinstructionsinTable\nquentlyprocesstheresultingconcatenatedvectorto 9. Moreover, we compare ACING with three\nproducethediscretepromptτ (usingtheinstruction recent representative baselines for black-box in-\ngeneration template depicted in Fig. 1 top right). struction learning: APE (Zhou et al., 2023), In-\nThistransformationisessentialforconvertingthe structZero(Chenetal.,2024),andINSTINCT(Lin\n5\nActor Exemplars Predictions Score\nInput: tiger, dog Output: tiger giant squid 1\nInput: spider, chamois Output: chamois reindeer\nThe instruction was to\nProjection\nWhite Box LLM Embedding Layers Black Box LLM\nSoft Prompt Embeddings Instruction Validation set\n0.14, 0.5, ... , -1, 0.76 0.57, 1, -0.9 , .... , -0.73, 0.89 The instruction was to create a program that takes Input: giant squid, owl\ntwo animals as input and outputs the animal that\nInput: hamster, reindeer\nis bigger\nRest of White LLM Layers\nFigure2: Illustrationofthepromptgenerationandtestinginsidetheenvironment.\netal.,2024),withresultsprovidedinTable2. theseresultsfromthemainpaperandfocusonlyon\nFollowingpreviousworksandtoensurefairness, taskswherethescoresofhumansandourmethod\nwedonotfine-tunethebudget. Instead,likeprevi- differed,asshowninTable1.\nousworks,weconsiderafixedbudgetofblack-box\nAsshowninthesetables,ourapproachoutper-\nLLM evaluations, set to T = 165 (Chen et al.,\nforms human instructions in several tasks, some-\n2024; Lin et al., 2024). Moreover, like previous\ntimesbyasignificantmargin. Forexample,inthe\nworks,weuseVicuna-13B(Chiangetal.,2023)as\nrhymingtask,ourmethodachieveda0.39improve-\nthewhite-boxLLMh. Furthermore,forprevious\nmentinthetestscore. Thehumaninstructionfor\nworks, weusetheirdefault(tuned)hyperparame-\nthistask—Writeawordthatrhymeswiththeinput\nters,includingtheintrinsicdimensiond′ = 10and\nword.—only resulted in a score of 0.61. In con-\nthenumberofsofttokensN = 5. Forfairness,we\nz\ntrast, the best ACING instruction, which was an\nrefrain from fine-tuning these parameters for our\nintriguingprompt: Inputthewordthattheprogram\nmethodandusethesamevaluesasinpriorworks.\nthoughtIwasinputtingandthenoutputtheword\nThis ensures that our ACING algorithm searches\nthatprogramthoughtIwasinputting, achieveda\ninthesamespace,[0,1]10,andusesthesametotal\nperfectscoreof1.00.\nnumberofqueriestotheblack-boxLLMasAPE,\nInstructZero, and INSTINCT for a fair compari- Furthermore, in the challenging sentence simi-\nson. Foreachalgorithm,afteridentifyingthebest laritytask,whilethehuman-providedinstruction\ninstructionusingthevalidationsetV,weevaluate achieved a score of 0.00, our approach improved\nthediscoveredinstructiononaseparatetestsetT thetestscorebyupto0.21. However,theinstruc-\nandreportthetestaccuracyasthescore. tion yielding the highest score appeared to bias\nWefurtherconductablationstudiesonthekey the prediction towards outputting “3” due to the\ndesign choices for ACING, including the use of followinginstruction: Findasentencepairthatis\nsmalllearningbudgets,providingrewardplotsfor probablynotsimilar,andtheoutputis3-probably,\nvariousselectedtasks. Furthermore,wetesttheim- whichresultedinthebestscore. Thesecond-best\npactofusingdifferentwhite-boxlanguagemodels, instruction,proposedbyACING,achievedanim-\nintrinsic dimensions, and numbers of exemplars. provementof0.14overthehumanscoreandwasas\nMore details on the experiments are provided in follows: Generateaprobabilityscoreforeachpair\ntheAppendix. of sentences, where 1 - probably not means that\nthetwosentencesarenotrelated,0-definitelynot\n5.1 ACINGvs. Humans\nmeansthatthetwosentencesarenotrelated,and3\nWeusehumaninstructionsprovidedin(Honovich -probablymeansthatthetwosentencesarerelated.\netal.,2023)forvariousinstruction-inductiontasks, Overall,ourapproachrecoversseveralhumantest\ntestthem,andcomparetheirscoresagainstthose scores while being fully automatic, and in many\nof our best instructions. The complete list of se- cases surpasses human performance in terms of\nlectedtasksisprovidedinTable1intheAppendix. thenumberofsuccessfultasks,averagescore,and\nSeveral instructions were solved by both human medianscoreacrosstasks. Forthecuriousreader,\nexpertsandourapproach,i.e.,achievingaperfect weprovideallourbestinstructionsalongwiththeir\ntestscoreof1.00. Forclarityandbrevity,weomit testscoresinTable10intheAppendix.\n6\nCategory Task HumanInstruction ACINGInstruction(Ours) Human Our\nScore Score\nSpelling Second_word_letter Extractthesecondletterofthe Outputthefirstletterofeachwordthat 0.96 0.59\ninputword. is a vowel (a, e, i, o, u) Example: In-\nput:yearOutput:eInput:trustOutput:r\nInput:qualificationOutput:uInput:de-\nfendantOutput:e\nSyntax Negation Negatetheinputsentence. Flipthetruthvalueofthestatementsin 0.81 0.82\ntheinput\nLexical Antonyms Writeawordthatmeanstheop- Takeawordandchangeittoitsopposite 0.70 0.82\nSemantics positeoftheinputword.\nSynonyms Write a word with a similar Inputawordthatisasynonymforthe 0.14 0.13\nmeaningtotheinputword. wordthatwasoutput\nPhonetics Rhymes Writeawordthatrhymeswith Inputthewordthattheprogramthought 0.61 1.00\ntheinputword. Iwasinputtingandthenoutputtheword\nthatprogramthoughtIwasinputting\nSemantics Cause_and_effect Find which of the two given Findthesentencethatisthecauseofthe 0.97 0.92\ncauseandeffectsentencesisthe effectinthepairofsentences\ncause.\nStyle Informal_to_formal Rephrasethesentenceinformal Converttheinputintooutputusingthe 0.63 0.5\nlanguage. samewordorderandwiththesamemean-\ning\nMulti- Translation_en-de TranslatethewordintoGerman. Provideatranslationforeachwordinthe 0.81 0.84\nlingual EnglishtextintoGerman\nTranslation_en-es TranslatethewordintoSpanish. TranslatethewordsfromEnglishtoSpan- 0.89 0.88\nish,butInoticedthatsomeofthetransla-\ntionsarenotaccurate\nTranslation_en-fr TranslatethewordintoFrench. CreateaprogramthatwouldtakeanEn- 0.86 0.87\nglishwordasinputandoutputitsFrench\nequivalent\nGLUE Sentiment Determinewhetheramoviere- Classifyeachinputaspositiveornega- 0.89 0.90\nviewispositiveornegative. tivebasedontheassessmentofthecorre-\nspondingmovie\nSentence_similarity Ratethesemanticsimilarityof Findasentencepairthatisprobablynot 0.00 0.21\ntwoinputsentencesonascaleof similar,andtheoutputis3-probably\n0-definitelynotto5-perfectly.\nmedianscore 0.81 0.83\naveragescore 0.69 0.71\n#best-performingtasks 5 7\nTable1: Tasksfromtheinstruction-inductiondatasetswherethehumanandACINGtestscoresdiffered. Foreachtask,we\nprovidethecorrespondinghumaninstructionasproposedin(Honovichetal.,2023)andourbestdiscoveredinstruction.We\ntestedtheseinstructionsonthetestdatasetandreporttheirrespectivescores.\n5.2 ACINGvs. OtherOptimizationMethods details,includingthebestpromptachievedforeach\ntaskandcorrespondingtestscores,areintheAp-\nWe compare our method against recent baselines pendix,Table10. Moreover,wecomparetheper-\non the 30 instruction-induction datasets. Table 2 formance of ACING against other methods on a\npresentstheaveragetestaccuracy(alongwiththe summarizationtaskusingSAMSumdataset(Gliwa\nstandarddeviation)overthreeindependentruns,us- etal.,2019)inTable3,whichshowsthatACING\ningthreedifferentseeds. Foreachseed,weselected outperformstheothermethods.\nthebestinstructionachievedbyeachmethodand\nevaluateditonthetestingdataset. Thetabledemon- Inthefollowingsection,wedemonstratethatthe\nstratesthatourmethod,ACING,outperformsoth- medianscoreofACINGcanbefurtherimproved\nersbyachievingthehighestaccuracyin14outof by 5 percentage points using a simple two-phase\n30tasks,comparedtoINSTINCT,InstructZERO, explorationtechnique(splittingthebudget),result-\nandAPEwhichsucceededin9tasksorlesseach. ingina10-percentage-pointimprovementoverthe\nAdditionally,ACINGachievesthehighestmedian bestbaselineconsidered. Furthermore,inablation\naccuracyacrosstasks,withavalueof0.71,which studies, we show that fine-tuning the intrinsic di-\nis 22percentagepointshigherthanAPE.Further\nmensionsd′\ncanleadtoadditionalimprovements.\n7\nCategory Task APE InstructZero INSTINCT ACING\nSpelling Letters_list 0.59(0.02) 1.00(0.00) 0.99(0.01) 1.00(0.00)\nFirst_word_letter 0.00(0.00) 1.00(0.00) 1.00(0.00) 1.00(0.00)\nSecond_word_letter 0.00(0.00) 0.35(0.09) 0.39(0.28) 0.70(0.15)\nMorpho-Syntax Singular_to_plural 1.00(0.00) 0.99(0.01) 0.95(0.03) 0.95(0.03)\nActive_to_passive 1.00(0.00) 0.98(0.01) 1.00(0.00) 1.00(0.00)\nNegation 0.79(0.00) 0.65(0.10) 0.58(0.22) 0.71(0.06)\nLexicalSemantics Antonyms 0.79(0.02) 0.76(0.00) 0.84(0.01) 0.74(0.01)\nSynonyms 0.14(0.01) 0.22(0.11) 0.19(0.08) 0.13(0.02)\nWord_unscrambling 0.54(0.00) 0.59(0.06) 0.54(0.02) 0.50(0.07)\nPhonetics Rhymes 0.59(0.01) 0.99(0.01) 0.36(0.04) 0.57(0.31)\nNumerical Sum 0.87(0.01) 1.00(0.00) 0.70(0.21) 0.69(0.31)\nDiff 0.00(0.00) 1.00(0.00) 0.93(0.09) 1.00(0.00)\nKnowledge Larger_animal 0.72(0.02) 0.63(0.07) 0.81(0.09) 0.84(0.07)\nPeriodic_elements 0.99(0.01) 0.96(0.03) 1.00(0.00) 0.98(0.00)\nCognitiveTasks Cause_and_effect 0.44(0.09) 0.52(0.09) 0.55(0.11) 0.69(0.15)\nCommon_concept 0.03(0.02) 0.14(0.04) 0.09(0.04) 0.19(0.05)\nObject_counting 0.30(0.02) 0.38(0.06) 0.40(0.12) 0.41(0.03)\nOdd_one_out 0.32(0.02) 0.57(0.02) 0.25(0.18) 0.64(0.00)\nOrthography_starts_with 0.23(0.01) 0.41(0.09) 0.54(0.06) 0.60(0.12)\nTaxonomy_animal 0.02(0.02) 0.67(0.14) 0.85(0.06) 0.71(0.02)\nAuto_categorization 0.31(0.01) 0.29(0.02) 0.07(0.07) 0.29(0.04)\nWord_sorting 0.58(0.01) 0.64(0.05) 0.23(0.20) 0.70(0.03)\nCLUE Sentence_similarity 0.00(0.00) 0.10(0.00) 0.00(0.00) 0.13(0.07)\nSentiment 0.90(0.00) 0.88(0.03) 0.88(0.02) 0.89(0.01)\nTranslation Num_to_verbal 0.13(0.02) 0.99(0.01) 1.00(0.00) 0.99(0.01)\nTranslation_en-de 0.83(0.01) 0.82(0.01) 0.77(0.02) 0.82(0.01)\nTranslation_en-es 0.86(0.01) 0.67(0.24) 0.89(0.00) 0.87(0.02)\nTranslation_en-fr 0.88(0.01) 0.77(0.06) 0.85(0.02) 0.83(0.01)\nStyle Informal_to_formal 0.57(0.01) 0.48(0.02) 0.54(0.09) 0.44(0.05)\nCoding Auto_debugging 0.25(0.00) 0.25(0.00) 0.07(0.07) 0.25(0.00)\nmedianscore 0.49 0.66 0.64 0.71\n#best-performingtasks 9 8 7 14\nTable2:Averagetestperformance(andstandarddeviations)across3randomseedscomparingACINGversusAPE,InstructZero,\nandINSTINCT.Thebottomrowsreportthemedianscoreandtotalnumberofbest-performingtasksforeachmethod.\nMetric APE InstructZero INSTINCT ACING can then be used at test time. In Table 5 in the\nROUGE-1 0.35(0.01) 0.33(0.00) 0.36(0.01) 0.37(0.01)\nAppendix,weshowthatACING,usingthisbudget\nROUGE-2 0.12(0.00) 0.11(0.00) 0.14(0.00) 0.14(0.00)\nROUGE-L 0.25(0.00) 0.24(0.01) 0.27(0.01) 0.28(0.01) splittechniquewithaninitialexplorationbudgetof\n150andaremainingbudgetof15calls(allocated\nTable3: Averagetestperformance(andstandarddevi-\ntouniformlyexplorethetopp = 5prompts,each\nations)forinstructionoptimizationforsummarization\nevaluated k = 3 times), achieves improved me-\ntaskusingSAMSumdataset.\ndianscoresacrosstasks,resultingin10percentage\npointsimprovementoverthebestbaselineconsid-\neredandhighertestaccuracyon15taskscompared\n5.3 SplittingtheBudget\ntoothermethods.\nDuetothestochasticnatureoftheblack-boxLLM,\nthe same instruction may yield different rewards 5.4 AblationStudies\nwhen evaluated by the LLM. For more robust\n5.4.1 UsingSmallerBudgets\nprompt selection, the budget T can be split into\ntwoparts: anactor-criticexplorationphase(repeat- Inthemainpaper,wereportthefinaltestscoreafter\ningsteps1to4),followedbyauniformexploration afixedbudgetof165black-boxLLMcalls,mainly\nphase. Inthelatterphase,thetopppromptsdiscov- following previous work (Lin et al., 2024; Chen\neredbytheactor-criticareevaluatedmultipletimes, etal.,2024),avoidinganypotentialadvantagethat\nk times each, using the black-box LLM. Specifi- could arise from optimizing this number. In the\ncally,thefirstphaseusesT −p·kAPIcalls,while Appendix,weproviderewardplotsfortheACING\nthe second uses the remaining calls. The prompt approach,showingthebestachievedrewardover\nwiththehighestaveragerewardacrossrepetitions the calls. As shown in various plots in Figure 3,\n8\ntheACINGapproachfoundtheoptimalinstruction well,matchingtheperformanceoffiveexemplars\n(achievingarewardof1)withinjustafewblack- across several tasks (e.g., phonetics, summation,\nboxcalls. Sometasksrequiredfewerthan10API morpho-syntax, translation) and sometimes even\ncallstofindtheoptimalinstruction,asinthecaseof outperformingthem. Nonetheless,fiveexemplars\n‘activetopassive’and‘letterslist’,andfewerthan offer an advantage, particularly for more cogni-\n20 calls for tasks like ‘translation’ and ‘diff’. It tivelycomplextasks.\ncanbeseenthatthevastmajorityoftasksachieved\ntheir best reward value within the first 60 to 80 6 RelatedWork\ncalls,demonstratingthatACINGcanevenbeused\nformoreconstrainedbudgets. ManyearlyapproachessuchasAutoPrompt(Shin\netal.,2020),FluentPrompt(Shietal.,2023),and\n5.4.2 UsingaDifferentWhite-boxModel other soft prompt-based methods (Lester et al.,\nInthemainpaper,wepresentresultsusingVicuna 2021;LiandLiang,2021;Zhongetal.,2021)fo-\nas the white-box language model, following the cusedonoptimizingpromptsforwhite-boxLLMs\nsetupofpriorwork. Toevaluatetheimpactofdif- andcannotbedirectlyappliedtoblack-boxLLMs.\nferentwhite-boxmodelsontheperformanceofour Moreover,severaltechniqueshavealsobeenpro-\nmethod, we also tested ACING with WizardLM- posedforoptimizingpromptsforgrey-boxLLMs:\n13B-v1.2 (Xu et al., 2024) compared to Vicuna- BBT(Sunetal.,2022b),BBTv2(Sunetal.,2022a),\n13B-v1.3. TheexperimentsinTable8indicatethat and clip-tuning (Chai et al., 2022) use evolution-\nWizardLM achieved a higher median test score ary algorithms for this purpose. However, these\nacrossalltasksandexcelledinagreaternumberof methodsstillrequireaccesstoinputtokenembed-\ntop-performingtasks,demonstratingthattuningthe dings and output logits, making them unsuitable\nchoiceofthewhite-boxmodelcanimproveresults. forsettingswhereonlyqueryaccessisallowed.\nOther approaches (Li and Liang, 2021; Zhong\n5.4.3 UsingDifferentActionDimensionalities et al., 2021) employ continuous soft prompts\nInthemainpaper,wepresentresultsusingactions by fine-tuning parameters of specific input to-\nwith dimension d′ = 10, following the setup of kens, buttheseoftengeneratehuman-unreadable\npriorwork. ToevaluatetheperformanceofACING prompts and are impractical for API-accessible\nwith different dimensionalities, we conducted an LLMs. Some other methods (Deng et al., 2022;\nexperiment using d′ ∈ {5,10,20,40,100}, keep- Zhangetal.,2023),createdforoptimizingdiscrete\ningotherparametersfixed,forabudgetof165calls. promptsinsteadofcontinuousones,canoptimize\nTheresults,showninTable6,indicatethatwhile discrete tokens but require access to the LLM’s\nthesmallestdimension,d′ = 5,recoveredthebest outputdistribution,whichistypicallyunavailable\nscores for some tasks, it generally has the lowest inblack-boxLLMs. Zhouetal.(2023)proposed\nperformanceacrossmosttasks. Furthermore,both generatingcandidatepromptsthroughresampling\nd′ = 10andd′ = 20yieldsimilarperformancein without specific optimizations, while Guo et al.\ntermsofthenumberofbest-performingtasks,indi- (2024)extendedthismodel-freeapproachbyusing\ncatingalowsensitivitytothisparameter. Forthe evolutionaryalgorithms. However,thesemethods\nlargerdimension,d′ = 40,themethodachievedthe usuallyrequirealargenumberofqueries.\nhighestnumberofbest-performingtasks,showing Instructionoptimizationinblack-boxLLMsre-\nthattuning(increasing)d′ canimproveresults. centlyfocusedonsearchingthroughthespaceof\nsoft prompts, then relying on open-source white-\n5.4.4 UsingaDifferentNumberofExemplars\nbox LLMs as a means to transform these soft\nIn the main paper, we present results using five promptsintodiscreteprompts. InstructZero(Chen\nexemplars, followingthesetupofpriorwork. To etal.,2024)learnsdiscretepromptsforChatGPT\nevaluatetheperformanceofACINGwithasmaller bysearchingthroughthecontinuousspaceofsoft\nnumberofexemplars,weconductedanexperiment prompts using Bayesian optimization, which are\nusing a single exemplar, i.e., |E| = 1. Results thentransformedintodiscretepromptsbyawhite-\nshowninTable7confirmthatusingfiveexemplars boxLLM.INSTINCT(Linetal.,2024),asucces-\ngenerallyyieldhighertestscoresandagreaternum- sortothisapproach,reliesonthesameapproach.\nber of best-performing tasks than a single exem- However,insteadofBayesianoptimization,ituses\nplar. However, a single exemplar still performs NeuralUCB(Zhouetal.,2020),acontextualbandit\n9\n(selection)algorithmthatsearcheswithinafinite References\nset of continuous actions generated using Sobol\nYekunChai,ShuohuanWang,YuSun,HaoTian,Hua\n(Erikssonetal.,2019). Ourapproachcombinesthe Wu,andHaifengWang.2022. Clip-Tuning:Towards\nhybridstrategyofusingwhite-boxandblack-box Derivative-freePromptLearningwithaMixtureof\nLLMswithanactor-criticapproachthatoperates Rewards. InEMNLP.\ninastateless,infinitecontinuous-actionspace.\nJiuhai Chen, Lichang Chen, Heng Huang, and\nTRIPLE(Shietal.,2024)modeledtheprompt Tianyi Zhou. 2023. When do you need Chain-\nlearningproblemasabest-arm-identificationprob- of-Thought Prompting for ChatGPT? Preprint,\narXiv:2304.03262.\nlem. However, itonlyconsidered theproblem as\naselectiontask,assumingagivenpoolofdiscrete Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng\nprompts. In contrast, our method addresses both Huang, and Tianyi Zhou. 2024. InstructZero: Ef-\nthegenerationandselectionofprompts. ficientInstructionOptimizationforBlack-BoxLarge\nLanguageModels. InICML.\n7 Conclusion\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghaoWu,HaoZhang,LianminZheng,Siyuan\nACING presents a promising approach for learn-\nZhuang,YonghaoZhuang,JosephE.Gonzalez,Ion\ning prompts for black-box LLMs by framing it Stoica, andEricP.Xing.2023. Vicuna: AnOpen-\nasastatelesscontinuous-actionRLproblem. Our SourceChatbotImpressingGPT-4with90%*Chat-\nGPTQuality.\nmethod shows significant improvements over re-\ncent baseline methods. Evaluations across 31 MingkaiDeng,JianyuWang,Cheng-PingHsieh,Yihan\ndatasetsrevealthatACINGenhancespromptqual- Wang, Han Guo, Tianmin Shu, Meng Song, Eric\nity, achieving a median score improvement of Xing,andZhitingHu.2022. RLPrompt: Optimizing\nDiscreteTextPromptswithReinforcementLearning.\n10 percentage points and outperforming human-\nInEMNLP.\ncraftedinstructionsbyupto39percentagepoints.\nThissetsanewbenchmarkforautomatedinstruc- DavidEriksson,MichaelPearce,JacobGardner,RyanD\nTurner, and Matthias Poloczek. 2019. Scalable\ntionoptimizationinblack-boxLLMs.\nGlobalOptimizationviaLocalBayesianOptimiza-\ntion. InNeurIPS.\n8 Limitations\nFares Fourati, Vaneet Aggarwal, and Mohamed-Slim\nWhile our proposed results were achieved using Alouini.2024. StochasticQ-learningforLargeDis-\ndefault and commonly used values for hyperpa- creteActionSpaces. InICML.\nrameters in network design, learning rates, and\nBogdanGliwa,IwonaMochol,MaciejBiesek,andAlek-\nfollowing previous works for some common pa-\nsanderWawer.2019. SAMSumCorpus: AHuman-\nrameters, the results presented depend on these annotatedDialogueDatasetforAbstractiveSumma-\nchoices. Theseresultscouldbefurtherimproved rization. InWorkshoponNewFrontiersinSumma-\nrization.\nwith fine-tuning, as demonstrated in the ablation\nstudies (§ 5.4). Another limitation of this work,\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nsimilartopreviousstudies,istheriskofoverfitting BingXu,DavidWarde-Farley,SherjilOzair,Aaron\nwhendealingwithalimitednumberofvalidation Courville, and Yoshua Bengio. 2014. Generative\nAdversarialNets. InNeurIPS.\nexamples. This can result in a prompt that mini-\nmizes the loss on the validation set but does not QingyanGuo,RuiWang,JunliangGuo,BeiLi,Kaitao\ngeneralizewelltothetestdataset,leadingtoadis- Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\njiuYang.2024. ConnectingLargeLanguageMod-\ncrepancy between training and test performance.\nels with Evolutionary Algorithms Yields Powerful\nAnotherlimitationofthiswork,similartoprevious\nPromptOptimizers. InICLR.\nwork (Lin et al., 2024; Chen et al., 2024), is the\nreliance on a white-box model, where the choice Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and\nSergeyLevine.2018. SoftActor-Critic: Off-Policy\nof model may lead to performance variations, as\nMaximum Entropy Deep Reinforcement Learning\ndemonstratedin§5.4.2. Avoidingthislimitationre-\nwithaStochasticActor. InICML.\nquiresactingdirectlyontheblack-boxLLM,which\ninvolveshandlingactionsinalargediscreteaction TuomasHaarnoja,AurickZhou,KristianHartikainen,\nGeorgeTucker,SehoonHa,JieTan,VikashKumar,\nspace. This necessitates adapting current RL ap-\nHenry Zhu, Abhishek Gupta, Pieter Abbeel, and\nproachesforlargediscreteactionspaces,asstudied\nSergeyLevine.2019. SoftActor-CriticAlgorithms\nin(Fouratietal.,2024)andthereferencestherein. andApplications. Preprint,arXiv:1812.05905.\n10\nOr Honovich, Uri Shaham, Samuel R. Bowman, and ChengshuaiShi,KunYang,JingYang,andCongShen.\nOmerLevy.2023. InstructionInduction: FromFew 2024. BestArmIdentificationforPromptLearning\nExamplestoNaturalLanguageTaskDescriptions. In under a Limited Budget. In ICLR 2024 Workshop\nACL. on Mathematical and Empirical Understanding of\nFoundationModels.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam:\nA Method for Stochastic Optimization. Preprint, Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtz-\narXiv:1412.6980. man, Yulia Tsvetkov, and Luke Zettlemoyer. 2023.\nTowardHumanReadablePromptTuning: Kubrick’s\nTor Lattimore and Csaba Szepesvári. 2020. Bandit TheShiningisagoodmovie,andagoodprompttoo?\nAlgorithms. CambridgeUniversityPress.\nInEMNLP.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV,\nBrianLester,RamiAl-Rfou,andNoahConstant.2021.\nEricWallace,andSameerSingh.2020. AutoPrompt:\nThePowerofScaleforParameter-EfficientPrompt\nEliciting Knowledge from Language Models with\nTuning. InEMNLP.\nAutomaticallyGeneratedPrompts. InEMNLP.\nXiangLisaLiandPercyLiang.2021. Prefix-Tuning: Aleksandrs Slivkins. 2019. Introduction to Multi-\nOptimizingContinuousPromptsforGeneration. In Armed Bandits. Foundations and Trends® in Ma-\nACL-IJCNLP. chineLearning,12(1-2).\nXiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Tianxiang Sun, Zhengfu He, Hong Qian, Xuanjing\nWenyangHu,YaoShu,See-KiongNg,PatrickJail- Huang,andXipengQiu.2022a. BBTv2: Towardsa\nlet, and Bryan Kian Hsiang Low. 2024. Use Your Gradient-FreeFuturewithLargeLanguageModels.\nINSTINCT:INSTructionoptimizationforLLMsus- InEMNLP.\nIngNeuralbanditsCoupledwithTransformers. In\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nICML.\nHuang,andXipengQiu.2022b. Black-BoxTuning\nforLanguage-Model-as-a-Service. InICML.\nPengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\nRichardSSuttonandAndrewGBarto.2018. Reinforce-\nTrain,Prompt,andPredict: ASystematicSurveyof\nmentLearning: AnIntroduction,2ndedition. MIT\nPromptingMethodsinNaturalLanguageProcessing.\nPress.\nACMComputingSurveys,55(9).\nHugoTouvron,ThibautLavril,GautierIzacard,Xavier\nSwaroopMishra,DanielKhashabi,ChittaBaral,Yejin Martinet,Marie-AnneLachaux,TimothéeLacroix,\nChoi, and Hannaneh Hajishirzi. 2021. Reframing BaptisteRozière,NamanGoyal,EricHambro,Faisal\nInstructionalPromptstoGPTk’sLanguage. InACL. Azhar,AurelienRodriguez,ArmandJoulin,Edouard\nGrave,andGuillaumeLample.2023. LLaMA:Open\nVolodymyrMnih,AdriaPuigdomenechBadia,Mehdi andEfficientFoundationLanguageModels. Preprint,\nMirza,AlexGraves,TimothyLillicrap,TimHarley, arXiv:2302.13971.\nDavidSilver,andKorayKavukcuoglu.2016. Asyn-\nchronousMethodsforDeepReinforcementLearning. JasonWei,XuezhiWang,DaleSchuurmans,Maarten\nInICML. Bosma,brianichter,FeiXia,EdChi,QuocVLe,and\nDenny Zhou. 2022. Chain-of-Thought Prompting\nOpenAI.2023a. ChatGPT:AConversationalAIModel. Elicits Reasoning in Large Language Models. In\nNeurIPS.\nOpenAI. 2023b. GPT-4 Technical Report. Preprint,\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\narXiv:2303.08774.\nPuZhao,JiazhanFeng,ChongyangTao,andDaxin\nJiang. 2024. WizardLM: Empowering Large Pre-\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nTrained Language Models to Follow Complex In-\nBansal.2023. GrIPS:Gradient-free,Edit-basedIn-\nstructions. InICLR.\nstructionSearchforPromptingLargeLanguageMod-\nels. InEACL.\nTianjunZhang,XuezhiWang,DennyZhou,DaleSchu-\nurmans,andJosephEGonzalez.2023. TEMPERA:\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\nTest-TimePromptEditingviaReinforcementLearn-\nguang Zhu, and Michael Zeng. 2023. Automatic ing. InICLR.\nPromptOptimizationwith“GradientDescent”and\nBeamSearch. InEMNLP. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaoleiWang,YupengHou,YingqianMin,Beichen\nSashankJReddi,SatyenKale,andSanjivKumar.2018. Zhang,JunjieZhang,ZicanDong,YifanDu,Chen\nOntheConvergenceofAdamandBeyond. InICLR. Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nLaria Reynolds and Kyle McDonell. 2021. Prompt Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2024.\nProgrammingforLargeLanguageModels: Beyond A Survey of Large Language Models. Preprint,\ntheFew-ShotParadigm. InCHIEA. arXiv:2303.18223.\n11\nZexuanZhong,DanFriedman,andDanqiChen.2021.\nFactualProbingIs[MASK]:Learningvs.Learning\ntoRecall. InNAACL.\nDongruo Zhou, Lihong Li, and Quanquan Gu. 2020.\nNeuralContextualBanditswithUCB-basedExplo-\nration. InICML.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiranPaster,SilviuPitis,HarrisChan,andJimmy\nBa.2023. LargeLanguageModelsareHuman-Level\nPromptEngineers. InICLR.\nKaijieZhu,JindongWang,JiahengZhou,ZichenWang,\nHaoChen,YidongWang,LinyiYang,WeiYe,Yue\nZhang,NeilZhenqiangGong,andXingXie.2024.\nPromptRobust: TowardsEvaluatingtheRobustness\nofLargeLanguageModelsonAdversarialPrompts.\nPreprint,arXiv:2306.04528.\nBrianDZiebart.2010. ModelingPurposefulAdaptive\nBehaviorwiththePrincipleofMaximumCausalEn-\ntropy. Ph.D.thesis,CarnegieMellonUniversity.\n12\nA ExperimentalDetails\nA.1 HyperparametersRemark\nAcrossthediversetasks,inthemainpaper,thesamehyperparameterswereused,whichshowsthatthe\nalgorithmgeneralizeswellacrossthe30taskswithoutspecificallytuninghyperparametersineachtask. A\nsummaryofthekeyparameterscabbefoundinthefollowingTable. AnimplementationofACINGis\navailableathttps://github.com/salmakh1/ACING.\nHyperparameter Choice\nWhite-boxh Vicuna-13B\nActor-network (1−1024−256−10)\nCritic-network (10−128−128−1)\nBudgetT 165\nIntrinsic(action)dimensiond′ 5\nNumberofsofttokensN 5\nz\nSoftpromptdimensiond 5120*N\nz\nNumberofexemplars|E| 5\nNumberoftokensgeneratedbyWb 64\nTable4: Keyhyperparametersandtheirvalues.\nA.2 Actor-CriticDetails\nAcrossallthetasks,weusedthreefully-connectedlayersforboththeactor(1−1024−256−10)and\nthecritics(10−128−128−1)networks,withlearningratesfixedat3·10−4 foreach. Weusedtwo\ncriticnetworksandconsidertheirminimumastheactualcritic. Welearntheentropyparameterαusinga\nlearningrateof9·10−4.\nB ACINGRewardsoverthe(Calls)Steps\nInthemainpaper,wereportthefinaltestscoreafterafixedbudgetof165black-boxLLMcalls. Inthis\nsection,weproviderewardplotsfortheACINGapproach,showingthebest-achievedrewardwithinthe\nconductedcalls. AsshowninvariousplotsinFigure3,theACINGapproachfoundtheoptimalprompt\n(achievingarewardof1)withinjustafewblack-boxcalls. Sometasksrequiredfewerthan10APIcalls\ntofindtheoptimalinstruction,suchasfor‘activetopassive’and‘letterslist’,andfewerthan20fortasks\nlike‘translation’and‘diff’. Itcanbeseenthatthevastmajorityoftasksachievedtheirbestrewardvalue\nwithinthefirst60to80calls,demonstratingthatACINGcanevenbeusedformuchmoreconstrained\nbudgets. Thechoiceof165callswasmainlybasedonpreviouswork(Linetal.,2024;Chenetal.,2024),\navoidinganypotentialadvantagethatcouldcomefromoptimizingthisnumber.\n13\nActive to Passive Translation Eng Es Larger Animal\n1.0 1.00 1.0\n0.8 0.99 0.8\n0.6 0.98 0.6\n0.4 0.97 0.4\n0.2 0.96 0.2\n0.0 0.95\n0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140\nIndex Index Index\nLetters List Auto Categorization Antonyms\n1.0 0.20\n0.18 0.8\n0.8 0.16\n0.6\n0.6 0.14\n0.12 0.4 0.4\n0.10\n0.2 0.08 0.2\n0.06\n0.0 0.0\n0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140\nIndex Index Index\nWord Unscrambling Rhymes Diff\n0.8 1.0\n0.6\n0.7 0.9\n0.6 0.5 0.8\n0.5 0.4 0.7\n0.4 0.3 0.6\n0.3 0.5\n0.2\n0.2 0.4\n0.1 0.1 0.3\n0.0 0.0 0.2\n0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140\nIndex Index Index\nPeriodic Elements Cause and Effect Common Concept\n1.0 0.7\n0.6 0.150\n0.8 0.5 0.125\n0.6 0.4 0.100\n0.3 0.075\n0.4\n0.2 0.050\n0.2 0.1 0.025\n0.0 0.000\n0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140\nIndex Index Index\nObject Counting Informal to Formal Auto Debugging\n0.6 0.6\n0.7\n0.6 0.5 0.5\n0.5 0.4\n0.4\n0.4 0.3\n0.3 0.3\n0.2\n0.2 0.2\n0.1 0.1\n0.1\n0.0 0.0\n0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140\nIndex Index Index\nNegation Orthography Starts With Sentiment\n1.0\n0.8\n0.8\n0.8\n0.6 0.6\n0.6\n0.4 0.4 0.4\n0.2 0.2 0.2\n0.0 0.0 0.0\n0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140 0 20 40 60 80 100 120 140\nIndex Index Index\nFigure3: RewardplotsforrunningACINGonvariousselectedtasks,showingthehighestachievedrewardonthe\ny-axisuntileachAPIcall(step),withthex-axisrepresentingthenumberofAPIcalls.\n14\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\ndraweR\ntseB\nC ACINGwithBudgetSplitting\nDue to the stochastic nature of the black-box LLM, the same instruction may yield different rewards\nwhenevaluatedbytheLLM.Toaddressthis,weaddamechanismformorerobustdecision-making. The\nbudgetT issplitintotwoparts: anexplorationphasewheresteps1to4arerepeated,andanexploitation\nphasewherethebestppromptsareevaluatedmultipletimes,ktimeseach,usingtheblack-boxLLM.The\nexplorationphaseusesT −p·k APIcalls,withtheremainingcallsusedforexploitation. Finally,the\npromptwiththehighestaveragerewardacrossrepetitionsisusedattesttime. InTable5,wedemonstrate\nthat ACING, with an exploration budget of T = 150 and the remaining 15 calls allocated to uniform\nexplorationofthetopp = 5prompts(evaluatedk = 3timeseach), achievesimprovedmedianscores\nacrosstasksandhighertestaccuracyon15taskscomparedtopreviouswork,unlikewithoutthesplitting\ntechniquewhichachievedhighertestaccuracyon14taskscomparedtothepreviouswork.\nCategory Task APE InstructZero INSTINCT ACING150+15 ACING165\n(budgetsplitting) (mainpaper)\nSpelling Letters_list 0.59(0.02) 1.00(0.00) 0.99(0.01) 1.00(0.00) 1.00(0.00)\nFirst_word_letter 0.00(0.00) 1.00(0.00) 1.00(0.00) 1.00(0.00) 1.00(0.00)\nSecond_word_letter 0.00(0.00) 0.35(0.09) 0.39(0.28) 0.40(0.17) 0.70(0.15)\nMorpho-Syntax Singular_to_plural 1.00(0.00) 0.99(0.01) 0.95(0.03) 0.99(0.01) 0.95(0.03)\nActive_to_passive 1.00(0.00) 0.98(0.01) 1.00(0.00) 1.00(0.00) 1.00(0.00)\nNegation 0.79(0.00) 0.65(0.10) 0.58(0.22) 0.82(0.00) 0.71(0.06)\nLexicalSemantics Antonyms 0.79(0.02) 0.76(0.00) 0.84(0.01) 0.76(0.06) 0.74(0.01)\nSynonyms 0.14(0.01) 0.22(0.11) 0.19(0.08) 0.12(0.02) 0.13(0.02)\nWord_unscrambling 0.54(0.00) 0.59(0.06) 0.54(0.02) 0.59(0.05) 0.50(0.07)\nPhonetics Rhymes 0.59(0.01) 0.99(0.01) 0.36(0.04) 0.60(0.38) 0.57(0.31)\nNumerical Sum 0.87(0.01) 1.00(0.00) 0.70(0.21) 0.81(0.30) 0.69(0.31)\nDiff 0.00(0.00) 1.00(0.00) 0.93(0.09) 0.97(0.04) 1.00(0.00)\nKnowledge Larger_animal 0.72(0.02) 0.63(0.07) 0.81(0.09) 0.86(0.06) 0.84(0.07)\nPeriodic_elements 0.99(0.01) 0.96(0.03) 1.00(0.00) 0.98(0.03) 0.98(0.00)\nCognitiveTasks Cause_and_effect 0.44(0.09) 0.52(0.09) 0.55(0.11) 0.76(0.18) 0.69(0.15)\nCommon_concept 0.03(0.02) 0.14(0.04) 0.09(0.04) 0.10(0.01) 0.19(0.05)\nObject_counting 0.30(0.02) 0.38(0.06) 0.40(0.12) 0.48(0.11) 0.41(0.03)\nOdd_one_out 0.32(0.02) 0.57(0.02) 0.25(0.18) 0.59(0.05) 0.64(0.00)\nOrthography_starts_with 0.23(0.01) 0.41(0.09) 0.54(0.06) 0.54(0.15) 0.60(0.12)\nTaxonomy_animal 0.02(0.02) 0.67(0.14) 0.85(0.06) 0.53(0.34) 0.71(0.02)\nAuto_categorization 0.31(0.01) 0.29(0.02) 0.07(0.07) 0.27(0.06) 0.29(0.04)\nWord_sorting 0.58(0.01) 0.64(0.05) 0.23(0.20) 0.72(0.02) 0.70(0.03)\nCLUE Sentence_similarity 0.00(0.00) 0.10(0.00) 0.00(0.00) 0.13(0.08) 0.13(0.07)\nSentiment 0.90(0.00) 0.88(0.03) 0.88(0.02) 0.88(0.03) 0.89(0.01)\nTranslation Num_to_verbal 0.13(0.02) 0.99(0.01) 1.00(0.00) 1.00(0.00) 0.99(0.01)\nTranslation_en-de 0.83(0.01) 0.82(0.01) 0.77(0.02) 0.82(0.01) 0.82(0.01)\nTranslation_en-es 0.86(0.01) 0.67(0.24) 0.89(0.00) 0.86(0.02) 0.87(0.02)\nTranslation_en-fr 0.88(0.01) 0.77(0.06) 0.85(0.02) 0.85(0.02) 0.83(0.01)\nStyle Informal_to_formal 0.57(0.01) 0.48(0.02) 0.54(0.09) 0.44(0.05) 0.44(0.05)\nCoding Auto_debugging 0.25(0.00) 0.25(0.00) 0.07(0.07) 0.29(0.07) 0.25(0.00)\nmedianscore 0.49 0.66 0.64 0.76 0.71\n#best-performingtasks 7 8 8 15 14\nTable5:Averagetestperformance(andstandarddeviations)across3randomseedscomparingACINGversusAPE,InstructZero,\nandINSTINCT.Thebottomrowsreportthemedianscoreandtotalnumberofbest-performingtasksforeachmethod.\n15\nD ACINGwithDifferentIntrinsic(Action)Dimensions\nIn the main paper, we present results using actions with a dimension of d′ = 10, following the setup\nofpriorwork. ToevaluatetheperformanceofACINGacrossdifferentdimensionalities,weconducted\nexperiments with d′ ∈ {5,10,20,40}, keeping other parameters fixed, for a budget of 165. We report\nthetestresultsoverdifferenttasksanddimensionalitiesforafixedseed. Theresults,showninTable6,\nindicatethatwhilethesmallestdimension,d′ = 5,recoveredthebestscoresforsometasks,itgenerally\nhas the lowest performance across most tasks. Furthermore, both d′ = 10 and d′ = 20 yield similar\nperformanceintermsofthenumberofbest-performingtasks(9-10tasks),indicatinglowsensitivityto\nthis parameter. For the much larger dimension, d′ = 40, the method achieved the highest number of\nbest-performingtasks(15tasks),demonstratingimprovedperformancewithincreaseddimensionality.\nFurtherincreasingthedimensionalitytod′ = 100canstillyieldhighresults,outperformingd′ ∈ 5,10,20.\nHowever,whileitremarkablyoutperformedd′ = 40insometasks,suchasthesecondwordlettertask,\nsynonyms,andantonyms,itonlyachieved14best-performingtasksoverall,indicatingsimilarbutslightly\nlowerperformancethand′ = 40.\nCategory Task d′ =5 d′ =10 d′ =20 d′ =40 d′ =100\n(mainpaper)\nSpelling Letters_list 1.00 1.00 0.98 1.00 1.00\nFirst_word_letter 1.00 1.00 0.97 1.00 1.00\nSecond_word_letter 0.23 0.91 0.30 0.29 0.92\nMorpho-Syntax Singular_to_plural 0.99 0.99 1.00 1.00 1.00\nActive_to_passive 1.00 1.00 1.00 1.00 1.00\nNegation 0.82 0.80 0.84 0.81 0.70\nLexicalSemantics Antonyms 0.73 0.76 0.76 0.82 0.84\nSynonyms 0.12 0.13 0.14 0.14 0.34\nWord_unscrambling 0.53 0.54 0.49 0.55 0.43\nPhonetics Rhymes 0.95 0.36 0.94 1.00 1.00\nNumerical Sum 0.99 0.79 1.00 0.99 1.00\nDiff 0.89 1.00 1.00 1.00 1.00\nKnowledge Larger_animal 0.79 0.94 0.93 0.65 0.68\nPeriodic_elements 1.00 0.98 0.94 0.98 0.98\nCognitiveTasks Cause_and_effect 0.64 0.52 0.92 0.56 0.56\nCommon_concept 0.12 0.23 0.11 0.12 0.02\nObject_counting 0.51 0.39 0.48 0.59 0.44\nOdd_one_out 0.60 0.64 0.64 0.68 0.26\nOrthography_starts_with 0.11 0.65 0.59 0.61 0.71\nTaxonomy_animal 0.79 0.68 0.59 0.85 0.97\nAuto_categorization 0.30 0.28 0.13 0.33 0.32\nWord_sorting 0.55 0.69 0.74 0.69 0.48\nCLUE Sentence_similarity 0.00 0.21 0.00 0.14 0.07\nSentiment 0.91 0.88 0.86 0.91 0.80\nTranslation Num_to_verbal 0.99 1.00 1.00 1.00 1.00\nTranslation_en-de 0.83 0.81 0.81 0.80 0.81\nTranslation_en-es 0.89 0.90 0.91 0.91 0.86\nTranslation_en-fr 0.84 0.84 0.86 0.88 0.73\nStyle Informal_to_formal 0.54 0.40 0.51 0.49 0.50\nCoding Auto_debugging 0.25 0.25 0.25 0.25 0.25\n#best-performingtasks 8 9 10 15 14\nTable6: AverageACINGtestperformanceforafixedrandomseed(0)withdifferentsoftpromptdimensionsd′ .Thebottom\nrowreportthetotalnumberofbest-performingtasks.\n16\nE ACINGwithDifferentNumberofExemplars\nInthissection,wetestACINGwithasingleexemplar,incontrasttothemainresultsinthepaper,which\nusefiveexemplarsforACINGandallotherbenchmarks. Fortheseexperiments,wefixallhyperparameters\nas in the main paper and run tests with a budget of 165. Intuitively, providing more exemplars to the\nlanguagemodelshouldfacilitatepromptlearning,sofiveexemplarsareexpectedtoyieldbetterprompts\nthanasingleexemplar. Ourexperiments,summarizedinTable7,supportthisintuition. Theresultsshow\nthatusingfiveexemplarsleadstohighertestscores,asreflectedinagreaternumberofbest-performing\ntasksandanincreaseinmediantestscoresacrosstasks. However,itisnotablethatperformancedidnot\ndecrease drastically with only one exemplar, suggesting that a single exemplar is sufficient to achieve\ndecentresults. Infact,acrossseveraltasksandcategories(e.g.,phonetics,summation,morpho-syntax,\nand translation), a single exemplar achieves the same performance of using five exemplars, and even\noutperforms the use of five exemplars in certain tasks. Nevertheless, using a single exemplar resulted\ninlowerperformancemainlyinmorecognitivelychallengingtasks,whichisunderstandable,asmore\ncomplextasksarelikelytobenefitfromadditionalexemplars.\nCategory Task ACING(|E|=1) ACING(|E|=5)\n(mainpaper)\nSpelling Letters_list 1.00(0.00) 1.00(0.00)\nFirst_word_letter 0.99(0.01) 1.00(0.00)\nSecond_word_letter 0.19(0.09) 0.70(0.15)\nMorpho-Syntax Singular_to_plural 1.00(0.00) 0.95(0.03)\nActive_to_passive 1.00(0.00) 1.00(0.00)\nNegation 0.76(0.08) 0.71(0.06)\nLexicalSemantics Antonyms 0.78(0.05) 0.74(0.01)\nSynonyms 0.09(0.03) 0.13(0.02)\nWord_unscrambling 0.41(0.09) 0.50(0.07)\nPhonetics Rhymes 0.89(0.08) 0.57(0.31)\nNumerical Sum 0.99(0.01) 0.69(0.31)\nDiff 0.99(0.01) 1.00(0.00)\nKnowledge Larger_animal 0.63(0.17) 0.84(0.07)\nPeriodic_elements 0.91(0.08) 0.98(0.00)\nCognitiveTasks Cause_and_effect 0.51(0.08) 0.69(0.15)\nCommon_concept 0.16(0.11) 0.19(0.05)\nObject_counting 0.26(0.06) 0.41(0.03)\nOdd_one_out 0.64(0.02) 0.64(0.00)\nOrthography_starts_with 0.06(0.05) 0.60(0.12)\nTaxonomy_animal 0.63(0.06) 0.71(0.02)\nAuto_categorization 0.01(0.01) 0.29(0.04)\nWord_sorting 0.70(0.02) 0.70(0.03)\nCLUE Sentence_similarity 0.07(0.05) 0.13(0.07)\nSentiment 0.70(0.12) 0.89(0.01)\nTranslation Num_to_verbal 1.00(0.00) 0.99(0.01)\nTranslation_en-de 0.72(0.11) 0.82(0.01)\nTranslation_en-es 0.88(0.00) 0.87(0.02)\nTranslation_en-fr 0.16(0.04) 0.83(0.01)\nStyle Informal_to_formal 0.42(0.05) 0.44(0.05)\nCoding Auto_debugging 0.25(0.00) 0.25(0.00)\nmedianscore 0.67 0.71\n#best-performingtasks 12 23\nTable7: AverageACINGtestperformance(andstandarddeviations)across3randomseedscomparing1exemplarversus5\nexemplars.Thebottomrowsreportthemedianscoreandtotalnumberofbest-performingtasks.\n17\nF ACINGwithDifferentWhite-boxmodels\nInthissection,weevaluatetheimpactofthechoiceofwhite-boxmodelontheACINGmethod. Specifi-\ncally,weappliedACINGforinstructionlearningwithaGPT-3.5-turboastheblack-boxLLM(asinthe\nmainpaper),butusingdifferentwhite-boxLLMs. Inthemainpaper,wereportedACINGwithVicuna-\n13B-v1.3;inTable8,wefurthertestitwithWizardLM-13B-v1.2. Asshowninthetable,changingthe\nwhite-box LLM results in slight variations in performance. WizardLM achieved a higher median test\nscoreacrossalltasksandexcelledinagreaternumberoftop-performingtasks.\nCategory Task ACING(Vicuna) ACING(WizardLM)\n(mainpaper)\nSpelling Letters_list 1.00(0.00) 1.00(0.00)\nFirst_word_letter 1.00(0.00) 1.00(0.00)\nSecond_word_letter 0.70(0.15) 0.36(0.18)\nMorpho-Syntax Singular_to_plural 0.95(0.03) 0.99(0.00)\nActive_to_passive 1.00(0.00) 1.00(0.00)\nNegation 0.71(0.06) 0.83(0.00)\nLexicalSemantics Antonyms 0.74(0.01) 0.81(0.02)\nSynonyms 0.13(0.02) 0.12(0.03)\nWord_unscrambling 0.50(0.07) 0.57(0.05)\nPhonetics Rhymes 0.57(0.31) 0.97(0.04)\nNumerical Sum 0.69(0.31) 1.00(0.00)\nDiff 1.00(0.00) 1.00(0.00)\nKnowledge Larger_animal 0.84(0.07) 0.94(0.01)\nPeriodic_elements 0.98(0.00) 0.97(0.02)\nCognitiveTasks Cause_and_effect 0.69(0.15) 0.76(0.20)\nCommon_concept 0.19(0.05) 0.21(0.05)\nObject_counting 0.41(0.03) 0.46(0.07)\nOdd_one_out 0.64(0.00) 0.56(0.11)\nOrthography_starts_with 0.60(0.12) 0.62(0.03)\nTaxonomy_animal 0.71(0.02) 0.60(0.32)\nAuto_categorization 0.29(0.04) 0.35(0.03)\nWord_sorting 0.70(0.03) 0.61(0.02)\nCLUE Sentence_similarity 0.13(0.07) 0.22(0.04)\nSentiment 0.89(0.01) 0.90(0.02)\nTranslation Num_to_verbal 0.99(0.01) 1.00(0.00)\nTranslation_en-de 0.82(0.01) 0.81(0.01)\nTranslation_en-es 0.87(0.02) 0.61(0.38)\nTranslation_en-fr 0.83(0.01) 0.83(0.05)\nStyle Informal_to_formal 0.44(0.05) 0.32(0.19)\nCoding Auto_debugging 0.25(0.00) 0.38(0.10)\nmedianscore 0.71 0.79\n#best-performingtasks 14 21\nTable8: AverageACINGtestperformance(andstandarddeviations)across3randomseedsusingVicunaandWizardLMas\nwhite-boxmodels.Thebottomrowsreportthemedianscoreandtotalnumberofbest-performingtasks.\n18\nG DemonstrationswithHumanInstructions\nCategory Task Demonstration HumanInstruction(Honovichetal.,2023) Human Our\nScore Score\nSpelling First_word_letter cat→c Extractthefirstletteroftheinputword. 1.00 1.00\nSecond_word_letter cat→a Extractthesecondletteroftheinputword. 0.96 0.59\nLetters_list cat→cat Breaktheinputwordintoletters,separated 1.00 1.00\nbyspaces.\nMorpho- Singular_to_plural cat→cats Converttheinputwordtoitspluralform. 1.00 1.00\nsyntax\nActive_to_passive Theartistintroducedthe Writetheinputsentenceinpassiveform. 1.00 1.00\nscientist. →Thescien-\ntist was introduced by\ntheartist.\nSyntax Negation Timeisfinite→Timeis Negatetheinputsentence. 0.81 0.82\nnotfinite.\nLexical Antonyms won→lost Writeawordthatmeanstheoppositeofthe 0.70 0.82\nSemantics inputword.\nSynonyms alleged→supposed Writeawordwithasimilarmeaningtothe 0.14 0.13\ninputword.\nPhonetics Rhymes sing→ring Writeawordthatrhymeswiththeinputword. 0.61 1.00\nKnowledge Larger_animal koala,snail→koala Writethelargerofthetwogivenanimals. 0.94 0.94\nSemantics Cause_and_effect Sentence 1: The soda Findwhichofthetwogivencauseandeffect 0.97 0.92\nwent flat. Sentence 2: sentencesisthecause.\nThebottlewasleftopen.\n→ The bottle was left\nopen.\nCommon_concept guitars,pendulums,neu- Findacommoncharacteristicforthegiven 0.11 0.11\ntrinos→involveoscilla- objects.\ntions.\nStyle Informal_to_formal Please call once you Rephrasethesentenceinformallanguage. 0.63 0.5\ngetthere→Pleasecall\nuponyourarrival.\nNumerical Sum 2210→32 Sumthetwogivennumbers. 1.00 1.00\nDiff 3222→10 Subtractthesecondnumberfromthefirst. 1.00 1.00\nNum_to_Verbal 26→twenty-six WritethenumberinEnglishwords. 1.00 1.00\nMulti- Translation_en-de game→Spiel TranslatethewordintoGerman. 0.81 0.84\nlingual\nTranslation_en-es game→juego TranslatethewordintoSpanish. 0.89 0.88\nTranslation_en-fr game→jeu TranslatethewordintoFrench. 0.86 0.87\nGLUE Sentiment The film is small in Determinewhetheramoviereviewispositive 0.89 0.90\nscope, yet perfectly ornegative.\nformed.→positive\nSentence_similarity Sentence 1: A man is Ratethesemanticsimilarityoftwoinputsen- 0.00 0.21\nsmoking.Sentence2:A tencesonascaleof0-definitelynotto5-\nman is skating. → 0 - perfectly.\ndefinitelynot\nmedianscore 0.89 0.90\naveragescore 0.78 0.79\n#best-performingtasks 14 16\nTable 9: Classified tasks into categories from the instruction-induction datasets. For each task, we provide a\ncorresponding demonstration, with → separating the input from the output, along with its respective human\ninstructionasproposedin(Honovichetal.,2023). Wetestedtheseinstructions,reporttheirtestscores,andcompare\nthemtoourbesttestscoresusingACINGwithVicuna-13Basthewhite-boxLLM.\n19\nH OurBestLearnedInstructions\nTask Bestinstruction TestScore\nactive_to_passive Changetheinputtomatchtheoutput,buttheoutputisalreadyinthe 1.00\npassivevoice\nantonyms Takeawordandchangeittoitsopposite 0.82\nauto_categorization Matchtheinputtotheoutput, andtheansweris:Input: NatureNan- 0.34\notechnology,AnnualReviewofBiochemistry,andTheLancetNeurology\nOutput:topjournalsInput:Jeans,Tops,andSuitsOutput:Apparel\nauto_debugging InputthecodeintoaPythoninterpreterandobservetheoutput 0.375\ncause_and_effect Findthesentencethatisthecauseoftheeffectinthepairofsentences 0.92\ncommon_concept Makeaconnectionbetweentheinputandoutput,buttheconnectionis 0.11\nnotclear\ndiff Findthedifferencebetweenthetwonumbers 1.00\nfirst_word_letter Createafunctionthattakesastringasinputandreturnsthefirstletterof 1.00\nthefirstwordinthestring\ninformal_to_formal Converttheinputintooutputusingthesamewordorderandwiththe 0.50\nsamemeaning\nlarger_animal Createaprogramthattakestwoanimalsasinputandoutputstheanimal 0.94\nthatisbigger\nletters_list Inputtheword\"year\"andtheoutputwas\"year\" 1.00\nnegation Flipthetruthvalueofthestatementsintheinput 0.82\nnum_to_verbal Convertnumberstowords 1.00\nobject_counting Provideanumberthatrepresentshowmanyitemsareintheinput 0.55\nodd_one_out Findthewordthatdoesnotbelongineachgroupbasedonthegiven 0.64\nwords\northography_starts_with Findawordinthetextthatstartswiththeletterprovidedandtooutput 0.71\nthatword\nperiodic_elements Findthenameoftheelementbasedonitsatomicnumber 1.00\nrhymes InputthewordthattheprogramthoughtIwasinputtingandthenoutput 1.00\nthewordthatprogramthoughtIwasinputting\nsecond_word_letter Outputthefirstletterofeachwordthatisavowel(a,e,i,o,u)Example: 0.59\nInput:yearOutput:eInput:trustOutput:rInput:qualificationOutput:\nuInput:defendantOutput:\nsentence_similarity Findasentencepairthatisprobablynotsimilar,andtheoutputis3- 0.21\nprobably\nsentiment Classifyeachinputaspositiveornegativebasedontheassessmentof 0.90\nthecorrespondingmovie\nsingular_to_plural Addthesuffix-stotheendofthewordtomakeitplural 1.00\nsum Findthesumofthetwonumbers 1.00\nsynonyms Inputawordthatisasynonymforthewordthatwasoutput 0.13\ntaxonomy_animal MaketheAIgenerateasequenceofanimalsbasedontheinputprovided 0.75\ntranslation_en-de ProvideatranslationforeachwordintheEnglishtextintoGerman 0.84\ntranslation_en-es TranslatethewordsfromEnglishtoSpanish,butInoticedthatsomeof 0.88\nthetranslationsarenotaccurate\ntranslation_en-fr CreateaprogramthatwouldtakeanEnglishwordasinputandoutput 0.87\nitsFrenchequivalent\nword_sorting Outputthewordsinalphabeticalorder,buttheoutputisnotinalphabeti- 0.73\ncalorder\nword_unscrambling ConverttheinputtoawordthatisacommonEnglishword 0.62\nTable10:ThebestinstructiondiscoveredbyACINGforallthe30instruction-inductiontasksusingwithVicuna-13B\nasthewhite-boxLLM.\n20",
    "pdf_filename": "ACING_Actor-Critic_for_Instruction_Learning_in_Black-Box_Large_Language_Models.pdf"
}