{
    "title": "Lifelong and Continual Learning Dialogue Systems",
    "context": "",
    "body": "This book draft is partial and contains only first two chapters. The full\nversion of the book has been published and is now available on Web.\nFor citation, please use the following published version:\nSahisnu Mazumder and Bing Liu. Lifelong and Continual\nLearning Dialogue Systems. Springer Nature, 2024.\nDraft Version\nLifelong and Continual Learning Dialogue Systems\nSahisnu Mazumder\nBing Liu\nNovember 11, 2022\narXiv:2211.06553v2  [cs.CL]  17 Jun 2024\n\nAuthors’ Biographies\nSahisnu Mazumder\nWeb: https://sahisnu.github.io/\nE-mail: sahisnumazumder@gmail.com\nSahisnu Mazumder is an AI Research Scientist at Intel Labs, Santa Clara, CA, USA where he\nworks on Human-AI collaboration and dialogue & interaction systems research. He obtained his\nPh.D. in Computer Science at the University of Illinois at Chicago (UIC), USA and his Masters\nin Computer Science from the Indian Institute of Technology (IIT) - Roorkee, India. His research\ninterests include Lifelong and Continual Learning, Dialogue and Interactive Systems, Open-world AI\n/ Learning, Knowledge base Reasoning, and Sentiment Analysis. He has published several research\npapers in leading AI, NLP and Dialogue conferences like AAAI, IJCAI, ACL, EMNLP, NAACL,\nSIGDIAL, CIKM; delivered tutorials in SIGIR-2022, IJCAI-2021 and Big Data Analytics (BDA -\n2014) and served as PC Member / Reviewer of premier conferences like AAAI, IJCAI, ACL, EMNLP,\nNAACL, EACL, COLING and journals like ACM TALLIP and IEEE TNNLS. He has also worked\nas a Summer Research Intern at Huawei Research USA on projects related to user activity & interest\nmining and at Microsoft Research - Redmond on Natural Language Interaction (NLI) system design.\nBing Liu\nWeb: https://www.cs.uic.edu/~liub/\nE-mail: liub@uic.edu\nBing Liu is a Distinguished Professor of Computer Science at the University of Illinois at Chicago.\nHe received his Ph.D. in Artificial Intelligence from the University of Edinburgh. His current re-\nsearch interests include lifelong and continual learning, lifelong learning dialogue systems, open-world\nlearning, sentiment analysis and opinion mining, machine learning and natural language processing.\nHis previous interests also include fake review detection, Web mining and data mining. He has\npublished extensively in top conferences and journals in these areas and authored four books: one\nabout lifelong/continual machine learning, two about sentiment analysis, and one about Web min-\ning. Three of his papers have received Test-of-Time awards and another one received Test-of-Time\nhonorable mention. Some of his works have been widely reported in popular and technology press\ninternationally. He served as the Chair of ACM SIGKDD from 2013-2017, as program chair of many\nleading data mining conferences including KDD, ICDM, CIKM, WSDM, SDM, and PAKDD, and\nas associate editor of many leading journals such as TKDE, TKDD, TWEB, and DMKD. He is the\nwinner of 2018 ACM SIGKDD Innovation Award, and is a Fellow of AAAI, ACM, and IEEE.\ni\n\nAbstract\nDialogue systems, commonly known as chatbots, have gained escalating popularity in re-\ncent times due to their wide-spread applications in carrying out chit-chat conversations\nwith users and task-oriented dialogues to accomplish various user tasks. Existing chatbots\nare usually trained from pre-collected and manually-labeled data and/or written with hand-\ncrafted rules. Many also use manually-compiled knowledge bases (KBs). Their ability to\nunderstand natural language is still limited, and they tend to produce many errors result-\ning in poor user satisfaction. Typically, they need to be constantly improved by engineers\nwith more labeled data and more manually compiled knowledge. This book introduces the\nnew paradigm of lifelong learning dialogue systems to endow chatbots the ability to learn\ncontinually by themselves through their own self-initiated interactions with their users and\nworking environments to improve themselves. As the systems chat more and more with\nusers or learn more and more from external sources, they become more and more knowl-\nedgeable and better and better at conversing. The book presents the latest developments\nand techniques for building such continual learning dialogue systems that continuously learn\nnew language expressions and lexical and factual knowledge during conversation from users\nand off conversation from external sources, acquire new training examples during conver-\nsation, and learn conversational skills. Apart from these general topics, existing works on\ncontinual learning of some specific aspects of dialogue systems are also surveyed. The book\nconcludes with a discussion of open challenges for future research.\nKeywords: Lifelong Machine Learning; Lifelong Learning; Continual Learning; Di-\nalogue Systems; Interactive Systems; Chatbots; Conversational AI ; Virtual Assistants;\nLearning during Conversation; Learning after Model Deployment; Open-world Learning.\nii\n\nPreface\nThe purpose in writing this book is to introduce the emerging topic of lifelong/continual\nlearning dialogue systems. Dialogue systems, also commonly known as chatbots, are com-\nputer programs that can converse with humans to perform some intended tasks.\nThey\ntypically employ text or speech modes. As deep learning has improved accuracy of both\nspeech recognition and text generation and processing dramatically since 2012 or so, dia-\nlogue systems are becoming increasingly ubiquitous and are used in all types of applications\nsuch as in smart phones, cars, home appliances, company web sites, mobile robots, etc. They\nprovide a very rich set of services, e.g., performing some specific tasks and chit-chatting with\nhuman users. However, the user experiences have not been very satisfactory so far. It is\nclearly unfair to expect a deployed dialogue system to be able to understand everything\nthat users may say, but these systems should be able to learn during conversation by in-\nteracting with users to become more and more knowledgeable and powerful. That is the\ngoal of building lifelong learning dialogue systems. That is also the key motivation for us\nto write this book to introduce and to promote the research of lifelong learning dialogue\nsystems.\nThe project of writing this book started with a tutorial titled “Continual Learning\nDialogue Systems - Learning on the Job after Model Deployment” that we gave at 2021\nInternational Joint Conference on Artificial Intelligence (IJCAI-2021), August 21-26, 2021,\nMontreal, Canada. As we believe that lifelong learning dialogue systems is a very important\ntopic for the future of dialogue systems and AI, we decided to develop the tutorial into a\nbook. Our original interest in the topic stemmed from our research in lifelong/continual\nmachine learning, dialogue systems, and natural language processing. Over the years, we\nhave used many dialogue systems in smart phones and customer support websites. Our\nexperiences have mostly been less than satisfactory. It is very natural to ask the question\nwhy the dialogue system cannot communicate with users and learn to improve itself when\nit cannot understand what the user says as we humans do. Most of the deployed dialogue\nsystems work in multi-user environments, e.g., Amazon Alexa and Apple Siri, and Google\nAssistant. If these systems can learn even a tiny amount of knowledge from each user when\nit gets stuck, it will be very knowledgeable and smart over time. Another reason for our\ninterest in lifelong learning dialogue systems is that our research group has been working\non lifelong/continual learning for many years. This combination of factors encouraged us\nto work on the topic and to write this book.\niii\n\nAs lifelong learning dialogue systems sit at the intersection of dialogue systems in natural\nlanguage processing and lifelong/continual learning in machine learning, this book will touch\nboth fields. We aim to present a comprehensive survey and review of the important research\nresults and latest ideas in these areas. We also want to propose a theoretical framework to\nbe used to guide the future research and development in the field. This framework is called\nSOLA (Self-initiated Open-world continual Learning and Adaptation), which is originally\nproposed for building autonomous and continual learning AI agents. Since lifelong learning\ndialogue systems are such agents, the framework is naturally suited for the topic. Presently,\nthere are several research topics in dialogue systems that are closely related to lifelong\nlearning dialogue systems. This book will bring all these topics under one roof and discuss\ntheir similarities and differences. Through this book, we would also like to motivate and\nencourage researchers to work on lifelong learning dialogue systems and practitioners to\nbuild lifelong learning dialogue systems that can be deployed for practical use to improve\nthe user experiences and to make dialogue systems smart and smart over time. Without\nthe capability of continually learning and accumulating knowledge, making inference about\nit, and using the knowledge to help future learning and problem solving, achieving true\nintelligence for AI agents is unlikely.\nTwo principles have guided the writing of this book. First, it should contain strong\nmotivations for conducting research in lifelong learning dialogue systems in order to en-\ncourage graduate students and researchers to work on the problem. Second, the writing\nshould be accessible to practitioners and upper-level undergraduate students who have ba-\nsic knowledge of natural language processing and machine learning. Yet there should be\nsufficient in-depth materials for graduate students who plan to pursue Ph.D. degrees in di-\nalogue systems, lifelong learning, or their integration of lifelong learning dialogue systems.\nWe also strongly believe that lifelong learning dialogue systems can be built and deployed\nfor practical applications.\nThis book is suitable for students, researchers, and practitioners who are interested in\ndialogue systems, natural language processing, and machine learning. Lecturers can readily\nuse the book in class for courses in any of these related fields.\nSahisnu Mazumder and Bing Liu\nNovember 2022\niv\n\nAcknowledgments\nWe would like to thank the current and former graduate students in our group and our\ncollaborators: Joydeep Biswas, Jiahua Chen, Zhiyuan Chen, Daniel A. DeLaurentis, Sepideh\nEsmaeilpour, Neta Ezer, Geli Fei, Hasan Ghadialy, Yiduo Guo, Scott Grigsby, Estevam R.\nHruschka Jr., Wenpeng Hu, Minlie Huang, Reid Hyland, Adam Kaufman, Zixuan Ke,\nGyuhak Kim, Tatsuya Konishi, Mori Kurokawa, Huayi Li, Jian Li, Tianrui Li, Yanni Li,\nHaowei Lin, Zhou Lin, Lifeng Liu, Qian Liu, Guangyi Lv, Chuhe Mei, Piero Molino, Chihiro\nOno, Arjun Mukherjee, Nianzu Ma, Shaoshuai Mou, Alexander Politowicz, Qi Qin, Steven\nRizzi, Eric Robertson, Gokhan Tur, Vipin Vijayan, Lei Shu, Hao Wang, Mengyu Wang,\nYijia Shao, Shuai Wang, Hu Xu, Yueshen Xu, Rui Yan, Yan Yang, Tim Yin, Tim Yuan,\nPhilip S. Yu, Lei Zhang, Dongyan Zhao, Hao Zhou and Xiaoyan Zhu for their contributions\nof numerous research ideas and helpful discussions over the years.\nThis book was also\nbenefited significantly from numerous discussions in DARPA SAIL-ON Program meetings.\nOur greatest gratitude go to our own families. Sahisnu Mazumder would like to thank his\nparents Ramesh Ch. Mazumder and Dulali Mazumder, elder sister Snigdha, brother-in-law\nSudip and (late) aunt Malaya Goswami for their invaluable support and encouragements.\nBing Liu would like to thank his wife Yue, his children Shelley and Kate, and his parents.\nThey have helped in so many ways.\nThe writing of this book was partially supported by a SAIL-ON Program Contract\n(HR001120C0023) of the Defense Advanced Research Projects Agency (DARPA), Northrop\nGrumman research gifts, three National Science Foundation (NSF) grants (1650900, IIS-\n1910424 and IIS-1838770), an NCI grant R01CA192240, and a Research Contract with\nKDDI. The content of the book is solely the responsibility of the authors and does not\nnecessarily represent the official views of DARPA, Northrop Grumman, NSF, NCI, KDDI,\nUIC or Intel.\nThe Department of Computer Science at the University of Illinois at Chicago provided\ncomputing resources and a very supportive environment for this project. We would par-\nticularly like to thank Patricia Brianne Barrera, Sheri Lyn Joscelyn, Denise Marie Kelly,\nEmily Lam, Sherice Nelson, and Ivy Yuan for their valuable support in the research projects\nrelated to this book.\nSahisnu Mazumder would also like to thank Intel Corporation, his colleagues at Multi-\nmodal Dialogue and Interaction (MDI) group and Intelligent Systems Research (ISR) divi-\nv\n\nsion at Intel Labs and specifically, Saurav Sahay and Lama Nachman for their invaluable\nsupport, advice and encouragement.\nSahisnu Mazumder and Bing Liu\nNovember 2022\nvi\n\nContents\nAuthors’ Biographies\ni\nPreface\niii\nAcknowledgments\nv\n1\nIntroduction\n1\n1.1\nDialogue and Interactive Systems: Background . . . . . . . . . . . . . . . .\n1\n1.1.1\nTask-oriented Dialogue Systems . . . . . . . . . . . . . . . . . . . . .\n2\n1.1.2\nChit-chat Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.2\nWeaknesses of Modern Dialogue Systems . . . . . . . . . . . . . . . . . . . .\n7\n1.3\nMotivation for Lifelong Learning Dialogue Systems . . . . . . . . . . . . . .\n9\n1.4\nLifelong Interactive Learning in Conversation . . . . . . . . . . . . . . . . .\n11\n1.5\nOrganization of the Book\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2\nOpen-world Continual Learning: A Framework\n15\n2.1\nClassical Machine Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.2\nA Motivating Example for SOLA . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2.3\nNovelty Detection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.4\nLifelong and Continual Learning\n. . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.5\nThe SOLA Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n2.5.1\nComponents of SOLA . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.5.2\nOpen World Continual Learning\n. . . . . . . . . . . . . . . . . . . .\n27\n2.5.3\nRelevance of Novelty . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n2.5.4\nNovelty Characterization and Adaptation . . . . . . . . . . . . . . .\n30\n2.5.5\nRisk Assessment and Learning\n. . . . . . . . . . . . . . . . . . . . .\n31\n2.6\nA Dialogue System based on SOLA . . . . . . . . . . . . . . . . . . . . . . .\n32\n2.7\nComparison with Related Work . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n2.8\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n3\nContinuous Factual Knowledge Learning in Dialogues\n36\n3.1\nOpportunities for Knowledge Learning in Dialogues . . . . . . . . . . . . . .\n36\n3.2\nExtracting Facts from Dialogue Context . . . . . . . . . . . . . . . . . . . .\n36\nvii\n\n3.3\nLexical Knowledge Acquisition in Dialogues . . . . . . . . . . . . . . . . . .\n36\n3.4\nInteractive Factual Knowledge Learning and Inference . . . . . . . . . . . .\n36\n3.5\nLearning new knowledge from external sources\n. . . . . . . . . . . . . . . .\n36\n3.6\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n4\nContinuous and Interactive Language Learning and Grounding\n37\n4.1\nModes of language learning human-chatbot interactions\n. . . . . . . . . . .\n37\n4.2\nLearning language games through interactions . . . . . . . . . . . . . . . . .\n37\n4.3\nDialogue-driven Learning of Self-adaptive NLIs . . . . . . . . . . . . . . . .\n37\n4.4\nInteractive semantic parsing and learning from feedback . . . . . . . . . . .\n37\n4.5\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5\nContinual Learning in Chit-chat Systems\n38\n5.1\nPredicting User Satisfaction in Open-domain Conversation . . . . . . . . . .\n38\n5.2\nLearning by Extracting New Examples from Conversation . . . . . . . . . .\n38\n5.3\nDialogue Learning via Role-Playing Games\n. . . . . . . . . . . . . . . . . .\n38\n5.4\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n6\nContinual Learning for Task-oriented Dialogue Systems\n39\n6.1\nOpen Intent Detection & Learning . . . . . . . . . . . . . . . . . . . . . . .\n39\n6.2\nContinual Learning for Semantic Slot Filling . . . . . . . . . . . . . . . . . .\n39\n6.3\nContinual Learning for Dialogue State Tracking . . . . . . . . . . . . . . . .\n39\n6.4\nContinual Learning for Natural Language Generation . . . . . . . . . . . . .\n39\n6.5\nJoint Continual Learning of all Dialogue Tasks\n. . . . . . . . . . . . . . . .\n39\n6.6\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n7\nContinual Learning of Conversational Skills\n40\n7.1\nLearning user behaviors and preferences . . . . . . . . . . . . . . . . . . . .\n40\n7.2\nLearning emotions, moods and opinions in dialogues . . . . . . . . . . . . .\n40\n7.3\nModeling situation-aware conversations\n. . . . . . . . . . . . . . . . . . . .\n40\n7.4\nSummary\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n8\nConclusion and Future Directions\n41\nviii\n\nChapter 1\nIntroduction\nBuilding dialogue systems or conversational agents capable of conversing with humans in\nnatural language (NL) and understanding human NL instructions is a long-standing goal\nof AI [Winograd, 1972]. These systems, also known as chatbots, have become the front\nrunner of AI advancement due to wide-spread applications such as assisting customers in\nbuying products, booking tickets, reducing stress, and executing actions like controlling\nhouse appliances and reporting weather information. However, the user experiences have\nnot been fully satisfactory.\nThere are many weaknesses with the current research and\nfielded dialogue systems. One of the major weaknesses is that they do not learn continuously\nduring conversation (i.e., post-deployment) with the user after they are deployed in practice.\nBuilding lifelong learning dialogue systems that posses the capability of continuous learning\nduring conversation is the main topic of discussion of this book.\nThis chapter aims to\nmotivate and provide the foundational idea of building such dialogue systems. Note that,\nwe use the term chatbots to refer to all kinds of conversational agents, such as dialogue\nsystems, personal assistants, conversational question-answering systems etc. onwards.\nIn the following sections, we first provide some background of modern dialogue systems\n(Section 1.1) and discuss their general weaknesses (Section 1.2), which provide the motiva-\ntions for studying and building lifelong learning dialogue systems (introduced in Sections\n1.3 and 1.4). Finally, we conclude the chapter with a discussion on the organization of this\nbook in Section 1.5.\n1.1\nDialogue and Interactive Systems: Background\nDialogue systems can broadly be categorized into two main types [Gao et al., 2019, Jurafsky\nand Martin, 2020]:\n(1)\nChit-chat systems [Vinyals and Le, 2015, Shang et al., 2015, Yao et al., 2015, Li\net al., 2016b,a, Serban et al., 2016, Chen et al., 2017, Xing et al., 2017, Wu et al.,\n2017, Shen et al., 2017, Mei et al., 2017, Serban et al., 2018, Shen et al., 2018, Wu\net al., 2018, Pandey et al., 2018] are chabots designed to engage users by conducting\n1\n\nthe chit-chat type of conversation on a wide range of topics without having a spe-\ncific goal to complete. Examples include Social Chatbots like ELIZZA [Weizenbaum,\n1966], PARRY [Colby et al., 1971], ALICE [Wallace, 2009], Microsoft XiaoIce [Zhou\net al., 2020], AliMe Chat [Qiu et al., 2017], etc. Such chatbots are built with the\ngoal of supporting seamless conversation with users, and helping them with useful\nrecommendations and mental supports.\n(2) Task-oriented chatbots [Williams and Young, 2007, Bordes et al., 2016, Wen et al.,\n2017b, Budzianowski et al., 2018, Wen et al., 2017a, Lowe et al., 2017, Shah et al., 2018,\nZhao et al., 2017, Luo et al., 2019] are chatbots designed to assist users to complete\ntasks based on users’ requests, e.g., providing the requested information and taking\nactions. Most of the popular personal assistants such as Alexa, Siri, Google Home, and\nCortana, are task-oriented chatbots. Besides, these types of chatbots are also built as\nQA Bots to support Question-answering (QA) over knowledge bases, conversational\nrecommendation systems for online product or service recommendations to end-users\nand as Natural Language Interaction systems to enable natural language (NL) driven\ntask completion.\nAthrough the broad goal of task-oriented chatbots is to perform actions or tasks on\nusers’ behalf, they can also be of two types based on their design and the nature of\ninteraction with users whom they are meant to support: (1) Systems that achieve task\ncompletion through multi-turn dialogues with users where users express their intent\nand refine them (based on the feedback from the chatbot) over a sequence of dia-\nlogue turns with the system. They are formally known as Task-oriented Dialogue\nSystems (ToDS). (2) The other kind of systems that intend to accomplish tasks\nthrough a single-turn dialogue where the user provides a NL instruction (command)\nand the system’s goal is to just interpret it by translating it into some actions to be\nexecuted by the underlying application. Such systems are formally known as Natu-\nral Language Interfaces (NLIs). NLIs are also sometimes referred to as Natural\nLanguage Interaction systems in general. Although NLIs are mostly built to sup-\nport a single-turn interaction with user per task completion goal, they can engage in\nmuti-turn dialogues with user as well (similar to traditional ToDS systems) to better\nunderstand the NL instruction and resolving ambiguities (if any) to serve the user\nbetter.\n1.1.1\nTask-oriented Dialogue Systems\nA full-fleged Task-oriented dialogue systems (ToDS) is mostly designed as a modular system,\nhaving six modules, viz., Automatic Speech Recognition (ASR), Natural Language Under-\nstanding (NLU), Dialogue State Tracking (DST), Dialogue Policy (DP) Learning, Natural\nLanguage Generation (NLG) and Text-to-speech (TTS) synthesis. Figure 1.1 shows the\narchitecture of a typical ToDS with all the modules integrated with each other. Here, the\nASR module is responsible for translating the spoken utterance from the user into text\n2\n\nFigure 1.1: A typical task-oriented dialogue system with its modules.\nwhich is fed to the NLU module for language understanding and the TTS module gener-\nates the speech from text which is the NL response generated by the NLG module. The\nDST and DP modules are often unified and referred to as Dialogue Manager (DM) which\nis responsible for progression of the dialogue by managing the dialogue turns.\nOften, ASR and TTS modules are studied as a seprate field of research and existing\nworks mostly focus on the rest of four components and their interactions as a typical ToDS\nframework. In particular, they assume ASR and TTS are available to use and deal with\nonly text-based user input and text-based generated ouput by the rest four modules. Thus,\nin the rest of our book, we mainly focus on NLU, DST, DP and NLG as the four main\nmodules of a typical ToDS system.\n(1) Natural Language Understanding (NLU): The goal of the NLU module is to\nidentify user intents and extract associated information (slots) from the user utterance.\nIn general, NLU involves solving three subtasks:\n• Domain Classification, i.e., classifying the domain of the task expressed in\nuser’s utterance (e.g., is this user talking about airlines, movie, or music?)\n• Intent Classification, i.e., predicting the general task or goal that the user is\ntrying to accomplish through the utterance (e.g., is this user wanting to search\nfor a flight, book a movie ticket, or play a song, etc.)\n• Slot filling, where the goal is to extract the particular slots and fillers that the\nuser intends the system to understand from his/her utterance with respect to\nhis/her intent. The problem of Slot filling is commonly formulated as a sequence\nlebelling task, where the goal is to tag each word of the utterance with one of\nthe pre-defined semantic slot names.\nLet’s have an example here. Considering the user utterance- “Find me an italian\nrestaurant for dinner”, the NLU module will classify the domain as “restaurant” and\nintent as “SearchRestaurant” and extract slots as {CUISINE: italian, TIME: dinner\n}.\n3\n\n(2) Dialogue State Tracking (DST): The DST module tracks the dialogue state that\ncaptures all the essential information in the conversation so far.\nSpecifically the\ndialogue state includes the entire state of the frame at this point (the fillers of\neach slot), as well as the user’s most recent dialogue act, summarizing all of the user’s\nconstraints. Below we show an example dialogue snippet, with the dialogue states\ntracked for each dialogue turn.\nUser: Find me an italian restaurant for dinner.\ninform(cuisine=Italian; time=dinner)\nSystem: Okk. What price range do you prefer?\nUser: may be in moderate\ninform(cuisine=Italian; time=dinner; price range=$$)\nSystem: Sure.\nThere an nice italian resturant at moderate price range\nnearby.\nUser: Awesome! What’s the name and address?\ninform(cuisine=Italian; time=dinner; price range=$$);\nrequest(name; address)\nSystem: The restaurant name is Franco’s Ristorante and address is\n300 W 31st St, Chicago, IL 60616.\n(3) Dialogue Policy (DP) Learning: The DP Learning module selects the next action\nbased on the current state (obtained from DST), i.e., deciding what action the system\nshould take next, meaning what dialogue act to generate. That is, at a given dialogue\nturn i, DP predicts which action ˆAi ∈A to take based on the entire dialogue state\n(A1, U1, . . . , Ai−1, Ui−1, Ui) [entire sequence of dialogue acts from the system (A) and\nfrom the user (U)]:\nˆAi = arg max\nAi∈A P(Ai|(A1, U1, . . . , Ai−1, Ui−1, Ui))\n(1.1)\nFor example, considering the first turn in the above dialogue snippet, with the cap-\ntured dialogue state as inform(cuisine=Italian; time=dinner), the dialogue act gener-\nated by dialogue policy is request(price range). This results in the system’s response\nwhere it asks the user for the price range in the dialogue turn.\ninform(cuisine=Italian; time=dinner)\n[ dialogue state ]\n⇓\nrequest(price range)\n[ dialogue act ]\n(4) Natural Language Generation (NLG): Once the policy has decided what speech\nact to generate (i.e., ˆAi ) for the current dialogue turn, the NLG module generates the\ntext response to the user. It is often modeled in two stages, content planning (i.e., what\nto say) and sentence realization (i.e., how to say it). Sentence realization is commonly\nachieved through delexicalization (i.e. mapping from frames to delexicalized sentences\n4\n\nby often using encoder-decoder models). Here, we show an example corresponding to\nthe last dialogue turn in the above dialogue snippet, where the system generates the\ndelexicalized response for the given dialogue act inform(name; address) as shown\nbelow and then, transforms it into an actual NL response by filling appropriate slots\nin the delexicalized response.\ninform(name; address)\n[ dialogue act ]\n⇓\nThe restaurant name is NAME SLOT and address is ADDRESS SLOT\n[ delexicalized response ]\nAlthough most of the existing ToDS systems are built as a modularized system, recent\napproaches have focused on training all these modules together end-to-end to reduce error\npropagation across modules.\n1.1.2\nChit-chat Systems\nChit-chat Systems (or social chatbots) are often implemented as a unitary (non-modular)\nsystem. Based on their design approaches, they can broadly be categorized into two types:\n(1) Rule-based systems: Rule-based systems work based on pattern and transform\nrules. Examples include ELIZA [Weizenbaum, 1966] and PARRY [Colby et al., 1971].\nEach pattern/rule is linked to a keyword that might occur in a user utterance and\ntrigers some transformation of a predfined template to generate response. For exam-\nple, a pattern / transform rule in ELIZA is the following:\n(0 YOU 0 ME)\n[pattern / decomposition rule]\n⇓\n(WHAT MAKES YOU THINK I 3 YOU)\n[transform / reassembly\nrule]\nHere, “0” in the pattern represents a kleen star (i.e., an indefinite number of words)\nand “3” in the transform indicates that the third component of the subject decom-\nposition is to be inserted in its place. An example of this pattern / transform rule\napplied in a dialogue turn would be-\nUser: You hate me\nSystem: WHAT MAKES YOU THINK I HATE YOU\n(2) Corpus-based systems: Corpus-based systems are trained to mimic human conver-\nsations by training on large amounts of human-human conversational data. Examples\nof this kind of system include modern chatbots like Microsoft XiaoIce [Zhou et al.,\n2020], AliMe Chat [Qiu et al., 2017] etc.\n5\n\nFigure 1.2: Response generation by a typical seq2seq model.\nCorpus-based systems are further categorized based on their response production\nmethod as:\n• Response by retrieval [Wang et al., 2013, Lowe et al., 2015, Wu et al., 2016,\nZhou et al., 2016, Yan et al., 2016, Wu et al., 2017, Bartl and Spanakis, 2017,\nChen et al., 2017, Yang et al., 2018, Tao et al., 2019a,b]: These methods for-\nmulate the response production task as an information retrieval (IR) problem.\nConsidering the user’s turn as a query q, these methods aim to retrieve and re-\npeat some appropriate turn ˆr as the response from a corpus of conversations D\n(training set for the system). The turns in D form a candidate set of responses,\nwhere each turn r ∈D is scored as a potential response with respect to the\ncontext q. The highest scored candiadate is then selected as response.\nresponse(q, D) = ˆr = arg max\nr∈D\nq.r\n|q||r|\n(1.2)\n• Response by Generation [Vinyals and Le, 2015, Shang et al., 2015, Yao et al.,\n2015, Li et al., 2016b,a, Serban et al., 2016, Chen et al., 2017, Xing et al., 2017,\nShen et al., 2017, Mei et al., 2017, Serban et al., 2018, Shen et al., 2018, Wu\net al., 2018, Pandey et al., 2018]: These methods view response production as\nan encoder-decoder task - transducing from the user’s prior turn to the system’s\nturn. Figure 1.2 shows an example of response generation by a typical Seq2seq\nmodel [Vinyals and Le, 2015], for a given conversation context. Here, the encoder\nis a Recurrent Neural Network (RNN) model that encodes the conversation con-\ntext into a fixed-sized context vector to summarize the information in the context\nand then, another RNN model takes the context vector as input and generates\nthe response word-by-word.\n6\n\nAlthough there have been a large number of works that solely focus on building a\nretrieval-based or generative chatbots as discussed above, there are approaches [Song et al.,\n2016, Serban et al., 2017, Qiu et al., 2017] that adopt a hybrid strategy, i.e., combining neural\ngenerative and retrieval based models which has been shown to achieve better performance.\n1.2\nWeaknesses of Modern Dialogue Systems\nDespite their widespread applications, chatbots still have some serious weaknesses:\n(1) Dull response: Existing social chatbots often generate uninformative responses like\n“I don’t know”, “I don’t have a clue” etc., unspecific responses like “I like all kinds of\nfoods” and repeated responses (i.e., telling the user same thing over and over again)\ndue to lack of diversity [Li et al., 2016a,b, Zhang et al., 2018, Jiang and de Rijke,\n2018, See et al., 2019]. Such dull or uninformative responses often disengage user\nfrom the conversation and leads to the abrupt end of the dialogue session. As we will\ndiscuss in Chapter 2, such responses may also occur when the chatbot cannot detect\nnovelty (what it does not understand) well and/or cannot characterize the novelty.\nAs a result, it cannot give a suitable response. In fact, when such a situation occurs,\nit is opportunity for the chatbot to learn new knowledge.\n(2) Out of context response: Whenever the chatbot encounters some unknwon topics\nor information in users utterance or changes in topics of discussion, they often go out\nof context by generating off the track or irrelevant responses, dissatisfying the user.\nLike the case above, this is also the issue of not being able to detect novelty well\nand not being able to accommodate or adapt to the novelty, which we will discuss in\nChapter 2.\n(3) Semantic Conflicts: Open-domain chatbots often suffer from semantic conflict is-\nsues in conversation. Here is an example from [Huang et al., 2020]:\nUser: I come from Shanghai.\nSystem: Cool! I am from Beijing, we come from the same city!\nUser: Back to Beijing now. It is so cold but I am very happy!\nSystem: When will you come to Beijing?\nHere, although the user has responded that he/she is currently in Beijing, the chatbot\nasks when he intends to visit Beijing, creating semantic conflict in the conversation\ndue to lack of understanding / reasoning over the context.\n(4) Inconsistency: Both open-domain and task-oriented chatbots often suffer from in-\nconsistency problem. Here is an example from [Huang et al., 2020] showing the lack\nof consistency in open-domain chatbots due to the lack of a coherent personality:\n7\n\nUser: Are you a boy or a girl?\nSystem: I am a boy.\nUser: Are you a girl?\nSystem: Yes, I am a girl.\nSchaub et al. [2021] also compiled a list of inconsistent behaviors often observed in\nmodern task-oriented dialogue systems. These include misunderstanding of Intent,\nBad entity, Repetition, “Bye” misunderstood, User insults, Bad API call, Incom-\npleteness and Negation error.\n(4) Interactiveness Issue: Open-domain chatbots also suffer from bad interactiveness\nissue due to lack of grounding on the conversation context. Here is an example from\n[Huang et al., 2020], where the chatbot is not providing any recommendation inspite\nof being explicitly asked by the user and generating uninformative response.\nUser: Terrible breakfast today.\nSystem: How about we having breakfast together tomorrow morning?\nUser: Sure. Would be great with your recommendation.\nSystem: Really?\n(5) Lack of context-awareness: Due to the lack of context-rich datasets used in build-\ning chatbots, the deployed chatbots often lack context-awareness (e.g, understanding\nthe situation of the user, dealing with ambiguous utterances from the user that needs\nreference resolution utilizing dialogue history, leveraging multi-model context inputs\nto generate responses etc.) while modeling their responses. This sometimes can lead\nto undesirable outcomes and dissatisfaction in users.\n(6) Knowledge limitation due to fixed-corpus training: A great deal of manual\neffort is needed to label training data or to write rules and compile knowledge bases\n(KBs). No matter how much data is used to train a chatbot, it is impossible to cover\nall possible variations of natural language. Thus, when deployed in practice, a well-\ntrained chatbot often performs poorly. The pre-compiled KBs cannot cover the rich\nknowledge needed in practice.\nA substantial amount of work has focused on solving the above weaknesses [1-5] over\nthe years to come up with better response generation model that strives to produce more\nrelevant and quality responses.\nHowever, very little effort have been made to address\nthe weakness-6 resolving which is the essential to build the next-generation dialogue and\ninteractive systems. In this book, we focus on presenting and discussing methods that deal\nwith weakness 6 which is the primary motivation for developing continual learning dialogue\nsystems (as discussed next). Although here we primarily focus on weakness 6, solving this\nissue has an indirect influence on resolving other issues [1-5] as well, as this provides the\nchatbot the scope to learn continuously from users and self-improve its response generation\ncapability over time as noted in the discussion of weaknesses 1 and 2.\n8\n\n1.3\nMotivation for Lifelong Learning Dialogue Systems\nAs we can see, current dialogue systems still have many limitations. Most of us should have\npersonal experiences of using dialogue systems either in our smart phones or on company\nwebsites that provide customer supports and the frustrations that we have experienced, and\nwished that real humans were behind such systems to serve us. Many researchers (including\nour own research group) have tried to address some of these issues in both task-oriented\nand chit-chat types of dialogue systems [Zhou et al., 2018, Shu et al., 2019b,a, Hu et al.,\n2019a]. In this book, we focus on only the lifelong or continual learning aspect. In fact,\nseveral limitations can be addressed by continual/lifelong learning.\nThe need for lifelong learning dialogue systems is a no-brainer simply because that is\nwhat we humans do and the current dialogue systems cannot do. Regardless of whether\nan existing dialogue system is based on hand-crafted rules or based on training via a deep\nlearning model, it is an isolated system and can only be improved by human engineers either\nthrough adding more labeled training data or hand-crafted rules, or through improving the\nunderlying deep learning models. After any such change, retraining of the whole system is\ntypically required to update the system. This process is highly time consuming as training\na deep learning model of a large system usually take days and weeks. However, this is not\nwhat we humans do. We humans have this remarkable ability to learn incrementally and\nquickly and we can learn by ourselves based on our own initiatives through our interactions\nwith those who converse with us and the real-life environment, and learn from books and\nweb pages without being supervised or guided by others. A dialogue system should do the\nsame in order to achieve some level of true intelligence.\nLet us use various lifelong/continual learning settings to further motivate why dialogue\nsystems should learn continually.\nContinual learning in machine learning is defined as\nlearning a sequence of tasks incrementally [Chen and Liu, 2018]. In supervised learning,\neach task is basically a classification problem that consists of a set of classes to be learned\nbased on its training data. In dialogue systems, it can be a set of new skills that needs to\nbe learned, e.g., booking a hotel and reserving a restaurant table. Continual learning can\nbe further subdivided into two major settings, class continual learning (CCL) (also called\nclass incremental learning) and task continual learning (TCL) (also called task incremental\nlearning). In CCL, the classes in each task are distinct and no overlapping with classes\nof other tasks.\nIn testing, no task related information is provided.\nDuring testing or\napplication, the user will not identify which task/service he/she wants to use and the system\nhas to automatically detect the task. For example, the user may say “can you find a near-by\nSushi restaurant for me.” The system needs to first predict whether the user wants to find\na restaurant or a hotel and then find some near-by Sushi restaurants and give to the user.\nTo be able to incrementally add new skills is clearly very important for a dialogue system\nbecause retraining the whole system from scratch whenever a new skill is added is highly\ntime consuming. In some cases, this kind of retraining may not be possible. For example,\nmany companies do not have in-house capabilities to build a dialogue system for themselves\nand they licence systems from others. However, in almost all cases, they cannot change\n9\n\nthe systems in any way.\nIf they want to add a new skill to provide some new services\nto their customers, they have go to the company from whom they licensed the dialogue\nsystem to request the addition. If the dialogue system has the ability to incrementally or\ncontinually learning, this problem is eliminated. The company can simply add the new skill\nby continual or incremental training using the data for the new skill.\nTCL differs from CCL in that TCL allows the user to indicate the task that he/she\nintends to perform.\nFor example, he/she can select the option of using the restaurant\nsubsystem. In TCL, each task model is usually built separately, but it can share knowledge\nwith models of the previous tasks for knowledge transfer. For example, booking a hotel room\nand reserving a table in a restaurant have a lot of similarities and knowing one can help do\nthe other. If the system has learned to book a hotel for the user before, it will be much easier\nfor it to learn to help the user reserve a restaurant table, i.e., requiring much less training\ndata. This kind of transfer in continual learning is automatic because the system decides\nfrom which previous tasks to transfer and what knowledge to be transferred [Ke et al., 2020,\n2021a]. Clearly, this ability to transfer knowledge is highly desirable in building practical\ndialogue systems because manually labeling/writing a large amount of training dialogues\nis very costly and highly time-consuming. It is also important to note that this kind of\ntransfer is different from the traditional transfer learning.\nTraditional transfer learning\ntransfers knowledge from a selected source task to a selected target task and the selection\nis usually done by humans to ensure that the two tasks are similar and can achieve positive\ntransfer. However, in continual learning, everything is done automatically with no manual\ninvolvement.\nThese two forms of learning based on existing research in continual learning, however,\nare still offline types of learning, meaning that both the tasks and their training data are\nprovided by human engineers, and learning is done offline rather than on the fly during\nconversation with users. To achieve even more intelligence, it is important that the system\ncan learn actively during conversation with users based on the initiation of the system itself\nin its interaction with users, i.e., learning on the job after deployment, from users. After\nall, we humans learn a great deal from our daily conversations.\nLearning during conversation or while working is highly desirable because of several\nreasons as a large number of unsatisfactory experiences or frustrations with current dialogue\nsystems come from the fact that these systems cannot understand what the user is saying\nand give nonsense responses. The question is why the dialogue system cannot communicate\nwith the user and learn to improve itself when it cannot understand a user utterance. If\nthis type of learning is possible, then the user will not be so frustrated and the system will\nnot have the same problem next time. Furthermore, most of the current dialogue systems\nwork in multi-user environments, e.g., Amazon Alexa, Apple Siri, and Google Assistant. If\nthese systems can learn even a tiny amount of knowledge from each user, the accumulated\nknowledge over time will make the system very smart and powerful. It is understandable\nthat many users may not be willing to help the dialogue system learn but there should be\nmany users who are willing to help. We will discuss this topic in greater detail in the next\nsection.\n10\n\nAnother important aspect that needs continual learning is the learning of personal traits,\nhabits, temperaments, emotional characteristics, and each user’s personal situations so that\nthe dialogue systems can be made more personalized to suit each individual user. This is\nespecially important for dialogue systems that serve as personal assistants. To learn these\ntypes of information, the system needs to learn incrementally through a long history of\ndialogues. To improve this kind of learning and to learn quickly, the system can borrow\nknowledge from like-minded users and users with similar personal circumstances. In such\ncases, continual monitoring and incremental learning and automated knowledge transfer are\nall critical.\nThe ultimate goal is for dialogue systems to achieve human-like behaviors and abilities\nin conversation so that they can continually learn and improve themselves on their own\nto become more and more knowledgeable and powerful without intervention from human\nengineers.\nContinual learning can be carried out either through interactions with users\nduring conversation and/or from other sources such as books and web pages. Achieving\nthis ultimate goal is clearly difficult at the moment, but it is possible to progressively\nimprove the dialogue systems technology to move towards the goal gradually.\n1.4\nLifelong Interactive Learning in Conversation\nIn this section, we dive more into online continual learning during conversation rather than\noffline continual learning. This is perhaps the most central capability that a lifelong learning\ndialogue system should have. That is, a dialogue system should not be limited by offline\ntraining or pre-compiled knowledge bases (KBs). It should learn online on the fly during\nconversation continually, which is also called learning on the job or while working. The\nprocess typically involves interaction with human users and learn to improve themselves\nin a self-motivated and self-initiated manner [Chen and Liu, 2018, Liu, 2020]. We humans\nalso learn a great deal of our knowledge in our conversations and interactions with others\nwhich improve our knowledge and conversational abilities. We call this form of learning\nLINC (Lifelong INteractive learning in Conversation) [Mazumder, 2021, Liu and\nMazumder, 2021].\nLINC focuses on three continuous learning capabilities of chatbots:\n(1) learning factual knowledge in open-ended and information-seeking conversations, (2)\nlearning to ground new natural language utterances, and (3) learning new conversational\nskills. Some initial attempts have been made in [Mazumder et al., 2020a,b, 2019, Hancock\net al., 2019, Luo et al., 2019, Mazumder et al., 2018]. A key idea for solving the LINC\nproblem is to exploit the wisdom of the crowd in a multi-user environment (where almost\nall chatbots operate) to learn new knowledge by actively asking or interacting with the\ncurrent user and/or other users to enable the chatbot to learn a large amount of knowledge\nquickly and effectively.\nIn Chapter 2, we will define a more general framework (called\nSOLA) for on-the-job learning.\nLINC can be more formally described as follows. During a conversation, the chatbot\ncreates a new task TN+1 on the fly when it wants to learn a piece of knowledge from\na user utterance (e.g., extracting an unknown fact), encounters a problem (e.g., unable\n11\n\nto understand a user utterance), encounters an unknown intent or slot in task-oriented\ndialogues, or is unable to answer a user query due to unknown entities or facts etc).1 The\nproblem of discovering the unknowns during conversation to formulate a new learning task is\nreferred to as novelty detection or open-set recognition [Liu, 2020], solving which is essential\nfor LINC. Once the new task TN+1 is formulated, the chatbot needs to acquire the ground\ntruth training data DN+1 for learning. To do this, the chatbot needs to formulate a dynamic\ninteraction strategy S to interact with the user, i.e., to decide what to ask and when to ask\nthe user, and then execute S to acquire the ground truth data. It then incrementally learns\ntask TN+1 with only one or a few examples.\nExisting approaches to obtaining the training data is through manual labeling or writ-\ning, which is both costly and time-consuming. As chatbots typically work in multi-user\nenvironments, we can exploit such environments to obtain the ground truth training data\ninteractively during actual conversations. This process incurs no cost.\nApart from traditional supervised learning through training using labeled examples, a\ndialogue system can learn during conversation in many other ways.\n1. Extracting data or information directly from user utterances (or dialogue\nhistory), which can be real-world facts, user preferences, etc. For example, while conversing\nabout movies, if the user says “I watched Forest Gump yesterday. The movie was awesome.\nLiked Tom Hanks’ performance very much.”, the chatbot can extract the new fact (Forest\nGump, isa, movie) and (Tom Hanks, performed in, Forest Gump) [Liu and Mei, 2020].\nLater, the chatbot can use these facts in future conversations while answering questions\nlike “Who acted in Forest Gump?” or generating a response to user’s utterance “I’m feeling\nbored. Can you recommend a good movie?”. The chatbot may even ask the user some related\nquestions [Liu and Mei, 2020] to obtain more knowledge.\nFor example, after obtaining\n(Forest Gump, isa, movie), the chatbot may ask a property question: “What is the genre of\nForest Gump?” If the user answers, then another piece of knowledge is learned. Note that\nthe extraction method proposed in [Liu and Mei, 2020] is rule-based, which works with rule-\nbased chatbots. Many deployed chatbots in industry are written with handcrafted rules.\nAdditional knowledge/data may be inferred from the acquired knowledge and the existing\nknowledge bases.\n2. Asking the current user when the chatbot (1) doesn’t understand a user utterance,\nor (2) cannot answer a user query, which forms a new learning task. To obtain the ground\ntruth data, for (1), the agent may ask the current user for clarification, rephrasing, or even\ndemonstration if it is supported [Mazumder et al., 2020b]. For (2), it may ask the user for\nsome supporting facts and then infer the query answer [Mazumder et al., 2019, 2020a].\n3. Asking other users to obtain the answers when the chatbot could not answer a\nuser query. For example, if a user asks “What is the capital city of the US?” and the chatbot\nis unable to answer or infer now, it can try to find a good opportunity to ask another user\nlater “Hey, do you happen to know what the capital city of the US is?” If the user gives\nthe answer “it’s Washington DC,” the chatbot acquires the ground truth (a piece of new\n1The knowledge learning tasks created by the chatbot for itself to learn are not the same as the tasks\nthat the end-user wants to perform via the chatbot.\n12\n\nknowledge) which can be learned and used in future conversations. Note that although the\nanswer cannot help the user who asked the question originally, it may be used in the future\nwhen a similar question is asked by another user.\n4. Observing user demonstrations. In some cases, the chatbots deployed in practice\nalso come with Graphical User Interfaces (GUIs) or remote control facilities to explicitly\ncontrol devices apart from controlling them via natural language commands. Examples of\nsuch systems include robots performing household tasks like cleaning robots and personal\nassistants integrated with home appliances like Smart TVs, Smart Lights, Smart Speakers,\netc. Considering the user has issued an command and the bot has failed to execute the\nintended action, the user may perform the intended action via the GUI or remote control.\nThe bot can record the sequence of executed action(s) performed by the user by accessing\nthe underlying application logs and store the executed APIs as ground truth for the input\nnatural language command. The command along with the invoked APIs can serve as labeled\nexamples for learning the command [Forbes et al., 2015, Wang et al., 2017].\n5. Extract ground-truth data from external sources, e.g., online documents or\nonline knowledge bases [Mitchell et al., 2018].\nAcquiring knowledge from end-users comes with a shortcoming. That is, the knowledge\nlearned from them can be erroneous. Some users may even purposely fool the system by\nproviding wrong information or knowledge. Since chatbots usually work in a multi-user\nenvironment, such issues can be addressed through cross-verification. After acquiring a\npiece of new knowledge (a new command pattern or a fact) in an interaction session, the\nagent can store these new examples in a unverified knowledge buffer. Next, while interacting\nwith some other users in future sessions to accomplish a related task, it can ask these users\nto verify the accumulated unverified knowledge. Once verified for K times (by K different\nrandom users), the knowledge can be considered as trustworthy and removed from the\nunverified buffer and used in learning or chatting.\n1.5\nOrganization of the Book\nThis book surveys and introduces the topic of lifelong or continual learning dialogue systems.\nAlthough the body of literature is not particularly large, many related papers are published\nin a number of conferences and journals. There is also a large number of papers that do\nnot exhibit all the characteristics of a continual learning dialogue system and are somewhat\nweakly-related to the topic. It is thus hard, if not impossible, to cover all of the important\nand related works in the field. As a result, this book should not be taken as an exhaustive\naccount of everything on this topic. However, we believe, the book provides a fairly broad\ncoverage and presents some of the representative works and sets the necessary foundation\nfor future advancements in this area. Also, this book mainly focuses on the advancement\nin the topic of continual learning dialogue systems. It does not intend to cover the basics\nof prerequisite knowledge (e.g., machine learning, dialogue systems, NLP, etc.) in depth.\nInterested readers can follow some of the closely-related books [McTear, 2020, Jurafsky and\nMartin, 2020, Gao et al., 2019, Jokinen and McTear, 2009, Goodfellow et al., 2016, Chen\n13\n\nand Liu, 2018] on these topics for prerequisites.\nThe rest of book is organized as follows. In Chapter 2, we introduce the framework for\nopen-world continual learning, namely, Self-initiated Open-world continual Learning and\nAdaptation (SOLA), which sets the foundational idea for building a continual learning\ndialogue system.\nIn Chapter 3, we discuss various opportunities for continuous factual knowledge learning\nin dialogues, and discuss methods about how a chatbot can learn by extracting factual\nknowledge from conversation, acquire lexical knowledge and facts interactively from end-\nusers and learning by extracting knowledge from the Web.\nChapter 4 discusses and present methods for continual and interactive language learn-\ning for natural language interface (NLI) design. In particular, we discuss about interactive\nlanguage learning in games, building self-adaptive NLIs by continuously learning new com-\nmands from users and interactive semantic parsing for knowledge base question answering\nwith user feedback to deal with parsing errors and ambiguities.\nChapter 5 is focused on continual learning in chit-chat dialogue systems. Here, we discuss\napproaches to predict user satisfaction in conversation and present methods that use user\ndissatisfaction as an implicit signal to acquire user feedback and leverage it to continuously\nimprove response generation after model deployment.\nIn Chapter 6, we discuss methods for building continual learning task-oriented dialogue\nsystems (ToDS). The chapter discusses the topic of open-world intent learning and present\nrecent continual learning approaches on various ToDS sub-tasks like slot filling, dialogue\nstate tracking, natural language generation, and also, an approach that attempts to jointly\nsolve all these sub-tasks in an end-to-end continual learning setting.\nChapter 7 provides the motivation for continual learning of various conversational skills\nlike personalized conversation modelling, learning of emotions, moods, and opinions in\nconversation and situation-aware conversation modeling. To the best of our knowledge, we\nhave not come across any work in this topic that is particularly related to continual learning.\nThus, here we provide a brief survey of existing works and describe an outline for future\nresearch in the topic.\nFinally, Chapter 8 concludes the book and discusses some challenges and future direc-\ntions of research.\n14\n\nChapter 2\nOpen-world Continual Learning: A\nFramework\nAs more and more AI agents are used in practice, we need to think about how to make\nthese agents fully autonomous so that they can (1) learn by themselves continually in\na self-motivated and self-initiated manner rather than being retrained offline periodically\non the initiation of human engineers and (2) accommodate or adapt to unexpected or\nnovel circumstances. As the real-world is an open environment that is full of unknowns or\nnovelties, detecting novelties, characterizing them, accommodating or adapting to them, and\ngathering ground-truth training data and incrementally learning the unknowns/novelties\nare critical to making AI agents more and more knowledgeable and powerful over time.\nThe key challenge is how to automate the process so that it is carried out continually on\nthe agent’s own initiative and through its own interactions with humans, other agents and\nthe environment just like human on-the-job learning.\nThis chapter develops a theoretical framework for open-world continual learning, which\nalso serves as a framework for lifelong learning dialogue systems because such a dialogue\nsystem works in an open environment. Section 2.6 will briefly describe a dialogue system\nthat follows the proposed framework. Since we want the framework to be applicable to\nAI agents in general, the framework will cover aspects that may not be necessary for a\nlifelong learning dialogue system. The key aspect of the framework is self-initiation, which\ninvolves no engineers. The proposed framework is called Self-initiated Open-world continual\nLearning and Adaptation (SOLA). SOLA is like human learning on the job or learning\nwhile working. It learns after model deployment.\nWe should note that the SOLA framework goes beyond the traditional concept of ma-\nchine learning or continual/lifelong learning, which normally starts with a labeled training\ndataset given to a machine learning algorithm to produce a model (e.g., a classifier). Thus,\nexisting paradigms of classical machine learning and lifelong/continual learning are only\ntwo aspects of the SOLA paradigm because in order to learn on the job, the system must\ndiscover and create new tasks to learn and also acquire labeled ground-truth training data\nduring application on the fly by the agent itself. Furthermore, the agent must adapt to or\n15\n\naccommodate unknowns or novelties. Before discussing the SOLA framework, let us first\ndiscuss the classical machine learning paradigm.\n2.1\nClassical Machine Learning\nThe current dominant paradigm for machine learning (ML) is to run an ML algorithm on a\ngiven dataset to generate a model. The model is then applied on a real-life performance task.\nThis is true for both supervised learning and unsupervised learning. We call this paradigm\nclosed-world learning because it makes the independent and identically distributed (I.I.D)\nassumption and it does not consider any other related information or the previously learned\nknowledge. Intuitively, by closed-world learning [Fei and Liu, 2016, Liu, 2020, Fei et al.,\n2016], we mean what the agent sees in testing or application have been seen in training (we\nwill discuss this further later).\nFigure 2.1 illustrates the classical isolated learning paradigm, which consists of two\nstages: (1) model building and (2) model application or deployment. In model building,\nthe training data D of task T is used by the Learner (a ML algorithm) to produce a\nModel. This process is reflected by the blue links in the figure. In model application or\ndeployment, the input data sensed by the Sensor from the application is sent to the Model,\nwhich produces a decision or action to be executed by the Executor in the application\nenvironment. This process is reflected by the black links. This classical learning paradigm\nhas several limitations.\nFigure 2.1: Architecture of the classical machine learning paradigm, where T is the task\nand D is its training data. The links in blue reflect the learning process and the links in\nblack reflect the application process of the learned model.\n(1). Most real-life learning environments do not satisfy the I.I.D assumption because\nthey are often dynamic and open, meaning that there may be constant data distribution\nchanges and unknown objects appearing. In such cases, periodical retraining is constantly\nneeded typically initiated by human engineers. To do retraining, manual labeling of new\nor additional training data is required, which is very labor-intensive and time-consuming.\nSince the world is too complex and constantly changing, labeling and retraining need to be\ndone periodically. This is a daunting task for human annotators and engineers. This is also\nthe case for building and maintaining a chatbot. Even for unsupervised learning, collecting\na large volume of data constantly may not be possible in many cases.\n(2). It learns in isolation, meaning that it does not retain or accumulate knowledge\nlearned in the past and use it to help future learning to achieve knowledge transfer. This\nis in sharp contrast to our human learning.\nWe humans never learn in isolation.\nWe\n16\n\naccumulate and maintain the knowledge learned from previous tasks and use it seamlessly\nin learning new tasks and solving new problems. That is why whenever we encounter a\nnew situation or problem, we may notice that many aspects of it are not really new because\nwe have seen them in the past in some other contexts. When faced with a new problem\nor a new environment, we can adapt our past knowledge to deal with the new situation\nand also learn from it. Over time we learn more and more. and become more and more\nknowledgeable and more and more effective at learning. In the context of chatbots, it is\neasy to see opportunities for knowledge transfer. For example, after learning to book a\nmeeting room, learning to reserve a table in a restaurant should be easier.\n(3). There is no learning on the job after the model has been deployed (also called\npost-deployment). Human learning is different as we continue to learn on the job after\nformal training. Studies have shown that about 70% of human knowledge is learned while\nworking on a task or on the job. Only about 10% is learned through formal training and\nthe rest 20% is learned through imitation of others. An AI system should also learn on\nthe job during model applications. In the context of chatbots, this means to learn during\nconversation with users.\nIn the next section, we use a motivating example to illustrate that this classical paradigm\nis not sufficient in many practical applications and what are involved in building an au-\ntonomous learning system that can learn after deployment on the fly. In order to be gen-\neral, the example is not mainly about dialogue systems but it involves a dialogue system.\nHowever, after presenting the SOLA framework, we will briefly discuss a lifelong learning\ndialogue system that follows the SOLA framework.\n2.2\nA Motivating Example for SOLA\nThe example is about a hotel greeting bot [Chen and Liu, 2018]. It works in an open\nenvironment full of unknowns or novel objects, which the classical machine learning cannot\nhandle. In general, to make an AI agent such as the greeting bot to thrive in the real open\nworld, like humans, it has to detect novelties and learn them incrementally to make the\nsystem more knowledgeable and adaptable over time. It must do so on its own initiative\non the job (after deployment) rather than relying on human engineers to retrain the system\noffline periodically. That is, it must learn in the open world in a self-motivated manner in\nthe context of its performance task (the main task of the agent).\nOur hotel greeting bot’s performance task is to greet hotel guests.\nWhen its vision\nsystem sees a guest (say, John) it has learned before, it greets him by saying,\n“Hi John, how are you today?”\nWhen it sees a new guest, it should detect this guest as new or novel. This is a novelty\ndetection problem (also known as out-of-distribution (OOD) detection). Upon discovering\nthe novelty, the new guest, it needs to accommodate or adapt to the novel situation. The\nbot may say to the new guest\n“Hello, welcome to our hotel! What is your name, sir?”\n17\n\nIf the guest replies “David,” the bot takes some pictures of the guest to gather training\ndata and then incrementally or continually learn to recognize David.\nThe name\n“David” serves as the class label of the pictures taken. Like humans, the detected novelty\nserves as an intrinsic self-motivation for the agent/bot to learn. When the bot sees this\nguest again next time, it may say\n“Hi David, how are you today?”\n(David is no longer novel)\nIn an actual hotel, the situation is, however, much more complex than this. For example,\nhow does the system know that the novel object is actually a person, not a dog? If the\nsystem can recognize the object as a person, how does it know that he/she is a hotel guest,\nnot a service provider for services such as delivery or security? In order to adapt to the\nnovel object or situation, the system must first characterize the novel object, as without\nit, the agent does not know how to adapt or respond. In this case, some classification\nor similarity comparison is needed to decide whether it is a person with luggage. If the\nobject looks like a person but has no luggage, the bot will not respond or learn to recognize\nthe person as it is irrelevant to its performance task. If the novel object looks like an\nanimal, it should notify a hotel employee and learn to recognize the object so that it will\nno longer be novel when it is seen next time. In short, for each characterization, there is\na corresponding response or adaptation strategy, which can be NIL (i.e., do nothing). This\ndiscussion shows that in order to characterize, the agent must already have a rich volume of\nworld knowledge. Last but not least, there is also risk involved when making an incorrect\ndecision.\nThe proposed SOLA framework is exactly for dealing with this complex learning envi-\nronment. Since novelty detection and continual learning are two key issues in SOLA and\nextensive research has been done about them in the research community, we will review\nthem first.\n2.3\nNovelty Detection\nNovelty detection is also called out-of-distribution (OOD) detection, open-world classifica-\ntion or open-set recognition. It is similar to or the same as the earlier outlier detection and\nanomaly detection depending on contexts. Novelty is an agent-specific concept. An object\nmay be novel to one agent based on its partial knowledge of the world but not novel to\nanother agent. We distinguish two types of novelty, absolute novelty and contextual novelty.\nAbsolute novelty. Absolute novelty represents something that the agent has never seen\nbefore. For example, in the context of supervised learning, the agent’s world knowledge\nis learned from the training data Dtr = {(xi, yi)}n\ni=1 with xi ∈X and yi ∈Ytr.\nLet\nh(x) be the latent or internal representation of x in the agent’s mind, h(Di\ntr) be the latent\nrepresentation of the training data of class yi, and k (= |Ytr|) be the total number of training\nclasses. We use µ(h(x), h(Di\ntr)) to denote the novelty score of a test instance x with respect\nto h(Di\ntr). The degree of novelty of x with respect to Dtr, i.e., µ(h(x), h(Dtr)), is defined\n18\n\nas the minimum novelty score with regard to every class,\nµ(h(x), h(Dtr)) = min(µ(h(x), h(D1\ntr)), ..., µ(h(x), h(Dk\ntr)))\n(2.1)\nThe novelty function µ can be defined based on specific applications. For example, if\nthe training data of each class follows the Gaussian distribution, one may use the distance\nfrom the mean to compute the novelty score.\nNovel instance: A test instance x is novel or out-of-distribution (OOD) if its novelty\nscore µ(h(x), h(Dtr)) is greater than or equal to a threshold value γ such that x can be\nassigned a new class that is not in Ytr.\nNovel class: A newly created class ynew (ynew /∈Ytr) assigned to some novel instances is\ncalled a novel class (or out-of-distribution (OOD), unknown or unseen class). The classes\nin Ytr are called in-distribution (IND), known or seen classes.\nContextual novelty. Based on the prior knowledge of the agent, the probability P(x|Q)\nof x occurring in a particular context Q is very low, but x has occurred in Q, which is\nsurprising or unexpected. Both x and Q are not absolutely novel as they separately have\nbeen seen before. A contextual novelty is also commonly called a surprise or unexpected\nevent. In human cognition, surprise is an emotional response to an instance which greatly\nexceeds the expected uncertainty within the context of a task. The definitions of contextual\nnovel instance and class are similar to those for absolute novelty.\nNovelty is not restricted to the perceivable physical world but also includes the agent’s\ninternal world, e.g., novel interpretations of world states or internal cognitive states that\nhave no correspondence to any physical world state. Interested readers may also read [Boult\net al., 2021] for a more nuanced and perception-based study of novelty.\nOutlier and anomaly: An outlier is a data point that is far away from the main data\nclusters, but it may not be unknown. For example, the salary of a company CEO is an\noutlier with regard to the salary distribution of the company employees, but it is known\nand thus not novel. Unknown outliers are novel. Anomalies can be considered as outliers\nor instances that are one off and never repeated. Though technically “novel” they may not\nneed to result in a new class.\nNote that this book does not deal with various types of data shift such as covariate shift,\nprior probability shift and concept drift as a large amount of work has been done [Moreno-\nTorres et al., 2012].\nWe will not discuss novelty detection further because it has been studied extensively in\nthe literature. Several excellent surveys exist [Pang et al., 2021, Parmar et al., 2021, Yang\net al., 2021].\n2.4\nLifelong and Continual Learning\nHuman brains have this extraordinary ability to learn a large number of tasks incrementally\nwith high accuracy. Both the learning process of and the learned knowledge for the tasks\nhave little negative interference of each other. In fact, the learned knowledge earlier can\neven help the learning of new tasks later. Continual learning or lifelong learning attempts\n19\n\nto make the computer to do the same. The concept of lifelong learning (LL) was proposed\naround 1995 in [Thrun and Mitchell, 1995].\nSince then, it has been pursued in several\ndirections, e.g., lifelong supervised learning [Chen and Liu, 2018], continual learning in\ndeep neural networks [Chen and Liu, 2018], lifelong unsupervised learning [Chen and Liu,\n2014], lifelong semi-supervised learning [Mitchell et al., 2018], and lifelong reinforcement\nlearning [Ammar et al., 2015]. LL techniques working in other areas also exist. Silver et al.\n[2013] wrote an excellent survey of early LL approaches. A more complete treatment of LL\ncan be found in [Chen and Liu, 2018].\nThe terms lifelong learning and continual learning have the same meaning and are used\ninterchangeably now, but the past research under the two names has focused on different\naspects of the same problem.\nThe early definition of lifelong learning (LL) in Chen and Liu [2018] as follows, which is\nbased on the early definitions in [Thrun and Mitchell, 1995, Silver et al., 2013, Ruvolo and\nEaton, 2013, Mitchell et al., 2018]:\nLifelong learning: At any time point, the learner has learned a sequence of N tasks,\nT1, T2, . . . , TN. When faced with the (N + 1)th task TN+1, the learner can leverage the\nknowledge learned in the past in the knowledge base (KB) to help learn TN+1. KB maintains\nthe knowledge learned from the previous N tasks. After the completion of learning TN+1,\nKB is updated with the knowledge gained from learning TN+1.\nWe can see the goal of the earlier lifelong learning is to leverage the knowledge learned in\nthe past to learn the new task TN+1 better, i.e., knowledge transfer. An implicit assumption\nof LL is that the tasks learned are very similar [Chen and Liu, 2018]. The learning setting\nis almost exclusively the task continual learning (TCL) setting (see below), where each task\nis a separate problem.\nContinual learning: The term continual learning (CL) is more commonly used than\nlifelong learning in the deep learning community. Although knowledge transfer is also a goal\nof CL, the focus of CL has been on solving the catastrophic forgetting (CF) problem [Rusu\net al., 2016, Kirkpatrick et al., 2017, Zenke et al., 2017, Shin et al., 2017, Serra et al., 2018,\nLee et al., 2019, Chaudhry et al., 2020, Guo et al., 2022, Ke et al., 2020, Kim et al., 2022].\nCF refers to the phenomenon that when a neural network learns a sequence of tasks, the\nlearning of each new task is likely to change the weights or parameters learned for previous\ntasks, which degrades the model accuracy for the previous tasks [McCloskey and Cohen,\n1989].\nIn the past few years, CF has attracted a great deal of research attention [Chen and\nLiu, 2018]. There are two main setups in continual learning: class continual learning (CCL)\n(also called class incremental learning (CIL)) and task continual learning (TCL) (also called\ntask incremental learning (TIL) [Van de Ven and Tolias, 2019].\nClass continual learning (CCL). In CCL, each task consists of one or more classes\nto be learned together but only one model is learned to classify all classes that have\nbeen learned so far. In testing, a test instance from any class may be presented to the\nmodel for it to classify with no task information given.\nFormally, given a sequence of\ntasks T1, T2, ..., TN, TN+1, ... and their corresponding datasets D1, D2, ..., DN, DN+1, .... The\n20\n\ndataset of task Tk is Dk = {(xi\nk, yi\nk)nk\ni=1}, where nk is the number of data samples in task\nk, and xi\nk ∈X is an input sample and yi\nk ∈Yk is its class label. All Yk’s are disjoint and\nST\nk=1 Yk = Y, where T is the index of the last task TT that has been learned. The goal\nof CCL is to learn a single prediction function or model f : X →Y that can identify the\ntarget class y for a given test instance x.\nTask continual learning (TCL). In TCL, each task is a separate classification problem\n(e.g., one classifying different breeds of dogs and one classifying different types of birds).\nTCL builds a set of classification models (one per task) in one neural network. In testing, the\nsystem knows which task each test instance belongs to and uses only the model for the task\nto classify the test instance. Note that classical LL mainly works in this TCL setting and\nassumes that the tasks are similar to each other to enable knowledge transfer across tasks.\nFormally, given a sequence of tasks T1, T2, ..., TN, TN+1, ... and their corresponding datasets\nD1, D2, ..., DN, DN+1, .... Each task Tk has a training dataset Dk = {((xi\nk, k), yi\nk)nk\ni=1}, where\nnk is the number of data samples in task Tk ∈T = {T1, T2, ..., TT }, where TT is the last\ntask that has been learned, and xi\nk ∈X is an input sample and yi\nk ∈Yk ⊂Y is its class\nlabel. The goal of TIL is to construct a predictor f : X × T →Y to identify the class label\ny ∈Yk for (x, k) (the given test instance x from task k).\nFrom now on, we will only use the term continual learning (CL) to mean both lifelong\nlearning and continual learning (LL). The goal of CL is to achieve two main objectives,\ni.e., (1) overcoming CF and (2) performing cross task knowledge transfer. Clearly, not all\nproblems can achieve both. For example, it is not obvious that different tasks or classes\ncan help each other in CCL except some feature sharing. For TCL, if the tasks are entirely\ndifferent, it is hard to improve the new task learning via knowledge transfer either. For\nexample, one task is to classify whether one has a heart disease or not but another is to\nclassify whether a loan application should be approved or not. In these cases, CF is the only\nproblem to solve. Recent research has shown that when a mixed sequence of similar and\ndissimilar tasks are learned in TCL, it is possible to perform selective knowledge transfer\namong similar tasks [Ke et al., 2020] and also to overcome CF for dissimilar tasks. Task\nsimilarity is detected automatically.\nThe architecture of CL systems is given in Figure 2.2.\nApart from those standard\ncomponents and processes of classical machine learning as explained in Section 2.1, this\nnew paradigm has some additions and changes.\n1. C-Learner: C-Learner means Continual-Learner.\nFor CL, it is beneficial for the\nlearner to use prior knowledge in learning the new task. We also call such a learner\na knowledge-based learner, which can leverage the knowledge in the KB to learn the\nnew task. The knowledge relevant to the current is mined by the Task Knowledge\nMiner (TKM) (see below). C-Learner will deal with CF in learning a new task.\n2. Knowledge Base and World Model (KB): It stores the previously learned knowl-\nedge and how the world related to the application works. It may also have a few\nsub-components:\n21\n\nFigure 2.2: Architecture of a traditional lifelong learning framework.\nT1, ..., TN are the\npreviously learned tasks, TN+1 is the current new task to be learned and DN+1 is its\ntraining data. The C-Learner (Continual Learner) learns by leveraging the relevant prior\nknowledge identified by the Task-based Knowledge Miner from the Knowledge Base (KB),\nwhich contains the retained knowledge in the past.\nIt also deals with the catastrophic\nforgetting.\n(a) Past Information Store (PIS): It stores the information resulted from the past\nlearning, including the resulting models, patterns, or other forms of outcome.\nAs for what information or knowledge should be retained, it depends on the\nlearning task and the learning algorithm. For a particular system, the user needs\nto decide what to retain in order to help future learning. For example, in the\nexperience-replay based CL approach, a small number of training examples from\neach previous task is saved so that they can be used to help deal with CF in\nlearning a new task.\n(b) Knowledge Miner (MKM). It performs meta-mining of the knowledge in the PIS.\nWe call this meta-mining because it mines higher-level knowledge from the saved\nknowledge. The resulting knowledge is stored in the Meta-Knowledge Store. Here\nmultiple mining algorithms may be used to produce different types of results.\n(c) Meta-Knowledge Store (MKS): It stores the knowledge mined or consolidated\nfrom PIS (Past Information Store) and also from MKS itself.\nSome suitable\nknowledge representation schemes are needed for each application.\n(d) Knowledge Reasoner (KR): It makes inference based on the knowledge in MKB\nand PIS to generate more knowledge. Most current systems do not have this\nsub-component. However, with the advance of CL, this component will become\nincreasingly important.\nSince the current CL research is still in its infancy, none of the existing systems has\n22\n\nall these sub-components.\n3. Task-based Knowledge Miner (TKM): This module makes use of the raw knowl-\nedge or information in the KB to mine or identify knowledge that is appropriate for\nthe current task. This is needed because in many cases, C-Learner cannot directly\nuse (all) the raw knowledge in the KB. For example, the irrelevant knowledge to the\ncurrent task needs to be filtered out or blocked. And, C-Learner may only be able to\nuse some more general knowledge mined from the KB [Chen and Liu, 2014, Ke et al.,\n2020].\n4. Model: This is the learned model, which can be a prediction model or classifier in su-\npervised learning, clusters or topics in unsupervised learning, a policy in reinforcement\nlearning, etc. Since the architecture enables continual learning, ModelN+1 includes\nall the models from T1 to TN+1, which may all be in one neural network. In the case\nof TCL, they may be separate models indexed by their task identifiers. In the case of\nCCL, ModelN+1 is just one model that covers all classes of the tasks learned so far.\n5. Task Manager (TM): It receives and manages the tasks that arrive in the system,\nand handles the task shift and presents the new learning task to the C-Learner in a\nlifelong manner.\nContinual Learning Process: A typical continual learning process starts with the Task\nManager assigning a new task to the C-Learner. C-Learner then works with the help of the\npast knowledge stored in the KB to produce an output model for the user and also send\nthe information or knowledge that needs to be retained for future use to the KB. Note that\ndealing with CF is not reflected in the architecture in Figure 2.2 as it stays in the algorithm\nof the C-Learner.\nMain Continual Learning Approaches. Most recent research papers on CL focus on\novercoming CF by protecting what the system has learned previously. There are a large\nnumber of existing approaches. They can be roughly grouped into three main categories.\nRegularization-based approaches: The main idea of these approaches is to compute the\nimportance of each parameter or gradient to the previously learned tasks, and then add\na regularization in the loss function to penalize changes to those important parameters\nto prevent CF on previous tasks [Jung et al., 2016, Camoriano et al., 2017, Lee et al.,\n2019, Schwarz et al., 2018, Ahn et al., 2019, Zhu et al., 2021, Dhar et al., 2019].\nFor\nexample, EWC [Kirkpatrick et al., 2017], one of the most popular algorithms, uses the\nFisher information matrix to represent the importance of parameters. This technique works\nto some extent, but is very weak at dealing with CF. SI [Zenke et al., 2017] was proposed to\nextend EWC so that it has less complexity in computing the penalty. Several approaches\n[Li and Hoiem, 2017, Buzzega et al., 2020] also use knowledge distillation [Hinton et al.,\n2015] to preserve the previous knowledge.\nReplay-based approaches: The idea of these approaches is to use a small memory buffer\nto save a small amount of data from previous tasks and replay or use them to jointly train\n23\n\nthe new task together with the new task data to prevent forgetting of the knowledge learned\nfrom previous tasks [Rusu et al., 2016, Lopez-Paz and Ranzato, 2017, Rebuffi et al., 2017,\nChaudhry et al., 2019, Rolnick et al., 2019, Rajasegaran et al., 2019, Liu et al., 2021, Cha\net al., 2021, Buzzega et al., 2020]. This approach is also called experience replay or the\nmemory-based approach. Example systems include GEM [Lopez-Paz and Ranzato, 2017]\nand A-GEM [Chaudhry et al., 2019]. Instead of saving some previous data, some approaches\nlearn a data generator for previous tasks [Gepperth and Karaoguz, 2016, Kamra et al., 2017,\nShin et al., 2017, Seff et al., 2017, Kemker and Kanan, 2018, Hu et al., 2019b, Ostapenko\net al., 2019]. In learning a new task, the generator generates pseudo-samples of previous\ntasks and uses them instead of real samples to jointly train the new task.\nParameter isolation-based (or architectural approaches: These approaches are mainly\nused in task-incremental learning (TIL). Its main idea is to learn a sub-network for each\ntask, and tasks may share some parameters and neurons [Serra et al., 2018, Ke et al., 2020].\nHAT [Serra et al., 2018] and SupSup [Wortsman et al., 2020] are two representative systems.\nHAT learns neurons (not parameters) that are important for each task and “hard masks”\nthem via the task embeddings based on gates. SupSup uses a different approach but it\nalso learns and fixes a sub-network for each task. Besides them, many other systems also\ntake similar approaches, e.g., Progressive Networks [Rusu et al., 2016], PackNet [Mallya\nand Lazebnik, 2018], HyperNet [von Oswald et al., 2020], and BNS [Qin et al., 2021].\nSeveral recent works have also attempted to deal with knowledge transfer and catas-\ntrophic forgetting (CF) together. For example B-CL [Ke and Xu, 2021] and CTR [Ke et al.,\n2021a] train a shared adapter for all tasks in a pre-trained language model. They prevent\nCF via task masks [Serra et al., 2018] (parameter isolation) and achieve knowledge transfer\nvia capsule networks [Sabour et al., 2017]. CLASSIC [Ke et al., 2021b] achieves knowledge\ntransfer via contrastive learning. CAT [Ke et al., 2020] learns a sequence of similar and\ndissimilar tasks and it also deals with both CF and knowledge transfer.\nLimitations. One key limitation of the existing CL paradigm is that the tasks and their\ntraining data are given by the user. This means that the system is not autonomous and\ncannot learn by itself. In order to do that, we extend the CL architecture in Figure 2.2 to\nenable open-world on-the-job learning to achieve full SOLA.\n2.5\nThe SOLA Framework\nThe SOLA architecture is given in Figure 2.3, which adds the orange-colored links and\nassociated components to the continual learning architecture in Figure 2.2. These newly\nadded links and components enable the system to learn by itself to achieve autonomy, which\nis what SOLA aims to achieve. It is called learning after deployment or learning on\nthe job during application or after model deployment.\nLearning after deployment refers to learning after the model has been deployed in an\napplication or during model application [Liu and Mazumder, 2021] on the fly. The basic\nidea is that during application, if the system/agent encounters anything that is out-of-\n24\n\nFigure 2.3: Architecture of the primary task performer or any supporting function. OWC-\nLearner means Open-World Continual Learner.\ndistribution (OOD) or novel, it needs to detect the novelty. Based on the novelty, the\nsystem creates a new task to learn and also acquires the ground-truth training data\nto learn the task on the initiation of the system itself through interactions with humans and\nthe environment. The system then learns the new task incrementally or continually.\nThe whole process is carried out on the fly during application.\n2.5.1\nComponents of SOLA\nSOLA is proposed as a framework for autonomous AI agents. An AI agent consists of a pair\nof key modules (P, S), where P is the primary task-performer that performs its performance\ntask (e.g., the dialogue system of the greeting bot) and S is a set of supporting or peripheral\nfunctions (e.g., the vision system and the speech system of the bot) that supports the\nprimary task-performer. The primary task-performer P or each supporting function Si ∈S\nconsist of eight core sub-systems (L, M, K, R, C, A, S, I). Figure 2.3 shows the relationships\nand functions of the sub-systems. We do not distinguish P and Si in terms of techniques\nor subsystems as we believe they have no fundamental difference.\n• L is an OWC-Learner (Open-World Continual Learner) that builds models to not\nonly classify the input into known classes but also detect novel objects that have\nnot been seen in training. For example, for the greeting bot, L of the primary task\nperformer P is a continual learning dialogue system similar to that in Section 2.6.\n25\n\nFor the supporting vision system, L is a continual learner that can learn to recognize\nguests and detect novel or unknown objects. Compared to C-Learner in continual\nlearning in Figure 2.2, OWC-Learner in SOLA not only can learn continually like\nC-Learner but also produce models that can detect novel instances in testing or in\napplication deployment.\n• K is the Knowledge Base & World Model (KB) that is important for the perfor-\nmance task, supporting functions or the OWC-Learner. KB is also in Figure 2.2 but\nplays more roles in SOLA. Apart from keeping the learned or prior knowledge of the\ndomain and the world model, if needed, reasoning capability may also be provided to\nhelp the other modules of the system (see the orange-colored links). Some knowledge\nfrom the application observed by the Adapter (see below) may be added to the KB,\nwhich can provide some knowledge to the Model for its decision making. World model\nrefers to the representation of the task environment and the commonsense knowledge\nabout the objects and their relationships within.\n• M is the Model learned by L. M takes the input or perception signals from the\napplication environment to make a decision to perform actions in the application. M\nmay also use some input or knowledge from other supporting functions.\n• R is the Relevance Module or focusing mechanism that decides whether the de-\ntected novelty is relevant to the current task or not. If it’s relevant, the agent should\nrespond to the novelty (discussed below); otherwise ignore it. For example, in the\ngreeting bot application, when the bot hears something from people who are chat-\nting with each other, whether understandable or not, it will ignore them as they are\nirrelevant to its performance task.\n• C is the Novelty Characterizer that characterizes the detected novelty based on\nthe knowledge in the KB so that the adaptor (below) can formulate a course of actions\nto respond or adapt to the novelty. For the characterizer C of P of the greeting bot,\nas P is a dialogue system, when it cannot understand the utterance of a hotel guest (a\nnovelty), it should decide what it can and cannot understand (see Section 2.6) and ask\nthe guest based on its partial understanding (see below). In the case of the supporting\nvision system, when a novel object it detected, the charaterizer may decide what the\nobject looks like and its physical attributes. For example, the novel object may look\nlike a dog based on the greeting bot’s KB (see Section 2.5.4 for more discussions).\n• A is the Adaptor that adapts to or accommodates the novelty based on the char-\nacterization result. It is a planner that produces a plan of actions for the executor\nE or the interactive module I to perform. Given the characterization (e.g., partial\nunderstanding) above, A may adapt by asking the guest to clarify (see Section 2.6)\nand then learn to understand the utterance. In the case of the vision system, if the\ncharacterizer believes that the novel object looks like a dog, the adaptor may decide\nto report to a hotel employee and then learns the new object by taking some pictures\n26\n\nas the training data and asking the hotel employee for the name of the object as the\nclass label. In the latter two cases, A needs to invoke I to interact with the human\nand L to learn the novelty so that it will not be novel in the future. That is, A is\nalso responsible for creating new tasks (e.g., learning to recognize new objects by the\ngreeting bot) on the fly and proceeds to acquire ground truth training data with the\nhelp of I (discussed below) to be learned by L. This adaptation process often involves\nreasoning.\n• S is the Risk Assessment module. Novelty implies uncertainty in adapting to the\nnovel situation.\nIn making each response decision, risk needs to be assessed (see\nSection 2.5.5 for more discussions).\n• I is the Interactive Module for the agent to communicate with humans or other\nagents, e.g., to acquire ground-truth training data or to get instructions when the\nagent does not know what to do in a unfamiliar situation. It may use the natural\nlanguage (for interaction with humans) or an agent language (for interaction with\nother agents).\nSeveral remarks are in order. First, not all agents need all these sub-systems and some\nsub-systems may also be shared. For example, the primary task performer P in the greeting\nbot application is a dialogue system. Its interaction module I can use the same dialogue\nsystem. In some cases, the Model may also be able to determine the relevance of a novel\nobject to the application and even characterize the novelty because characterization in many\ncases is about classification and similarity comparison. Second, as we will see, every sub-\nsystem can and should have its own local learning capability. Third, the interaction module\nI and the adapter A will create new tasks to learn and gather ground truth training data\nfor learning. Fourth, most links in Figure 2.3 are bidirectional, which means that the sub-\nsystems need to interact with each other in order to perform their tasks. The interactions\nmay involve requesting for information, passing information, and/or going back and forth\nwith hypothesis generation and hypothesis evaluation to make more informed decisions.\nSince the primary task performer P and each supporting sub-system Si has the same\ncomponents or sub-systems, we will discuss them in general rather than distinguishing them.\nBelow, we first discuss the SOLA learner L.\n2.5.2\nOpen World Continual Learning\nAs discussed earlier, the classical ML makes the i.i.d assumption, which is often violated in\npractice. Here we first define some concepts and then present the paradigm of open-world\ncontinual learning (OWC-Learning).\nLet the training data that have been seen so far from previous tasks be Dtr = {(xi, yi)}n\ni=1\nwith xi ∈X and yi ∈Ytr. Let the set of class labels that may appear in testing or appli-\ncation be Ytst. Classical ML makes the closed-world assumption.\nClosed-world assumption: There are no new or novel instances or classes that may\nappear in testing or application, i.e., Ytst ∈Ytr. In other words, every class seen in testing\n27\n\nor application must have been seen in training.\nOpen world: There are test classes that have not been seen in training, i.e., Ytst−Ytr ̸=\n∅.\nDefinition (closed-world learning): It refers to the learning paradigm that makes\nthe closed-world assumption.\nDefinition (open world learning (OWL)): It refers to the learning paradigm that\nperforms the following functions: (1) classify test instances belonging to training classes\nto their respective classes and detect novel or out-of-distribution instances, and (2) learn\nthe novel classes labeled by humans in the identified novel instances to update the model\nusing the labeled data. The model updating is initiated by human engineers and involves\nre-training or incremental learning.\nDefinition (open-world continual learning (OWC-learning)): OWC-learning is\nthe learning paradigm that performs open-world learning but the learning process is initiated\nby the agent itself after deployment with no involvement of human engineers. The new task\ncreation and ground-truth training data acquisition are done by the agent via its interaction\nwith the user and the environment. The learning of the new task is incremental, i.e., no\nre-training of previous tasks/classes. The process is lifelong or continuous, which makes the\nagent more knowledgeable over time.\nSteps in OWC-Learning\nThe main process in OWC-Learning involves the following three steps, which can be re-\ngarded as part of the novelty adaptation or accommodation strategy (see Section 2.5.4).\nStep 1 - Novelty detection. This step involves detecting data instances whose classes do\nnot belong to Ytr. As mentioned earlier, a fair amount of research has been done on this\nunder open-set classification or out-of-distribution (OOD) detection [Pang et al., 2021].\nStep 2 - Acquiring class labels and creating a new learning task on the fly: This step first\nclusters the detected novel instances. Each cluster represents a new class. Clustering may\nbe done automatically or through interaction with humans using the interaction module\nI. Interacting with human users should produce more accurate clusters and also obtain\nmeaningful class labels. If the detected data is insufficient to learn an accurate model to\nrecognize the new classes, additional ground-truth data may be collected via interaction\nwith human users (and/or passively by downloading data from web, like searching and\nscrapping images of objects of a given class). A new learning task is created.\nIn the case of our hotel greeting bot, since the bot detects a single new guest (automat-\nically), no clustering is needed. It then asks the guest for his/her name as the class label.\nIt also takes more pictures as the training data. With the labeled ground-truth data, a new\nlearning task is created to incrementally learn to recognize the new guest on the fly.\nThe learning agent may also interact with the environment to obtain training data. In\nthis case, the agent must have an internal evaluation system that can assign rewards\nto different states of the world, e.g., for reinforcement learning.\nStep 3 - Incrementally learn the new task. After ground-truth training data has been ac-\nquired, the OWC-Learner L incrementally learns the new task. This is continual learning.\n28\n\nWe will not discuss it further as there are already numerous existing techniques (see Sec-\ntion 2.4). Many can leverage existing knowledge to learn the new task better [Chen and\nLiu, 2018].\nAcquiring Training Data Automatically\nExisting approaches to obtaining the training data is through manual labeling or writing (in\nthe case of dialogue data), which is both costly and time-consuming. In the case of chatbots,\nas they typically work in multi-user environments, we can exploit such environments to\nacquire the ground truth training data interactively during actual online conversations.\nThis process is both automatic and free. We have discussed this issue in Section 1.4 and\nwe briefly reproduce it here for completeness of this section.\n1. Extracting data or information directly from user utterances (or dialogue\nhistory), which can be real-world facts, user preferences, etc. Additional knowledge/data\nmay be inferred from the acquired and existing KB knowledge.\n2. Asking the current user when the agent (1) doesn’t understand a user utterance,\nor (2) cannot answer a user query, which forms a new learning task. To obtain the ground\ntruth data, for (1), the agent may ask the current user for clarification or rephrasing. For (2),\nit may ask the user for some supporting facts and then infer the query answer [Mazumder\net al., 2019, 2020a]. In this process, it obtains command-action pairs, question-answer pairs,\netc.\n3.\nAsking other users to obtain the answers when the chatbot could not answer\na user query. For example, if a user asks “What is the capital city of the US?” and the\nchatbot is unable to answer or infer now, it can try to find a good opportunity to ask\nanother user later “Hey, do you happen to know what the capital city of the US is?” If the\nuser gives the answer “it’s Washington DC,” the agent acquires the ground truth (a piece\nof new knowledge) which can be used in its future conversations or as a piece of training\ndata.\n4. Observing user demonstrations if supported. See Section 1.4 and also [Mazumder\net al., 2020b].\nBeyond these, the agent may also extract ground-truth data from online documents or\nonline knowledge bases.\n2.5.3\nRelevance of Novelty\nAs an AI agent has a performance task, it should focus on novelties that are critical to\nthe performance task.\nFor example, a self-driving car should focus on novel objects or\nevents that are or may potentially appear on the road in front of the car. It should not\npay attention to novel objects in the shops along the street (off the road) as they do not\naffect driving. This relevance check involves gathering information about the novel object\nto make a classification decision. As this is a normal classification task, it is not discussed\nfurther.\n29\n\n2.5.4\nNovelty Characterization and Adaptation\nIn a real-life application, classification may not be the primary task of the agent.\nFor\nexample, in a self-driving car, object classification or recognition supports its primary per-\nformance task of driving. To drive safely, the car has to take some actions to adapt or\nrespond to any novel/new objects, e.g., slowing down and avoiding the objects. In order to\nknow what actions to take to adapt, the agent must characterize the new object. The char-\nacterization of a novel object is a partial description of the object based on the agent’s\nexisting knowledge about the world. According to the characterization, appropriate actions\nare formulated to adapt or respond to the novel object. The process may also involve\nlearning.\nNovelty characterization and adaptation (or response) form a pair (c, r), where c is the\ncharacterization of the novelty and r is the adaptation response to the novelty, which is a\nplan of dynamically formulated actions based on the characterization of the novelty. The\ntwo activities go hand-in-hand. Without an adaptation strategy for a characterization, the\ncharacterization has little use. If the system cannot characterize a novelty, it takes a low\nrisk-assessed default response.\nIn our greeting bot example, when it can characterize a\nnovelty as a new guest, its response is to say ”Hello, welcome to our hotel! What is your\nname, sir?” If the bot has difficulty with characterization, it can take a default action,\ne.g., ‘do nothing.’ The set of responses are specific to the application. For a self-driving\ncar, the default response to a novel object is to slow down or stop the car so that it will\nnot hit the object. In some situations, the agent must take an action under low confidence\ncircumstances, e.g., the agents engage in reinforcement learning, trying actions and assessing\noutcomes.\nCharacterization can be done at different levels of detail, which may result in more or less\nprecise responses. Based on an ontology and object attributes related to the performance\ntask in the domain, characterization can be described based on the type of the object and\nthe attribute of the object.\nFor example, in the greeting bot application, it is useful to\ndetermine whether the novel object is a human or an animal because the responses to them\nare different.\nFor self-driving cars, when sensing a novel object on the road, it should\nfocus on those aspects that are important to driving, i.e., whether it is a still or a moving\nobject. If it is a moving object, the car should determine its direction and speed of moving.\nThus, classification of movement is needed in this case to characterize the novelty, which,\nin turn, facilitates determination of the agent’s responding action(s). For instance, if the\nnovel object is a mobile object, the car may wait for the object to leave the road before\ndriving.\nAnother characterization strategy is to compare the similarity between the novel object\nand the existing known objects. For example, if it is believed that the novel object looks\nlike a dog (assuming the agent can recognize a dog), the agent may react like when it sees\na dog on the road.\nThe above discussion implies that in order to effectively characterize a novelty, the agent\nmust already have a great deal of world knowledge that it can use to describe the novelty.\nAdditionally, the characterization and response process is often interactive in the sense that\n30\n\nthe agent may choose a course of actions based on the initial characterization. After some\nactions are taken, it may get some feedback from the environment. Based on the feedback\nand the agent’s additional observations, the course of actions may change.\nLearning to respond. In some situations, the system may not know how to respond\nto a novel object or situation. It may try one or more of the following ways.\n(1) Asking a human user. In the case of the self-driving car, when it does not know\nwhat to do, it may ask the passenger using the interactive module I in natural language\nand then follow the instruction from the passenger and also learn it for future use. For\nexample, if the car sees a black patch on the road that it has never seen before, it can ask\n“what is that black thing in front?” The passenger may answer “that is tar.” If there is no\nready response, e.g., no prior information on tar, the system may progress with a further\ninquiry, asking the passenger “what should I do?”\n(2) Imitation learning. On seeing a novel object, if the car in front drives through it\nwith no issue, the car may choose the same course of action as well and also learn it for\nfuture use if the car drives through without any problem.\n(3) Reinforcement learning. By interacting with the environment through trial and error\nexploration, the agent learns a good response policy. This is extremely challenging as any\naction taken has consequences and cannot be reversed. For this to work, the agent must\nhave an internal evaluation system that is able to assign rewards to (possible) states.\nIf multiple novelties are detected at the same time, it is more difficult to respond as the\nagent must reason over the characteristics of all novel objects to dynamically formulate an\noverall plan of actions that prioritize the responses.\n2.5.5\nRisk Assessment and Learning\nThere is risk in achieving performance goals of an agent when making an incorrect decision.\nFor example, classifying a known guest as unknown or an unknown guest as known may\nnegatively affect guest impressions resulting in negative reviews.\nFor a self-driving car,\nmisidentifications can result in wrong responses, which could be a matter of life and death.\nThus, risk assessment must be done in making each decision. Risk assessment can also be\nlearned from experiences or mistakes. In the example of a car passing over tar, after the\nexperience of passing over shiny black surfaces safely many times, if the car slips in one\ninstance, the car agent must assess the risk of continuing the prior procedure. Given the\ndanger, a car may weight the risk excessively, slowing down on new encounters of shiny\nblack surfaces.\nSafety net: Another aspect of risk is safety net. If a wrong decision is made, it should\nnot cause a catastrophe. A wrong decision may be due to two reasons. The first is the\nmodel inaccuracy. For example, the perception system of a self-driving car may classify a\npedestrian wearing a black dress as a patch of tar on the surface of the road, and run over\nthe person. The second is wrong knowledge acquired from human users during human-agent\ninteraction. For example, if a self-driving car sees an unknown object (e.g., a big stone) in\nthe middle of the road and asks the passenger what it is and what to do, the passenger may\n31\n\nsay it is a stone and it is safe to drive through it. In this case, the car should not go ahead\nand hit the stone.\n2.6\nA Dialogue System based on SOLA\nWe now briefly describe a dialogue system (called CML) in [Mazumder et al., 2020b] that\nuses the SOLA framework and learns continually by itself on the fly during conversation\nwith users to become more and more powerful. Details will appear in Section 4.2. Another\ntwo related systems [Mazumder et al., 2019, 2020a] can be found in Section 3.4, which are\nmore sophisticated than CML.\nCML is a natural language interface system like Amazon Alexa and Apple Siri. Its\nperformance task is to take a user command in natural language (NL) and perform the\nuser requested API action in the underlying application, e.g., “switch off the light in the\nbedroom.” Since CML is a text-based research system, it does not have any other support\nfunctions or sub-systems. The key issue of CML is to understand paraphrased NL commands\nfrom the user in order to map a user command to a system’s API call. The Model of CML\nconsists of two parts. The first part is a store S of seed commands (SCs). Each SC in S is\nsimilar to an actual user command except that the arguments used by the associated API\nfunction is replaced by variables, e.g., “switch off the light in the X,” where X is the variable\nrepresenting the argument of the API function for this command. A SC is either prepared by\nthe API developer before the system is deployed or continually learned from users on the fly\nduring conversation. The second part is a command matching system that matches a user\ncommand to a saved SC in S. If no match can be found, it means an unexpected or novel\ncommand from the user is detected, i.e., a novelty, which equates to the system’s failure\nin understanding or grounding a user command. The novelty detection task is performed\nby the Model. CML assumes that every novelty is relevant to the performance task,\nbut if needed, a Relevance Module can be built as a classifier to classify whether a user\ncommand is relevant or not to the application.\nNovelty characterization of CML is\nalso performed by the Model. In the matching process, CML tries to identify the part\nof the user command that the system does not understand and how similar it is to some\nknown SCs in S. Based on the characterization, the Adaptor dynamically formulates an\ninteraction strategy based on the context (e.g., dialogue history, command from the user,\ninformation acquired from user and outstanding information needed to complete a task,\netc.) and executes the strategy to carry out multi-turn dialogues for knowledge acquisition\n(i.e, obtaining the ground truth data from the user). Specifically, in CML, it decides when\nto ask and what to ask the user, to achieve two goals: (1) understand the user command so\nthat it can perform the correct API action for the user, and (2) create a new task and use\nthe correct API action as the ground-truth for the user command for learning by OWC-\nLearner. CML uses a rule-based system for adaptation. Note that a finite state machine\nis used in another system [Mazumder et al., 2020a]. Based on the user’s command and the\nground-truth API action, OWC-Learner learns this command incrementally by creating\na new SC and adding it to the SC store S so that the system will be able to match this and\n32\n\nsimilar commands in the future. In the adaptation or accommodation process, risk is also\nconsidered (see below). There is no separate Interaction Module because the main task\nperformer is a dialogue system, which is an interactive system itself and can serve the role\nof the Interaction Module.\nConsider the following example user command “turn off the light in the kitchen” that\nthe Model cannot match or ground to an existing SC (i.e., it does not understand). A\nnovelty is reported. The Model also characterizes the novelty by deciding which part\nof the command it can match or understand, which part it has difficulty with, and what\nknown SCs are similar to the user command. Based on the characterization result, the\nAdapter formulates actions to be taken. In this case, it provides the user a list of top-k\npredicted actions (see below) described in natural language and asks the user to select the\nmost appropriate action from the given list.\nBot: Sorry, I didn’t get you. Do you mean to:\noption-1.\nswitch off the light in the kitchen, or\noption-2.\nswitch on the light in the kitchen, or\noption-3.\nchange the color of the light?\nThe user selects the desired action (option-1). The action API [say, SwitchOffLight(arg:place)]\ncorresponding to the selected action (option-1) is retained as the ground truth action for\nthe user-issued command. The Adaptor invokes the Executor to perform the API action\nfor the user. In subsequent turns of the dialogue, the system can also ask the user ques-\ntions to acquire ground truth values associated with arguments of the selected action, as\ndefined in the API. Based on the acquired information, OPC-Learner of CML incremen-\ntally learns to map the original command “turn off the light in the kitchen” to the API\naction, SwitchOffLight(arg:place), by creating a new SC (“turn off the light in the X ”)\nand adding it to the SC store, which ensures that in the future the system will not have\nproblem understanding the related commands.\nRisk is considered in CML in two ways.\nFirst, it does not ask the user too many\nquestions in order not to annoy the user. Second, when the characterization is not confident,\nthe Adapter simply asks the user to say his/her command again in an alternative way,\ni.e., ”Can you say it again in another way?” (the new way may be easier for the system\nto understand) rather than providing a list of random options for the user to choose from.\nIf the options have nothing to do with the user command, the user may lose confidence in\nthe system.\n2.7\nComparison with Related Work\nThis section summarizes the difference between the SOLA framework and some existing\nmachine learning frameworks or paradigms. There are three machine learning paradigms\nthat are closely related to SOLA, i.e., novelty detection, open-world learning, and continual\nlearning. We compare SOLA with these three topics below.\n33\n\nNovelty Detection\nNovelty detection is also known as anomaly detection, out-of-distribution (OOD) detection,\nopen set detection, or open classification.\nExtensive research has been done on novelty\ndetection [Xu et al., 2019, Fei and Liu, 2016]. Two recent surveys of the topic can be found\nin [Yang et al., 2021, Pang et al., 2021]. Novelty detection is only the first step in SOLA.\nNovelty detection does not involve lifelong or continual learning, which is the backbone of\nSOLA, and it also does not involve novelty characterization and adaptation or the associated\nrisk assessment.\nOpen-World Learning\nSome researchers have also studied learning the novel objects after they are detected and\nmanually labeled [Bendale and Boult, 2015, Fei et al., 2016, Xu et al., 2019], which are\ncalled open-world learning.\nA survey of the topic is given in [Parmar et al., 2021].\nA\nposition paper [Langley, 2020] presented some blue sky ideas about open-world learning,\nbut it does not have sufficient details or an implemented system. SOLA differs from open-\nworld learning in many ways. The key difference that in open-world learning, the tasks\nand their trained data are given by the user or engineers. SOLA stresses self-initiation in\nlearning, which means that all the learning activities from start to end are self-motivated\nand self-initiated by the agent itself. The process involves no human engineers, which is\nimportant for AI autonomy. Furthermore, the research in open-world learning so far has\nnot involved continual learning of a sequence of tasks and thus has not dealt with the issue\nof catastrophic forgetting and knowledge transfer.\nDue to self-initiation, SOLA enables learning after the model deployment like human\nlearning on the job or while working, which has barely been attempted before. In existing\nlearning paradigms, after a model has been deployed, there is no more learning until the\nmodel is updated or retrained on the initiation of human engineers.\nContinual Learning\nContinual learning (CL) aims to learn a sequence of tasks incrementally. However, existing\nresearch assumes that the tasks and their training data are given by users or engineers.\nSOLA differs from CL in two main ways.\n(1). SOLA includes modules to characterize and adapt to novel situations so that the\nagent can work in the open world environment without stopping. These are not included\nin continual learning.\n(2). SOLA makes learning autonomous and self-initiated. This involves online interac-\ntions of the learning agent with human users, other AI agents, and the environment. The\npurpose is to acquire ground-truth training data on the fly by itself. This is very similar to\nwhat we humans do when we encounter something novel or new and ask others interactively\nand learn it. It is very different from collecting a large amount of unlabeled data and asking\nhuman annotators to label the data. Incidentally, SOLA also differs from active learning\n34\n\n[Settles, 2009, Ren et al., 2021] as active learning only focuses on acquiring labels from users\nfor selected unlabeled examples in a given dataset.\nFinally, note that although SOLA focuses on self-initiated continual learning, it does\nnot mean that the learning system cannot learn a task given by humans or other AI agents.\nAdditionally, SOLA also allows learning from other resources, e.g., the Web, to gain new\nknowledge, like a human reading a book. We will describe the system NELL [Mitchell et al.,\n2018] that continuously learns new knowledge by extracting facts from Web documents in\nSection 3.5.\n2.8\nSummary\nThis chapter presented a new framework called self-initiated open world continual learning\nand adaptation (SOLA) that should be followed by AI agents (including chatbots or dialogue\nsystems) that want to continually learn after model deployment or learn on the job. We\nbelieve that this capability is necessary for the next generation machine learning or AI\nagents and any form of intelligence in general. The core of this framework is self-motivation\nand self-initiation. That is, the AI agent must learn autonomously and continually in the\nopen world on its own initiative after deployment (or post-deployment), detecting novelties,\nadapting to the novelties and the ever-changing world, and learning more and more to\nbecome more and more powerful over time. An example SOLA based dialogue system (called\nCML) is briefly described (see Section 4.2 for more details of the system). Almost all existing\ndialogue systems related to lifelong learning are only partial continual learning dialogue\nsystems.\nThey do not include all functions in the proposed framework.\nFor example,\nsome only perform continual learning with given tasks and training data, some only detect\nnovelties, and some only continually collect additional training data during chatting.\n35\n\nChapter 3\nContinuous Factual Knowledge\nLearning in Dialogues\n3.1\nOpportunities for Knowledge Learning in Dialogues\n3.2\nExtracting Facts from Dialogue Context\n3.3\nLexical Knowledge Acquisition in Dialogues\n3.4\nInteractive Factual Knowledge Learning and Inference\n3.5\nLearning new knowledge from external sources\n3.6\nSummary\n36\n\nChapter 4\nContinuous and Interactive\nLanguage Learning and Grounding\n4.1\nModes of language learning human-chatbot interactions\n4.2\nLearning language games through interactions\n4.3\nDialogue-driven Learning of Self-adaptive NLIs\n4.4\nInteractive semantic parsing and learning from feedback\n4.5\nSummary\n37\n\nChapter 5\nContinual Learning in Chit-chat\nSystems\n5.1\nPredicting User Satisfaction in Open-domain Conversa-\ntion\n5.2\nLearning by Extracting New Examples from Conversa-\ntion\n5.3\nDialogue Learning via Role-Playing Games\n5.4\nSummary\n38\n\nChapter 6\nContinual Learning for\nTask-oriented Dialogue Systems\n6.1\nOpen Intent Detection & Learning\n6.2\nContinual Learning for Semantic Slot Filling\n6.3\nContinual Learning for Dialogue State Tracking\n6.4\nContinual Learning for Natural Language Generation\n6.5\nJoint Continual Learning of all Dialogue Tasks\n6.6\nSummary\n39\n\nChapter 7\nContinual Learning of\nConversational Skills\n7.1\nLearning user behaviors and preferences\n7.2\nLearning emotions, moods and opinions in dialogues\n7.3\nModeling situation-aware conversations\n7.4\nSummary\n40\n\nChapter 8\nConclusion and Future Directions\n41\n\nBibliography\nH. Ahn, S. Cha, D. Lee, and T. Moon. Uncertainty-based continual learning with adaptive\nregularization. Advances in neural information processing systems, 32, 2019.\nH. B. Ammar, E. Eaton, J. M. Luna, and P. Ruvolo. Autonomous cross-domain knowledge\ntransfer in lifelong policy gradient reinforcement learning. In Twenty-fourth international\njoint conference on artificial intelligence, 2015.\nA. Bartl and G. Spanakis. A retrieval-based dialogue system utilizing utterance and context\nembeddings.\nIn 2017 16th IEEE International Conference on Machine Learning and\nApplications (ICMLA), pages 1120–1125. IEEE, 2017.\nA. Bendale and T. Boult. Towards open world recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1893–1902, 2015.\nA. Bordes, Y.-L. Boureau, and J. Weston. Learning end-to-end goal-oriented dialog. arXiv\npreprint arXiv:1605.07683, 2016.\nT. Boult, P. Grabowicz, D. Prijatelj, R. Stern, L. Holder, J. Alspector, M. M. Jafarzadeh,\nT. Ahmad, A. Dhamija, C. Li, et al. Towards a unifying framework for formal theories\nof novelty. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npages 15047–15052, 2021.\nP. Budzianowski, T.-H. Wen, B.-H. Tseng, I. Casanueva, S. Ultes, O. Ramadan, and M. Ga-\nsic. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue\nmodelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 5016–5026, 2018.\nP. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara. Dark experience for\ngeneral continual learning: a strong, simple baseline. Advances in neural information\nprocessing systems, 33:15920–15930, 2020.\nR. Camoriano, G. Pasquale, C. Ciliberto, L. Natale, L. Rosasco, and G. Metta. Incremen-\ntal robot learning of new objects with fixed update time. In 2017 IEEE International\nConference on Robotics and Automation (ICRA), pages 3207–3214. IEEE, 2017.\n42\n\nH. Cha, J. Lee, and J. Shin. Co2l: Contrastive continual learning. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 9516–9525, 2021.\nA. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny. Efficient lifelong learning with\na-gem. In International Conference on Learning Representations, 2019.\nA. Chaudhry, N. Khan, P. Dokania, and P. Torr. Continual learning in low-rank orthogonal\nsubspaces. Advances in Neural Information Processing Systems, 33:9900–9911, 2020.\nH. Chen, X. Liu, D. Yin, and J. Tang. A survey on dialogue systems: Recent advances and\nnew frontiers. Acm Sigkdd Explorations Newsletter, 19(2):25–35, 2017.\nZ. Chen and B. Liu. Topic modeling using topics from many domains, lifelong learning and\nbig data. In International conference on machine learning, pages 703–711. PMLR, 2014.\nZ. Chen and B. Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence\nand Machine Learning, 12(3):1–207, 2018.\nK. M. Colby, S. Weber, and F. D. Hilf. Artificial paranoia. Artificial Intelligence, 2(1):1–25,\n1971.\nP. Dhar, R. V. Singh, K.-C. Peng, Z. Wu, and R. Chellappa. Learning without memorizing.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 5138–5146, 2019.\nG. Fei and B. Liu. Breaking the closed world assumption in text classification. In Pro-\nceedings of the 2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 506–514, 2016.\nG. Fei, S. Wang, and B. Liu. Learning cumulatively to become more knowledgeable. In\nProceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, pages 1565–1574, 2016.\nM. Forbes, R. P. Rao, L. Zettlemoyer, and M. Cakmak. Robot programming by demon-\nstration with situated spatial language understanding. In 2015 IEEE International Con-\nference on Robotics and Automation (ICRA), pages 2014–2020. IEEE, 2015.\nJ. Gao, M. Galley, and L. Li. Neural approaches to conversational AI: Question answering,\ntask-oriented dialogues and social chatbots. Now Foundations and Trends, 2019.\nA. Gepperth and C. Karaoguz. A bio-inspired incremental learning architecture for applied\nperceptual problems. Cognitive Computation, 8(5):924–934, 2016.\nI. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016.\nY. Guo, W. Hu, D. Zhao, and B. Liu. Adaptive orthogonal projection for batch and online\ncontinual learning. Proceedings of AAAI-2022, 2, 2022.\n43\n\nB. Hancock, A. Bordes, P.-E. Mazare, and J. Weston. Learning from dialogue after de-\nployment: Feed yourself, chatbot!\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages 3667–3684, 2019.\nG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\nW. Hu, Z. Chan, B. Liu, D. Zhao, J. Ma, and R. Yan. Gsn: A graph-structured network\nfor multi-party dialogues. In Proceedings of International Joint Conference on Artificial\nIntelligence (IJCAI-2019, 2019a.\nW. Hu, Z. Lin, B. Liu, C. Tao, Z. T. Tao, D. Zhao, J. Ma, and R. Yan.\nOvercoming\ncatastrophic forgetting for continual learning via model adaptation.\nIn International\nconference on learning representations, 2019b.\nM. Huang, X. Zhu, and J. Gao.\nChallenges in building intelligent open-domain dialog\nsystems. ACM Transactions on Information Systems (TOIS), 38(3):1–32, 2020.\nS. Jiang and M. de Rijke. Why are sequence-to-sequence models so dull? understanding the\nlow-diversity problem of chatbots. In Proceedings of the 2018 EMNLP Workshop SCAI:\nThe 2nd International Workshop on Search-Oriented Conversational AI, pages 81–86,\n2018.\nK. Jokinen and M. McTear. Spoken dialogue systems. Synthesis Lectures on Human Lan-\nguage Technologies, 2(1):1–151, 2009.\nH. Jung, J. Ju, M. Jung, and J. Kim. Less-forgetting learning in deep neural networks.\narXiv preprint arXiv:1607.00122, 2016.\nD. Jurafsky and J. H. Martin. Speech and language processing: An introduction to natural\nlanguage processing, computational linguistics, and speech recognition. 2020.\nN. Kamra, U. Gupta, and Y. Liu. Deep generative dual memory network for continual\nlearning. arXiv preprint arXiv:1710.10368, 2017.\nZ. Ke and H. Xu. Adapting bert for continual learning of a sequence of aspect sentiment\nclassification tasks. In Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, 2021.\nZ. Ke, B. Liu, and X. Huang.\nContinual learning of a mixed sequence of similar and\ndissimilar tasks. Advances in Neural Information Processing Systems, 33:18493–18504,\n2020.\nZ. Ke, B. Liu, N. Ma, H. Xu, and L. Shu. Achieving forgetting prevention and knowledge\ntransfer in continual learning. Advances in Neural Information Processing Systems, 34:\n22443–22456, 2021a.\n44\n\nZ. Ke, B. Liu, H. Xu, and L. Shu. Classic: Continual and contrastive learning of aspect sen-\ntiment classification tasks. In Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 6871–6883, 2021b.\nR. Kemker and C. Kanan.\nFearnet: Brain-inspired model for incremental learning.\nIn\nInternational Conference on Learning Representations, 2018.\nG. Kim, S. Esmaeilpour, C. Xiao, and B. Liu. Continual learning based on ood detection\nand task masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3856–3866, 2022.\nJ. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,\nJ. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting\nin neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526,\n2017.\nP. Langley. Open-world learning for radically autonomous agents. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 34, pages 13539–13543, 2020.\nK. Lee, K. Lee, J. Shin, and H. Lee. Overcoming catastrophic forgetting with unlabeled\ndata in the wild. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 312–321, 2019.\nJ. Li, M. Galley, C. Brockett, J. Gao, and W. B. Dolan. A diversity-promoting objec-\ntive function for neural conversation models. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 110–119, 2016a.\nJ. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao.\nDeep reinforcement\nlearning for dialogue generation. In Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, pages 1192–1202, 2016b.\nZ. Li and D. Hoiem. Learning without forgetting. IEEE transactions on pattern analysis\nand machine intelligence, 40(12):2935–2947, 2017.\nB. Liu. Learning on the job: Online lifelong and continual learning. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 34, pages 13544–13549, 2020.\nB. Liu and S. Mazumder. Lifelong and continual learning dialogue systems: learning during\nconversation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\npages 15058–15063, 2021.\nB. Liu and C. Mei.\nLifelong knowledge learning in rule-based dialogue systems. arXiv\npreprint arXiv:2011.09811, 2020.\nY. Liu, B. Schiele, and Q. Sun. Adaptive aggregation networks for class-incremental learn-\ning.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2544–2553, 2021.\n45\n\nD. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. Advances\nin neural information processing systems, 30, 2017.\nR. Lowe, N. Pow, I. V. Serban, and J. Pineau.\nThe ubuntu dialogue corpus: A large\ndataset for research in unstructured multi-turn dialogue systems. In Proceedings of the\n16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages\n285–294, 2015.\nR. Lowe, N. Pow, I. V. Serban, L. Charlin, C.-W. Liu, and J. Pineau. Training end-to-end\ndialogue systems with the ubuntu dialogue corpus. Dialogue & Discourse, 8(1):31–65,\n2017.\nL. Luo, W. Huang, Q. Zeng, Z. Nie, and X. Sun. Learning personalized end-to-end goal-\noriented dialog. In Proceedings of the AAAI Conference on Artificial Intelligence, vol-\nume 33, pages 6794–6801, 2019.\nA. Mallya and S. Lazebnik. Packnet: Adding multiple tasks to a single network by iter-\native pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern\nRecognition, pages 7765–7773, 2018.\nS. Mazumder. On-the-job Continual and Interactive Learning of Factual Knowledge and\nLanguage Grounding. PhD thesis, University of Illinois at Chicago, 2021.\nS. Mazumder, N. Ma, and B. Liu. Towards a continuous knowledge learning engine for\nchatbots. arXiv preprint arXiv:1802.06024, 2018.\nS. Mazumder, B. Liu, S. Wang, and N. Ma. Lifelong and interactive learning of factual\nknowledge in dialogues. In Proceedings of the 20th Annual SIGdial Meeting on Discourse\nand Dialogue, pages 21–31, 2019.\nS. Mazumder, B. Liu, N. Ma, and S. Wang. Continuous and interactive factual knowledge\nlearning in verification dialogues. In NeurIPS-2020 Workshop on Human And Machine\nin-the-Loop Evaluation and Learning Strategies, 2020a.\nS. Mazumder, B. Liu, S. Wang, and S. Esmaeilpour. An application-independent approach\nto building task-oriented chatbots with interactive continual learning. In NeurIPS-2020\nWorkshop on Human in the Loop Dialogue Systems, 2020b.\nM. McCloskey and N. J. Cohen. Catastrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learning and motivation, volume 24, pages\n109–165. Elsevier, 1989.\nM. McTear.\nConversational ai: Dialogue systems, conversational agents, and chatbots.\nSynthesis Lectures on Human Language Technologies, 13(3):1–251, 2020.\nH. Mei, M. Bansal, and M. R. Walter. Coherent dialogue with attention-based language\nmodels. In Thirty-first AAAI conference on artificial intelligence, 2017.\n46\n\nT. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, B. Yang, J. Betteridge, A. Carlson,\nB. Dalvi, M. Gardner, B. Kisiel, et al. Never-ending learning. Communications of the\nACM, 61(5):103–115, 2018.\nJ. G. Moreno-Torres, T. Raeder, R. Alaiz-Rodr´ıguez, N. V. Chawla, and F. Herrera. A\nunifying view on dataset shift in classification. Pattern recognition, 45(1):521–530, 2012.\nO. Ostapenko, M. Puscas, T. Klein, P. Jahnichen, and M. Nabi. Learning to remember:\nA synaptic plasticity driven framework for continual learning.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 11321–11329,\n2019.\nG. Pandey, D. Contractor, V. Kumar, and S. Joshi. Exemplar encoder-decoder for neural\nconversation generation. In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 1329–1338, 2018.\nG. Pang, C. Shen, L. Cao, and A. V. D. Hengel. Deep learning for anomaly detection: A\nreview. ACM Computing Surveys (CSUR), 54(2):1–38, 2021.\nJ. Parmar, S. S. Chouhan, V. Raychoudhury, and S. S. Rathore.\nOpen-world machine\nlearning: applications, challenges, and opportunities. ACM Computing Surveys (CSUR),\n2021.\nQ. Qin, W. Hu, H. Peng, D. Zhao, and B. Liu. Bns: Building network structures dynamically\nfor continual learning. Advances in Neural Information Processing Systems, 34:20608–\n20620, 2021.\nM. Qiu, F.-L. Li, S. Wang, X. Gao, Y. Chen, W. Zhao, H. Chen, J. Huang, and W. Chu.\nAlime chat: A sequence to sequence and rerank based chatbot engine. In Proceedings\nof the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2:\nShort Papers), pages 498–503, 2017.\nJ. Rajasegaran, M. Hayat, S. Khan, F. S. Khan, L. Shao, and M.-H. Yang. An adaptive ran-\ndom path selection approach for incremental learning. arXiv preprint arXiv:1906.01120,\n2019.\nS.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert. icarl: Incremental classifier and\nrepresentation learning. In Proceedings of the IEEE conference on Computer Vision and\nPattern Recognition, pages 2001–2010, 2017.\nP. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, B. B. Gupta, X. Chen, and X. Wang. A\nsurvey of deep active learning. ACM computing surveys (CSUR), 54(9):1–40, 2021.\nD. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne.\nExperience replay for\ncontinual learning. Advances in Neural Information Processing Systems, 32, 2019.\n47\n\nA. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu,\nR.\nPascanu,\nand\nR.\nHadsell.\nProgressive\nneural\nnetworks.\narXiv\npreprint\narXiv:1606.04671, 2016.\nP. Ruvolo and E. Eaton. Ella: An efficient lifelong learning algorithm. In International\nconference on machine learning, pages 507–515. PMLR, 2013.\nS. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules. Advances in\nneural information processing systems, 30, 2017.\nL.-P. Schaub, V. Hudecek, D. Stancl, O. Dusek, and P. Paroubek. Defining and detecting\ninconsistent system behavior in task-oriented dialogues. In Traitement Automatique des\nLangues Naturelles, pages 142–152. ATALA, 2021.\nJ. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y. W. Teh, R. Pascanu,\nand R. Hadsell. Progress & compress: A scalable framework for continual learning. In\nInternational Conference on Machine Learning, pages 4528–4537. PMLR, 2018.\nA. See, S. Roller, D. Kiela, and J. Weston. What makes a good conversation? how con-\ntrollable attributes affect human judgments. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pages 1702–1723, 2019.\nA. Seff, A. Beatson, D. Suo, and H. Liu. Continual learning in generative adversarial nets.\narXiv preprint arXiv:1705.08395, 2017.\nI. Serban, A. Sordoni, Y. Bengio, A. Courville, and J. Pineau. Building end-to-end dialogue\nsystems using generative hierarchical neural network models. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 30, 2016.\nI. V. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian, T. Kim, M. Pieper,\nS. Chandar, N. R. Ke, et al. A deep reinforcement learning chatbot. arXiv preprint\narXiv:1709.02349, 2017.\nI. V. Serban, R. Lowe, P. Henderson, L. Charlin, and J. Pineau. A survey of available corpora\nfor building data-driven dialogue systems: The journal version. Dialogue & Discourse, 9\n(1):1–49, 2018.\nJ. Serra, D. Suris, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with\nhard attention to the task.\nIn International Conference on Machine Learning, pages\n4548–4557. PMLR, 2018.\nB. Settles. Active learning literature survey. 2009.\nP. Shah, D. Hakkani-Tur, B. Liu, and G. T¨ur. Bootstrapping a neural conversational agent\nwith dialogue self-play, crowdsourcing and on-line reinforcement learning. In Proceedings\n48\n\nof the 2018 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), pages\n41–51, 2018.\nL. Shang, Z. Lu, and H. Li. Neural responding machine for short-text conversation. In\nProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pages 1577–1586, 2015.\nX. Shen, H. Su, Y. Li, W. Li, S. Niu, Y. Zhao, A. Aizawa, and G. Long. A conditional\nvariational framework for dialog generation. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Volume 2: Short Papers), pages 504–509,\n2017.\nX. Shen, H. Su, S. Niu, and V. Demberg. Improving variational encoder-decoders in dialogue\ngeneration. In Proceedings of the AAAI conference on artificial intelligence, volume 32,\n2018.\nH. Shin, J. K. Lee, J. Kim, and J. Kim. Continual learning with deep generative replay.\nAdvances in neural information processing systems, 30, 2017.\nL. Shu, P. Molino, M. Namazifar, H. Xu, B. Liu, H. Zheng, and G. T¨ur. Flexibly-structured\nmodel for task-oriented dialogues. In Proceedings of the 20th Annual SIGdial Meeting on\nDiscourse and Dialogue, pages 178–187, 2019a.\nL. Shu, H. Xu, B. Liu, and P. Molino. Modeling multi-action policy for task-oriented dia-\nlogues. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1304–1310, 2019b.\nD. L. Silver, Q. Yang, and L. Li.\nLifelong machine learning systems: Beyond learning\nalgorithms. In 2013 AAAI spring symposium series, 2013.\nY. Song, R. Yan, X. Li, D. Zhao, and M. Zhang. Two are better than one: An ensemble of\nretrieval-and generation-based dialog systems. arXiv preprint arXiv:1610.07149, 2016.\nC. Tao, W. Wu, C. Xu, W. Hu, D. Zhao, and R. Yan. Multi-representation fusion network\nfor multi-turn response selection in retrieval-based chatbots. In Proceedings of the twelfth\nACM international conference on web search and data mining, pages 267–275, 2019a.\nC. Tao, W. Wu, C. Xu, W. Hu, D. Zhao, and R. Yan. One time of interaction may not be\nenough: Go deep with an interaction-over-interaction network for response selection in\ndialogues. In Proceedings of the 57th annual meeting of the association for computational\nlinguistics, pages 1–11, 2019b.\nS. Thrun and T. M. Mitchell. Lifelong robot learning. Robotics and autonomous systems,\n15(1-2):25–46, 1995.\n49\n\nG. M. Van de Ven and A. S. Tolias. Three scenarios for continual learning. arXiv preprint\narXiv:1904.07734, 2019.\nO. Vinyals and Q. Le. A neural conversational model. arXiv preprint arXiv:1506.05869,\n2015.\nJ. von Oswald, C. Henning, J. Sacramento, and B. F. Grewe. Continual learning with hyper-\nnetworks. In 8th International Conference on Learning Representations (ICLR 2020)(vir-\ntual). International Conference on Learning Representations, 2020.\nR. S. Wallace. The anatomy of alice. In Parsing the turing test, pages 181–210. Springer,\n2009.\nH. Wang, Z. Lu, H. Li, and E. Chen. A dataset for research on short-text conversations. In\nProceedings of the 2013 conference on empirical methods in natural language processing,\npages 935–945, 2013.\nS. I. Wang, S. Ginn, P. Liang, and C. D. Manning. Naturalizing a programming language\nvia interactive learning. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 929–938, 2017.\nJ. Weizenbaum. Eliza—a computer program for the study of natural language communica-\ntion between man and machine. Communications of the ACM, 9(1):36–45, 1966.\nT.-H. Wen, Y. Miao, P. Blunsom, and S. Young. Latent intention dialogue models. In\nInternational Conference on Machine Learning, pages 3732–3741. PMLR, 2017a.\nT.-H. Wen, D. Vandyke, N. Mrkˇsi´c, M. Gasic, L. M. R. Barahona, P.-H. Su, S. Ultes,\nand S. Young. A network-based end-to-end trainable task-oriented dialogue system. In\nProceedings of the 15th Conference of the European Chapter of the Association for Com-\nputational Linguistics: Volume 1, Long Papers, pages 438–449, 2017b.\nJ. D. Williams and S. Young. Partially observable markov decision processes for spoken\ndialog systems. Computer Speech & Language, 21(2):393–422, 2007.\nT. Winograd. Understanding natural language. Cognitive psychology, 3(1):1–191, 1972.\nM. Wortsman, V. Ramanujan, R. Liu, A. Kembhavi, M. Rastegari, J. Yosinski, and\nA. Farhadi. Supermasks in superposition. Advances in Neural Information Processing\nSystems, 33:15173–15184, 2020.\nY. Wu, W. Wu, Z. Li, and M. Zhou.\nTopic augmented neural network for short text\nconversation. CoRR abs/1605.00090, 2016.\nY. Wu, W. Wu, C. Xing, M. Zhou, and Z. Li.\nSequential matching network: A new\narchitecture for multi-turn response selection in retrieval-based chatbots. In Proceedings\nof the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 496–505, 2017.\n50\n\nY. Wu, W. Wu, D. Yang, C. Xu, and Z. Li. Neural response generation with dynamic\nvocabularies. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32,\n2018.\nC. Xing, W. Wu, Y. Wu, J. Liu, Y. Huang, M. Zhou, and W.-Y. Ma. Topic aware neural\nresponse generation.\nIn Proceedings of the AAAI conference on artificial intelligence,\nvolume 31, 2017.\nH. Xu, B. Liu, L. Shu, and P. Yu. Open-world learning and application to product classifi-\ncation. In The World Wide Web Conference, pages 3413–3419, 2019.\nR. Yan, Y. Song, and H. Wu. Learning to respond with deep neural networks for retrieval-\nbased human-computer conversation system. In Proceedings of the 39th International\nACM SIGIR conference on Research and Development in Information Retrieval, pages\n55–64, 2016.\nJ. Yang, K. Zhou, Y. Li, and Z. Liu. Generalized out-of-distribution detection: A survey.\narXiv preprint arXiv:2110.11334, 2021.\nL. Yang, M. Qiu, C. Qu, J. Guo, Y. Zhang, W. B. Croft, J. Huang, and H. Chen. Response\nranking with deep matching networks and external knowledge in information-seeking\nconversation systems.\nIn The 41st international acm sigir conference on research &\ndevelopment in information retrieval, pages 245–254, 2018.\nK. Yao, G. Zweig, and B. Peng. Attention with intention for a neural network conversation\nmodel. arXiv preprint arXiv:1510.08565, 2015.\nF. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In\nInternational Conference on Machine Learning, pages 3987–3995. PMLR, 2017.\nY. Zhang, M. Galley, J. Gao, Z. Gan, X. Li, C. Brockett, and B. Dolan. Generating in-\nformative and diverse conversational responses via adversarial information maximization.\nAdvances in Neural Information Processing Systems, 31, 2018.\nT. Zhao, A. Lu, K. Lee, and M. Eskenazi. Generative encoder-decoder models for task-\noriented spoken dialog systems with chatting capability. In Proceedings of the 18th Annual\nSIGdial Meeting on Discourse and Dialogue, pages 27–36, 2017.\nH. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu. Emotional chatting machine: Emotional\nconversation generation with internal and external memory. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 32, 2018.\nL. Zhou, J. Gao, D. Li, and H.-Y. Shum. The design and implementation of xiaoice, an\nempathetic social chatbot. Computational Linguistics, 46(1):53–93, 2020.\nX. Zhou, D. Dong, H. Wu, S. Zhao, D. Yu, H. Tian, X. Liu, and R. Yan. Multi-view response\nselection for human-computer conversation. In Proceedings of the 2016 conference on\nempirical methods in natural language processing, pages 372–381, 2016.\n51\n\nF. Zhu, X.-Y. Zhang, C. Wang, F. Yin, and C.-L. Liu. Prototype augmentation and self-\nsupervision for incremental learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 5871–5880, 2021.\n52",
    "pdf_filename": "Lifelong and Continual Learning Dialogue Systems.pdf"
}