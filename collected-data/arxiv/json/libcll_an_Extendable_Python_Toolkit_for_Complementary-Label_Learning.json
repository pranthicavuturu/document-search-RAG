{
    "title": "LIBCLL: AN EXTENDABLE PYTHON TOOLKIT FOR",
    "abstract": "Complementary-labellearning(CLL)isaweaklysupervisedlearningparadigmformulticlassclassi- fication,whereonlycomplementarylabels—indicatingclassesaninstancedoesnotbelongto—are providedtothelearningalgorithm. DespiteCLL’sincreasingpopularity,previousstudieshighlight twomainchallenges: (1)inconsistentresultsarisingfromvariedassumptionsoncomplementary labelgeneration,and(2)highbarrierstoentryduetothelackofastandardizedevaluationplatform acrossdatasetsandalgorithms. Toaddressthesechallenges,weintroducelibcll,anextensible PythontoolkitforCLLresearch. libcllprovidesauniversalinterfacethatsupportsawiderangeof generationassumptions,bothsyntheticandreal-worlddatasets,andkeyCLLalgorithms. Thetoolkit isdesignedtomitigateinconsistenciesandstreamlinetheresearchprocess,witheasyinstallation, comprehensiveusageguides,andquickstarttutorialsthatfacilitateefficientadoptionandimplementa- tionofCLLtechniques. Extensiveablationstudiesconductedwithlibclldemonstrateitsutilityin generatingvaluableinsightstoadvancefutureCLLresearch. 1 Introduction Inmanyreal-worldapplications,trainingeffectiveclassifierstypicallydependsonobtaininghigh-quality,accurate labels. However, acquiring such labels is often difficult and costly. To address this challenge, many researchers haveturnedtheirattentiontoweaklysupervisedlearning(WSL),amethodologyaimedattrainingreliableclassifiers usingonlyincomplete,imprecise,orinaccuratedata[1,2]. NumerousWSLstudieshavebeenconductedtoextend our understanding of machine learning capabilities, covering topics such as complementary labels [3, 4], multiple complementarylabels[5,6],noisylabels[7],andlearningfrompartiallabels[8]. Thisworkfocusesoncomplementary-labellearning(CLL),aWSLproblemwhereeachlabelindicatesonlyaclass towhichadatainstancedoesnotbelong[3]. CLLaimstotrainmodelswiththesecomplementarylabelswhilestill enablingaccuratepredictionsoftheordinarylabelsduringtesting. CLLmakesmachinelearningmorepracticalin scenarioswhereobtainingordinarylabelsisdifficultorcostly[3]. Additionally,CLLbroadensourunderstandingof machinelearning’spracticalpotentialunderlimitedsupervision. CurrentresearchonCLLhasintroducednumerouslearningalgorithms[4,9,10,11]thathavebeenevaluatedusing adiverserangeofdatasets,fromsyntheticdatasetsbasedonvariedcomplementary-labelgenerationassumptionsto real-worlddatasets[12]. However,theperformanceofthesealgorithmsoftenvariessignificantlyacrossstudiesdueto differencesinunderlyinglabel-generationassumptions,theabsenceofastandardizedevaluationplatform,andtheuse ofdiversenetworkarchitectures[4,9,3,11]. Establishingafair,reproducible,andstableevaluationenvironmentis thereforeessentialforadvancingCLLresearch. Forinstance,variationsinnetworkarchitectures,suchastheuseof ResNet18[13,12]versusResNet34[9,4],contributetoinconsistenciesinperformanceandhinderfaircomparisons acrossstudies. Furthermore,mostCLLresearchhasnotpubliclyreleasedimplementations[6,11,4,14],particularly regarding details like loss calculation and data pre-processing. This lack of accessibility presents a challenge for researchersseekingtovalidateandbuilduponexistingworkinCLL. ToenablemeaningfulcomparisonsamongCLLalgorithmsandcreateauser-friendlyenvironmentforimplementation andinnovation,weintroducelibcll,acomplementary-labellearningtoolkitbuiltwithPyTorch-Lightning.Thistoolkit 4202 voN 91 ]GL.sc[ 1v67221.1142:viXra",
    "body": "LIBCLL: AN EXTENDABLE PYTHON TOOLKIT FOR\nCOMPLEMENTARY-LABEL LEARNING\nNai-XuanYe, Tan-HaMai, Hsiu-HsuanWang, Wei-ILin, Hsuan-TienLin\nNationalTaiwanUniversity\n{b09902008, d10922024, b09902033, r10922076, htlin}@csie.ntu.edu.tw\nABSTRACT\nComplementary-labellearning(CLL)isaweaklysupervisedlearningparadigmformulticlassclassi-\nfication,whereonlycomplementarylabels—indicatingclassesaninstancedoesnotbelongto—are\nprovidedtothelearningalgorithm. DespiteCLL’sincreasingpopularity,previousstudieshighlight\ntwomainchallenges: (1)inconsistentresultsarisingfromvariedassumptionsoncomplementary\nlabelgeneration,and(2)highbarrierstoentryduetothelackofastandardizedevaluationplatform\nacrossdatasetsandalgorithms. Toaddressthesechallenges,weintroducelibcll,anextensible\nPythontoolkitforCLLresearch. libcllprovidesauniversalinterfacethatsupportsawiderangeof\ngenerationassumptions,bothsyntheticandreal-worlddatasets,andkeyCLLalgorithms. Thetoolkit\nisdesignedtomitigateinconsistenciesandstreamlinetheresearchprocess,witheasyinstallation,\ncomprehensiveusageguides,andquickstarttutorialsthatfacilitateefficientadoptionandimplementa-\ntionofCLLtechniques. Extensiveablationstudiesconductedwithlibclldemonstrateitsutilityin\ngeneratingvaluableinsightstoadvancefutureCLLresearch.\n1 Introduction\nInmanyreal-worldapplications,trainingeffectiveclassifierstypicallydependsonobtaininghigh-quality,accurate\nlabels. However, acquiring such labels is often difficult and costly. To address this challenge, many researchers\nhaveturnedtheirattentiontoweaklysupervisedlearning(WSL),amethodologyaimedattrainingreliableclassifiers\nusingonlyincomplete,imprecise,orinaccuratedata[1,2]. NumerousWSLstudieshavebeenconductedtoextend\nour understanding of machine learning capabilities, covering topics such as complementary labels [3, 4], multiple\ncomplementarylabels[5,6],noisylabels[7],andlearningfrompartiallabels[8].\nThisworkfocusesoncomplementary-labellearning(CLL),aWSLproblemwhereeachlabelindicatesonlyaclass\ntowhichadatainstancedoesnotbelong[3]. CLLaimstotrainmodelswiththesecomplementarylabelswhilestill\nenablingaccuratepredictionsoftheordinarylabelsduringtesting. CLLmakesmachinelearningmorepracticalin\nscenarioswhereobtainingordinarylabelsisdifficultorcostly[3]. Additionally,CLLbroadensourunderstandingof\nmachinelearning’spracticalpotentialunderlimitedsupervision.\nCurrentresearchonCLLhasintroducednumerouslearningalgorithms[4,9,10,11]thathavebeenevaluatedusing\nadiverserangeofdatasets,fromsyntheticdatasetsbasedonvariedcomplementary-labelgenerationassumptionsto\nreal-worlddatasets[12]. However,theperformanceofthesealgorithmsoftenvariessignificantlyacrossstudiesdueto\ndifferencesinunderlyinglabel-generationassumptions,theabsenceofastandardizedevaluationplatform,andtheuse\nofdiversenetworkarchitectures[4,9,3,11]. Establishingafair,reproducible,andstableevaluationenvironmentis\nthereforeessentialforadvancingCLLresearch. Forinstance,variationsinnetworkarchitectures,suchastheuseof\nResNet18[13,12]versusResNet34[9,4],contributetoinconsistenciesinperformanceandhinderfaircomparisons\nacrossstudies. Furthermore,mostCLLresearchhasnotpubliclyreleasedimplementations[6,11,4,14],particularly\nregarding details like loss calculation and data pre-processing. This lack of accessibility presents a challenge for\nresearchersseekingtovalidateandbuilduponexistingworkinCLL.\nToenablemeaningfulcomparisonsamongCLLalgorithmsandcreateauser-friendlyenvironmentforimplementation\nandinnovation,weintroducelibcll,acomplementary-labellearningtoolkitbuiltwithPyTorch-Lightning.Thistoolkit\n4202\nvoN\n91\n]GL.sc[\n1v67221.1142:viXra\nFigure1: CoverageofthelibcllToolkit: Thisfigureprovidesanoverviewofthekeycomponentsincludedinthe\nlibclltoolkit,whichencompasses15datasetsspanningsyntheticandreal-worldscenarios,5commonlyusedmodels\ninComplementaryLabelLearning(CLL),4CLLassumptions,and14CLLalgorithms. Tothebestofourknowledge,\nlibcllisthefirstcomprehensivetoolkitspecificallydedicatedtoCLL.\nstandardizestheevaluationprocesswhileofferingextensivecustomizationoptions,makingiteasierforresearchers\nto develop, test, reproduce, and refine algorithms. By performing comprehensive benchmark experiments across\nestablishedCLLdatasets,variousalgorithms,andarangeofcomplementary-labeldistributions,asillustratedinFigure\n1,libcllprovidesarobustandreproducibleevaluationframework. Ourgoalisforlibclltoaccelerateprogressin\nCLLresearchandfosteracollaborativeresearchcommunity.\nFurthermore, libcll includes functions to generate complementary labels using a user-defined transition matrix\nandsupportsmultiplecomplementary-labelgenerationmethods. Additionally,libclloffersextensivebenchmarks\nacrossawiderangeofdatasets,fromsynthetictoreal-world,usingvariousCLLalgorithmsandcomplementarylabel\ndistributions. Figure1illustratesthecomprehensivecoverageoflibcll. Thistoolkit,togetherwiththesebenchmark\nresults,providesthecommunitywithaholisticviewofCLL,allowingresearcherstoassessthestrengthsandweaknesses\nofdifferentapproaches, pinpointareasforimprovement, anddevelopmoreresilientalgorithms. Bystandardizing\nevaluationmetricsandexperimentalsetups,libcllpromotesreproducibilityandcomparability,drivingprogressin\nthefieldandencouragingcollaborativeresearchefforts.\nInsummary,thecontributionsofourlibclltoolkitareasfollows:\n• To the best of our knowledge, libcll is the first publicly available toolkit for CLL, now accessible at\nhttps://github.com/ntucllab/libcll.\n• Weintroducelibcll,aplatformthatoffersin-depthinsightsintoComplementaryLabelLearning(CLL)by\nbenchmarking15datasets,14algorithms,anddiverselabeldistributions,whilesupportingcustomizationof\nCLLcomponentsandhighlightingeachapproach’sstrengthsandlimitations(seeFigure1).\n2 Preliminariesandrelatedworks\n2.1 Complementary-LabelLearning\nIn ordinary multi-class classification, a dataset D = {(x ,y )}N is provided to the learning algorithm, where N\ni i i=1\ndenotesthenumberofsamples,andthedatasetisi.i.d. sampledfromanunknowndistribution. Foreachi,x ∈Rd\ni\nrepresentsthed-dimensionfeatureoftheith sampleandy ∈ RK,whereK denotesthetotalnumberofclassesin\ni\nthedataset,withK > 2. Theset[K] = {1,2,...,K}representsthepossibleclassestowhichx canbelongs. The\ni\nobjective of the learning algorithm is to learn a classifier f(x) : Rd → RK that minimizes the classification risk:\nE [ℓ(f(x),e )],whereℓisthelossfunctionande isone-hotvectorcorrespondingtolabely. Thepredicted\n(x,y)∼D y y\n2\nlabelyˆforaninstancexisdeterminedbyapplyingtheargmaxfunctiononf(x),i.e. yˆ=argmax f(x) ,where\nk∈K k\nf(x) representsthekthoutputcomponentoff(x).\nk\nIncontrasttoordinary-labellearning,complementary-labellearning(CLL)sharesthesamegoaloftrainingaclassifier\nbutlearnsfromadifferentlabelset. InCLL,theground-truthlabelyisnotaccessibletothelearningalgorithm. Instead,\na complementary label y¯is provided, which is a class that the instance x does not belong to. The training set of\ncomplementarybecomesD¯ =X×Y¯,whereX isinputspaceandY¯ isthecorrespondingcomplementarylabelsspace.\nTheobjectiveofCLLremainsconsistentwiththatofordinarymulti-classclassification: tolearnaclassifiercapableof\npredictingthecorrectlabelofunseeninstances,evenwhentrainedonacomplementarydatasetD¯ ={(x ,y¯)}N .\ni i i=1\n2.2 Assumptionsofcomplementarylabels\nComplementary-labellearningaimstodevelopaclassifierundertheguidanceofweaksupervisionfromcomplementary\nlabels. Forsyntheticdatageneration,priorstudiesassumethedistributionofcomplementarylabelsonlydependson\nthe ordinary labels instead of features; thus, P(y¯ | x,y) = P(y¯ | y). The transition probability P(y¯ | y) is often\nrepresentedbyaK×K transitionmatrixT,withT =P(y¯=j |y =i).\nij\nThetransitionmatrixcanbefurtherclassifiedintothreecategories:\n• Uniform[3]: AllcomplementarylabelsareuniformlyandrandomlyselectedfromK−1classes. Basedon\nthisassumption,thetransitionmatrixisT = 1 (1 −I ).\nK−1 K K\n• Biased[9]: Anytransitionmatrixthatisnotuniformisbiased.\n• Noisy[11]: Aportionofthetruelabelsaremislabeledascomplementarylabels. Thus,thediagonalsofthe\ntransitionmatrixarenotnecessarilyzero.\n• MCL[6]: Eachinstancehasmorethanonecomplementarylabel.\nAdditionally,duetothedifficultyofobtainingatransitionmatrixinreal-worldscenarios,CONU[15]proposedSCAR\n(SelectedCompletelyAtRandom),wherethegenerationofcomplementarylabelsisindependentofbothinstances’\nfeaturesandground-truthlabels;thatis,P(y¯|x,y)=P(y¯)=c ,wherec isaconstantrelatedonlytothek-thclass.\nk k\nFurthermore,somestudies,suchasMCL[6],assumethateachinstancecanhavemultiplecomplementarylabels. This\napproachinvolvesrandomlygeneratingalabelsetyˆwhere1≤[yˆ]≤K−1andthenaskingannotatorswhetherthe\ngivenlabelsetyˆcontainsthetruelabel.\nTable1: TheassumptionsofcomplementarylabeldistributionamongdifferentCLLmethods.\nIncludein class-\nuniform biased MCL noisy SCAR\nlibcll imbalanced\nPC[3] ✓ ✓\nFWD[9] ✓ ✓ ✓\nGA[16] ✓ ✓\nMCL[6] ✓ ✓ ✓\nSCL[4] ✓ ✓ ✓\nLW[10] ✓ ✓ ✓\nrob[17] ✓ ✓ ✓\nCPE[11] ✓ ✓ ✓\nOP[18] ✓\nComCo[14] ✓ ✓ ✓\nWCLL[19] ✓ ✓ ✓\nCONU[15] ✓ ✓ ✓ ✓\nTheuniformassumptionforcomplementarylabelsoriginatesfromalabel-collectionperspective. Inthisapproach,each\ndatainstanceisassociatedwitharandomlyassignedlabel,andworkersverifythevalidityofeachlabelbyresponding\nwitheither’yes’or’no.’ Thismethodologyofferspotentiallygreaterefficiencycomparedtoidentifyingthetruelabel\nforeverydatainstance. Asthepioneeringworkincomplementary-labellearning,[3]assumedthatthedistributionof\ncomplementarylabelsisnoise-freeandcanberepresentedbyEquation(1).\n1 (cid:88)\np(y¯=y |x,y)=0, p(x,y¯)= p(x,y) (1)\nK−1\ny̸=y¯\nHowever,assumingcomplementarylabelstobeuniformlydistributedisnotalwaysrealistic[9]. Humanannotators\nmay exhibit biases toward specific classes and the data instance x. Building on this observation, studies such as\n3\n[9,17,11,15]haveexpandedthelossfunctiontoaccommodatebiasedcomplementarylabels. Theseworksemploya\npredefined,feature-independenttransitionmatrixT torepresentthedistributionofcomplementarylabelsconditioned\nontheirground-truthlabels.\nAdditionally,thereisagrowingbodyofresearchfocusedonleveragingmultiplecomplementarylabelsforsupervision\n[6, 20, 14, 15], where each instance is assigned multiple complementary labels generated from a transition matrix\nwithout replacement. In fact, the problem of learning from multiple complementary labels can be connected to\npartial-labellearningornegative-unlabeledlearning[15]. Buildingontheseconcepts,[12]curatedahuman-labeled\nCIFAR[21]datasetwithcomplementarylabelstobetterunderstandreal-worldCLLdistributions,wherethetransition\nmatricesarebothbiasedandnoisy,andeachinstancehasthreecomplementarylabels.\nThereremainseveralopenproblemsinCLL.First,becausethetransitionmatrixT ispredefined,thereisnouniversal\ngenerationprocessforbiasedcomplementarylabels,andageneralframeworkisneededforfaircomparison. Second,in\ntheabsenceofordinarylabels,thetransitionmatrixT isoftenassumedtobegiven. Ifasmallportionoftruelabelsis\navailable,thetransitionmatrixcanbeestimatedusingtheanchorpointmethodproposedin[9]. Thesevariationscan\nleadtoinconsistentexperimentaloutcomes. Finally,withoutordinarylabels,thereliabilityofvalidationusingonly\ncomplementarylabelsisuncertain. Toaddressthesechallenges,weintroducelibcll,thefirstCLLtoolkit,tosupport\nfutureCLLresearchandadvanceunderstandingintheweakly-supervisedlearningcommunity.\n2.3 PreviousmethodsonCLL\nInthissection, wepresentatimelineofkeydevelopmentsinCLL,asillustratedinFigure2. Weimplementthree\nprimary categories of CLL methods in libcll: URE (unbiased risk estimator), CPE (complementary probability\nestimation),andMCL(multiplecomplementarylabel)methods. Additionally,weincludeseveralbridgingworksthat\nconnectCLLwithotherlearningframeworks.\n*\nR\nN L o A\nA C C L F U\nPC\nFWD\nGA\nCCG MCL SCL\nLW reg rob\nGan- CPE\nOP\nCom WCL CLCI CON\n2017 2018 2019 2020 2021 2022 2023\n*Benchmark dataset\nuniform distribution multiple CL\nbiased distribution SCAR assumption\nnoisy distribution class-imbalanced assumption\nFigure2: Thedevelopmentofcomplementary-labellearning.\nURESeriesofWorksTheconceptofcomplementary-labellearningwasinitiallyproposedby[3],whointroduceda\nriskestimatorusingPairwiseComparison(PC)andOne-vs-All(OvA)strategiesforrestrictedlossfunctions. With\nbiasedcomplementarylabels,[9]employedforwardcorrectiontoreconstructclassificationriskusingcross-entropy\nlossoncomplementarylabelsandthetransitionmatrix. Furthermore,[16]removedtheserestrictions,extendingthe\nunbiasedriskestimatortosupportarbitrarylossfunctionsandmodels.\nCPEFramework[11]offeredanewperspectivebyapproachingpredictionasadecodingprocesswiththetransition\nmatrix. Thisproposeddecodingframeworkcanunifyvariousriskestimatormethods[9,4,10].\nMCLSeriesofWorks[6]introducedtheconceptoflearningfrommultiplecomplementarylabels,proposingthat\nsingle-CLmethodscanbegeneralizedtohandlemulti-CLdistributionsthroughdecomposition.\nRecentstudieshaveextendedtheCLLframeworktovariousfields,includingGenerativeAdversarialNetworks[13,22],\nnegative-labeledlearning[15],robustlossfunctionsfornoisycomplementarylabels[17],andsemi-supervisedlearning\n[23]. Inthiswork,weidentifythethreefundamentalbranchesofCLL:URE,CPE,andMCL,andimplementthese\nmethodsinlibcll. Throughextensiveexperiments,wevalidatetheeffectivenessoftheseapproaches. Wehopethat\nthistoolkitwillinspirefurtheradvancementswithintheCLLcommunityinthenearfuture.\n4\n3 Librarydesign\nThecodestructureoflibcllishighlymodularandseamlesslyintegrateswithPyTorch. Eachcomponentcanbeadded,\nmodified,orremovedindividuallytosupportdiverseexperimentaldesigns. Inthefollowingparagraphs,wewilloutline\nthedefinitionsofstrategies,datasets,models,evaluation,andreproducibility.\nStrategies in CLL algorithms are used to calculate the loss. All strategies inherit from the base class\nlibcll.strategies.Strategy, which itself extends pytorch_lightning.LightningModule. This means ev-\nery strategy defined in libcll can be integrated into any other PyTorch Lightning framework. Additionally,\nlibcll.strategies.Strategy already includes implementations for the validation and testing steps, as well as\nevaluation metrics. Users only need to create a new class that inherits from it and modify the training_step to\nincorporatenewcomplementary-labellearningmethodsintothelibrary.\nFigure3: Trainingpipelineoflibcll. Theprocessbeginsbyinitializingthedataset,eitherreal-worldorsynthetic. For\nreal-worlddatasets,thepipelinedirectlyproceedstothecalculationofthetruetransitionmatrix. Ifsyntheticdatasets\narechosen,complementarylabels(CLs)aregeneratedbasedonuser-specifiedparameters,suchasthenumberofCLs\nperinstanceandtheirdistribution(uniform,biasedornoisy). Thepreprocessorthencalculatesthetransitionmatrix\nbasedonthetrueandcorrespondingCLsforeachinstance. Afterward,thedatapreprocessorpreparestheDataLoader\nalongwiththecalculatedtransitionmatrix. Thetrainingprocessisinitiatedusingtheselectedstrategymodule,the\ntransitionmatrix,andtheDataLoader.\nDatasetsinlibcllincludebothordinarylabelsandcomplementarylabels(CLs). Duringdatapreparation,asshown\ninFigure3,webeginbyinitializinganordinary-labeldatasetforsyntheticscenarios. UserscanthengenerateCLs\nbased on any distribution derived from a specified transition matrix and set the number of CLs per instance. For\nreal-worlddatasets, weutilizepaireddatacontaininghuman-annotatedCLs, ordinarylabels, andimagesprovided\nby[12],eliminatingtheneedforCLgeneration. Userscanselectthenumberofhuman-annotatedCLsperinstance,\nup to a maximum of three. libcll provides 11 synthetic datasets. Five of these (MNIST, FMNIST, KMNIST,\nCIFAR10,andCIFAR20)arewidelyusedimagedatasetsinCLLresearch[4,16,10,11,6,9]. TheYeast,Texture,\nDermatology,andSyntheticControldatasetsarefromtheUCIMachineLearningRepositoryandhavebeenappliedin\nmulti-complementary-labellearningresearch[6]. Additionally,MicroImageNet10andMicroImageNet20arerecent\ndatasetsproposedbytheresearchteamin[12]. Forreal-worlddatasets,libcllincludesCLCIFAR10,CLCIFAR20,\nCLMicroImageNet10,andCLMicroImageNet20,allofwhicharealsorecentcontributionsfrom[12].\nModelslibcllincludesfivemodels: Linear,MLP,DenseNet,ResNet18,andResNet34. TheLinearandMLPmodels\naresuitableforsimplerdatasets,suchasMNIST,KMNIST,andFMNIST,whiletheDenseNet,ResNet18,andResNet34\nmodelsarebettersuitedformorecomplexdatasets,suchasCIFARand(Tiny)ImageNet.\nEvaluationTomaintainflexibilityinthevalidationsetlabelsandencouragefutureresearcherstoadoptmorerealistic\nsetups,weprovidetheunbiasedriskestimator(URE)andsurrogatecomplementaryestimationloss(SCEL)metrics,as\nproposedby[11],forevaluationonvalidationsetscontainingonlycomplementarylabels. Additionally,weinclude\naccuracymetricsforevaluatingordinarylabels.\nReproducibility libcll saves all hyperparameters in a YAML file, along with the best model weights, and logs\ntraininglossandvalidationmetricsatspecifiedintervals. Additionally,libcllensuresconsistentresultswhenusing\nthesamehyperparametersandcomputationaldevice.\n5\n4 Benchmarkexperiments\n4.1 ExperimentalSetups\nIn the subsequent experiments in Sections 4.2 through 4.5, we evaluate the performance of all CLL algorithms\navailableinlibcll. Eachsectionutilizesdistincttransitionmatricesandrecordstestaccuracyforeachdatasetand\nalgorithm,enablingacomprehensiveassessmentofperformance. Additionally,wecalculatetheaveragerankoftest\naccuracyacrossallexperiments,whichisdisplayedinthe’AvgRank’columnofeachtableastheprimarymetric\nforperformancecomparison. Weuseaone-layerMLPmodel(d-500-c)fortheMNIST,KMNIST,FMNIST,Yeast,\nTexture,Control,andDermatologydatasets,andResNet34fortheCIFAR10,CIFAR20,MicroImageNet10(MIN10),\nMicroImageNet20(MIN20),CLCIFAR10,CLCIFAR20,CLMicroImageNet10(CLMIN10),andCLMicroImageNet20\n(CLMIN20) datasets. Following the setup in [12], we apply standard data augmentation techniques, including\nRandomHorizontalFlip,RandomCrop,andnormalization,toeachimageintheCIFARandMicroImageNetseries\ndatasets.\nTable2: Baselinetestingaccuraciesofvariousstrategieswithauniformdistributiononsyntheticdatasets. Results\naregroupedintotwocategories: T-agnosticalgorithms(uppersection)andT-awarealgorithms(lowersection). The\nbestperformanceforeachdatasetishighlightedinbold,whilethesecond-bestresultisunderlined. The’AvgRank’\ncolumnprovidesanoverallrankingofthealgorithmsacrossalldatasets,wherealowervalueindicatesbettergeneral\nperformance.\nMNIST KMNIST FMNIST CIFAR10 CIFAR20 MIN10 MIN20 AvgRank\nSCL-NL 94.20±0.10 71.88±0.45 84.59±0.41 65.45±0.75 21.43±1.00 37.59±2.96 11.75±2.36 2.57\nSCL-EXP 94.29±0.29 67.82±0.52 84.30±0.32 61.16±1.18 21.33±0.20 39.14±3.49 10.83±2.32 3.86\nURE-NN 92.19±0.04 69.27±1.24 82.73±0.27 46.83±1.41 15.48±0.67 29.46±1.85 10.98±0.83 5.71\nURE-GA 94.41±0.16 74.32±0.45 83.51±0.22 58.00±1.49 15.18±0.47 33.70±1.34 8.62±1.68 4.71\nDM 92.41±0.17 68.72±0.92 82.67±0.53 54.04±1.26 18.45±0.97 36.49±4.66 8.85±1.33 5.86\nMCL-MAE 91.84±3.95 64.33±3.61 82.63±0.31 38.07±9.22 11.65±1.77 26.66±3.60 9.07±1.64 7.57\nMCL-EXP 94.24±0.27 68.25±0.56 84.03±0.58 61.19±1.05 21.45±0.93 37.44±2.65 9.05±1.09 4.14\nMCL-LOG 94.31±0.16 70.52±2.42 84.50±0.32 66.25±0.66 21.59±1.11 40.04±2.71 11.89±1.30 1.57\nFWD 93.78±0.35 70.16±1.56 84.16±0.25 64.99±0.64 20.50±0.75 36.72±4.30 13.35±2.40 1.86\nURE-TNN 91.85±0.23 69.16±1.14 82.10±0.30 47.25±2.35 15.29±1.11 30.52±2.32 10.33±1.33 5.00\nURE-TGA 93.79±0.14 75.53±1.10 83.76±0.39 57.37±1.20 13.61±4.51 34.32±2.53 6.33±2.46 3.43\nCPE-I 91.30±0.25 64.82±0.64 81.25±0.77 54.44±1.95 12.01±0.93 28.98±1.16 10.57±1.36 5.57\nCPE-F 93.77±0.35 70.15±1.57 83.95±0.34 64.97±0.61 20.73±1.17 36.46±4.06 14.98±1.53 2.14\nCPE-T 93.40±0.27 69.75±1.49 83.66±0.14 57.94±0.97 20.58±0.78 37.16±2.82 13.14±2.10 3.00\nTable 3: Baseline testing accuracies of strategies with uniform distribution on Synthetic datasets and Real-World\ndatasets.\nSyntheticTabularDatasets Real-WorldDatasets\nYeast Texture Dermatology Control AvgRank CLCIFAR10 CLCIFAR20 CLMIN10 CLMIN20 AvgRank\nSCL-NL 49.33±5.46 97.22±0.83 72.97±6.04 77.50±10.57 3.25 36.21±2.06 7.92±0.23 16.59±3.46 6.82±0.53 2.25\nSCL-EXP 47.15±6.44 97.40±0.79 70.27±6.62 77.08±11.39 4.38 35.95±1.97 7.79±0.06 12.60±0.58 5.81±0.86 4.75\nURE-NN 48.66±2.15 86.95±3.77 85.14±2.34 51.25±6.17 4.25 34.66±1.64 11.77±0.72 19.51±3.21 4.93±0.42 4.00\nURE-GA 47.82±3.33 79.59±5.87 79.05±6.16 45.42±5.45 5.25 37.32±0.61 7.09±0.77 14.34±2.48 5.99±0.92 3.50\nDM 34.73±7.45 95.34±1.32 70.27±6.89 78.75±8.53 4.88 36.78±0.81 7.66±0.10 14.03±1.69 5.48±0.74 4.50\nMCL-MAE 33.72±5.66 60.51±9.40 66.22±11.55 43.33±7.73 8.00 20.01±1.41 6.55±0.76 11.94±1.00 5.78±0.54 7.25\nMCL-EXP 51.01±6.49 97.28±1.00 73.65±5.85 77.50±10.96 2.62 34.54±1.09 7.82±0.23 13.75±2.39 5.43±1.03 5.75\nMCL-LOG 55.37±6.30 96.93±1.05 73.65±7.73 76.67±7.55 3.38 37.35±1.17 7.49±0.39 13.93±2.75 5.79±0.47 4.00\nFWD 58.72±3.17 97.12±0.75 79.05±9.63 90.83±6.72 1.75 39.25±1.55 19.82±0.38 29.63±2.92 10.58±1.14 1.25\nURE-TNN 37.75±14.53 88.01±3.08 81.08±3.82 62.92±5.45 4.00 31.21±1.11 9.47±2.67 17.87±4.33 6.30±0.96 5.50\nURE-TGA 35.57±15.60 81.42±2.74 78.38±5.06 58.75±6.28 6.00 33.68±1.06 5.17±0.25 21.35±4.38 5.88±1.21 5.25\nCPE-I 52.68±1.93 91.39±0.86 81.76±5.85 59.17±13.15 3.50 33.94±1.40 16.79±0.48 20.18±4.25 8.61±0.43 4.25\nCPE-F 56.04±3.84 96.67±0.89 79.73±2.34 90.00±6.24 2.50 38.94±1.39 19.48±0.35 29.82±3.13 10.16±0.84 1.75\nCPE-T 57.89±2.62 95.60±0.53 79.05±1.17 87.92±5.94 3.25 38.80±1.16 19.33±0.75 28.85±0.55 9.36±1.32 3.00\nFollowingthehyperparametersfrom[11],wetrainedeachsetupusingtheAdamoptimizerwithalearningrateselected\nfrom {1e-3, 5e-4, 1e-4, 5e-5, 1e-5}anda fixedbatch sizeof256for 300epochson NVIDIATesla V100\n32GB.Wereserved10%ofthetrainingdataasthevalidationset,assumingthatallground-truthlabelsinthevalidation\nsetareknown. Weselectedthemodelswiththehighestvalidationaccuracyandconducted4trialswithdifferentrandom\nseedsforallexperimentsinthisstudy. Certainalgorithms,referredtoasT-awarealgorithms,useatransitionmatrix\ntocalculatetheloss, whileothers, knownasT-agnosticalgorithms, donotutilizethisinformation. SinceT-aware\nalgorithmsbenefitfromadditionalinformationaboutthetruelabels,adirectcomparisonwithT-agnosticalgorithms\n6\nTable4: ComparisonoftestingaccuraciesunderWeakandStronglevelsofdeviationintransitionmatricesonsynthetic\ndatasets. ThedefinitionofWeakandStrongdeviationaredescribedinsection4.3\n.\nMNIST KMNIST FMNIST CIFAR10 MIN10 AvgRank\nWeak Strong Weak Strong Weak Strong Weak Strong Weak Strong Weak Strong\nSCL-NL 93.94±0.34 33.29±7.21 70.44±2.20 26.40±3.89 82.92±0.31 27.93±3.75 58.63±2.78 24.01±4.27 37.18±0.68 15.64±3.75 3.00 3.20\nSCL-EXP 94.05±0.30 27.64±5.29 68.88±2.00 22.49±2.56 79.65±3.86 24.43±3.67 58.71±3.24 19.03±2.52 35.98±2.76 14.12±4.50 4.60 5.60\nURE-NN 92.17±0.38 48.66±7.65 69.98±0.42 33.62±3.98 82.08±0.57 41.57±4.05 50.09±2.20 31.31±2.63 29.04±2.11 23.04±3.82 5.60 1.20\nURE-GA 94.49±0.23 33.26±14.30 75.46±0.64 23.81±4.71 82.28±0.51 20.85±3.33 59.66±0.89 18.45±5.75 33.44±2.22 16.63±4.24 2.40 5.00\nDM 91.24±0.27 31.05±4.02 67.33±1.66 22.18±2.51 80.55±0.37 26.36±2.21 55.40±2.03 32.92±4.55 32.26±1.23 21.04±3.44 6.20 4.00\nMCL-MAE 67.61±7.37 20.37±6.82 48.64±6.40 18.08±2.23 67.01±9.04 19.91±2.34 21.08±4.87 12.89±2.70 18.63±5.63 11.85±0.97 8.00 8.00\nMCL-EXP 94.06±0.27 26.76±5.43 68.60±1.35 22.65±2.59 79.82±3.87 27.03±2.91 56.76±3.66 18.80±5.52 36.61±0.92 13.82±4.15 4.60 5.60\nMCL-LOG 94.17±0.23 33.65±7.26 71.12±2.27 26.88±4.17 82.61±0.37 28.07±3.98 63.41±1.70 22.92±3.53 37.50±1.82 13.31±4.22 1.60 3.40\nFWD 93.69±0.07 96.23±0.13 70.82±2.10 82.34±0.49 83.77±0.52 86.66±0.25 66.38±2.16 80.04±1.88 38.35±2.86 53.20±2.62 1.20 1.20\nURE-TNN 90.28±0.79 90.85±0.63 65.96±0.85 65.71±4.11 81.04±0.85 82.15±0.93 43.82±2.04 45.30±8.13 24.13±2.69 25.29±4.44 5.40 5.60\nURE-TGA 92.49±0.50 90.79±3.25 71.52±1.09 70.52±5.30 82.52±0.61 64.98±31.88 56.62±1.05 49.13±23.02 31.96±1.78 35.01±9.76 3.60 5.40\nCPE-I 89.27±0.26 93.15±0.50 63.84±1.20 74.39±1.43 80.40±0.30 83.91±1.17 60.20±1.35 77.11±1.67 28.24±1.42 43.72±2.00 5.40 3.80\nCPE-F 93.63±0.06 96.08±0.23 70.78±2.04 82.13±0.60 83.58±0.48 86.44±0.46 66.24±2.04 79.59±1.87 36.74±2.38 52.85±2.56 2.40 2.20\nCPE-T 93.59±0.10 96.07±0.24 70.16±2.17 82.00±0.61 83.29±0.47 85.70±0.76 64.26±1.92 76.68±3.27 36.82±1.09 54.65±2.04 3.00 2.80\nTable5: Comparisonoftestingaccuracieswithdifferentlevelsofnoiseintransitionmatrices. λstandsfortheweights\nbetweenuniformnoisetransitionmatrix 11 andStrongdeviationtransitionmatrix.Datasetswithhigherλarenoisier.\nK K\nMNIST KMNIST FMNIST CIFAR10 MIN10 AvgRank\nλ=0.1 λ=0.5 λ=0.1 λ=0.5 λ=0.1 λ=0.5 λ=0.1 λ=0.5 λ=0.1 λ=0.5 λ=0.1 λ=0.5\nSCL-NL 24.97±5.78 27.12±5.47 19.27±3.75 18.90±2.55 22.72±2.74 25.61±3.09 20.70±2.82 20.29±0.27 18.94±4.80 15.75±3.13 4.80 3.80\nSCL-EXP 18.24±8.41 24.20±5.27 20.36±2.88 19.12±2.60 21.04±1.91 23.97±2.82 19.78±3.39 19.89±1.40 13.80±1.80 14.25±1.22 6.20 5.60\nURE-NN 40.09±3.61 26.81±2.94 31.80±5.54 23.70±3.66 34.61±6.09 30.94±2.70 32.30±4.64 22.42±1.99 22.55±3.71 20.50±1.58 1.00 1.60\nURE-GA 34.89±9.62 27.15±4.16 23.88±3.84 20.49±3.94 26.05±3.47 22.03±1.38 21.97±4.14 20.39±0.72 18.17±4.59 17.59±3.29 2.80 3.20\nDM 27.71±3.70 28.60±2.84 22.05±1.57 19.46±1.01 26.14±3.70 25.31±1.04 22.39±0.47 18.88±0.95 20.30±3.69 15.16±1.59 2.40 3.80\nMCL-MAE 19.45±5.32 21.07±2.35 17.08±2.69 17.70±2.93 18.59±4.96 18.09±5.79 12.96±3.32 12.51±2.91 11.22±0.55 11.79±1.18 7.40 8.00\nMCL-EXP 18.23±8.42 24.12±5.57 20.24±2.93 19.00±2.61 17.70±2.74 23.20±2.76 20.55±2.84 18.28±1.81 16.58±4.92 14.63±2.53 6.60 6.40\nMCL-LOG 22.44±7.10 26.28±5.96 19.99±3.05 19.77±2.75 24.31±4.30 24.43±2.48 20.93±3.33 20.39±1.69 17.87±3.42 15.69±2.00 4.80 3.60\nFWD 95.17±0.21 84.27±3.98 79.56±0.43 60.83±3.83 85.32±0.11 74.84±2.03 75.21±1.02 52.73±2.96 50.10±3.54 30.76±1.61 1.00 3.40\nURE-TNN 89.62±1.01 78.13±3.18 64.66±2.88 47.14±5.24 80.52±1.44 74.54±1.02 46.44±2.28 27.91±3.09 27.21±0.85 20.83±2.29 6.00 6.00\nURE-TGA 91.94±0.58 83.45±2.36 69.39±2.46 49.87±5.56 82.88±0.77 76.36±1.41 49.17±14.47 33.08±3.18 33.13±2.64 24.61±1.19 5.00 4.80\nCPE-I 92.95±0.51 87.92±0.61 72.65±1.57 60.77±2.20 83.22±1.20 78.78±1.05 73.48±2.06 54.94±1.20 41.46±0.97 27.58±2.44 4.00 2.80\nCPE-F 95.16±0.39 87.66±2.36 79.41±0.79 63.15±3.38 84.44±0.73 77.84±2.02 74.21±1.04 55.41±2.34 49.56±3.28 28.15±2.15 2.20 2.60\nCPE-T 94.93±0.43 87.93±1.77 79.24±0.99 63.19±3.55 84.12±1.09 78.55±2.04 74.89±1.10 55.94±1.54 42.93±1.22 28.42±1.29 2.80 1.40\nwouldbeinequitable. Therefore,wepresentandcomparethesealgorithmsseparately,listingT-awarealgorithmsinthe\nuppersectionandT-agnosticalgorithmsinthelowersectionofeachtable.\n4.2 Uniformdistribution\nInthissection,weestablishbaselinesforeachalgorithmunderthestandardCLLsetting,wherethecorrecttransition\nmatrixisprovidedtoT-agnosticmethods,andcomplementarylabels(CLs)areuniformlysampledfromthecomple-\nmentaryset. AnexceptionismadeforCLCIFAR10,CLCIFAR20,CLMIN10,andCLMIN20,whoseCLsarederived\nfromhumanannotationsandarethusnoisy. Toevaluatetheadaptabilityofcurrentalgorithmstoreal-worldscenarios,\nwedividethedatasetsintosyntheticandreal-worldsets.\nTheresultsareshowninTable2and3. Asobserved,FWDandCPE-Fdemonstratethebestoverallperformanceon\nbothsyntheticandreal-worlddatasets,withCPE-FslightlyoutperformingCPE-T,consistentwithfindingsfrom[11].\nSurprisingly,providingthetransitionmatrixtothelearnerinURE-TNNandURE-TGAdoesnotconsistentlyyield\nbetterperformancecomparedtoURE-NNandURE-GA,particularlyonnon-uniformdatasetssuchasCLCIFAR10,\nCLCIFAR20,CLMIN10,andCLMIN20.Wesuggestthatthetransitionmatricesofthesedatasetsmaybeill-conditioned,\nleadingtoinstabilityinURE.\n4.3 Biaseddistribution\nToexaminetheimpactofdisturbancesinthecomplementary-labeldistribution,wefollowtheprocedurefrom[10]to\ngeneratetwobiaseddistributionswithvaryinglevelsofdeviationfromauniformdistribution,asfollows: Foreach\nclassy,thecomplementaryclassesarerandomlydividedintothreesubsets,withprobabilitiesassignedasp ,p ,and\n1 2\np within each subset. We consider two cases for (p ,p ,p ): (a) Strong: (0.75,0.24,0.01) to simulate a stronger\n3 1 2 3 3 3 3\ndeviation. (b)Weak: (0.45,0.30,0.25)tosimulateamilderdeviation. Sincetheseconfigurationsareapplicableonly\n3 3 3\n7\nTable6: Comparisonoftestaccuracieswhereinstanceshave3CLsonSyntheticdatasets.\nMNIST KMNIST FMNIST CIFAR10 CIFAR20 MIN10 MIN20 AvgRank\nSCL-NL 96.83±0.08 81.97±2.19 86.74±0.26 82.76±0.15 34.64±0.37 57.71±1.46 22.25±3.95 1.29\nSCL-EXP 96.67±0.19 77.16±2.85 86.56±0.13 81.80±0.34 34.43±0.93 52.25±1.67 22.42±2.46 3.14\nURE-NN 94.07±0.22 75.61±0.58 84.54±0.03 58.05±1.49 21.66±0.47 40.24±1.37 14.76±0.72 7.43\nURE-GA 95.91±0.12 78.92±0.80 85.77±0.15 74.38±1.49 26.72±0.49 42.65±2.90 17.03±1.59 5.57\nDM 94.74±0.21 77.11±1.60 85.70±0.27 78.14±0.52 30.42±1.40 51.97±2.77 18.76±2.53 5.57\nMCL-MAE 96.53±0.14 74.37±1.42 86.35±0.15 65.15±4.54 11.93±2.26 34.04±3.93 11.47±1.72 7.00\nMCL-EXP 96.60±0.10 76.15±2.72 86.61±0.24 80.43±0.56 33.98±0.55 56.20±1.30 21.74±2.69 3.86\nMCL-LOG 96.63±0.05 81.08±2.69 86.57±0.33 82.36±0.58 34.51±0.95 56.35±1.75 23.03±2.54 2.14\nFWD 96.35±0.12 80.35±1.89 86.73±0.27 81.77±0.63 33.70±1.33 52.69±1.75 23.91±2.00 1.57\nURE-TNN 93.94±0.26 76.00±0.98 85.16±0.09 59.66±0.42 21.49±0.69 36.96±2.14 14.87±0.70 5.71\nURE-TGA 95.99±0.22 79.35±1.08 85.92±0.22 74.09±1.26 25.63±0.72 44.23±2.35 17.13±1.44 4.00\nCPE-I 94.08±0.42 76.42±0.69 84.69±0.18 76.38±0.88 24.55±0.73 38.93±0.97 14.25±1.21 5.14\nCPE-F 96.34±0.12 80.36±1.89 86.74±0.27 81.68±0.75 34.84±1.11 52.64±1.67 23.88±1.73 1.86\nCPE-T 96.28±0.11 79.02±2.32 86.77±0.16 81.69±0.58 32.95±1.14 50.13±2.29 22.99±1.37 2.71\nTable7: Comparisonoftestaccuracieswhereinstanceshave3CLsonSyntheticdatasetsandReal-Worlddatasets.\nSyntheticTabularDatasets Real-WorldDatasets\nYeast Texture Dermatology Control AvgRank CLCIFAR10 CLCIFAR20 CLMIN10 CLMIN20 AvgRank\nSCL-NL 61.58±2.98 98.80±0.25 98.65±1.35 98.33±1.67 1.50 47.30±0.50 8.59±0.75 12.87±2.33 6.87±0.39 3.75\nSCL-EXP 60.40±0.95 98.45±0.50 98.65±1.35 97.92±1.38 2.88 47.12±0.91 9.74±0.52 12.78±1.42 7.10±0.83 3.50\nURE-NN 51.68±5.02 93.35±0.37 85.81±8.41 77.92±11.51 6.50 40.74±1.12 18.43±0.27 21.78±2.66 7.16±0.70 2.75\nURE-GA 50.17±3.33 85.86±4.17 91.22±6.99 73.75±14.55 7.00 47.02±0.45 14.07±0.60 20.83±2.88 7.71±1.24 2.00\nDM 61.24±1.53 97.70±0.37 97.97±2.24 96.25±3.20 4.50 46.89±0.32 9.11±0.31 14.46±2.22 6.84±0.62 4.50\nMCL-MAE 33.22±5.06 72.40±8.37 98.65±1.35 70.42±17.69 6.75 19.83±2.91 7.88±0.28 11.57±0.80 6.64±0.89 7.50\nMCL-EXP 59.90±0.99 98.37±0.35 98.65±1.35 97.50±0.83 3.88 46.97±1.23 8.42±0.27 12.37±1.61 6.62±1.03 6.25\nMCL-LOG 60.40±0.82 98.67±0.43 98.65±1.35 97.50±0.83 3.00 46.13±0.57 8.57±0.20 15.02±2.25 6.55±0.95 5.75\nFWD 61.74±2.07 98.32±0.45 100.00±0.00 97.50±1.86 2.38 52.48±0.63 24.56±0.95 29.33±0.85 10.11±1.29 1.25\nURE-TNN 44.30±10.75 93.48±1.19 99.32±1.17 80.42±6.60 4.75 35.60±0.87 9.91±2.98 17.98±3.25 8.31±0.59 5.25\nURE-TGA 41.11±9.05 89.59±0.54 97.97±2.24 68.75±5.70 5.75 45.08±0.70 5.83±1.36 17.95±5.70 5.78±0.14 5.75\nCPE-I 56.21±4.28 97.39±0.47 92.57±2.95 92.92±1.38 4.50 45.54±2.83 20.41±1.44 24.10±2.26 8.48±1.37 4.00\nCPE-F 62.08±1.81 98.57±0.54 100.00±0.00 97.50±1.86 1.62 51.74±0.98 24.44±1.07 29.51±0.95 9.52±1.71 2.00\nCPE-T 59.73±2.89 98.45±0.36 100.00±0.00 97.50±0.83 2.00 49.79±1.45 20.85±0.52 27.97±1.06 9.70±1.13 2.75\ntodatasetswith10classes,weusethesetwodistributionstogeneratecomplementarylabelsforMNIST,KMNIST,\nFMNIST,MIN10,andCIFAR10,andreporttheresultsinTable4.\nTheresultsindicatethatT-agnosticmethodsperformwellundertheWeakdistributionbutexperienceasignificant\naccuracydropundertheStrongdistribution,suggestingthatthesemethodsaresensitivetodistributionaldeviations. In\ncontrast,T-awaremethodsexhibitmorestableandconsistentperformance. Specifically,weobservethatURE-Tmay\nshowaslightperformancedecreasewhenthedistributionisclosetouniformbutachievessolidresultsunderconditions\nwithstrongdeviationfromauniformdistribution.\n4.4 Noisydistribution\nFollowingthestepsoutlinedin[11],wesimulatemorerestrictiveenvironmentsbyintroducingbothnoisycomple-\nmentarylabelsandincorrecttransitionmatricestothelearners. Weachievethisbygeneratingnoisydatasetsthrough\nthe interpolation of a strong deviation matrix, T , and a uniform noise transition matrix, 11 . The resulting\nstrong K K\ncomplementarylabelsfollowthedistribution(1−λ)T +λ11 ,whileonlyT isprovidedtothelearners,\nstrong K K strong\nwhereλcontrolstheweightofthenoise.\nTheresults,presentedinTable5,indicateaperformancedropacrossallmethods,particularlyasthenoisefactorλ\nincreases. This demonstrates that, while T-aware methods can manage some degree of deviation and noise in the\ntransitionmatrix,theybecomeincreasinglyvulnerableasthegapwidensbetweentheprovidedtransitionmatrixand\ntheactualdistributionwithhighernoiselevels. ThishighlightswhyCPE-ToutperformsotherT-awaremethodsinnoisy\nsettings,asitincludesatrainabletransitionlayerthatmitigatesthisgap.\n4.5 Multi-labeldistribution\nTodemonstratetheversatilityofCLL,weassignthreecomplementarylabelstoeachinstance,sampledfromauniform\ndistributionwithoutrepetition. Forreal-worlddatasets,weusethreehuman-annotatedcomplementarylabelspersample.\nAftergeneratingmultiplecomplementarylabels,one-hotencodingisappliedforMCLserieslosscalculations. For\n8\notherlossfunctions,instancesaredecomposedintomultipleexamples,eachcontainingasinglecomplementarylabel,\nwithashuffledtrainingset.\nTheresultsinTable6revealthattheperformanceofMCLimprovessignificantly,achievingthehighestaccuracyin\nsyntheticmultiple-CLscenarios. However,asshowninTable7,thenoisydistributioninreal-worlddatasetscontinues\ntoimpairtheperformanceofT-agnosticalgorithms. Additionally,astheuseofmultiplecomplementarylabelsincreases\nthenoiselevelinreal-worlddatasets,theperformanceofbothT-agnosticandT-awarealgorithmsdoesnotimprove—or\nmayevendecline—comparedtosingle-CLscenariosonthesamedatasets. Thissuggeststhatsimplyincreasingthe\nnumberofcomplementarylabelsisinsufficienttoeffectivelyaddressthechallengesposedbynoisydistributions.\n5 Conclusion\nInthisstudy,weintroducelibcll,anopen-sourcePyTorchlibrarydesignedtoadvanceresearchincomplementary-\nlabellearning(CLL).TheprimarygoaloflibcllistoprovideastandardizedplatformforevaluatingCLLalgorithms,\naddressingchallengesinstandardization,accessibility,andreproducibility.Thislibraryenablesuserstoeasilycustomize\nvariouscomponentsoftheend-to-endCLLprocess,includingdatapre-processingutilities,implementationsofCLL\nalgorithms,andcomprehensivemetricevaluationsthatreflectrealisticconditions. Todemonstratelibcll’sflexible\nandmodulardesign,weconductdiverseexperimentsencompassingmultipleCLLalgorithms,datasetsrangingfrom\nsynthetictoreal-world,andvariousdistributionassumptions.\nOurexperimentsrevealthatCPEandFWDarethemosteffectiveapproachesforhandlinguniform,biased,andnoisy\ncomplementary-labeldistributions. Incaseswherethereisasubstantialdiscrepancybetweentheknowntransition\nmatrixandtheactualdistribution,suchaswhenusinganestimatedtransitionmatrixinreal-worldscenarios,westrongly\nrecommendCPE.Formulti-complementary-labellearninginsyntheticscenarios,SCL-NLalgorithmsarerecommended.\nHowever,wenotealimitationinhandlingdeviationsfromauniformdistribution. ForMCLfromreal-worlddatasets,\nourfindingsconsistentlyshowthatCPEandFWDalgorithmsoutperformotherexistingalgorithms.\n6 LimitationsandFutureWork\nThereareseveralwaystofurtherenhancethecomprehensivenessofthelibrary. Currently,ourstrategydoesnotinclude\nother T-aware algorithms that may be competitive with CPE and FWD. Additionally, there is growing interest in\nleveragingcomplementarylabelsfromsimilarinstancestoimproveperformance,aframeworkandfunctionsforwhich\narenotyetsupportedinourlibrary. Infuturework,weplantointegratemorerecentlypublishedCLLalgorithmsand\ndevelopframeworkstoaccommodatemoreflexiblelabelingtypesindatasets.\n7 Broaderimpacts\nThelibraryhasthepotentialtoadvancealgorithmsforlearningfromcomplementarylabels,enablingclassifierstobe\ntrainedwithlimitedinformation. However,thiscapabilitymayincreasetheriskofcompromisinguserprivacy. We\nrecommendthatpractitionersremainmindfulofprivacyconcernswhenusingcollecteddatasetsandCLLalgorithms.\n9\nReferences\n[1] MasashiSugiyama,HanBao,TakashiIshida,NanLu,TomoyaSakai,andGangNiu. Machinelearningfrom\nweaksupervision: Anempiricalriskminimizationapproach. MITPress,2022.\n[2] Zhi-HuaZhou. Abriefintroductiontoweaklysupervisedlearning. Nationalsciencereview,5(1):44–53,2018.\n[3] TakashiIshida,GangNiu,WeihuaHu,andMasashiSugiyama. Learningfromcomplementarylabels. Advances\ninneuralinformationprocessingsystems,30,2017.\n[4] Yu-TingChou,GangNiu,Hsuan-TienLin,andMasashiSugiyama. Unbiasedriskestimatorscanmislead: Acase\nstudyoflearningwithcomplementarylabels,2020.\n[5] YuzhouCao,ShuqiLiu,andYitianXu. Multi-complementaryandunlabeledlearningforarbitrarylossesand\nmodels. PatternRecognition,124:108447,2022.\n[6] LeiFeng,TakuoKaneko,BoHan,GangNiu,BoAn,andMasashiSugiyama. Learningwithmultiplecomplemen-\ntarylabels. InInternationalConferenceonMachineLearning,pages3072–3081.PMLR,2020.\n[7] NagarajanNatarajan,InderjitSDhillon,PradeepKRavikumar,andAmbujTewari. Learningwithnoisylabels. In\nAdvancesinNeuralInformationProcessingSystems.CurranAssociates,Inc.,2013.\n[8] RongJinandZoubinGhahramani. Learningwithmultiplelabels. InAdvancesinNeuralInformationProcessing\nSystems,volume15,2002.\n[9] XiyuYu,TongliangLiu,MingmingGong,andDachengTao. Learningwithbiasedcomplementarylabels,2018.\n[10] YiGaoandMin-LingZhang. Discriminativecomplementary-labellearningwithweightedloss. InInternational\nConferenceonMachineLearning,pages3587–3597.PMLR,2021.\n[11] Wei-I Lin and Hsuan-Tien Lin. Reduction from complementary-label learning to probability estimates. In\nProceedingsofthePacific-AsiaConferenceonKnowledgeDiscoveryandDataMining(PAKDD),May2023.\n[12] Hsiu-Hsuan Wang, Wei-I Lin, and Hsuan-Tien Lin. Clcifar: Cifar-derived benchmark datasets with human\nannotatedcomplementarylabels,2023.\n[13] YanwuXu,MingmingGong,JunxiangChen,TongliangLiu,KunZhang,andKayhanBatmanghelich. Generative-\ndiscriminativecomplementarylearning,2019.\n[14] Haoran Jiang, Zhihao Sun, and Yingjie Tian. Comco: Complementary supervised contrastive learning for\ncomplementarylabellearning. NeuralNetworks,169:44–56,2024.\n[15] WeiWang,TakashiIshida,Yu-JieZhang,GangNiu,andMasashiSugiyama. Learningwithcomplementarylabels\nrevisited: Aconsistentapproachvianegative-unlabeledlearning,2023.\n[16] TakashiIshida,GangNiu,AdityaKrishnaMenon,andMasashiSugiyama. Complementary-labellearningfor\narbitrarylossesandmodels,2019.\n[17] HirokiIshiguro,TakashiIshida,andMasashiSugiyama. Learningfromnoisycomplementarylabelswithrobust\nlossfunctions. IEICETRANSACTIONSonInformationandSystems,105(2):364–376,2022.\n[18] ShuqiLiu,YuzhouCao,QiaozhenZhang,LeiFeng,andBoAn. Consistentcomplementary-labellearningvia\norder-preservinglosses. InFranciscoRuiz,JenniferDy,andJan-WillemvandeMeent,editors,Proceedingsof\nThe26thInternationalConferenceonArtificialIntelligenceandStatistics,volume206ofProceedingsofMachine\nLearningResearch,pages8734–8748.PMLR,25–27Apr2023.\n[19] MengWei,YongZhou,ZhongnianLi,andXinzhengXu. Class-imbalancedcomplementary-labellearningvia\nweightedloss,2023.\n[20] Deng-Bao Wang, Lei Feng, and Min-Ling Zhang. Learning from complementary labels via partial-output\nconsistencyregularization. InIJCAI,pages3075–3081,2021.\n[21] AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. pages32–33,2009.\n[22] JiabinLiu,HanyuanHang,BoWang,BiaoLi,HuadongWang,YingjieTian,andYongShi. Gan-cl: Generative\nadversarialnetworksforlearningfromcomplementarylabels. IEEETransactionsonCybernetics,53(1):236–247,\n2023.\n[23] Qinyi Deng, Yong Guo, Zhibang Yang, Haolin Pan, and Jian Chen. Boosting semi-supervised learning with\ncontrastivecomplementarylabeling,2022.\n10",
    "pdf_filename": "libcll_an_Extendable_Python_Toolkit_for_Complementary-Label_Learning.pdf"
}