{
    "title": "libcll an Extendable Python Toolkit for Complementary-Label Learning",
    "context": "Complementary-label learning (CLL) is a weakly supervised learning paradigm for multiclass classi- fication, where only complementary labels—indicating classes an instance does not belong to—are provided to the learning algorithm. Despite CLL’s increasing popularity, previous studies highlight two main challenges: (1) inconsistent results arising from varied assumptions on complementary label generation, and (2) high barriers to entry due to the lack of a standardized evaluation platform across datasets and algorithms. To address these challenges, we introduce libcll, an extensible Python toolkit for CLL research. libcll provides a universal interface that supports a wide range of generation assumptions, both synthetic and real-world datasets, and key CLL algorithms. The toolkit is designed to mitigate inconsistencies and streamline the research process, with easy installation, comprehensive usage guides, and quickstart tutorials that facilitate efficient adoption and implementa- tion of CLL techniques. Extensive ablation studies conducted with libcll demonstrate its utility in generating valuable insights to advance future CLL research. 1 In many real-world applications, training effective classifiers typically depends on obtaining high-quality, accurate labels. However, acquiring such labels is often difficult and costly. To address this challenge, many researchers have turned their attention to weakly supervised learning (WSL), a methodology aimed at training reliable classifiers using only incomplete, imprecise, or inaccurate data [1, 2]. Numerous WSL studies have been conducted to extend our understanding of machine learning capabilities, covering topics such as complementary labels [3, 4], multiple complementary labels [5, 6], noisy labels [7], and learning from partial labels [8]. This work focuses on complementary-label learning (CLL), a WSL problem where each label indicates only a class to which a data instance does not belong [3]. CLL aims to train models with these complementary labels while still enabling accurate predictions of the ordinary labels during testing. CLL makes machine learning more practical in scenarios where obtaining ordinary labels is difficult or costly [3]. Additionally, CLL broadens our understanding of machine learning’s practical potential under limited supervision. Current research on CLL has introduced numerous learning algorithms [4, 9, 10, 11] that have been evaluated using a diverse range of datasets, from synthetic datasets based on varied complementary-label generation assumptions to real-world datasets [12]. However, the performance of these algorithms often varies significantly across studies due to differences in underlying label-generation assumptions, the absence of a standardized evaluation platform, and the use of diverse network architectures [4, 9, 3, 11]. Establishing a fair, reproducible, and stable evaluation environment is therefore essential for advancing CLL research. For instance, variations in network architectures, such as the use of ResNet18 [13, 12] versus ResNet34 [9, 4], contribute to inconsistencies in performance and hinder fair comparisons across studies. Furthermore, most CLL research has not publicly released implementations [6, 11, 4, 14], particularly regarding details like loss calculation and data pre-processing. This lack of accessibility presents a challenge for researchers seeking to validate and build upon existing work in CLL. To enable meaningful comparisons among CLL algorithms and create a user-friendly environment for implementation and innovation, we introduce libcll, a complementary-label learning toolkit built with PyTorch-Lightning. This toolkit arXiv:2411.12276v1  [cs.LG]  19 Nov 2024",
    "body": "LIBCLL: AN EXTENDABLE PYTHON TOOLKIT FOR\nCOMPLEMENTARY-LABEL LEARNING\nNai-Xuan Ye, Tan-Ha Mai, Hsiu-Hsuan Wang, Wei-I Lin, Hsuan-Tien Lin\nNational Taiwan University\n{b09902008, d10922024, b09902033, r10922076, htlin}@csie.ntu.edu.tw\nABSTRACT\nComplementary-label learning (CLL) is a weakly supervised learning paradigm for multiclass classi-\nfication, where only complementary labels—indicating classes an instance does not belong to—are\nprovided to the learning algorithm. Despite CLL’s increasing popularity, previous studies highlight\ntwo main challenges: (1) inconsistent results arising from varied assumptions on complementary\nlabel generation, and (2) high barriers to entry due to the lack of a standardized evaluation platform\nacross datasets and algorithms. To address these challenges, we introduce libcll, an extensible\nPython toolkit for CLL research. libcll provides a universal interface that supports a wide range of\ngeneration assumptions, both synthetic and real-world datasets, and key CLL algorithms. The toolkit\nis designed to mitigate inconsistencies and streamline the research process, with easy installation,\ncomprehensive usage guides, and quickstart tutorials that facilitate efficient adoption and implementa-\ntion of CLL techniques. Extensive ablation studies conducted with libcll demonstrate its utility in\ngenerating valuable insights to advance future CLL research.\n1\nIntroduction\nIn many real-world applications, training effective classifiers typically depends on obtaining high-quality, accurate\nlabels. However, acquiring such labels is often difficult and costly. To address this challenge, many researchers\nhave turned their attention to weakly supervised learning (WSL), a methodology aimed at training reliable classifiers\nusing only incomplete, imprecise, or inaccurate data [1, 2]. Numerous WSL studies have been conducted to extend\nour understanding of machine learning capabilities, covering topics such as complementary labels [3, 4], multiple\ncomplementary labels [5, 6], noisy labels [7], and learning from partial labels [8].\nThis work focuses on complementary-label learning (CLL), a WSL problem where each label indicates only a class\nto which a data instance does not belong [3]. CLL aims to train models with these complementary labels while still\nenabling accurate predictions of the ordinary labels during testing. CLL makes machine learning more practical in\nscenarios where obtaining ordinary labels is difficult or costly [3]. Additionally, CLL broadens our understanding of\nmachine learning’s practical potential under limited supervision.\nCurrent research on CLL has introduced numerous learning algorithms [4, 9, 10, 11] that have been evaluated using\na diverse range of datasets, from synthetic datasets based on varied complementary-label generation assumptions to\nreal-world datasets [12]. However, the performance of these algorithms often varies significantly across studies due to\ndifferences in underlying label-generation assumptions, the absence of a standardized evaluation platform, and the use\nof diverse network architectures [4, 9, 3, 11]. Establishing a fair, reproducible, and stable evaluation environment is\ntherefore essential for advancing CLL research. For instance, variations in network architectures, such as the use of\nResNet18 [13, 12] versus ResNet34 [9, 4], contribute to inconsistencies in performance and hinder fair comparisons\nacross studies. Furthermore, most CLL research has not publicly released implementations [6, 11, 4, 14], particularly\nregarding details like loss calculation and data pre-processing. This lack of accessibility presents a challenge for\nresearchers seeking to validate and build upon existing work in CLL.\nTo enable meaningful comparisons among CLL algorithms and create a user-friendly environment for implementation\nand innovation, we introduce libcll, a complementary-label learning toolkit built with PyTorch-Lightning. This toolkit\narXiv:2411.12276v1  [cs.LG]  19 Nov 2024\n\nFigure 1: Coverage of the libcll Toolkit: This figure provides an overview of the key components included in the\nlibcll toolkit, which encompasses 15 datasets spanning synthetic and real-world scenarios, 5 commonly used models\nin Complementary Label Learning (CLL), 4 CLL assumptions, and 14 CLL algorithms. To the best of our knowledge,\nlibcll is the first comprehensive toolkit specifically dedicated to CLL.\nstandardizes the evaluation process while offering extensive customization options, making it easier for researchers\nto develop, test, reproduce, and refine algorithms. By performing comprehensive benchmark experiments across\nestablished CLL datasets, various algorithms, and a range of complementary-label distributions, as illustrated in Figure\n1, libcll provides a robust and reproducible evaluation framework. Our goal is for libcll to accelerate progress in\nCLL research and foster a collaborative research community.\nFurthermore, libcll includes functions to generate complementary labels using a user-defined transition matrix\nand supports multiple complementary-label generation methods. Additionally, libcll offers extensive benchmarks\nacross a wide range of datasets, from synthetic to real-world, using various CLL algorithms and complementary label\ndistributions. Figure 1 illustrates the comprehensive coverage of libcll. This toolkit, together with these benchmark\nresults, provides the community with a holistic view of CLL, allowing researchers to assess the strengths and weaknesses\nof different approaches, pinpoint areas for improvement, and develop more resilient algorithms. By standardizing\nevaluation metrics and experimental setups, libcll promotes reproducibility and comparability, driving progress in\nthe field and encouraging collaborative research efforts.\nIn summary, the contributions of our libcll toolkit are as follows:\n• To the best of our knowledge, libcll is the first publicly available toolkit for CLL, now accessible at\nhttps://github.com/ntucllab/libcll.\n• We introduce libcll, a platform that offers in-depth insights into Complementary Label Learning (CLL) by\nbenchmarking 15 datasets, 14 algorithms, and diverse label distributions, while supporting customization of\nCLL components and highlighting each approach’s strengths and limitations (see Figure 1).\n2\nPreliminaries and related works\n2.1\nComplementary-Label Learning\nIn ordinary multi-class classification, a dataset D = {(xi, yi)}N\ni=1 is provided to the learning algorithm, where N\ndenotes the number of samples, and the dataset is i.i.d. sampled from an unknown distribution. For each i, xi ∈Rd\nrepresents the d-dimension feature of the ith sample and yi ∈RK, where K denotes the total number of classes in\nthe dataset, with K > 2. The set [K] = {1, 2, ..., K} represents the possible classes to which xi can belongs. The\nobjective of the learning algorithm is to learn a classifier f(x) : Rd →RK that minimizes the classification risk:\nE(x,y)∼D[ℓ(f(x), ey)], where ℓis the loss function and ey is one-hot vector corresponding to label y. The predicted\n2\n\nlabel ˆy for an instance x is determined by applying the argmax function on f(x), i.e. ˆy = arg maxk∈K f(x)k, where\nf(x)k represents the kth output component of f(x).\nIn contrast to ordinary-label learning, complementary-label learning (CLL) shares the same goal of training a classifier\nbut learns from a different label set. In CLL, the ground-truth label y is not accessible to the learning algorithm. Instead,\na complementary label ¯y is provided, which is a class that the instance x does not belong to. The training set of\ncomplementary becomes ¯D = X × ¯Y , where X is input space and ¯Y is the corresponding complementary labels space.\nThe objective of CLL remains consistent with that of ordinary multi-class classification: to learn a classifier capable of\npredicting the correct label of unseen instances, even when trained on a complementary dataset ¯D = {(xi, ¯yi)}N\ni=1.\n2.2\nAssumptions of complementary labels\nComplementary-label learning aims to develop a classifier under the guidance of weak supervision from complementary\nlabels. For synthetic data generation, prior studies assume the distribution of complementary labels only depends on\nthe ordinary labels instead of features; thus, P(¯y | x, y) = P(¯y | y). The transition probability P(¯y | y) is often\nrepresented by a K × K transition matrix T, with Tij = P(¯y = j | y = i).\nThe transition matrix can be further classified into three categories:\n• Uniform [3]: All complementary labels are uniformly and randomly selected from K −1 classes. Based on\nthis assumption, the transition matrix is T =\n1\nK−1(1K −IK).\n• Biased [9]: Any transition matrix that is not uniform is biased.\n• Noisy [11]: A portion of the true labels are mislabeled as complementary labels. Thus, the diagonals of the\ntransition matrix are not necessarily zero.\n• MCL [6]: Each instance has more than one complementary label.\nAdditionally, due to the difficulty of obtaining a transition matrix in real-world scenarios, CONU [15] proposed SCAR\n(Selected Completely At Random), where the generation of complementary labels is independent of both instances’\nfeatures and ground-truth labels; that is, P(¯y | x, y) = P(¯y) = ck, where ck is a constant related only to the k-th class.\nFurthermore, some studies, such as MCL [6], assume that each instance can have multiple complementary labels. This\napproach involves randomly generating a label set ˆy where 1 ≤[ˆy] ≤K −1 and then asking annotators whether the\ngiven label set ˆy contains the true label.\nTable 1: The assumptions of complementary label distribution among different CLL methods.\nInclude in\nlibcll\nuniform\nbiased\nMCL\nnoisy\nclass-\nimbalanced\nSCAR\nPC [3]\n✓\n✓\nFWD [9]\n✓\n✓\n✓\nGA [16]\n✓\n✓\nMCL [6]\n✓\n✓\n✓\nSCL [4]\n✓\n✓\n✓\nLW [10]\n✓\n✓\n✓\nrob [17]\n✓\n✓\n✓\nCPE [11]\n✓\n✓\n✓\nOP [18]\n✓\nComCo [14]\n✓\n✓\n✓\nWCLL [19]\n✓\n✓\n✓\nCONU [15]\n✓\n✓\n✓\n✓\nThe uniform assumption for complementary labels originates from a label-collection perspective. In this approach, each\ndata instance is associated with a randomly assigned label, and workers verify the validity of each label by responding\nwith either ’yes’ or ’no.’ This methodology offers potentially greater efficiency compared to identifying the true label\nfor every data instance. As the pioneering work in complementary-label learning, [3] assumed that the distribution of\ncomplementary labels is noise-free and can be represented by Equation (1).\np(¯y = y | x, y) = 0, p(x, ¯y) =\n1\nK −1\nX\ny̸=¯y\np(x, y)\n(1)\nHowever, assuming complementary labels to be uniformly distributed is not always realistic [9]. Human annotators\nmay exhibit biases toward specific classes and the data instance x. Building on this observation, studies such as\n3\n\n[9, 17, 11, 15] have expanded the loss function to accommodate biased complementary labels. These works employ a\npredefined, feature-independent transition matrix T to represent the distribution of complementary labels conditioned\non their ground-truth labels.\nAdditionally, there is a growing body of research focused on leveraging multiple complementary labels for supervision\n[6, 20, 14, 15], where each instance is assigned multiple complementary labels generated from a transition matrix\nwithout replacement. In fact, the problem of learning from multiple complementary labels can be connected to\npartial-label learning or negative-unlabeled learning [15]. Building on these concepts, [12] curated a human-labeled\nCIFAR [21] dataset with complementary labels to better understand real-world CLL distributions, where the transition\nmatrices are both biased and noisy, and each instance has three complementary labels.\nThere remain several open problems in CLL. First, because the transition matrix T is predefined, there is no universal\ngeneration process for biased complementary labels, and a general framework is needed for fair comparison. Second, in\nthe absence of ordinary labels, the transition matrix T is often assumed to be given. If a small portion of true labels is\navailable, the transition matrix can be estimated using the anchor point method proposed in [9]. These variations can\nlead to inconsistent experimental outcomes. Finally, without ordinary labels, the reliability of validation using only\ncomplementary labels is uncertain. To address these challenges, we introduce libcll, the first CLL toolkit, to support\nfuture CLL research and advance understanding in the weakly-supervised learning community.\n2.3\nPrevious methods on CLL\nIn this section, we present a timeline of key developments in CLL, as illustrated in Figure 2. We implement three\nprimary categories of CLL methods in libcll: URE (unbiased risk estimator), CPE (complementary probability\nestimation), and MCL (multiple complementary label) methods. Additionally, we include several bridging works that\nconnect CLL with other learning frameworks.\nPC\nFWD\nGA\nCCGAN\nMCL\nSCL\nLW\nreg\nrob\nCPE\nGan-CL\nComCo\nOP\nCONU\nCLCIFAR*\n2017\n2018\n2019\n2020\n2021\n2022\n2023\nuniform distribution\nbiased distribution\nnoisy distribution\nWCLL\nmultiple CL\nSCAR assumption\nclass-imbalanced assumption\n*Benchmark dataset\nFigure 2: The development of complementary-label learning.\nURE Series of Works The concept of complementary-label learning was initially proposed by [3], who introduced a\nrisk estimator using Pairwise Comparison (PC) and One-vs-All (OvA) strategies for restricted loss functions. With\nbiased complementary labels, [9] employed forward correction to reconstruct classification risk using cross-entropy\nloss on complementary labels and the transition matrix. Furthermore, [16] removed these restrictions, extending the\nunbiased risk estimator to support arbitrary loss functions and models.\nCPE Framework [11] offered a new perspective by approaching prediction as a decoding process with the transition\nmatrix. This proposed decoding framework can unify various risk estimator methods [9, 4, 10].\nMCL Series of Works [6] introduced the concept of learning from multiple complementary labels, proposing that\nsingle-CL methods can be generalized to handle multi-CL distributions through decomposition.\nRecent studies have extended the CLL framework to various fields, including Generative Adversarial Networks [13, 22],\nnegative-labeled learning [15], robust loss functions for noisy complementary labels [17], and semi-supervised learning\n[23]. In this work, we identify the three fundamental branches of CLL: URE, CPE, and MCL, and implement these\nmethods in libcll. Through extensive experiments, we validate the effectiveness of these approaches. We hope that\nthis toolkit will inspire further advancements within the CLL community in the near future.\n4\n\n3\nLibrary design\nThe code structure of libcll is highly modular and seamlessly integrates with PyTorch. Each component can be added,\nmodified, or removed individually to support diverse experimental designs. In the following paragraphs, we will outline\nthe definitions of strategies, datasets, models, evaluation, and reproducibility.\nStrategies in CLL algorithms are used to calculate the loss.\nAll strategies inherit from the base class\nlibcll.strategies.Strategy, which itself extends pytorch_lightning.LightningModule. This means ev-\nery strategy defined in libcll can be integrated into any other PyTorch Lightning framework.\nAdditionally,\nlibcll.strategies.Strategy already includes implementations for the validation and testing steps, as well as\nevaluation metrics. Users only need to create a new class that inherits from it and modify the training_step to\nincorporate new complementary-label learning methods into the library.\nFigure 3: Training pipeline of libcll. The process begins by initializing the dataset, either real-world or synthetic. For\nreal-world datasets, the pipeline directly proceeds to the calculation of the true transition matrix. If synthetic datasets\nare chosen, complementary labels (CLs) are generated based on user-specified parameters, such as the number of CLs\nper instance and their distribution (uniform, biased or noisy). The preprocessor then calculates the transition matrix\nbased on the true and corresponding CLs for each instance. Afterward, the data preprocessor prepares the DataLoader\nalong with the calculated transition matrix. The training process is initiated using the selected strategy module, the\ntransition matrix, and the DataLoader.\nDatasets in libcll include both ordinary labels and complementary labels (CLs). During data preparation, as shown\nin Figure 3, we begin by initializing an ordinary-label dataset for synthetic scenarios. Users can then generate CLs\nbased on any distribution derived from a specified transition matrix and set the number of CLs per instance. For\nreal-world datasets, we utilize paired data containing human-annotated CLs, ordinary labels, and images provided\nby [12], eliminating the need for CL generation. Users can select the number of human-annotated CLs per instance,\nup to a maximum of three. libcll provides 11 synthetic datasets. Five of these (MNIST, FMNIST, KMNIST,\nCIFAR10, and CIFAR20) are widely used image datasets in CLL research [4, 16, 10, 11, 6, 9]. The Yeast, Texture,\nDermatology, and Synthetic Control datasets are from the UCI Machine Learning Repository and have been applied in\nmulti-complementary-label learning research [6]. Additionally, MicroImageNet10 and MicroImageNet20 are recent\ndatasets proposed by the research team in [12]. For real-world datasets, libcll includes CLCIFAR10, CLCIFAR20,\nCLMicroImageNet10, and CLMicroImageNet20, all of which are also recent contributions from [12].\nModels libcll includes five models: Linear, MLP, DenseNet, ResNet18, and ResNet34. The Linear and MLP models\nare suitable for simpler datasets, such as MNIST, KMNIST, and FMNIST, while the DenseNet, ResNet18, and ResNet34\nmodels are better suited for more complex datasets, such as CIFAR and (Tiny)ImageNet.\nEvaluation To maintain flexibility in the validation set labels and encourage future researchers to adopt more realistic\nsetups, we provide the unbiased risk estimator (URE) and surrogate complementary estimation loss (SCEL) metrics, as\nproposed by [11], for evaluation on validation sets containing only complementary labels. Additionally, we include\naccuracy metrics for evaluating ordinary labels.\nReproducibility libcll saves all hyperparameters in a YAML file, along with the best model weights, and logs\ntraining loss and validation metrics at specified intervals. Additionally, libcll ensures consistent results when using\nthe same hyperparameters and computational device.\n5\n\n4\nBenchmark experiments\n4.1\nExperimental Setups\nIn the subsequent experiments in Sections 4.2 through 4.5, we evaluate the performance of all CLL algorithms\navailable in libcll. Each section utilizes distinct transition matrices and records test accuracy for each dataset and\nalgorithm, enabling a comprehensive assessment of performance. Additionally, we calculate the average rank of test\naccuracy across all experiments, which is displayed in the ’Avg Rank’ column of each table as the primary metric\nfor performance comparison. We use a one-layer MLP model (d-500-c) for the MNIST, KMNIST, FMNIST, Yeast,\nTexture, Control, and Dermatology datasets, and ResNet34 for the CIFAR10, CIFAR20, MicroImageNet10 (MIN10),\nMicroImageNet20 (MIN20), CLCIFAR10, CLCIFAR20, CLMicroImageNet10 (CLMIN10), and CLMicroImageNet20\n(CLMIN20) datasets. Following the setup in [12], we apply standard data augmentation techniques, including\nRandomHorizontalFlip, RandomCrop, and normalization, to each image in the CIFAR and MicroImageNet series\ndatasets.\nTable 2: Baseline testing accuracies of various strategies with a uniform distribution on synthetic datasets. Results\nare grouped into two categories: T-agnostic algorithms (upper section) and T-aware algorithms (lower section). The\nbest performance for each dataset is highlighted in bold, while the second-best result is underlined. The ’Avg Rank’\ncolumn provides an overall ranking of the algorithms across all datasets, where a lower value indicates better general\nperformance.\nMNIST\nKMNIST\nFMNIST\nCIFAR10\nCIFAR20\nMIN10\nMIN20\nAvg Rank\nSCL-NL\n94.20±0.10\n71.88±0.45\n84.59±0.41\n65.45±0.75\n21.43±1.00\n37.59±2.96\n11.75±2.36\n2.57\nSCL-EXP\n94.29±0.29\n67.82±0.52\n84.30±0.32\n61.16±1.18\n21.33±0.20\n39.14±3.49\n10.83±2.32\n3.86\nURE-NN\n92.19±0.04\n69.27±1.24\n82.73±0.27\n46.83±1.41\n15.48±0.67\n29.46±1.85\n10.98±0.83\n5.71\nURE-GA\n94.41±0.16\n74.32±0.45\n83.51±0.22\n58.00±1.49\n15.18±0.47\n33.70±1.34\n8.62±1.68\n4.71\nDM\n92.41±0.17\n68.72±0.92\n82.67±0.53\n54.04±1.26\n18.45±0.97\n36.49±4.66\n8.85±1.33\n5.86\nMCL-MAE\n91.84±3.95\n64.33±3.61\n82.63±0.31\n38.07±9.22\n11.65±1.77\n26.66±3.60\n9.07±1.64\n7.57\nMCL-EXP\n94.24±0.27\n68.25±0.56\n84.03±0.58\n61.19±1.05\n21.45±0.93\n37.44±2.65\n9.05±1.09\n4.14\nMCL-LOG\n94.31±0.16\n70.52±2.42\n84.50±0.32\n66.25±0.66\n21.59±1.11\n40.04±2.71\n11.89±1.30\n1.57\nFWD\n93.78±0.35\n70.16±1.56\n84.16±0.25\n64.99±0.64\n20.50±0.75\n36.72±4.30\n13.35±2.40\n1.86\nURE-TNN\n91.85±0.23\n69.16±1.14\n82.10±0.30\n47.25±2.35\n15.29±1.11\n30.52±2.32\n10.33±1.33\n5.00\nURE-TGA\n93.79±0.14\n75.53±1.10\n83.76±0.39\n57.37±1.20\n13.61±4.51\n34.32±2.53\n6.33±2.46\n3.43\nCPE-I\n91.30±0.25\n64.82±0.64\n81.25±0.77\n54.44±1.95\n12.01±0.93\n28.98±1.16\n10.57±1.36\n5.57\nCPE-F\n93.77±0.35\n70.15±1.57\n83.95±0.34\n64.97±0.61\n20.73±1.17\n36.46±4.06\n14.98±1.53\n2.14\nCPE-T\n93.40±0.27\n69.75±1.49\n83.66±0.14\n57.94±0.97\n20.58±0.78\n37.16±2.82\n13.14±2.10\n3.00\nTable 3: Baseline testing accuracies of strategies with uniform distribution on Synthetic datasets and Real-World\ndatasets.\nSynthetic Tabular Datasets\nReal-World Datasets\nYeast\nTexture\nDermatology\nControl\nAvg Rank\nCLCIFAR10\nCLCIFAR20\nCLMIN10\nCLMIN20\nAvg Rank\nSCL-NL\n49.33±5.46\n97.22±0.83\n72.97±6.04\n77.50±10.57\n3.25\n36.21±2.06\n7.92±0.23\n16.59±3.46\n6.82±0.53\n2.25\nSCL-EXP\n47.15±6.44\n97.40±0.79\n70.27±6.62\n77.08±11.39\n4.38\n35.95±1.97\n7.79±0.06\n12.60±0.58\n5.81±0.86\n4.75\nURE-NN\n48.66±2.15\n86.95±3.77\n85.14±2.34\n51.25±6.17\n4.25\n34.66±1.64\n11.77±0.72\n19.51±3.21\n4.93±0.42\n4.00\nURE-GA\n47.82±3.33\n79.59±5.87\n79.05±6.16\n45.42±5.45\n5.25\n37.32±0.61\n7.09±0.77\n14.34±2.48\n5.99±0.92\n3.50\nDM\n34.73±7.45\n95.34±1.32\n70.27±6.89\n78.75±8.53\n4.88\n36.78±0.81\n7.66±0.10\n14.03±1.69\n5.48±0.74\n4.50\nMCL-MAE\n33.72±5.66\n60.51±9.40\n66.22±11.55\n43.33±7.73\n8.00\n20.01±1.41\n6.55±0.76\n11.94±1.00\n5.78±0.54\n7.25\nMCL-EXP\n51.01±6.49\n97.28±1.00\n73.65±5.85\n77.50±10.96\n2.62\n34.54±1.09\n7.82±0.23\n13.75±2.39\n5.43±1.03\n5.75\nMCL-LOG\n55.37±6.30\n96.93±1.05\n73.65±7.73\n76.67±7.55\n3.38\n37.35±1.17\n7.49±0.39\n13.93±2.75\n5.79±0.47\n4.00\nFWD\n58.72±3.17\n97.12±0.75\n79.05±9.63\n90.83±6.72\n1.75\n39.25±1.55\n19.82±0.38\n29.63±2.92\n10.58±1.14\n1.25\nURE-TNN\n37.75±14.53\n88.01±3.08\n81.08±3.82\n62.92±5.45\n4.00\n31.21±1.11\n9.47±2.67\n17.87±4.33\n6.30±0.96\n5.50\nURE-TGA\n35.57±15.60\n81.42±2.74\n78.38±5.06\n58.75±6.28\n6.00\n33.68±1.06\n5.17±0.25\n21.35±4.38\n5.88±1.21\n5.25\nCPE-I\n52.68±1.93\n91.39±0.86\n81.76±5.85\n59.17±13.15\n3.50\n33.94±1.40\n16.79±0.48\n20.18±4.25\n8.61±0.43\n4.25\nCPE-F\n56.04±3.84\n96.67±0.89\n79.73±2.34\n90.00±6.24\n2.50\n38.94±1.39\n19.48±0.35\n29.82±3.13\n10.16±0.84\n1.75\nCPE-T\n57.89±2.62\n95.60±0.53\n79.05±1.17\n87.92±5.94\n3.25\n38.80±1.16\n19.33±0.75\n28.85±0.55\n9.36±1.32\n3.00\nFollowing the hyperparameters from [11], we trained each setup using the Adam optimizer with a learning rate selected\nfrom {1e-3, 5e-4, 1e-4, 5e-5, 1e-5} and a fixed batch size of 256 for 300 epochs on NVIDIA Tesla V100\n32GB. We reserved 10% of the training data as the validation set, assuming that all ground-truth labels in the validation\nset are known. We selected the models with the highest validation accuracy and conducted 4 trials with different random\nseeds for all experiments in this study. Certain algorithms, referred to as T-aware algorithms, use a transition matrix\nto calculate the loss, while others, known as T-agnostic algorithms, do not utilize this information. Since T-aware\nalgorithms benefit from additional information about the true labels, a direct comparison with T-agnostic algorithms\n6\n\nTable 4: Comparison of testing accuracies under Weak and Strong levels of deviation in transition matrices on synthetic\ndatasets. The definition of Weak and Strong deviation are described in section 4.3\n.\nMNIST\nKMNIST\nFMNIST\nCIFAR10\nMIN10\nAvg Rank\nWeak\nStrong\nWeak\nStrong\nWeak\nStrong\nWeak\nStrong\nWeak\nStrong\nWeak\nStrong\nSCL-NL\n93.94±0.34\n33.29±7.21\n70.44±2.20\n26.40±3.89\n82.92±0.31\n27.93±3.75\n58.63±2.78\n24.01±4.27\n37.18±0.68\n15.64±3.75\n3.00\n3.20\nSCL-EXP\n94.05±0.30\n27.64±5.29\n68.88±2.00\n22.49±2.56\n79.65±3.86\n24.43±3.67\n58.71±3.24\n19.03±2.52\n35.98±2.76\n14.12±4.50\n4.60\n5.60\nURE-NN\n92.17±0.38\n48.66±7.65\n69.98±0.42\n33.62±3.98\n82.08±0.57\n41.57±4.05\n50.09±2.20\n31.31±2.63\n29.04±2.11\n23.04±3.82\n5.60\n1.20\nURE-GA\n94.49±0.23\n33.26±14.30\n75.46±0.64\n23.81±4.71\n82.28±0.51\n20.85±3.33\n59.66±0.89\n18.45±5.75\n33.44±2.22\n16.63±4.24\n2.40\n5.00\nDM\n91.24±0.27\n31.05±4.02\n67.33±1.66\n22.18±2.51\n80.55±0.37\n26.36±2.21\n55.40±2.03\n32.92±4.55\n32.26±1.23\n21.04±3.44\n6.20\n4.00\nMCL-MAE\n67.61±7.37\n20.37±6.82\n48.64±6.40\n18.08±2.23\n67.01±9.04\n19.91±2.34\n21.08±4.87\n12.89±2.70\n18.63±5.63\n11.85±0.97\n8.00\n8.00\nMCL-EXP\n94.06±0.27\n26.76±5.43\n68.60±1.35\n22.65±2.59\n79.82±3.87\n27.03±2.91\n56.76±3.66\n18.80±5.52\n36.61±0.92\n13.82±4.15\n4.60\n5.60\nMCL-LOG\n94.17±0.23\n33.65±7.26\n71.12±2.27\n26.88±4.17\n82.61±0.37\n28.07±3.98\n63.41±1.70\n22.92±3.53\n37.50±1.82\n13.31±4.22\n1.60\n3.40\nFWD\n93.69±0.07\n96.23±0.13\n70.82±2.10\n82.34±0.49\n83.77±0.52\n86.66±0.25\n66.38±2.16\n80.04±1.88\n38.35±2.86\n53.20±2.62\n1.20\n1.20\nURE-TNN\n90.28±0.79\n90.85±0.63\n65.96±0.85\n65.71±4.11\n81.04±0.85\n82.15±0.93\n43.82±2.04\n45.30±8.13\n24.13±2.69\n25.29±4.44\n5.40\n5.60\nURE-TGA\n92.49±0.50\n90.79±3.25\n71.52±1.09\n70.52±5.30\n82.52±0.61\n64.98±31.88\n56.62±1.05\n49.13±23.02\n31.96±1.78\n35.01±9.76\n3.60\n5.40\nCPE-I\n89.27±0.26\n93.15±0.50\n63.84±1.20\n74.39±1.43\n80.40±0.30\n83.91±1.17\n60.20±1.35\n77.11±1.67\n28.24±1.42\n43.72±2.00\n5.40\n3.80\nCPE-F\n93.63±0.06\n96.08±0.23\n70.78±2.04\n82.13±0.60\n83.58±0.48\n86.44±0.46\n66.24±2.04\n79.59±1.87\n36.74±2.38\n52.85±2.56\n2.40\n2.20\nCPE-T\n93.59±0.10\n96.07±0.24\n70.16±2.17\n82.00±0.61\n83.29±0.47\n85.70±0.76\n64.26±1.92\n76.68±3.27\n36.82±1.09\n54.65±2.04\n3.00\n2.80\nTable 5: Comparison of testing accuracies with different levels of noise in transition matrices. λ stands for the weights\nbetween uniform noise transition matrix 1\nK 1K and Strong deviation transition matrix. Datasets with higher λ are noisier.\nMNIST\nKMNIST\nFMNIST\nCIFAR10\nMIN10\nAvg Rank\nλ = 0.1\nλ = 0.5\nλ = 0.1\nλ = 0.5\nλ = 0.1\nλ = 0.5\nλ = 0.1\nλ = 0.5\nλ = 0.1\nλ = 0.5\nλ = 0.1\nλ = 0.5\nSCL-NL\n24.97±5.78\n27.12±5.47\n19.27±3.75\n18.90±2.55\n22.72±2.74\n25.61±3.09\n20.70±2.82\n20.29±0.27\n18.94±4.80\n15.75±3.13\n4.80\n3.80\nSCL-EXP\n18.24±8.41\n24.20±5.27\n20.36±2.88\n19.12±2.60\n21.04±1.91\n23.97±2.82\n19.78±3.39\n19.89±1.40\n13.80±1.80\n14.25±1.22\n6.20\n5.60\nURE-NN\n40.09±3.61\n26.81±2.94\n31.80±5.54\n23.70±3.66\n34.61±6.09\n30.94±2.70\n32.30±4.64\n22.42±1.99\n22.55±3.71\n20.50±1.58\n1.00\n1.60\nURE-GA\n34.89±9.62\n27.15±4.16\n23.88±3.84\n20.49±3.94\n26.05±3.47\n22.03±1.38\n21.97±4.14\n20.39±0.72\n18.17±4.59\n17.59±3.29\n2.80\n3.20\nDM\n27.71±3.70\n28.60±2.84\n22.05±1.57\n19.46±1.01\n26.14±3.70\n25.31±1.04\n22.39±0.47\n18.88±0.95\n20.30±3.69\n15.16±1.59\n2.40\n3.80\nMCL-MAE\n19.45±5.32\n21.07±2.35\n17.08±2.69\n17.70±2.93\n18.59±4.96\n18.09±5.79\n12.96±3.32\n12.51±2.91\n11.22±0.55\n11.79±1.18\n7.40\n8.00\nMCL-EXP\n18.23±8.42\n24.12±5.57\n20.24±2.93\n19.00±2.61\n17.70±2.74\n23.20±2.76\n20.55±2.84\n18.28±1.81\n16.58±4.92\n14.63±2.53\n6.60\n6.40\nMCL-LOG\n22.44±7.10\n26.28±5.96\n19.99±3.05\n19.77±2.75\n24.31±4.30\n24.43±2.48\n20.93±3.33\n20.39±1.69\n17.87±3.42\n15.69±2.00\n4.80\n3.60\nFWD\n95.17±0.21\n84.27±3.98\n79.56±0.43\n60.83±3.83\n85.32±0.11\n74.84±2.03\n75.21±1.02\n52.73±2.96\n50.10±3.54\n30.76±1.61\n1.00\n3.40\nURE-TNN\n89.62±1.01\n78.13±3.18\n64.66±2.88\n47.14±5.24\n80.52±1.44\n74.54±1.02\n46.44±2.28\n27.91±3.09\n27.21±0.85\n20.83±2.29\n6.00\n6.00\nURE-TGA\n91.94±0.58\n83.45±2.36\n69.39±2.46\n49.87±5.56\n82.88±0.77\n76.36±1.41\n49.17±14.47\n33.08±3.18\n33.13±2.64\n24.61±1.19\n5.00\n4.80\nCPE-I\n92.95±0.51\n87.92±0.61\n72.65±1.57\n60.77±2.20\n83.22±1.20\n78.78±1.05\n73.48±2.06\n54.94±1.20\n41.46±0.97\n27.58±2.44\n4.00\n2.80\nCPE-F\n95.16±0.39\n87.66±2.36\n79.41±0.79\n63.15±3.38\n84.44±0.73\n77.84±2.02\n74.21±1.04\n55.41±2.34\n49.56±3.28\n28.15±2.15\n2.20\n2.60\nCPE-T\n94.93±0.43\n87.93±1.77\n79.24±0.99\n63.19±3.55\n84.12±1.09\n78.55±2.04\n74.89±1.10\n55.94±1.54\n42.93±1.22\n28.42±1.29\n2.80\n1.40\nwould be inequitable. Therefore, we present and compare these algorithms separately, listing T-aware algorithms in the\nupper section and T-agnostic algorithms in the lower section of each table.\n4.2\nUniform distribution\nIn this section, we establish baselines for each algorithm under the standard CLL setting, where the correct transition\nmatrix is provided to T-agnostic methods, and complementary labels (CLs) are uniformly sampled from the comple-\nmentary set. An exception is made for CLCIFAR10, CLCIFAR20, CLMIN10, and CLMIN20, whose CLs are derived\nfrom human annotations and are thus noisy. To evaluate the adaptability of current algorithms to real-world scenarios,\nwe divide the datasets into synthetic and real-world sets.\nThe results are shown in Table 2 and 3. As observed, FWD and CPE-F demonstrate the best overall performance on\nboth synthetic and real-world datasets, with CPE-F slightly outperforming CPE-T, consistent with findings from [11].\nSurprisingly, providing the transition matrix to the learner in URE-TNN and URE-TGA does not consistently yield\nbetter performance compared to URE-NN and URE-GA, particularly on non-uniform datasets such as CLCIFAR10,\nCLCIFAR20, CLMIN10, and CLMIN20. We suggest that the transition matrices of these datasets may be ill-conditioned,\nleading to instability in URE.\n4.3\nBiased distribution\nTo examine the impact of disturbances in the complementary-label distribution, we follow the procedure from [10] to\ngenerate two biased distributions with varying levels of deviation from a uniform distribution, as follows: For each\nclass y, the complementary classes are randomly divided into three subsets, with probabilities assigned as p1, p2, and\np3 within each subset. We consider two cases for (p1, p2, p3): (a) Strong: ( 0.75\n3 , 0.24\n3 , 0.01\n3 ) to simulate a stronger\ndeviation. (b) Weak: ( 0.45\n3 , 0.30\n3 , 0.25\n3 ) to simulate a milder deviation. Since these configurations are applicable only\n7\n\nTable 6: Comparison of test accuracies where instances have 3 CLs on Synthetic datasets.\nMNIST\nKMNIST\nFMNIST\nCIFAR10\nCIFAR20\nMIN10\nMIN20\nAvg Rank\nSCL-NL\n96.83±0.08\n81.97±2.19\n86.74±0.26\n82.76±0.15\n34.64±0.37\n57.71±1.46\n22.25±3.95\n1.29\nSCL-EXP\n96.67±0.19\n77.16±2.85\n86.56±0.13\n81.80±0.34\n34.43±0.93\n52.25±1.67\n22.42±2.46\n3.14\nURE-NN\n94.07±0.22\n75.61±0.58\n84.54±0.03\n58.05±1.49\n21.66±0.47\n40.24±1.37\n14.76±0.72\n7.43\nURE-GA\n95.91±0.12\n78.92±0.80\n85.77±0.15\n74.38±1.49\n26.72±0.49\n42.65±2.90\n17.03±1.59\n5.57\nDM\n94.74±0.21\n77.11±1.60\n85.70±0.27\n78.14±0.52\n30.42±1.40\n51.97±2.77\n18.76±2.53\n5.57\nMCL-MAE\n96.53±0.14\n74.37±1.42\n86.35±0.15\n65.15±4.54\n11.93±2.26\n34.04±3.93\n11.47±1.72\n7.00\nMCL-EXP\n96.60±0.10\n76.15±2.72\n86.61±0.24\n80.43±0.56\n33.98±0.55\n56.20±1.30\n21.74±2.69\n3.86\nMCL-LOG\n96.63±0.05\n81.08±2.69\n86.57±0.33\n82.36±0.58\n34.51±0.95\n56.35±1.75\n23.03±2.54\n2.14\nFWD\n96.35±0.12\n80.35±1.89\n86.73±0.27\n81.77±0.63\n33.70±1.33\n52.69±1.75\n23.91±2.00\n1.57\nURE-TNN\n93.94±0.26\n76.00±0.98\n85.16±0.09\n59.66±0.42\n21.49±0.69\n36.96±2.14\n14.87±0.70\n5.71\nURE-TGA\n95.99±0.22\n79.35±1.08\n85.92±0.22\n74.09±1.26\n25.63±0.72\n44.23±2.35\n17.13±1.44\n4.00\nCPE-I\n94.08±0.42\n76.42±0.69\n84.69±0.18\n76.38±0.88\n24.55±0.73\n38.93±0.97\n14.25±1.21\n5.14\nCPE-F\n96.34±0.12\n80.36±1.89\n86.74±0.27\n81.68±0.75\n34.84±1.11\n52.64±1.67\n23.88±1.73\n1.86\nCPE-T\n96.28±0.11\n79.02±2.32\n86.77±0.16\n81.69±0.58\n32.95±1.14\n50.13±2.29\n22.99±1.37\n2.71\nTable 7: Comparison of test accuracies where instances have 3 CLs on Synthetic datasets and Real-World datasets.\nSynthetic Tabular Datasets\nReal-World Datasets\nYeast\nTexture\nDermatology\nControl\nAvg Rank\nCLCIFAR10\nCLCIFAR20\nCLMIN10\nCLMIN20\nAvg Rank\nSCL-NL\n61.58±2.98\n98.80±0.25\n98.65±1.35\n98.33±1.67\n1.50\n47.30±0.50\n8.59±0.75\n12.87±2.33\n6.87±0.39\n3.75\nSCL-EXP\n60.40±0.95\n98.45±0.50\n98.65±1.35\n97.92±1.38\n2.88\n47.12±0.91\n9.74±0.52\n12.78±1.42\n7.10±0.83\n3.50\nURE-NN\n51.68±5.02\n93.35±0.37\n85.81±8.41\n77.92±11.51\n6.50\n40.74±1.12\n18.43±0.27\n21.78±2.66\n7.16±0.70\n2.75\nURE-GA\n50.17±3.33\n85.86±4.17\n91.22±6.99\n73.75±14.55\n7.00\n47.02±0.45\n14.07±0.60\n20.83±2.88\n7.71±1.24\n2.00\nDM\n61.24±1.53\n97.70±0.37\n97.97±2.24\n96.25±3.20\n4.50\n46.89±0.32\n9.11±0.31\n14.46±2.22\n6.84±0.62\n4.50\nMCL-MAE\n33.22±5.06\n72.40±8.37\n98.65±1.35\n70.42±17.69\n6.75\n19.83±2.91\n7.88±0.28\n11.57±0.80\n6.64±0.89\n7.50\nMCL-EXP\n59.90±0.99\n98.37±0.35\n98.65±1.35\n97.50±0.83\n3.88\n46.97±1.23\n8.42±0.27\n12.37±1.61\n6.62±1.03\n6.25\nMCL-LOG\n60.40±0.82\n98.67±0.43\n98.65±1.35\n97.50±0.83\n3.00\n46.13±0.57\n8.57±0.20\n15.02±2.25\n6.55±0.95\n5.75\nFWD\n61.74±2.07\n98.32±0.45\n100.00±0.00\n97.50±1.86\n2.38\n52.48±0.63\n24.56±0.95\n29.33±0.85\n10.11±1.29\n1.25\nURE-TNN\n44.30±10.75\n93.48±1.19\n99.32±1.17\n80.42±6.60\n4.75\n35.60±0.87\n9.91±2.98\n17.98±3.25\n8.31±0.59\n5.25\nURE-TGA\n41.11±9.05\n89.59±0.54\n97.97±2.24\n68.75±5.70\n5.75\n45.08±0.70\n5.83±1.36\n17.95±5.70\n5.78±0.14\n5.75\nCPE-I\n56.21±4.28\n97.39±0.47\n92.57±2.95\n92.92±1.38\n4.50\n45.54±2.83\n20.41±1.44\n24.10±2.26\n8.48±1.37\n4.00\nCPE-F\n62.08±1.81\n98.57±0.54\n100.00±0.00\n97.50±1.86\n1.62\n51.74±0.98\n24.44±1.07\n29.51±0.95\n9.52±1.71\n2.00\nCPE-T\n59.73±2.89\n98.45±0.36\n100.00±0.00\n97.50±0.83\n2.00\n49.79±1.45\n20.85±0.52\n27.97±1.06\n9.70±1.13\n2.75\nto datasets with 10 classes, we use these two distributions to generate complementary labels for MNIST, KMNIST,\nFMNIST, MIN10, and CIFAR10, and report the results in Table 4.\nThe results indicate that T-agnostic methods perform well under the Weak distribution but experience a significant\naccuracy drop under the Strong distribution, suggesting that these methods are sensitive to distributional deviations. In\ncontrast, T-aware methods exhibit more stable and consistent performance. Specifically, we observe that URE-T may\nshow a slight performance decrease when the distribution is close to uniform but achieves solid results under conditions\nwith strong deviation from a uniform distribution.\n4.4\nNoisy distribution\nFollowing the steps outlined in [11], we simulate more restrictive environments by introducing both noisy comple-\nmentary labels and incorrect transition matrices to the learners. We achieve this by generating noisy datasets through\nthe interpolation of a strong deviation matrix, Tstrong, and a uniform noise transition matrix,\n1\nK 1K. The resulting\ncomplementary labels follow the distribution (1 −λ)Tstrong + λ 1\nK 1K, while only Tstrong is provided to the learners,\nwhere λ controls the weight of the noise.\nThe results, presented in Table 5, indicate a performance drop across all methods, particularly as the noise factor λ\nincreases. This demonstrates that, while T-aware methods can manage some degree of deviation and noise in the\ntransition matrix, they become increasingly vulnerable as the gap widens between the provided transition matrix and\nthe actual distribution with higher noise levels. This highlights why CPE-T outperforms other T-aware methods in noisy\nsettings, as it includes a trainable transition layer that mitigates this gap.\n4.5\nMulti-label distribution\nTo demonstrate the versatility of CLL, we assign three complementary labels to each instance, sampled from a uniform\ndistribution without repetition. For real-world datasets, we use three human-annotated complementary labels per sample.\nAfter generating multiple complementary labels, one-hot encoding is applied for MCL series loss calculations. For\n8\n\nother loss functions, instances are decomposed into multiple examples, each containing a single complementary label,\nwith a shuffled training set.\nThe results in Table 6 reveal that the performance of MCL improves significantly, achieving the highest accuracy in\nsynthetic multiple-CL scenarios. However, as shown in Table 7, the noisy distribution in real-world datasets continues\nto impair the performance of T-agnostic algorithms. Additionally, as the use of multiple complementary labels increases\nthe noise level in real-world datasets, the performance of both T-agnostic and T-aware algorithms does not improve—or\nmay even decline—compared to single-CL scenarios on the same datasets. This suggests that simply increasing the\nnumber of complementary labels is insufficient to effectively address the challenges posed by noisy distributions.\n5\nConclusion\nIn this study, we introduce libcll, an open-source PyTorch library designed to advance research in complementary-\nlabel learning (CLL). The primary goal of libcll is to provide a standardized platform for evaluating CLL algorithms,\naddressing challenges in standardization, accessibility, and reproducibility. This library enables users to easily customize\nvarious components of the end-to-end CLL process, including data pre-processing utilities, implementations of CLL\nalgorithms, and comprehensive metric evaluations that reflect realistic conditions. To demonstrate libcll’s flexible\nand modular design, we conduct diverse experiments encompassing multiple CLL algorithms, datasets ranging from\nsynthetic to real-world, and various distribution assumptions.\nOur experiments reveal that CPE and FWD are the most effective approaches for handling uniform, biased, and noisy\ncomplementary-label distributions. In cases where there is a substantial discrepancy between the known transition\nmatrix and the actual distribution, such as when using an estimated transition matrix in real-world scenarios, we strongly\nrecommend CPE. For multi-complementary-label learning in synthetic scenarios, SCL-NL algorithms are recommended.\nHowever, we note a limitation in handling deviations from a uniform distribution. For MCL from real-world datasets,\nour findings consistently show that CPE and FWD algorithms outperform other existing algorithms.\n6\nLimitations and Future Work\nThere are several ways to further enhance the comprehensiveness of the library. Currently, our strategy does not include\nother T-aware algorithms that may be competitive with CPE and FWD. Additionally, there is growing interest in\nleveraging complementary labels from similar instances to improve performance, a framework and functions for which\nare not yet supported in our library. In future work, we plan to integrate more recently published CLL algorithms and\ndevelop frameworks to accommodate more flexible labeling types in datasets.\n7\nBroader impacts\nThe library has the potential to advance algorithms for learning from complementary labels, enabling classifiers to be\ntrained with limited information. However, this capability may increase the risk of compromising user privacy. We\nrecommend that practitioners remain mindful of privacy concerns when using collected datasets and CLL algorithms.\n9\n\nReferences\n[1] Masashi Sugiyama, Han Bao, Takashi Ishida, Nan Lu, Tomoya Sakai, and Gang Niu. Machine learning from\nweak supervision: An empirical risk minimization approach. MIT Press, 2022.\n[2] Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National science review, 5(1):44–53, 2018.\n[3] Takashi Ishida, Gang Niu, Weihua Hu, and Masashi Sugiyama. Learning from complementary labels. Advances\nin neural information processing systems, 30, 2017.\n[4] Yu-Ting Chou, Gang Niu, Hsuan-Tien Lin, and Masashi Sugiyama. Unbiased risk estimators can mislead: A case\nstudy of learning with complementary labels, 2020.\n[5] Yuzhou Cao, Shuqi Liu, and Yitian Xu. Multi-complementary and unlabeled learning for arbitrary losses and\nmodels. Pattern Recognition, 124:108447, 2022.\n[6] Lei Feng, Takuo Kaneko, Bo Han, Gang Niu, Bo An, and Masashi Sugiyama. Learning with multiple complemen-\ntary labels. In International Conference on Machine Learning, pages 3072–3081. PMLR, 2020.\n[7] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. In\nAdvances in Neural Information Processing Systems. Curran Associates, Inc., 2013.\n[8] Rong Jin and Zoubin Ghahramani. Learning with multiple labels. In Advances in Neural Information Processing\nSystems, volume 15, 2002.\n[9] Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary labels, 2018.\n[10] Yi Gao and Min-Ling Zhang. Discriminative complementary-label learning with weighted loss. In International\nConference on Machine Learning, pages 3587–3597. PMLR, 2021.\n[11] Wei-I Lin and Hsuan-Tien Lin. Reduction from complementary-label learning to probability estimates. In\nProceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), May 2023.\n[12] Hsiu-Hsuan Wang, Wei-I Lin, and Hsuan-Tien Lin. Clcifar: Cifar-derived benchmark datasets with human\nannotated complementary labels, 2023.\n[13] Yanwu Xu, Mingming Gong, Junxiang Chen, Tongliang Liu, Kun Zhang, and Kayhan Batmanghelich. Generative-\ndiscriminative complementary learning, 2019.\n[14] Haoran Jiang, Zhihao Sun, and Yingjie Tian. Comco: Complementary supervised contrastive learning for\ncomplementary label learning. Neural Networks, 169:44–56, 2024.\n[15] Wei Wang, Takashi Ishida, Yu-Jie Zhang, Gang Niu, and Masashi Sugiyama. Learning with complementary labels\nrevisited: A consistent approach via negative-unlabeled learning, 2023.\n[16] Takashi Ishida, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. Complementary-label learning for\narbitrary losses and models, 2019.\n[17] Hiroki Ishiguro, Takashi Ishida, and Masashi Sugiyama. Learning from noisy complementary labels with robust\nloss functions. IEICE TRANSACTIONS on Information and Systems, 105(2):364–376, 2022.\n[18] Shuqi Liu, Yuzhou Cao, Qiaozhen Zhang, Lei Feng, and Bo An. Consistent complementary-label learning via\norder-preserving losses. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of\nThe 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine\nLearning Research, pages 8734–8748. PMLR, 25–27 Apr 2023.\n[19] Meng Wei, Yong Zhou, Zhongnian Li, and Xinzheng Xu. Class-imbalanced complementary-label learning via\nweighted loss, 2023.\n[20] Deng-Bao Wang, Lei Feng, and Min-Ling Zhang. Learning from complementary labels via partial-output\nconsistency regularization. In IJCAI, pages 3075–3081, 2021.\n[21] Alex Krizhevsky. Learning multiple layers of features from tiny images. pages 32–33, 2009.\n[22] Jiabin Liu, Hanyuan Hang, Bo Wang, Biao Li, Huadong Wang, Yingjie Tian, and Yong Shi. Gan-cl: Generative\nadversarial networks for learning from complementary labels. IEEE Transactions on Cybernetics, 53(1):236–247,\n2023.\n[23] Qinyi Deng, Yong Guo, Zhibang Yang, Haolin Pan, and Jian Chen. Boosting semi-supervised learning with\ncontrastive complementary labeling, 2022.\n10",
    "pdf_filename": "libcll_an_Extendable_Python_Toolkit_for_Complementary-Label_Learning.pdf"
}