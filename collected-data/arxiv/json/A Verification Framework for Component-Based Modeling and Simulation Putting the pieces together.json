{
    "title": "A Verification Framework for Component-Based Modeling and Simulation Putting the pieces together",
    "context": "",
    "body": "A Verification Framework for Component \nBased Modeling and Simulation \n“Putting the pieces together” \n \nImran Mahmood \n \n  \n \n \n \n \n \n \n \n \nDoctoral thesis in Electronics and Computer Systems \nStockholm, Sweden 2013 \n \n\n   \nPage \n2 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nISBN 978-91-7501-628-3 \nTRITA-ICT/ECS AVH 13:01 \nISSN 1653-6363 \nISRN KTH/ICT/ECS/AVH-13/01-SE \n \n \nKTH School of Information and \nCommunication Technology \nSE-164 40 Kista \nSweden \n \n \n \nAkademisk avhandling som med tillstand av Kungliga Tekniska högskolan \nframlägges till offentlig granskning för avläggande av teknologie doktorsexamen \ntisdagen den 26 feb 2013 kl 14.00 i Sal E, Forum, Isafjordsgatan 39, Kista.  \n \n Imran Mahmood, February 2013. \n \nTryck: Universitetservice US AB  \n \n\n   \nPage \n3 \n \nAcknowledgement \nI would like to dedicate this manuscript to my loved ones, including some dignitaries, \nmy parents who recently passed away, my guardians Mr.  & Mrs. Sajid Latif who \nraised me well to make me see this day and most important of all: my beloved wife \nand my little daughter. Their sacrifice for being apart and for my long absence cannot \nbe compensated for anything.  \n \nI offer my deepest gratitude to my supervisor Professor Rassul Ayani for this \ndevotion and support. Instead of just giving me the directions he actually grabbed my \nhand and took me to the destination like a true guide. I am honored to work under \nhis supervision.  I am thankful to Assoc. Professor Vladimir Vlassov who gave sound \nadvice and provided valuable contributions in my research. I offer my affectionate \ntribute to the esteemed palace of knowledge, the Royal Institute of Technology, and \nspecially the school of Information and Communication Technology.  \n \nI am thankful for continuous support and encouragement from Dr. Farshad Moradi \nfrom Swedish Defense Research agency (FOI). I am grateful for the constructive \ncritics I received from my opponent Dr. Gary Tan and the member of the evaluation \ncommittee Dr. Oliver Dale. \n \nI am very grateful for the Higher Education Commission of Pakistan to provide \nentire financial support for my studies. I thank Mrs. Mumtaz Begum for her support. \nI would like to offer special thanks to Mr. Awais Ali Sohrawardi and Dr. B. \nMuhammad for their moral support during my study period. I thank all my \ncolleagues, friends and especially the cricket team for wonderful time.  \n \nFinally I thank Sweden for its hospitality, care and warm memories.  \n \n \n \n \nImran Mahmood  \nJanuary 2013, Stockholm \n \n \n \n \n\n   \nPage \n4 \n \nAbstract \n \nThe discipline of component-based modeling and simulation offers promising gains \nincluding reduction in development cost, time, and system complexity. This \nparadigm is very profitable as it promotes the use and reuse of modular components \nand is auspicious for effective development of complex simulations. It however is \nconfronted by a series of research challenges when it comes to actually practise this \nmethodology. One of such important issue is Composability verification. In modeling \nand simulation (M&S), composability is the capability to select and assemble \ncomponents in various combinations to satisfy specific user requirements. Therefore \nto ensure the correctness of a composed model, it is verified with respect to its \nrequirements specifications.   \nThere are different approaches and existing component modeling frameworks that \nsupport composability. Though in our observation most of the component modeling \nframeworks possess none or weak built-in support for the composability verification. \nOne such framework is Base Object Model (BOM) which fundamentally poses a \nsatisfactory potential for effective model composability and reuse. However it falls \nshort of required semantics, necessary modeling characteristics and built-in \nevaluation techniques, which are essential for modeling complex system behavior and \nreasoning about the validity of the composability at different levels. \nIn this thesis a comprehensive verification framework is proposed to contend with \nsome important issues in composability verification and a verification process is \nsuggested to verify composability of different kinds of systems models, such as \nreactive, real-time and probabilistic systems. With an assumption that all these \nsystems are concurrent in nature in which different composed components interact \nwith each other simultaneously, the requirements for the extensive techniques for the \nstructural and behavioral analysis becomes increasingly challenging. The proposed \nverification framework provides methods, techniques and tool support for verifying \ncomposability at its different levels. These levels are defined as foundations of \nconsistent model composability. Each level is discussed in detail and an approach is \npresented to verify composability at that level. In particular we focus on the \nDynamic-Semantic Composability level due to its significance in the overall \ncomposability correctness and also due to the level of difficulty it poses in the \nprocess. In order to verify composability at this level we investigate the application of \nthree different approaches namely (i) Petri Nets based Algebraic Analysis (ii) Colored \nPetri Nets (CPN) based State-space Analysis and (iii) Communicating Sequential \nProcesses based Model Checking. All the three approaches attack the problem of \nverifying dynamic-semantic composability in different ways however they all share \nthe same aim i.e., to confirm the correctness of a composed model with respect to its \nrequirement specifications. Beside the operative integration of these approaches in \nour framework, we also contributed in the improvement of each approach for \neffective applicability in the composability verification. Such as applying algorithms \nfor automating Petri Net algebraic computations, introducing a state-space reduction \ntechnique in CPN based state-space analysis, and introducing function libraries to \nperform verification tasks and help the modeler with ease of use during the \ncomposability verification. We also provide detailed examples of using each approach \nwith different models to explain the verification process and their functionality. \nLastly we provide a comparison of these approaches and suggest guidelines for \n\n   \nPage \n5 \n \nchoosing the right one based on the nature of the model and the available \ninformation. With a right choice of an approach and following the guidelines of our \ncomponent-based M&S life-cycle a modeler can easily construct BOM based \ncomposed models and can verify them with respect to the requirement specifications. \n \n \n \nKeywords: \nModeling \nand \nSimulation, \nComponent-based \ndevelopment, \nComposability, \nSemantic \nComposability, Dynamic-Semantic Composability, Verification, Correctness, Petri Nets Analysis, \nAlgebraic Techniques, Colored Petri Nets, State-space Analysis, Communicating Sequential \nProcesses,  Model Checking. \n \n \n\n   \nPage \n6 \n \nTable of  Contents \n \nAcknowledgement ..................................................................................................................... 3 \nAbstract...................................................................................................................................... 4 \nTable of Contents ...................................................................................................................... 6 \nList of Figures ........................................................................................................................... 9 \nList of Tables ............................................................................................................................ 11 \nList of Acronyms....................................................................................................................... 13 \nChapter 1 ................................................................................................................................... 16 \nIntroduction .............................................................................................................................. 16 \n1.1 \nBackground and the opening perspective ......................................................................................... 16 \n1.1.1 \nComponent based Software Engineering .............................................................................. 17 \n1.1.2 \nComponent based Modeling & Simulation ........................................................................... 18 \n1.1.3 \nModeling and Analysis using Petri Nets ................................................................................ 19 \n1.1.4 \nModeling and Analysis using Process Algebra ..................................................................... 19 \n1.1.5 \nModel Verification ..................................................................................................................... 19 \n1.2 \nSummary of the opening perspective ................................................................................................ 20 \n1.3 \nPreliminaries ........................................................................................................................................... 20 \n1.3.1 \nDefinition 1: Set of Components ............................................................................................ 20 \n1.3.2 \nDefinition 2: Requirement Specification ............................................................................... 20 \n1.3.3 \nDefinition 3: Composition & Composability Pattern ......................................................... 21 \n1.3.4 \nDefinition 4: Satisfiability Operator........................................................................................ 21 \n1.4 \nProblem Statement ............................................................................................................................... 21 \n1.5 \nApproach ................................................................................................................................................ 22 \n1.5.1 \nProblem Domain ....................................................................................................................... 22 \n1.5.2 \nSolution Domain ........................................................................................................................ 22 \n1.5.3 \nSolution Statement ..................................................................................................................... 24 \n1.6 \nScope of the Thesis ............................................................................................................................... 24 \n1.6.1 \nCorrectness.................................................................................................................................. 24 \n1.6.2 \nValidation .................................................................................................................................... 24 \n1.6.3 \nEmergence................................................................................................................................... 25 \n1.6.4 \nGeneralization ............................................................................................................................ 25 \n1.7 \nSummary of the Contributions ........................................................................................................... 25 \n1.8 \nStructure of the Thesis ......................................................................................................................... 26 \nChapter 2 .................................................................................................................................. 28 \nComponent Based Modeling and Simulation .......................................................................... 28 \n2.1 \nComposability in M&S ......................................................................................................................... 28 \n2.2 \nA Brief History of Composability and related work ....................................................................... 29 \n2.2.1 \nInitiation ...................................................................................................................................... 29 \n2.2.2 \nTheoretical evolution ................................................................................................................ 29 \n2.2.3 \nStandards & Frameworks ......................................................................................................... 29 \n2.2.4 \nTechnological Advances ........................................................................................................... 29 \n2.2.5 \nComposability verification and Validation ............................................................................ 30 \n2.3 \nTheory of Composability ..................................................................................................................... 30 \n2.4 \nConcepts related to Composability .................................................................................................... 32 \n2.4.1 \nComposability vs. Reusability .................................................................................................. 32 \n2.4.2 \nComposability vs. Interoperability .......................................................................................... 33 \n2.5 \nComposability Levels ........................................................................................................................... 34 \n2.5.1 \nSyntactic level: ............................................................................................................................ 35 \n2.5.2 \nStatic-Semantic level: ................................................................................................................. 35 \n2.5.3 \nDynamic-Semantic level: .......................................................................................................... 35 \n2.5.4 \nPragmatic level: ........................................................................................................................... 36 \n2.6 \nComposability frameworks.................................................................................................................. 36 \n2.6.1 \nDiscrete Event System Specification (DEVS) ...................................................................... 37 \n2.7 \nBase Object Model (BOM) framework ............................................................................................. 38 \n2.7.1 \nStructure of BOM ...................................................................................................................... 39 \n2.7.2 \nBOM Assembly .......................................................................................................................... 41 \n\n   \nPage \n7 \n \n2.7.3 \nModel Mapping and Object Model Definition ..................................................................... 42 \n2.7.4 \nFormal specification for the Compositon of BOM ............................................................. 42 \n2.7.5 \nSummary ...................................................................................................................................... 45 \nChapter 3 .................................................................................................................................. 46 \nExecutable Modeling Formalisms ........................................................................................... 46 \n3.1 \nPetri Nets ................................................................................................................................................ 46 \n3.1.1 \nPN Definitions and Concept ................................................................................................... 47 \n3.1.2 \nPetri net graph ............................................................................................................................ 47 \n3.1.3 \nProperties of PN ........................................................................................................................ 49 \n3.1.4 \nPN Analysis ................................................................................................................................. 51 \n3.1.5 \nPN Classes ................................................................................................................................... 58 \n3.2 \nCommunicating Sequential Processes ............................................................................................... 62 \n3.2.1 \nBasic Concepts and Definitions .............................................................................................. 62 \n3.2.2 \nCSP Analysis Techniques ......................................................................................................... 64 \n3.2.3 \nTemporal Logics ........................................................................................................................ 65 \n3.2.4 \nTime CSP .................................................................................................................................... 66 \n3.2.5 \nProbabilistic Systems ................................................................................................................. 66 \n3.2.6 \nCSP Implementation Tools ...................................................................................................... 67 \n3.2.7 \nProcess Analysis Toolkit (PAT) .............................................................................................. 67 \n3.3 \nSummary ................................................................................................................................................. 68 \nChapter 4 .................................................................................................................................. 69 \nVerification and Analysis .......................................................................................................... 69 \n4.1 \nSome Basic Concepts in Modeling and Simulation ........................................................................ 70 \n4.1.1 \nVerification and Validation in a Modeling Process .............................................................. 71 \n4.2 \nThe Principles of Top-Down Refinement ....................................................................................... 73 \n4.3 \nVerification techniques ........................................................................................................................ 74 \n4.3.1 \nInformal Techniques ................................................................................................................. 74 \n4.3.2 \nStatic Analysis: ............................................................................................................................ 75 \n4.3.3 \nDynamic Analysis:...................................................................................................................... 76 \n4.3.4 \nFormal Analysis .......................................................................................................................... 76 \n4.4 \nSummary ................................................................................................................................................. 77 \nChapter 5 .................................................................................................................................. 79 \nProposed Methodology and the Verification Framework ........................................................ 79 \n5.1 \nComponent-based Modeling & Simulation life-cycle ..................................................................... 79 \n5.2 \nInception ................................................................................................................................................. 80 \n5.3 \nModeling ................................................................................................................................................. 81 \n5.4 \nExecution ............................................................................................................................................... 82 \n5.5 \nAnalysis ................................................................................................................................................... 82 \n5.6 \nComposability Verification Framework ............................................................................................ 83 \n5.6.1 \nDiscovery Matching and Composition (DMC) .................................................................... 83 \n5.6.2 \nStructural and Behavioral Evaluation ..................................................................................... 84 \n5.6.3 \nStatic Analysis ............................................................................................................................. 84 \n5.6.4 \nDynamic Analysis....................................................................................................................... 90 \n5.7 \nPN Algebraic Technique...................................................................................................................... 93 \n5.7.1 \nBOM to PNML Transformation ............................................................................................ 93 \n5.7.2 \nPN Algebraic computations ..................................................................................................... 93 \n5.7.3 \nProperty Verification Method.................................................................................................. 95 \n5.8 \nCPN based State-Space Analysis Technique .................................................................................... 96 \n5.8.1 \nBOM Extension ......................................................................................................................... 97 \n5.8.2 \nE-BOM to CPN Component Transformation ..................................................................... 99 \n5.8.3 \nVerification of the composed CPN model ........................................................................ 105 \n5.9 \nCSP based Model Checking Technique ......................................................................................... 110 \n5.9.1 \nBOM Extension ...................................................................................................................... 110 \n5.9.2 \nE-BOM to CSP# Transformation ....................................................................................... 112 \n5.9.3 \nVerification of the composed CPN model ........................................................................ 113 \n5.10 \nSummary ........................................................................................................................................ 115 \nChapter 6 ................................................................................................................................ 116 \nComposability Verification Process ........................................................................................ 116 \n6.1 \nComposability Verification Process ................................................................................................ 116 \n6.1.1 \nFormulation of Simuland, Requirements and Conceptual Model.................................. 126 \n\n   \nPage \n8 \n \n6.1.2 \nSyntactic Matching Process ................................................................................................... 127 \n6.1.3 \nStatic-Semantic Matching Process ....................................................................................... 127 \n6.1.4 \nState-machine Matching Process .......................................................................................... 127 \n6.1.5 \nApproach Selection for Dynamic-Semantic Composability Verification ..................... 127 \n6.1.6 \nPN Algebraic Technique ....................................................................................................... 128 \n6.1.7 \nState-Space Analysis Technique ........................................................................................... 128 \n6.1.8 \nModel Checking ...................................................................................................................... 128 \n6.2 \nSummary .............................................................................................................................................. 129 \nChapter 7 ................................................................................................................................ 130 \nFairness verification using PN Algebraic Techniques ........................................................... 130 \n7.1 \nFairness ................................................................................................................................................ 130 \n7.2 \nFairness Verification .......................................................................................................................... 131 \n7.3 \nManufacturing system ....................................................................................................................... 132 \n7.3.1 \nScenario I .................................................................................................................................. 132 \n7.3.2 \nScenario II ................................................................................................................................ 138 \n7.4 \nSummary .............................................................................................................................................. 141 \nChapter 8 ................................................................................................................................ 142 \nModel Verification using State-space Analysis techniques .................................................... 142 \n8.1 \nCombat Modeling .............................................................................................................................. 142 \n8.1.1 \nSituated Environment ............................................................................................................ 142 \n8.1.2 \nMoving ...................................................................................................................................... 142 \n8.1.3 \nLooking (or sensing) ............................................................................................................... 143 \n8.1.4 \nShooting .................................................................................................................................... 143 \n8.1.5 \nCommunication: ...................................................................................................................... 143 \n8.2 \nField Artillery ...................................................................................................................................... 144 \n8.2.1 \nSimuland ................................................................................................................................... 145 \n8.2.2 \nField Artillery Model .............................................................................................................. 145 \n8.2.3 \nRequirement Specification..................................................................................................... 148 \n8.3 \nVerification of the FA model using CPN State-Space Analysis ................................................ 149 \n8.3.1 \nStatic and Dynamic Analysis ................................................................................................. 149 \n8.3.2 \nBOM to E-BOM extension .................................................................................................. 149 \n8.3.3 \nE-BOM to CPN Transformation ........................................................................................ 153 \n8.3.4 \nComposition of CPN Components ..................................................................................... 158 \n8.3.5 \nState space Analysis ................................................................................................................ 159 \n8.4 \nState Space Reduction ....................................................................................................................... 162 \n8.5 \nSummary .............................................................................................................................................. 164 \nChapter 9 ................................................................................................................................ 165 \nModel Verification using CSP based Model Checking Technique ........................................ 165 \n9.1 \nField Artillery Scenario ...................................................................................................................... 165 \n9.2 \nRequirement Specification ................................................................................................................ 168 \n9.3 \nVerification using Model Checking ................................................................................................ 169 \n9.3.1 \nStatic and Dynamic Analysis ................................................................................................. 169 \n9.3.2 \nBOM to E-BOM extension .................................................................................................. 169 \n9.3.3 \nE-BOM to CSP# Transformation ....................................................................................... 171 \n9.3.4 \nModel Checking of Field Artillery Model ........................................................................... 174 \n9.4 \nSummary .............................................................................................................................................. 176 \nChapter 10 ............................................................................................................................... 177 \nSummary and Conclusion....................................................................................................... 177 \n10.1 \nGuidelines for choosing an approach ....................................................................................... 181 \n10.1.1 \nPN Algebraic Technique .................................................................................................. 181 \n10.1.2 \nCPN based State-Space analysis Technique ................................................................. 182 \n10.1.3 \nCSP based Model Checking Technique ........................................................................ 182 \n10.2 \nThesis Contributions.................................................................................................................... 183 \n10.3 \nConclusions ................................................................................................................................... 185 \n10.4 \nFuture Directions ......................................................................................................................... 186 \nReferences .............................................................................................................................. 187 \n \n \n \n\n   \nPage \n9 \n \nList of  Figures \nFigure 1: A model as computable function (acquired from [34]) .................................................................. 30 \nFigure 2: Sequence of executions (acquired from [50]) .................................................................................. 31 \nFigure 3: Composed Model  (acquired from [50]) ........................................................................................... 31 \nFigure 4: Generic vs. Specific component design ............................................................................................ 32 \nFigure 5: Black Box, Glass Box, White Box ..................................................................................................... 33 \nFigure 6 Syntactic vs. Semantic Composability (acquired from [38]) ........................................................... 34 \nFigure 7: Ping-Pong DEVS [Wikipedia] ............................................................................................................ 38 \nFigure 8: BOM structure ...................................................................................................................................... 39 \nFigure 9: BOM Assembly ..................................................................................................................................... 41 \nFigure 10: (a) PingPong BOM in BOM Works ................................................................................................ 41 \nFigure 11: Composed BOM ................................................................................................................................ 44 \nFigure 12: Transition firing sequence  (acquired from [68]) .......................................................................... 49 \nFigure 13: Petri Net Analysis Techniques ......................................................................................................... 51 \nFigure 14: Producer Consumer Example .......................................................................................................... 53 \nFigure 15: M0 to M3 throguh firing sequece σ = t2, t1, t2 ............................................................................. 54 \nFigure 16: Seasons in a year (acquired from [68]) ............................................................................................ 54 \nFigure 17: (a) PN Model (b) Reachability Graph (acquired from [68]) ........................................................ 56 \nFigure 18: Producer Consumer PN Model and its Coverability Graph ...................................................... 56 \nFigure 19: A CPN Model ..................................................................................................................................... 59 \nFigure 20: Hierarchical Colored Petri Net ........................................................................................................ 60 \nFigure 21: Modeling Process (acquired from [108]) ........................................................................................ 71 \nFigure 22: Modeling Process (acquired from [29]) .......................................................................................... 72 \nFigure 23: Simulation study life-cycle (acquired from [28]) ........................................................................... 73 \nFigure 24: Verification Techniques .................................................................................................................... 74 \nFigure 25: CBM&S life-cycle ............................................................................................................................... 79 \nFigure 26: Simuland using UML Diagrams ....................................................................................................... 80 \nFigure 27: Implemenation and Simulation ........................................................................................................ 83 \nFigure 28: Discovery, Matching, Composition (DMC) .................................................................................. 84 \nFigure 29: Syntactic Matching ............................................................................................................................. 85 \nFigure 30: Some of the sub-classes of Data Type ontololgy .......................................................................... 88 \nFigure 31: Semantic Matching Technique ......................................................................................................... 88 \nFigure 32: Static-Semantic Matching .................................................................................................................. 90 \nFigure 33: SCXML format ................................................................................................................................... 91 \nFigure 34: State-machine Matching Process ..................................................................................................... 92 \nFigure 35: BOM to PN transformation ............................................................................................................. 93 \nFigure 36: PN Algebraic Technique ................................................................................................................... 96 \nFigure 37: Buffer Extended finite state-machine [120] ................................................................................... 97 \nFigure 38: BOM and E-BOM comparison ....................................................................................................... 99 \nFigure 39: CPN-CM represention of Queue component ............................................................................ 103 \nFigure 40: CPN State-space analysis ............................................................................................................... 106 \nFigure 41: State-space Analysis Technique .................................................................................................... 108 \nFigure 42: CSP based Model Checking Technique ....................................................................................... 115 \nFigure 43: Formulation of Simuland ............................................................................................................... 117 \nFigure 44: Syntactic Matching Process............................................................................................................ 118 \nFigure 45: Static-Semantic Matching Process ................................................................................................ 119 \nFigure 46: State-machine Matching Process .................................................................................................. 120 \nFigure 47: Approach Selection | PN Algebraic Technique ........................................................................ 121 \nFigure 48: PN Algebraic Technique (continued) .......................................................................................... 122 \nFigure 49: Implementation ................................................................................................................................ 122 \nFigure 50: State-Space Analysis Technique .................................................................................................... 123 \nFigure 51: State-Space Analysis Technique (continued) .............................................................................. 124 \nFigure 52: Model Checking ............................................................................................................................... 125 \nFigure 53: Model Checking (continued) ......................................................................................................... 126 \nFigure 54: Manufacturing System (acquired from [124]) ............................................................................. 132 \nFigure 55: Manufacturing System BOM based Composed Model ............................................................ 134 \nFigure 56: State-machine matching of manufacturing system .................................................................... 136 \nFigure 57: PN model of the manufacturing System ..................................................................................... 136 \n\n   \nPage \n10 \n \nFigure 58: Modified manufacturing system composed BOM .................................................................... 139 \nFigure 59: Modified PN model of the manufacturing System ................................................................... 140 \nFigure 60: Activities of Combat Modeling ..................................................................................................... 144 \nFigure 61: Elements of Field Artliiery & Indirect Fire ................................................................................ 145 \nFigure 62: Field Artillery Composed BOM.................................................................................................... 148 \nFigure 63: State-machine Matching of Field Artillery Model...................................................................... 149 \nFigure 64: Observer CPN Component ........................................................................................................... 154 \nFigure 65: Field CPN Component .................................................................................................................. 155 \nFigure 66: BHQ CPN Component ................................................................................................................. 156 \nFigure 67: Battery CPN Component............................................................................................................... 157 \nFigure 68: FDC CPN Component .................................................................................................................. 158 \nFigure 69: Field Artillery CPN Composed Model ........................................................................................ 159 \nFigure 70: State space of Field Artillery CPN Model ................................................................................... 160 \nFigure 71: Reduced State-Space graph of Field Artillery Model ................................................................ 163 \nFigure 72: Field Artillery Composed Model .................................................................................................. 168 \nFigure 73: State-machine Matching of Field Artillery Model...................................................................... 169 \nFigure 74: Global code Block of Field Artillery Model ............................................................................... 171 \nFigure 75: CSP representation of Observer Component ............................................................................ 172 \nFigure 76: CSP representation of BHQ Component ................................................................................... 172 \nFigure 77: CSP representation of Battery Component ................................................................................ 173 \nFigure 78: CSP representation of Field Component .................................................................................... 173 \nFigure 79: Field Artillery Composed Model .................................................................................................. 174 \nFigure 80: Field Artillery Verificataion Assertions........................................................................................ 174 \nFigure 81: Verification Result of assertion 1 .................................................................................................. 175 \nFigure 82: Verification result of assertion 2 ................................................................................................... 175 \nFigure 83: Field Artillery Verificataion Assertions with TOT .................................................................... 175 \nFigure 84: Verification result of assertion 3 ................................................................................................... 176 \n \n \n \n\n   \nPage \n11 \n \nList of  Tables \nTable 1: Entity A .................................................................................................................................................... 43 \nTable 2: Entity B .................................................................................................................................................... 44 \nTable 3: Composed BOM .................................................................................................................................... 44 \nTable 4: Incidence Martic A ................................................................................................................................ 53 \nTable 5: State equation .......................................................................................................................................... 53 \nTable 6: Informal Verification Techniques ....................................................................................................... 75 \nTable 7: Static Analysis Techniques ................................................................................................................... 75 \nTable 8: Dynamic Analysis Techniques ............................................................................................................. 76 \nTable 9: Formal Analysis Techniques ................................................................................................................ 77 \nTable 10: Mandatory constraints in composability verification..................................................................... 81 \nTable 11: Semantic Matching Algorithm ........................................................................................................... 89 \nTable 12: State-machine Matching algorithm ................................................................................................... 91 \nTable 13: Incidence Matrix Calculation ............................................................................................................. 94 \nTable 14: Place-Invariants .................................................................................................................................... 95 \nTable 15: Transformation Rules....................................................................................................................... 102 \nTable 16: Compositional State-space generation algorithm ........................................................................ 109 \nTable 17: Time functions in E-BOM .............................................................................................................. 111 \nTable 18: Probability Distribution Functions ................................................................................................ 111 \nTable 19: E-BOM to CSP# transformation rules......................................................................................... 113 \nTable 20: Some examples of PAT Assertions ............................................................................................... 114 \nTable 21: Formal definition of Machine1 Base-BOM ................................................................................. 133 \nTable 22: Formal definition of Machine2 Base-BOM ................................................................................. 133 \nTable 23: Formal definition of Robot Base-BOM........................................................................................ 134 \nTable 24: Formal definition of Manufacturing System composed BOM ................................................. 134 \nTable 25: Syntactic Matching ............................................................................................................................ 135 \nTable 26: Static-Semantic Matching ................................................................................................................ 135 \nTable 27: Initial Marking and Incidence Matrix (Scenaro I) ....................................................................... 137 \nTable 28: P-Invariants and T-Invariants (Scenaro I) .................................................................................... 137 \nTable 29: B-Fairness Verification .................................................................................................................... 138 \nTable 30: Formal definition of Controller Base-BOM ................................................................................ 139 \nTable 31: Manufacturing System composed BOM....................................................................................... 139 \nTable 32: Initial Marking and Incidence Matrix (Scenaro II)...................................................................... 140 \nTable 33: P-Invariants and T-Invariants (Scenaro II) .................................................................................. 140 \nTable 34: Observer Basic-BOM ....................................................................................................................... 146 \nTable 35: Field Basic-BOM............................................................................................................................... 146 \nTable 36: BHQ Basic-BOM .............................................................................................................................. 147 \nTable 37: FDC Basic-BOM .............................................................................................................................. 147 \nTable 38: Battery (1,2,3) Basic-BOM .............................................................................................................. 147 \nTable 39: Field Artillery Composed BOM ..................................................................................................... 147 \nTable 40: Observer E-BOM ............................................................................................................................. 150 \nTable 41: Field E-BOM ..................................................................................................................................... 151 \nTable 42: BHQ E-BOM .................................................................................................................................... 152 \nTable 43: FDC E-BOM ..................................................................................................................................... 152 \nTable 44: Battery E-BOM ................................................................................................................................. 153 \nTable 45: Reduction Statisitics .......................................................................................................................... 163 \nTable 46: Observer Basic-BOM ....................................................................................................................... 166 \nTable 47: Field Basic-BOM............................................................................................................................... 166 \nTable 48: BHQ Basic-BOM .............................................................................................................................. 167 \nTable 49: Battery (1,2,3) Basic-BOM .............................................................................................................. 167 \nTable 50: Field Artillery Composed BOM ..................................................................................................... 167 \nTable 51: Observer E-BOM ............................................................................................................................. 169 \nTable 52: Field E-BOM ..................................................................................................................................... 170 \nTable 53: BHQ E-BOM .................................................................................................................................... 170 \nTable 54: BHQ E-BOM .................................................................................................................................... 171 \nTable 55: Kinds of properties that can be verified ....................................................................................... 178 \nTable 56: Type of the models that can be verified ....................................................................................... 179 \nTable 57: Scalability ............................................................................................................................................ 179 \n\n   \nPage \n12 \n \nTable 58: Infinite Model Verification .............................................................................................................. 179 \nTable 59: Usability .............................................................................................................................................. 180 \nTable 60: Automation ........................................................................................................................................ 180 \n \n \n\n   \nPage \n13 \n \nList of  Acronyms \nABV \nAssertion-based Verification \nAc \nCommunicating Arcs \nACP \nAlgebra of Communicating Processes \nALSP \nAggregate Level Simulation Protocol \nAOI \nArea-of-Interest \nAP \nAtomic propositions \nAPI \nApplication programming interface \nARC \nAdelaide Refinement Checker \nASV \nState-variable arc \nAT \nTransiting arc \nBB \nBasic BOM \nBDD \nBinary Decision Diagram \nBHQ \nBattalion Headquarters \nBHQSM \nBattalion Headquarters State-machine \nBID \nBattery ID \nBL \nBehavioral Layer \nBOM \nBase Object Model \nCB \nComposed BOM \nCBM&S \nComponent Based Modeling and Simulation \nCBSE \nComponent Based Software Engineering \nCBT \nComposable Behavioral Technologies \nCCA \nCommon Component Architecture \nCCP \nColor set of communicating port \nCCS \nMilner's Calculus of Communicating Systems \nCL \nCommunication Layer \nCM \nConceptual Model \nCODES \nComposable Discrete-Event scalable Simulation \nCOST \nComponent Oriented Simulation Toolkit \nCP \nCommunicating Port \nCPN \nColored Petri Nets \nCPN-CM \nColored Petri Nets Component Model \nCPN-ML \nML scripting language for Colored Petri Nets \nCSP \nHoare's Communicating Sequential Processes \nCSV \nColor set of State variable \nCTL \nComputation Tree Logic \nDEDS \nDiscrete Event Dynamic Systems \nDES \nDiscrete Event Systems \nDEVS \nDiscrete Event System Specification \nDIS \nDistributed Interactive Simulation   \nDMC \nDiscovery, Matching & Composition \nDOT \nDOT file format \nEC \nEvent Controller \nEFSM \nExtended Finite State-machine \nEIC \nDEVS input port couplings \nEOC \nDEVS output port couplings \nEXPR \nExpression \nFA \nField Artillery \nFD \nField Data \nFDC \nFire Direction Center \nFSM \nFinite State-Machine \nHLA \nHigh Level Architecture \nHPC \nHigh Performance Computing \nIC \nDEVS Internal Coupling \nIDE \nIntegrated Development Environment \n\n   \nPage \n14 \n \nINT \nInteger \nISV \nInitialization function of State-variable \nJCSP \nJava based Communicating Sequential Process \nJSIMS \nJoint Simulation System \nJUNG \nJava Universal Network Graph library \nLCIM \nLevels of Conceptual Interoperability \nLTL \nLinear Temporal Logic \nLVC \nLive, virtual, or constructive Simulation \nMBSC \nModel based simulation composition \nMCT \nModel Coupling Toolkit \nMDF \nMatrix Definitional Form \nMDP \nMarkov Decision Processes \nMOCCA \nComponent based Grid Environment \nMPD \nMarkov decision processes \nMUSCLE \nA Multi-scale Coupling Library and Environment \nNET \nNetwork \nOMT \nHigh Level Architecture Object Model Template \nOSA \nOpen Simulation Architecture \nOWL \nWeb Ontology Language \nPAT \nProcess Analysis Toolkit \nPIPE \nPlatform Independent Petri Net Editor \nPLTL \nProbabilistic Linear Temporal Logic \nPN \nPetri Net \nPNML \nPetri Net Markup Language \nPOI \nBOM Pattern Of Interplay \nRS \nRequirement Specifications \nSAT \nBoolean Satisfiability \nSCT \nSemantic Composability Theory \nSCXML \nState Chart extensible markup language \nSE \nSoftware Engineering \nSIMNET \nSimulation Networking \nSISO \nSimulation Interoperability Standards Organization \nSL \nStructural Layer \nSM \nSyntactic Matching \nSML \nScripting language \nSMM \nState-Machine Matching \nSSM \nStatic-Semantic Matching \nSV \nState Variable \nSVIN \nInput State Variable \nSVOUT \nOutput State Variable \nTCSP \nTimed Communicating Sequential Processes \nTENA \nTest and Training Enabling Architecture \nTOT \nTime On Target \nUML \nUnified Modeling Language \nVCP \nCommunication Port Variable \nV&V \nVerification and Validation \nVVA \nVerification, Validation and Accreditation \nVVT \nVerification, Validation and Testing \nXML \nExtensible Marking Language \nXMSF \nExtensible Modeling and Simulation Framework \nXT \nFiring Vector \n \n \n \n \n \n\n   \nPage \n15 \n \nPart I \nEpisteme \n \n \n \n \n \n \nEpistêmê in Greek means “to know”. It is the theoretical knowledge; a principled system of \nunderstanding; fundamental body of ideas and collective presuppositions that determine the \nknowledge which is intellectually certain at any particular period of time; Pure-Science; episteme deals \nwith “what” and “why” of the subject. \n \n \nPart-I covers the epistemology of the research under discussion where the theory, \nconcepts, principles, paradigms, philosophy and rationale of the problem domain and \nthe solution domain are sketched. In essence Part-I contains theoretical knowledge \nand the background information required to understand the problem and proposed \nsolution discussed in the second part.  \n \n \n \n \n \n“If you can't explain it simply, you don't understand it well \nenough”.   \n- Albert Einstein \n \n \n\n   \nPage \n16 \n \nChapter 1 \nIntroduction \n \nThis chapter provides the opening statement and general information about the research presented in \nthis thesis. It outlines background, history, the formal definition and the basic philosophy of the \nproblem under question and covers the motivation, goals and scope of the research and the \ncontributions of the thesis. In the end, a section on the thesis organization is rendered. \n \n1.1 \nBackground and the opening perspective  \nOver the last fifty years, there has been a revolutionary development influencing \nalmost all of the sciences. This progress is mainly instigated by the astonishing \ngrowth of the use of the digital computers and the subsequent rise of the age of \ncomputer simulations [1]. It is the emergence and widespread availability of \ncomputing power and resources that have made possible the new dimension of \nexperimentation with complex models and their simulations [2]. Computer \nsimulations are now widely used in various scientific disciplines and application \ndomains. They are used for studying complex systems and gaining insight into the \noperation of an existing system without disturbing the actual system. Furthermore \nthey are used for testing new concepts of the systems before implementation, \nvisualizing and predicting behavior of a future system. Besides, they are used for \nanalyzing and solving problems, drawing conclusions and aiding the process of \ncrucial decision making [3]. Therefore computer simulation is regarded as third \nbranch of science [4] and stands alongside of the first two branches namely theory and \nexperimentation. \nModeling and Simulation (M&S) is a discipline with its own body of knowledge, \ntheory, and research methodology [4]. The goals of M&S are aligned with the \nsystems theory, and include modeling & analysis, design & synthesis, control, \nperformance evaluation and optimization of a real system. The M&S community has \ndemonstrated a longstanding focus on providing support for these goals. With the \nadvent of the net-centric era of methods and technologies in designing complex \nsimulation systems, the focus of M&S industry has been driven by the most \nrecognized potential benefits of reduced development cost, time  and system \ncomplexity [5]. This is because M&S development process is costly, time consuming, \nresource intensive. Models can be large, complex and require a great deal of time, \nresource and domain specific expertise to develop. Beside this, an enormous effort is \nrequired to evaluate that the model is correct and meets its requirements. Therefore \nM&S community has taken a deep interest in the quality design principles and their \nunderlying supportive theories to alleviate these challenges. It has been realized that \nconstructing a model from scratch each time it is needed is inefficient. Instead, the \npractice of model reuse has been increasingly appreciated and is inspired from the \nvision of software reuse, which was originally introduced in 1968 [6]. Apparently this \napproach looks very appealing however it poses many obstacles in implementing, \nsuch as lack of flexibility and adaptability in design, difficulty of integration, \nmismatched interface, incomplete specification etc. [7]. These obstacles are \n\nChapter 1  \n \nIntroduction \n   \nPage \n17 \n \nconsidered elusive research challenges and are now the primary research interests of \nthe software engineering and M&S communities [8]  \n1.1.1 Component based Software Engineering \nComponent-based software engineering (CBSE) has been identified as a key enabler \nin the construction of complex systems by combining software components that are \nalready developed and prepared for integration [8].  \nSoftware Component \nA software component is defined as a unit of composition which is independently developed \nand can be combined with other components to build larger units. It must have clearly \nspecified interfaces to communicate with its environment while the implementation must be \nencapsulated in the component and is not directly reachable from the environment [9], and \ntherefore can be easily used by the third party without having to know implementation details \n[8], [10].  \n \nBuilding software from components contributes to a major paradigm shift in \nsoftware engineering. The basic philosophy behind the idea of component-based \ndevelopment is to carry out the software development process by (quickly) \nproducing software applications through assembling prefabricated software \ncomponents and to archive these interoperable software components in form of an \nincreasingly large repository for further reuse [11]. CBSE promotes the principle of \nmodularity. That essentially helps to master the complexity of the reality by \ndecomposing it into parts [12] and enables the designer to use and reuse appropriate \nparts for different purposes. These parts are the sub-systems built in a component-\nbased fashion. These subsystem components may have been separately developed by \ndifferent teams. They may also have been developed for different purposes unrelated \nto the current context of the usage. CBSE has many advantages, such as effective \nmanagement of complexity, logical separation, reduced time and cost, increased \nproductivity, improved quality, a greater degree of consistency, increased \ndependability, and a wider range of usability. In addition, the growing connectivity of \nreal world problems is reflected in the requirement to compose cross domain \nsolutions [13], and therefore support knowledge sharing to a wider user community. \nCBSE is therefore a discipline of software engineering that deals with the \ncomposition of components to construct software systems which are capable of \nperforming functions according to the user’s requirements [14].  \nIn CBSE, component integration and component composition are two distinguished \nterms. Component integration is merely the task of connecting components together \nwhereas composition also includes reasoning about the semantic behavior of the \nresulting assembly [14]. With the advent of component technology the integration \nproblems are becoming a difficulty of the past. Instead more crucial problems of \npredicting the emergent behavior of assemblies and the problem of reasoning about \nhow well components will play together are now in debate. Component composition \nsupports this type of reasoning and provides a foundation for fundamental reasoning \nto justifying validity of the resulting assemblies, their run-time compatibility and \nemergent behavior. The main reason for the difference between integration and \ncomposition is due to the fact that component interfaces do not provide enough \ninformation to determine how well the composed components will play together \n[14]. An interface can only help to determine if the component can be connected to \n\nChapter 1  \n \nIntroduction \n   \nPage \n18 \n \nsome other component but cannot supports reasoning about emergent properties of \nthe assemblies [14], [15]. Component composition promises such rationale; however \nis still a subject of open research. \n1.1.2 Component based Modeling & Simulation \nInspired by the discipline of component based software engineering, M&S \ncommunity has also started to develop simulation models by reusing previously \ndeveloped and validated “simulation components”, and composing them in a new \nsimulation model according to the desired user objectives [16], [17], [18], [19], [20]. \nThe basic and effective strategy for tackling any large and complex simulation \nproblem is “divide and conquer.” One major idea in component-based simulation \ndevelopment is to create models that are themselves self-contained and \nindependently deployable. Thus different simulationist will be able to work on \ndifferent components independently, without needing much communication among \neach other, (and particularly without the need to share the classified domain \nknowledge) and yet the components will work together seamlessly. In addition, \nduring the maintenance phase, it is possible to modify some of the components \nwithout affecting all of the others [21].  \nIn simulation community the research on component based development falls under \nthe rubric of composability [22], where simulation models are considered to be the \nbuilding blocks and are referred as “model-components”.  \nModel Component1 \nA model component is an independent element of a simulation model that conforms to certain \ncomponent standard, has well-defined functionalities (inputs/outputs) and behaviors, \npresented through its interface describing its communication with other components and a \nformalized description of its internal behavior. A model component is not a stand-alone \ncomponent, but can be independently deployed, and it is subject to third-party composition \nwith or without modification [19]. \n \nIn component based development, some basic reusable model components are \ncomposed together to create complex and sophisticated simulations, without \nbuilding them from scratch. The model components can be composed if their inputs \nand outputs physically match each other however it is difficult to say whether this \ncombination is meaningful. Besides it cannot be said for sure if it will perform \naccording to the desired requirements unless the correctness of the composability is \nchecked.  \nComposability is the property of the models, as it essentially contends with the \nalignment of issues on the modeling level [13], where it is viewed as creation of \ncomplex models by selection and integration of basic reusable model-components. A \nset of components can be integrated if their inputs and outputs are compatible, but \nin order to guarantee that their combination is valid in the required executable \nscenarios, we study the degree of composability.   \nWith a slightly greater number of components, which are somewhat complex in \nnature, the composability becomes an increasingly challenging problem. In the \n                                                 \n1The term Model component should be differentiated from the term Component Model, which in \ntext refers to the underlying technology being used by the component based software engineering \nplatforms such as CORBA, EJB etc. \n\nChapter 1  \n \nIntroduction \n   \nPage \n19 \n \npresence of functional and non-functional application requirements it poses severe \nimplications on the effort involved in verifying the requirements, and increasing \ndynamism. Even though, the individual components are pre-verified; their \nverification is usually done in a limited context, with assumptions that may not hold \nafter composition. As a result, the complexity of system verification grows \nexponentially with the number of applications [23]2.  \n1.1.3 Modeling and Analysis using Petri Nets \nPetri nets (PN) is a mechanism of modeling complex systems, in which states and \nevents can be manipulated according to certain rules and explicit conditions. PN \nformalism was introduced by Carl Adam Petri in 1962. It provides an elegant and \nuseful graphical and mathematical formalism for modeling concurrent systems and \ntheir behaviors [24].  \nPN graphs are quite suitable for representing Discrete Event Systems (DES) in \nwhich operations depend on potentially complex control schemes. PN graphs are \nintuitive and capture a lot of structural and behavioral information about the system. \nAnother motivation for considering PN for the modeling of DES is the body of \nanalysis techniques that have been developed for over three decades and are used for \nreasoning about structural and behavioral properties of PN models. These \ntechniques include reachability analysis, state-space analysis, and model-checking as \nwell as linear-algebraic techniques [25].  \nThe PN research has been developed in two directions for the past three decades: (i) \nPN theory that focused on the development of basic tools, techniques and concepts \nneeded for the PN application; (ii) Applied PN theory which is mainly concerned \nwith the PN application for the modeling of systems and their analysis. Successful \nwork in this direction requires good knowledge of the application area in which PN \nare applied and PN theories and techniques [26].  \n1.1.4 Modeling and Analysis using Process Algebra \n \nProcess Algebra is an algebraic approach for the modeling and analytical study of \nconcurrent processes. It has a diverse family of algebraic formalisms for modeling \nconcurrent systems. These formalisms comprise of algebraic language for the \nspecification of processes and provide calculi in form of algebraic laws that allow \nprocess descriptions to be manipulated and analyzed, and permit formal reasoning \nabout their correctness and equivalence [27]. The main Process algebraic formalisms \nare: \n CCS, Milner's Calculus of Communicating Systems  \n CSP, Hoare's Communicating Sequential Processes \n ACP, Algebra of Communicating Processes \n LOTOS, Language Of Temporal Ordering Specification \n1.1.5 Model Verification \nIn M&S, verification is concerned with building the model right. It is typically \ndefined as a process of determining whether the model has been implemented \ncorrectly [28] and whether it is consistent with its specifications [29]. In principle, \n                                                 \n2 Even though the referred text corresponds to the electronic components which are physically \ncomposable, however the problem of composability complexity is the same and is mutually \nunderstood by different communities. \n\nChapter 1  \n \nIntroduction \n   \nPage \n20 \n \nverification is concerned with the accuracy of transforming the model’s requirements \ninto a conceptual model and the conceptual model into an executable model [29]. \nThe distinction of a conceptual model and executable model is of great importance \nand is a fundamental principle in M&S. A conceptual model is abstract description of \na real system [30], captured based on given requirements and modeling objectives. \nThis is later refined and implemented into a more concrete executable model. In \nthese terms, conceptual modeling is a subset of model design [31]. Conceptual \nmodeling is about moving from a problem situation, through model requirements to \na definition of what is going to be modeled, and is independent of its implementation \ndetails [30], which are later addressed in form of an executable model.  \n1.2 \nSummary of the opening perspective \nIn essence, component-based approach is highly favored in M&S community for \nbuilding large and complex models. But to ensure that the model is correct and \nmeets its requirement specifications, a substantial effort is required to evaluate its \ndegree of composability. In M&S community, the discipline of Model Verification \nprovides basic concepts and fundamental principles for the compressive study of the \ndegree of composability and reasoning its correctness with respect to the given \nspecifications. However the existing component-based simulation frameworks offer \nlimited built-in extensive verification techniques or none at all. Therefore third party \napproaches such as PN analysis techniques and process algebra are considered for \nthe thorough examination of composed models at various levels of depth.  \nThe sub-topics: (i) Component-Based Modeling & Simulation, (ii) PN /CSP Analysis \nand (iii) Model-Verification are the elementary pillars and theoretical foundations of \nthis thesis and are expanded in details in chapter 2, 3 & 4 respectively. \n1.3 \nPreliminaries \nBased on the previous discussion, the formal definition of the problem of this thesis \nis furnished in this section. In order to define the problem statement, following \ndefinitions are used:  \n1.3.1 Definition 1: Set of Components \nLet C = {c1, c2, c3 …, cn} be a given set of components discovered and selected from \na component repository R, as per the abstraction of the real-system. \n1.3.2 Definition 2: Requirement Specification \nThe Requirement specification of the system model is defined as a tuple:   \nRS = 〈O, S〉 where: \nO = {o1, o2, o3 …, on} is a set of objectives (or goals) and \nS = {s1, s2, s3 …, sn} is a set of system constraints (or system properties). \nObjective: \nAn objective oi ∈ O can be defined as a reachable “final-state” of the \ncomposed model or an aggregated desirable output (a data value or event) \nproduced by the composed model which cannot be produced by individual \ncomponents. \n\nChapter 1  \n \nIntroduction \n   \nPage \n21 \n \nSystem Constraint:  \nIn modeling terms, a system constraint si ∈ S is defined as a system property \nthat must be satisfied; for instance a good state; which must be reached or a \nbad state; which must be avoided (never be reached) during the execution.  \nThe notions of constraints are different from Objectives, because they can be \nnecessary requirements but not the ultimate goals. E.g., a manufacturing system \nshould not only produce the desired products (objective) but also fulfill safety \nrequirements (constraints).  \n1.3.3 Definition 3: Composition & Composability Pattern  \nLet CM〈c1, c2, c3 …, cn〉 be a composition of a set of given components C, composed \nusing a particular composability pattern P. A pattern describes how the components \nare attached to each other, i.e., the topology of the components. And provide \nimportant information for composability verification. A pattern of composability can \nbe sequential, parallel, fork, join, iterative, or composite.  \n1.3.4 Definition 4: Satisfiability Operator \nFor each element in the requirement specification RS, a Satisfiability operator╞ maps \na given composed model CM to a Boolean (True or False) formally described as \nfollows: \n• CM〈c1, c2, c3 …, cn〉    ╞ i  oi∈O → true | false \n• CM〈c1, c2, c3 …, cn〉    ╞ j  sj∈S → true | false \nFor each relation ╞ i we define a verification function (algorithm or theorem) based \non which the satisfiability operator maps the resultant value. This verification \nfunction determines whether a given composed model satisfies a required property.  \n1.4 \nProblem Statement \nBased on the above definitions the problem statement is defined as follows: \n“Given a composed model CM, composed from a set of components C using \na pattern P, and the requirement specification RS, can we verify that CM fulfills \nall the objectives and satisfy all the constraints given in the requirement \nspecification”. \n \nFormally: \n \nThis problem statement is considered as an initial point and basis of the research \npresented in this thesis proposal. In this work it will be shown how a modeler can \ncorrectly compose component models and verify the composition at different levels \nthrough utilization of our proposed verification framework.  \nCM=Compose ([c1, c2, c3 …, cn], P) ∧ RS=〈O, S〉  → {CM ╞𝒊 ∀oi ∈ O ∧  CM ╞𝒋 ∀sj ∈ S} \n(1.1) \n\nChapter 1  \n \nIntroduction \n   \nPage \n22 \n \n1.5 \nApproach \nIn this section an overview of the approach and methodology is presented. Based on \nthe software engineering principle, this section is divided into two main parts (i) \nProblem Domain and (ii) Solution Domain.  \n1.5.1 Problem Domain \nProblem domain (or problem space) is an engineering term referring to all \ninformation that defines the problem and its constraints. It includes the goals \nthat the problem-owner 3 wishes to achieve, the context within which the \nproblem exists, and all rules that define required essential functions or other \naspects of any solution product. It represents the environment in which a \nsolution will have to operate [Wikipedia]. \n \nAll the information provided in this thesis related to modeling & Simulation, \ncomponent-based model development, conceptual modeling, model components, \ncomposability, model-verification and the problem of composability correctness \ncorrespond to the problem-domain. In particular, Chapter 2 covers the main aspect \nof the problem domain where the component based modeling and simulation is \ndiscussed in detail. Following sub-sections briefly describe the selected method of \nspecification of the problem domain. \nBase Object Model (BOM) as Composability Framework \nBOM is selected in this thesis as a component specification standard which can be \nused as a foundation for developing model components at conceptual level. They are \ncomposed and are subjected to the composability verification process to evaluate \nthat they satisfy given requirements, hence represent component framework of the \nproblem domain. \nRequirement Specification Template \nA “Requirement Specification Template” is defined which is used to formulate \nrequirement specifications. It essentially contains a set of objectives and constraints \n(of standard or scenario-specific properties), which are required to be satisfied for the \nproof of correctness of the composed model. \n1.5.2 Solution Domain \nSolution domain (or solution space) is a term referring to all information that \ndefines the proposed solution of the problem. It includes the concepts, \nprinciples, methods, techniques, algorithms, programs, software architects, \nframeworks, processes and recommended practices, which help in solving the \nproblem under study. \n \nFollowing sub-section gives a brief overview of the approach used in this thesis: \n                                                 \n3  A problem owner can be the customer, solution buyer, organization or a prospective target \ncommunity. A problem owner sees the problem as an opportunity, whereas the solution engineer sees \nthe problem for which he/she has to provide a solution. \n\nChapter 1  \n \nIntroduction \n   \nPage \n23 \n \nMulti-tier Composability Verification \nThe composed model undergoes multiple iterations for composability verification at \ndifferent levels. Each level corresponds to a tier in the verification process. When the \ncomposability at a particular level is successfully verified, next level is iterated. When \nall the levels are completed, the components are said to be fully composable. These \nlevels are discussed in detail in chapter 2. The verification of these levels is discussed \nin chapter 5.  \nPN Formalism \nPN formalism (and specially the Colored Petri Nets extension) is chosen for creating \nexecutable models of the BOM based conceptual models. The proposed framework \nautomatically transforms BOM components in form of an executable PNML 4 or \nCPN-based component which can be executed or undergo a verification process \nusing the corresponding PN execution environment.  \nCSP Formalism \nCSP5 formalism with an extension of Timed-CSP is picked as another executable \nmodeling language for BOM based conceptual models. The proposed framework \ntransforms BOM components into executable CSP process components and \ncomposed for execution and verification. \nAutomatic Transformation Tools \n \nAn automatic transformation tool is proposed, which transforms a BOM component \nmodel into the selected executable modeling formalism such as PNML, CPN based \nor CSP based executable model. It may also be required to provide additional details, \nwhich cannot be modeled or represented by BOM. \nDynamic Analysis Approach \nThree main dynamic analysis approaches are selected for composability verification \nof BOM base composed models at dynamic-semantic composability level: \nAlgebraic analysis approach \nThis approach is used to transform a BOM composition into a classical PN model \nusing PNML format and verifies the properties using PN algebra. \nState-space analysis approach \nThis approach is based on using Colored Petri Nets and State-Space analysis. CPN \ntool is a strong simulation and verification tool. State-space analysis is a very accurate \ncorrectness reasoning technique; however it is costly in terms of computational \npower and memory. Therefore a reduction technique is also proposed to reduce a \nstate-space graph of a composed model, in order to avoid state-space explosion. \nModel Checking approach \nCSP based model checking is used for the formal verification of BOM based \ncomposed model. In formal logic, model checking designates the problem of \ndetermining whether a formula or a correctness property ϕ defined using LTL, CTL6 \nor similar property specification formalism, evaluates to true or false in an \n                                                 \n4 Petri Net Markup Language \n5 Communicating Sequential Process \n6 Linear Temporal Logic, Computational Tree Logic \n\nChapter 1  \n \nIntroduction \n   \nPage \n24 \n \ninterpretation of a system K, written as K ╞ ϕ. Efficient algorithms are selected to \ndetermine whether K ╞ ϕ holds [32].  \nIn summary, these three approaches are extensively being used in formal verification \nfor over a couple of decades and therefore equipped with rich theoretical \nfoundations and practical tools and techniques.  We however believe that they are \nbeing considered in this thesis for the composability verification of BOM based \nmodels (or for that matter any M&S composition framework) for the first time and \nwill prove to be very promising and effective. A basic foundation is built using these \napproaches in this thesis, and their usage are shown though appropriate examples. \nAlso necessary guidelines are provided for developing new verification methods \nusing these approaches and tools, in order to address various verification issues.  \nThis research aims to propose a multi-tier verification life cycle for defining, \ndevelopment, \narchiving, \ndiscovering, \nmatching, \nselection \nand \ncomposing, \ntransforming, executing, verifying and finally reasoning about the correctness of the \ncomposed models. This life-cycle extensively relies on the integrated component \ndevelopment, composition and verification framework that is being proposed in this \nresearch. This life-cycle follows our proposed process to perform verification of a \ncomposed model at different levels. This life-cycle can be adapted by M&S \npractitioners for rapid model construction, analysis, refinements and reuse and thus it \nwill boost the process of modeling and simulation of complex dynamic systems. \n \n1.5.3 Solution Statement \nBased on the proposed approach the solution statement is described as follows: \n“A verified composed model guarantees that the selected components are \ncomposable at all composability levels, and they meet the requirement \nspecification by satisfying given objectives and fulfilling the required \nconstraints”.  \nA correctly composed model, promotes reuse of base components thus support \nrapid model development and can be reused as yet another component later on.  \n1.6 \nScope of the Thesis \nIn this section, the scope and the boundaries of the thesis are outlined.  \n1.6.1 Correctness \nIn this thesis, “Correctness” is the main focus of the research. The approach, \nmethods, process and framework mainly deal with the correctness issue of the \ncomposability verification. The other issues such as performance, efficiency and cost \nestimation of the solution are currently beyond the scope of this thesis and \nconsidered as future work.  \n1.6.2 Validation \nValidation is a vital part of model evaluation and always goes hand in hand with \nverification. However it is beyond the scope of this thesis. Although we believe that, \nour framework is flexible and open-ended. Therefore it can accommodate necessary \nextensions to support validation with a minor effort. \n\nChapter 1  \n \nIntroduction \n   \nPage \n25 \n \n1.6.3 Emergence \nEmergent behavior due to composition of sub-systems is an important and open \nresearch topic in the composability domain. We however do not address this issue in \nthis thesis and consider it a future work. \n1.6.4 Generalization \nCurrently, the proposed approach is based on Base Object Model, only as a \ndemonstration of how our approach can be applied on an existing component \nstandard. However the framework presented in this thesis is open-ended and can be \ngeneralized to accommodate any other component standard. Furthermore, \nheterogenic composability can also be supported. We however do not address \ngeneralization issues in this thesis. \n1.7 \nSummary of the Contributions \nThe existing work in the area of component based modeling and simulation is \nfragmentary in nature, especially when the verification of component composability \nof model at a conceptual level is concerned. Furthermore, even though different \ncomposability verification approaches exist, but they have not been studied in depth \nat different granular levels.  \nIn this research, composability of BOM based model is studied in depth, focusing \nmainly on the different levels. A multi-tier component based verification life-cycle is \nproposed that tackles key issues of such as model development, discovery, selection, \nmatching, composition, requirement specification, transformation, implementation, \nexecution, analysis and most importantly verification.  \nIn terms of verification, the major contributions of this thesis include development \nof a composability verification framework, which integrates different methods, and \ntechniques to support different tasks in the composability verification process of a \ncomposed model. These different tasks are categorized in different phase of a \nproposed component based modeling and simulation (CBM&S) life-cycle. We \npropose methods for evaluating structural and behavioral consistency of the \ncomposed BOMs. For structural evaluation we propose a set of static analysis \ntechniques to verify that the components can be correctly connected and their \ncommunication is semantically consistent, meaningful and is understood as intended.  \nFor behavioral consistency of the composition we suggest a state-machine matching \ntechnique. It verifies that the components can correctly interact with each other in a \nright causal order to reach final states. For the further evaluation of the behavioral \ncomposability our framework incorporates three main approaches: (a) PN Algebraic \ntechnique (b) CPN-based State-space analysis technique and (c) CSP based model \nchecking. For each approach we develop automatic transformation tool that \ntransforms a BOM based composed model into the executable model of the \ncorresponding approach. We present three different case studies for the proof of \nconcept and for the evaluation of our verification framework.  \nWe also suggest various extensions in each approach to suit the needs of \ncomposability verification. For instance we propose algorithms for automation of the \nPN algebraic approach. Also a CPN based component model is proposed for the \nState-space algebraic approach in order to describe a BOM component (or any other \nsimulation component) in form of an executable model that can be executed using \n\nChapter 1  \n \nIntroduction \n   \nPage \n26 \n \nCPN execution environment. We also introduce a State-space reduction technique \nfor the CPN based state-space analysis approach to avoid the risk of state-space \nexplosion. For the CSP based model checking approach we propose an external \nfunction library for methods to support various modeling tasks such as definition of \nprobability distribution functions for probabilistic system models.  \n \n1.8 \nStructure of the Thesis  \nThis thesis is divided into two main parts: \n \nPart I Episteme: This part mainly covers the theoretical concepts, principles and \ndiscussions. It comprises of chapters 1, 2, 3 & 4. \n \nChapter 1: Introduction:  \nChapter 1 gives a bird’s eye view of the research presented in this thesis. It addresses \nthe concept, historical background and the basic philosophy of composability. The \nproblem is defined and the approach is briefly introduced. A section on the scope of \nthe thesis and main contributions are also presented.  \n \nChapter 2: Component Based Modeling and Simulation: \nChapter 2 introduces and discusses component based modeling and simulation in \ndetails, as it is the foundation of the problem domain. This chapter mainly covers the \ntheory, issues, different levels, framework and the formalism of model composition. \nIt also introduces Base Object Model (BOM) in details as a choice of Model \ncomposition standard of this thesis.  \nChapter 3: Executable Modeling Formalism \nThis chapter provides introduction, theory, basic definition and classification of PN \nand CSP as executable modeling formalisms and regarded as solution domain. It also \ndescribes basic concepts of the analysis techniques that are used later in this thesis. \nChapter 4: Verification and Analysis \nChapter 4 discusses theory and principles of verification. It also categorizes some of \nthe important verification techniques that are used in this thesis. \n \n \n \nPart II Techne: This part contains practical aspects including approaches, methods, \ntools, development frameworks and lifecycle. It also contains examples related to our \nproposed solutions for the proof of concept. It comprises of chapters 5, 6, 7, 8 & 9. \n \nChapter 5: Proposed Approach and Framework  \nChapter 5 is the center of the thesis as it provides the most important details of our \ncontributions. It describes the proposed verification framework and verification life-\ncycle. It covers our proposed methods, techniques, algorithms, procedures as our \n\nChapter 1  \n \nIntroduction \n   \nPage \n27 \n \ncontributions at different phases of composability verification process. These phases \nand their concerning activities are outlined as composability verification life-cycle.  \n \nChapter 6: Composability Verification Process \nThis chapter presents the proposed composability verification process. It provides \nessential guidelines of how to use our proposed composability verification \nframework (discussed in chapter 5). It uses work flow diagrams to describe the \noverall process and gives necessary guidelines to the modeler at each step. \n \nChapter 7: Fairness verification using PN Algebraic Technique  \nChapter 7 describes a case study of a manufacturing system as an example to explain \nhow the proposed framework helps to verify fairness property in a composed \nsystem. The purpose of this chapter is to practically demonstrate algebraic \nverification method. \n \nChapter 8: Model verification using State-space analysis technique \nChapter 8 covers an example of the verification of a Field Artillery Model. It \npractically demonstrates how state-space analysis is used to verify a composed \nsystem. The field artillery model is introduced in detail along with requirement \nspecifications and it is shown how the proposed approach can help to verify its \ncomposability.  \n \nChapter 9: Model Checking  \nThis chapter demonstrates an example of verification using CSP based Model \nChecking. The field artillery model discussed in chapter 8 is modified into a real-time \nprobabilistic system and is verified using CSP based model checking.  \n \n \n \nChapter 10: Conclusion and Future work \nThis chapter provides summary and conclusion, discussion and future work of the \nthesis. \n \n\n   \nPage \n28 \n \nChapter 2 \nComponent Based Modeling and \nSimulation  \n \nComposability is an important quality characteristic and an effective means to achieve several benefits \nin M&S discipline, but in reality, it is a challenging and daunting problem. The community has \nconducted active research on its theoretical and practical intricacies. In theory, composability is \nstudied under various facets and views primarily distinguished, by its different “layers” or “levels” as \nidentified by different research groups. Whereas in practice, various practical challenges associated \nwith it are investigated. Most important of these issues are component specification, development, \nintegration, composability verification and validation, collectively referred to as phases of a \nComponent based life-cycle. In this chapter both theoretical and practical aspects of composability are \ndiscussed in detail. \n \n2.1 \nComposability in M&S \nIn M&S applications, composability has been defined in different ways. Much of \nthese definitions have been collected by A. Tolk in his article [13]. Harkrider and \nLunceford defined composability as:   \nThe ability to create, configure, initialize, test, and validate an exercise by logically assembling \na unique simulation execution from a pool of reusable system components in order to meet a \nspecific set of objectives [33]. \nKasputis and Ng defined composability as:  \nThe ability to compose models across a variety of application domains, levels of resolution, and \ntime scales [16] \nPetty and Weisel recommended the following definition in their article on theory \nof composability, which later was appended by P. K. Davis: \nComposability is the capability to select and assemble simulation components in various \ncombinations into valid simulation systems to satisfy specific user requirements, meaningfully \n[17] [34]. \n \n \nIt has been realized that composing models is more difficult than composing general \nsoftware components. This argument is predicated on the assumptions that models \nare more complex; they are developed for particular purposes, and they depend on \ncontext-sensitive assumptions [8] [17]. Model development is a hard design task, \nmainly due to the complexity involved in the process. Nowadays this complexity is \nincreasing to levels in which the utilization of pre-defined models is considered very \nuseful to cut short the development time. Thus model composition is a paradigm, \nwhere existing components are the building blocks for the construction of new larger \nand more sophisticated models. When a model is composed, it must be evaluated in \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n29 \n \nterms of correctness with respect to its requirements. In short the predictability of \nguaranteeing the correctness of model composition is called Composability.  \n2.2 A Brief History of Composability and related work \n2.2.1 Initiation  \nComposability in M&S has primarily been investigated by the defense research \nsector. The earliest uses of the term composability within the context of defense \nsimulation dates back to the Composable Behavioral Technologies (CBT) project \nduring the mid-1990s [35]. Later on the Joint Simulation System (JSIMS) project \ninvestigated composability as a system objective [36]. In 1998, a project on model \nbased simulation composition (MBSC) was started in which a prototype composition \nenvironment for JSIMS was developed. In 1999 Page and Opper investigated the \ncomposability problem from a computability and complexity theoretic perspective \n[35]. Composability became a key system objective for OneSAF project in 1999 [22].  \n2.2.2 Theoretical evolution \nLater on a series of numerous articles were published which addressed various issues \nof and methodologies of composability and became the theoretical foundations for \nfurther research. Important works in this series include: Kasputis and Ng [16]; Davis \net al. [37]; Petty & Weisel [38]. Petty and Weisel extended the work of Page and \nOpper, provided a broad survey of the uses of the term composability, and examined \nthe composite validation problem within the context of automata theory and \ncomputable functions. Later a comprehensive report was published by Davis and \nAnderson in 2003 [17] that provides a broad survey of the composability and \nsuggests its applications for the DoD7 in this area.  \n2.2.3 Standards & Frameworks \nLater on, the research on composability remained focused on the development of \nstandard composition frameworks and its practical application in various domains of \nmodeling and Simulation. In 2005 the Extensible Modeling and Simulation \nFramework (XMSF) was initiated by the Naval Postgraduate School to develop a \nweb-based simulation environment [39]. Advances in M&S technologies, gave rise to \ndifferent distributed simulation standards and protocols such as Simulation \nNetworking (SIMNET), Distributed Interactive Simulation (DIS), Aggregate Level \nSimulation Protocol (ALSP) and the High Level Architecture (HLA). The details of \nthese standards are well documented by Moradi [19]. Due to the complex nature of \nthe standards, and distributed simulation itself, different composability frameworks \nwere introduced to co-op with these requirements. More general-purpose \nframeworks such as the Discrete Event System Specification (DEVS) [40], the Open \nSimulation Architecture (OSA) [41], the Base Object Model (BOM) [42], and the \nComponent Oriented Simulation Toolkit (COST) emerged and contributed to \nvarious issues of composability in different ways.  \n2.2.4 Technological Advances \nDue to the technological advances in computer engineering, many approaches \nemerged with the aim to address issues and high end requirements of modeling and \n                                                 \n7 United State Department of Defense \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n30 \n \nsimulation such as representation of Complex, Dynamic and Adaptive Systems; \nintegration of large interdependent Systems; multi-resolution and multi-scale \nmodeling [43], and much more. In this period, many tools and techniques were \ndeveloped using composability paradigm. Model Coupling Toolkit (MCT) was \ndeveloped to support and simplify the construction of parallel coupled models [44]. \nMUSE is another composable simulation environment for astrophysical applications \nin which different simulation models of star systems are incorporated into a single \nframework [45]. Some frameworks such as Common Component Architecture \n(CCA) [46] and Component based Grid Environment (MOCCA) [47], were \nproposed to be used in high-performance computing, where scientific components \nare directly connected by their Users and Providers ports. A Multi-scale Coupling \nLibrary and Environment (MUSCLE) provided a software framework for building \ncomposable simulations according to the complex automata theory [48]. Compo-\nHLA is an environment proposed for supporting HLA component [49]. \n2.2.5 Composability verification and Validation \nMost of these frameworks lack strong built-in composability evaluation support. \nTherefore some third-party composition, verification and validation frameworks \nwere developed by individual research teams such as Composable Discrete-Event \nscalable Simulation (CODES) [20] and Semantic Web-based BOM composition \nframework [19], where verification and validation of composability are strongly \nfocused.  \n2.3 Theory of Composability \nThe formal theory of composability was pioneered by Petty and Weisel [34], [38], \n[50] in an initiative developed at the Virginia Modeling, Analysis & Simulation Center \n(VMASC).  It was also called “semantic composability theory” (SCT). The aim of the \nSCT is to check and prove the semantic composability of components using formal \ndescriptions and reasoning. A model is defined as a computable function: y = ƒ(x), \nwhere function is calculable by a finite procedure and relates each input to a unique \noutput, as shown in Figure 1 \n \n \nFigure 1: A model as computable function (acquired from [34]) \n \nA simulation is a sequence of executions of a model ƒ(x), where the output from \neach execution step is the input to the next step of the execution: \n \nWhere i = input value; m=memory value; o=output value and n=current iteration, as \nshown in Figure 2 \n(mn, on) = ƒ(mn-1, in-1) \n(2.1) \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n31 \n \n \nFigure 2: Sequence of executions (acquired from [50]) \n \nThe composition is defined as output of one function to be the input of another: \n \nFigure 3 shows the representation of a composed model, which is developed through \ncomposing other models (f1, f2 & f3). A composed model as a whole has also a set \nof inputs, outputs, current states and next-states as shown in Figure 3. \n \nFigure 3: Composed Model  (acquired from [50]) \nThe composition of models in SCT is in fact the composition of functions. Since a \nset of computable functions is closed under composition any set of models can be \ncomposed if the composition exists, but there is no guarantee that the resultant will \nbe a useful model. Thus focus of SCT is semantic composability, the question of \nwhether the model composition is meaningfully valid or not.  \nValidity \nA model is defined as valid, if it is an accurate representation of the real-world with respect to \nthe intended use. For formal validation, the simulation of a composition is represented as \nLabeled Transition System where nodes are model states, edges are function executions, and \nlabels are model inputs. A composition is valid if and only if its simulation is close to the \nsimulation of a perfect model. \nPerfect Model \nA model is perfect with respect to a natural system N 8 if and only if it represents a system of \nperfect observations of the natural system [50]. \n                                                 \n8 A natural system N is a real or imagined system. \nh(x) = ƒ(g(x)) \n(2.2) \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n32 \n \n \nFor details of different classes of models, their equivalence relations, formal \ntheorems and proofs of equivalence, interested readers should refer to [50]. The basic \nconcepts of a formal theory of semantic composability include formal definitions for \nmodel, simulation, validity, and composition. A theory of composability can facilitate \nthe convenient reuse of simulation components, which holds the potential to the \ntime and cost of simulation development [34] [38] [50]. \n2.4 Concepts related to Composability \nIn this sub-section, some of the concepts and idea related to composability are \ndiscussed.  \n2.4.1 Composability vs. Reusability \nComposability is differentiated from reusability in many aspects. Balci et al. define \nReusability as the degree to which an artifact, method, or strategy is capable of being \nused again or repeatedly [5]. Robinson et al. on the other hand suggest that the term \nsimulation model reuse can be taken to mean various things from the reuse of small \nportions of code, through component reuse, to the reuse of complete models [51]. \nComposability offers means to achieve reusability, but reusability might not always \nbe the ultimate objective of model composition. For instance, in a particular \nsituation, a set of modular components are purpose-fully built and composed to \nconstruct a large model, but they cannot be reused in a different project, due to their \nhighly specific design.  To be widely reusable, a component must be sufficiently \ngeneral, scalable, and adaptable. A requirement for reusability may lead to another \ndevelopment approach, for example, a design on a more abstract level [9]. The \ncomparison between usability and reusability of composable components poses a \ntradeoff between them being very specific in function and behavior so that they can \nbe used in a particular case to satisfy specific user’s requirements or them being very \ngeneric and abstract so that they can be reused in different situations again and again.  \n \nFigure 4: Generic vs. Specific component design \nFigure 4 illustrates a component is often more reusable if it has a generic design \nand less reusable if it has functionally specific design.  \n \nBoth use and reuse of composable components share three levels of transparency. \n[7]. A component can be seen as a box, which contains the interfaces and internal \nimplementation. Three levels of composability transparency are defined: \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n33 \n \nBlack Box Composition \nIn black box composition, the user sees the interface, but not the implementation of \nthe component. The user documentation is provided that contains the details of the \ninputs and outputs, requirements and restrictions of the component. All the \nimplementation details are hidden. The clients will get what the contract promises. \nThe changes are not feasible at the deployment end. The advantage of black-box \ncomposition is that the testing done at the development side is persevered and there \nis no need of further testing at the deployment side. \nGlass Box Composition \nIn glass box composition the inside structure of a component can be viewed, but it is \nnot possible to modify. This solution has an advantage when compared to black box \nreuse, as the modeler can understand the box and its use better. However it is not \npossible to make any changes in the implementation. The advantage of this level \nremains the same as that of black-box composition however an additional benefit is \nthat the user can gain knowledge of the internal implementation and can understand \nthe mechanics of the component.  \nWhite Box Composition \nIn white box composition it is possible to see and change the inside of the box as \nwell as its interface. A white box can share its internal structure and implementation \nwith another box through inheritance or delegation. The advantage of this level is \ngreater flexibility due to the provision of modifications. However this level incurs an \nextra burden of testing at the deployment end. \n \nFigure 5: Black Box, Glass Box, White Box \nFigure 5 illustrates difference between black box, glass box and white box \ncomposition.  \n \n2.4.2 Composability vs. Interoperability \nBearing in mind the definition of composability mentioned previously, the IEEE \ndefinition of interoperability is: \nThe ability of two or more systems or components to exchange information and to use the \ninformation that has been exchanged \nThe concept of interoperability is mainly about inter-connecting systems of various \ntypes developed for different purposes; for different platforms, and about their \nsyntactically and semantically agreed upon communication [13]. In the context of \nInternals \nnot known \nInternals \nknown \nInternals \nknown \nNo \nmodification \nNo \nmodification \nModifiable \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n34 \n \nmodeling and simulation, interoperability is the ability of different simulations \nconnected in a distributed system to collaboratively simulate a common scenario [19]. \nPage et al. [52] distinguishes composability and Interoperability as follows: \nComposability contends with the alignment of issues on the modeling level. The underlying \nmodels are purposeful abstractions of reality used for the conceptualization being implemented \nby the resulting systems; whereas Interoperability contends with the software and \nimplementation details of interoperations; this includes exchange of data elements via \ninterfaces, the use of middleware, mapping to common information exchange models. \n2.5 Composability Levels \nPetty and Weisel emphasized on two basic types of composability: syntactic and \nsemantic in their theory of composability [38] [50]. According to which the syntactic \ncomposability requires that the composable components should be constructed with \ncompatible implementation details such as parameter passing mechanisms, external \ndata accesses, and timing assumptions. The question of syntactic composability is \nwhether the components can be connected.  In contrast, semantic composability is a \nquestion of whether the models can be meaningfully composed to form a composed \nsimulation system and whether the combined computation is semantically valid. It is \npossible that two components may be syntactically linked, so that one can pass data \nto the other, but they can be semantically invalid. Figure 6 represents the difference \nbetween syntactic and semantic composability metaphorically.  \n \nFigure 6 Syntactic vs. Semantic Composability (acquired from [38]) \nComposability is studied in more depth under different levels, as identified by \ndifferent research groups. Several levels of understanding and agreement are required \nbetween the models in order for them to be meaningfully composed—that is, for \ntheir composition to produce meaningful results [17]. \nDavis recommended five distinctions of levels namely: syntax, semantics, pragmatics, \nassumptions, and validity to study composability [43]. He describes these levels as \ndifferent consistencies of composability, which all together are examined for the \ncorrectness of model composability. Petty & Weisel have suggested nine levels of \ncomposability in terms of composition units. These levels are: Application, Federate, \nPackage, Parameter, Module, Model, Data, Entity and Behavior [38]. Tolk described a six \nlayered model called Levels of Conceptual Interoperability (LCIM) to study \ncomposability and interoperability. This model includes: technical layer, syntactic \nlayer, semantic layer, pragmatic layer, dynamic layer, and the conceptual layer [13].  \nSimilarly Medjahed & Bouguettaya introduced a composability stack in which the \ncomposability of semantic web services is checked at four levels: Syntactic, Static \nSemantic, Dynamic Semantic and Qualitative level [53].  First three levels of \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n35 \n \nMedjahed & Bouguettaya’s composability stack were adopted by Moradi, et al. to \nstudy the degree of composability of Base object Model (BOM) components [54]. In \nthis thesis, these levels are considered as fundamental benchmarks for the evaluation \nof model composability. The notion of model composability and its correctness \nstrongly depend on the consistency of these levels as explained in the following \nsubsections.  \n2.5.1 Syntactic level:  \nAt this level, the structure of the components is studied to know if they can fit \ntogether i.e., the output of one can be read as an input to the other and that the \nsyntactic information of the connected components, such as message name, mode of \naction and number of parameters match each other e.g., A “passenger airplane” \ncomponent will be a syntactic misfit in a military training simulation, where a “fighter \njet” component is required whose input will be a signal from “ground station” \ncomponent to engage a target and output will be an airstrike on the “target” \ncomponent. A passenger plane can neither take a target designation as input, not it \ncan fire on a ground target. So this component is not composable at syntactic level.  \n2.5.2 Static-Semantic level:  \nIt is concerned with the meaningful interaction of the composed components. Static-\nSemantic level of composability involves in having a concise and mutual \nunderstanding of the data exchanged by the components participating in the \ncomposition. At this level, it is ensured that all the components possess the same \nunderstanding of the terms, parameters, data types and units, so basically this level \ndeals with the interpretation of same meaning of concepts for the information \nexchanged between the composed components. For instance, if two components \nbeing composed interpret units of quantities in a different way, they will incorrectly \nprocess data values during the information exchange and thus result in a situation not \nintended by the user e.g., if a integer data value is intended to be the bearing of a \ntarget (in degrees) but interpreted as target distance (in Km) by the other component \nthen it is a semantic mismatch.  \nThe term “static” is prefixed, because all the information that is required to evaluate \nthis level is static and does not change during the entire component interaction.  \n2.5.3 Dynamic-Semantic level:  \nDynamic Semantic Composability implies that the components are dynamically \nconsistent, i.e., they have suitable state-full behavior, necessary to reach the desired \ngoals and subsequently satisfy user requirements. The dynamic level of composability \nensures in having a behavioral consistency and coherency among the participating \ncomponents in achieving the common goals. The dynamic semantic composability \ncan only be achieved if the components are at the right states during their interaction. \nAlso they should possess required behavior to make a collective progress. E.g., in a \ncomposed model of a restaurant, a waiter component may have two different \nbehaviors (i) Classical restaurant where a waiter takes order from customer, serves \nfood and then collects payment or (ii) Fast food restaurant where waiter takes order, \ncollects payment and then serves food. The selection of the correct behavior and the \ncorrect customer component (the one who can correctly interact with the classical \nrestaurant waiter or fast food waiter) will affect the overall composability of the \nmodel. This example presents how the components should be at right states to make \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n36 \n \nprogress. A customer (expecting classical treatment) will wait forever for the (fast \nfood waiter) to serve food and vice versa.  \nEven if the components are at the right states, but their behavior is not correct, the \ncomposition may not reach its goals. E.g., in a manufacturing system two machine \ncomponents produce two different parts that are later combined to make a finished \ngood, and they share a single robot component for input of raw material, it is required \nthat the robot component should be fair so that both machines get more or less equal \nchance to proceed. If the robot is not fair the proportion of good produced will be \nunbalanced and therefore the system will fail to meet its objectives even though the \ncomponents are at right states and continue to progress. \nThe term dynamic is prefixed, because the information such as current state of \ncomponents changes dynamically during component interaction.  \n2.5.4 Pragmatic level:  \nConsistency of meaning is not always straightforward because the same word means \nvery different things depending on context [43]. Pragmatic consistency refers to a \ncontext based meaningful composition of the components. In linguistics the study of \nthe relations between linguistic phenomena and aspects of the context of language use is called \npragmatics whereas Context is defined as something that consists of the ideas, situations, events, \nor information that relates to it and makes it possible to fully understand it [55]. \nThe pragmatic level of composability evaluates the difference of actual effect of the \nmessages with the intended effect of messages during communication [43]. The \nresearch of pragmatic level of composability involves in-depth study of \ncomputational linguistics, cognitive technologies and contextual computing [55]. An \nimportant issue at this level is pragmatic ambiguity. Pragmatic ambiguity arises when \nthe message is not specific, and the context does not provide the information needed \nto clarify the statement, and due to which the components do not interact according \nto the desired objectives. An example of pragmatic ambiguity is the story of King \nCroesus and the Oracle of Delphi (derived from [56]): \n    \"King Croesus consulted the Oracle of Delphi before warring with Cyrus of Persia.  The Oracle \nreplied that, \"If Croesus went to war with Cyrus; he would destroy a mighty kingdom\".  Delighted, \nCroesus attacked Persia, and Croesus’ army and kingdom were crushed.  Croesus complained \nbitterly to the Oracle’s priests, who replied that the Oracle had been entirely right.  By going to war \nwith Persia, Croesus had destroyed a mighty kingdom – his own.\" \nIn essence, a set of components can possibly fit together (syntactically), and their \ncommunication is meaningful and understood (semantically), but unless all \ncomponents preserve essential behavior (dynamically) in order to reach the desired \ncomposition goals, and they share the correct contextual knowledge (pragmatically), \nthe composability cannot be qualified as correct with respect to given requirement \nspecifications. \n2.6 Composability frameworks \nComposability essentially relies on a suitable composition framework that can \nprovide accurate reasoning of its correctness and support means to be able to \nleverage certain component standard. Various component standards and their \nrespective frameworks have been developed for M&S to support composability. \nSome of these frameworks contribute to conceptual modeling by providing the \nneeded formalism and influence the ability to develop and compose model \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n37 \n \ncomponents at conceptual level, while others support model composition at \nexecutable level. These frameworks practically support composability, as they usually \noffer features such as model specification, development, and execution. A brief \ndescription of some of the composability frameworks is provided below: \n2.6.1 Discrete Event System Specification (DEVS) \nDEVS [57] is a component based formalism based on dynamic systems theory. It \nwas developed for the purpose of describing the structure and behavior of systems. \nIt supports the concept of hierarchical and modular model construction through \ncoupling of components [19]. DEVS is basically a model specification formalism \nhowever it incorporates different implementation frameworks such as DEVS-Java, \nDEVS-C++ and DEVS-Sharp which are used to implement DEVS models into \nexecutable form.  \n \nTwo types of DEVS models exist, namely, atomic and coupled [20].  \nAn atomic DEVS is a tuple M = 〈X, S, Y, δint, δext, λ, τ〉 where: \nX = {(p, v) | p ∈ InPorts, v∈Xp} is the set of input ports and values \nY = {(p, v) | p ∈ OutPorts, v∈Yp} is the set of output ports and values \nS is the set of states \nδint : S →S is the internal transition function \nδext: Q × X→S is the external transition function, where \n       Q = {(s, e) | s ∈S, 0 ≤ e ≤ τ(s)} is the total state set \n        e is the time elapsed since last transition \n λ : S →Y is the output function \nτ : S →R0,∞\n+  0, ∞ is the time advance function \n \nA DEVS atomic component has inputs X, outputs Y, and a set of S states. At a given \nmoment, a DEVS model is in a state s∈S. In the absence of external events, it \nremains in that state for a lifetime defined by τ(s). When τ(s) expires, the model \noutputs the valueλ(s) through a port y ∈ Y, and it then changes to a new state given \nby δint(s). A transition that occurs due to the consumption of time indicated by τ(s) is \ncalled an internal transition. On the other hand, an external transition occurs due to \nthe occurrence of an external event. In this case, the external transition function \ndetermines the new state, given by δext (s, e, x), where s is the current state, e is the \ntime elapsed since the last transition, and x∈X is the external event that has been \nreceived. The time advance function can take any real value between 0 and ∞. A \nstate for which τ(s)=0 is called a transient state (which will trigger an instantaneous \ninternal transition). In contrast, if τ(s)=∞, then s is said to be a passive state, in \nwhich the system will remain perpetually unless an external event is received.  \n \n \n \n \n \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n38 \n \nA coupled DEVS is a tuple: M = (X, Y, D, {Md | d ∈ D}, EIC, EOC, IC, \nSelect) where: \nX = {(p, v) | p ∈ InPorts, v∈Xp} is the set of input ports and values \nY = {(p, v) | p ∈ OutPorts, v∈Yp} is the set of output ports and values \nD is the set of component names \nMd is a DEVS model with \nXd = {(p, v) | p ∈ InPortsd, v ∈ Xp} \nYd = {(p, v) | p ∈OutPortsd, v ∈ Yp} \nEIC is the set of input port couplings \nEIC  ⊆ {((N, ipN), (d, ipd)) | ipN ∈ InPorts, d ∈ D, ipd ∈ InPortsd} \nEOC is the set of output port couplings \nEOC ⊆ {((d, opd), (N, opN)) | opN ∈ OutPorts, d ∈ D, opd ∈ OutPortsd} \nIC is the set of internal couplings \nIC ⊆ {((a, opa), (b, ipb)) | a, b ∈ D, opa ∈ OutPortsa, ipb ∈ InPortsb} \nSelect is the tie-break function \n \nA system modeled using DEVS can be described as a composition of atomic and \ncoupled components. A coupled model comprises a set of input and output ports, a \nset of component names D, a set of DEVS components Md, input port EIC and \noutput port EOC couplings, and, a set of internal couplings IC connecting internal \ncomponents with each other. The tie-break function decides which component to \nproceed when two or more components have internal transitions scheduled at the \nsame time.  \nFigure 7 describes a DEVS example. In this example two atomic component A & B \nare coupled together. Both components have two states Send τ(s)=0.1 and Wait \nτ(s)=∞. Input port: ?receive and Output port: !send are defined and connected to each \nother in coupled DEVS. \n \nFigure 7: Ping-Pong DEVS [Wikipedia] \n2.7 Base Object Model (BOM) framework \nThe SISO 9 standard BOM is defined as, “a piece part of a conceptual model \ncomposed of a group of interrelated elements, which can be used as a building block \nin the development and extension of simulations and simulation environments” [58].  \n \nBOM provides a simulation standard that allows model developers and simulation \nengineers to create modular conceptual models in form of composable objects, \n                                                 \n9 Simulation Interoperability Standards Organization \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n39 \n \nwhich can be used as the basis for a simulation or simulation environment [59], [60]. \nThe concept of BOM is based on the assumption that components of models, \nsimulations, and federations can be reused as building blocks in the development of a \nnew simulation or a federation [54].  \nBOMs are unique because they provide a means to represent aspects of a conceptual \nmodel that captures structural and behavioral descriptions of items abstracted from \nthe real system (simuland). Then they allow these conceptual models to be mapped \nto one or more class definitions, which may be used by a software design, variety of \nprogramming languages, or distributed simulation architectures such as HLA or \nTENA10 [61], [62]. \nBOM standard also offers a general purpose modeling architecture for defining \ncomponents to be represented within a live, virtual, or constructive (LVC) simulation \nenvironment. It is well suited for characterizing models including the structural and \nanticipated behavior of interacting systems, individuals, and other entities. Primarily \nBOMs framework poses a satisfactory potential for effective composability of \nconceptual models at syntactic and semantic levels, resulting in a framework for the \nassembly of a system (i.e. simulation) or system of systems (i.e. distributed simulation \nenvironment) [62].   \nIn spite of these reasonable qualities, BOM framework still falls short of required \nbehavioral semantics and necessary built-in evaluation techniques, which are essential \nfor modeling complex system behavior and \nreasoning about the correctness of the \ncomposability at each of its different level. \nTherefore it becomes a most suitable candidate \nand a preferred choice of a composition \nframework (in this thesis) for studying model \ncomposability in depth and applying proposed \nmethods on BOM based compositions to \nexplain the approach.  \n2.7.1 Structure of BOM  \nA BOM is constituted of elements specifying \nmetadata information, conceptual model and \nthe class structure information defined using \nHLA OMT constructs [59]. Figure 8 presents \ndifferent parts of BOM, explained as follows: \nModel Identification  \nModel Identification associates the metadata \ninformation with the BOM. Its purpose is to \ndocument certain key identifying information \nwithin the BOM description. It provides a \nminimum but sufficient degree of descriptive \ninformation about a BOM \n                                                 \n10 Test and Training Enabling Architecture \nFigure 8: BOM structure \n(acquired from [59]) \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n40 \n \nConceptual Model Definition \nFrom the composability point of view, this is the most important part of BOM and \ntherefore the main focus of this thesis. To understand this part, the definition of a \nconceptual model should first be considered: \nConceptual Model \nA Conceptual model is an abstract description or an appropriate simplification of a real (or \nproposed) system, which is later, refined and implemented in to a more concrete executable \nmodel (or simulation model). In these terms, conceptual modeling is a subset of model design \nwhich is formed through an iterative process according to the objectives of system modeling \n[63], [64]. \nThe term conceptual model is used in different ways in the literature. A conceptual \nmodel could be a specific diagram like UML class diagram or it could be \ndocumentation of a particular aspect of the simuland11 [29]. To better understand the \nconcept of BOMs, consider the home construction analogy. When a new house is to \nbe built the conceptual understanding of features of the building is captured in \narchitectural drawings, which is analogous to a conceptual model (BOM) [60]. BOM \nConceptual Model definition consists of following parts: \nPattern of Interplay (POI) \nPOI models a specific purpose or capability and is represented by one or more \npattern actions. For each pattern action, one or more senders and receivers are \nspecified to provide a means for understanding and the behavioral relationship \namong conceptual entities. POI is represented by UML sequence diagram [60]. \nState Machine  \nThe state machine is used to model the behavior of a BOM’s conceptual entity. The \nstate machine is specified by a set of states where each state may transit to a \nsubsequent state called next state, upon an exit action, which is identified in a pattern \nof interplay. UML state-machine diagram is used to represent BOM’s state-machine \n[60]. \nEntity Type  \nA conceptual entity is an abstraction of a real world entity. It defines a relationship \nwith other entities within a pattern of interplay and acts as a sender or receiver of the \nevents [60]. \nEvent Type \nConceptual events include information about the source, target, and content \n(parameters) of a message or trigger. The difference between a trigger and a message \nis that a trigger is used to broadcast information whereas the messages are directed \nexchanges of information where the sender knows about the intended receiver of the \nmessage [60].  \n \nEntities and Events represent data about the real world objects and their interaction \n(physical description), whereas the pattern of interplay and state-machine collectively \nrepresents the dynamic behavior of the component. \n                                                 \n11 A simuland is the real world system of interest. It is the object, process, or phenomenon to be \nsimulated [29]. \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n41 \n \n2.7.2 BOM Assembly \nThe BOM concept provides a mechanism for combining BOMs and creating High-\nLevel BOMs, called BOM Assemblies, as shown in Figure 9. A BOM Assembly \nrepresenting a composition of BOMs, is built in a hierarchical manner and includes \ninformation about composed BOMs, which in turn is used to identify a composite \ninterface, and represent a federate, federation within the simulation space12. Typically \na developer of a simulation would search a BOM repository for suitable BOM \ncandidates for use in a simulation and combine those into a BOM Assembly (i.e. a \nsimulation model), which is then used to create the actual simulation [19].   \nA BOM assembly contains Model Identification, and pattern of the interplay among \nconceptual entities being represented, which is provided through the association of \nBOMs to the various Pattern Description actions that the BOM Assembly identifies, \nwithin the Conceptual Model view [60].  \n \nFigure 9: BOM Assembly \nBOM models can be created using XML script. But for constructing BOM models \ngraphically, a free IDE tool called BOM Works [65] is available. Figure 10 represents \nan example developed using BOM Works. It is similar to the DEVS example shown \nin Figure 7, to compare the difference.  \n \nFigure 10: (a) PingPong BOM in BOM Works  (b) POI (c) State-machineA (d) State-machineB \n(e) EntityA (f) EntityB (g) EventA (h) EventB \n \n \n \n                                                 \n12 Although use of HLA is not a mandatory subsequent step, it is likely that BOM assemblies are \nintended to support an HLA based federation [59]. \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n42 \n \n2.7.3 Model Mapping and Object Model Definition \nThe model mapping provides a mechanism for mapping between the elements of the \nconceptual model and the class structure elements of the Object Model Definition \nthat are described using HLA OMT 13 specification constructs. The object model \ndefinition defines the structure of an object and interaction class, and their associated \nattributes and parameters. HLA Object classes include HLA attributes and HLA \ninteraction classes include HLA parameters. These parts of BOM are not used in this \nthesis, however interested readers can find more details in [58], [59], [60], [61], [62]. \n2.7.4 Formal specification for the Compositon of BOM \nUnlike DEVS, BOM does not have a graphical and mathematical formalism for \nspecifying how components are composed (even though parts of BOM such as state-\nmachine and POI can be represented in UML and BOM documents can be \ndescribed using XML). This initiates a need for a graphical and formal representation \nof BOM composition.  \nIn this section, we introduce a formal and graphical specification of BOM14. We \ndefine two types of BOM: (i) Basic BOM and (ii) Composed BOM. A basic BOM is \nan undividable atomic BOM component, with an assumption that it represents only \none conceptual entity at the most. A composed BOM is a hierarchical combination \nof basic and other composed BOM.  \nBasic BOM  \nWe propose that a basic BOM (BB) can formally be defined as: \n Where: \n EnT is an entity type. We assume that a basic BOM has only one entity. EnT is \ndefined as: \n     EnT =  Name {Characteristic: Type} \nWhere Name is the name of an entity uniquely defined by an identifier15 and characteristic is a set of \nattributes of an entity. Each characteristic is uniquely defined by an identifier and has a type16 \n \n EvT is a set of event types, each with sender, receiver and content \n \nEvt = {(Name, Sender, Receiver, {Content: Type}) | Name ∈ Identifier, Sender \n& Receiver ∈ EnT, Content∈ Identifier: Type ∈ type} \n \n S is a set of states, each has an exit-condition and a next state: \n     S = {(Name, ExitCondition{Action, NextState})} | Name ∈ Identifier, Action ∈ \nAct, NextState ∈ S \n \n \n                                                 \n13 High Level Architecture Object Model Template \n14 These concepts are not new and exist in literature for other component-based approaches [21]. In \nthis thesis, their application in BOM is intended for facilitating specification and ease of understanding \n15 An identifier is a unique sequence of letters & digits, starting with a letter. \n16 Type := Integer | String | Double | Complex \nBB = 〈 EnT, EvT, AcT,  S 〉 \n(2.3) \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n43 \n \n AcT is a set of actions, each has name, sender, receiver and an associated event: \n     AcT= {(Name, Sender, Receiver, Event) | Name ∈ Identifier, Sender & Receiver \n∈ EnT, Event ∈ EvT} \n \n \nComposed BOM  \nA composed BOM (CB) can formally be defined as: \nWhere: \n AcTIN is a set of input actions that are received from other BOM. This set can be \nempty if the Composed BOM is closed. \n    AcTIN = {(Name, Sender, Receiver, BOM) | Name ∈ Identifier, Sender &              \nReceiver ∈ EnT, BOM ∈ File} \n \n AcTOUT is a set of input actions that are sent to other BOM. This set can also be \nempty if the Composed BOM is closed. \nAcTOUT = {(Name, Sender, Receiver, BOM) | Name ∈ Identifier, Sender & Receiver \n∈ EnT, BOM ∈ File} \n \n POI is the pattern of interplay that defines how basic or composed BOMs are \nconnected to each other (through actions). It maps a list of send actions to a list \nof receive actions. ‘ ! ’ symbol means send and ‘ ? ’ symbol means receive. \nPOI = {({!AcTSEND} , {?AcTRECV})} | AcTSEND & AcTRECV ∈AcT \n \n \n \nExample \nAs an example, BOMs from Figure 10 can formally be represented as: \nBB0 = 〈 EnT, EvT, AcT, S 〉 where: \nEnT = EntityA {C0(Message:String)} \nEvT = {E0(EventA, BB0, BB1, BB0.C0), { E1(EventB, BB1, BB0, BB1.C0)} \nAct = { A0(ActionA, BB0, BB1, E0), A1(ActionB, BB1, BB0, E1)} \nS = { S0(Sending, A0, S1), S1(Waiting, A1, S0)} \n \nTable 1: Entity A \n \n \n \n \nCB = 〈 AcTIN,  AcTOUT ,  POI 〉 \n(2.3) \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n44 \n \n \nBB1 = 〈 EnT, EvT, S, AcT 〉 where: \nEnT = EntityB {C1(Message:String)} \nEvT = {E2(EventA, BB0, BB1, BB0.C0), { E3(EventB, BB1, BB0, BB1.C1)} \nAct = { A2(ActionA, BB0, BB1, E2), A3(ActionB, BB1, BB0, E3)} \nS = { S2(Waiting, A2, S3), S3(Sending, A3, S2)} \n \nTable 2: Entity B \n \nSimilarly a composed BOM CB0 can be formally described as: \nCB0 = 〈 AcTIN,  AcTOUT ,  POI 〉where: \nAcTIN = ∅ (since there is no incoming actions from any other BOM) \nAcTOUT = ∅  \nPOI = {I/O0(!A0 , ?A2), I/O1(!A3, ?A1)} \n \nTable 3: Composed BOM \n \nWe propose a graphical notation for representing basic BOM and their composition \nshown in Figure 11. In this figure two basic BOM EntityA and EntityB are composed.  \nFigure 11: Composed BOM \nThe general information of a component such as entity name, characteristics, actions \nand states are defined in the main block. In the lower block the states and their \ntransitions (with blue arrow) are shown. Each transition is mapped with actions (in \nred arrow) with parameter labels (the IDs of characteristics).  The direction of the \narrow shows the type of the associated action (send or receive). The composition of \nBOMs is shown through connectors (in green color).  \n \n\nChapter 2  \n \nComponent Based Modeling and Simulation \n   \nPage \n45 \n \n2.7.5 Summary \nIn this thesis, we harness the capability of BOM as a conceptual modeling \nframework, because it provides a component standard using an XML specification; \ngives guidelines for the further development of the executable model and helps \ndetermine the appropriateness of the model or its parts for model reuse; and most \nimportantly due to its strong support for syntactic and semantic composability. It will \nbe shown, how BOM with its existing potential can be facilitated by composability \nevaluation for accurate and rapid construction and modification of its corresponding \nfederates in HLA based simulations and hence brings forth an improvement in the \ndistributed simulation community. \n \n \n \n\n   \nPage \n46 \n \nChapter 3 \nExecutable Modeling Formalisms \n \nIn this chapter two popular model description formalisms are discussed namely Petri Nets and \nCommunicating Sequential Processes (CSP)17, which are normally used for modeling, execution (or \nsimulation) and verification of concurrent systems. This chapter provides an introduction, theory, \nproperties, classification, modeling methods and analysis techniques of PN and CSP. PN and CSP \nare both considered as a part of solution domain in this thesis, because of their impressive \naccumulation of knowledge in concurrency modeling and analysis techniques. These aspects are \nimported in this thesis and used for composability verification.  \n \nPN and CSP formalisms are relatives since they are used to model same class of \nsystems called concurrent systems. Unlike other systems such as transitions systems \nor automata, the formalisms of concurrent systems are strongly based on \nconcurrency theory. One of the major contributors of concurrency theory are: Carl \nAdam Petri who initiated concept of interacting sequential processes and introduced \nPetri Nets; C. A. R. Hoare who focused on developing programming language (CSP) \nfor concurrent systems; and Robin Milner who introduced Calculus of \nCommunicating System (CCS) and  π-Calculus. These are variants of approaches for \nformally modeling concurrent systems and are the member of the family of \nmathematical theories of concurrency known as process algebras, or process calculi. \nCSP is also a member of process algebra. The main difference between PN and CSP \nis that the former are based on graphs, while the latter are based on a textual \ndescription. However both offer strong formal semantics for modeling executable \nsystems and share a broad pool of knowledge of theoretical principles and practical \ntechniques for the analysis and verification of models of complex behavior. In this \nthesis, we propose using these two formalisms to model executable form of \ncomponents and study their composability.  \n3.1 \nPetri Nets \nPN were introduced by Carl Adam Petri (and named after him) in 1962. They \nprovide an elegant and useful graphical and mathematical formalism [24]. With PN \nthe main idea is to represent states of subsystems separately. In this way, the \ndistributed activities of a system can be represented very effectively. PN are widely \nused for modeling and control in a variety of the sorts of systems. Particularly, in \nDiscrete Event Dynamic Systems (DEDS) 18 in which many properties such as \nsynchronization, sequentiality (producer-consumer problem), concurrency and \n                                                 \n17 The \"Sequential\" word of the CSP name is now something of a misnomer, since modern CSP \nallows component processes to be defined both as sequential processes, and parallel [Wikipedia]. \n18 Examples of DEDS are air traffic control systems; automated manufacturing systems; computer \nand communication networks; embedded and networked systems; and software systems etc. The \nactivity in these systems is governed by operational rules designed by humans and their dynamics is \noften driven by asynchronous occurrences of discrete events [67]. \n \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n47 \n \nconflict (mutual exclusion) concurrency, and choices can be well presented and \nanalyzed using PN [66]. Their structural and behavioral properties have been \nsuccessfully exploited for solving various problems of complex and dynamic systems. \nSignificant progress in these directions was made over three decades. Most essential \nfeatures of PN are the principles of locality, concurrency, graphical and algebraic \nrepresentation. They can be used not only for the specification and analysis of the \nstructural system design but also for design of the system behavior. [66], [67].  \nPN present two interesting characteristics. Firstly, they make it possible to model and \nvisualize systems with complex behaviors including parallelism, concurrency, \nsynchronization and resource sharing. Secondly the properties of these nets, their \nanalysis and theorems have been extensively studied [68]. \n \n3.1.1 PN Definitions and Concept \nIn PN, two basic elements of modeling are places and transitions. Events are \nassociated with transitions which occur when some conditions are satisfied. \nInformation related to these conditions is contained in places. There are two types \nof places namely: Input places and Output places. Input places are associated with \nthe conditions required for this transition to occur. Output places are associated with \nconditions that are affected by the occurrence of this transition [25]. Transitions, \nplaces, and certain relationships between them define the basic components of a \nPetri net graph. A PN graph has two types of nodes, places and transitions, and \narcs connecting these. It is a bipartite graph in the sense that arcs cannot directly \nconnect nodes of the same type; rather, arcs connect place nodes to transition nodes \nand transition nodes to place nodes [25]. \n3.1.2 Petri net graph \nMathematically a PN is a 5 tuple: PN = 〈P, T, F, W, M0〉 where: \n P is a finite set of places P = {p1, p2… pm} represented as oval shaped node in the \nPN graph \n T is a finite set of transitions T = {t1, t2… tn} represented as a line or a rectangular \nshaped node in the graph  \n F is a flow function such that F ⊆ (P ×T)∪(T×P) →N 19  \n W: F →N + where N∈{1, 2, 3…} is arc weight function. \n M0: P→N is a function called the initial marking, where each element M0(p) has \nN number of tokens20 initially in place p where N is a set of non-negative integers. \n For each transition t∈T a set of input places denoted as •t are those places which \nare connected to t through incoming arcs: \n Similarly, for each transition t∈T a set of output places denoted as t• are those \nplaces to which t is connected through outgoing arcs: \n                                                 \n19Such that P∩T= ∅ (i.e. P&T are disjunctive sets) and P ∪ T≠∅ (i.e. neither P nor T are isolated).  \nAlso an arc can be connected from place to transition (input arc) or from transition to place (output \narc) but not to the node of same type. \n20 In classical PN, tokens are represented as black dots. They are assigned to, and can be thought to \nreside in, the places of a Petri net. \n•t = {pi | (pi, t) ∈F} \n(3.1) \nt• = {pi | (t, pi)∈F} \n(3.2) \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n48 \n \nDefinition: Marking \nA marking is an assignment of tokens to the places of a PN. The number and \nposition of tokens defines a system state, and it may change when the tokens move. \nThis movement of tokens due to the firing of transitions causes the execution of a \nPN [26]. The marking M can be defined as an n-vector, M = (m0, m1, m2 … mn), \nwhere n = |P| (no. of places), and each mn ∈ N, i = 1...n.  The vector M gives for \neach place pi in a PN the number of tokens in that place.  \nDefinition: PN State-space \nThe state of a PN model is defined by its marking. The firing of a transition \nrepresents a change in the marking of the net. The state space of a PN with n places \nis the set of all markings.  State-space will be discussed in detail later in this chapter. \nDefinition: Enabling of a Transition \nA transition t in a given PN is called enabled or fire-able by a marking Mi  iff for each \ninput place p∈•t its marking is equal or greater than the weight of the arc from it to t, \n(or t has no input place). Mathematically, a transition t is fire-able iff \nDefinition: Firing of a Transition \nIf a transition t is enabled, it may fire by removing W(p, t) number of tokens from \neach input place p and putting W(t, p’) tokens in each output place p’, due to which \na new marking Mn+1 is generated. \nMn+1 is immediately reachable from Mn. Mn is reachable from M0 if firing a sequence \nσ = t1, t2 … tk of enabled transitions leads M0 to Mn, written as M0\nσ→ Mn \n \n \nExample21 \n \n \nConsider the PN model PN = 〈P, T, F, W, M0〉 as shown in Figure 12 where: \nP = {p1, p2, p3, p4, p5} and T = {t1, t2, t3, t4},  \nLet W = 1 for all arcs  \nInitial marking M0 = [1 0 0 0 0] \n                                                 \n21 This example is inspired from [68] \n∀p ∈ •t | M(p)≥W(p, t) ∨ •t=∅ \n(3.3) \nMn+1 \n𝑡→ Mn | M(p’) = M(p) - W(p, t) +  W(t, p’) ∀p∈P \n(3.4) \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n49 \n \n \n \n \n \n \n  \n \n \nFigure 12: Transition firing sequence  (acquired from [68]) \n \nσ1: M0 = [1 0 0 0 0]  \n𝑇1\nሱሮ  M1 = [0 1 0 1 0]  \n𝑇2\nሱሮ   M2  [0 0 1 1 0]  \n𝑇3\nሱሮ    M4  [0 0 1 0 1]   \n𝑇4\nሱሮ   \nM4  [1 0 0 0 0] \n \n \nσ2: M0 = [1 0 0 0 0]   \n𝑇1\nሱሮ  M1 = [0 1 0 1 0]  \n𝑇3\nሱሮ   M3  [0 1 0 0 0]  \n𝑇2\nሱሮ   M4  [0 0 1 0 1]   \n𝑇4\nሱሮ   \nM4  [1 0 0 0 0] \n \nIn this example there are two possible transition firing sequences σ1= T1, T2, T3, T4 \nand σ2 = T1, T3, T2, T4 \n3.1.3 Properties of PN \nJust like other models, PN are constructed from informal requirement specifications, \nwhich is not a trivial task, and requires a great deal of modeling experience. If a \nsystem being modeled is very complex, a PN model may differ considerably from its \noriginal specification. A model can only be useful if it is logically correct with respect \nto its specifications [69]. Different concepts of correctness exist. A system is said to \nbe correct when two aspects, namely the specification and the implementation, are \nequivalent, or when the system satisfies a set of desirable properties. These desirable \nproperties allow the system designer to identify the correctness of the system [69].  \nIn PN literature a “basic kit of PN properties” is referred to a set of properties that \nare related to frequently occurring problems or the key issues related to the logical \nstructure and behavior of complex systems, therefore they are classified into two \nmain categories namely (i) Structural Properties and (ii) Behavioral Properties. It is \nimportant to note that fulfillment of these properties answer many questions of \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n50 \n \nsystem correctness, therefore they contribute in the analysis of PN models. Some of \nthe selected behavioral PN properties are listed and briefly discussed informally22 \nbelow.  \nReachability \nReachability is a fundamental property for studying the dynamic behavior of a \nsystem. In PN, reachability property is studied to analyze if a particular system state \n(in terms of markings) can be reached or not. A marking Mn is said to be reachable \nfrom an initial marking M0 if there exists a sequence of firings that transforms M0 to \nMn. In reachability analysis, a set of all possible firing sequences from M0 are \npopulated in a reachability graph R(N, M0) and the reachability problem for PN is the \nproblem of finding if a given marking Mn ∈ R(N, M0) [70]. \nBoundedness \nIn classical systems theory, a state variable that is allowed to grow to infinity is \ngenerally an indicator of instability in the system [25]. Therefore it is desirable that a \nsystem holds boundedness. A PN is said to be bounded (or k-bounded) if the \nnumber of tokens in each place does not exceed a finite number k for any marking \nreachable from initial marking, i.e., M(p) ≤ k for every place p and every marking Mn \n∈ R(N, M0) [70]. \nDeadlock-free and Liveness \nA PN is said to be deadlock-free if from any reachable marking at least one transition \ncan always occur. A stronger condition than deadlock-freeness is liveness. A \ntransition is live if it is potentially fire-able in all reachable markings. In other words, \na transition is live if it never loses the possibility of firing. A net is live if all \ntransitions are live [69]. \nReversibility \nA PN is said to be reversible if, from each marking Mn, the initial marking M0 is \nreachable. Thus, in a reversible net one can always get back to the initial marking or \nstate [70].  \nFairness \nFairness has different meanings and understanding in literature. In specific terms, \nfairness means to give some contenders an equal number of chances, such that no \none proceeds for more than “k-times” without letting the others to take their turn. In \nPN s, two transitions tl and t2 are said to be in a bounded-fair (or B-fair) relation if \nthe maximum number of times that either one can fire while the other is not firing is \nbounded. A PN is said to be a B-fair net if every pair of transitions in the net are in a \nB-fair relation [70].  \nMutual Exclusion \nThis property captures constraints such as the impossibility of a simultaneous access \nof a critical section (resource) by two or more processes. In PN, mutual exclusion \ncan be defined in terms of places or transitions. Two places p and q are mutually \n                                                 \n22 In literature these properties are discussed in detail with mathematical definitions and proofs [70]. In \nthis chapter they are only discussed for background concept. Some of these properties are used later \nin this thesis. \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n51 \n \nexclusive in a PN if their token counts cannot be both positive in the same marking, \ni.e., ∀m ∈ RS m(p)·m(q) = 0. Similarly, two transitions in a PN are mutually \nexclusive if they cannot be both enabled in any marking [71]. \n \nSome of the important structural properties of PN are defined below: \nControllability: \nA PN is said to be completely controllable if any marking is reachable from any other \nmarking [70]. \nConservativeness:  \nA PN N is said to be (partially) conservative if there exists a positive integer y(p) for \nevery place p such that the weighted sum of tokens, is a constant, for every marking. \nGiven a PN model, we are often required to ensure conservation with respect to \ncertain weights representing the fact that resources are not lost or gained. \nPersistence \nA PN is said to be persistent if, for any two enabled transitions, the firing of one \ntransition will not disable the other. A transition in a persistent net, once it is enabled, \nwill stay enabled until it fires [25]. \n \n3.1.4 PN Analysis \nThe major strength of PN is the modeling of systems that exhibit concurrency. \nHowever modeling by itself is of little use. It is necessary to be able to analyze the \nmodeled system. The analysis leads to important insights into the structure and \nbehavior of the modeled system [26].  There are many techniques available for the \nanalysis of PN models and can be employed for verification depending upon the \nnature of the model. Each technique may also have different variants. In this section \ntwo of the most commonly used techniques for the analysis of a PN model are \ndiscussed:  \n \nFigure 13: Petri Net Analysis Techniques \n \nThese techniques provide solutions and mechanism for verifying the properties \nmentioned in the previous section. In this thesis, these techniques are selected for \ncomposability verification and their application is shown in Part II with suitable \nexamples. In this chapter, they are briefly explained and discussed, with their \nadvantages and limitations. \nPetri Net  \nAnalysis Techniques \nAlgebraic Method \nState-Space \nAnalysis \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n52 \n \nAlgebraic Method \nThis technique is also called Linear-Algebraic Technique (or Linear Invariant due to \nits abundant use of invariants).  In the framework of using algebraic techniques for \nreasoning about PN, solving a PN problem is reduced to finding a solution for an \nalgebraic equation associated with the PN [24]. Due to the nature of this technique, \nthe method is in general efficient (and in most cases, polynomial in the size of the \nPN). The dynamic behavior of PN models can be described by algebraic equations. \nIn order to work with Algebraic method, the following basic concepts are applied: \nMatrix Definitional Form (MDF)  \nA PN model has a Matrix Definitional Form (MDF) that consists of three n×m2F23 \nmatrices:  \n(i) Output matrix A+ \n \ni.e., if pj is connected to the output of ti then 𝒂𝒊𝒋\n+ is equal to the weight of output arc; \n0 otherwise [70].  \n \n(ii) Input matrix A- \ni.e., if pj is connected to the input of ti then 𝒂𝒊𝒋\n− is equal to the weight of output arc; 0 \notherwise [70].  \n \n(iii) Incidence matrix A \nIn the incidence matrix A, each entry aij represents the change of tokens in place j \nwhen transition i fires once [70].  \n \nFiring Count Vector \nA marking Mk is an m × 1 column vector. The jth entry of Mk denotes the number of \ntokens in place j after the kth firing in some firing sequence. An n×1 column vector X \nof nonnegative integers is called firing count vector, where the ith entry of X denotes \nthe number of times transition t must be fired to transform Mk-1 to Mk [70]. \nState Equation  \nState equation for a PN is written as: \nWhere:  \nMk-1 is the current marking \n                                                 \n23 n×m refers n transitions and m places. \nA+ = [𝒂𝒊𝒋\n+] n×m, where 𝒂𝒊𝒋\n+ = w(ti, pj); if pj ∈ ti•, and i ∈ n; j ∈ m \n(3.5) \nA- = [𝒂𝒊𝒋\n−] n×m, where 𝒂𝒊𝒋\n− = w( pj , ti); if pj ∈•ti \n(3.6) \nA =  A+ - A- , where [𝒂𝒊𝒋] = [𝒂𝒊𝒋\n+ −𝒂𝒊𝒋\n−]  \n(3.7) \nMk =  Mk-1 + A.X  \n(3.7) \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n53 \n \nMk is the new marking \nA is incidence matrix  \nX is the firing count vector  \nExample \nAn example of a Producer-Consumer PN model is shown in Figure 14. \n \nFigure 14: Producer Consumer Example \n \nUsing equation 3.7 the incidence matrix A of this model is calculated as follows: \n \nA+ T1 \nT2 \nT3 \nT4 \nP1 \n1 \n0 \n0 \n0 \nP2 \n0 \n1 \n0 \n0 \nP3 \n0 \n1 \n0 \n0 \nP4 \n0 \n0 \n0 \n1 \nP5 \n0 \n0 \n1 \n0 \n \n \n \n \n \n- \n \nA- \nT1 \nT2 \nT3 \nT4 \nP1 0 \n1 \n0 \n0 \nP2 1 \n0 \n0 \n0 \nP3 0 \n0 \n1 \n0 \nP4 0 \n0 \n1 \n0 \nP5 0 \n0 \n0 \n1 \n \n \n \n \n \n= \n \nA \nT1 \nT2 \nT3 \nT4 \nP1 \n1 \n-1 \n0 \n0 \nP2 \n-1 \n1 \n0 \n0 \nP3 \n0 \n1 \n-1 \n0 \nP4 \n0 \n0 \n-1 \n1 \nP5 \n0 \n0 \n1 \n-1 \n \n \n \n \n \nTable 4: Incidence Martic A \n \n \nIn this model, the initial marking is [1 0 0 1 0]. With a firing sequence σ = t2, t1, t2 \nthe firing count vector will be [1 2 0 0]. Using the state equation, the marking Mx can \nbe generated as follows: \n \n \nM0 \nP1 1 \nP2 0 \nP3 0 \nP4 1 \nP5 0 \n+ \n \nA \nT1 \nT2 \nT3 \nT4 \nP1 \n1 \n-1 \n0 \n0 \nP2 \n-1 \n1 \n0 \n0 \nP3 \n0 \n1 \n-1 \n0 \nP4 \n0 \n0 \n-1 \n1 \nP5 \n0 \n0 \n1 \n-1 \n \n \n \n \n \n. \n \nX \nT1 1 \nT2 2 \nT3 0 \nT4 0 \n \n \n \n= \n \nMx \nP1 0 \nP2 1 \nP3 2 \nP4 1 \nP5 0 \nTable 5: State equation \n \n \n \nFigure 15 graphically illustrates, how a firing sequence of σ = t2, t1, t2 can lead M0 to \nM3. Green color highlights the firing of a transition. It can be noted that the marking \nM3 in the lower right corner matches the marking generated by matrix state-equation \nin Table 5. \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n54 \n \n \nFigure 15: M0 to M3 throguh firing sequece σ = t2, t1, t2 \nState equation alone can only help to algebraically compute a future marking. In \norder to analyze the model algebraically, some more concepts are used, such as PN \nInvariants.  \n \nPN Invariants \nOccurrences of transitions transform the token distribution of a net, but they \noften respect some global properties of markings, regarded as Linear Invariant \nLaws. Invariants are very useful for analyzing structural and behavioral properties \nof PN. From an initial marking, the marking of a PN can evolve by the firing of \ntransitions (and if there is no deadlock) the number of firings is unlimited. \nHowever, not just any marking can be reached, all the reachable markings have \nsome properties in common; a property which does not vary when the transitions \nare fired is said to be invariant. Similarly, not just any transition sequence can be \nfired; some invariant properties are common to the possible firing sequences. \nHence, invariants enable certain properties of the reachable markings and firable \ntransitions to be characterized, irrespective of the evolution.  \nFigure 16 illustrates a PN model of different seasons in a year. It can be seen that, \nregardless of the change of seasons, there will always be one and only one token \nfor all 4 places. Thus at all times, M(p1) + M(p2)  + M(p3)  + M(p4)  = 1. This \ninvariant property has an obvious meaning that at all time there is one and only \none season [68]. It also means that the net is structurally bounded. \n \nFigure 16: Seasons in a year (acquired from [68]) \n \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n55 \n \n \nThere are two important types of invariants of PN: \nP-Invariant \nPlace Invariants formalize invariant properties regarding places in PN, e.g., if in a set \nof places the sum of tokens remains unchanged independently of any firing, then this \nset can define a place invariant. They are useful to evaluate structural properties of \nPN. In simple words, a place belonging to a P-invariant is bounded [24], [70]. \nA P-invariant exists in a PN if \nWhere y is an m × 1 column vector of integers such that ∃ y = (y1, y2 … yn) > 0 i.e., \nhas at least one positive non-zero entry [71]. It means the firing of any transition does \nnot change the weighted sum of tokens in the PN. More generally, a vector y is called \nP-Invariant if  \nA . y =  0 \nIt is easy to see that if there is a P-invariant, for all p ∈ P, then the PN is guaranteed \nto be structurally bounded. Hence, place invariants can be used for reasoning about \nstructural boundedness [24]. P-invariant is a P-semi-flow if every element of it is \nnon-negative [67]. \nT-Invariants \nTransition Invariants on the other hand formalize properties regarding transition \nfiring sequences applicable to a PN. They are useful to evaluate behavioral properties \nsuch as liveliness and fairness [24], [70]. \nA n × 1 firing count vector X, is called a T-Invariant if \nA . X =  0 \ni.e., firing each transition the number of times specified in X, brings the PN back to \nits initial marking M0 [24]. T-invariant is a T-semi-flow if every element of J is non-\nnegative [67]. \nA T-Invariant X is a minimal T-invariant, if there is no other T-invariant X′ such that \nx′i ≤ xi for all i∈T. There can be multiple T-invariants for a PN. A minimal T-\nInvariant is called the Reproduction vector of the net. \nThe intrinsic difference between P- and T-invariants are the facts that all places in a \nPN if covered by P-invariants is a sufficient condition for boundedness, whereas the \nexistence of T- invariants is only a necessary condition for a PN model to be able to \nreturn to a starting state, because there is no guarantee that a transition sequence \nwith transition count vector equal to the T- invariants can actually be fired [71].  \nAdvantages and Disadvantages \nThe advantage of algebraic analysis is that the net structure is much less than the \nnumber of reachable markings and therefore there is no risk of state-space explosion. \nVarious properties of PN consequently can be proven using linear algebraic \ntechniques. However the weakness of this method is that it only entertains limited set \nof properties and provides only sufficient or necessary conditions. Also this method \n෍𝑚 . 𝑦𝑝= ෍𝑚0 . 𝑦𝑝\n𝑛\n𝑝=1\n \n𝑛\n𝑝=1\n ∀m ∈R(N, m0)  \n(3.8) \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n56 \n \ninvolves complex underlying mathematical theorems, each one different for different \nproperty verification and thus cannot be generalized for automated reasoning.  \nState-Space Analysis \nState space analysis is one of the most prominent approaches for conducting formal \nanalysis and verification. In contrast to algebraic techniques, it is relatively simpler \napproach for analyzing the behavior of a model. The basic idea in this approach is to \ncalculate all possible system states and the events which cause the change of states \nand represent them in a directed graph. When the graph is completely constructed, \ndifferent search techniques can be applied to analyze the model.  \nIn PN terms, this method is also commonly known as Reachability graph analysis. \nThe state-space analysis of a PN model is performed by exhaustively generating all \nthe reachable markings from a given initial marking, and then reasoning about the \nPN properties of the model by examining the structure of the reachability graph. \nThe reachability graph consists of vertices which correspond to reachable markings \nand of arcs corresponding to firing of transitions resulting in the passing from one \nmarking to another. A simple example of reachability graph is shown in Figure 17. \n \nFigure 17: (a) PN Model (b) Reachability Graph (acquired from [68]) \n \nIn some cases, the construction of reachability graphs becomes infinite if the PN or \nsome of its parts are repetitive and the net is unbounded, or in other words the PN \nhas infinite number of reachable markings. Therefore instead of keep on \nconstructing nodes of the graph infinitely, an alternative technique is used, in which a \nfinite graph is constructed by abstracting out certain details and inserting the symbol \nω (the symbol of “infinity”) to representing the marking of an unbounded place. This \nis called cover-ability graph. The coverability graph of the Producer-Consumer PN \nmodel is shown in Figure 18 \n \nFigure 18: Producer Consumer PN Model and its Coverability Graph \n \nIt can be seen that the markings in which place P3 is unbounded contain ω symbols.  \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n57 \n \nA constructed state space can help in answering a large set of analytical questions \nconcerning the structure and behavior of the model such as verifying deadlock-\nfreedom, absence of live-locks; presence of liveness, the possibility of being able to \nreach good states, and impossibility of reaching bad states and the guarantee of \nfulfilling the objectives.  Following are some examples of how state space analysis \nhelp in model verification:  \nBoundedness \nThe problem of boundedness is easily solved using a coverability tree with an \nassumption that a PN is bounded if the symbol ω never appears in its coverability \ntree. Since ω represents an infinite number of tokens in some place, therefore its \nabsence can guarantee that the PN is structurally bounded [25]. \nDeadlock freedom \nA deadlock freedom problem is solved, if there is no node in the graph (which is not \na final node), and yet it does not have an outgoing arc meaning there is no further \nenabled transition. Existence or one or more such nodes shows that the model has \npossibility of deadlock and can also help to find out the exact cause of it. \nLive lock freedom \nSimilarly, a live-lock can be detected using state space analysis. For concurrent \nsystems, a process is tasked to perform some particular actions [72]. These actions \nare normally intended to make progress and are called progress actions. A live lock is \ndetected, if there exists a cycle within the reachability graph, in which no progress \naction is being executed.  \nState Reachability \nReachability of good states (or bad states) can be guaranteed using state space \nanalysis. A state is reachable if there is a valid firing sequence that leads to that state \nfrom the initial marking. (In graph, there exists a path from the initial node to the \ncorresponding node of the desired state). There could be multiple paths in a graph \nthat reach the desired state. A shortest path analysis can be useful to analyze the \nminimum number of steps required to reach that state.  \nFor details on how state space analysis are conduced, interested readers are \nrecommended to refer to a very informative step by step tutorial on PN state space \nanalysis [73]. \n \nAdvantages and Disadvantages \nThe main advantage of state space method is that it is a way to explore all the \npossible states of the system. Also it provides counter examples as to why an \nexpected property does not hold. Furthermore, the automatic calculation and \ngeneration of state-space provides an ease of use, due to the fact that the computer \ntool hides a large portion of the underlying complex mathematics from the user, who \nis only required to formulate the property which is to be investigated and a suitable \nquery function to evaluate it [74]. \n \n \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n58 \n \nThe main disadvantage of using state spaces is the state explosion problem. The \nconstruction of the reachability graph is very expensive and intensive from a \ncomputational point of view. This is because the size of the state space may grow \nexponentially with respect to the size of the PN model (measured, for example, by \nthe number of places). Even relatively small systems may have an astronomical or \ninfinite number of reachable states. This problem escalates severely, when the models \nincludes time. A lot of effort has been invested in the development of reduction \nmethods to alleviate this problem. Reduction methods represent the state space in a \ncompact form. The reduction should not affect the properties of the system and they \nshould be preserved and can still be derived from the reduced state space. However, \ndue to the complexity and diversity in verification, there is no single reduction \nmethod which works well in all situations. Therefore the choice of a reduction \nmethod completely depends on the nature of the system being verified [74]. Some of \nthe important reduction methods are Sweep line method [75], Hash Compaction \nMethod [76], Symmetry Method [77] and Equivalence Method [78].  \nIn this thesis, we propose another reduction method which suits our need \n(Composability verification) and can help to alleviate the state explosion problem, if \nthe model under consideration becomes large and resource intensive. \n \n3.1.5 PN Classes \nThe computational power of basic or classical PN is weak as it has been shown that \nPN are not as expressive as Turing machines, making them inadequate for modeling \ncertain real-world systems. To overcome this shortcoming, a number of extended \nPN have been introduced to enhance the expressive capabilities of PN. There are \ndifferent ways to classify PN. In structural sense, they can be classified into three \nmain categories [79]: \n \nLevel-1 PN: are characterized by 'Boolean tokens', i.e. places are marked with at most \none unstructured token. \n \nLevel-2 PN:  are characterized by 'Integer tokens', i.e. places are marked with several \nunstructured tokens - they represent counters. \n \nLevel-3 PN:  are characterized by high-level tokens, i.e. places are marked with \nstructured tokens where information is attached to them. \n \nThere are many extensions of PN formalism. In this section we only discuss some of \nthe extensions of PN, which are used in this thesis. \nColored Petri Nets (CPN) \nCPN is a level-3 extension of PN, in which places are marked with structures token \nrepresenting data. CPN is a graphical language for constructing models of concurrent \nsystems and analyzing their properties. CPN is a general purpose discrete event \nlanguage which combines the capabilities of PN, as a foundation of the graphical \nnotation and a programming language (CPN ML), which is based on Standard ML \n[80] functional programming language, that provides the primitives for the definition \nof data types and for specifying data manipulation routines [78].  \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n59 \n \nCPN is formally defined by the tuple [81]: \nCPN = (P, T, A, Σ, V, C, G, E, I) where: \nP is a finite set of places  \nT is a finite set of transitions such that:  P ∩ T = ∅ \nA ⊆ P×T ∪T ×P is a set of directed arcs. \nΣ is a finite set of non-empty color sets. \nV is a finite set of typed variables such that: Type[v] ∈ Σ for all variables v ∈ V \nC: P→Σ is a color set function that assigns a color set to each place. \nG: T → Expression is a guard function that assigns a guard to each transition t \nE: A→ Expression is an arc expression function that assigns an arc expression to \neach arc a  \nI: P → Expression is an initialization function that assigns an initialization \nexpression to each place p. \nTokens of an ordinary PN have no types. With CPN it is possible to define token \nusing data types and complex data manipulation i.e., each token has attached a data \nvalue called the token color. The token colors can be investigated and modified by \nthe occurring transitions [81]. \n“CPN Tools” is a software package for the editing, simulation, state space analysis, \nand performance analysis of CPN models [82]. The tool acts as an integrated \ndevelopment environment (IDE) for the construction of CPN models. It provides a \ncanvas for creating PN graphs, offers features for writing CPN ML code with a \nfacility of incremental syntax checking. It also comes along with a bundled simulator \nthat efficiently handles the execution of untimed and timed nets. The most important \nfeature of CPN tool from our point of view is the generation and analysis of state \nspaces. The analysis of state space includes various built-in state-space querying \nfunctions, and support for creating analysis report which altogether greatly \ncontributes to the verification process. For further details of CPN formalism and its \napplication [78], [81] are referred.  \n \nFigure 19: A CPN Model \n \nFigure 19 shows a basic example of a CPN model. The nodes A and B in oval shape \nrepresent places. The place is initialized with three tokens of String type. The \nrectangular shaped node represents transition. An input arc connects Place A with \nthe transition with an arc variable v of type String (to carry tokens of the same type). \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n60 \n \nSimilarly an output arc connects transition to place B. The transition has a guard \nexpression that checks the token value. If the expression is true only then the \ntransition can be fired. The second part of the Figure 19 shows the result of the \nfiring of transition, i.e., the token “Token2” being deposited to place B. \nHierarchical CPN \nCPN model can be organized as a set of modules; where modules can be seen as \nblack boxes which make it possible to work at different abstraction levels, \nconcentrating on one at a time.  \nSubstitute Transitions \nCPN tools offer facility to construct hierarchical CPN models. In hierarchical nets a \ntransition can represent an entire piece of net structure. Such a transition is called \nsubstitution transition [82].  \nSub-page /Super-page \nA page that contains a substitution transition is called a super-page. When a CP-net \nuses a substitution transition the logic that the transition represents is kept on a page \ncalled a subpage [82]. \nPorts and sockets \nSuper-pages and sub-pages are connected by ports and sockets. A socket is a place in \nthe super-page that has at least one arc between a substitution transition and a \nsocket. A port on the other hand is a place in a subpage, marked with one of the \nport-type tags: (i) In-Port (ii) Out-Port or (iii) In/Out-Port. It is bound with a socket \nin the main page using Port & socket assignment. This relationship is used to define how \na subpage should be connected with the surroundings of its super-page. Some of the \nassignment rules are as follows: \n• A port with an In-tag must be assigned to a socket which is an input arc of the \nsubstitution transition.  \n• An Out-tag indicates that the port must be related to a socket which is an output \narc,  \n• I/O-tag indicates that the socket must be both an input and output arc [82]. \n \n \nFigure 20: Hierarchical Colored Petri Net \n \nFigure 20 presents an example of hierarchical CP-net. In the super-page (above), a \nsubstitute transition Process is shown which represents a sub-module (below). A \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n61 \n \nprocess has three stages, and input and an output marked with In and Out ports \nwhich are connected with A and B socket places in the super-page.  \nTimed Petri Nets \nPN with timing dependencies can be classified according to the way of specifying \ntiming constraints. These constraints can be timing intervals or single numbers, or \nelements of the net these constraints are associated with i.e., places, transitions or \narcs [83]. The next criterion is an interpretation of the timing constraints. When \nassociated with a transition, the constraint can be viewed as  \n \n(i) Firing time  \nA transition consumes the input tokens when it becomes enabled, but does not \ncreate the output tokens until the delay time associated with it has elapsed [83]. \n \n(ii) Holding time  \nWhen the transition fires, the actions of removing and creating tokens are performed \ninstantaneously, but the tokens created are not available to enable new transitions \nuntil they have been in their output places for the time specified as the duration time \nof the transition which created them [83]. \n \n(iii) Enabling time  \nA transition is forced to be enabled for a specified period of time before it can fire, \nand tokens are removed and created in the same interval [83]. \nTimed extensions are known also for high-level PN. One of them is timed Colored \nPetri nets [78], in which the time concept is based on introducing a global clock used \nto represent the model time. Tokens are equipped with time stamps, which describe \nthe earliest model times at which they can be used to fire a transition. Stamps are \nmodified according to expressions associated either with transitions, or with their \noutput arcs. Timing intervals can be interpreted as periods of non-activity of tokens, \nand the transitions are fired according to the strong earliest firing rule [78]. \nFormally a time PN is a tuple: \n \n \nN = (P, T, F,m0,Eft, Lft) \n \nWhere: \n(P, T, F, m0) is a PN,  \nEft = Earliest firing time for each t∈T \nLft = Latest firing time for each t∈T \n \n \n \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n62 \n \n3.2 Communicating Sequential Processes \nCSP is the second formalism that is selected in this thesis for the modeling of \nexecutable components. CSP is a language developed by Sir Charles Antony Richard \nHoare [84]. It aimed to be used for specification and reason about the concurrent \ninteraction of the system processes. The idea of CSP was conceived for the study of \nconcurrent processes using formal notation with required expressive power and \nalgebraic laws. The formal notation and the associated algebraic laws allow the \nprocess models to be controlled and analyzed. They also enable formal reasoning \nabout their correctness and prove equivalences between the processes. They also \nprovide sufficient theoretical foundations for the development of the necessary tools \nfor these purposes.  \n3.2.1 Basic Concepts and Definitions \nThe main primitives of CSP formalism are (i) Processes (ii) Events and (iii) Algebraic \nOperators. \nProcess \nIn CSP terms, a process is an independent, self-contained, modular description of an \nentity and a basic unit to capture behavior. A process has particular interface, \ncaptured by events that are used to interact with the environment which itself is a \nprocess, called the universe of the system (Σ). The environment can be viewed as a \nsystem of concurrently evolving processes. In any run a process performs a sequence \nof events. A process has a name, list of parameters and expression which determines \nits computational logic: \nProcess (parameters) = Expression \nExpression is behavior of a process which can be described as an occurrence of an \nevent or the sequence of some events, known as a trace. A process can only perform \na finite number of events in any finite time, and thus all traces have finite length [85]. \nEvents \nThe ultimate unit in the behavior of a process is an event [85]. Events characterize \ncommunications or interactions. Events are abstraction of observations. Each event \nforms an interaction between the process and its environment. If the interaction does \nnot occur then the process is blocked. Event can be defined with no data or data \nwith typed values. A set of all events of a Process P are called Alphabet of P (αP). \nThe following line describes a simple vending machine which takes in a coin and \ndispatches a coffee every time [84]. \nVM() = insert-coin → coffee → VM(); \nWhere VM() is a process (with no parameters) and its expression contains a sequence \nof atomic events: insert-coin and coffee and then the process is self-referenced \n(recursion). Events can be written in compound form, i.e., with parameters as shown \nin the following line: \nVM() = insert-coin.10 → coffee.1 → VM(); \nAlso there could be data operations using statement blocks inside the event body: \nVM() = insert-coin.10{Balance= Balance +10} → coffee.1{coffee--} → VM(); \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n63 \n \nA statement block could be a complete sequential program contains assignment \nstatements, if-then-else clauses, for or while loops and math functions etc. \nInput/output Channels \nProcesses may also communicate through channels. Channels are special type of \nevents, called communication events. Usually a communication on a channel results \nfrom an input and output occurring in parallel. The input channel is represented by \n‘?’ symbol whereas the output channel is represented by ‘!’ symbol. The channel \nparameters can be send or received using the form: c ! x or c ? y \nAlgebraic operators \nThere are many different useful operators that are used to represent different notions \nof process behavior and their compositions [85]. Some are described as follows: \n Prefix a → P \nThe prefix operator combines an event and a process to produce a new process. \n \n Sequential composition  P ; Q \nIt composes two processes P and Q in a sequential order i.e. the latter only starts \nwhen the former terminates  \n \n Deterministic Choice P \u0000 Q \nThe deterministic (or external) choice operator allows choosing between two \ncomponent processes, and allows the environment to resolve the choice \nexternally. \n \n Non-deterministic Choice P ⊓ Q \nThe nondeterministic (or internal) choice operator allows a choice between two \ncomponent processes, but does not allow the environment any control over \nwhich one of the component processes will be selected. \n \n Conditional Choice if cond  P  else  Q  \nThe choice depends on the evaluation of a condition to choose between P or Q. \n \n Interleaving P |||  Q \nThe interleaving operator represents completely independent concurrent activity \nbetween the processes P and Q i.e., without barrier synchronization. \n \n Parallel Composition P || Q \nThe parallel composition operator represents concurrent activity between P and Q \nthat requires barrier synchronization between the component processes. If an \nevent is in the alphabet of both P and Q, then it can only occur when both \nprocesses are able to engage in that event. \n \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n64 \n \n3.2.2 CSP Analysis Techniques  \nMany techniques have been developed for the analysis of CSP models however \nModel Checking has surpassed them all in many aspects and is commonly favored by \nmost of the CSP based modeling environments. In this section Model Checking \ntechnique is briefly described. \nModel Checking \n“Model checking is an automated technique that, given a finite-state model of a system \nand a formal property, systematically checks whether this property holds for that model \n[86]” \nThe instigation and rapid advancements of model checking methods is one of the \ntowering achievements in the area of model based software verification, especially \nwith the advent of difficulties faced by the computing communities when the \nstruggle of sequential program verification was followed by even more daunting \nexertion of verifying concurrent programs [87]. The growing difficulty in error \ntracing of such programs is due to the increase of complexity of the system behavior \nand the arbitrariness of large portion caused by emergent system states which cannot \nbe easily tacked by ordinary testing and debugging methods.  \nStarting from late 70’s Model checking and other similar algorithmic and automata \ntheoretic approaches are the result of efforts of notable researchers who pioneered \ndifferent standards that can be marked as a collective foundation of principles that \nshaped the modern model checking techniques [87].  \nModel checking became successful in different communities due to following \nreasons: \n Unlike traditional testing methods it is an exhaustive approach that provides an \nin-depth analysis of a system model to certify absence of bugs (instead of just \nfinding few of them through debugging). \n Model \nchecking \nreturns \nanswers \n— \neither \nsuccessful \noutcomes \nor \ncounterexamples showing the exact trace of errors and their causes \n Improvements in model checking techniques have effectively alleviated the risk of \nstate-space explosion problem [87]. \n Model Checking has a sound and mathematical underpinning and is based on \ntheory of graph algorithms, data-structures, and logic [86]. \n Model checking support formalism both for the specification of the input models \n(such as FSM, PN, CSP or others) and the specification of system properties \nbeing verified (which are mostly in the form of LTL or CTL or their extensions). \nTherefore any 3rd party community can use a model checker as a black box \nwithout knowing the insights and complexity of the process. \nBeside its various strengths some of the weaknesses include: \n Most model checkers require the models to have reduced details using compact \nand less expressive states and without specifying enumerations due to the risk of \nstate-explosion. Therefore the reduction in the system expressiveness may cost \nextra effort and possibly lead to overlooking important features and getting \ninadequate verification results. \n Despite the development of several very effective methods and improved data-\nstructures to combat the state-explosion problem, models of realistic systems may \nstill be too large to verify. \n \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n65 \n \nTypes of Model Checking  \nModel checking approaches are classified into two types: (i) Explicit and (ii) Symbolic \nbased on how they enumerate states [88]. \nExplicit model checking techniques store the explored states in a hash table, \nwhere each entry corresponds to a single system state. For just a few hundred states \nthe nodes in the state space graph becomes as large as ~1011 [88]. On the other hand \nexplicit model checkers support state-enumeration that gives detailed expressiveness \nof the system states. \nSymbolic model checking techniques store sets of explored states symbolically by \nusing efficient data structures represented by canonical structures such as Binary \nDecision Diagrams (BDDs) [89], and traverse the state-space symbolically by \nexploring a set of states in a single step. The use of these BDD-based methods has \ngreatly improved scalability in comparison to explicit state enumeration techniques, \nyet they have performance degradation because  BDDs constructed in the course of \nsymbolic traversal grow extremely large, and BDD size is critically dependent on \nvariable ordering. This causes a newer trend of research towards separating Boolean \nreasoning and representation. Hence Boolean Satisfiability (SAT) [90] has been \nstudied and explored for Boolean reasoning and efficient semi-canonical \nrepresentations which results in  the development of SAT-solvers which are efficient \nand have compact representation compared to BDDs. SAT, together with efficient \nrepresentation, have become a viable alternative to BDDs for model checking \napplications [88]. \nBounded model Checking is a model checking approach where the number of \nsteps in forward traversal of the state space are bounded and checks whether a \nproperty violation can occur in k or fewer steps [88]. The approach reports either \n“violation found” or “no violation possible within the bounded depth (i.e., k steps), \nwhich can be incremented to look ahead for possible violation of the property. This \nmethod is promising because it does not cause state-space explosion or at least let \nthe user control its possibility.  \nIn this thesis all three model checking approaches are accompanied by the tools \nselected for composability verification of CSP based models. \n \n3.2.3 Temporal Logics \nLogic provides formal languages containing formulas for the representation of the \nstatements and their logical reasoning within some area of application [91]. Generally, \na logical language is given by an alphabet of different symbols and the definition of \nthe set of formulas which are strings over the alphabet [91]. In logic, the term \ntemporal logic is used for representing and reasoning about propositions qualified in \nterms of time. Temporal logic has found an important application in formal \nverification, where it is used to specify system requirements. Linear Temporal Logic \n(LTL) and Computational Tree Logic (CTL) are its two main variants. LTL formulas \nare interpreted on computation paths. Let A and B be atomic predicates and ¬ , ∧ , \n∨ , ↔ and True be the operators of classical logic, whereas \n, \n, \n and U are \nthe operators of linear temporal logic called Next, Always and Eventually and Until.   \n \n \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n66 \n \nThe intuitive meanings of some LTL statements are: \n• ¬ A : A does not hold \n• A ∧ B : Both A & B hold \n• \nA: A holds at the next state \n• \nA: A holds in all states \n• \nA: A will eventually hold \n• A U B: A will hold until B holds. \n \nIn CTL there are additional path quantifiers ‘∃’ and ‘∀’ denoting ‘there exists a path’ \nand ‘for all paths’, respectively. CTL formulas are interpreted on computation trees. \nWith respect to a tree the intuitive meanings of the formulas mentioned above are: \n• ∃ \nA: There exists a path in which A holds at the next state \n• ∀ \nA: For all paths A always holds in all states \n \n3.2.4 Time CSP \nCSP has been in evolution for decades. One of the major extensions of CSP is \ndevised with timing primitives, denoted as TCSP, to support time sensitive process \nmodeling [92].  In TCSP, each of the untimed CSP operators is interpreted in a timed \ncontext, and two primitive timing operators are added: (i) timeout and (ii) interrupt, \nwith a Newtonian Time assumption (i.e., that all the processes have a single global \nclock with same progress rate). \n Timeout P ⊳d Q   \nTimeout operator can be used to introduce delay in the processes.  \n Timed Interrupt P △e Q \nInterrupt is used if the process is permitted to run for no more than a particular \nlength of time. \n \nThe concept of TCSP is used later in this thesis to model and perform verification of \nreal-time systems. \n \n3.2.5 Probabilistic Systems \nSystems that exhibit probabilistic aspects essential for designing randomized \nalgorithms, modeling unreliable or unpredictable behavior or specifying model-based \nperformance evaluation are called probabilistic systems [86]. In order to model \nrandom phenomena in such systems, transition systems are enriched with \nprobabilities. Probabilistic systems can be specified in different ways. Two very \npopular ways are: (i) Markov chains (MC) and (ii) Markov decision processes (MPD). \nIn this thesis, we considered MPDs as specification formalism for probabilistic \nsystems because they support both nondeterministic and probabilistic choices and \nunlike MC they can model the interleaving behavior of the concurrent processes in \nan adequate manner [86].  \n \n \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n67 \n \nA Markov Decision Process is a tuple 〈S, Act, P, linit, AP, L 〉 [86]. \nWhere: \nS = Set of states \nAct = set of actions \nP: S × Act × S → [0, 1] is the transition probability function such that for all states \ns∈S and actions α∈ Act: \n෍P(s, α, s′)∈{0,1}\ns′∈S\n \n \nlinit: S → [0, 1] is the initial distribution such that:  \n෍𝑙𝑖𝑛𝑖𝑡(s) = 1\ns′∈S\n \nAP is a set of atomic propositions \nL: S → 2AP is a labeling function \nThe concept of MDP is used later in this thesis to model and perform verification of \nprobabilistic systems. \n3.2.6 CSP Implementation Tools \nThere are a variety of implementation support tools and languages for developing \nCSP models such as CTJ (Java), CSP++ (C++), CSP.NET, PyCSP (Python), JCSP \n(Java) and CSP# (C-Sharp) [93].  \n \nSimilarly various techniques exist for CSP analysis such as: \n• FDR2 model checker is developed by Formal Systems Europe Ltd [94]. \n• ARC, the Adelaide Refinement Checker, is a CSP verification tool [95]. \n• ProB is an animator and model-checker and support refinement checking and \nLTL model-checking of CSP [96]. \n• PAT is a model checker, simulator and refinement checker for CSP [97]. \n \nIn this thesis we selected PAT model checker because of its user friendly \nenvironment for modeling CSP models, fast simulator and model checker and above \nall its support for CSP extensions such as Real-Time CSP, Probabilistic CSP and \nReal-time Probabilistic CSP. \n3.2.7 Process Analysis Toolkit (PAT) \nPAT is an established tool developed by National University of Singapore in \nconcurrent system verification and has been used in real-world industrial projects. \nPAT is designed to develop, compose, simulate and analyze event-based system \nmodels using an extension of CSP formalism called CSP-Sharp (or CSP#24). This \nextension comprises of some additions such as shared variables and asynchronous \nmessage passing. Moreover it supports using complex data types (such as Set, Queue, \nand Stacks) and functions from external libraries written in C# therefore allow to \n                                                 \n24It uses C# like syntax for the specification of CSP processes \n\nChapter 3  \n \nExecutable Modeling Formalisms \n   \nPage \n68 \n \nmodel complex process behaviors. PAT also supports automated refinement \nchecking and model checking of LTL extended with events [98]. \nPAT is an appropriate modeling, composition, simulation, verification and reasoning \nframework of CSP based process models. These models can be of different nature \nsuch as concurrent, real-time and probabilistic systems. The main strength of this \nframework is that it implements various model checking techniques and provide \nverification support for different properties. That includes general system properties \nsuch as deadlock-freeness, divergence-freeness or reachability and user specific \nproperties defined in terms of LTL assertions. It also includes refinement checking, \nmodel checking of real-time and probabilistic systems. To achieve good \nperformance, advanced optimization techniques are also implemented in PAT, such \nas partial order reduction using BDD, symmetry reduction and parallel model \nchecking [97]. \n \n3.3 Summary \nIn this chapter we have discussed two executable modeling formalisms namely: (i) \nPetri Nets and (ii) Communicating Sequential Processes and their associated \nconcepts, tools and techniques. Both formalisms are used in this thesis for describing \nexecutable models. The conceptual background of both PN and CSP is required to \nunderstand the approach presented later in this thesis.  \n \n \n\n   \nPage \n69 \n \nChapter 4 \nVerification and Analysis \n \nVerification and Validation are important aspects of any software engineering expedition. They are \nindependent procedures with different characteristics that are used to check that a program, service, \nmodel or a system is correct, meets requirements specifications and that it fulfills its intended purpose. \nThey are critical constituents for achieving the necessary levels of quality assurance, and are essential \nprerequisites for a credible and reliable use of the delivered product. The main focus of this chapter is \non Verification and its different analysis techniques. The aim of this chapter is to outline basic \nconcepts, principles, issues and different approaches of software verification. This chapter can be \nviewed as a manual to understand the verification process being proposed later in this thesis. \n \nThe correctness of a program is a relative concept, meaning that the program is \ndoing no less than prescribed by its specification [99]. Verification, Validation and \nTesting (VVT) in combination is a broader and more complex discipline of system \nengineering. In M&S the combination of Verification, Validation and Accreditation \n(VVA) is generally referred where “Accreditation” is the formal certification that a \nmodel or simulation is acceptable to be used for a specific purpose [100]. \nNevertheless the goal is to assure the quality of the product and the impetus behind \nthis assurance is intensified when the systems are highly critical, either because they \nare very expensive to produce, such as land rovers investigating outer planets, or \nbecause human lives depend on them, such as computers controlling airplanes and \ncars, and life assisting real-time systems in hospitals [101]. These systems need to be \ncorrect, because their failure can lead to loss of human lives or enormous economic \nlosses. Moreover correct systems can be used in a wrong manner which can also \nresults in a failure. This is a general problem when systems are designed in a modular \nfashion, and are implemented with assumptions on a new environment. A similar \ncase caused a drastic failure at the launch of Ariane-5 expendable rocket launch \nsystem, because a software module was reused from Ariane-3 with certain \nassumptions that did not hold for Ariane-5 which self-destructed just because one \nsingle variable of 64 bit floating point value was erroneously converted to a 16 bit \ninteger causing the system to crash [102]. So for critical systems it is worth the effort \nto have a guarantee that they are correct and have no errors. \nVerification and validation aim to increase the credibility of models and simulation \nresults by providing evidence and indication of correctness and suitability. \nVerification in particular deals with the correctness of the model perceived from a \nreal-system, whereas validation deals with the suitability or fitness of the model with \nrespect to its real-system. Testing on the other hand aims to uncover incorrectness in \nthe system. In the following section, definitions and concepts of these inter-related \nterms are discussed.  \n\nChapter 4  \n \nVerification and Analysis \n   \nPage \n70 \n \n4.1 \nSome Basic Concepts in Modeling and Simulation \nThe first applied technical discipline that began to struggle with the methodology and \nterminology of V&V was the operations research (OR) community, also referred to \nas systems analysis or modeling and simulation (M&S) community [103]. \n \nVerification \nAccording to the Department of Defense (DoD) Defense Modeling and Simulation \nOffice verification is defined: as a process of determining that a model implementation \naccurately represents the developer’s conceptual description and specification [104]. \n \nIn general verification refers to an evaluation process that determines whether a \nproduct is consistent with its specifications or compliant with applicable regulations. \nIn M&S, verification is typically defined as the process of determining if a model is \nconsistent with its specification [29]. Verification deals with the model correctness \nand is concerned with building the model right [28], i.e., a model which works \ncorrectly and has no bugs. In principle, verification is concerned with the accuracy of \ntransforming the model’s requirements into a conceptual model and the conceptual \nmodel into an executable model [29]. \nFor the sake of clarity the notions of correctness are defined as follows: \nCorrect: Free from error; accurate; in accordance with the fact, truth, or reason; Conforming to the \nacknowledged standards of a method, routine or behavior [Oxford Dictionary] \nCorrectness \nThe degree to which a program, model or a system as a whole is free from defects in its specification, \ndesign, and implementation [105] \nThe ability of a software product (or a simulation model) to perform the exact task, as defined by its \nspecification [106]. \nWe define a composed model to be correct if its structure and behavior matches its \nspecification. Correctness of a composed model is therefore relative to its \nspecifications. A software entity can exist in three apparent states of correctness \nnamely: (i) correct when it has been established correct against its specification; (ii) \ndefective when it has been established incorrect against its specification and (iii) \nunknown when its correctness has not been established against a specification [107]. \nIn SE a software entity's specification is the sum of all its passing unit-tests [107]. We \ndefine specification to be a set of goals (or objectives) and property constraints (see \n1.3.2) that must be fulfilled by the composed model to be established as correct. \n \nValidation \nAccording to the Department of Defense (DoD) Defense Modeling and Simulation \nOffice validation is defined: as a process of determining the degree to which a model is an \naccurate representation of the real world from the perspective of intended uses of the model [104]. \nModel validation on the contrary, deals with building the right model, i.e., the model \nwhich is an accurate representation of the real system [28]. Model validation is usually \ndefined to mean “substantiation that a computerized model within its domain of \n\nChapter 4  \n \nVerification and Analysis \n   \nPage \n71 \n \napplicability possesses a satisfactory range of accuracy consistent with the intended \napplication of the model [108]. \nTesting \nModel Testing on the other hand, ascertains whether inaccuracies or errors exist in \nthe model. The objective of testing is to show that the model (or system) is incorrect \n(rather than proving that it is correct). Testing can only find errors but cannot \nguarantee the absence of errors; therefore it is more of an ad-hoc and inexpensive \nmethod of necessity, where the correctness is established merely on the fact that all \ntests have passed, which is insufficient and unreliable. When the test fails, it succeeds \nin revealing an error. When a test is passed, it fails to detect an error. If a number of \ntests fail to detect a bug, they increase a confidence level in the system even if the \ncorrectness cannot be guaranteed [99]. \n \n4.1.1 Verification and Validation in a Modeling Process \nA Modeling Process has been defined by Sargent [108] as shown in Figure 21. In this \nprocess Verification is referred to as an activity which ensures that the computer \nprogramming and implementation of the conceptual model is correct. \n \nFigure 21: Modeling Process (acquired from [108]) \n \nWhereas validation is defined in three perspectives: \nConceptual model validity is defined as determining that the assumptions \nunderlying the conceptual model are correct and that the model representation of the \nproblem entity (simuland) is “reasonable” for its intended purpose.  \nOperational validity is defined as determining that the model’s output behavior has \nsufficient accuracy for the model’s intended purpose.  \nData validity is defined as ensuring that the data necessary for the model execution \nand model experiments to solve the problem are adequate and correct [108].  \n \nMike Petty in his article [29] also clarifies the difference between the two terms at \ndifferent stages of model evaluation process as illustrated in Figure 22. \n\nChapter 4  \n \nVerification and Analysis \n   \nPage \n72 \n \n \nFigure 22: Modeling Process (acquired from [29]) \n \nA simuland is the real system that is to be simulated whereas a model is a \nrepresentation of the simuland, developed with its intended application in mind and \ntherefore captures only the necessary abstractions of the simuland and omit others. \nThe requirements are driven by the intended application. Conceptual models \ndocument those aspects of the simuland including the structural and behavioral \naspects such as objects, entities, events, functions, environmental phenomena etc. \nThe executable model is the computer program that can be executed and is intended \nto simulate the simuland as detailed in the conceptual model. Therefore the \nconceptual model can be viewed as a design specification for the executable model. \nThe results are the output produced by a model during a simulation. \nFigure 22 presents Verification and Validation as activities that compare one thing to \nanother. Verification compares the requirements with the conceptual model. In this \ncomparison, verification seeks to determine if the conceptual model satisfies the \ngiven requirements. The second comparison is between the conceptual model and \nthe executable model, where the goal is to determine if the implemented executable \nmodel is consistent with respect to the conceptual model. Validation compares the \nsimuland with the conceptual model to determine if the simuland has been accurately \ndescribed in the conceptual model. The second comparison is between the simuland \nand the results which determine if the output of the simulation is sufficiently accurate \nwith respect to the actual behavior of the simuland [29].  \n \nAnother comprehensive VV&T model is presented by Balci [28] in the form of a \nsimulation study life-cycle as shown in Figure 23. The phases are shown by oval \nsymbols. The dashed arrows describe the processes which relate the phases to each \nother. The solid arrows refer to the credibility assessment stage. Every phase of the \nlife-cycle has an associated VV&T activity. Problem Formulation (or problem \ndefinition) is the process of formulating a problem which is sufficiently well-defined \nto enable specific research action and the investigation of suitable solution \ntechniques. The output of system investigation results in the System and objective \ndefinition which further aids in model formulation. Model formulation is the process \nof defining a conceptual model which abstracts or envisions the real system under \nstudy. The conceptual model is further represented inform of a Communicative \nModel which is a model representation and can be communicated to other designers \nand can be compared against the system and the study objectives. It is further \n\nChapter 4  \n \nVerification and Analysis \n   \nPage \n73 \n \ntransformed into an executable model through the process of programming. An \nExperimental Model is the programmed model incorporating an executable \ndescription of operations along with the design of experiments, for experimenting \nwith the simulation model with a specific purpose. The process of experimentation \nproduces the Simulation Results, which are presented for decision makers for their \nacceptance and implementation or undergo refinements if required. \n \nFigure 23: Simulation study life-cycle (acquired from [28]) \nThe model-evaluation life-cycles shown in Figure 21, Figure 22 and Figure 23 have been \nconsidered as guidelines and they are used as inspiration for the verification life-cycle \nproposed and presented later in this thesis.  \n4.2 The Principles of Top-Down Refinement \nThe principle of top-down refinement has been appreciated in the area of model \nverification. Constructing a highly detailed model that satisfies all levels of \ncorrectness in one attempt is very difficult. Instead it is easy to construct a less \ndetailed abstract model at first. Let S1 be an initial model. To get from S1 to the final \nshape of the model, the Top-Down Refinement paradigm advocates the derivation \n\nChapter 4  \n \nVerification and Analysis \n   \nPage \n74 \n \nof an (ordered) sequence S1, S2…Sf of models of S. For i = 1...f, model Si+1 is a \nrefinement of its immediate predecessor model Si if the following conditions are met: \n \n(i) Si+1 is more expressive than Si  \n \n(ii) Si+1 is less abstract than Si  \n \n(iii) It is relatively easy to evaluate Si+1 on the basis of verified Si \n \nConsequently, the last model in the refinement sequence should be correct by \nconstruction. The following are some consequences of the top-down refinement \nparadigm. First, Si+1 is harder to understand than Si and therefore harder to prove on \nits own; it is precisely the refinement step that allows the verification of Si+1 under \nthe assumption that Si has already been proved correct [99]. \nIn this thesis the proposed verification process is based on this fundamental principle \nwhere the verification is performed iteratively and on a relatively refined shape of the \nmodel. \n4.3 Verification techniques \nThere exist a large variety of verification methods. The diversity is due to the range \nof different simulation project types, different subjects (simuland), and different \ntypes of data. Most of the verification methods are inspired from software \nengineering domain, because the executable models in simulation projects are almost \nalways realized as software [29].  \nIn literature, Verification techniques are generally classified into four main categories \nas show in Figure 24. \n \nFigure 24: Verification Techniques \n \n \n4.3.1 Informal Techniques \nThese techniques are most commonly used. They are called informal because the \ntools and methods used rely heavily on human reasoning and inspection without any \nunderlying mathematical formalism [28]. These techniques are well structured and are \nconducted with proper guidelines by following standard policies and procedures, \nhowever these techniques are tedious and not very much effective [109].  \n \n \nVerification  \nTechniques \nInformal \nTechniques \nStatic Analysis \nDynamic \nAnalysis \nFormal Analysis \n\nChapter 4  \n \nVerification and Analysis \n   \nPage \n75 \n \n \nSome of the commonly used informal methods are shown in Table 6. \nAudit \nAn audit is undertaken to assess how adequately the system study is \nconducted with respect to established plans, policies, procedures, \nstandards and guidelines [28].  \nDesk \nchecking \nDesk checking or self-inspection is a thorough examination \nperformed by an individual as a first step. In this method syntax \nchecking, specification comparison, code, control flow graph \nanalysis are performed [28].   \nInspections \nInspections are conducted by a team and performed at different \nphases of developments such as problem definition, conceptual \nmodeling, executions etc. Inspections are conducted to find and \ndocument faults [28].   \nTuring Tests \nTuring test is performed by domain experts (of the system under \nstudy). They are presented with two sets of output data obtained \none from the model and one from the specification (without \nidentifying which one is which) and are asked to differentiate both \nand based on their feedback model corrections are made [28].  \nTable 6: Informal Verification Techniques \n4.3.2 Static Analysis:  \nThese techniques are applied to assess the static model design and the \nimplementation (source code), without executing the model. They aim at checking \nthe structure of the model, the dataflow and control flow, the syntactical accuracy, \nand the consistency. Some of the commonly used static analysis methods are shown \nin Table 7. \nStructure \nAnalysis \nStructure Analysis is used to examine the model structure. It is \nconducted by constructing a control flow graph of the model \nstructure [28].  \nData \nAnalysis \nIt involves data dependency tests and data flow analysis to ensure \nthat data used by the model is properly defined and proper \noperations are applied to data objects [28].  \nCause-\nEffect \nGraphing \nCause-Effect graphing assists model correctness evaluation by \nanswering “what causes what” questions in the model representation.  \nIt is performed by identifying causes and effects in the model and \nchecking if they are reflected accurately in the specification [28]. \nSyntactic \nAnalysis \nSyntactic analysis is usually performed by the compiler of the \nsimulation language being used. Syntactic analysis can also be \nperformed using a set of rules applied on the model representation to \nverify if it satisfies given specification. \nSemantic \nAnalysis \nThis technique is used to determine the modeler’s intent and verify \nthat the true intent is accurately reflected in the model representation \n[28].  \nTable 7: Static Analysis Techniques \n \n\nChapter 4  \n \nVerification and Analysis \n   \nPage \n76 \n \n4.3.3 Dynamic Analysis:  \nDynamic analysis techniques are based on the execution of the model in order to \nevaluate its behavior. They do not simply examine the output of an execution but \nalso observe the model as it is being executed. The insertion of additional code into \nthe model called instrumentation is needed to collect or monitor the behavior during its \nexecution [109]. Table 8 presents some of the important dynamic analysis verification \ntechniques. \n \nAssertion \nChecking \nAn assertion is a statement that should be true during the \nexecution of a model. Assertions are placed in various parts of \nthe model and monitored during execution [28]. \nBottom up \nChecking \nThis technique is used in conjunction with the bottom up \nmodel development strategy. The sub models are checked \nindividually. Then the parents at the higher level are checked \n[28].  \nFault/Failure \ninsertion  \nThis approach is used to insert a fault or a failure in the model \nand observe whether the expected incorrect behavior is \nproduced. This approach is effective to detect unexplained \nbehavior and hence uncover errors [28].  \nFunctional \nTesting \nThis technique is used to assess the accuracy of model input-\noutput transformation, to evaluate how accurately a model \ntransforms a given input into a set of output data [28]. \nSensitivity \nAnalysis \nSensitivity analysis is performed by changing the values of \nmodel input variables and parameters over some range of \ninterest and observing the effect on model behavior. \nUnexpected effects may reveal errors [28]. \nTable 8: Dynamic Analysis Techniques \n \n \n4.3.4 Formal Analysis \nFormal analysis refers to mathematical analysis of proving or disproving the \ncorrectness of a system with respect to a certain unambiguous specification or \nproperty. The methods for analysis are known as formal verification methods, and \nunambiguous specifications are referred as formal specifications. Formal verification \ncan provide complete coverage on an abstract model of the system, modeled using \nfinite state machines, PN or any other specification formalism. However it should be \nnoted that formal verification can ensure the correctness of a design only with \nrespect to certain properties that it is able to prove [88]. There are many formal \nanalysis techniques, which we classify in four main groups: \n \n \n \n \n \n\nChapter 4  \n \nVerification and Analysis \n   \nPage \n77 \n \nEquivalence \nChecking \nIt is also called Reference Model Checking, which is widely \nused verification technique that allows two behavioral models \nto be compared with each other. In general, one of the two is \ntaken as the reference model and represents the so-called \ngolden model (or perfect model). It verifies that the behavior of \ntwo models is the same for the exercised scenarios. This \ntechnique has limitation that it does not actually verify that the \ndesign is bug free, and provides proof of relative correctness \n[109]. \nTheorem \nProving \nThis method involves verifying the truth of mathematical \ntheorems that are postulated or inferred throughout the design \nusing a formal specification language. The procedure involves \ntwo main components: (i) proof checker (which can be \ncompletely automated in most cases) and (ii) an inference \nengine (which may require occasional human guidance) [109].  \nProperty \nVerification \nFormal properties specify the requirements of the correct \nsystem design. The objective of this method is to check \nwhether an implementation satisfies these requirements. Static \nAssertion-based Verification (ABV) and dynamic [110]. \nModel \nChecking  \nModel checking establishes a solid confidence in a reliable V&V \nprocess. Model checking is an automated and comprehensive \nverification technique that can be used to verify whether the \nproperties specified (usually using Temporal Logic) for a given \ndesign or its components are satisfied for all legal design inputs. \nModel checking also faces a limitation, since it suffers from the \nwell-known state explosion problem. In a worst-case scenario, \nthe state space of the design may grow exponentially large with \nthe number of state variables. Model checking can be fully \nautomated for design verification and can yields results much \nmore quickly than theorem proving [109].  \nTable 9: Formal Analysis Techniques \n \nSome of these techniques have been adopted in our proposed verification \nframework.  \n4.4 Summary \nIn this chapter, different concepts of verification, validation and testing are discussed \nas they collectively contribute to proving the correctness and accuracy of a model. \nSome existing model development processes (devised mainly by M&S community) \nare also discussed, since they are the bases of the proposed verification life-cycle \npresented later in this thesis. The proposed framework essentially focuses on \nVerification (however its design is also open to adopt validation techniques). \nDifferent verification techniques are classified into four main groups and some of the \nselected techniques are briefly explained, as they will be used later in this thesis. \n \n\nChapter 4  \n \nVerification and Analysis \n   \nPage \n78 \n \n \n \n \n \nPart II \nTechne \n \n \n \n \n \nTechnê in Greek is translated as craftsmanship or craft or art. In science it is the practice of \nknowledge; Techne resembles Epistēmē in the implication of knowledge of principles, although techne \ndiffers in that its intent is making or doing, as opposed to \"in-depth understanding\"; Applied-\nScience; It deals with “How” of the subject. \n \n \nPart-II covers the technology of the research under discussion, where the theoretical \nconcepts provided in Part I are applied, and technically discussed under an integrated \nframework of methods, techniques, algorithms and processes and their practical \nimplications are provided in the form of a proposed solution. \n \n \n“Without knowledge the practice is useless, and without practice \nthe knowledge is useless” \n– Ali bin Usman Hajvery \n(Kashaf-Almahjoob) \n \n \n\n   \nPage \n79 \n \nChapter 5 \nProposed Methodology and the \nVerification Framework \n \nThis chapter renders the core of the solution framework proposed in this thesis.  In this chapter, a \ncollection of methods, techniques, algorithms, sub-processes, activities and approaches are presented, \nas proposed solution to various issues in the composability verification of BOM based model \ncomponents. All these contributions are integrated into a unified framework which we refer to as: \nComposability Verification Framework. \n \nThe proposed verification Framework consists of different methods, techniques, \nalgorithms, sub-processes, activities and approaches which all together encompass \nthe component based modeling & simulation (CBM&S) life-cycle.   \n5.1 \nComponent-based Modeling & Simulation life-cycle \nCBM&S life-cycle is inspired by different modeling architectures proposed by \nSargent, Petty and Balci and discussed in section 4.1.1. It is extended with our \nproposed contributions at its different stages. The proposed CBM&S life-cycle is \nmainly divided into four main quadrants: (i) Inception (ii) Modeling (iii) Execution \nand (iv) Analysis. Each quadrant has different phases and in each phase there are \nmultiple activities (or cycle of activities). Each activity consists of methods and \ntechniques pertinent to its respective phase. These phases are revisited iteratively \nduring the life-cycle; where each iteration represents a tier; hence the entire CBM&S \nlife-cycle is a multi-tier process; whilst each tier results into a refinement of the \nsolution of the problem under investigation; as it follows the principle of top down \nrefinement, discussed in section 4.2. All the above mentioned features of the \nCBM&S life-cycle are shown in Figure 25 divided into four quadrants:  \n \nFigure 25: CBM&S life-cycle \n \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n80 \n \nThe following sub-sections provide microscopic details of each quadrant along with \ntheir associated inside activities, methods and techniques. \n5.2 Inception \nThe first quadrant of the CBM&S life-cycle called “Inception” initiates the process. \nAt first the abstraction of a real-system is accumulated as simuland. A simuland can \nbe ingested in the form of UML diagrams (Figure 26) or using any other formal or \ninformal representation. \n \nFigure 26: Simuland using UML Diagrams \nThe basic idea is to gather the body of knowledge so that the modelers can envision \nthe real system under a certain frame of reference i.e., the context under which the \nsystem is being studied. When the simuland is ingested into the framework, it is used \n(i) to gather requirements, through the process of requirement engineering and (ii) to \nsearch and discover suitable components from a BOM repository for the \nconstruction of a composed model. If a required component does not exist in the \nrepository then it is built from scratch and added in the repository. The outcome of \nthe requirement engineering activity results in formulation of requirements \nspecifications. The requirement specification formalism (as defined in section 1.3.2) \nis used to express formal requirements for this framework:  \nRS = 〈O, S〉 \nWhere  \n \nO = {o1, o2, o3 …, on} is a set of objectives or goals that must ultimately be fulfilled. \nThese goals are usually defined in the context of the scenario of the modeling \ndomain. Therefore the properties expressed as goals or objectives may be scenario-\nspecific and not the standard system properties e.g. in a restaurant model the \nobjectives could be that the customers are served food and payments are collected, \nand not that the model should be deadlock free (which however might be a necessary \ncondition).  \n \nS = {s1, s2, s3 …, sn} is a set of system constraints (system properties or scenario-\nspecific safety/liveness properties). Deadlock freedom (or other similar system \nproperties) could be the required constraints necessary to fulfill the above objectives \nand therefore must be satisfied. We propose to define the following mandatory (or \ndefault) constraints in the requirements specification of the composability \nverification framework:  \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n81 \n \n \nS1 = All the interacting components25should be composable at Syntactic level \nS2 = All the interacting components should be composable at static-semantic level \nS3a = State-machines of the interacting components should match each other such that they can \ncontinue to progress until they reach the final or goal states26. \nS3b = If the conceptual model is transformed into an executable model, the latter should correctly \nrepresent the structure and behavior of the former.  \nTable 10: Mandatory constraints in composability verification \nWe assert that [S1 ∧ S2 ∧ (S3a ∧ S3b)] is a necessary condition for the overall \ncomposability verification. S1 and S2 ensure that the composed model is structurally \nconsistent. Whereas S3a confirms that the behavior of the composed model is \ncoherent for reaching given objectives. The satisfaction of S3b obeys the definition \nof Model verification (see section 4.1) in the sense that it confirms the second part of \nthe definition that is: “the accuracy of transforming the conceptual model into an executable \nmodel” and therefore the overall success of the verification process depends on the \nsatisfaction of S3b constraint. The conjunction of these default constraints impose \nthe three C’s of requirements namely (i) Consistency, (ii) Completeness, and (iii) \nCorrectness [111]. Consistency is required for the evenness in the input and output \nconnections of the composed components. Completeness is required for the totality \nof the information of the components being composed to check that the \ncomposition does not lack required inputs for making progress. Correctness is \nneeded to confirm that the composed components interact in a correct way as they \nare supposed to.  \nIf all the objectives are fulfilled and all the constraints are satisfied and then we say \nthat the model is composable at all levels and is verified with respect to its \nspecifications. The overall objective of our proposed framework is to provide \nenvironment and tool support to assess this postulation. \nThe outcome of discovery results in a set of candidate BOMs and their matching \nwith the simuland and the requirements results in a selection of BOMs suitable for \nthe composition. This selection is composed to form a conceptual model. \n5.3 Modeling \nIn the Modeling quadrant, a BOM based composed model is taken as an input and \nthe conceptual model is formed. Also a formal model and its graphical notation (as \nproposed in section 2.7.4) are produced for the purpose of documentation of the \nconceptual model26F27. Considering that BOM itself is a conceptual framework and is \nused to model passive components which cannot undergo any form of execution \ntherefore the conceptual model is subjected to a series of extensions and refinements \n                                                 \n25  In a composed model it is not necessary that every component interacts with every other \ncomponent for instance A, B and C are composed such that A interacts with B and B interacts with C \nbut A does not interact with C.  \n26 If there are no final-states defined in a model and the model is non-terminating then we assume that \ncertain important states called goal-states are present in the model, reachability of which confirms that \nthe goals are fulfilled.  \n27 This step is optional but beneficial if different teams are working on different phases of the \ndevelopment life-cycle. This documentation makes it easy to understand the structure and behavior of \nbasic components and their composition. \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n82 \n \nusing external input and our proposed model transformation algorithms so that it \ncan be implemented into executable forms and sent to the “Execution” quadrant \n(Figure 25) for abstract level execution. Our proposed extensions and refinements are \nlisted as follows: \n• BOM State-machines to State Chart XML (Transformation) \n• Composed-BOM to Petri Net –PNML (Transformation) \n• Basic-BOM to Extended-BOM (Extension) \n• Extended-BOM  (E-BOM) component to Colored Petri Net (CPN) Component \nModel (Transformation) \n• Basic-BOM to Extended-BOM with Time (Extension) \n• Basic-BOM to Extended-BOM with probabilistic factors (Extension) \n• BOM to CSP based Process Model (Extension & Transformation) \nIn the later section these extensions, refinements and transformations will be \nexplained in detail. It is important to note that each time the conceptual model is \nextended or refined the Modeling quadrant is revisited in iteration. \n5.4 Execution \nAs previously discussed this quadrant is mainly for the abstract-level execution \nactivities. It takes following implemented and executable forms of the conceptual \nmodel from the Modeling quadrant as input: \n• State Chart XML (SCXML)  \n• Petri Net –PNML  \n• Colored Petri Net (CPN) Composed Component Model \n• Communicating Sequential Process (CSP) based Component Processes \nIn the later section these executable forms and their abstract level execution \nprocesses will be discussed in detail.  \n5.5 Analysis \nThe outcome of an execution process yields some results. These results are analyzed \nin the Analysis quadrant.  Our verification framework supports different analysis \ntechniques listed as follows: \n• State-machine matching Analysis \n• Petri Nets based Algebraic Analysis \n• Colored Petri Net based State-Space Analysis \n• Model Checking Analysis \n \nThese analysis techniques will be discussed in later section. When all the necessary \nsteps in the composability verification are complete and the composed model under \ninvestigation is said to be verified with respect to the given requirement specification \nthen the CBM&S life-cycle proceeds to the further steps for implementation and \nsimulation as shown in Figure 27. The details of these steps are out of the scope of \nthis thesis. \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n83 \n \n \nFigure 27: Implemenation and Simulation \n5.6 Composability Verification Framework \nIn this section different method, techniques, procedures, algorithms and modules of \nour proposed composability verification framework are discussed in detail and \nconsidered as building blocks in the CBM&S life-cycle and will be connected to its \ndifferent phases. These details are necessary to understand the composability \nverification process being presented in chapter 6.  \n5.6.1 Discovery Matching and Composition (DMC) \nIn component based development, it is a normal practice to construct reusable \ncomponents and store them in a library or repository so that they can be reused later \nas required. To reuse an existing component, a Discovery, Matching, Composition \n(DMC) paradigm [19] is used. We assume that a library of BOM components is \nmaintained in a repository. Using the information given in the simuland a modeler \nattempts to search and discover BOM components from the repository. If a \ncollection of candidate components is retrieved, they are filtered through matching \nprocess. A matching process matches the candidate components from the simuland \nand requirement specifications and results in a selection of components suitable for \nthe composition. The aspects of syntactic and semantic matching during the \ndiscovery and selection of BOM components are proposed and discussed in detail in \n[54]. In this article a set of discovery rules are presented which must be fulfilled while \nmatching a candidate selection from the simuland. We apply these rules for the \nsyntactic and semantic matching of the candidate selection with the simuland. We \nfurther suggest matching the candidate selection with given requirements, because a \nselection may match with its respective simuland but if it does not match with its \nrequirements then the composability verification will fail. We implement the concept \nof DMC process in our framework as shown in Figure 28. It is also assumed that if a \nrequired component does not exist in the repository, then it is constructed from \nscratch and is added in the repository for reuse. The result of DMC process is a \nBOM-based composed model. This composed model is taken as input in the \nModeling quadrant and considered as a conceptual model of the system. It is \nrecommended that the modelers also use our proposed formal specification and \ngraphical notation presented in section 2.7.4 to construct a formal model. This \nformal model can be used for documentation and shows how the components are \ncomposed. It is however an optional step and is not considered as a phase in our \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n84 \n \nCBM&S life-cycle. In chapter 7 & 8 the formal models of the examples are also \ndescribed for reader’s understanding.  \n  \nFigure 28: Discovery, Matching, Composition (DMC) \n5.6.2 Structural and Behavioral Evaluation \nThe conceptual model ingested in the Modeling quadrant requires structural and \nbehavioral evaluation so that we can confirm that the model is consistent, complete \nand correct. And it is suitable for thorough verification at different levels of \ncomposability. Checking the structure and behavior of the conceptual model before \nsubjecting it to the deeper levels of composability verifications is useful. If the model \nis structurally and behaviorally consistent then the confidence level is increased based \non which different useful assumptions can be made later during the in-depth \nverification. \nIf there are discrepancies in the structure or behavior of the model then we can skip \nfurther steps, save time and computational resources and perform necessary design \nrefinements before the entire process is repeated.  This setup obeys the principle of \ntop-down refinement as discussed in section 4.2. The structure of the model is \nanalyzed using static analysis techniques (see section 4.3.2), whereas the behavior of \nthe model is evaluated using dynamic analysis techniques.  \n \n5.6.3 Static Analysis \nWe propose two types of Static analysis procedures (i) Syntactic Matching and (ii) \nStatic-Semantic Matching. These procedures are used to evaluate the structure and \nverify composability at syntactic and static-semantic levels. They are called static \nanalysis because they are evaluated based on pre-defined rules and do not require any \nform of execution and the information on which these rules are applied is static. \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n85 \n \nSyntactic Matching (SM) \nThis module is responsible for evaluating BOM composability at syntactic level based \non the following rules. The outcome of this module verifies that the components can \nbe correctly connected to each other syntactically. These rules were introduced in a \nBOM matching technique presented in [54]. \nSM-Rule 1: \nThe name of each event28 exchanged between the two components should be same i.e., \nthe send-event should have the same name as the receive-event.  \n \nA send-event is defined in the BOM’s event types where the sender is the BOM itself \nand the receiver is some other BOM (in the composition) whereas a receive-event is \nthe definition of an event in the BOM event types, where the sender is some other \nBOM (in the composition) and the receiver is the BOM itself. \nSM-Rule 2: \nEach send-event should have at least one corresponding receive-event and vice-versa i.e., \nthe send/receive pair should be complete.  \n \nSM-Rule 3:  \nThe number of parameters (content characteristics of event types) of the send-events \nshould be the same as the number of parameters of the receive-events. \nThe satisfaction of Syntactic Matching rule1, rule2 and rule3 fulfills the default \nconstraint S1 (see Table 10) which is a necessary condition for the overall \ncomposability verification. Figure 29  shows different steps in the syntactic matching \nactivity.  \n \nFigure 29: Syntactic Matching \n \n                                                 \n28 It is assumed that in the BOM construction the events and their corresponding actions are given the \nsame name \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n86 \n \nStatic-Semantic Matching (SSM) \nThis module is responsible for evaluating BOM composability at static-semantic level \nbased on certain rules. The outcome of this module verifies that the composition of \nthe components is meaningful and the communication between the components is \nunderstood as intended. In order to certify these facts we propose static-semantic \nmatching at two levels: (i) Operational Level matching and (ii) Message level [53]: \n(i) \nOperational Level matching \nIn BOM-based composed models Operations are described by Pattern-of-Interplay \n(POI). POI is formed by a collection of actions from the basic BOMs being \ncomposed. In operational-level semantic matching, it is ensured that the composed \ncomponents share the same “domain of interest” and they are composed for the \nsame purpose (or aim) so that we can guarantee that the composition is (static) \nsemantically meaningful and without any pragmatic ambiguity. Even with the same \ndomain of interest, the component may serve for varied purposes e.g., in Military \ndomain a Battalion Head Quarter (BHQ) component may have many purposes and \ncan take part in many different operations.  Therefore it is also important that the \npurpose of the selected components should be clear for a meaningful outcome. \nIn order to ensure semantic consistency at operational-level we propose to specify \nfollowing semantic-attributes 29 in the definition of actions at the time of the \nconstruction of Basic BOMs and in the POI when the basic BOMs are being \ncomposed. In the static-semantic matching these attributes are used to compare \nthat the correct actions are involved in the BOM composition. \n \no Area-of-Interest: It describes the area or the domain of interest of the system that \nis being modeled using the components and the operation. We propose to define \n“Area-of-Interest” as a semantic-attribute in each action of Basic BOM and also in \nthe POI. This attribute will confirm that all the components share the same domain \nknowledge. If of some general purpose components that may belong to multiple-\ndomains (e.g., Queues etc.) we propose to construct a specialization of the \ncomponent and make it a member of the selected area-of-interest. E.g., In a \nrestaurant composed model a generic queue component can be specialized into a \nrestaurant-queue with actions JoinRestaurantQueue() and ServeCustomer() instead of \nPut() and Get() actions. \n \no Purpose: Purpose describes the aim or goal of the entire operation. In BOM \ncomposition, POI represents a single operation being performed by the composed \ncomponents. However it is also possible that one or more composed components \nmay be designed to serve multiple purposes; and in a given scenario only some part \nof the multi-purpose components is involved in the composition. e.g., a Customer \ncomponent could be generic and can have multiple purposes whereas a Restaurant \nwaiter component is specific to a restaurant scenario, so it is important that if a \nCustomer component is selected in a Restaurant scenario then its purpose should \nbe aligned with the other components in this scenario. Hence we define “purpose” \nas a semantic-attribute of actions in the basic BOM (with multiplicity  ≥ 1). \n \n                                                 \n29 In BOM the conceptual modeling elements (Entities, Events States and Actions) support semantic \nfields [65] \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n87 \n \n(ii) \nMessage Level matching \nBOM represent event driven components and function by sending or receiving \nevents (messages). At the message level it is required that the communication \nbetween composed components is meaningful and semantically understood by the \nreceivers as intended by the senders. At this level we propose to match Data-Types \nand Units of measurements of the parameters of send-events and receive-events [53] \n[54]. \n \nIt is assumed that the BOM components have corresponding OWL attachments as \nproposed in [54]. The BOM-OWL attachments are used to define semantic classes of \nthe domain ontology, their properties, data-types and the individuals and stored in \nthe BOM repository. In order to evaluate static-semantic matching at both \nOperational and Message levels, we apply following rules: \n \nSSM-Rule 1 \nThe intersection of the “Area-of-Interest” attribute of all the actions (involved in an \noperation) should be exactly the same as that of POI or should belong to an \nequivalent class30 in the respective ontology: \nሩ\nActi. AOI\nn\ni=1\n ≅ POI. AOI \n \n \nSSM-Rule 2 \nThe intersection of the “Purpose” attribute of all the actions should be exactly the \nsame as that of POI or should belong to an equivalent class in the respective ontology: \nሩ\nActi. purpose\nn\ni=1\n≅ POI. purpose \n  \nSSM-Rule 3 \nData types of each element in the event parameters of the send-event and receive-events \nshould be of the same class, equivalent class or should be in direct hierarchical \nrelationship i.e., the sender’s parameter data-type should belong to the direct child class \nof the receiver’s parameter data-type (but not the inverse). \n \ne.g., a send-event contains a parameter of type ‘second’, whereas the receive-event \nexpects a parameter of type ‘time’ which according to the rule it is a semantic match. \nFigure 30 presents primitive data-types as an example. In real situations BOM \ncomponents will have more domain specific complex data-types. \n                                                 \n30In OWL two classes can be marked equivalent if they have same semantic meanings and both classes \nhave the same individuals (instances) e.g., Healthcare and Medical are synonyms. We denote it as  ≅ \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n88 \n \n \nFigure 30: Some of the sub-classes of Data Type ontololgy \nSSM-Rule 4 \nThe units of the measurements expressed in the event parameters should be same or \nequivalent or should belong to a direct class hierarchy such that they are convertible \nwithout (or with acceptable) loss of information.  \nWe assume that if two measurement units are in either of the direct relationship i.e., \nparent or child then their conversion loss will be acceptable e.g., a send-event has a \nparameter with unit m/s (meter per second) to express speed whereas the receive-\nevent expects Km/hr (Kilometer per hour). This is a valid semantic match because \nthe quantities are convertible without loss.  \nSemantic Matching Technique \nIn order to match two elements we propose a semantic matching technique as shown \nin Figure 31. This technique uses OWL-API [112], a semantic reasoning engine \n(FaCT++, Pellet, or HermiT) and an OWL ontology document to process a query of \nany two elements A & B and outputs their semantic relationship as one of the \nfollowing: \n1. Exact (A = B) \n2. Equivalent (A ≅ B i.e., A and B belong to equivalent classes)  \n3. Direct-Parent (A is a direct parent of B) \n4. Direct-Child (A is a direct child of B) \n5. Indirect (A and B are not in direct contact but belong to same hierarchy) \n6. No relationship (A and B are not related) \n \nFigure 31: Semantic Matching Technique \nThis technique is used to evaluate Static-Semantic Matching Rules 1, 2, 3 & 4 using \nthe algorithm31 given in Table 11.  \n                                                 \n31 The Pseudo-code conventions and format of the algorithms provided in this thesis, for most parts, \nfollows the guidelines set by [132]. \nOWL \nDoc \nA \nReasoner \nOWL-API \nB \nQuery \nRelation \nResult \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n89 \n \nAlgorithm: Semantic Matching \nInput: {Actions}, POI, BOM-OWL \nOutput: TRUE, FALSE \n1 Owl ←  Load Ontology(BOM-OWL) \n2 {CommonAOI} ← ⋂\n𝑎𝑖 \n𝑛\n𝑖=0\n∈ Actions.AOI  ⊳ Gives a set of common area of interest of all actions \n3 for caoi ∈ {CommonAOI} do \n4 \n   SR1 ← Get-Semantic-Relation(caoi, POI.AOI, Owl) ⊳ It is assumed that Get-Semantic-Relation()  \n5 \n    function is implemented using semantic matching technique shown in Figure 31 ⊲ \n6 \nif SR1 = “Exact” or “Equivalent” then  ⊳ Rule1 satisfy...continue \n7 \nnext \n8 \nelse \n9 \nReturn FALSE \n10 \nend if \n11 end for \n12  \n13 {CommonP} ← ⋂\n𝑎𝑖 \n𝑛\n𝑖=0\n∈ Actions.purpose  ⊳ Gives a set of common purpose of all actions \n14 for cp ∈ { CommonP } do \n15 \nSR2 ← Get-Semantic-Relation(cp, POI.purpose, Owl) \n16 \nif SR2 = “Exact” or “Equivalent” then  ⊳ Rule2 satisfy...continue \n17 \nnext  \n18 \nelse \n19 \nReturn FALSE \n20 \nend if \n21 end for \n22  \n23 {Events} ←  Get-Events(Actions) ⊳ gets corresponding Events of Actions \n24 \nfor  e ∈ Events  do \n25 \nif e=Send-Event then \n26 \nf ← Get-Receive-Event(e, Events) ⊳ gets corresponding Receive Event of e \n27 \n{PE} ← e.Parameters ⊳ Set of parameters of send-event e \n28 \n{PF} ← f.Parameters  ⊳ Set of parameters of receive-event f \n29 \n⊳ No. of parameters of e and f must be same because of SM-Rule3 \n30 \nfor pe∈PE & pf ∈PF do \n31 \nSR3 ← Get-Semantic-Relation(pe.Type, pf.Type, Owl) ⊳ Compare Parameter types \n32 \nif SR3 = “Exact” or “Equivalent” or “Direct-Child” then   \n33 \n⊳ Rule3 satisfy…continue to rule4 \n34 \nSR4 ← Get-Semantic-Relation(pe.Unit, pf.Unit, Owl) ⊳ Compare Units \n35 \nif SR3 = “Exact” or “Equivalent” or “Direct-Parent” or “Direct-Child” then   \n36 \nReturn TRUE ⊳ Static-Semantic Matching Successful \n37 \nelse \n38 \nReturn FALSE \n39 \nend if \n40 \nelse \n41 \nReturn FALSE \n42 \nend if \n43 \nend for \n44 \nelse \n45 \nnext \n46 \n⊳ Goes to the next send-event and need not to check receive-events (because SM-Rule2) \n47 \nend if \n48 \nend for \nTable 11: Semantic Matching Algorithm \n \n \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n90 \n \nThe semantic matching algorithm takes a set of actions (parsed from Basic BOMs \nwhich are being composed); the pattern of interplay (POI) which specifies how the \nactions are connected to each other and the corresponding OWL ontology document \nas input. The output of this algorithm is TRUE if the static-semantic matching is \nsuccessful otherwise FALSE if any of the rule is violated. Figure 32 shows steps in \nthe verification of BOM composability at Static-Semantic level. \n \nFigure 32: Static-Semantic Matching \nIf the semantic matching is successful, it will fulfill the default constraint (S2) of the \nrequirement specification (see Table 10) which is a necessary condition for the overall \ncomposability verification.  \n5.6.4 Dynamic Analysis \nWe use Dynamic Analysis technique (see section 4.3.3) to evaluate the behavior of \nthe conceptual model. At first the components undergo a state-machine matching \nprocess for the evaluation of the behavior consistency. When this evaluation is \nsuccessful, we proceed with the in-depth verification at the dynamic-semantic \ncomposability level, choosing one of the different proposed set of dynamic analysis \ntechniques. These analyses are called dynamic analysis because they require execution \nat different abstract levels as mentioned in section 5.4 \nState-Machine Matching (SMM) \nState-machines represent behavior of the components and are the essential dynamic \npart of BOM components. In the verification of BOM composability at dynamic-\nsemantic level, it is important that the behavior of the composed components should \nbe coherent with each other i.e., their interactions are consistent in order to make \nprogress towards composition goals. To ensure this fact we assert (as a necessary \ncondition) that the state-machines of the composed components should match each \nother. BOM state-machines are event driven in nature and make progress by \nexchanging events. In order to ensure that the state-machines of the composed BOM \ncomponents match each other they are required to be executed at an abstract level. \nTherefore we proposed a technique in [113] which transforms each BOM state-\nmachine to SC-XML (State-Chart XML) [114] format. A sample of SCXML is shown \nin Figure 33.  \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n91 \n \n \nFigure 33: SCXML format \nWe develop a runtime environment using SCXML API for the execution. This \nenvironment parses SCXML files (transformed BOM state-machines) and creates \ninstances. Then it initializes all the state-machines to their initial states and simulates \nsending and receiving of the events to observe state-machine transitions until they \nreach their final state. The state-machine matching process is based on the following \nalgorithm: \nAlgorithm: State-Machine Matching \nInput: {SM} ∈ BOM State-Machines, {Actions} \nOutput: TRUE, FALSE \n1  {SCXML} ←  TransformSMtoScXML(SM)   \n2 ⊳ Transform all BOM-Statemachines in SCxml format ⊲ \n3  \n4 Create and Initialize EventController: EC \n5 ⊳ Event Controller controls sending and receiving of events ⊲ \n6  \n7 for scxml ∈ { SCXML } do \n8 \n   SC  ← Parse(scxml)  ⊳ Parse scxml document  \n9 \n    Create and Initialize  SCXMLExecutor(SC)  \n10 \n⊳Instantiate SCXMLExecutor thread for each state-machine ⊲ \n11  \n12 \nDone ← FALSE \n13 \nwhile  (Done =FALSE) do \n14 \nCurrentState ← GetCurrentState() ⊳ SCXMLExecutor returns current state \n15 \nif CurrentState.IsFinal = TRUE then \n16 \nDone ← TRUE \n17 \nend if \n18 \n⊳Get Next Action to send or receive ⊲ \n19 \n{NextActions}← CurrentState.GetActions() \n20 \nfor next ∈ NextActions  do \n21 \nif next.Type = “Send” then \n22 \nEC.Put(next) ⊳ Simulate sending of next action \n23 \nSCXMLExecutor.Trigger(next) ⊳Transit from the current state to next state  \n24 \nelse \n25 \nEC.Get(next) ⊳ Simulate recieving of next action \n26 \nSCXMLExecutor.Trigger(next) ⊳Transit from the current state to next state \n27 \nend if \n28 \nend for \n29 \nend while ⊳Due to either of the send or receive actions the state-machine will  \n30 \ntransit to the next state and therefore the current state will be updated.  \n31 \nIf the final state is reached then the state-machine matching will be  \n32 \nterminated successfully⊲ \n33 end for \nTable 12: State-machine Matching algorithm \n \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n92 \n \nFigure 34 shows the state-machine matching process. It takes BOM state-machines \nas modeling objects, automatically transforms it into a SCXML executable format \nand perform state-machine matching using abstract level execution environment.  \nA successful run of this routine implies that all the state-machines match each other, \nwhich satisfies a necessary (but not sufficient) condition of BOM composability i.e. \nconstraint S3a of the requirement specification. The fulfillment of S3a certifies \nconsistency and completeness of the behavioral design of the composed \ncomponents. Consistency is due to the fact that the components are in correct causal \norder and Completeness, because their inputs and outputs (send and receive-events) \nare complete to reach their final states. However we still cannot guarantee \ncorrectness the 3rd C of requirements, unless the composition satisfies its requirement \nspecification i.e., all the assigned objectives and required constraints. Also the state-\nmachine matching approach may result in reaching final-states but it does not \nexplore all possibilities of the behavioral interaction of the composed components. \nSo it is required to analyze the model at a greater depth using an appropriate dynamic \nanalysis approach. \n \nFigure 34: State-machine Matching Process \nTherefore for deeper evaluation we propose to utilize the modeling and analytical \nstrength of Petri Net and CSP formalism and incorporate three analysis approaches \nin our verification framework as introduced and discussed chapter 3. The selection of \na suitable approach for the composability verification at dynamic-semantic level \ndepends on the nature of the model. In the following subsections, each of these \napproaches is discussed in detail. \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n93 \n \n5.7 PN Algebraic Technique \nThe basic idea of this technique is to transform BOM into Petri Net format and \nverify the properties given in the requirement specifications using algebraic methods. \nIn the verification framework, following steps are proposed to conduct algebraic \nanalysis: \n5.7.1 BOM to PNML Transformation \nIn the first step, BOM components are transformed into Petri Net Markup Language \nPNML format [115] which is an XML based form to specify Place/Transition Nets. \nAt first BOM state-machines of all components are parsed and each state is \ntransformed as a Place in the PN model. Similarly each event (send or receive event) \nis transformed into a Transition in PN with no duplication. An outgoing arc is \nconnected from a place-P to a transition-t if the corresponding state-S (of the sender) \nhas a corresponding event-t as its exit condition and next state S′. An incoming arc is \nconnected from transition-t to another place-P′ which represents the next state S′. \nSimilarly state-R (of the receiver) is transformed into place-Q and the next state R′ \ninto Q′. The incoming and outgoing arcs are connected to t. The sender and receiver \nentities (of BOM) are represented as tokens in the places. Figure 35 shows how part \nof a sender and receiver state-machine is transformed into a PN. The place P and Q \nhave tokens showing the current state (or marking) of the composed model. When \ntransition t is fired (meaning event t is sent by P and received by Q) the tokens are \ntransported to P′ and Q′ showing the next marking of composed model. \n \nFigure 35: BOM to PN transformation \nThe transformation process is complete, when all the states and events of every state-\nmachine in BOM are plotted in the PN model such that no element is duplicated, \nand each place or transition is connected so that there are no broken links.  \n5.7.2 PN Algebraic computations \nIn this step the PN incidence matrix and Place/Transition invariants are calculated. \nTo perform this step we use Platform Independent Petri Net Editor (PIPE) API \n[116]. PIPE is a java based open source API for performing different Petri Net \nrelated operations. It offers API functions to automatically compute algebraic \nresources of a PN model such as Incidence matrix and Place/Transition invariants.  \nIncidence Matrix \nAn incidence matrix of a PN model is calculated by subtracting A- from A+ \nincidence matrices: \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n94 \n \nAlgorithm: Incidence Matrix Calculation \nInput: PN Model (P-places × T-transitions) \nOutput: m × n Matrix A \n1 Initialize a Matrix Aminus of size m × n such that m=|P| and n=|T| \n2 for i=0 to m do \n3 \nfor j=0 to n do \n4 \nif pi ∈ P is connected to tj ∈ T  then   ⊳ i.e., p is the input place of t \n5 \nA[i][j] ← arc weight  ⊳ arc weight is always ≥ 1 \n6 \nelse \n7 \nA[i][j] ← 0 \n8 \nend if \n9 \nend for \n10 end for \n11  \n12 Initialize a Matrix Aplus of size m × n such that m=|P| and n=|T| \n13 for i=0 to m do \n14 \nfor j=0 to n do \n15 \nif tj ∈ T is connected to pi ∈ P  then ⊳ i.e., p is the output place of t \n16 \nA[i][j] ← arc weight  ⊳ arc weight is always ≥ 1 \n17 \nElse \n18 \nA[i][j] ← 0 \n19 \nend if \n20 \nend for \n21 end for \n22  \n23 Initialize a Matrix A of size m × n \n24 for i=0 to m do \n25 \nfor j=0 to n do \n26 \nA[i][j] ←  Aplus[i][j] - Aminus[i][j]  \n27 \nend for \n28 end for \n29 Return A \nTable 13: Incidence Matrix Calculation \nLines 10 calculate the A- matrix. Lines 12-21 calculate A+ matrix and lines 23-28 \ncalculate the final incidence matrix. \n \nPlace and Transition Invariants \nThe methods for calculating P-Invariants and T-Invariants of a PN model have been \nextensively studied. The basic principle to compute the fundamental set of P-\ninvariants and T-Invariants is based on Farkas Method [117]. The algorithm for \nfinding P-Invariant is presented as follows. The input of the procedure is the \nIncidence Matrix A and an Identity matrix B, both of size m × n. The output is a \nmatrix C whose rows are the fundamental set of P-Invariants.  \n \n \n \n \n \n \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n95 \n \nAlgorithm: P-Invariant Calculation \nInput: Incidence Matrix A, Identity Matrix B \nOutput: Matrix C (rows of C = P-Invariants) \n1 C ← A | B                 ⊳ Augmentation of A with m × n identity matrix B  \n2 for i=1 to n  do      ⊳ n = |T| \n3 \nfor each pair of rows c1, c2 in C[i-1] where c1[i]  and c2[i]  have the opposite signs  do \n4 \nc ← |c2[i]|. c1 + |c1[i]|. c2 \n5 \nc´ ← c/g.c.d of each element of row c   ⊳ g.c.d =Greatest common divisor \n6 \naugment matrix C[i-1]with row c´ \n7 \nend for \n8 \nDelete all rows of C[i-1] whose ith component is non-zero, the result is C \n9 end for \n10 Return C \nTable 14: Place-Invariants \n \nThe same procedure is used to find T-invariants by taking the transpose of the \nIncidence Matrix A. Details and a discussion about the improvement of this \nalgorithm are presented in [118]. These algorithms are implemented in PIPE API and \ncan be used in form of function calls. \n \n5.7.3 Property Verification Method \nThe outcome of algebraic analysis technique is the satisfaction or violating of a \nproperty with respect to a PN model. There are different methods to perform \nproperty verification however there is usually certain theorems behind the reasoning \nof necessary and sufficient conditions for the fulfillment of a property. In Petri Net \nliterature many solutions (proofs) for the property proving theorems are contributed \nand can be applied to prove different properties when required. Using these \ntheorems and the available algebraic resources a property verification method \n(algorithm) is developed which evaluates the conditions given in the theorem on the \nPN model and results in satisfaction or violation of the required property. Figure 36 \npresents the mechanism of algebraic verification technique in the verification \nframework: \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n96 \n \n \nFigure 36: PN Algebraic Technique \n \nTo explain our approach we present the theorems and an example property \nverification method for the analysis of fairness property in a PN model in chapter 7. \nPNML Execution and State-space Graph \nIt should be noted that PIPE library also offers an execution environment which can \nbe used to run the transformed PNML model. If the tokens (each representing a \nBOM entity) eventually reaches its final state (place) then the execution is successful. \nThis asserts that the model is correctly transformed and it correctly represents the \nbehavior of its source i.e. the conceptual model. PIPE library also offers a function \nto generate and visualize state-space graph of the PNML model. This can be useful \nto find deadlocks and verify other system properties through graph reachability.  \n5.8 CPN based State-Space Analysis Technique \nThe second approach proposed for the dynamic semantic composability verification \nis based on Colored Petri Nets and State-space analysis technique. This approach \neffectively utilizes the potential of Colored Petri Net formalism, CPN modeling and \nprogramming language, its execution environment and supporting tools in order to \nverify a composed model at dynamic-semantic level with respect to the requirement \nspecifications. The unique feature of this approach is its data-centric nature. As \ndiscussed in section 3.1.5 CPN supports level-3 PN modeling where tokens are \nstructured and can represent data objects. Also the transitions cover greater details of \nthe system behavior. Therefore the structure and the behavior of the system can be \nmodeled with greater details. In order to exploit the data-centric nature of our \napproach we proposed the following stages: \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n97 \n \n5.8.1 BOM Extension  \nThe current BOM standard lacks certain structural and behavioral semantics which \nare essential for modeling complex system behavior therefore we require \nspecification of additional modalities that can help in capturing the structure and \nbehavior of a system at a greater detail [119]. We therefore propose to extend the \nBOM conceptual model specification by applying the concept of Extended Finite State-\nMachines (EFSM), which is introduced and discussed with detail in [120]. An \nExtended Finite State Machine (EFSM) is defined by the tuple: \nM = (Q, I, Σ1, Σ2, V, Λ) where: \nQ (≠∅) is a finite set of states.  \nI ⊂ Q is the set of initial states \nΣ1 is a finite set of (send or receive) events. \nΣ2 is a finite set of actions (Actions are the instructions to be executed and should \nnot be confused with the BOM actions, which are used in pattern of interplay). \nV is the set of state variables.  \nΛ is a set of transitions; each transition λ ∈ Λ \n \nWhere \nq and q′ ∈ Q \ne ∈ Σ1 is an event \ng is a condition (or guard) \na ∈ Σ2 is an action.  \n \nIt means if the system is at a state q, an event e occurs, and the guard g is satisfied, \nthen action a will be executed and the system will transit to the next state q′. During \nthe firing of transition λ ∈ Λ the variables {vin} are used as input and the variables \n{vout} are used as output. \nExample: \nThis example is a modified version of an extended finite state-machine of a queue \ndiscussed in [120] and is intended to explain the notions of EFSM. A queue \ncomponent is either empty or nonempty, and in which insertions are done at the rear of \nthe queue and deletions are done at the front of the queue. Also the queue has a \nmaximum size. Two events put and get are used to update the states of the queue. \n \nFigure 37: Buffer Extended finite state-machine [120] \n \ne [g] / a \nλ = q    \nq′ \n{vin} | {vout} \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n98 \n \nThe EFSM model of the buffer is: M = (Q, I, Σ1, Σ2, V, Λ) where \nQ= {empty, nonempty} \nΣ = {put(string obj), get} \nq0: empty \nV = {front, rear, M, Data} \nΛ: Transition Specifications: \n \nTransition 1 allows Queue to transit from empty state to non-empty when put \nevent is received. During this transition the variable rear is incremented. Also \nthe parameter “Obj” of Put event is stored in Data at the rear location. \n \n \n \nTransition 2 lets Queue to revisit non-empty state when put event is received if \nrear is less than the maximum size. During this transition the variable rear is \nincremented. Also the parameter “Obj” of Put event is stored in Data at the \nrear location.    \n \n \nTransition 3 lets Queue to revisit non-empty state. It is fired if rear variable is \ngreater or equal to front+1 and less than the maximum size. It will send Get \nevent with data at the front location is sent as parameter. During this transition \nthe variable front is incremented.  \n \n \n \nTransition 4 allows Queue to return back to empty state when if front+1 \nreaches the maximum size. It will send Get event with Data at front location. \nDuring this transition both front and rear variables are reset to zero. \n \nWe apply the concept of EFSM to the BOM conceptual model, so that we can \nintroduce state-variables and extended representation for transitions (events, guards, \nactions), to a form, which we name: Extended BOM or E-BOM. There are several \nadvantages in the BOM extension:  \nThe usage of variables (or state-variables) in BOM state-machines allows to model \nthe attributes of a component (structure) and their effects caused due to the change \nof states and occurrence of transitions (behavior). And values of these attributes can \nPut(obj) [ ] / action{ rear++; Data[rear]=obj;} \n1: empty \nnonempty \n{rear} | {rear, Data} \nPut(obj) [rear<M]/ action{ rear++; Data[rear]=obj;} \n2: nonempty \n \nnonempty \n{M, rear} | {rear, Data} \nGet(Data[front]) [rear≥front+1∧rear <M]/ action{ front++} \n3: nonempty \n \nnonempty \n{M, front, rear, Data } | { front } \nGet(Data[front])  [front+1=M]/ action{rear=0; front=0} \n4: nonempty \n \nempty \n{M, front, Data} | { rear, front } \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage \n99 \n \nbe used in the arithmetic or logical evaluation to assess trigger conditions of the \ntransitions and to transfer variable values from one model to another model, thus in \na composition, a value output by one component can be consumed by the other in \nthe composition. Similarly by introducing actions we can manipulate state-variables \nof a component during an occurrence of a transition and thus we can model realistic \nand complex behavior of a system which cannot be obtained through simple labeled \ntransitions (such as events/actions in standard BOM). \nIn the process of BOM to E-BOM extension some of the features are imported \nfrom the existing BOM while others are provided by the modelers. Figure 38 provides \na comparison of which parts can be imported from BOM and how they are used in \nE-BOM and which parts are externally provided by the modelers. A user input utility \nis designed for the modeler’s input where the modeler can choose initial states, select \nstate-variables from the existing BOM entity characteristic or add new variables. \nAssign data-types to these variables. Design transitions using events from existing \nBOM and adding guards (conditional statements) and actions. Currently we allow the \nmodelers to use SML language for scripting guards and actions. This is because the \nextended BOM will later be used in CPN environment which uses SML for code \nscripts. When each BOM component is extended to the respective E-BOM we \nproceed to the next stage. \n \nFigure 38: BOM and E-BOM comparison \n5.8.2 E-BOM to CPN Component Transformation \nWe propose a three layered Component Model based on CPN constructs 32 and \ntransform previously developed E-BOM into this component model using Meta-\nModel transformation rules (defined later in this section). Our proposed CPN \ncomponent Model is defined as: \nCPN-CM = 〈SL, BL, CL〉 where: \nSL = Structural layer \nBL = Behavioral layer \nCL = Communication layer \n                                                 \n32  To understand this mechanism background knowledge of CPN formalism and CPN tools \nenvironment is required. We recommend to consult the online documentation [82] \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 100 \n \nStructural layer \nThe structural layer represents physical attributes or properties of the components. \nThe state-variables defined in E-BOM are transformed into the structural layer. In \nCPN-CM these State-Variables are defined in form of CPN-places with distinct data-\ntypes represented by colors. They are manipulated by the firing of connected \ntransitions during the execution of the CPN model. The state-variables are connected \nto the transitions if they are defined in their input or output sets (as discussed in \nEFSM). The values of the state-variables are specified using CPN-tokens. The data-\ntypes of the tokens are the same as that of their respective state-variable places. The \nformal definition of the structural layer is: \nSL = 〈SV, ΣSV, CSV, VSV, ISV〉 where \n• SV is a set of places (that represent state-variables of the Conceptual Model)  \n• ΣSV represents a set of color sets that represent state-variable data types  \n• CSV: SV→ΣSV is a function that assigns a color set to each state-variable  \n• VSV is a finite set of typed variables33, assigned to each member of SV.  \n• ISV: SV → EXPR is an initialization function that assigns an initial value \nexpression of type Type [SV] to each state-variable.  \nBehavioral Layer  \nThe Behavioral Layer represents state-machine of an E-BOM component in form of \nCP-Net, where each state from E-BOM component becomes a CPN place of integer \ntype, each E-BOM transition becomes a CPN transition, and the flow integer type \ntoken(s), represent the change of component’s state. The state-variables of the \nstructural layer are connected to the transitions of this layer, if they are specified as \ninputs or outputs in the E-BOM transition. The guards and actions from E-BOM are \ntransformed into CPN transition inscriptions. They should satisfy CPN-ML scripting \nsyntax otherwise the CPN-ML complier will give errors. Formally the Behavioral \nlayer is: \nBL = 〈S, T, AT, ASV, AC, G, Act〉 where \n• S is a set of places, called states. Each place is assigned a type INT. This set \nrepresents the states of E-BOM state-machine \n• T is a set of transitions (representing E-BOM transitions)  \n• AT is the set of arcs, called transiting-arcs. Each transiting-arc connects a state s \n∈ S to a transition t ∈ T, and t to another state s′ ∈ S, if s → t → s′. Each arc \nin AT is assigned a variable v of type INT. When a transition t is fired a token is \ntransferred from state-s to state-s′ to reflect a change of state in the state-\nmachine. \n• ASV is another set of arcs, called “sv-arcs” that connects state-variables (of the \nstructural layer) to the transitions. If a state-variable SV is assigned as an input \nvariable in the E-BOM transition, then it is connected to the transition with an \nincoming arc. If a state-variable is assigned as an output variable in E-BOM \ntransition, then the transition is connected to the state-variable with an \noutgoing arc. A variable v is assigned on each sv-arc whose type is same as that \nof the attached state-variable. \n                                                 \n33 Note that these variables are built-in CPN variables that are used to define arc bindings for \ntransporting tokens and are different from our notions of state-variables \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 101 \n \n• AC is another set of arcs, called “communication-arcs”, that connects \ncommunication ports (CP) of the communication layer to the transitions. A \ncp∈CP is connected to a transition t through a communication-arc if it \ncorresponds to a receiving event (as specified in E-BOM). Whereas a transition \nt is connected to a cp∈CP through an arc if it corresponds to a sending event. \nA variable v is assigned to each communication-arc whose type is same as that \nof the attached CP. \n• G: T→EXPR is a guard function that assigns a guard to a transition t (as \nspecified in E-BOM) such that Type[G(t)] = Bool  \n• Act: T→EXPR is an action and assigned to a transition t (as specified in E-\nBOM). The action can be a single executable statement or a complete \nprocedure. Usually the actions are used to read data from attached input state-\nvariables or communication ports; to process the data and to produce an \noutput to the attached state-variables or communication-ports. Actions can \nalso be used to type-cast the data. \nCommunication Layer  \nThe Communication Layer is responsible for communication with the other \ncomponents. It provides interface for connecting the inputs and outputs of the \ncomponents through “port places” and also provides information about the type of \ndata exchange i.e., the tokens of complex data-types that carry message parameters \ncontents, as described in E-BOM event types. The transitions of behavioral layer are \nconnected to the port places of this layer. The arc direction depends on the type of \nthe transition (send or receive). When a message (E-BOM event) is received at the \nIn-port of the communication layer it causes the transition in the behavioral layer to \nfire as a result of which the state-machine (in the behavioral layer) progresses. This \nmay also happen when a transition causes an outgoing event at the Out port which is \nexpected by another component. Formally the Communication layer CL is: \n〈CP, ΣCP, CCP, VCP, PT〉 \n• CP is a set of port-places called “communication-ports”. A communication-port \nconnects the inputs/outputs of the component to other components (see section \n3.1.5). Each CP corresponds to an event (send or receive). \n• ΣCP represents a set of color sets that represents communication ports data types \n(as specified by the corresponding event parameters of the E-BOM). Each color \nset is constructed by combining the data-types of all the parameters. If there are \nno parameters then NULL type is assigned, which carries a blank token. \n• CCP: CP→ΣCP is a function that assigns a color set to each port-place.  \n• VCP is a finite set of typed variables, assigned to each CP with Type[v]∈ \nType[CP].  \n• PT: CP→ {In, Out, I/O} is a port type function that assigns a port type to each \nport place (as discussed in section 3.1.5).  \n \n \n \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 102 \n \nE-BOM to CPN-CM Transformation Rules \nThe rules of transformation from E-BOM to CPN-ML are summarized as follows: \n \n \nE-BOM \n→ \nCPN Component Model \n \n \n \nStructural Layer \nBehavioral Layer \nComm. Layer \n1 \nStates \n→  \nState-Places of Type[int] \n \n2 \nInitial-States \n→  \nAdd token type[int], Value \n= {0, 1…n} for each initial-\nstate \n \n3 \nState-Variables (SV) \n→ \nSV-Places of (type color-sets \ndefined in 4), Add tokens if SV \nneed initialization \n \n \n4 \nSV Data Types \n→ Define Color-Sets of SV-Places \n \n \n5 \nEvents \n→  \n \nAdd Communicating-Place \nfor each Event of type color-\nset defined in 6 \n6 \nEvent Parameters and \ntheir types \n→  \n \nAdd a color-set of type same \nas event parameters. If there \nare multiple parameters then \nadd color-set of type product, \nwhere each event parameter \ntype is added in the product. \n(If there is no parameter, use \nNull type) \n7 \nBOM-Action \n→  \nTransitions  \n \n8 Exit-Condition (Action, \nNext-State) \n→  \nConnect a transiting arc \nfrom a state-place to \ntransition and from \ntransition to next state-\nplace, Declare v variable of \ntype[int] on each arc. \n \n9 \nGuards of Transitions \n→  \nAdd guard on each \ntransition \n \n10 Actions of Transitions \n→  \nAdd action script on each \ntransition \n \n11 \n{Input-Variables} of \nTransitions  \n→ \nConnect SV-arc from each SV to \nthe transition, if it is added as \ninput variable in that transition. \nDeclare sv of Type SV on the \narc. \n \n \n12 {Output-Variables} of \nTransitions \n→ \nConnect SV-arc to each SV from \na transition if it is added as \noutput variable in that transition. \nDeclare sv of Type[SV] on the \narc.  \n \n \n13 \nReceive-Events ∈ \nEvents \n→  \n \nConnect an arc from CP to \nthe transition if it has a \nreceive event. Declare cp \nvariable of type[CP] on the \narc. Mark the CP as in-port. \n14 Send-Events ∈ Events \n→  \n \nConnect an arc from \ntransition to the CP if it has a \nsend event. Declare cp \nvariable of type[CP] on the \narc, Mark the CP as Out port \nTable 15: Transformation Rules \n \n \n \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 103 \n \nExample \nLet us consider the E-BOM of the Queue example discussed in the previous section: \n \nBased on the above E-BOM the transformation will result in the CPN-CM model \nshown in Figure 39. This model is completely executable in CPN environment and \ncan be composed with other components through communication layer. \n \nFigure 39: CPN-CM represention of Queue component \n \nThe red area represents structural layer and consists of state-variables. The green area \nrepresents behavioral layer and shows the state-machine. Blue area represents \ncommunication layer and shows the Communication Ports (CPs). When some data is \nreceived at CP-Put, the value of rear and Max variables are read as input. Guard \nrear<M is satisfied. A token is taken from the empty state. Action is executed that \nE-BOM: Queue Component \n States \n{empty, nonempty} \n  Initial States \n{empty} \n  State Variables \n& \n  their Data types \n{front : integer, rear : integer, Max : integer, data: string[ ]} \nEvents \n{Put (obj:String), Get(obj:String)} \nTransitions \n \nPut \nPut \nGet \nGet \n   Current State \nempty \nnonempty \nnonempty \nempty \n   Event \nPut(obj) \nPut(obj) \nGet(null) \nGet(null) \n   Guard \n[ ] \n[rear<M ] \n[ rear≥front+1∧rear <M ] [ front+1=M  ] \n   Action \nrear++ \nrear++ \nfront++ \nrear=0; front=0 \n   Input Variables rear \nrear, M \nrear, front, M, data[front] \nrear,  front, data[front] \n   Output Variables rear, data(obj) rear, data(obj) front \nrear, front \n   Next State \nnonempty \nnonempty \nnonempty \nempty \n \n \n \n \n \nStructural Layer \nBehavioral Layer \nComm. Layer \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 104 \n \nincrements the rear variable then the transition Put is finally fired. After which a \ntoken is produced at the nonempty state showing the state-transition. Also rear and \ndata variables are updated. Max variable retains the token (due to bi-directional arc). \nIf Put is fired again it will repeat the same process. If Get is fired provided the guard \nis satisfied, then front, Max and data variables are read as input. The data (picked \nfrom the front of the queue) will be sent to the out-CP. When the data is emptied the \ntoken will be sent to the empty state. \n \nAutomated Transformation Tool \nIn order to automate the E-BOM to CPN transformation process, we develop a \ntransformation utility, which takes an E-BOM component as input and produces \nCPN- code for all three layers of CPN component model automatically. The code \nfollows CPN-XML specifications. For each E-BOM component, a separate CPN \nsub-page is generated (programmatically) and the necessary CPN elements (places, \ntransitions, arcs, color sets, variable declarations, initial markings multi-sets, guards, \nactions, code segments, CPN ML functions, ports, ports-tag) are generated in one \nCPN output file, which can be loaded in CPN tools. Once all the CPN models of the \nBOM composition are generated, the modeler creates a main model and “manually” \ncombines the generated CPN-CM modules (using CPN hierarchical features). The \noutput of this step is a composed CPN model. The modeler is also required to \ninitialize each component with data (in form of token assignments i.e. the initial \nvalues of the tokens of state-variables and initial states of the state-machine).  \nS3b Evaluation \nThe S3b constraint in the requirement specification requires that “If the conceptual \nmodel is transformed into an executable model, the latter should correctly represent \nthe structure and behavior of the former” (see Table 10). Therefore we have to \ncompare each CPN component (executable model) with its respective BOM \n(Conceptual Model) to check that its structure and behavior is preserved after the \ntransformation. To show that S3b holds after the transformation we rely on the \nfollowing assertions: \n1. As BOM is extended to E-BOM hence BOM ⊂ E-BOM. Any information \nadded by the modeler in E-BOM cannot cause loss of structural information \nof BOM. Therefore E-BOM structurally preserves BOM.  \n2. To check that the generated CPN component contains all the Events and \ntheir parameters, States and their exit-conditions, Actions and their \nsenders/receivers we need at least one transformation rule that is responsible \nto transform these elements: \na. Rules 6 & 7 (see Table 15) are responsible for transforming Events \nand their parameters into CPN component.  \nb. Rules 1 & 8 are responsible for transforming states and their exit-\nconditions. \nc. Rule 7 is responsible for specification of BOM-actions as transitions \nin CPN model. Also rule 5 defines port-places which are used to \nconnect senders or receivers. \nExistence of Rule 1, 6, 7 & 8 confirm that the structure of corresponding \nBOM is preserved in the transformation. \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 105 \n \n3. CPN Tools provide a built-in compiler for the compilation of CPN models \nand report if there is any syntax error in the model. The absence of error \nconfirms that the transformed model is structurally consistent and \nbehaviorally functional.  \nFor the behavioral bi-similarity we propose an inspection technique. At first we \nevaluate that all the generated components possess the same behavior as defined in \nthe conceptual model. So we test the functional output of each CPN model by giving \nthe needed inputs. If by giving correct inputs, the model produces desired output \nthen its functional behavior is correct. To perform functional testing, the modeler \ninitializes all the IN-type communicating-ports (CPs) with tokens of required \nparameters. (See Figure 39 for an example where IN-CP “Put” is initialized with \ntokens of type String). Then the model is executed. If the model produces desired \noutput on the corresponding Out-CPs (In Figure 39 the desired output should be a \ntoken of type string retrieved at Get Out-CP), then the functional test is successful. \nThe modeler performs functional test on all generated CPN components.  \nIn the second step, when all CPN components are composed (i.e. the socket-places \nof the main model are connected to the Communicating-Port places of the CPN \ncomponents then the modeler is required to inspect that CPN components are \nconnect exactly according to the Pattern of Interplay of the BOM composition. Also \nwhen the composed model is executed the sequence of sending and receiving events \nfrom one component to another (which can be observed at the main model by \nseeing the movement of tokens) follows the pattern of interplay. If the execution is \naccording to the pattern of interplay and the components make progress until they \nreach their final states, then we say that the behavior of the transformed model is bi-\nsimilar to the conceptual model. This confirms the satisfaction of S3b constraint. \nThe execution can be automated or interactive. In automated mode the choices \nbetween multiple progressive paths are randomly picked whereas in interactive model \nthe modelers can pick a path of his choice. Using this option the modeler can probe \npaths that can lead to a successful execution scenario. During the execution CPN \ntool also offers Data Collection Monitors for recording the data values, which are very \nvaluable for collecting statistics and results of the execution.  \n5.8.3 Verification of the composed CPN model \nIn the next step, the state-space analysis is performed. At first the state-space of the \ncomposed CPN model is generated using CPN state space calculation tool. As \ndiscussed in section 3.1.5 a state-space is a graph of nodes (of system-states or \nmarkings) and arcs (transitions). When the state-space is generated, different query \nfunctions can be used to explore the state space graph for various verification \nquestions. A query function is like an algorithm that explores the state-space graph. \nThese algorithms are based on theoretical concepts of Petri Nets state-space analysis \nand are used to verify PN properties. Therefore we translate a system property given \nin the requirement specification into a suitable PN property. There have been a lot of \ncontributions in the PN literature in specifying PN properties and methods of \nreasoning of their satisfiability or violation. In CPN state-space analysis, the existing \nmethods can be utilized in developing query functions for their respective PN \nproperties. \nCPN tools provide some built-in-functions for the common query tasks. We also \npropose a library of additional functions to perform queries specific to our \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 106 \n \ncomposability verification framework and the requirement specifications. Figure 40 \nillustrates the state-space analysis of a composed CPN model using a query function.  \nWe divide these query functions into two categories: \n \n(i) General System Properties  \nThis category includes commonly known system properties such as freedom of deadlock, \nlive lock, starvation, or existence of boundedness, mutual exclusion, fairness, sequentiality, time-\nsynchronization etc. if any of these or similar system properties are included as a \nconstraint in the requirement specification then it is translated in CPN terms and a \nsuitable query function is selected from the Function library to perform verification \nusing state-space of the composed CPN model. \n \nFigure 40: CPN State-space analysis  \nFor instance a deadlock freedom property can be translated into CPN terms as:  \n“An absence of a marking with no outgoing arcs in the entire state-space graph” \nSo essentially we need to find such a node in the state-space graph that violates \nabove condition. If no such node is found then the model is said to be deadlock free. \nA library function ListDeadMarking() returns a set of all those markings (if \nany) which have no outgoing arcs. If the result of this query is an empty list, then we \nassert that the model is deadlock free. Similarly there are other library functions that \ndeal with the evaluation of other system properties. \n(ii) Scenario Specific Properties  \nThese properties are specific to the scenario (of the real system) under which the \nmodel is built. The objectives or goals from the requirement specification are usually \ntranslated in form of scenario specific properties. In CPN terms a typical goal or \nobjective can be translated as a certain desirable marking, where the values of state-\nvariables in structural layer evaluate to a particular criteria or reaching of particular \nstate(s) in behavioral layer is desired or certain data at the output port(s) of the \ncommunication layer is looked-for. A goal or objective can be expressed in a \ncombination of all these possibilities too.  \nScenario specific properties may also include certain safety or liveness assumptions, \nwhich represent certain desirable (or un-desirable) situations that must (or must not) \noccur in order to satisfy (or violate) the requirements. These properties are mostly \nthe CPN translations of the constraints defined in the requirement specifications. \nConceptual \nModel \nRequirement \nSpecification \n \nSystem Property \n \nSatisfie\n \nViolate\n \nState Space \nSystem property   \nCPN Translation \nComposed CPN \nModel \nQuery function \n(Algorithm) \nFunction \nLibrary \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 107 \n \nUnlike general system properties, verifying scenario specific properties is not a \nstandard operation, and depends on the way they are defined. Most commonly, we \nmake use of our proposed library functions: IsEqual(), IsNotequal(), \nIsBetween(), IsUpperBound() or IsLowerBound() to construct a \n“predicate”, that serves as a condition evaluation criteria. Then we use \nSearchNode(predicate)function to find those nodes, which satisfies the \npredicate. If one or more nodes are found, then it is verified that the goal is reachable. \nIn cases, where it is important to know how a sequence of the occurrence of \ntransitions, leads to a particular situation when a property is satisfied (e.g., how an \nobjective or goal is reached) we use SearchArc()function with the predicate. \nThis tells us the path in the graph that leads to fulfillment of a property. We also \ndevelop an export function, that creates a .DOT file of the entire state-space and can \nbe viewed in graph tools such as GraphViz or Gephi, for visualization and \nperforming further tests on the graph such as finding certain paths/shortest \npaths/longest paths between two particular nodes. When, a CPN composed model \nsatisfies all the properties in the requirement specification, we say that it is verified at \ndynamic semantic composability level. In chapter 8 we discuss a Field Artillery \nScenario as an example of CPN state-space analysis to explain our approach. \nAn example of translating a scenario-specific property in CPN terms is a restaurant \nmodel where we assume that customers may leave the restaurant without paying the \nbill because they have been waiting for a long time for the waiter to bring bill. This \nact of the customers is known as “Balking” and is undesirable. Its translation in CPN \ncan be as follows: \n \n“There should be no arc with the name “balk” that leads to any marking in the graph” \n \nArcs are generated due to firing of the transitions. Existence of balk arc means \nsomewhere in the model an incidence occurred when a customer balked (by firing \nbalk transition). So essentially we need to find that such arc is absent in the state-\nspace graph. This can be done by using SearchArc()function. Note that this is a \nsimple example. There could be cases in which a sequence of transitions (called \ntraces) or cycles are searched to verify a property. Error! Reference source not \nfound. describes the overall process of state-space analysis technique.  \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 108 \n \n \nFigure 41: State-space Analysis Technique \n \n \nState-Space Reduction Technique \nIn order to alleviate the well-known problem of state-space explosion we propose a \nreduction technique called “compositional state space”. The main idea of this technique is that in \na hierarchical composition of CPN model, we propose to only consider the places in the \nmain model and treat all the composed components as black boxes. The inputs and outputs \nof each component can be observed using the flow of Tokens and the data they carry. \nTherefore in the state-space graph we only keep the markings in which any token is present \nin the Main model (i.e., any of the place in the main model has at least one token) and delete \nall other nodes in the state-space graph using the algorithm presented in Table 16. The \nresultant graph will be a reduced form of the actual graph and only considers those markings \nthat reflect a compositional state-space. It is called compositional state-space because it only \nrepresents a part of the actual state-space which is the result of interactions due to the \ncomposition of components. In our experience this subset state-space of the whole state-\nspace is sufficient to evaluate whether the objectives, goals and the constraints are satisfied \nor not.  \n \n \n \n \n \n \n \n \n \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 109 \n \nAlgorithm: Compositional State-Space Generation \nInput: Original State-Space Graph G  \n|              Output: Reduced State-Space Graph G \n1 {Vertices}  ← Get-Vertices(G) ⊳ Retrieve all the nodes of the graph in a collection \n2 for v∈to {Vertices} do \n3 \nIf  False ← Is-Filtered(v) then \n4 \nG ← Remove-Vertex(G, v) \n5       Else \n6 \nnext \n7 \nend if \n10 end for \n11 Return G ⊳ Reduced state-space \n12  \n13 Procedure Remove-Vertex(Graph G, Vertex v) \n14 {Predecessors} ← Get-Predecessors(G, v) ⊳ Retrieve all the predecessor vertices of v in G \n15 {Successors} ← Get- Successors (G, v) ⊳ Retrieve all the successors vertices of v in G \n16 for p∈to { Predecessors } do \n17 \nfor s∈to { Successors } do \n18 \nG ← Add-Edge(p, s, “DIRECTED”) ⊳ Add a directed arc from each predecessor  \n19 \n                                                                            to each successor ⊲ \n20 \nG ← Delete-Vertex(v) \n21 \nend for \n22 end for \n23 Return G \n24  \n25 Procedure Is-Filtered (Vertex v) \n26 \nIf {places}← GetData(v) then ⊳ Each vertex is a marking, which contains data of all  \n27              ⊳ the places of the model with their names and their token values ⊲ \n29            for p∈to { places } do \n30 \nif p is a main place and it is not empty then \n31 \nReturn  TRUE ⊳ A valid marking with a non-empty Main place is found \n32 \nelse \n33 \nnext  \n34 \nend if \n35 \nend for \n36 \n⊳ if the loop is complete then there is no main place which is not empty  \n37 Return FALSE \nTable 16: Compositional State-space generation algorithm \n \nUsing the Compositional state-space generation algorithm we can filter unnecessary \nnodes and reduce the size of the graph. The current limitation of this approach is \nthat we first need to construct the actual state-space which is a bottleneck if the \nmodel is too large. But this limitation is due to the fact that the process of CPN \nstate-space graph generation cannot be externally modified otherwise if the principle \nof our reduction technique is applied to the state-space generation algorithm it will \ndirectly generate the reduced graph. In chapter 8 we will present the results of our \nreduction technique by applying it to the Field Artillery example model. \n \n \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 110 \n \n5.9 CSP based Model Checking Technique \nThe third approach proposed for the dynamic semantic composability verification is \nbased on Model Checking, which is widely accepted as formal technique for software \nverification. In this approach we propose to use Communicating Sequential \nProcesses formalism as a model description language and Process Analysis Toolkit \n(PAT) as its execution and verification environment in order to verify a composed \nmodel at dynamic-semantic level with respect to the requirement specifications. The \nstrength of this approach is in its ability to answer a large variety of verification \nquestions due to the fact that the verification criteria can be specified using LTL, \nCTL or any of the temporal logic extensions. The Model Checking technique is \nbecoming more promising and acceptable by many software verification users since \nthere is an abundance of improved algorithms, efficient data-structures and faster \ntechniques which are constantly being contributed by the model checking \ncommunity in order to manage large models with complex modeling requirements. \nWe propose to integrate CSP based model checking verification approach in our \ncomposability verification framework. The following stages are proposed in order to \nperform composability verification using model checking approach. \n5.9.1 BOM Extension  \nThe E-BOM extension for CSP based Model Checking approach is also inspired \nfrom the concept of Extended Finite State-Machine as discussed in section 5.8.1. \nThe extended BOMs for CSP can also have state-variables but since CSP# \nspecification does not allow declaration of strings or higher data-types, the state-\nvariable definitions are restricted to integer and Boolean34, which in our experience \nare sufficient to model the behavior of BOM components using CSP (or otherwise it \nis required to narrow it down to a less detailed version of the component, where only \nthe necessary behavioral details are specified). The transitions of E-BOM contain \ncurrent-state, event (with parameters), guard, actions and next states. However in this \ncase the action scripts are written in CSP# specification language instead of CPN-\nML language. And instead of input and output variables, we have local variables \nwhich are accessible only to the component and global variables which are accessible \nto all the components of the composed model. Some additional information such as \ntime constraints and probability factors are further proposed to be included in the \nBOM extension so that the behavior of complex systems such as real-time systems \nand probabilistic systems can be modeled and verified.  \nSince Timed-CSPs support a number of timed behavioral patterns to capture \nquantitative timing requirements, such as delay, timeout, deadline, therefore we \nsuggest using these patterns as time functions in the BOM extension, which helps in \nthe automatic transforming of E-BOM into Timed-CSP components. These time \nfunctions are essentially assigned to the E-BOM transitions as explained in the \nfollowing table: \n \n                                                 \n34 Generally the high level or user defined data types are not permissible in most of the model \nchecking description languages due to the economy of state size, and to avoid risk to state-space \nexplosion. However if the use of such type is inevitable, the PAT tool do provide mechanisms of \nimporting classes from external libraries. If this is the case then the modeler is required to program the \ncomponents in PAT manually, instead of relying on our automatic transformation tool.  \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 111 \n \nTime Function \nUsage and Explanation \nWait[duration]  \nWait is assigned to model the delay in an activity. An enabled \ntransition waits for the given duration before it is fired.  \nTimeOut[duration, next] When a timeout function is assigned to a transition, it waits for an \nevent to occur. If the event occurs before timeout, it transits to the \nnext state described in the transition definition; otherwise it transits \nto the next state described in the timeout parameters.  \nDeadline[duration] \nA transition is constrained to fire when the deadline is reached. The \ndifference between Wait[] and Deadline[] function is that the \nformer makes the system inactive, i.e., it cannot do anything but \nwait, whereas when the latter is used the system is active and can \nrespond to events etc., until its deadline is reached. \nTable 17: Time functions in E-BOM \nUsing any of these time functions during the BOM extension is useful to capture the \nbehavior of the real-time systems. In order to further capture the behavior of the \ncomplex reactive systems, we also propose to introduce probabilistic factors in the \nBOM extension. These probabilistic factors can either be used to model the system \nbehavior in form of Markov Decision Processes (MDP) as discussed in section 3.2.5. \nOr the probability factors can be used to model random time delays, timeouts or \ndeadline, using a particular probability distribution function.  For modeling the MDP \nbehavior probability factors can be assigned to multiple transitions of a component’s \nstate using the following notation provided by the PAT tool: \nPcase { \n[P1]: Transition 1 \n[P2]: Transition 2 \n… \n[Pn]: Transition n \n} ; \nWhere  ∑\n𝑃𝑖\n𝑛\n𝑖=1\n= 1 \nFor randomizing time functions, we propose to assign the commonly used \nprobability distribution functions as parameters in the E-BOM: \nProbability Distribution \nFunctions \nUsage and Explanation \nNormal[mean, variance]  \nReturns a random value from a normal distribution with a given \nmean and variance. (Since PAT does not support higher types \nso we have confined these functions to use integers). \nDiscrete[a, b] \nReturns a random value from a discrete uniform distribution \nbetween a and b (a and b included), such that a < b \nExponential [1/lambda] \nReturns a random value from a an exponential distribution with \nparameter 1/lambda \nTable 18: Probability Distribution Functions \n \nIn order to implement these assignments we develop an external function library in \nC# which can be imported and used in PAT. A call to these functions generates a \nrandom number according to the specified probability distribution. Beside the time \nfunctions, these functions can also be used to generate random values for global or \nlocal variables, which can help in modeling different probabilistic system behaviors.  \nWhen each BOM component is extended to the respective E-BOM we proceed to \nthe next stage. \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 112 \n \n5.9.2 E-BOM to CSP# Transformation \nAt this stage, each E-BOM component is transformed into a CSP# process \ncomponent and composed into an executable system. The main idea of this \ntransformation is based on [121], which discusses the transformation of UML state \nmachines to CSP. We however extend this transformation with Communication \nchannels, Time-functions and probability factors to be able to use it for E-BOM \ntransformation. Table 19 shows the rules used in the transformation process: \nE-BOM \n \nCSP# Statement and Description \nStates \n→ \nState-Name() \nEach state in E-BOM is defined as a CSP process. \n \nFinal-State() = Skip; \nThis statement defines a final state in CSP where Skip is a reserved word means the process terminates \nsuccessfully. If no such statement exists in any component of the composed model then it is said to be a \nnon-terminating model. \nComponent \n→ \nComponent-Name = Initial-State() or Component-Name(i) = Initial-State(i) \nAn initial state is defined and assigned to the component. If a component has multiple instances it is \npassed a parameter ‘i’ which represents the instance number. \nTransitions \n→ \nSimple Transitions: \n \nState() = [guard] event !/? parameters {action} → NextState(); \n \nThe transitions are defined using the above format, where State() is the current state of a component. \n[guard] is a conditional statement. If it is true only then the transition will be enabled. Event is sent using \n‘!’ symbol or received using ‘?’ symbol through an event channel. For each event in an E-BOM \ncomponent, we define a channel as follows: \nchannel event-name 0; \nIn CSP# “0” means the buffer size of the communication channel is zero, which further means that it is a \nsynchronous channel. Parameters are the values that are passed during an event exchange and a separated \nusing ‘.’ Actions are scripts that should be executed when the transition is fired. Usually these actions are \nused to update local or global variables. NextState() is the new state which will be reached when the \ntransition is fired.  It must be defined within the CSP component body. \nTransitions with Time functions: \n \nFollowing statement represents a transition with timed-functions: \n \nState() = [guard] Wait[d];  event !/? parameters {action} → NextState(); \n \nState()=[guard] event!/?parameters {action} → NextState() deadline[d]; \n \nState()=[guard]event?parameters{action}→NextState() timeout[d] NextState2 (); \n \nNote that in case of timeout, the transitions should only be receiving an event.  \nMarkov Decision Process style Transitions: \nState() = pcase{ \n  [Prob1]: [guard] event !/? parameters {actionA} → NextStateA() \n \n  [Prob2]: [guard] event!/? parameters {actionB} → NextStateB() \n};  \n \nNote the postfixes A and B in action or next states of the transitions. Using this CSP# code style multiple \ntransitions can be modeled with different probabilities for either creating a variation of the action which is \nfired when one of these transitions is selected in a simulation run, or the next states (or both). \nTransitions with Probability distribution functions: \nFor using probability functions, at first it is required to import our external probability function library \nusing: \n \n#import \"PAT.Lib.ProbabilityDistributionFunctions\"; \n \n \nFollowing are some examples of how the function calls can be made: \nvar x = call(Normal, 10, 4);   \nAn integer variable is defined which will randomly be assigned a value using normal distribution with \nmean=10 and variance=4 \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 113 \n \n \nWait[call(Exponential, 1/4)];   \nDelay function with an exponential distribution, where the inter-arrival rate is ¼. \nState-\nvariables \n→ \nvar Variable-Name=Initial-Value; \nor  \n#define Constant initial-value; \n \nIn CSP# weakly typed variables are used which means that while declaring a variable, the type is not \nspecified. The global variables can be accessed by all components whereas the local variables can only be \naccessed by the component they belong.  \nComponent \n→ \nComponent-Name = Initial-State() or Component-Name(i) = Initial-State(i) \nAn initial state is defined and assigned to the component. If a component has multiple instances it is \npassed a parameter ‘i’ which represents the instance number. \nComposed \nModel \n→ \nComposed-Model = Component1 ||| Component2 ||| … ComponentN; \nThe composed model (name) is defined as a composition of CSP process components with an \ninterleaving operator ‘|||’ between each other. However if there are broadcast events (i.e., one event is \nsent to all components); or one to many; or many to one synchronization events are used then a parallel \noperator ‘||’ is used to compose CSP process components.  \nTable 19: E-BOM to CSP# transformation rules \nWe develop a transform tool that takes all the E-BOMs as input, and outputs a single \ncomposed model using CSP# description language. The generated CSP# composed \nmodel can be opened in PAT tool and compiled for checking errors. If no errors are \nfound then the transformed model is said to be structurally consistent and \nbehaviorally functional and it is ready for simulation and verification. It can also be \ndirectly compiled, executed and verified using command line operation. \nS3b Evaluation \nThe S3b constraint in the requirement specification requires that “If the conceptual \nmodel is transformed into an executable model, the later should correctly represent the structure and \nbehavior of the former” (see Table 10). In order to evaluate S3b, i.e., to check that the \nstructure and the behavior of the generated executable model (CSP composed \nmodel) correctly represents its conceptual model (BOM composition), we propose \nfollowing steps: \n1. For each CSP component, manually inspect that it contains all the states that exist \nin its corresponding BOM component \n2. Inspect that the exit condition(s) of each State in BOM correspond to a \ntransition(s) and a next state(s) in CSP. \n3. Execute the generated CSP model in PAT simulator and observer that all the \ncomponents reach their final states (or in case of a non-terminating model each \ncomponent re-visit its initial state iteratively). \nStep 1 & 2 confirms by inspection that the structure of the generated model correctly \nrepresents its conceptual mode whereas step 3 confirms that it behavior is bi-similar \nto the conceptual model and therefore satisfies S3b constraint. \n5.9.3 Verification of the composed CPN model \nAt this stage, the CSP composed model undergoes composability verification using \nPAT model checker. At first the requirement specification is translated into CSP# \nproperty (or assertion) description language. This language is based on a mix of \nclassical Linear Temporal Logic (LTL) and its different extensions such as Real-Time \nLTL and Probabilistic LTL and is used to construct assertions (verification \nquestions) of various types, such as reachability properties, safety properties, liveness \nproperties, deadlock freeness etc. We use the syntax of assertion specification \nlanguage of PAT to translate the objectives and constraints of given requirement \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 114 \n \nspecifications. Following are some generic examples of how to specify PAT \nassertions: \n \n1 \n#assert System deadlockfree; \nThis \nassertion \nchecks \ndeadlock \nfreedom in the ‘System’ \n2 \n#assert System reaches goal?0; \nThis assertion checks that whether \nthe ‘System’ can reach its goal (by \nreceiving a goal event with ‘0’ \nparameters) \n3 \n#assert System |= <>goal?0; \nThis is an equivalent LTL assertion \nIt checks if the goal is eventually \nreachable. \n4 \n#assert System |= []<>goal?0; \nThis LTL assertion checks if the \ngoal is always eventually reachable \nby the system. Note that it is \ndifferent from assertion 2. \n5 \n#assert System |= <>goal?0 deadline[50]; \nThis \nassertions \nverifies \ngoal \nreachability with time constraint i.e., \nits checks if the goal is reachable \nwithin 50 time units or not? \n6 \n#assert System |= <>goal?0 with prob; \nThis assertion checks the min and \nmax \nprobability \nof \nthe \ngoal \nreachability.  \n7 \n#define goal (Some-Variable == True); \n#assert System reaches goal; \nThis is another way to verify goal \nreachability, \nwhere \nthe \ngoal \ndefinition is based on some value of \na variable. \nTable 20: Some examples of PAT Assertions \n \nWhen an assertion is defined and its syntax is correct, we can verify it by running the \nPAT model checker and select the desired assertion from the list. The model checker \nwill present the verification results with success, showing that the assertion is verified \nor it will provide a counter example if the assertion is not satisfied. In chapter 9, an \nexample of field artillery is presented to show how a CSP composed model is verified \nwith requirement specifications defined as PAT assertions. Figure 42 describes the \noverall process of state-space analysis technique.  \n\nChapter 5  \n \nProposed Methodology and the Verification Framework \n   \nPage 115 \n \n \nFigure 42: CSP based Model Checking Technique \n \n5.10 Summary \nIn this chapter the proposed composability verification framework is discussed in \ndetails with its structural and functional specifications. Each activity, algorithm, \ntechnique and the process is explained in the perceptive of Component based M&S \nlife-cycle. The composability verification is performed at three levels of \ncomposability called static, semantic and dynamic-semantic composability. The main \nobjective of the proposed framework is to verify composability at these levels with \nrespect to requirement specifications. The first two levels are suggested to be \nevaluated using static-analysis techniques whereas the third level is proposed to be \nverified using dynamic analysis techniques. At first the behavior of the composed \ncomponents is evaluated using State-machine matching technique. If they pass this \nstep, they are subjected to one of the three proposed approaches called (i) Algebraic \nAnalysis Technique, (ii) State-space analysis technique or (iii) Model checking for \ndynamic-semantic composability verification. The choice of these approaches \ndepends on the nature of the model. In chapter 10 we will present some guidelines \non how to choose an appropriate approach.  \n \nWhen the entire composability verification process is successful, it implies that the \nBOM based composed model is structurally and behaviorally consistent, it is \ncomposable at syntactic, semantic and dynamic-semantic level and is correct with \nrespect to the given requirement specifications.  \n \n \n\n   \nPage 116 \n \nChapter 6 \nComposability Verification Process \n \nChapter 5 mainly presented the specification of our proposed composability verification framework \nincluding details of different modules, their mechanics and the procedures they perform. In this \nchapter we present how to use our framework. It can be used as a manual of our composability \nverification framework. At the end of this chapter we also provide necessary recommendations for the \nselection of appropriate approach based on the given inputs. \n \nThe description of shapes used in the following flow diagrams is as follows: \nObject, \nData, \nModel, \nComponent etc. \n \nAny shape of this color \nexpress a 3rd party tool \n \n List, Collection or Set of \nobjects, Data, Model \n \nStop \nmeans \nthat \nthe \nprocess has failed.  \n  \nProcess or action \n \nGo means it is successful, \ntherefore process with \nthe implementation phase \n \nIterative process.  \n \nCompare two objects. \n \nExtension or Transformation \nof object \n \nCompare multiple objects \n \nExtension or transformation \nof many objects  \n \n \n \nComments \n \n \n \nData, Process, Information \nflow \n \n \n \nPage connector \n \n \n \nRepeat \nprocess \n(Go \nto \nprevious step) \n \n \n \n6.1 \nComposability Verification Process \nFigure 43 to Figure 53 illustrate the composability verification process in the form of a \nflow chart. (The illustrated steps are explained later in this section): \nStop \nGo \nX \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 117 \n \n \nFigure 43: Formulation of Simuland, Requirements and Conceptual Model \nAbstraction \nReal System \nRequirement \nEngineering \nSimuland \nUML \ndiagrams \n(or \nothers) \nrepresenting \nstructure \nand \nbehavior of the \nsystem \nFormal \nor \nInformal \nBOM \nRepository \nConstruct \ncomponents \nfrom scratch and store \nthem in the repository \nSearch components that \nmatch the requirements \n  \nCandidate BOMs \nSelect suitable BOMs \nFilter \nBOMs \nand select the \nmost \nsuitable \nset of BOMs \nthat match the \nrequirements \nCompose selected BOMs \n  \nConceptual Model \n1 \nMatching and \nSelection \n1 \n2 \n3 \n4 \n5 \n6 \n7 \nRequirements \n  \nObjectives \n  \nConstraints \n  \nSelected BOMs \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 118 \n \n                                                                         \n  \n \n \n \nIf all candidate \nBOMs \nare \ntried \nwithout \nsuccess \nthen \nrepeat step 3 \n \nSyntactic Matching Process \nStart \nRule \nbased \nSyntactic \nMatching \nprocess to check that \nthe components can \ncorrectly fit to gather \nand their inputs and \noutputs match each \nother. \nSender BOM-i \nEvents \nEntities \nStates \nActions \nReceiver BOM j \n \nEvents \nEntities \nStates \nActions \nCheck SM-Rule 1: \nName of the send-event and \nreceive-event \nshould \nbe \nsame \nSend-Event  \nRule-1 \nPassed\n? \nNo \nStop \nand \nrepeat from \nstep 5.  \n8 \nYes \nProceed \nto Rule-2 \nCheck SM-Rule 2: \nEach send-event should have at \nleast one corresponding receive-\nevent and vice-versa \nSender BOM-i \nEvents \nEntities \nStates \nActions \nReceiver BOM j \n \nEvents \nEntities \nStates \nActions \nEqual \nNames \nRule-2 \nPassed\n? \nNo \nStop \nand \nrepeat from \nstep 5. \nYes \nProceed \nto Rule-3 \nCheck SM-Rule 3: \nThe \nnumber \nof \nparameters \n(content characteristics of event \ntypes) of the send-events should \nbe the same as the number of \nparameters of the receive-events. \nSender BOM-i \nEvents \nEntities \nStates \nActions \nReceiver BOM j \n \nEvents \nEntities \nStates \nActions \nEqual no. of \nparameters \nReceive-Event  \nRule-3 \nPassed\n? \nNo \nStop \nand \nrepeat from \nstep 5. \nYes \n9 \n10 \n11 \n2 \n1 \nFigure 44: Syntactic Matching Process \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 119 \n \n \n \n  \nStatic-Semantic \nMatching Process \nCheck SSM-Rule 1: \n“Area-of-Interest” attribute of \nall the actions should be exactly \nsame as that of POI or should \nbelong to an equivalent class in \nthe respective ontology \nRule-1 \nPassed\n? \nNo \nStop \nand \nrepeat from \nstep 5. \n12 \nYes \nProceed \nto Rule-2 \n13 \n3 \n2 \nStart Rule based Static-\nSemantic \nMatching \nprocess to check that \nthe \ncomposition \nis \nmeaningful \nand \nthe \ncomponents \ncan \ncorrectly \nunderstand \neach other.  \nPattern of Interplay (POI) \nBOM-1 \nEvents \nEntities \nStates \nActions \nBOM N \n \nEvents \nEntities \nStates \nActions \nBOM-OWL \n  \n  \n  \n  \n  \nAOI \nPurpose \nData Type \nUnits \nCheck SSM-Rule 2: \n“Purpose” attribute of all the \nactions should be exactly same \nas that of POI or should belong \nto an equivalent class in the \nrespective ontology \nRule-2 \nPassed\n? \nNo \nStop \nand \nrepeat from \nstep 5. \nYes \nProceed \nto Rule-3 \nCheck SSM-Rule 3: \nData types of each element in the \nevent parameters of the send-event \nand receive-events should be of same \nclass, equivalent class or should be in \ndirect hierarchical relationship \nRule-3 \nPassed\n? \nNo \nStop \nand \nrepeat from \nstep 5. \nYes \nProceed \nto Rule-4 \n14 \n15 \nFigure 45: Static-Semantic Matching Process \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 120 \n \n \n \n \n  \nCheck SSM-Rule 4: \nThe units of the quantities expressed \nin \neach \nelement \nin \nthe \nevent \nparameters of the send-event and \nreceive-events should be of same class, \nequivalent class or should be in direct \nhierarchical relationship \nNo \nStop \nand \nrepeat from \nstep 5. \n3 \nYes \nIf Static-Semantic Matching \nRule 1, 2, 3 & 4 are satisfied \nthen we can confirm that the \ncommunication among the \ncomponents is meaningful as \nintended.    \nPerform State-machine \nMatching Process \n \nRule-4 \nPassed\n? \n15 \n16 \nStart \nDynamic-Semantic \nComposability evaluation. \nIn the first step, the state-\nmachines \nof \nall \nthe \ncomposed \nBOM \ncomponents are matched \nto check their behavior \ncompatibility.   \n  \nCandidate BOMs \nTransform  \nBOM State-machines     \nto SCXML \nAbstract level execution \nSCXML\n1 \nSCXML\n2 \nSCXML\nN \nIs \nFinal? \nNo \nStop \nand \nrepeat from \nstep 5. \nYes \nThe composed \ncomponents \nare behaviorally \ncompatible! \nIf the SCXML instances of composed \nBOM components are executed at an \nabstract level, and they all reach their \nfinal states then the state-machines are \nsaid to be matched. \n4 \n17 \n18 \nFigure 46: State-machine Matching Process \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 121 \n \n \nFigure 47: Approach Selection | PN Algebraic Technique \n \n4 \nSelect an appropriate approach for dynamic-\nsemantic composability analysis \nPN Algebraic \nTechnique \n19 \nCPN State-Space Analysis \nModel Checking \nIf \nstandard \nBOM \ncomponents \nare \ncomposed \n(with \nno \ninformation \navailable \nfor their extension) and \nonly general structure \nand behavior is to be \nanalyzed \nthen \nthis \napproach is suitable.  \n  \n  \nConceptual \nModel \nTransform  \nBOM to PNML \nPNML \nModel \n23 \n21 \n20 \n22 \nIf details of the BOM \ncomponents are available \nwhich are required to \nextend them into to E-\nBOMs \nand \nfunctional \nspecification \nof \nthe \ncomponents \nis \nto \nbe \nevaluated \nthen \nthis \napproach is suitable. \n  \n5 \nTransform BOM-\nstate \nmachines \ninto a single PN \nmodel  \nIf \nthe \nmodel \nrequirements \ncontain time constraints and (or) \nthe \nmodel \npossess \nnon-\ndeterministic behavior and (or) \nthe model has a large number of \nstate (but does not require \ndetailed enumerations) then we \npropose to use this approach.  \n6 \nPNML Execution \n24\n \nIf \nthe \nexecution \nis \nsuccessful such that it \nleads to final states, \nthen the model satisfies \nS3b. \nMeaning \nthe \nbehavior \nof \ntransformed \nmodel \ncorrectly represents its \nconceptual model. \nPIPE execution \nEnvironment \nNo \nStop \nand \nrepeat from \nstep 5. \nYes \nContinue \nVerification  \n7 \nSuccess \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 122 \n \n \n \n \nFigure 49: Implementation \n \n  \nRequirements \nCan\ndid\n \nCan\ndid\n \nObjectives \nCan\ndida\n \nCan\ndida\n \nConstraints \nProperty Verification \nTranslate objectives \nand Constraints into \nPN properties \n26\n \n25\n \nNext \nproperty \nAlgebraic Computation \nResources \n(Incidence Matrix, P-Invariants,     \nT-Invariants etc.) \n \\ \nProperty-\nProving \nTheorem \n  \nProperty \nVerification \nMethod \nPIPE Function \nlibrary \nCalculate algebraic computation \nresources of the PNML model \nusing PIPE library functions \nConstruct a property verification \nmethod using an appropriate PN \nproperty proving theorem \nRS \nSatisfied? \nNo \nComposability \nverification failed. \nModifications in \nthe \nconceptual \nmodel \nare \nrequired. \nYes \nComposability verification \nis successful.  \n \nThe conceptual model is \nqualified \nfor \nthe \nimplementation phase \nGo \nStop \nPNML \nModel \n7 \nComposed \nModel \nSimulation \nModel \nExperimental \n Model \nCode \nDesign of  \nExperiment \nSimulation Results \nSimulation \nGo \nFigure 48: PN Algebraic Technique (continued) \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 123 \n \n \nFigure 50: State-Space Analysis Technique \n  \n\\  \nConceptual \nModel \n  \nBOM to E-BOM \nExtension \n27 \nModeler’s input is \nrequired here to \nextend BOM into \nE-BOM \n  \nE-BOM \n  \nTransform E-BOM to \nCPN Component Model \nEach \nE-BOM \ncomponent \nis \nautomatically \ntransform into CPN \ncomponent Model \n  \nCPN   \nComponent models \n28 \n5 \nE-BOM \nExtension Utility \nCPN-CM is our proposed \ncomponent specification \nbased on CPN language \nStructural Comparison \nwith the Conceptual \nModel \nDoes the transformed \nmodel contain all Events, \nparameters, \nActions, \nStates, exit-conditions as \nspecified \nin \nthe \nconceptual Model? \nIs the behavior of the \ntransformed model bi-\nsimilar to the conceptual \nmodel? \n \nPerform functional test \non each component to \ncheck that the inputs \nproduce desired output \naccording \nto \nthe \nconceptual model. \n \nPerform CPN execution \nto compare that the \nprogress is made by all \nthe \ncomponents \naccording to the pattern \nof interplay defined in \nthe conceptual model.  \n \n  \nBehavioral Comparison \n(Functional Testing) \nCPN Execution \nEnvironment \nPerform Model inspection and \ncheck that all the elements of \nConceptual model are present \nin the transform model \nPerform functional testing: \nInitialize \ninputs \nof \neach \ntransformed \ncomponent \nseparately and execute the \ncomponent \nin \nthe \nCPN \nexecution \nenvironment \nto \ncheck if it produces desired \noutput \naccording \nto \nthe \nconceptual l model \n \nIs \nSuccessful? \n \nNo \nStop \nand \nrepeat from \nstep 27. \n8 \nYes \nS3b \npartially \nsatisfied \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 124 \n \n \nFigure 51: State-Space Analysis Technique (continued) \nRequirements \nCan\ndid\n \nCan\ndid\n \nObjectives \nCan\ndida\n \nCan\ndida\n \nConstraints \nTranslate objectives \nand Constraints into \nCPN properties \nNext property \nModeler is required to manually \ncompose all the generated CPN \ncomponents into a main CPN \nmodel.  \nCompose CPN \nComponents \nCPN \nComposed \nModel \nInitialize and Execute \nCPN Model  \nSuccessful \nNo \nThere are exceptions in the \nexecution.  Check and \nRepeat step 29, or 28, 26 \n \n  \n29 \n30\n \nGenerate State-Space \n31 \nYes \nS3b completely \nsatisfied.  \n  \nPerform CPN Property \nVerification \n32 \nRS \nSatisfied? \nNo \nComposability \nverification failed. \nModifications \nin \nthe \nmodel \nare \nrequired \nYes \nComposability \nverification \nis \nsuccessful, go to implementation \nphase  \nGo \nStop \n8 \nCPN \nHierarchical \nModeling Tool  \nCPN Execution \nEnvironment \nCPN State-Space \nAnalysis Tool  \nProperty Verification \nQuery Function \n(written in CPN-ML)  \nCPN ML  \nProgramming \nEnvironment \nCPN Query \nFunction library \nNext \nfunction \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 125 \n \n \nFigure 52: Model Checking \nContinue from step 22 \n\\ \n \nConceptual \nModel \n  \nBOM to E-BOM \nExtension \n33 \nModeler’s input is \nrequired here to \nextend BOM into \nE-BOM \n  \nE-BOM \n  \nTransform E-BOM to \nCSP Process \nEach \nE-BOM \ncomponent \nis \nautomatically \ntransform into CSP \nProcess model. \n \n \nCSP  \nProcess Components \n34 \n6 \nE-BOM \nExtension Utility \nCSP process components \nare \nrepresented \nusing  \nPAT’s CSP# specification \nStructural Comparison  \nDoes the transformed \nmodel contain all states, \nand events as specified in \nthe conceptual Model? \nBehavioral Comparison \nPAT Simulation \nEnvironment \nPerform Model inspection and \ncheck that all the elements of \nConceptual model are present \nin the transform model \nPerform behavioral similarity \nevaluation by simulating the \nCSP composed model in the \nPAT simulation environment. \nIf it reaches final states then it \ncorrectly \nrepresents \nthe \nbehavior of its corresponding \nconceptual model.  \n Pass? \n \nNo \nStop \nand \nrepeat from \nstep 33, 19 \nor 5 \n9 \nYes \nS3b satisfied \nE-BOM with Time \nconstraints \nand \nprobabilistic factors \nCompose CSP \nComponents \nCSP Composed Model \n35 \nCompose all the generated \nCSP components using \nparallel operator \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 126 \n \n \nFigure 53: Model Checking (continued) \n \nThe Composability Verification Process is explained as follows: \n \n6.1.1 Formulation of Simuland, Requirements and Conceptual \nModel \nIn step 1, the Real system is studied and a suitable simuland is formulated. It can be \ndescribed formally or informally. We assume that UML diagrams are used to describe \nthe simuland. The system is also studied to gather requirements and formulate \nrequirements using our proposed formal requirement specification method (step 2). \nWith this information at hand, suitable components are searched in the BOM \nrepository, with an assumption that a composition of these components will form a \nconceptual model that represents the simuland (step 3).  If a desired component is \nRequirements \nCan\ndid\n \nCan\ndid\n \nObjectives \nCan\ndida\n \nCan\ndida\n \nConstraints \nTranslate objectives \nand Constraints into \nCSP assertions \nStart PAT \nModel Checking Tool \n36 \n  \nVerify Assertion \n37 \nRS \nSatisfied? \nNo \nComposability \nverification failed. \nModifications \nin \nthe \nmodel \nare \nrequired \nYes \nComposability \nverification \nis \nsuccessful, go to implementation \nphase  \nGo \nStop \n9 \nPAT Model \nChecker \n  \nAssertions  \nLTL, CTL, RT-\nLTL, PLTL \nNext  \nAssertion \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 127 \n \nnot found it is constructed form the scratch, added to the repository, and then used \nin the current context (step 4).  \nThe discovered components are called candidate components. Among these \ncandidates, most suitable ones i.e., those that best match the simuland and the given \nrequirements, are selected (step 5, 6).  \nThese BOM components are composed and a conceptual model is constructed (step \n7). We recommend that the modeler also creates a formal model of the conceptual \nmodel using our proposed BOM formalism and graphical notation. This will help in \ndocumentation and understanding details of conceptual model and its composition.  \n \n6.1.2 Syntactic Matching Process  \nWhen the verification process starts the Composed BOM model (conceptual model) \nis passed through a rule-based static analyzer to verify the composability at syntactic \nlevel (step 8-11). If this level is passed then the constraint S1 (as defined in Table 10) \nis satisfied and only then the model is cleared for the next step (otherwise the \nverification process is stopped and another candidate selection is picked, composed \nand this step is revised).  \n \n6.1.3 Static-Semantic Matching Process  \nIn the next step the components are analyzed at static-semantic level using the \nsemantic analyzer (step 12-15). When this step is passed then the constraint S2 (as \ndefined in Table 10) in is satisfied and the BOM composition is ready to be verified at \ndynamic-semantic level.  \n \n6.1.4 State-machine Matching Process  \nAt this level, the first step is to perform state-machine matching of BOM \ncomponents using State-machine checker (step 15 – 18). A successful state-machine \nmatching satisfies the constraint S3a (as defined in Table 10). \n \n6.1.5 Approach Selection for Dynamic-Semantic Composability \nVerification \nIn the next stage the verification framework offers three choices of verification \ntechnique for the analysis of dynamic-semantic composability level. The modeler can \nchoose algebraic technique if there is no information available to extend the BOM \ncomponents into E-BOM. Therefore the conceptual model will be transformed into \nPNML without requiring any extension. If the modeler has details and data available \nto transform BOM into E-BOM and the model does not represent a real-time \nsystem, then it is highly recommended that the second proposed approach (CPN \nstate-space analysis) should be chosen. If the model represents a real-time system and \nit is stochastic in nature then the modeler should choose the third approach (Model \nchecking). These are general guidelines and are not concrete rules. The ultimate \nchoice of the approach depends on the nature of the model, nature of the \nrequirement specification properties and the available information. \n \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 128 \n \n6.1.6 PN Algebraic Technique \nWhen Algebraic technique is selected, at first the conceptual model is transformed \ninto PNML model (step 23). This PNML model is executed in PIPE execution \nenvironment to evaluate S3b (step 24). If successful then the requirement \nspecification properties are taken one by one and translated into a PN property (step \n25). Thereafter a property proving theorem is selected that proves this PN property. \nBased on this theorem a property verification method is constructed inform of an \nalgorithm. Running this algorithm proves or falsifies the requirements specification \nproperty (step 26). If all the properties in the requirement specification are satisfied \nthen the model is successfully verified otherwise the process is stopped and model \nrefinements are made. \n \n6.1.7 State-Space Analysis Technique \nWhen the CPN state-space analysis technique is selected, at first each BOM \ncomponent is extended to E-BOM (step 27). This step requires modeler’s input and \ncan be delivered using the BOM-to-E-BOM extension utility. When the extension is \ncomplete, each E-BOM is transformed into our proposed CPN component model \nusing our automatic transformation tool (step 28). The output of this step is a set of \nCPN components. At this step it is required to conduct structural and behavioral \ncomparison between the generated components and the respective BOM using \ninspection and functional testing methods. If the comparison is successful then S3b \nconstraint of the requirement specification is partially satisfied.  \nThe modeler is then required to compose these generated components in a main \nmodel using CPN hierarchical tool (step 29). (Binding IN-ports and OUT-ports of \neach component using sockets in the main model). When the model is composed, it \nis executed (step 30) using CPN execution environment to test that all components \ncorrectly interact with each other and make necessary progress to reach the final \nstates. If the execution is successful then the constraint S3b is fully satisfied, \nconforming that the structure and behavior of the executable model correctly \nrepresents its respective conceptual model and therefore any verification operation \nperformed on the executable model will imply correctness of its conceptual model.  \nIn the next step the CPN model is subjected to the state-space analysis (step 31). At \nfirst a state-space graph of the model is generated. Then for each objective and \nconstraint in the requirement specification a verification query function is either \ncreated or selected from the function library. The execution of this function is done \nusing CPN-ML program execution environment and the result of this function tells \nif the property is satisfied or violated (step 32). If all the properties are satisfied we \nsay that the composability verification process is successful.  \n \n6.1.8 Model Checking \nWhen the Model Checking technique is selected, at first each BOM component is \nextended to E-BOM (step 33). This step requires modeler’s input and can be \ndelivered using the BOM-to-E-BOM extension utility. It is possible to assign Time \nconstraints and the probabilistic factors with the states or the transitions.  When the \nextension is complete, each E-BOM is automatically transformed into CSP# process \nspecification (step 34) and composed (step 35). The composition of each CSP \n\nChapter 6  \n \nComposability Verification Process \n   \nPage 129 \n \nprocess representing a BOM component be done using sequential operator ‘;’ parallel \noperator ‘||’ interleaving operator ‘|||’ or (non-deterministic or user’s) choice \noperator ‘[ ]’ depending upon the nature of the composed components. We suggest \ncomposing each CSP in parallel so that each process executes in parallel and \nsynchronizes with each other by sending or receiving events at their respective \ncommunication channels. The composed model can then be simulated using the \nPAT simulator. A successful simulation run with at least one path leading to the final \nstate(s) shows that the behavior of the composed model correctly represents its \nconceptual model and thus satisfies constraint S3b.  \nIn the next step, the assertions defined in CSP format (using LTL, Real-Time LTL or \nPLTL) are verified using PAT model checker which results in its satisfaction or \nviolation (step 36, 37). If all the assertions are satisfied we say that the composability \nverification process is successful. \n \n6.2 Summary \nIn this chapter, a flow diagram of composability verification process is presented. It \nindicates different steps and forms of inputs and outputs of each step in the process. \nThis flow diagram can be used as a guideline to perform composability verification \nusing three different approaches. Some recommendations are also presented in \nmaking a suitable choice. Once the verification process is completed successfully, the \ncomposed model can undergo implementation phase where it is programmed and \nsimulated using a suitable simulation platform. Also the experimental model can be \nconstructed to perform different experiments on the implemented model and \nsimulation results are generated for study and decision making. The implementation \nphase is out of the scope of this thesis. \n \n \n \n\n   \nPage 130 \n \nChapter 7 \nFairness verification using PN \nAlgebraic Techniques \n \nThis chapter explains how algebraic techniques can help in verifying system properties of a Composed \nModel, using an example of a manufacturing system in which fairness is selected to be the required \nsystem characteristic. \n7.1 \nFairness \nFairness has been defined in section 3.1.3 in terms of a Petri Nets property. In this \nchapter the concept of fairness is covered in detail. Intuitively, fairness is a liveness \nproperty that means no component of the system which becomes possible (or \nbecomes enabled) sufficiently often should be delayed indefinitely. [122].  \nOn the basis of the extent of sufficiency, fairness is generally categorized in the \nfollowing three types in literature: \nUnconditional Fairness \nAlso called Impartial implies that every component in a system proceeds infinitely \noften without any condition. The term “proceed” means to make progress, (e.g., \nfiring of a transition). Unconditional fairness is also known as non-deterministic \nchoice and is usually present among the components that are independent of each \nother [122]. \nWeak Fairness \nAlso called Just, implies that every component in a system that is enabled \ncontinuously from some point onwards eventually proceeds. \nStrong fairness \nEvery component in a system that is enabled infinitely often proceeds infinitely \noften. A noticeable difference in weak and strong fairness is that weak fairness \ninvolves persistent enabling of a component that wants to proceed, whereas strong \nfairness is not persistently enabled.  \nSome important generalizations of fairness exist in literature [122]: \nEqui fairness: means to give each component an equal chance to proceed. It can be \nregarded as Justice. This type of fairness does not always apply in real world \nscenarios because of priority policies or some other reasons. \nBounded fairness: means to give each component an equal number of chances such \nthat no component proceeds for more than “k-times” without letting the others to \ntake their turn \nFor instance there is a check-in service at the airport that serves two types of queues: \n(i) Business class and (ii) Economy class at a time. It will be called fair, if it mostly \nserves business class passengers but not more than (say) 10 times, without serving a \npassenger from the economy class queue. \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 131 \n \nIn Petri Nets, fairness can be viewed in two perspectives namely: Transition fairness \nand Marking fairness. The former corresponds to fairness of choice of transitions, \nand the latter deals with the fair reachability of states.  \n7.2 Fairness Verification \nThere are different ways to verify fairness of a model.  The focus of this chapter is to \ndiscuss the technique for the verification of fairness property using PN Algebraic \nanalysis and provide the necessary and sufficient conditions for a PN model to be \nfair. The evaluation of these conditions in a PN model involves theorems and linear \nalgebraic computations; therefore it is classified as an Algebraic technique. Based on the \ntheorems below, we propose an algorithm for automatic fairness verification.  \nIn Petri Nets, fairness is mainly perceived in terms of occurrences (or firing) of \ntransitions. Two transitions t1 and t2 are said to be in a fair relation if there exists a \npositive integer k such that for any reachable marking M and any firing sequence σ:  \n(The symbol #(t/σ) denotes the number of times a transition t occurs in a firing sequence σ)  \nIn words, neither of the transitions should occur more than a finite number of times \n(k) without letting the other to occur at least once. This is known as bounded fairness \n(or B-Fairness) with upper bound = k. If every pair of transition is in a bounded fair \nrelation, then the entire net is said to be fair [123]. \nFor the algebraic verification of fairness property in a PN model the following \ntheorems are applied. Details and proofs of these theorems are discussed in [123].  \nTheorem I \nGiven a PN with an incidence matrix A, if there exists a firing-count vector X, such that:  \nA.X ≥ 0 and X≠0 \nThen a necessary condition for the PN to be fair is that each entry of X is positive.  \n \nTheorem II \nIf a Petri Net N is bounded for any initial marking M 0 then the condition in Theorem I is \nnecessary and sufficient for N to be fair.  \nCorollary: If there exists a P-Invariant Y of positive integers such that: A.Y=0 then the PN is \nguaranteed to be structurally bounded.  \n \nTheorem III \nA fair Petri Net PN has only one reproduction vector (i.e., a minimal T-Invariant) at the most.  \n \nBased on the above definition of the bounded fairness and theorems I, II and III a \nPN is said to be fair if it satisfies two conditions: (i) There must exist a single T-\nInvariant X of a given PN model whose each entry is non-zero and the product    \nAX = 0 and (ii) There must be at least one P-Invariant, which means that the net is \nstructurally bounded. \n# (t1/σ) = 0 ⇒ #(t2/σ) ≤ k  ∧ #(t2/σ) = 0 ⇒ #(t1/σ) ≤ k \n(7.1) \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 132 \n \n7.3 Manufacturing system \nIn this section, a component based composed model of a manufacturing system is \npresented. Using this composed model, it is shown how the proposed verification \nframework is used to verify the required specification, which in this example consists \nof fairness as an important quality constraint. Two different scenarios of this \nexample are discussed. In the first scenario the model is shown to be unfair as \nverified by our verification framework. In the second scenario the model is modified. \nIt is then verified and it satisfies the fairness constraint.  \n \n7.3.1 Scenario I \nIt is assumed that the manufacturing system model is composed of two machines M1 \n& M2 and a shared Robot R as shown in Figure 54. The robot loads raw material on \nthe machines and operate on them for producing goods. The Robot is assigned \n(“loaded”) to either of the machines at a time. When the Robot is loaded, it deposits \nraw material on the machine and process it. When the good is produced the robot is \nunloaded and is available for the other machine. \n \nFigure 54: Manufacturing System (acquired from [124]) \nThe process of composability verification is initiated as follows: \n \nSimuland and Requirement Specification \nIn the first step, the entities, events and the states of the simuland are perceived \naccording to Figure 54. The simuland and the requirement specifications are used to \nconstruct an appropriate conceptual model according to the steps given in the \ncomposability verification process described in Chapter 6. \n \nWe define Requirement speciation of the manufacturing system as: \nRS0 = 〈O, S〉 where: \n \nObjectives O = {o1, o2, o3} \no1: Machine1 should continuously produce product1 without any infinite delay \no2: Machine2 should continuously produce product2 without any infinite delay \no3: Both machines should produce products with a ratio of 1:1 \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 133 \n \n \nSystem Constraints S = {s1, s2, s3, s4} \ns1: Machine1, Machine2 and the Robot components should be composable at \nsyntactic level \ns2: Machine1, Machine2 and the Robot components should be composable at static-\nsemantic level \ns3a: State-machine matching of the composed model should be successful. Since the \nmodels are non-terminating so there are no final states, instead the goal-states: \n“Machine1 completes production” & “Machine2 completes production” will be considered.   \ns3b: The transformed executable model correctly represents the structure and \nbehavior of the conceptual model. \ns4: The shared robot should treat both machines with fairness (i.e., k-fairness; k=1). \n \nConceptual Model \nThe formal specification and graphical representation of each BOM model \nparticipating in the manufacturing composed model are as follows. For the ease of \nreadability following color codes are used for different BOM elements in the formal \ndefinition: \n \nBB0 = 〈 EnT, EvT, S, AcT 〉 where: \nEnT = Machine1 {C0(Id:Integer)} \n \nEvT = {E0(LoadingM1, Robot, Machine1, null), E1(UnloadingM1, Robot, Machine1, null), \nE2(ResetM1, Robot, Machine1, C0)} \n \nAct = { A0(LoadingM1, Robot, Machine1, E0), A1(UnloadingM1, Robot, Machine1, E1), A2(ResetM1, \nRobot, Machine1, E2)} \n \nS = {S0(M1Waiting, A0, S1),  S1(M1Processing, A1, S2), S2(M1Completed, A2, S0)}  \nTable 21: Formal definition of Machine1 Base-BOM \n \n \nBB1 = 〈 EnT, EvT, AcT, S 〉 where: \nEnT = Machine2 {C1(Id:Integer)} \n \nEvT = {E3(LoadingM2, Robot, Machine2, null), E4(UnloadingM2, Robot, Machine2, null), \nE5(ResetM2, Robot, Machine2, C1)} \n \nAct = { A3(LoadingM2, Robot, Machine2, E3), A4(UnloadingM2, Robot, Machine2, E4), A5(ResetM2, \nRobot, Machine2, E5)} \n \nS = {S3(M2Waiting, A0, S4),  S4(M2Processing, A1, S5), S5(M2Completed, A2, S3)} \nTable 22: Formal definition of Machine2 Base-BOM \n \n \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 134 \n \nBB2 = 〈 EnT, EvT, AcT, S 〉 where: \nEnT = Robot {} \n \nEvT = {E6(LoadingM1, Robot, Machine1,  null), E7(UnloadingM1, Robot, Machine1, null), \nE8(LoadingM2, Robot, Machine2, null), E9(UnloadingM2, Robot, Machine2, null)} \n \nAct = {A6(LoadingM1, Robot, Machine1, E6), A7(UnloadingM1, Robot, Machine1, E7), \nA8(LoadingM2, Robot, Machine2, E8), A9(UnloadingM2, BB2, Machine2, E9)} \n \nS = {S6(Idle, {A6, S7},{A8, S7} ),  S7(Busy, {A7, S6}, {A9, S6})} \n \nTable 23: Formal definition of Robot Base-BOM \n \n \n \nCB0 = 〈 AcTIN,  AcTOUT ,  POI 〉 where: \nAcTIN = AcTOUT = ∅  \n \nPOI = {POI0(!A6 , ?A0), POI1(!A7 , ?A1),  POI2(!A2, ?A2),  POI3(!A8 , ?A3), POI4(!A9 , ?A4), POI5(!A5, \n?A5) } \n \nTable 24: Formal definition of Manufacturing System composed BOM \n \nFigure 55: Manufacturing System BOM based Composed Model \n \n \nFigure 55 represents the BOM based Conceptual Model of the manufacturing system \nwhich includes three BOMs, formally defined using our proposed graphical notation. \nThe figure shows how the characteristics, Events, Actions and states are mapped to \neach other (using dotted red line). In machine 1 characteristic C0 is mapped to Event \nE2 which means event uses characteristic C0 as parameter. Similarly Event E0 is \nmapped to A0, E1 to A1 and E2 to A2 respectively which means the Actions uses their \nmapped events. The mapping of actions to the states in the figure shows which \naction will cause which state to transit to the new state (shown by blue arrow). The \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 135 \n \nbasic BOM components are connected to each other using the formal definition \nshown in Table 24, which describes the source (!) and destination (?) of an action \nfrom one component to other. In Figure 55 this is shown using black arrow lines with \ntheir input/output (I/O) label. This is called Pattern of Interplay (in BOM \nspecification). \nStatic Analysis  \nRules \nMachine1 \nMachine2 \nRobot \n• Name of the send-event and \nreceive-event should be same \n• Each send-event should have at \nleast one corresponding receive-\nevent and vice-versa \n• The number of parameters \n(content characteristics of event \ntypes) of the send-events should \nbe the same as the number of \nparameters \nof \nthe \nreceive-\nevents. \n?LoadingM1(null) \n \n!LoadingM1(null) \n?UnloadingM1(null)  \n!UnloadingM1(null) \n \n?LoadingM2(null) \n!LoadingM2(null) \n \n?UnloadingM2(null) \n!UnloadingM2(null) \nTable 25: Syntactic Matching \n \nIt can be seen in Table 25 that the name of the send-event and receive-events are the \nsame. ( !=Send, ?=Receive). And they are in one-to-one relationship. Also the no. of \nparameters of each event is equal to 1. Based on these facts the components are said \nto be syntactically composable (S1 satisfied).  \n \nWe assume that Machine1, Machine2 and Robot components have the semantic-\nattributes as shown in Table 26 which satisfy all the static-semantic matching rules. \nThe attributes highlighted in red color are semantically equivalent (Exact match) \ntherefore S2 is satisfied.  \n \nMachine1 \nMachine2 \nRobot \nAOI = {Production, Manufacturing, \nProduction-line, Lathing} \nAOI = {Production, Manufacturing, \nProduction-line, Polishing} \n \nAOI = {Production, Manufacturing, \nConveyer, Automation} \nPurpose = {Manufacture Product1, \nManufacture Product3} \nPurpose = {Manufacture Product2, \nManufacture Product3} \nPurpose = {Manufacture Product3} \nData Types of parameters= {null} \nData Types of parameters = {null} \nData Types of parameters = {null} \nUnits of Measurement = {} \nUnits of Measurement = {} \nUnits of Measurement = {} \nTable 26: Static-Semantic Matching \n \nDynamic Analysis  \nThe state-machine matching process is successfully conducted as both Machine1 and \nMachine2 reach their goal-states namely: Mcompleted and M2-completed and satisfy S3a. \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 136 \n \n \nFigure 56: State-machine matching of manufacturing system \n \nBOM to PNML Transformation \nIn the next step the components are subjected to PNML transformation process. \nThe output of the transformation process is a PN model shown in Figure 57. It can \nbe seen from the inspection that the States and their exit conditions, Events and \nActions all are present in the transformed model (as specified in the original \nconceptual model). Also this PN model is executed in the PIPE runtime \nenvironment. The execution is successful because the places P3 and P6 acquired \ntokens (showing that these goal states were reached during the execution). This \nsatisfies S3b.  \n \n \nFigure 57: PN model of the manufacturing System \n \n \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 137 \n \nAlgebraic Resource Computation \nAt this step, the initial marking M0 and the Incidence Matrix A of the PN composed  \nmodel shown in Figure 57 are calculated using PIPE library functions as follows: \n \n \nM 0 P1 P2 P3 P4 P5 P6 P7 P8 \n \n1 \n0 \n0 \n1 \n0 \n0 \n1 \n0 \n \nA P1 P2 P3 P4 P5 P6 P7 P8 \nT1 -1 \n1 \n0 \n0 \n0 \n0 -1 \n1 \nT2 0 \n-1 \n1 \n0 \n0 \n0 \n1 \n-1 \nT3 1 \n0 \n-1 \n0 \n0 \n0 \n0 \n0 \nT4 0 \n0 \n0 \n-1 \n1 \n0 -1 \n1 \nT5 0 \n0 \n0 \n0 \n-1 \n1 \n1 \n-1 \nT6 0 \n0 \n0 \n1 \n0 \n-1 0 \n0 \n \nTable 27: Initial Marking and Incidence Matrix (Scenaro I) \nNote that the labels of rows and columns in A and elements in M0 correspond to \nplaces and transitions in Figure 57. The matrix A is given as input to the Invariant \ncalculation module that calculates the following P-Invariants and T-Invariants in the \nPN model of the Manufacturing System: \n \nP1 P2 P3 P4 P5 P6 P7 P8 \n1 1 1 0 0 0 0 0 \n \nT1 1 \nT2 1 \nT3 1 \nT4 0 \nT5 0 \nT6 0 \n \nT1 0 \nT2 0 \nT3 0 \nT4 1 \nT5 1 \nT6 1 \n \nTable 28: P-Invariants and T-Invariants (Scenaro I) \n \nProperty Verification Function \nIn order to proceed with the verification, we have to translate the objectives and \nconstraints of the requirement specification into PN properties: \no1: Machine1 should continuously produce product1 without any infinite delay \no2: Machine2 should continuously produce product2 without any infinite delay \no3: Both machines should produce products with a ratio of 1:1 \ns4: The shared robot should treat both machines with fairness (i.e., k-fairness; k=1). \n \nIt is clear from the {o1, o2, o3 and s4} that if the robot serves both machines with \nfairness (S4) only then both of them will be able to produce their respective products \ncontinuously without indefinite delay (O1 & O2). And if the fairness is bounded \nsuch that k=1, then both machines will produce products with equal ration 1:1.  \nTherefore the translation of the requirement specification in PN form is as follows: \n \nPN Property P1 = “The model should be Bounded-Fair (with K=1) such that the robot serves \nboth machines alternatively”.  \n \nIn order to verify bounded-fairness we consider the property proving theorems I, II \n& III defined in section 7.2. Based on these theorems we construct a property \nverification function using the following algorithm: \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 138 \n \n \nAlgorithm: B-Fairness Verification  \nInput: {P-Invariants}, {T-Invariants}, A; Output: TRUE \n1 If |{T-Invariants}| = 1 then ⊳ List of T-Invariants has exactly 1 invariant, meaning it is a \n2 \nReproduction vector , Theorem III⊲ \n3 \nXT ← T-Invariants[0]        ⊳ Get the only T-Invariant from the list \n4 \nif A.XT ≥ 0 and each element in XT >0 then   ⊳ Multiply XT with Incidence matrix and \n5 \nCheck that each element of T-invariant is  \n6 \npositive, Theorem I⊲ \n7 \nif |{P-Invariants}|>0  then            ⊳ Check if there is any P-Invariant, meaning PN  \n8 \n        Model is bounded, Theorem II⊲ \n9 \nReturn TRUE \n10 \nelse \n11 \nReturn FALSE ⊳ Theorem II violated \n12 \nend if \n13 \nelse \n14 \nReturn FALSE  ⊳ Theorem I violated \n15 \nend if \n15 else \n17 \nReturn FALSE ⊳ Theorem III violated \n18 end if \n \nTable 29: B-Fairness Verification \n \n \nBased on this algorithm we perform property verification of the given PN model. It \nis evident that the T-invariants (see Table 28) contain zero entries which violate \nTheorem I. Also there is more than one T-invariant which violates Theorem III \ntherefore the net is said to be unfair. As the PN is unfair, it is impossible to guarantee \nthat objectives o1, o2, o3 will be satisfied because either of the machines may over \nperform by acquiring robot multiple times without letting the other to get the robot \nfor at least once (failure of o1 & o2). Therefore either of the machines may face a \nsituation in which it is unable to produce enough number of products to meet the \nrequired objectives i.e., the ratio 1:1 for producing products cannot be fulfilled \n(failure of o3); consequently the composed model may fail to satisfy given \nspecifications. \n \n \n \n7.3.2 Scenario II \nIn order to understand the fairness verification process, a counterexample is \npresented. In this example another component is added to the composition called \nController that can supervise the robot assignments. The job of the Controller is to \nenforce fairness in the system.  The BOM model of the controller is defined as \nfollows: \n \n \n \n \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 139 \n \nBB3 = 〈 EnT, EvT, AcT, S 〉 where: \nEnT = Controller {} \n \nEvT = {E10(LoadingM1, Controller, Robot , Machine1,  null), E11(LoadingM2, Controller, Robot, \nMachine2, null)} \n \nAct = {A10(LoadingM1, Controller, Robot, Machine1,  E0), A11(LoadingM2, Controller, Robot, \nMachine2, E1)} \n \nS = {S8(AssignM1, {A10, S9}),  S9(AssignM2, {A11, S8})} \n \nTable 30: Formal definition of Controller Base-BOM \n \nCB0 = 〈 AcTIN,  AcTOUT ,  POI 〉 where: \nAcTIN = AcTOUT = ∅  \nPOI = {POI0(!A10 , {?A0, ?A6}), POI1(!A7, ?A1),  POI2(!A2, ?A2), POI3(!A11 , {?A3, ?A8}),  POI4(!A9 , \n?A4), POI5(!A5, ?A5) } \n \nTable 31: Formal definition of Modified Manufacturing System composed BOM \nThe other components have the same definition except that the sender of Event E0 \nand E1 is BB3 (controller) and the receivers of event E0 are BB0 (Machine1) and BB2 \n(Robot); whereas the receivers of event E1 are BB1 (Machine2) and BB2 (Robot). \nFigure 58 shows the composed BOM of modified manufacturing system.  \n \nFigure 58: Modified manufacturing system composed BOM \n \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 140 \n \nWhen the verification process is started, the BOM components are transformed into \nPN model as shown in Figure 59 where the controller component is attached to both \nmachines and the robot and controls the machine assignment to enforce Kfairness. \nIt is evident from the figure that when the robot is assigned to Machine1 once, it \ncannot be reassigned (because of the lack of token in P9), and vice-versa. If the \nnumber of tokens are increased to ‘n’, the same model can work for k=n fairness.  \nIn the initialization phase, the initial marking M0 and Incidences Matrix A were \ncalculated as follows: \nM0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 \n \n1 0 0 1 0 0 1 0 1 \n0 \n \nA P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 \nT1 -1 1 0 0 0 0 -1 1 -1 \n1 \nT2 0 -1 1 0 0 0 1 -1 0 \n0 \nT3 1 0 -1 0 0 0 0 0 0 \n0 \nT4 0 0 0 -1 1 0 -1 1 1 \n-1 \nT5 0 0 0 0 -1 1 1 -1 0 \n0 \nT6 0 0 0 1 0 -1 0 0 0 \n0 \n \nTable 32: Initial Marking and Incidence Matrix (Scenaro II) \n \n \n \nFigure 59: Modified PN model of the manufacturing System \n \nWhen the Invariant calculation module is executed, the following T-Invariant and P-\nInvariant were discovered for the model shown in Figure 59: \nT1 1 \nT2 1 \nT3 1 \nT4 1 \nT5 1 \nT6 1 \n \nP1 P2 P3 P4 P5 P6 P7 P8 P9 P10 \n1 1 1 0 0 0 0 0 0 \n0 \n \nTable 33: P-Invariants and T-Invariants (Scenaro II) \n \nHaving only one T-Invariant (and the only one) with non-zero entries and having a \nP-Invariant (with some non-zero entries), satisfies the conditions (of Theorem I, II & \nIII) required for the model to be bounded fair.  \n\nChapter 7  \n \nFairness verification using PN Algebraic Techniques \n   \nPage 141 \n \n \n \nBased on these result from PN algebraic analysis technique, we can confirm that the \ncomposed model satisfies given requirement specifications. Due to the supervised \ncontroller, the Robot is bound to operate fairly between the two machines, which \nresults in fulfillment of the objectives O1, O2 and O3 and also satisfied required \nconstraint S1.  \n7.4 Summary \nFairness property becomes significant in the composability verification of a \ncomposed model because it does not allow any component to dominate and \nexcessively proceed, while other components do not proceed even for once. As \nillustrated by the example of the manufacturing system, fairness of Robot allocation \ncan ensure that both machines will perform to produce a required number of \nproducts. If there is no fairness we cannot guarantee that this objective will be \nreached. \nUsing the example of Fairness verification in the manufacturing system, we explain \nhow our Algebraic Verification Technique works. It is a notable fact that this \ntechnique does not face state-space explosion because it does not involve reachability \ngraph construction and can work only by calculating incidence matrix and P/T \ninvariants. There are a lot of PN properties which can be verified using these PN \nalgebraic computation resources. On the other hand this approach can only be used \nto verify a limited set of PN properties (for which suitable theorems exist).  \n \n \n\n   \nPage 142 \n \nChapter 8 \nModel Verification using State-space \nAnalysis techniques \n \nColored Petri Nets and its analysis techniques are very useful for accurate and efficient verification as \nit is one of the competitive formalisms in the specification of the concurrent systems. Its application in \nthe Composability verification proves to be very constructive, especially with a focus on the dynamic \nsemantic composability level. The analysis techniques contributed by the CPN community over a \ncouple of decades provide a significant improvement on efficient and accurate reasoning regarding the \nmodel correctness. In this chapter a Field Artillery Model is presented as an example. It is shown \nhow the BOM based Field Artillery Model is transformed into our proposed Colored Petri Net \ncomponents and verified using state-space analysis.  \n \nCombat Modeling is about the models that describe or represent weapon systems \nand combat situations. There are numerous types of combat models. These types are \ndistinguished by their modeling objectives. Some of the fundamental objectives of \ncombat modeling are training, war-games, weapon testing etc. \n8.1 \nCombat Modeling \nCombat modeling purposefully abstracts and simplifies combat entities, their \nbehaviors, activities, and interrelations to answer defense-related research questions. \nThere cannot be a general model that answers all questions however there is a \nconcept of a generic situated environment and four core activities that can be found \non every battlefield [125].  \n8.1.1 Situated Environment \nCombat Modeling starts with analyzing the challenges to model the Situated \nEnvironment. All modeled combat entities are situated in the environment, the virtual \nbattlefield. They perceive the environment including other entities, and map their \nperception to an internal representation based on the knowledge and goals. They \ncommunicate and act with other combat entities within the environment. The \nenvironment contains all objects, passive ones like obstacles, as well as active ones \nlike enemy or friendly units [125].  \n8.1.2 Moving \nMoving is the core activity of combat modeling that deals with the movement of \nindividual entities. These entities could be weapon, people etc. or aggregate models \nthat are used to model the movements of groups of entities. The models use patches \nand grids; they use physical models for weapon systems and reference schemas for \nunit movement [125]. \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 143 \n \nthermal, and optical sensors, can contribute to perceiving the environment and the \nother entities as close to reality as possible. Intelligence, surveillance, and \nreconnaissance operations contribute to similar requirements. In order to sense \nspecial properties of an entity, each of these special properties needs to be modeled \nexplicitly. If it is modeled explicitly, it needs to make a difference in the \nreconnaissance process. Furthermore, if a detail is important for the military decision \nprocess, it needs to be part of the perception, and hence needs to be observed by \nsensors, which requires that the respective things are modeled as properties of the \nentities [125]. \n8.1.4 Shooting \nModeling the outcomes of duels between weapon systems and battles between units \nis still a topic of major interest. On the weapon system level, direct and indirect fires \nare analyzed. Direct fire means that the target is in the line of sight of the shooter. In \ncase of indirect fire systems such as Artillery and other ballistic weapons, they do not \nneed to see the target and shoot at it straight. Their weapons follow a ballistic curve \nbeing described by the term indirect fire. Many models have been developed to keep \nup with the score. For instance a game based point systems that count how often and \nwhere a target is hit and use “hit-and-kill” probabilities (which are based on real-\nworld data) to simulate hitting or missing a target  [125].  \n \n8.1.5 Communication:  \nThis core activity deals with the modeling of Communications, Command and \nControl. It ties all the earlier activities together as command and control is situated in \nthe environment and commands the entities to shoot, move, observe, and \ncommunicate. Several models of command and control in military headquarters are \ndiscussed, as more and more simulation models have to come up with decisions \nbased on available information where until recently human decision makers had to be \ninvolved. The better command and control is modeled the less military experts are \nneeded to provide a realistic training environment [125]. Based on these principles of \ncombat modeling an example model of Field Artillery is presented to explain the \napproach of composability verification using state-space analysis. Figure 60 highlights \nthe activities of combat modeling.  \n \nFigure 60: Activities of Combat Modeling \n \nSituated \nEnvironment \nMoving \nLooking  \nor Sensing \nShooting \nCommunication \n(Command & \nControl) \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 144 \n \n8.2 Field Artillery \nField Artillery (FA) is one of the indirect fire systems35 that engage the opponent \nwithout requiring line of sight between the shooting system and the target. Infantry \nuses small, medium or heavy howitzers (artillery guns) that provide fire support for \ncombat units. Similarly Navy artillery provides fire power, where missiles can be fired \non land based or sea based direct or indirect targets. The general mission of FA is to \ndestroy, neutralize or suppress the enemy by cannon, rocket, and missile fires and to \nhelp integrate all fire support assets into combined arms operations [126]. The field \nartillery system provides close support to maneuver friendly forces, counter fire and \ninterdiction as required. These fires neutralize, canalize, or destroy enemy attack \nformations or defenses; obscure the enemy’s vision or otherwise inhibit his ability to \nacquire and attack friendly targets; and destroy targets deep in the enemy rear with \nlong-range rocket or missile fires [127].  \nFA weapons are usually located in defiladed areas in order to protect them from \nenemy detection. This nature of FA gunnery makes it an indirect fire problem. \nObserved fire (the technique that solves the indirect FA gunnery problem) is carried \nout by the coordinated efforts of the Forward Observers, Head Quarter (HQ), the \nFire Direction Center (FDC), and firing sections of the firing unit (Batteries) [126]. \nFigure 61 gives an overview of the essential elements of a field artillery and the \nsituation of an indirect fire, where a forward observer spots an enemy unit and \nrequests fire support from a nearby friendly unit. It should be noted that this \nscenario is only assumed and simplified for the sake of an example, whereas the \ntoday’s state of the art of field artillery systems is much more modernized and \ntechnologically advanced.  \n \nFigure 61: Elements of Field Artliiery & Indirect Fire \n8.2.1 Simuland \nBased on Figure 61 an Indirect Fire Support scenario is considered. In this scenario \nthe enemy units are not in the line of sight of the firing units. A soldier (forward \nobserver) from the observation post observes the enemy field and detects potential \ntargets. When a target is spotted, he calls BHQ for fire support and provides the \ntarget details. BHQ requests FDC to process the target tactically & technically. In \ntactical terms, the target should be of high importance to gain tactical advantage. In \ntechnical terms the target should be in the firing range of the supporting artillery. If \n                                                 \n35 Although there are some exceptions, in which Field Artillery engages in direct fire mode \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 145 \n \nthe target is valid FDC approves the request otherwise the request is denied. If the \nrequest is approved BHQ assigns the target to the firing units (batteries). We suppose \nthat the target can be one of three types: light (e.g., camps, troops, and trucks), \nmedium (e.g., tanks, light guns) or heavy (e.g., artillery units, missile launchers). The \ntarget is assigned to one, two or three batteries respectively. This is because medium \nand heavy targets require the fire power of more than one battery for complete \ndestruction. Based on this assumption, BHQ assigns target to the batteries. Battery \ncomponents align themselves for correct orientation and elevation by computing the \ntarget’s range and bearing (angle), load appropriate ammunition and fire the round. \nWhen a Field component receives fire, and if the detonation is within a destruction \nradius then the target is said to be destroyed otherwise it is missed, as will be \nobserved by the observer, who provides this information to the BHQ. This process \nis restarted for other potential targets, until all the enemy-units are suppressed, which \nis the ultimate goal. \n \n8.2.2 Field Artillery Model \nBased on the above informal description of the simuland a Field Artillery Model is \nconstructed. There could be multiple objectives of modeling field artillery including \ntraining, exercises, weapon testing or operational optimization. The following BOM \nbased models were discovered, selected and composed with respect to the simuland. \n \nField Artillery Conceptual Model \nThe BOM based conceptual model of Field Artillery is formally defined as follows: \n \nObserver = 〈 EnT, EvT, S, AcT 〉 where: \nEnT = Observer {C0(Id), C1(Loc), C2(CurrentTarget), C3(Result)} \n \nEvT = {E0(ObserveField, Observer, Field, null), E1(TargetSpotted, Field, Observer, target), \nE2(CallForFireSupport, Observer, BHQ, currtgt), E3(RequestApproved, FDC, BHQ, Observer, null), \nE4(RequestDenied, FDC, BHQ, Observer, null), E5(Detonation, Field, Observer, detonation), \nE6(TargetDestroyed, Observer, BHQ, null), E7(TargetMissed, Observer, BHQ, null)} \n \nAct = {A0(ObserveField, Observer, Field, E0), A1(TargetSpotted, Field, Observer, E1), \nA2(CallForFireSupport, Observer, BHQ, E2), A3(RequestApproved, FDC, Observer, E3), \nA4(RequestDenied, FDC, Observer, E4), A5(Detonation, Field, Observer, E5), A6(TargetDestroyed, \nObserver, BHQ, E6), A7(TargetMissed, Observer, BHQ, E7)} \n \nS = {S0(ObserverReady, A0, S1),  S1(ObservingField, A1, S2), S2(RequestingFireSupport, A2, S3), \nS3(WaitingForReponse, {A3, S4}, {A4, S0}) , S4(WaitingForImpact, A5, S5) , S5(EvaluateDamage, {A6, \nS0}, {A7, S0}} \nTable 34: Observer Basic-BOM \n \n \n \n \n \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 146 \n \nField = 〈 EnT, EvT, S, AcT 〉 where: \nEnT = Field {C4(Id), C5(FD), C6(Impacts)} \n \nEvT = {E8(ObserveField, Observer, Field, null), E9(TargetSpotted, Field, Observer, target), E10(Fire, \nBattery1, Field, fire), E11(Fire, Battery2, Field, fire), E12(Fire, Battery3, Field, fire), E13(Detonation, \nField, Observer, Impacts), E14(UpdateField, BHQ, Field, update) } \n \nAct = {A8(ObserveField, Observer, Field, E8), A9(TargetSpotted, Field, Observer, E9), A10(Fire, \nBattery1, Field, E10), A11(Fire, Battery2, Field, E11), A12(Fire, Battery3, Field, E12), A13(Detonation, \nField, Observer, E13), A14(UpdateField, BHQ, Field, E14) } \n \nS = {S6(FieldReady, {A8, S7}, {A10, S8}, {A11, S8}, {A12, S8}),  S7(BeingObserved, A9, S6), \nS8(TakingFire, A13, S9) , S9(WaitingForUpdate, A14, S6)} \nTable 35: Field Basic-BOM \n \nBHQ = 〈 EnT, EvT, S, AcT 〉 where: \nEnT =BHQ {C7(Id), C8(Loc), C9(CurTarget), C10(TargetStatus) } \n \nEvT = {E15(CallForFireSupport, Observer, Field, target), E16(ProcessRequest, BHQ, FDC, target), \nE17(RequestApproved, FDC, BHQ, Observer, null), E18(RequestDenied, FDC, BHQ, Observer, null), \nE19(AssignTarget, BHQ, Battery1, Battery2, Battery3, assign_target), E20(FiringCompleted, Battery1,  \nBHQ, null), E21(FiringCompleted, Battery2,  BHQ, null), E22(FiringCompleted, Battery3,  BHQ, null), \nE23(TargetDestroyed, \nObserver, \nBHQ, \nnull), \nE24(TargetMissed, \nObserver, \nBHQ, \nnull), \nE25(UpdateField,  BHQ, Field, update) } \n \nAct = {A15(CallForFireSupport, Observer, Field, E15), A16(ProcessRequest, BHQ, FDC, E16), \nA17(RequestApproved, FDC, BHQ, Observer, E17), A18(RequestDenied, FDC, BHQ, Observer, E18), \nA19(AssignTarget, BHQ, Battery1, Battery2, Battery3, E19), A20(FiringCompleted, Battery1,  BHQ, \nE20), A21(FiringCompleted, Battery2,  BHQ, E21), A22(FiringCompleted, Battery3,  BHQ, E22), \nA23(TargetDestroyed, \nObserver, \nBHQ, \nE23), \nA24(TargetMissed, \nObserver, \nBHQ, \nE24), \nA25(UpdateField,  BHQ, Field, E25) } \n \nS={S10(BHQReady, A15, S11),  S11(CallFDC, A16, S12), S12(WaitingForApproval, {A17, S13}, {A18, S10}), \nS13(AssigningTarget, \nA19, \nS14), \nS14(WaitingForFire, \n{A20, \nS15}, \n{A21, \nS15}, \n{A22, \nS15}), \nS15(WaitingForDamageReport,  {A23, S16}, {A24, S16}), S16(UpdatingField, A25, S10)} \n \nTable 36: BHQ Basic-BOM \n \nFDC = 〈 EnT, EvT, S, AcT 〉 where: \nEnT =FDC{C11(Id), C12(FD) , C13(CurTarget), C14(Result)} \n \nEvT = {E26(ProcessRequest, BHQ, FDC, target), E27(RequestApproved, FDC, BHQ, Observer, \nnull), E28(RequestDenied, FDC, BHQ, Observer, null)} \n \nAct = {A26(ProcessRequest, BHQ, FDC, E26), A27(RequestApproved, FDC, BHQ, Observer, E27), \nA28(RequestDenied, FDC, BHQ, Observer, E28)} \n \nS = {S17(FDCReady,  A26, S18),  S18(Processing, {A27, S17}, {A28, S17})} \nTable 37: FDC Basic-BOM \n \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 147 \n \nBattery1,2,336 = 〈 EnT, EvT, S, AcT 〉 where: \nEnT = Battery1,2,3 {C15(Id), C16(CurTarget) } \n \nEvT = {E29(AssignTarget, BHQ, Battery1, Battery2, Battery3, assign_target), E30(Fire, Battery123, \nField, fire),  E31(FiringCompleted, Battery123,  BHQ, null)} \n \nAct = {A29(AssignTarget, BHQ, Battery1, Battery2, Battery3, E29), A30(Fire, Battery1/2/3, Field, E30),  \nA31(FiringCompleted, Battery1/2/3,  BHQ, E31)} \n \nS = {S19(ReadyToFire,  A29, S20),  S20(PreparingCannon,  A30, S21),  S21(Firing,  A31, S19)} \nTable 38: Battery (1,2,3) Basic-BOM \n \nFA = 〈 AcTIN,  AcTOUT ,  POI 〉 where: \nAcTIN = AcTOUT = ∅  \n \nPOI = { POI-0(!A0, ?A8), POI-1(!A9, ?A1), POI-2(!A2, ?A15), POI-3(!A16, ?A26), POI-4(!A27, \n{?A17, ?A3}), POI-5(!A28, {?A18, ?A4}), POI-6(!A19, ?A29), POI-7(!A30, {?A10, ?A11, ?A12}), \nPOI-8(!A31, {?A20, ?A21, ?A22}), POI-9(!A13, ?A5), POI-10(!A6, ?A23), POI-11(!A7, ?A24), POI-\n12(!A25, ?A14)} \nTable 39: Field Artillery Composed BOM \n \nFigure 62 shows the formal representation of Field artillery composed model.  \n                                                 \n36 This component has three instances. \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 148 \n \n \nFigure 62: Field Artillery Composed BOM \n \n8.2.3 Requirement Specification \nWe define Requirement speciation of the field artillery model as: \nRS0 = 〈O, S〉 where: \nObjectives O = {o1} and System Constraints S = {s1, s2 s3, s4} \no1: All the enemy units must be destroyed \ns1, 2, and 3: The model should be composable at syntactic and static-semantic level. The \nstate-machines should match and the executable mode should correctly represent the \nconceptual model.  \ns4: There should never be a friendly fire. \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 149 \n \n8.3 Verification of the FA model using CPN State-Space \nAnalysis  \nAfter the BOMs are discovered, selected and composed according to Figure 62, the \nconceptual model is ready for verification. We select CPN state-space analysis \ntechnique for its verification. \n8.3.1 Static and Dynamic Analysis \nWe assume that the model qualifies syntactic and static-semantic analysis. Also when \nit undergoes state-machine matching process it is able to make progress until the \ngoal-states are reached. Figure 63 shows how the components interact with each other \nthrough the exchange of events (horizontal arrows) due to which their state-\nmachines make progress (vertical dotted arrows). Based on the fact that the \nconstraint S1, S2 and S3a are satisfied we proceed to BOM-to-E-BOM extension \nwhich is a pre-requisite step for the transformation of conceptual model into \nexecutable model.  \n \nFigure 63: State-machine Matching of Field Artillery Model \n8.3.2 BOM to E-BOM extension \nAt this stage all the BOM components are extended to our proposed E-BOM \nextension with the help of the modeler’s input. The following tables present E-BOM \nextensions of BOMs in the FA model.  \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 150 \n \nObserver E-BOM \nSV and types {C0(Id:Integer), C1(Loc:Integer), C2(CurrentTarget:TARGET), C3(Result:Bool)} \nwhere TARGET = (Id:Integer, Grid37:Integer, Description:String) \n \nInitial States {S0:ObserverReady} \nTransitions \nState \nEvent \nGuard \n{SVIN} {SVOUT} \nAction \nNext State \nObserver \nReady \nObserve \nField \n \n \n \n \nObserving \nField \nObserving \nField \nTargetSpotted \n \n \nC2 \n \nRequesting \nFireSupport \nRequesting \nFireSupport CallForFireSupport \n \nC2 \n \n \nWaitingFor \nReponse \nWaitingFor \nReponse \nRequestApproved \n \n \nC2 \n \nWaitingFor \nImpact \nWaitingFor \nReponse \nRequestDenied \n \n \n \n \nObserver \nReady \nWaitingFor \nImpact \nDetonation \n \nC2 \nC3 \nAction1 \nEvaluate \nDamage \nEvaluate \nDamage \nTarget \nDestroyed \n[Result=true] \nC3 \n \n \nObserver \nReady \nEvaluate \nDamage \nTarget \nMissed \n[Result=false] \nC3 \n \n \nObserver \nReady \n \nAction1 { \ninput (target, detonation); \noutput (result); \naction \nlet \n val grid= #2 target; \nin \n (Destroyed(grid, detonation)) \nEnd \n} \nfun Destroyed (x, []) = false \n  | Destroyed (x, h::t) = IsDestroyed(x, h)  orelse Destroyed (x, t); \nfun IsDestroyed(grid, impact) = \nlet \nval gridst = Int.toString(grid); \nval impactst= Int.toString(impact); \nval gridX = valOf(Int.fromString(substring (gridst, 0, 3))); \nval impactX = valOf(Int.fromString(substring (impactst, 0, 3))); \nval gridY = valOf(Int.fromString(substring (gridst, 3, 3))); \nval impactY = valOf(Int.fromString(substring (impactst, 3, 3))); \nval X = abs(gridX - impactX); \nval Y = abs(gridY - impactY); \nin \nif (abs(X)<4 andalso  abs(Y)<4)  \nthen true \n else false \nend; \n \nTable 40: Observer E-BOM \n \n \nAction1 is a CPN-ML script that evaluates if the target is destroyed or not. We \nassume that the destruction radius of the rounds fired by artillery guns is 4x4 grids \ni.e. if the round hits the target within this radius it will be destroyed otherwise missed. \nNote that Action1 calls other functions which are also specified in Table 40. \n \n \n \n                                                 \n37  In military map, Grid reference system is used to identify a position of an object. We assume that \nthe grid in this scenario is of 6 figures, first three integers define Easting and the other three define \nNorthings. For details see [131] \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 151 \n \nField E-BOM \nSV:Types \n{C4(Id:Integer), C5(FD:FIELD_DATA), C6(Impacts:IMPACTS)}          \nFIELD_DATA = list (Id:Integer, Grid:Integer, Description:String, \nType:String); IMPACTS = list (Grid:Integer) \nInitial States \n{S6: FieldReady} \nTransitions \nState \nEvent \nGuard \n{SVIN} {SVOUT} \nAction \nNext State \nFieldReady \nObserveField \n \n \n \n \nBeingObserved \nBeing Observed \nTargetSpotted \n[length \nFD>0] \nC5 \nC5 \nAction2 \nFieldReady \nFieldReady \nFire \n \nC6 \n \n \nTakingFire \nTakingFire \nDetonation \n \n \nC6 \n \nWaitingFor \nUpdate \nWaitingFor \nUpdate \nUpdateField \n \nC5 \nC5 \nAction3 \nFieldReady \n \nAction2 { \ninput (fd); \noutput (target); \naction \nlet \nval indx = discrete (0,(length fd)); \nval F = List.nth(fd, indx); \nin \n((#1 F, #2 F, #3 F)) \nend } \nAction3 { \ninput(update,fd); \noutput (ufd); \naction \nlet \n val status = #2 update; \n val target = #1 update; \n val U = (#1 target, #2 target, #3 target, \"Enemy\"); \nin \n if (status=true) then \n   rm U fd \n else \n  fd \nend } \n \nTable 41: Field E-BOM \nAction2 randomly picks a target from a list of targets (Field Data) and sends it as \nparameters to the observer, simulating that the observer has spotted a target in the \nenemy area. Action3 is executed when Update-Field event is received from BHQ. This \naction removes an object if the target destroyed. \n \nBHQ E-BOM \nSV:Types \n{C7(Id:Integer),C8(Loc:Integer),C9(CurrentTarget:TARGET), C10(TargetStatus:Bool)} \nInitial States \n{S10: BHQReady } \nTransitions \nState \nEvent \nGuard \n{SVIN} \n{SVOUT} \nAction \nNext State \nBHQReady \nCallForFireSupport \n \nC9 \n \n \nCallFDC \nCallFDC \nProcessRequest \n \n \nC9 \n \nWaitingFor \nApproval \nWaitingFor \nApproval \nRequestApproved \n \nC9 \n \n \nAssigning \nTarget \nWaitingFor \nReponse \nRequestDenied \n \n \n \n \nBHQReady \nAssigning \nTarget \nAssignTarget \n \n \nC9 \nAction4 \nWaitingForFire \nWaitingForFire \nFiringCompleted \n \n \n \n \nWaitingFor \nDamageReport \nWaitingFor \nTargetDestroyed \n \nC10 \n \n \nUpdatingField \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 152 \n \nDamageReport \nWaitingFor \nDamageReport \nTargetMissed \n \nC10 \n \n \nUpdatingField \nUpdatingField \nUpdateField \n \n \nC10 \n \nBHQReady \n \nAction4 { \ninput (target); \noutput (assign_target); \naction \nlet \nin \nif ((#3 target) = \"Artillery\") then \n  ((#1 target, #2 target, #3 target, [true, true, true])) \nelse if ((#3 target)  = \"Tank\") then \n   ((#1 target, #2 target, #3 target, [true, true, false])) \nelse \n   ((#1 target, #2 target, #3 target, [true, false, false])) \nend} \n \nTable 42: BHQ E-BOM \nAction 4 is used to assign light, medium or heavy targets. If a target is heavy then all \nthree batteries are assigned to hit the target. If the target is medium then battery 1 \nand 2 are assigned otherwise only battery 1 is assigned.  \n \nFDC E-BOM \nSV:Types \n{C11(Id:Integer), C12(FD:FIELD_DATA) , C13(CurrentTarget:TARGET), \nC14(Result:Bool)} \nInitial States \n{S17: FDCReady } \nTransitions \nState \nEvent \nGuard \n{SVIN} \n{SVOUT} \nAction \nNext State \nFDCReady \nProcessRequest \n \nC12, C13, \nC14 \n \nAction5 \nProcessing \nProcessing \nRequestApproved \n[Result=true] \n \nC13, C14 \n \nFDCReady \nProcessing \nRequestDenied \n[Result=false] \n \nC13, C14 \n \nFDCReady \n \nAction5 { \ninput (target, fdcd); \noutput (fdc_result); \naction \nlet \nval tid = #1 target; \nval r = List.nth((listsub fdcd (GetFieldByID tid fdcd)),0); \nin \n(if (#4 r = \"Enemy\") then true else false) \nend \n} \n \nTable 43: FDC E-BOM \n \nAction 5 is used to process targets. Here we only check from the internal FDC data \nthat the target is an enemy unit. This method can be expanded to compute target \npriorities and process other tactical and technical fire direction rules.  \n \n \n \n \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 153 \n \nBattery E-BOM \nSV:Types \n{C15(Id:Integer), C16(CurrentTarget:TARGET) } \nInitial States \n{S19: ReadyToFire } \nTransitions \nState \nEvent \nGuard {SVIN} {SVOUT} Action \nNext State \nReadyToFire \nAssignTarget \n \nC16 \n \n \nPreparingCannon \nPreparingCannon \nFire \n \n \nC16 \nAction6 \nFiring \nFiring \nFiringCompleted \n \n \n \n \nReadyToFire \nAction6 { \ninput (assign_target); \noutput (fire); \naction \nlet \nval bid = inst(); \nval tid = #1 assign_target; \nval grid = #2 assign_target; val impact=grid; \nin \n(bid, tid, grid, impact) \nend \n   } \n \nTable 44: Battery E-BOM \n \nAction 6 is used to initiate the fire. It creates a token of type “Fire” to the Field \ncomponent containing the information of the firing battery id, target id, grid location \nof the target and the location of the impact.  \n \n8.3.3 E-BOM to CPN Transformation \nIn this step, all the extended BOM models (E-BOM) are automatically transformed \ninto our proposed CPN components in such a way that all variables from the \ncorresponding E-BOMs are added in the Structural Layer (shown in Red color in the \nfollowing figures) and the State-machine is transformed into the Behavioral Layer \n(shown in Green color). In communication layer (shown in blue color), receive-\nevents are transformed into input ports and send-events are converted into output \nports. Figure 64Figure 65Figure 66Figure 67Figure 68 represent the CPN component \nmodels of each component: \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 154 \n \n \nFigure 64: Observer CPN Component \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 155 \n \n \nFigure 65: Field CPN Component \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 156 \n \n \nFigure 66: BHQ CPN Component \n \n \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 157 \n \n \nFigure 67: Battery CPN Component \n \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 158 \n \n \nFigure 68: FDC CPN Component \n \nWe assume that each transformed CPN component has passed structural evaluation \nwhich is conducted using inspection method and behavioral evaluation conducted \nusing Functional Testing method therefore S3b is also partially satisfied.  \n \n8.3.4 Composition of CPN Components  \nIn this step all CPN modules are combined together through socket places in a CPN \nComposed Model as shown in Figure 69. In this composed model some general \npurpose auxiliary components are also introduced such as Join and Fork to facilitate \nthe composition. \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 159 \n \n \nFigure 69: Field Artillery CPN Composed Model \n \nWhen the model is composed it is executed in the CPN execution environment. The \nsuccessful execution of the model (according to Figure 63) satisfies S3b completely.  \n \n8.3.5 State space Analysis  \nIn the next step the state-space of the entire Field Artillery Model is generated using \nCPN state-space calculation tool, and is used to perform verification. The generated \nstate-space graph consists of 1960 nodes and 6469 arcs as shown in Figure 70. \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 160 \n \n \nFigure 70: State space of Field Artillery CPN Model (1960 nodes, 6469 edges) \n \nAfter the state-space is constructed in CPN tools, it is exported into a GraphML file \nformat. It is to be noted that the layout of the state-space graph in Figure 70 is \nrendered using Gephi Tool. In that “node-1” represents initial marking of the \ncomposed model whereas “node-1956” represents the goal state (explained later in \nthis section). Shades of green color (from dark to light) represent proximity from \nnode 1. All the nodes are connected with edges (some of which may not be visible \ndue to light colors). \n \n \n \n \n \n \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 161 \n \nTranslation of Requirements specification into CPN Properties \nTo proceed with the verification we first translate Objectives and Constraints defined \nin the requirement specification to CPN properties. We assume that the default \nconstraints S1, S2 and S3 are already verified. \nObjective \nO1: All the enemy units must be destroyed \nCPN Translation \nAs the Observer detects enemy units, therefore we say that if no more \nenemy units can be detected (because field-data is empty) then all the \nenemies should be destroyed. Therefore a marking where TS (Target-\nspotted) place has a null token should exist. If such marking is found then \nthe objective is said to be reached. The following CPN function can be \nused to verify this property. \nCPN Function \nfun AllTargetDestroyed() = \nlet \n  val token = 1`(0,0,\"\"); /*Create a search criteria */ \n  val predicate = fn n => (Mark.Main'TS 1 n) =  token; /*Create a predicate \nfunction*/ \n  val TS = PredAllNodes (predicate);  /* Built-in Node search function \nin \n if (length TS > 0)  \n     then true \n else false \nend; \nResult \nWhen the function AllTargetDestroyed() is executed it returns True. This is \nalso evident from Figure 70 where the marking 1956 represents the goal-\nstate and is reachable form the initial state 1. \n \nConstraint \nS4: There should never be a friendly fire. \nCPN Translation \nWhen “UpdateField” (UF) place gets a token from BHQ component \n(which will be taken as input by the Field component), it shows which \nfield unit is destroyed. We can collect all such nodes in the state-space \n(where UF field has tokens) and compare that all field units that have \nbeen destroyed are of type “enemy”. If this condition holds in the entire \nstate-space then S4 holds. Following CPN-ML function can be used to \ncheck if friendly fire has ever happened or not. The result should be false \nto satisfy S4 \nCPN Function \nfun CheckFriendlyFire() = \nlet \n  val predicate = fn n => IsNotEnemy(Mark.Main'UF 1 n) = true;  \n  val ListOfFrieldlyUnits = PredAllNodes (predicate);   \nin \n if (length ListOfFrieldlyUnits > 0) /* Means there is a friendly fire */ \n     then true \n else false \nend; \nfun IsNotEnemy (update) =  \nlet \n  val upd:UPDATE = List.nth(update, 0); \n  val target = #1 upd; /*Extract information from the token at the place: UF */ \nin \n   if GetType(#1 target) <> \"Enemy\" then  true /* Checks field unit type*/ \nelse  \n   false \nend; \nResult \nThe function CheckFriendlyFire() results false because no such incidence \noccurred with the data (initial state) provided to the model.  \nTo check that this function works correctly we created a counter example \nin which FDC component is assumed to be erroneous (i.e. it wrongly \naccepts fire support requests of the friendly units), we ran the routine and \nfound traces of the occurrence of friendly fire. \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 162 \n \nAs all the constraints and Objectives are satisfied we say that the field artillery model \nis composable at all levels and is verified with respect to its given specifications.  \nTherefore the BOM based composed model is qualified for further implementation. \n \n8.4 State Space Reduction \nIn this section the application of our proposed state-space reduction technique is \npresented. In order to proceed with the state-space reduction of the FA model \ngenerated by CPN tools we perform following steps. \n \n1. Trimming Node Description \nEach node in the CPN state-space graph has a description. This description \nessentially tells about the presence of tokens (or multiple tokens) in all places of the \nmodel, which is called marking. This description is very lengthy if the model has \nmany places or sub-models. In this step we remove all the descriptions and only keep \nthe information related to the places of the main model. To perform this step we use \nthe library function NodeDescriptorOptions() (see manual [73]). When the \ndescriptions are trimmed we will only get information of a node pertinent to the \nmain places otherwise it will be a “Null” string. (This is an important difference for \nfurther steps). \nConceptually we hypothesize that trimming the node description does not cause loss \nof information because all the information other than the one in the main places is \nproduced by the internal logic of the composed components. Since the composed \ncomponents are considered as black boxes and they will eventually output important \ninformation (in form of tokens) in any of the main places. This information would be \nsufficient to answer any verification query related to the model under consideration. \n \n \n2. Export to GraphML \nIn the next step we export the state-space graph to an external file. Since CPN state-\nspace graph cannot be manipulated internally within the CPN environment therefore \nwe export the graph to a standard GraphML format [128] along with the trimmed \nnode descriptions and the information of the edges. To perform this step we develop \na GraphML writer function in CPN-ML.  \n \n3. Reduction Algorithm \nIn the next step, we apply our reduction algorithm specified in Table 16. This \nalgorithm is implemented in a Java application which uses JUNG library for graph \nmanipulation functions. In brief, all the nodes which have “Null” descriptions are \nremoved (because they are irrelevant). When a node is removed all its incoming and \noutgoing edges are removed. So we connect each predecessor of the node with each \nsuccessor to preserve the structure of the graph. When all the nodes are checked the \nreduction is completed. The output of the reduce graph of Field Artillery mode is \nshown in Figure 71.  \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 163 \n \n \nFigure 71: Reduced State-Space graph of Field Artillery Model \n \nIn Figure 71 node-2 represents initial marking (note that node-1 was trimmed in the \nreduction process). Also Node-1956 still represents goal state. (Note that the nodes \nIDs remain the same in the reduction process).  \n \n \nReduced Graph Original Graph Percentage \nNodes 428 \n1960 \n21% \nEdges 2503 \n6469 \n38 % \nTable 45: Reduction Statisitics \nTable 45 shows that the nodes are reduced to 21% and the edges are reduced to 38% \nof the original graph.  \n \n \n\nChapter 8  \n \nModel Verification using State-space Analysis techniques \n   \nPage 164 \n \n8.5 Summary \nIn this chapter the verification of BOM based composed models is discussed using \nCPN based state-space analysis technique. An example model of Field Artillery is \nintroduced and the entire verification process is applied on this model. It is shown \nhow requirement specifications are translated into CPN properties and how they are \nverified using state-space analysis and Query functions.  \n \nState-space analysis is advantageous as it is exhaustive and leads the modeler to all of \nthe possibilities that can occur during the abstract level execution of a composed \nmodel. A state-space graph helps to study all of these possibilities and to understand \nthe dynamic behavior of the components in detail. Also a state-space query functions \nproposed along with the approach help in answering different verification questions \nand evaluate the correctness of model with respect to the requirements. This \napproach however is vulnerable to the state-space explosion as for a simple model of \nField Artillery 2503 nodes and 6469 edges were formed. To deal with this situation \nwe proposed an effective state-space reduction technique which not only reduces \nlarge state-spaces into reasonable size but also preserves important information for \ncorrect verification. We demonstrated how our proposed state-space reduction \ntechnique is applied to the Field Artillery model for proof of concept. \n \n \n\n   \nPage 165 \n \nChapter 9 \nModel Verification using CSP based \nModel Checking Technique \n \nModel Checking is becoming a standard approach for the software verification due to its numerous \nadvantages over traditional formal methods. Communicating Sequential Processes (CSP) is an event \nbased formal language for describing patterns of interactions in concurrent systems and very useful for \nconcurrent behavioral specification and verification due to its theoretical foundations of process \nalgebra (also called process calculi). The application of CSP based Model Checking technique in the \nComposability verification also proves to be very useful, especially with a focus on the dynamic \nsemantic composability level. In this chapter the Field Artillery Model presented in Chapter 8 is \nreused and extended with information to capture the behavior of a real-time probabilistic system. It is \nshown how the Probabilistic-Timed Field Artillery Model is transformed into a composed CSP \nmodel and verified using PAT. \n \nIn this chapter, the modified version of Field Artillery Model presented in chapter 8 \nis discussed as an example. The objective of this example is to represent a model of a \nsystem with time constraints and probabilistic behavior. To the best of our \nknowledge the PN algebraic approach does not support verification of the timed \nmodels or probabilistic systems at all. Also the CPN based approach has a limited \nsupport for the verification of timed system but it does not cover probabilistic \nsystems. We therefore propose to apply Modeling Checking for real-time \nprobabilistic systems and show how a composed model of one such system can be \nverified using a CSP based Model checker called PAT (see 3.2.7). \n9.1 \nField Artillery Scenario \nThe scenario of the Field Artillery Model is slightly different.  It is assumed that a \nsoldier observes the field and detects enemy units. When a target is spotted, he calls \nBHQ for fire support and provides the target details. In military practice, Time-On- \nTarget (TOT) is a Field Artillery coordination protocol observed by multiple firing \nunits. This technique was developed by the U.S. Army during World War II. It uses a \nprecise pre-determination of the estimated preparation time and the time of flight of \nthe munitions from each firing battery to the target area. When a Time on Target \n(TOT) is designated each battery that will join in firing on that target subtracts the \ntime of flight from the TOT to determine the time to fire. The firing units fire their \nrounds so that all the munitions arrive at the target at precisely the same time. This is \ndone in order to achieve maximum target destruction. If there is a gap between the \nmultiple impacts the enemy soldiers get time to prone or takeover in the hideouts \nand mobile vehicles can escape [129].  \nBHQ assigns target to the batteries, and also schedules a certain “TOT” for the \nbatteries to comply. Each battery needs some time to prepare for loading appropriate \nammunition and setting up the correct alignment and orientation of the barrel \naccording to the computed firing solution using range (distance) and bearing (angle) \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 166 \n \nof the assigned target. It is assumed that each battery needs a random preparation \ndelay. When each battery is ready, it will fire in its own time such that all the rounds \nhit the target at the given TOT. We also assume that the probability of hitting on the \nexact target location for each battery is ‘0.9’. In contrast to the previous scenario in \nchapter 8, we assume that there is only one target in the field component and all \nthree batteries are taking part in the firing operation. To construct a conceptual \nmodel for this scenario, the following BOM components are composed: \nField:   \nTarget location (We assume there is only one target). \nObserver:  \nA soldier who request for the fire support from BHQ.  \nBHQ:  \nSupervises the entire operation of fire support, responds to the calls \nfor fire support and assigns targets to the batteries.  \nBattery:  \nThree units of artillery batteries (cannons and crew) responsible to hit \nthe target exactly at a given time.  \n(Note that FDC component is removed from the composition. Also some entity \ncharacteristics and event parameters are reduced for simplification). The modified \nBOM components of the Field Artillery conceptual model are formally defined as \nfollows: \n \nObserver = 〈 EnT, EvT, S, AcT 〉 where: \nEnT = Observer { C0(target)} \n \nEvT = {E0(CallForFireSupport, Observer, BHQ, target),  E1(Detonation, Field, Observer, \ndetonation)} \n \nAct = {A0(CallForFireSupport, Observer, BHQ, E0), A1(Detonation, Field, Observer, E1)} \n \nS = {S0(ObserverReady, A0, S1),  S1(WaitingForImpact, A1, S0) } \nTable 46: Observer Basic-BOM \n \nField = 〈 EnT, EvT, S, AcT 〉 where: \nEnT = Field {C1(destruction[3])} \n \nEvT = {E2(Fire, Battery1, Field, BID), E3(Fire, Battery2, Field, BID), E4(Fire, Battery3, Field, BID), \nE5(Detonation, Field, Observer, destruction)} \n \nAct = {A2(Fire, Battery1, Field, E2), A3(Fire, Battery2, Field, E3), A4(Fire, Battery3, Field, E4), \nA5(Detonation, Field, Observer, E5) } \n \nS = {S2(FieldReady, {A2, S3}+{A3, S3}+{A4, S3}) , S3(TakingFire, A5, S2)} \nTable 47: Field Basic-BOM \n \n \n \n \n \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 167 \n \nBHQ = 〈 EnT, EvT, S, AcT 〉 where: \nEnT =BHQ {C2(TOT) } \n \nEvT = {E6(CallForFireSupport, Observer, Field, target), E7(AssignTarget, BHQ, Battery1, Battery2, \nBattery3, TOT), E8(FiringCompleted, Battery1,  BHQ, null), E9(FiringCompleted, Battery2,  BHQ, \nnull), E10(FiringCompleted, Battery3,  BHQ, null} \n \nAct = {A6(CallForFireSupport, Observer, Field, E6), A7(AssignTarget, BHQ, Battery1, Battery2, \nBattery3, E7), A8(FiringCompleted, Battery1,  BHQ, E8), A9(FiringCompleted, Battery2,  BHQ, E9), \nA10(FiringCompleted, Battery3,  BHQ, E10)} \n \nS={S4(BHQReady, A6, S5),  S5(AssigningTarget, A7, S6), S6(WaitingForFire, {A8, S4} + {A9, S4} + \n{A10, S4})} \nTable 48: BHQ Basic-BOM \n \nBattery1,2,3 = 〈 EnT, EvT, S, AcT 〉 where: \nEnT = Battery1,2,3 { C3(BID), C4(Destroyed) } \n \nEvT = {E11(AssignTarget, BHQ, Battery1, Battery2, Battery3, TOT), E12(ReadyToFire, Battery1/2/3, \nBattery1/2/3, null), E13(Fire, Battery123, Field, BID, Destroyed),  E14(FiringCompleted, Battery123,  \nBHQ, null)} \nAct = {A11(AssignTarget, BHQ, Battery1, Battery2, Battery3, E11), A12(ReadyToFire, Battery1/2/3, \nBattery1/2/3, E12),  A13(Fire, Battery1/2/3, Field, E13),  A14(FiringCompleted, Battery1/2/3,  BHQ, \nE14)} \n \nS = {S7(BatteryIdle,  A11, S7),  S8(Preparing,  A12, S9),  S9(ReadyToFire,  A13, S10), S10(Firing,  A14, S7)} \nTable 49: Battery (1,2,3) Basic-BOM \n \nFA = 〈 AcTIN,  AcTOUT ,  POI 〉 where: \nAcTIN = AcTOUT = ∅  \nPOI = { POI-0(!A0, ?A6), POI-1(!A7, ?A11), POI-2(!A12), POI-3(!A13, {?A2, ?A3, ?A4}), POI-\n4(!A14, {?A8, ?A9, ?A10}), POI-5(!A5,?A1} \nTable 50: Field Artillery Composed BOM \n \nThe composed field artillery model is shown in Figure 72 using our proposed \ngraphical notation. \n \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 168 \n \nObserver\nS1\nObserver\nReady\nWaiting\nforImpact\nS0\nCharacteristics:\nC0 = target : Integer\nActions:\nA0 = CallForFireSupport\nA1 = Detonation\nStates:\nS0 = ObserverReady\nS1 = WaitingForImpact\nC0\nz\nField\nA2\nS2\nFieldReady\nTakingFire\nA1\nS3\nCharacteristics:\nC1 = destruction : Boolean\nActions:\nA2 = Fire\nA3 = Fire\nA4 = Fire\nA5 = Detonation\nStates:\nS2 = FieldReady\nS3 = TakingFire\nC1\nA3\nA4\nBHQ\nA8\nS6\nWaitingForFire\nBHQReady\nA6\nS4\nCharacteristics:\nC2 = TOT : Integer\nActions:\nA6=CallForFireSupport\nA7=AssignTarget\nA8=FiringCompleted\nA9=FiringCompleted\nA10=FiringCompleted\nStates:\nS4=BHQReady\nS5=AssigningTarget\nS6=WaitingForFire\nC2\nA9\nA10\nA7\nS5\nAssigningTarget\nBattery1\nA14\nS9\nReadyToFire\nBatteryIdle\nA11\nS7\nCharacteristics:\nC3=BID:Integer=1\nC4=Destroyed:Boolean\nActions:\nA11=AssignTarget\nA12=ReadyToFire\nA13=Fire\nA14=FiringCompleted\nStates:\nS5=BatteryIdle\nS6=Preparing\nS7=ReadyToFire\nS8=Firing\nC3, C4\nA13\nA12\nS8\nPreparing\nS10\nFiring\nBattery2\nA14\nS9\nReadyToFire\nBatteryIdle\nA11\nS7\nCharacteristics:\nC3=BID:Integer=1\nC4=Destroyed:Boolean\nActions:\nA11=AssignTarget\nA12=ReadyToFire\nA13=Fire\nA14=FiringCompleted\nStates:\nS5=BatteryIdle\nS6=Preparing\nS7=ReadyToFire\nS8=Firing\nC3, C4\nA13\nA12\nS8\nPreparing\nS10\nFiring\nBattery3\nA14\nS9\nReadyToFire\nBatteryIdle\nA11\nS7\nCharacteristics:\nC3=BID:Integer=1\nC4=Destroyed:Boolean\nActions:\nA11=AssignTarget\nA12=ReadyToFire\nA13=Fire\nA14=FiringCompleted\nStates:\nS5=BatteryIdle\nS6=Preparing\nS7=ReadyToFire\nS8=Firing\nC3, C4\nA13\nA12\nS8\nPreparing\nS10\nFiring\nA0\nA1\n \nFigure 72: Field Artillery Composed Model \n9.2 Requirement Specification \nWe define Requirement speciation of the modified field artillery model as: \n \nRS0 = 〈O, S〉 where: \nObjectives O = {o1, o2} and System Constraints S = {s1, s2 s3, s4} \no1: All the firing units should fire precisely at the target location \no2: All the firing units should fire at the target exactly at the given time (i.e., the Time \non Target property should be satisfied) \n \ns1, 2, and 3: The model should be composable at syntactic and static-semantic level. The \nstate-machines should match and the executable mode should correctly represent the \nconceptual model.  \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 169 \n \n9.3 Verification using Model Checking  \nAfter the BOM are discovered, selected they are composed to form a conceptual \nmodel according to the simuland. This composed model is now ready for \nverification. At this stage we select model checking technique for its verification. \n9.3.1 Static and Dynamic Analysis \nWe assume that the model qualifies syntactic and static-semantic analysis. Also when \nit undergoes state-machine matching process it is able to make progress until the \ngoal-states are reached. Figure 73 shows the interaction of the state-machine of each \ncomponent during the state-machine matching process. \nBased on the fact that the constraint S1, S2 and S3a are satisfied we proceed to \nBOM-to-E-BOM extension. \nObserver\nBHQ\nField\nObserver\nReady\nWaitingFor\nImpact\nField\nReady\nTaking\nFire\nBHQ\nReady\nAssigning\nTarget\nWaiting\nForFire\nBATTERY\n(1,2,3)\nIdle\nPreparing\nFiring\nCallForFireSupport\nAssignTarget\nFire (1,2,3)\nFiringCompleted \n(1,2,3)\nDetonation\nReadytoFire\n \nFigure 73: State-machine Matching of Field Artillery Model \n9.3.2 BOM to E-BOM extension \nAt this stage all the BOM components are extended to our proposed E-BOM \nextension with the help of the modeler’s input. Here additional information such as \ntiming constraints and probabilistic factors are proposed to be included. Following \ntables present E-BOM extensions of BOMs in the FA model.  \nObserver E-BOM \nSV and types \n{C0(Target:Integer} \nInitial States \n{S0:ObserverReady} \nTransitions \nState \nEvent \nTime \nGuard \nAction \nNext State \nObserver \nReady \nCallForFireSupport \n \n \n \nWaitingFor \nImpact \nWaitingFor \nImpact \nDetonation \n \n  \n \nObserver \nReady \n \nTable 51: Observer E-BOM \n \n \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 170 \n \nField E-BOM \nSV and types {C1(Firing_Result_Of_Battery1:Boolean},  \n{C1(Firing_Result_Of_Battery2:Boolean}, \n{C1(Firing_Result_Of_Battery3:Boolean} \nInitial States \n{S2:FieldReady} \nTransitions \nState \nEvent \nTime \nGuard \nAction \nNext State \nFieldReady \nFire(1) \n \n \nAction1 \nTakingFire \nFire(2) \n \n \nAction2 \nFire(3) \n \n \nAction3 \nTakingFire \nDetonation \n \n  \n \nFieldReady \n \n   /* A CSP script for defining a probabilistic action, with 95% chance that the \ntarget will be destroyed when an event fire is received from battery1 and 5% chance \nthat the target will be missed */ \nAction1{      \npcase{        \n               [0.05] : fire?1 → atomic{tau{ Destruction [0]=False;} → Skip} \n               default : fire?1→ atomic{tau{ Destruction [0]=True;}→ Skip} \n           } \n \n} \n/* From battery 2 */ \nAction2{      \npcase{        \n               [0.05] : fire?2 → atomic{tau{ Destruction [1]=False;} → Skip} \n               default : fire?2→ atomic{tau{ Destruction [1]=True;}→ Skip} \n           } \n \n} \n/* From battery 3 */ \nAction3{      \npcase{        \n               [0.05] : fire?3 → atomic{tau{ Destruction [2]=False;} → Skip} \n               default : fire?3→ atomic{tau{ Destruction [2]=True;}→ Skip} \n           } \n \n} \n \nTable 52: Field E-BOM \n \n \nBHQ E-BOM \nSV and types \n{C2(TOT:Integer} \nInitial States \n{S4: BHQReady } \nTransitions \nState \nEvent \nTime \nGuard \nAction \nNext State \nBHQReady CallForFireSupport \n \n  \n \nAssigningTarget \nAssigningTarget \nAssignTarget \n \n \n \nWaitingForFire \nWaitingForFire \nFiringCompleted(1) \n \n \n \nBHQReady \nFiringCompleted(2) \n \n \n \nFiringCompleted(3) \n \n \n \n \nTable 53: BHQ E-BOM \n \n \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 171 \n \nBattery(1,2,3) E-BOM \nSV and types \n{C3(BID:Integer} \nInitial States \n{S7: BatteryIdle } \nTransitions \nState \nEvent \nTime \nGuard \nAction \nNext State \nBatteryIdle \nAssignTarget \n \n  \n \nPreparing \nPreparing \nreadytofire \nWait[Prob] \n \nAction1 ReadyToFire \nReadyToFire \nFire \n \n \n \nFiring \nFiring \nFiringCompleted \n \n \n \nBatteryIdle \n \n/* A CSP script for defining a probabilistic wait action, with 94% chance that each \nbattery will launch the fire exactly at given time on target, and 6% chance that  it \nwill fire earlier or later */ \nAction1{      \npcase{ \n \n[0.01] :  Wait[TOT-3]; readytofire → ReadyToFire(i) \n \n[0.01] :  Wait[TOT-2]; readytofire → ReadyToFire(i) \n \n[0.01] :  Wait[TOT-1]; readytofire → ReadyToFire(i) \n \n[0.94] :  Wait[TOT];     readytofire → ReadyToFire(i) \n \n[0.01] :  Wait[TOT+1]; readytofire → ReadyToFire(i) \n \n[0.01] :  Wait[TOT+2]; readytofire → ReadyToFire(i) \n \n[0.01] :  Wait[TOT+3]; readytofire → ReadyToFire(i) \n \n}; \n \n} \n \nTable 54: BHQ E-BOM  \n9.3.3 E-BOM to CSP# Transformation \nIn this step, all the probabilistic timed extended BOM models are automatically \ntransformed into CSP# using our automatic BOM-to-CSP transformation tool. \nFigure 74 shows the global code block which is used to define global variables and the \ncommunication channels for each BOM send-receive event pair.  \n \n//------------Global Block ---------------------------------- \n#define TOT 30; //Constant pre-defined Time on Target \nenum {Hit, Miss}; //Hit or Miss flag \n//Each battery has a hit/miss ratio = 95:5 % \nvar Firing_Result_Of_Battery1=Miss; \nvar Firing_Result_Of_Battery2=Miss; \nvar Firing_Result_Of_Battery3=Miss; \n \n//For each event a channel is defined \nchannel callforfire 0; \nchannel detonate 0; \nchannel assigntarget 0; \nchannel firingcomplete 0; \nchannel fire 0;  \n \nFigure 74: Global code Block of Field Artillery Model \n \n \n \n \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 172 \n \nFigure 75 shows the CSP code of Observer BOM component. The send-events are \ntransformed into channels with send operator ‘!’ and receive-events are transformed \ninto channels with receive operator ‘?’. \n \n \n//=========================================================== \n//OBSERVER Component \n//=========================================================== \n \nObserverSM = ObserverReady(); //Initial State \n \nObserverReady()= \n (callforfire!0 ->WaitingForImpact()); \n \nWaitingForImpact()= \n (detonate?0 -> ObserverReady()); \n \n//=========================================================== \nFigure 75: CSP representation of Observer Component \n \n \n \n//=========================================================== \n//BHQ \n//=========================================================== \nBHQSM = BHQReady();//Initial State \n \nBHQReady()= \n(callforfire?0 ->AssigningTarget()); \n \nAssigningTarget()= \n(assigntarget!> assigntarget!2 -> assigntarget!3 ->   \n \nWaitingforfire()); \n//Sending assigntarget to multiple recievers \n \nWaitingforfire()= \n \nfiringcomplete?>firingcomplete?2->firingcomplete?3-> \n \n \nBHQReady(); \n//Recieving firingcomplete from multiple senders \n \n \n//=========================================================== \nFigure 76: CSP representation of BHQ Component \n \n \n \n \n \n \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 173 \n \n//=========================================================== \n//BATTERY i={1,2,3} \n//=========================================================== \nBatterySM(i) = BatteryIdle(i); //Initial State \nBatteryIdle(i)= \n(assigntarget?i->Preparing(i)); \nPreparing(i)= pcase{ \n \n \n[0.01] :  Wait[TOT-3]; readytofire-> ReadyToFire(i) \n \n \n[0.01] :  Wait[TOT-2]; readytofire-> ReadyToFire(i) \n \n \n[0.01] :  Wait[TOT-1]; readytofire-> ReadyToFire(i) \n \n \n[0.94] :  Wait[TOT];   readytofire-> ReadyToFire(i) \n \n \n[0.01] :  Wait[TOT+1]; readytofire-> ReadyToFire(i) \n \n \n[0.01] :  Wait[TOT+2]; readytofire-> ReadyToFire(i) \n \n \n[0.01] :  Wait[TOT+3]; readytofire-> ReadyToFire(i) \n \n \n}; \n// TOT is a global constant \n//readytofire is an internal event \n \n \nReadyToFire(i)= fire!i->Firing(i);  \nFiring(i)= firingcomplete!i ->BatteryIdle(i); \n//=========================================================== \nFigure 77: CSP representation of Battery Component \n \n//=========================================================== \n//FIELD Component \n//=========================================================== \nFieldSM = FieldReady(); //Initial State \n \nFieldReady()=  \n \n \n  \n \npcase{        \n  \n[0.05]: fire?1 ->  \n    \n \natomic{tau{Firing_Result_Of_Battery1=Miss;} -> Skip} \n default : fire?1 ->  \n         \natomic{tau{Firing_Result_Of_Battery1=Hit;} -> Skip} \n } \n|||  \n/* ||| is the interleaving operator between the synchronizing events \nfire(1), fire(2) and fire(3) */ \n \npcase{        \n  \n \n[0.05]: fire?2 ->  \n    \n \n \natomic{tau{Firing_Result_Of_Battery2=Miss;} -> Skip} \n  \n \ndefault : fire?2 ->  \n         \n \natomic{tau{Firing_Result_Of_Battery2=Hit;} -> Skip} \n } \n||| \n \npcase{        \n  \n \n[0.05]: fire?3 ->  \n    \n \n \natomic{tau{Firing_Result_Of_Battery3=Miss;} -> Skip} \n  \n \ndefault : fire?3 ->  \n        \n \n atomic{tau{Firing_Result_Of_Battery3=Hit;} -> Skip} \n }; \n \n \n/* This code randomly sets hit or miss effect for the firing of each \nbattery */ \n \nDetonation(); \nDetonation()= detonate!0 -> FieldReady(); \n//=========================================================== \n \nFigure 78: CSP representation of Field Component \n \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 174 \n \n// FIELD ARTILLERY COMPOSED MODEL \n//====================================================================== \n \nFieldArtillery = ObserverSM || BHQSM || FieldSM || BatterySM(1)|| \nBatterySM(2)|| BatterySM(3) \n \n// || is the parallel operator between all the components \n// BatterySM has three instances initialized with battery id parameter. \n \nFigure 79: Field Artillery Composed Model \n \nFigure 79 shows the CSP representation of how the transformed components are \ncomposed using the parallelism operator ‘||’. This means that all the components \nexecute in parallel, however they perform barrier synchronization while exchanging \nevents in their respective communication channels.  \n \n9.3.4 Model Checking of Field Artillery Model \nThe CSP based Field Artillery Model can be opened and executed in PAT tool. A \nsuccessful compilation of this model shows that it has no errors. When this model is \nexecuted, and if each component reaches its final states then we say that the \nconstraint S3b of requirement specification is satisfied i.e., the transformed \nexecutable model correctly represents the behavior of its conceptual model.  \nIn the verification process, we define the following assertions to be verified by PAT \nbuilt-in model checker. Since the nature of the input model is probabilistic and real-\ntime, we use Probabilistic-Real-Time module of the PAT tool.   \nFigure 80 shows how we define goal reachability assertions using PAT’s Probabilistic \nCSP LTL specification.  \n \n//=========================================================== \n// FIELD ARTILLERY COMPOSABILITY VERIFICATION \n//=========================================================== \n \n// ASSERT1: Goal state Reachability \n#assert FieldArtillery |= [](callforfire.0 -> <>detonate.0); \n \n// Goal Definition \n#define goal (Firing_Result_Of_Battery1==Hit  \n&& Firing_Result_Of_Battery2==Hit  \n&& Firing_Result_Of_Battery3==Hit); \n \n//ASSERT2: //Goal Reachability \n#assert FieldArtillery |= <>goal with prob; \n \nFigure 80: Field Artillery Verificataion Assertions \n \nAssertion1 uses LTL construct to verify that if there is a “callforfire” then detonation \nat the target location will eventually occur. If assertion1 is satisfied, it shows that \nthere exists a valid execution path, which leads to the goal state.   \n \n \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 175 \n \nThe result of PAT model checker is shown in Figure 81 which shows that the goal \nstate is reachable.  \n********Verification Result******** \nThe Assertion (FieldArtillery() |= []( callforfire.0-><> detonate.0)) is VALID. \n \n********Verification Statistics******** \nVisited States:160477 \nTotal Transitions:475422 \nTime Used:8.8263759s \nEstimated Memory Used:111668.696KB \nFigure 81: Verification Result of assertion 1 \n \nAssertion2 uses an LTL construct to verify that the “goal” is eventually reachable \nwhere “goal” is defined as a condition that all the batteries successfully hit at the \nexact location of the target. Note that assertion2 uses “with prob” construct, which \nmakes it a PLTL statement. Figure 82 shows the verification result which means that \nthe probability of reaching the goal is between 77% and 94%. \n********Verification Result******** \nThe Assertion (FieldArtillery() |= <> goal with prob) is Valid with Probability [0.77378, 0.94526]; \n \n********Verification Statistics******** \nVisited States:84019 \nTotal Transitions:245561 \nMDP Iterations:63123 \nTime Used:5.5017483s \nEstimated Memory Used:97028.192KB \nFigure 82: Verification result of assertion 2 \n \nNow we check whether the goal is reachable within the time constraints defined by \nTime-On-Target property. To perform this evaluation we use the PAT’s deadline \noperator as shown in Figure 83. We define three assertions: Early, Exactly and Late. \n//=========================================================== \n// FIELD ARTILLERY COMPOSABILITY VERIFICATION \n//=========================================================== \n \n//Goal Reachability with TOT constraint \nEarly = FieldArtillery deadline[TOT-3]; \nExactly = FieldArtillery deadline[TOT]; \nLate = FieldArtillery deadline[TOT+3]; \n \n \n//ASSERT3: //Goal Reachability at TOT \n#assert Early reaches goal with prob; \n#assert Exactly reaches goal with prob;  \n#assert Late reaches goal with prob; \n \nFigure 83: Field Artillery Verificataion Assertions with TOT \n \n \n \n\nChapter 9  \n \nModel Verification using CSP based Model Checking Technique \n   \nPage 176 \n \nThe verification result of assertion 3 is shown in Figure 84 according to which the \nearly reachability of the goal is impossible. Whereas the maximum probability of \nreaching the goal exactly on TOT is 86% which satisfies objectives O1 and O2 \nHowever the maximum probability of reaching goal at a later time is 94% which \nsatisfies O1 with a higher probability but does not satisfy O2.  \n \n********Verification Result******** \nThe Assertion (Early() reaches goal with prob) is NOT valid. \n \n********Verification Statistics******** \nVisited States:58414 \nTotal Transitions:146548 \nMDP Iterations:2557 \nTime Used:4.46633s \nEstimated Memory Used:69901.84KB \n \n********Verification Result******** \nThe Assertion (Exactly() reaches goal with prob) is Valid with Probability [0, 0.86271]; \n \n********Verification Statistics******** \nVisited States:169998 \nTotal Transitions:342091 \nMDP Iterations:28115 \nTime Used:9.6064619s \nEstimated Memory Used:148703.552KB \n \n********Verification Result******** \nThe Assertion (Late() reaches goal with prob) is Valid with Probability [0, 0.94526]; \n \n********Verification Statistics******** \nVisited States:274934 \nTotal Transitions:584498 \nMDP Iterations:112797 \nTime Used:15.4390606s \nEstimated Memory Used:232989.792KB \n \nFigure 84: Verification result of assertion 3 \n \nBased on the verification results, we can say that the field artillery model satisfies it’s \ngiven requirements with a certain probability factor. Since it is a non-deterministic \nmodel, the reliability of the success depends on the threshold between how tight the \nTime-On-Target deadline is that BHQ can assign and how efficiently the batteries \ncan prepare and how accurately they can fire on the target. \n9.4 Summary \nIn this chapter the model checking approach is presented with an example and \nverified using Process Analysis Toolkit (PAT). The example of Field Artillery Model \n(from chapter 8) is modified to represent a Probabilistic-Timed model in order to \nexplain how the CSP based model checking approach using PAT can be effective in \nthe composability verification. Using the example of Field Artillery model, it is \nexplained how the verification of time constraints is performed and how different \nproperty assertions are verified with probability using PLTL. A successful verification \nof this approach is a result of satisfaction of all the assertions defined in the \nrequirement specification, with an acceptable probability factor, and hence shows \nthat the components are composable.  \n \n\n   \nPage 177 \n \nChapter 10 \nSummary and Conclusion \n \nThis chapter makes a comparison between the composability verification approaches presented in this \nthesis and provides some guidelines for choosing the appropriate approach according to the nature of \nthe composed model. This discussion is followed by a summary of the major contributions of the thesis \nand some suggestions for future research in this area are suggested.    \n \nIn this thesis we propose a verification framework that follows the fundamental \nprinciples of M&S domain in terms of the notions of model correctness. It integrates \nseveral methods, techniques and tools to support different tasks in the multi-tier \ncomposability verification process of a composed model. It also inherits useful \ntechnological characteristics related to model verification from other communities \nsuch as Petri Nets, Model Checking and Process-Algebra community. And utilizes \nthe existing knowledge shared by these communities for the verification of \ncomponent based simulation models.  These simulation models are called \ncomponent based models because they are designed in form of components and can \nfurther be composed to construct sophisticated models (called composed models or \ncompositions). To ensure correctness, a composed simulation model is required to \nbe verified at its different composability levels, where each level poses certain degree \nof difficulty in verification. The initial levels of composability require that all the \ncomponents in a composition can be syntactically connected to each other through \nvalid interfaces. And they can correctly communicate with valid semantics. Whereas a \ndeeper level of composability is the dynamic-semantic composability which requires \nthat all the composed components should possess suitable behavior in order to \ncorrectly interact with each other for pursuing mutual objectives. The validity of \nbehavior in a component composition relies on two factors: (i) each component \nshould always be at the right state while interacting with the others and (ii) the \ncomposition should satisfy required behavioral properties, as prescribed in the \nrequirement specifications.  \nThe proposed verification framework not only provides complete support for the \nverification of initial composability levels, but also the most important characteristic \nof this framework is its ability to verify the composed model components at the \ndeeper level of dynamic-semantic composability. Composability Verification at this level is \na daunting task and requires a dynamic analysis approach. The behavior of \ncomponents can be studied when they are set to interplay with each other in an \nexecution environment, where they communicate through the exchange of events \nand make progress by the change of their internal states. Therefore an appropriate \ndynamic analysis approach is required which not only provides suitable execution \nenvironment but also support built-in verification techniques to evaluate the \ncomposability behavior at the runtime.  \nAccording to our findings not a single approach completely covers all the intricacies \nrequired for proving correctness at this the dynamic-semantic composability level \ndue to its complex nature. The effectiveness of a certain approach also varies due to \nthe varied nature of the composed model and the modeling formalism used. Since \n\nChapter 10  \n \nSummary and Conclusion \n   \nPage 178 \n \nsome models have complicated structure and demand rich expressiveness in terms of \ndata-centric details for the abstraction of a system; Whereas others have behavior of \ncomplex nature including notions of concurrency and temporal constraints. Besides \nthe system behavior can be deterministic or stochastic. Therefore it is difficult to \ndepend on a single approach for the challenging task of dynamic-semantic \ncomposability verification. \nFor this reason we investigated three different dynamic analysis approaches in our \nframework namely: (i) PN based Algebraic Analysis, (ii) CPN based State-Space \nAnalysis and (iii) CSP based Model Checking Technique. These approaches inherit \ntheories, methods, tools and techniques from their corresponding ancestry \ncommunities such as PN, CSP and Model checking. We adapt these inherited \nresources and integrate them in our framework. We also propose several extensions \nin each approach to suit the needs of dynamic-semantic composability verification. \nSome of these extensions are listed as follows: \n A component-based description format is proposed. This description format is \nused to represent the BOM based composed model in the required form in order \nto apply the selected approach. For instance a CPN based component model is \nproposed which represents the structural and behavioral aspects of a BOM \ncomponent in form of a CPN model. Similarly for CSP, a Component oriented \nCSP process model is introduced which represents a BOM component using CSP \nnotation.  \n For each approach a rule based transformation technique is proposed which \nconverts BOM components into the description format of the corresponding \napproach while keeping the structure and behavior of the model preserved. To \nensure this fact methods are proposed to compare the original model (BOM) and \nthe transformed model to assert that the latter correctly represents the former.  \n For PN algebraic approach algorithms are proposed to automate the process. \nAlso a function library is developed for the ease of conducting repeated \nverification tasks. \n In case of state-space analysis, a reduction technique is proposed which helps in \nreducing a large state-space and ease the process of verification. \nThe advantages and disadvantages of these three approaches are categorized as \nfollows: \n \nCategory: \nKinds of properties that can be verified \nPN Algebraic \nAnalysis \nThis approach only verifies a limited number of properties because it depends \non the applicability of underlying mathematical theorems which are limited in \nnumber and may not cover all types of properties \nCPN State-\nSpace Analysis \nIt constructs state-space of all possibilities that a system could be in. Therefore \nit allows to specify and verify different kinds of general system properties as \nwell as scenario specific properties.  \nCSP based \nModel Checking \nIn this approach the verification depends on the specification of properties \nusing LTL or CTL assertions, which along with their variety of extensions \nprovide rich expressiveness to define different kinds of properties. Therefore it \ncovers a bigger pool of verification questions both in terms of generic as well \nas scenario specific properties \nTable 55: Kinds of properties that can be verified \n \nAdvantage \nDisadvantage \nNeutral \n\nChapter 10  \n \nSummary and Conclusion \n   \nPage 179 \n \nCategory: \nType of the models that can be verified \nPN Algebraic \nAnalysis \nThis approach supports simple event-driven PN models. It does not support \nmodels with rich data, or models of real-time or probabilistic systems. \nCPN State-\nSpace Analysis \nThis approach support models with rich data-centric structure and behavior \nsince it offers flow of the tokens of complex data-types and their manipulations \nduring the transitions. It also offers limited support for Timed systems. \nHowever it does not support model verification of probabilistic nature.  \nCSP based \nModel Checking \nThis approach limits size of information in the model and does not entertain \nmodels with rich data-centric expressiveness. However it offers a variety of \ntypes of systems that can be verified such as reactive systems, real-time \nsystems, probabilistic and stochastic systems. Therefore this approach is much \nstronger in verifying different kinds of systems. \nTable 56: Type of the models that can be verified \n \n \nCategory: \nScalability \nPN Algebraic \nAnalysis \nVerification is dependent on the structure of the PN model (i.e., number of \nplaces and transitions). This factor is much less than the number of reachable \nmarkings produced by other approaches. Therefore for larger models this \napproach proves to be scalable \nCPN State-\nSpace Analysis \nVerification is dependent on the state-space, which tends to grow large for \neven ordinary models and hence can easily subject to state-space explosion. \nSome reduction techniques (including one of our own) may minimize this risk \nbut cannot completely omit it. \nCSP based \nModel Checking \nModel checking is also exposed to state-space explosion however it has gone \nthrough a continuous evolution of improved algorithms and compact data-\nstructures to minimize this risk. Therefore it promises a better resolution of \nscalability as compared to the State-space analysis. \nTable 57: Scalability \n \n \nCategory: \nInfinite Model Verification \nPN Algebraic \nAnalysis \nIt is not affected in its reasoning if the model is finite or infinite, because in \nmost of the cases it uses invariants for reasoning which are derived from the \nalgebraic computations and do not depend upon the number of  reachable \nsystem states \nCPN State-\nSpace Analysis \nIf the model is infinite it will require a construction of infinite state-space \nwhich is infeasible.  \nCSP based \nModel Checking \nInfinite model verification using this approach is possible by applying bounded \nmodel checking or by abstracting an infinite system into a finite one however \nthis may lead to results with partial correctness. \nTable 58: Infinite Model Verification \n \n \n \n \n \n\nChapter 10  \n \nSummary and Conclusion \n   \nPage 180 \n \n \nCategory: \nUsability \nPN Algebraic \nAnalysis \nThis approach is difficult to use due to complex mathematics and requirement \nto underlying applicable theorems for correct reasoning.  \nCPN State-\nSpace Analysis \nThis approach is easy to use. Most of the operations are automatic.  \nCSP based \nModel Checking \nThis approach requires some effort to understand the formalisms used for \nmodel input and property specifications. However its operations are easy and \nall run in a black box i.e., the model checker. \nTable 59: Usability \n \nCategory: \nAutomation \nPN Algebraic \nAnalysis \nThis approach is not automatic because the definition of a property and its \ntheorem applicability requires manual effort. When a property is defined, and a \ntheorem is selected, the modeler has to perform mathematical computations \nand manually infer whether a condition is satisfied or not. \nCPN State-\nSpace Analysis \nThis approach is semi-automatic because defining a verification task and a \nsuitable verification function requires modeler’s input. However the execution \nof the function is automatic and it searches all state-space to return a result. \nCSP based \nModel Checking \nThis approach is totally automatic. Once a temporal logic assertion is defined, it \nis executed automatically by and model checker to find out whether it is \nsatisfied or otherwise a counter example is generated. \nTable 60: Automation \n \nTable 55 compares the proposed approaches in terms of the different types of \nproperties that can be verified. It highlights that the PN algebraic technique is limited \nto verify only general properties (such as deadlock, liveness, fairness) since it depends \non the underlying theorems for the proof of their satisfiability. Whereas the other \ntwo approaches are relatively more flexible to the specification and verification of \nproperties of varied types, including general and scenario specific properties.  Table 56 \npresents a comparison of the proposed approaches in terms of the type of models. \nPN Algebraic approach only supports PN models with simple events without any \nparameters, guards, actions or input/output state-variables. These features are rather \nsupported by CPN based state-space analysis approach which also provides limited \nsupport for Time based CPN models. But for models of complex real-time systems \nor probabilistic systems Model Checking approach is the suitable choice.  \n \nTable 57 compares these approaches in terms of scalability of the models. In case of \nAlgebraic technique most of the operations in the property verification require \nmatrix computations such as Incidence matrix, P-Invariants, T-invariants. Therefore, \nthe scalability factor is dependent on the size of the matrix i.e., the number of places \n× number of transitions of the composed model. Thus, the algebraic technique is \nrelatively salable. With regard to scalability the CPN based state-space approach has \nserious limitations due to its rich data expressiveness and enumeration features. It is \nreported [130] that if the model is very large it generates state-space around 105 -106 \nnodes. Consequently ordinary PCs cannot easily handle such a large state-space. \nHowever there are different approaches to make it more scalable. We also believe \n\nChapter 10  \n \nSummary and Conclusion \n   \nPage 181 \n \nthat if our proposed state-space reduction technique is directly implemented in the \nCPN tools environment, this limitation can further be relaxed. Model Checking \ntechnique is relatively more scalable. Since it relies on the usage of PAT tool which \ncan handle about 107 states in a reasonable amount of time [98]. This should be \nsufficient for the verification of most industrial scale system models. \n \nAccording to the Table 58 the algebraic approach is indifferent whether the model is \nfinite or infinite in nature. An infinite model is a non-terminating model which keeps \non evolving indefinitely. Such models are difficult to be verified using State-space \napproach because its state-space construction is impossible. Although some \ntechniques have been developed such as coverability graphs, to resolve this problem \nhowever they fail in some cases, such as in case of timed models. To verify infinite \nmodels using Model Checking is somewhat possible using bounded model checking \nor by abstracting an infinite system into a finite one. However this may lead to results \nwith partial correctness because only a portion of the system state-space can be \nconsidered for the reasoning of property correctness. Table 59: UsabilityTable 59 and \nTable 60 compare the ease of use of these approaches in terms of their application in \na verification task and the extent of automation they provide.  \nIn short, there is no ultimate winner and making the right choice of an approach \nentirely depends on the kind of model under investigation and the types of \nverification properties in question. There are also no exact rules however some \nfundamental guidelines can be used to help the modeler select a suitable approach: \n10.1 Guidelines for choosing an approach \nIn this section some basic guidelines are presented for the modelers in making a \nsuitable choice \n10.1.1 PN Algebraic Technique \nThis approach is most suitable when the analysis of a BOM composition is in \nquestion which with simple state/transitions and does not require any extension (i.e., \nit does not have state-variables, or complex notions of transitions with parameters, \nguards, actions, inputs and outputs etc.). Also its requirement specification includes \nproperties which can be translated in form of PN properties (for which the solution \nof PN algebraic verification exist). Therefore it should be used when the requirement \nspecifications can be defined in terms of PN properties. For instance, in chapter 7 \nthe objectives are translated into “Fairness” which means that they can be satisfied if \nthe model is fair so the objective of verification is to prove this assumption and can \nbe done using PN algebraic approach. Also it is not effected by the model size, \nbecause it performs computations on matrices of the order of (No. of places × No. \nof Transitions) which remain static, therefore it can also be used for somewhat larger \nmodels.  \nIt should not be used if the requirement specification contains reachability \nproperties. Though it is possible to verify them using the PN state equation however \nit is rather difficult and inefficient as compared to State-Space Analysis approach. \nThis approach cannot be used if the composed model has notions of time, colored-\ntokens (i.e., the BOM events have parameters) or non-determinism.  \n\nChapter 10  \n \nSummary and Conclusion \n   \nPage 182 \n \n10.1.2 CPN based State-Space analysis Technique \nThis approach is best suitable when the given model has (or requires) rich data-\ncentric structure and behavior such as state-variables, events parameters, guards and \nactions. In this case the BOM components are required to be extended to capture \nmore details. If the modeler has the necessary information to extend the BOM \ncomponents then he should use this approach otherwise he should choose the \nAlgebraic technique. This approach is also suitable if the modeler wants to execute \nthe composed model at an abstract level to study the behavior of the components \nbefore actually implementing them. Although other proposed approaches also have \nexecution/simulation environments, but the CPN based execution is more detailed \nand comprehensive to study the interaction between composed components, as it \nprovides a hierarchical interconnection between the CPN components and their \nexecution is shown by the flow of data carrying colored tokens among inputs and \noutputs of each component in an interactive, step-by-step or an automatic fashion. \nThis allows the modeler to closely inspect the composition and its dynamics in a run-\ntime environment. Using this approach has many benefits from a component-based \ndevelopment point of view and the chances of its success are further elevated with \nour proposed state-space reduction technique called “Compositional State-Space”, \nwhich reduces the risk of state-space explosion.  \nThis approach can also be used for timed systems since CPN environment supports \nmodeling and verifying timed systems. However few limitations exist since the state-\nspace of timed systems is much more expensive and memory intensive, due to the \nfact that each state carries an overhead of timed-stamps so even for a simpler model, \nits state-space will be much heavier than a similar model with no time.  Moreover, if \nthe model has even one non-terminating loop, its state-space cannot be constructed \nas it keeps growing to infinity by incrementing the time-stamps.  (i.e., the system may \nreturn back to previous states in loops and no new state is being added in the state-\nspace but the time increases so the time-stamps keep on increasing. Therefore with \ndifferent time-stamps the same states keep on adding infinitely).  \nThis approach however completely fails when certain non-determinism is involved in \nthe model. Even though CPN specification allows using different probability \ndistribution functions, but when they are used, the resultant state-spaces are \ngenerated with variations, which cannot be used for verification reasoning. Therefore \nwe do not recommend this approach if the model is stochastic in nature.  \n \n10.1.3 CSP based Model Checking Technique \nThis approach is usually favored by majority of the software verification community. \nIt also has a greater flexibility of adopting a new technique or algorithm with specific \nrequirements at hand, and thus can be useful in a variety of contexts. This approach \nallows the modeler to execute the composed model using PAT simulator (see 3.2.7) \ntherefore it also contends with CPN based State-space analysis in terms of studying \nsystem behavior at runtime. However its main strength is revealed when it offers \nanswers to a variety of verification questions, and to a variety of types of systems \n(real-time, probabilistic etc.) using model checking.  \nThis approach however restricts model expressiveness since it limits the use of data \ntypes such as strings, products, records (unlike CPN). This requires an extra effort \nfrom the modelers to represent a model in reduced or compact forms using smaller \n\nChapter 10  \n \nSummary and Conclusion \n   \nPage 183 \n \ndata-types. For instance Boolean flags may be used instead of strings in the \nparameters such as a pair of string parameters: “Target_Destroyed”, “Target_Missed” can \nbe represented as True/False. Similarly a set of string parameters: {“Red”, “Blue”, \n“Green”} can be represented by corresponding integer values {0, 1, 2}. This kind of \nreduction is required for this approach to work correctly.  \nFor example, we presented a detailed data-centric model of field artillery in chapter 8 \nto be verified with CPN state-space analysis approach. But when it was required to \nverify a specific timed property (with non-determinism) we reduced unnecessary \ndetails and presented a simpler prototype of the Field Artillery model in chapter 9, \nfocusing only on its behavior relevant to the desired property. By doing this \nmodification the model was useable with this approach which successfully verified \nthe required properties that could not be verified using CPN based approach.  \nAs a final note, each approach has its own benefits and drawback and the choice \ndepends on the modeler’s objectives, nature of the task and available information. \nHowever we also encourage using multiple approaches for a single task and \ncomparing the results. It gives different perspectives and can better help in \nconfirming correctness. \n \n10.2 Thesis Contributions \nComponent based modeling and simulation is a promising approach to develop and \nsimulate system models. It incorporates numerous benefits such as modular design, \nlogical separation, flexible change management, reusability of existing components, \ncross-domain model integration and thus consequently helps in reducing cost, time \nand system complexity. A key characteristic in this expedient paradigm is composability \nthat is the ability to add or select and assemble reusable components in order to \nsatisfy user’s requirements. In this thesis we mainly endeavored to investigate \ndifferent aspects of this quality characteristic of component based model design and \nproposed a composability verification framework for the assessment of its \ncorrectness. Our proposed framework uses Base Object Model (BOM), a SISO \nstandard for component based modeling, and performs composability verification of \nBOM based model compositions with respect to given requirement specifications. In \norder to prove the correctness of composability of a set of BOM components, our \nframework undergoes a prescribed verification process, which has different phases \nstarting from system abstraction, requirement gathering, selection of BOM \ncomponents, their composition to form a conceptual model and then verifying its \ndifferent levels of composability, in an iterative top-down refinement fashion. When \nthe entire process is completed successfully the composed model is said to be \nverified with respect to its specifications and can be used for implementation using \nan implementation architecture (such as HLA) and simulated to serve its purpose.  \n \nFollowing are the key contributions of this thesis: \n We developed a composability verification framework, which stands on \nfundamental verification principles and backed by the theoretical underpinnings \nof M&S, the details of which are mainly covered in Part-I. It integrates different \nmethods, techniques, paradigms, algorithms, formalisms, templates, tools and 3rd \nparty libraries (or APIs) to support different tasks in the multi-tier composability \n\nChapter 10  \n \nSummary and Conclusion \n   \nPage 184 \n \nverification process of a composed model with respect to its requirement \nspecification. \n We outlined a component based modeling and simulation (CBM&S) life-cycle by \ncategorizing its different phases, and activities under each phase. A pictorial \nrepresentation has been used to explain different tasks conducted under each \nphase. This life-cycle provides guidelines for using various features of our \nframework, and allows the user to conduct verification operations in a systematic \nfashion. \n A template to define and express requirements in a formal way is proposed. Our \nrequirement specification template can be used to specify a set of objectives and \nsystem constraints. Objectives can be seen as ultimate goals while the constraints \nare necessary quality requirements that must be satisfied for achieving the \nobjectives. \n Inspired from the Discovery, matching and composition (DMC) paradigm of \nmodel development [19], we propose method for rapid development of BOM \nbased conceptual models. \n We propose a formal description of BOM components and their compositions \nfor documentation purpose. We also propose a graphical notation38  to describe \nthe structure of the BOM component and to show how they are connected to \neach other in a compact form. This notation can be used as blue prints of \ndifferent model compositions and can be shared among different teams or \narchived in the repository for reference.  \n We propose methods for evaluating the structural consistency of the composed \nBOMs using rule based static analysis technique. The structural analysis involves \nchecking that the components are correctly connected and they can communicate \nwith each other with correct semantics. For semantic analysis, we propose an \nOWL based differencing approach which checks that the communication of the \ncomponents is semantically consistent, meaningful and is understood as intended.  \n We suggest a behavioral evaluation technique which implicates that the \ncomponents can correctly interact with each other in a right causal order to reach \nfinal states or pass through the goal states. For this purpose we propose state-\nmachine matching process, which transforms BOM state-machines of each \ncomponent into an executable SCXML format and execute them to analyze their \ninteraction. If there is no deadlock and all the state-machines make required \nprogress then the behavior of the components is reported to be consistent.  \n For the evaluation of dynamic-semantic composability level, our framework \nincorporates three main approaches: (a) PN Algebraic technique (b) CPN-based \nState-space analysis technique and (c) CSP based model checking. These three \napproaches are offered to be used as alternatives to each other and their selection \nis dependent on the nature of the model being investigated and decision of the \nmodeler. We also present basic guidelines to help the modeler choose an \nappropriate approach.  \n For each approach we develop automatic transformation tool that transforms a \nBOM based composed model into its respective executable model description \nformalism. This method is inspired from Model Driven Architecture, in which a \nplatform independent model is transformed into platform specific model using \n                                                 \n38 It should be noted that different UML diagrams such as State charts and sequence diagrams are \nused to describe BOMs informally. Our graphical notation follows the pattern of CBSE. \n\nChapter 10  \n \nSummary and Conclusion \n   \nPage 185 \n \nsome transformation rules. We also propose BOM extensions based on certain \nadditional details that are required for correct transformation. For this purpose we \ndevelop a BOM extension editor that takes modeler’s input for extending BOM \ncomponents.  \n We have applied our proposed approaches in three different case studies \ndiscussed in chapter 7, 8 and 9 respectively. Each case study provides a proof of \nconcept and validates specific characteristics of our framework. For PN based \nalgebraic technique we presented a manufacturing system, in which fairness \nproperty is verified.  For CPN-based state-space analysis approach a field artillery \nmodel is presented in which a set of scenario specific properties are verified. For \nmodel checking, the same field artillery scenario is modified into a timed non-\ndeterministic model and a particular time property is verified with some \nprobabilistic assumptions.  \n We introduce a CPN based component model in order to describe a BOM \ncomponent (or any other simulation component) in form of an executable model \nthat can be executed using CPN execution environment. This CPN component \nmodel can also represent any other simulation component using its three layers \nnamely (i) structural layer: which is used to define component attributes and \nvariables; (ii) behavioral layer: which is used to describe the state-machine of a \ncomponent and (iii) communication layer: which is used to describe components \ninterfaces and how it can connect with other components and communicate. We \ntransform all BOM components into the proposed CPN based component model \nand compose them to form a composed model which can be executed in CPN \nenvironment and verified using CPN based state-space analysis technique. \n We introduce a State-space reduction technique called Compositional state-space. \nThis technique assumes that all the composed components are black-boxes and \ntheir inputs and outputs are exchanged in the main model. Therefore we can \nselect all the nodes from the state-space which are relevant to any activity \nhappening in the main model and filter all the other nodes, by replacing them with \nedges. The resultant graph will be a reduced state-space representing only those \nnodes which describe the interactions of components in the main model and \nprovide sufficient information for composability verification.  \n \n10.3 Conclusions \nThe verification framework proposed in this thesis expedites the process of \ncomposability verification of BOM based composed models with respect to the \nrequirement specifications. A verified composed model ensures consistent structure \nand behavior and guarantees the satisfaction of its objectives and required \nconstraints. A rapid development of the conceptual model using Discovery, \nMatching and Composition paradigm, its automatic transformation into an \nexecutable form and its composability verification helps in studying its structural and \nbehavioral correctness with respect to the given requirement specifications.  This \nhelps in rectifying any possible defects in the model design before it is actually \nimplemented and simulated to serve its purpose, and thus saves a significant amount \nof time, cost and achieve robustness. Moreover this process strongly supports \nreusability as the entire process can easily be repeated to compose same components \nfor different scenarios with varied configurations or with different requirement \nspecifications (as in chapter 8 and 9).  \n\nChapter 10  \n \nSummary and Conclusion \n   \nPage 186 \n \nThe entire composability verification framework is acclimated by a systematic \nComponent Based M&S life-cycle which gives an outline of different phases of \ncomponent based M&S development process, where each phase has different \nactivities. This life-cycle inherits important features and characteristic of some \nexisting M&S development life-cycles and the Model Driven Architecture with an \nexpansion of component based model development and guides the modelers with \nnecessary directions to perform different tasks at different phases.   \nAn important feature of this life-cycle is the software engineering principle of top-\ndown refinement. According to this principle a conceptual model is refined into an \nexecutable form through a number of intermediary steps. Each step generates a \nrelatively detailed version of the abstract model and is easier to reason about its \ncorrectness based on assumptions of its previously verified version. For instance, \nwhen the state-machine matching process is successful we can proceed to a more \ndetailed dynamic level execution/verification with an assumption that the behavior \nof the composed components is consistent. \nOur experience with the three different dynamic analysis approaches proves to be \nvery constructive for composability verification. Each approach in its own way \nprovides significant improvement on efficient and accurate reasoning regarding \nmodel correctness. We profess that the cross domain sharing of existing knowledge \nand valuable contributions from other communities (such as PN, CSP, model \nchecking in our case) bridges cooperation in problem solving and helps in \naccomplishing quality research. \n10.4 Future Directions \nSome of the key future directions of this work include: \n We intend to deploy the composability verification framework in different \napplication areas to evaluate its potential and to make use of its valuable features \nin verification. One area is the component based design for robotics applications. \nMany software architectures for robotic applications support component oriented \ndesign and thus can be explored for the utilization of our composability \nverification process, such as in studying various aspects of behavioral \ncomposability in different robotic applications.  \n \n In the context of improvements in the verification framework following are some \nkey future directions: \no We intend to include verification of requirement specifications. Correctness \nof requirements is a necessary aspect for successful verification. \n \no We also intend to produce viable solution for the validation of the composed \nmodel with respect to the real system. \n \no We defined Pragmatic composability level in chapter 2 however the \ncomposability verification at this level is still under investigation. We intend \nto explore this direction in future.  \n \n In general we are interested to explore the area of component based design \noptimization and to study the composability of component design for \noptimization with multiple objectives.  \n\n   \nPage 187 \n \nReferences \n[1] Eric Winsberg, Science in the Age of Computer Simulation.: University Of Chicago \nPress, 2010. \n[2] Louis G. Birta and Gilbert Arbez, Modelling and Simulation Exploring Dynamic \nSystem Behaviour, 1st ed.: Springer, 2007. \n[3] Christopher A. Chung, Simulation modeling handbook a practical approach, 1st ed.: \nCRC PRESS, 2004. \n[4] Catherine M. Banks, \"What Is Modeling and Simulation?,\" in Principles of \nModeling and Simulation: A Multidisciplinary Approach. Norfolk, VA: WILEY, \n2009, ch. 1. \n[5] O Balci, J D Arthur, and W F Ormsby, \"Achieving reusability and \ncomposability with a simulation conceptual model,\" Journal of Simulation, vol. \n5, no. 3, pp. 157-165, August 2011. \n[6] Charles W. Krueger, \"Software reuse,\" ACM Computing Surveys, vol. 24, no. 2, \npp. 13183, 1992. \n[7] Johannes Sametinger, Software Engineering with Reusable Components, 1st ed.: \nSpringer, May 25, 2001. \n[8] Robert G. Bartholet, David C. Brogan, Paul F. Reynolds, and Joseph C. \nCarnahan, \"In Search of the Philosopher’s Stone: Simulation Composability \nVersus Component-Based Software Design,\" in Proceedings of the Fall \nSimulation Interoperability Workshop, Orlando, FL, 2004. \n[9] Ivica Crnkovic, Brahim Hnich, Torsten Jonsson, and Zeynep Kiziltan, \"Basic \nConcepts in CBSE,\" in Building Reliable Component-Based Software Systems. MA, \nUSA: Artech House, 2002, ch. 1. \n[10] Clemens Syperski, Component Software Beyond Object-Oriented Programming, 2nd \ned. New York: Addison-Wesley, 2002. \n[11] Elfatatry Ahmed, \"Dealing with change: components versus services,\" \nCommun. ACM, vol. 50, no. 8, August 2007. \n[12] Marko Hofmann, \"Component based military simulation: lessons learned \nwith ground combat simulation systems,\" in Proceedings 15th European \nSimulation Symposium, Delft, Netherlands, 2003. \n[13] Andreas Tolk, \"Interoperability and Composability,\" in MODELING AND \nSIMULATION FUNDAMENTALS Theoretical Underpinnings and Practical \nDomains.: John Wiley, 2010, ch. 12. \n[14] Judith A. Stafford and Kurt Wallnau, \"Component Composition and \nIntegration,\" in Building Reliable Component-Based Software Systems. MA, USA: \nArtech House, Inc., 2002, pp. 179 - 192. \n[15] Scott A. Hissam, Gabriel A. Moreno, Judith Stafford, and Kurt C. Wallnau, \n\n   \nPage 188 \n \n\"Packaging predictable assembly with Prediction-Enabled Component \nTechnology,\" Carnegie Mellon University, Pittsburgh, PA, Technical Report \nCMU/SEI-200TR-024, November 2001. \n[16] Stephen Kasputis, \"Composable Simulations,\" in Winter Simulation Conference, \nOrlando, USA, 2000, pp. 1577–1584. \n[17] Paul K. Davis and Robert H. Anderson, Improving the composability of department \nof defense models and simulations.: RAND National Defense Research Institute, \n2003. \n[18] Hessam S. Sarjoughian, \"MODEL COMPOSABILITY,\" in Winter Simulation \nConference, Monterey, CA, USA , 2006. \n[19] Farshad Moradi, \"A Framework for Component Based Modelling and \nSimulation using BOMs and Semantic Web Technology,\" School of \nInformation and Communication Technology, KTH-Royal Institute of \nTechnology, Stockholm, Ph.D. Dissertation KTH/ICT/ECS AVH-08/05—\nSE, 2008. \n[20] Claudia Szabo, \"Composable simulation models and their formal validation,\" \nDepartment of computer science national university of singapore, Singapore, \nPh.D. Dissertation 2010. \n[21] Andy Ju An Wang and Kai Qian, Component-oriented programming, 1st ed.: John \nWiley & sons, Publication, 2005. \n[22] Ernest H. Page, \"Theory and Practice for Simulation Interconnection: \nInteroperability and Composability in Defense Simulation,\" in Handbook of \nDynamic System Modeling.: Chapman & Hall, 2007, ch. 16. \n[23] Andreas Hansson and Kees Goossens, On-Chip Interconnect with Aelite \nComposable and Predictable Systems, 1st ed. New York: Springer , 2011. \n[24] Hsu-Chun Yen, \"Introduction to Petri Net Theory,\" in Recent Advances in \nFormal Languages and Applications.: Springer Berlin, 2006, ch. 25. \n[25] Christos G. Cassandras and Stéphane Lafortune, Introduction to Discrete Event \nSystems, 2nd ed.: Springer, 2008. \n[26] James L. Peterson, Petri Net theory and the modeling of systems, 1st ed.: \nPRENTICE-HALL, INC., Englewood Cliffs, N.J. 07632, 1981. \n[27] Jos C M Baeten, \"A brief history of process algebra,\" Theoretical Computer \nScience - Process algebra, vol. 335, no. 2-3, pp. 131 - 146, May 2005. \n[28] Osman \nBalci, \n\"VERIFICATION, \nVALIDATION \nAND \nACCREDITATION OF SIMULATION MODELS,\" in Proceedings of the \nWinter Simulation Conference, Atlanta, GA, 1997. \n[29] Mikel D. Petty, \"Verification and Validation,\" in Principles of Modeling and \nSimulation.: John Wiley & Sons, 2009, ch. 6. \n[30] Stewart Robinson, Roger Brooks, Kathy Kotiadis , and Durk-Jouke Van Der \nZee, Conceptual Modeling for Discrete-Event Simulation. FL, USA: CRC Press, Inc., \n\n   \nPage 189 \n \nBoca Raton, 2010. \n[31] Paul A. Fishwick, Simulation Model Design and Execution: Building Digital Worlds \n(1st edition). NJ, USA: Prentice Hall PTR, 1995. \n[32] Stephan MERZ, \"An Introduction to Model Checking,\" in Modeling and \nVerification of Real-time Systems, Nicolas Navet and Stephan Merz , Eds.: Wiley, \n2010, ch. 3. \n[33] Susan Harkrider and Lunceford H. W. , \"Modeling and simulation \ncomposability,\" in Proceedings of the Interservice/Industry Training, Simulation and \nEducation Conference, Orlando, FL, 1999. \n[34] Mikel D. Petty and Eric W. Weisel, \"A theory of simulation composability,\" \nVirginia Modeling Analysis & Simulation Center, Old Dominion University, \nNorfolk, Virginia, 2004. \n[35] Ernest H. Page and Jeffrey M. Opper, \"Observations on the complexity of \ncomposable simulation,\" in Proceedings of the Winter Simulation Conference., NJ, \n1999, pp. 553–560. \n[36] David R. Pratt, Charles L. Ragusa, and Sonia von der Lippe, \"Composability \nas an Architecture Driver,\" in The Interservice/Industry Training, Simulation & \nEducation Conference (I/ITSEC), 1999. \n[37] Paul K. Davis, Paul A. Fishwick, Michael C. Overstreet, and Dennis C. \nPegden , \"Model Composability As A Research Investment: Responses To \nThe Featured Paper,\" in Winter SimulationConference, Orlando, FL, 2000, pp. \n1585–1591. \n[38] Mikel D. Petty and Eric W. Weisel, \"A Composability Lexicon,\" in Proceedings \nof the Spring 2003 Simulation Interoperability Workshop, Orlando, FL, April 2003. \n[39] Extensible \nModeling \nand \nSimulation \nFramework \n(XMSF).  \nhttps://www.movesinstitute.org/xmsf/xmsf.html \n[40] DEVS.  http://en.wikipedia.org/wiki/DEVS \n[41] OSA.  http://osa.inria.fr/publications.html \n[42] Base Object Model.  www.boms.info/ \n[43] Paul Davis, \"Composability,\" in Defense Modeling, Simulation, and Analysis: \nMeeting the Challenge. Washington, D.C.: The National Academies Press, 2006. \n[44] Jay Larson, Robert Jacob, and Everest Ong, \"The Model Coupling Toolkit: A \nnew Fortran90 toolkit for building multiphysics parallel coupled models,\" \nInternational Journal of High Performance Computing Applications, vol. 19, no. 3, pp. \n277–292, 2005. \n[45] Simon Portegies Zwart, Steve McMillan, and et al (24 authors in total), \"A \nmultiphysics and multiscale software environment for modeling astrophysical \nsystems,\" in 8th International Conference on Computational Science, Berlin, 2008, pp. \n207–216. \n\n   \nPage 190 \n \n[46] Rob Armstrong et al., \"The CCA component model for high-performance \nscientific computing,\" Concurr. Comput. : Pract. Exper., vol. 18, no. 2, pp. 215-\n229, 2006. \n[47] Maciej Malawski, Marian Bubak, Michal Placek, Dawid Kurzyniec, and Vaidy \nSunderam, \"Experiments with distributed component computing across grid \nboundaries,\" in In Proceeding of the HPC-GECO/CompFrame Workshop, Paris, \nFrance, 2006. \n[48] Jan Hegewald, Manfred Krafczyk, Jonas Tölke, Alfons Hoekstra, and Bastien \nChopard, \"An agent-based coupling platform for complex automata,\" in 8th \nInternational Conference on Computational Science, Berlin, 2008, pp. 227–233. \n[49] Katarzyna Rycerz and Marian Bubak, \"Building and Running Collaborative \nDistributed Multiscale Applications,\" in Large-Scale Computing Techniques for \nComplex System Simulations, Albert Y. Zomaya, Ed.: John Wiley & Sons, 2012, \nch. 6. \n[50] Eric W. Weisel, Mikel D. Petty, and Roland R. Mielke, \"Validity of Models \nand Classes of Models in Semantic,\" in Fall Simulation Interoperability Workshop, \nOrlando, FL, 2003. \n[51] Stewart Robinson, Richard E. Nance, Ray J. Paul, Michael Pidd , and Simon \nJ.E. Taylor, \"Simulation model reuse: definitions, benefits and obstacles,\" \nSimulation Modelling Practice and Theory, , vol. 12, no. 7–8, pp. 479-494, \nNovember 2004. \n[52] Ernest H. Page, Richard Briggs, and John A. Tufarolo , \"Toward a family of \nmaturity models for the simulation interconnection problem,\" in Proceedings of \nthe Simulation Interoperability Workshop, Arlington, VA, 2004. \n[53] Brahim Medjahed and Athman Bouguettaya, \"A Multilevel Composability \nModel for Semantic Web Services,\" Journal of IEEE Transactions on Knowledge \nand Data Engineering, vol. 17, no. 7, (July 2006. \n[54] Farshad Moradi, Rassul Ayani, Shahab Mokarizadeh, Gholam Hossein \nAkbari Shahmirzadi, and Gary Tan, \"A Rule-based Approach to Syntactic \nand Semantic Composition of BOMs,\" in 11th IEEE Symposium on Distributed \nSimulation and Real-Time Applications, Chania, 2007. \n[55] Robert Porzel , Contextual Computing Models and Applications, 1st ed. Berlin \nHeidelberg : Springer-Verlag , 2011. \n[56] Micheal Axelsen, \"Information request ambiguity and end user query \nperformance : theory and empirical evidence,\" School of Business, University \nof Queensland, Queensland, Australia, Master's Thesis 2000. \n[57] Bernard P. Zeigler , Herbert Praehofer , and Tag Gon Kim, Theory of Modeling \nand Simulation, 2nd ed.: Academic Press, 2000. \n[58] Paul Gustavson, \"Building and Using Base Object Models (BOMs) for \nModeling \nand \nSimulation \n(M&S) \nfocused \nJoint \nTraining,\" \nin \nInterservice/Industry Training, Simulation, and Education Conference (I/ITSEC), \n\n   \nPage 191 \n \nOrlando, Florida, 2005. \n[59] SISO-I, \"Base Object Model (BOM) Template Specification,\" Simulation \nInteroperability Standard Organization (SISO), Orlando, FL USA, 2006. \n[60] SISO-II, \"Guide for Base Object Model (BOM) Use and Implementation,\" \nOrlando, FL, 2006. \n[61] Paul Gustavson and Tram Chase, \"Building Composable bridges between the \nconceptual space and the implementation space,\" in Proceedings of the winter \nsimulation conference, Washington, DC, USA, 2007. \n[62] Mikel D. Petty and Paul Gustavson, \"Combat Modeling with the High Level \nArchitecture and Base Object Models,\" in Engineering Principles of Combat \nModeling and Distributed Simulation, Andreas Tolk, Ed.: A John Wiley & Sons, \nInc., Publication, 2012, ch. 19. \n[63] Robinson Stewart, Roger Brooks, Kathy Kotiadis, and Durk-Jouke Van Der \nZee, Conceptual Modeling for Discrete-Event Simulation. FL, USA: CRC Press, Inc., \n2010. \n[64] Fishwick A. Paul, Simulation Model Design and Execution: Building Digital Worlds, \n1st ed. NJ, USA: Prentice Hall PTR, , 1995. \n[65] BOM Works.  http://www.simventions.com/bomworks/ \n[66] Hruz Branislav and Meng Chu Zhou, Modeling and Control of Discrete-event \nDynamic Systems with Petri Nets and Other Tool, 1st ed.: Springer, 2007. \n[67] ZhiWu Li and Meng Chu Zhou, Deadlock Resolution in Automated Manufacturing \nSystems A Novel Petri Net Approach, 1st ed.: Springer-Verlag, 2009. \n[68] René David and Hassane Alla, Discrete, Continuous, and Hybrid Petri Nets, 1st \ned.: Springer, 2010. \n[69] Girault Claude and Valk Rüdiger, Petri Nets for Systems Engineering A Guide to \nModelling, Verification, and Applications.: Springer-Verlag, 2001. \n[70] Tadao Murata, \"Petri nets: Properties, analysis and applications 77(4), 541–\n580 (1989),\" in Proceedings of the IEEE, 1989. \n[71] Gianfranco Balbo, \"Introduction to Stochastic Petri Nets,\" in Lectures on \nFormal Methods and Performance Analysis, Holger Hermanns and Joost-Pieter \nKatoen, Eds.: Lecture Notes in Computer Science Springer Berlin, 2001, pp. \n84-155. \n[72] Zohar Manna and Amir Pnueli, The Temporal Logic of Reactive and Concurrent \nSystems: Specification, 1st ed.: Springer Verlag, 1992. \n[73] Kurt Jensen, Søren Christense, and Lars M Kristensen, \"CPN Tools State \nSpace Manual,\" Aarhus , Denmark, Manual 2006. \n[74] Lars Michael Kristensen, \"State Space Methods for Coloured Petri Nets,\" \nDepartment of Computer Science, University of Aarhus, Aarhus, Denmark, \nPh.D. Dissertation 2000. \n\n   \nPage 192 \n \n[75] Søren Christensen, Lars Kristensen, and Thomas Mailund, \"A Sweep-Line \nMethod for State Space Exploration,\" in Tools and Algorithms for the Construction \nand Analysis of Systems.: Springer Berlin / Heidelberg, 2001, pp. 450-464. \n[76] Michael Westergaard, Lars Michael Kristensen, Gerth Stølting Brodal, and \nLars Arge, \"The ComBack Method – Extending Hash Compaction with \nBacktracking,\" in Petri Nets and Other Models of Concurrency.: Springer Berlin / \nHeidelberg, 2007, pp. 445-464. \n[77] Louise Elgaard, \"The Symmetry Method for Coloured Petri Nets Theory, \nTools and Practical Use,\" Aarhus, Denmark, PhD Dissertation July 2002. \n[78] Kurt Jensen and Lars M Kristensen, Coloured Petri Nets Modelling and Validation \nof Concurrent Systems.: Springer, 2009. \n[79] Kevin Mcleish. Petri Nets \nhttp://www.peterlongo.it/Italiano/Informatica/Petri/index.html \n[80] Standard ML.  http://www.smlnj.org/ \n[81] Kurt Jensen, \"Coloured Petri Nets,\" Computer Science Department \nUniversity of Aarhus, Technical Report. \n[82] CPN Tools.  http://cpntools.org/  \n[83] Wojciech Penczek and Agata Półrola, A Temporal Logic Approach Advances in \nVerification of Time Petri Nets and Timed Automata, 1st ed.: Springer, 2006. \n[84] Charles Antony Richard Hoare, Communicating Sequential Processes, 1st ed.: \nPrentice Hall International, 1985. \n[85] Stephen D. Brookes and Charles Antony Richard Hoare , \"A Theory of \nCommunicating Sequential Processes,\" Journal of the ACM, vol. 31, no. 3, pp. \n560 - 599, July 1984. \n[86] Christel Baier and Joost-Pieter Katoen, Principles Of Model Checking, 1st ed.: \nThe MIT Press, April 2008. \n[87] Ganesh Gopalakrishnan, \"Model Checking: Basics,\" in Computation Engineering \nApplied Automata Theory and Logic, Ganesh Gopalakrishnan, Ed. University of \nUtah, Salt Lake City, UT: Springer, 2006, ch. 21. \n[88] Malay Ganai and Aarti Gupta, SAT-Based Scalable Formal Verification Solutions, \n1st ed., Anantha Chandrakasan, Ed.: Springer, 2007. \n[89] BDD.  http://en.wikipedia.org/wiki/Binary_decision_diagram \n[90] SAT.  http://en.wikipedia.org/wiki/Boolean_satisfiability_problem \n[91] Fred Kroger and Stephan Merz, Temporal Logic and State Systems, 1st ed.: \nSpringer, 2008. \n[92] George M. Reed and William A. Rosco, \"A Timed Model for \nCommunicating \nSequential \nProcesses,\" \nin \nAutomata, Languages and \nProgramming, 13th International Colloquium, ICALP86, Rennes, France, 1986, \n\n   \nPage 193 \n \npp. 314-323. \n[93] Communicating sequential processes \nhttp://en.wikipedia.org/wiki/Communicating_sequential_processes#Analysi\ns_tools \n[94] Formal Systems.  http://www.fsel.com/software.html \n[95] The Adelaide Refinement Checker \n http://cs.adelaide.edu.au/~esser/arc.html \n[96] The ProB Animator and Model Checker.  http://www.stups.uni-\nduesseldorf.de/ProB/index.php5/Main_Page \n[97] PAT: Process Analysis Toolkit.  http://www.comp.nus.edu.sg/~pat/ \n[98] Jun Sun, Yang Liu, and Jin Song Dong , \"Model checking csp revisited: \nIntroducing a process analysis toolkit,\" in 3rd International Symposium on \nLeveraging Applications of Formal Methods, Verification and Validation , Greece, \n2008. \n[99] Janusz Laski and William Stanley, Software Verification and Analysis An \nIntegrated, Hands-On Approach, 1st ed.: Springer, 2009. \n[100] Jerry Banks , Handbook of Simulation: Principles, Methodology, Advances, \nApplications, and Practice, 1st ed.: Wiley, 1998. \n[101] Collection of software bugs.  http://www5.in.tum.de/~huckle/bugse.html \n[102] Klaus Schneider, Verification of Reactive Systems: Formal Methods and Algorithms, \n2nd ed.: Springer-Verlag, 2004. \n[103] William L. Oberkampf and Christopher J. Roy, Verification And Validation In \nScientific Computing, 1st ed.: Cambridge University Press, 2010. \n[104] Avner Engel, Verification, validation, and testing of engineered systems, 1st ed., \nAndrew P. Sage, Ed.: A John Wiley & Sons, Inc., Publication, 2010. \n[105] Steve McConnell, Code Complete, 2nd ed.: Microsoft Press, 2004. \n[106] Bertrand Meyer, Object Oriented Software Construction, 2nd ed.: Prentice-Hall, \n1997. \n[107] Matthew Wilson, \"Quality Matters: Correctness, Robustness and Reliability,\" \nOverload Journal Process Topics , no. 93, October 2009. \n[108] Robert G. Sargent, \"Verification, validation, and accreditation of simulation \nmodels,\" in Proceedings of the 2000 Winter Simulation Conference, Orlando, FL, \n2000. \n[109] Mourad Debbabi, Fawzi Hassaïne, Yosr Jarraya, Andrei Soeanu, and Luay \nAlawneh, Verification and Validation in Systems Engineering Assessing \nUML/SysML Design Models, 1st ed.: Springer, 2010. \n[110] Pallab Dasgupta, A roadmap for formal property verification, 1st ed.: Springer, \n\n   \nPage 194 \n \n2006. \n[111] Didar Zowghi and Vincenzo Gervasi , \"The Three Cs of Requirements: \nConsistency, Completeness, and Correctness,\" in Proceedings of 8th International \nWorkshop on Requirements Engineering: Foundation for Software Quality, \n(REFSQ'02), 2002. \n[112] OWL-API.  http://owlapi.sourceforge.net/ \n[113] Imran Mahmood, Rassul Ayani, Vladimir Vlassov, and Farshad Moradi, \n\"Statemachine Matching in BOM Based Model Composition,\" in In \nProceedings of the 2009 13th IEEE/ACM International Symposium on Distributed \nSimulation and Real Time Applications (DS-RT '09), Singapore, 2009. \n[114] State-chart XML.  http://www.w3.org/TR/scxml/ \n[115] PNML.  http://www.pnml.org/ \n[116] Platform Independent Petri net Editor API.  http://pipe2.sourceforge.net/ \n[117] Julius Farkas, \"Theory of simple inequalities,\" Journal of Pure and Applied \nMathematics , vol. 1902, no. 124, 1902. \n[118] Mario D'Anna, \"Concurrent system analysis using Petri nets: an optimized \nalgorithm for finding net invariants,\" Computer Communications, vol. 11, no. 4, \nAugust 1988. \n[119] Imran Mahmood, Rassul Ayani, Vladimir Vlassov, and Farshad Moradi, \n\"Verifying Dynamic Semantic Composability of BOM-based composed \nmodels using Colored Petri Nets,\" in To appear in: 26th Workshop on Principles of \nAdvanced and Distributed Simulation, Zhangjiajie, China, 2012. \n[120] V S Alagar and K Periyasamy, \"Extended Finite State Machine,\" in \nSpecification of Software Systems, 2nd edition.: Springer, 2011, ch. 7. \n[121] Shao Jie Zhang and Yang Liu, \"An Automatic Approach to Model Checking \nUML State Machines,\" in Secure Software Integration and Reliability Improvement \nCompanion 2010, Singapore, June 2010. \n[122] Marta Kwiatkowska, \"Survey of fairness notions,\" Information and Software \nTechnology, vol. 31, no. 7, September 1989. \n[123] Tadao Murata and Zhehui Wu, \"Fair relation and modified synchronic \ndistances in a petri net,\" Journal of the Franklin Institute, vol. 320, no. 2, August \n1985. \n[124] Imran Mahmood, Rassul Ayani, Vladimir Vlassov, and Farshad Moradi, \n\"Fairness Verification of BOM-Based Composed Models Using Petri Nets,\" \nin IEEE Workshop on Principles of Advanced and Distributed Simulation (PADS), \nNice, France, June 2011. \n[125] Andreas Tolk, \"Challenges of Combat Modeling and Distributed \nSimulation,\" in Engineering Principles of Combat Modeling and Distributed \nSimulation.: John Wiley & Sons, Inc., Publication, 2012, ch. 1. \n\n   \nPage 195 \n \n[126] U.S. Army, Tactics, Techniques, And Procedures For The Field Artillery Cannon \nBattery (U.S. Army Field Manual, FM 6-50). Washington: Marine Corps \nWarfighting Publication, 1996. \n[127] Indirect Fire.  http://www.fas.org/man/dod-101/sys/land/indirect.htm \n[128] The GraphML File Format.  http://graphml.graphdrawing.org/ \n[129] Time-On-Target.  \nhttp://www.answers.com/topic/artillery#Time_on_Target \n[130] Michael Westergaard and Fabrizio Maggi, \"Modeling and Verification of a \nProtocol for Operational Support using Coloured Petri Nets,\" in Applications \nand Theory of Petri Nets, Lars Kristensen and Laure Petrucci, Eds.: Springer \nBerlin / Heidelberg, 2011. \n[131] Military grid reference system \nhttp://en.wikipedia.org/wiki/Military_grid_reference_system \n[132] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford \nStein, Introduction to Algorithms, 3rd ed.: MIT Press, 2009.",
    "pdf_filename": "A Verification Framework for Component-Based Modeling and Simulation Putting the pieces together.pdf"
}