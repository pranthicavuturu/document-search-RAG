{
    "title": "AI's Spatial Intelligence Evaluating AI's Understanding of Spatial Transformations in PSVTR and Augm",
    "context": "Spatial intelligence is important in many fields such as Architecture, Engineering, and Construction (AEC), Science, Technology, Engineering, and Mathematics (STEM), and Medicine. Understanding three-dimensional (3D) spatial rotations can involve verbal descriptions and visual or interactive examples, illustrating how objects move and change orientation in 3D space. Recent studies show Artificial Intelligence (AI) with language and vision capabilities still face limitations in spatial reasoning. In this paper, we have studied advanced generative AI’s spatial capabilities of understanding rotations of objects in 3D space utilizing its image processing and language processing features. We examined the spatial intelligence of the GPT-4 model with vision in understanding spatial rotation process with spatial rotation diagrams based on the Revised Purdue Spatial Visualization Test: Visualization of Rotations (Revised PSVT:R). Furthermore, we incorporated an added layer of a coordinate system axes on Revised PSVT:R to study the variations in GPT-4’s performance. We additionally examined GPT-4’s understanding of 3D rotations in Augmented Reality (AR) scene images that visualize spatial rotations of a physical object in 3D space and observed increased accuracy of GPT-4’s understanding of the rotations by adding supplementary textual information depicting the rotation process or mathematical representations of the rotation (e.g., matrices) superimposed on the object. The results indicate that while GPT-4 as a major current Generative AI model lacks the understanding of a spatial rotation process, it has the potential to understand the rotation process with additional information that can be provided by methods such as AR. AR can superimpose textual information or mathematical representations of the rotations on spatial transformation diagrams or images and create a more intelligible input for AI to comprehend or for training AI’s spatial intelligence. Furthermore, by combining the potentials in spatial intelligence of AI with AR's interactive visualization abilities, we expect to offer enhanced guidance for students’ spatial learning activities. Such spatial guidance can greatly benefit understanding spatial transformations and additionally support processes like assembly, construction, fabrication, manufacturing, as well as learning in AEC, STEM, and Medicine that require precise 3D spatial understanding and instruction. Spatial visualization skills refer to the ability to mentally rotate, manipulate, twist, or invert 3D objects [1], [2]. Education in many STEM disciplines demands instructional approaches that offer experimental practices and tasks with high representational accuracy and realistic simulations to enhance the learning experience [3]. Many first-year undergraduate STEM students struggle with visualization and drawing tasks, even though they have access to various computer assisted learning resources for geometry, spatial transformations, and related mathematical concepts [4]. These difficulties include understanding the symbolic notation of linear algebra, generalizing geometric reasoning, and transitioning to matrix representations of transformations, which often weakens their intuitive grasp of the concepts  [5], [6].",
    "body": "1 \nAI’s Spatial Intelligence: Evaluating AI’s Understanding of \nSpatial Transformations in PSVT:R and Augmented  Reality \nUttamasha Monjoree and Wei Yan \nTexas A&M University \n \nAbstract \nSpatial intelligence is important in many fields such as Architecture, Engineering, and \nConstruction (AEC), Science, Technology, Engineering, and Mathematics (STEM), and \nMedicine. Understanding three-dimensional (3D) spatial rotations can involve verbal descriptions \nand visual or interactive examples, illustrating how objects move and change orientation in 3D \nspace. Recent studies show Artificial Intelligence (AI) with language and vision capabilities still \nface limitations in spatial reasoning. In this paper, we have studied advanced generative AI’s spatial \ncapabilities of understanding rotations of objects in 3D space utilizing its image processing and \nlanguage processing features. We examined the spatial intelligence of the GPT-4 model with vision \nin understanding spatial rotation process with spatial rotation diagrams based on the Revised \nPurdue Spatial Visualization Test: Visualization of Rotations (Revised PSVT:R). Furthermore, we \nincorporated an added layer of a coordinate system axes on Revised PSVT:R to study the variations \nin GPT-4’s performance. We additionally examined GPT-4’s understanding of 3D rotations in \nAugmented Reality (AR) scene images that visualize spatial rotations of a physical object in 3D \nspace and observed increased accuracy of GPT-4’s understanding of the rotations by adding \nsupplementary textual information depicting the rotation process or mathematical representations \nof the rotation (e.g., matrices) superimposed on the object. The results indicate that while GPT-4 as \na major current Generative AI model lacks the understanding of a spatial rotation process, it has \nthe potential to understand the rotation process with additional information that can be provided by \nmethods such as AR. AR can superimpose textual information or mathematical representations of \nthe rotations on spatial transformation diagrams or images and create a more intelligible input for \nAI to comprehend or for training AI’s spatial intelligence. Furthermore, by combining the potentials \nin spatial intelligence of AI with AR's interactive visualization abilities, we expect to offer enhanced \nguidance for students’ spatial learning activities. Such spatial guidance can greatly benefit \nunderstanding spatial transformations and additionally support processes like assembly, \nconstruction, fabrication, manufacturing, as well as learning in AEC, STEM, and Medicine that \nrequire precise 3D spatial understanding and instruction. \n1. Introduction \nSpatial visualization skills refer to the ability to mentally rotate, manipulate, twist, or invert 3D objects [1], \n[2]. Education in many STEM disciplines demands instructional approaches that offer experimental \npractices and tasks with high representational accuracy and realistic simulations to enhance the learning \nexperience [3]. Many first-year undergraduate STEM students struggle with visualization and drawing \ntasks, even though they have access to various computer assisted learning resources for geometry, spatial \ntransformations, and related mathematical concepts [4]. These difficulties include understanding the \nsymbolic notation of linear algebra, generalizing geometric reasoning, and transitioning to matrix \nrepresentations of transformations, which often weakens their intuitive grasp of the concepts  [5], [6]. \n\n \n2 \nDespite studying drawing in secondary school, many students have not sufficiently developed their spatial \nabilities, leading to significant challenges in mentally manipulating figures in space [7]. \nPersonalized STEM education using interactive technologies can enhance learning experiences. It can \npromote engagement through hands-on simulations and immersive environments and provide real-time \nfeedback to improve understanding of complex concepts [3]. By leveraging tools such as augmented reality \n(AR), virtual labs, and Artificial Intelligence (AI)-driven assistance, students can explore STEM topics at \ntheir own pace and develop critical problem-solving skills in an interactive, engaging way [8]. \nGenerative AI, encompassing technologies that can create new content such as text, images, 3D models, \nand audio, have rapidly evolved in recent years, driven by advancements in deep learning and neural \nnetwork architecture. Large language models (LLMs) such as GPT-4 and its successors have demonstrated \nremarkable proficiency in generating coherent and contextually relevant text. GPT-4 with Vision - GPT-\n4V(ision) - has capabilities to analyze user-input images [9],[10]. In an extensive study of GPT-4V(ision), \nevaluations based on many samples demonstrate GPT-4V’s unprecedented ability in understanding visual \ninputs, spatial relationships of humans and objects, and the genericity of its capabilities [11]. \nAugmented Reality (AR) can overlay digital information on the real world and create a more perceivable \nrepresentation for AI. The use of AR-based technologies can reduce the cognitive load of users required for \nspatial learning by superimposing virtual instruction on the real world. AR has three distinguishing features: \ncombining real and virtual; interactive in real-time; and registered in three dimensions [12]. Using AR in \neducation has several positive impacts: increased content understanding, learning spatial structures, \nlanguage associations, long-term memory retention, improved collaboration, and motivation [13]. \nGPT-4's visual capabilities are highly relevant in AR, virtual design, and other applications that require real-\ntime understanding of visual input [14]. For instance, it can assist in designing objects in 3D space[15], \nguide users through augmented experiences, or help in manipulating visual content interactively [16]. \nCombined textual and graphical information superimposed in real world utilizing AR technology has the \npotential to generate a more intelligible input for GPT-4's visual capabilities. \nUsing conversational AI in education is one of the major approaches to enhancing and promoting a more \npersonalized learning experience [17]. Personalized interactive assistance of generative conversational AI \nthat has visual capabilities along with spatial qualities of AR can help students achieve better learning \noutcomes. Understanding AI’s ability to perceive rotation in AR is expected to have a significant impact on \nlearning spatial visualization skills.  \nSpatial reasoning is a key area in psychometrics, and several datasets exist for evaluating these skills, such \nas the Mental Rotation Tests [18], the Purdue Spatial Visualization Test (PSVT)/The Purdue Visualization \nof Rotations Test [19], and the Revised Purdue Spatial Visualization Test: Visualization of Rotations \n(Revised PSVT:R) [20]. The Revised PSVT:R is considered one of the most effective tools for evaluating \nspatial reasoning skills. This test is a widely used assessment designed to measure an individual's ability to \nvisualize and mentally manipulate 3D objects in space, particularly in terms of rotating these objects.  \nWe can use both a verbal description and some visual or interactive examples to explain spatial rotations \nand convey how objects move or change orientations in 3D space [21]. Combining the visual and verbal \nabilities of conversational AI and visual and interactive abilities of AR has the potential to provide enhanced \nguidance in spatial rotation activities [8]. This type of spatial guidance can help mathematical education as \nwell as govern assembly, construction, and fabrication process that requires spatial instructions. The goal \nof this study is to evaluate AI’s capability of understanding 3D spatial rotations by performing a series of \n\n \n3 \ntests based on Revised PSVT:R and an interactive AR application (AR-Classroom) that can visualize \nrotation along different axes [22]. The research questions that guide this study are as follows: \n1. Can GPT-4 answer questions in Revised PSVT:R and how its performance compare with human \ntest takers? \n2. Can GPT-4 perceive a spatial rotation process created based on Revised PSVT:R and describe the \nprocess in terms of axis of rotation, angle of rotation, and (clockwise or counterclockwise) direction \nof rotation? \n3. Can GPT-4 perceive a spatial rotation process visualized in AR and describe the process in terms \nof axis of rotation, angle of rotation, and (clockwise or counterclockwise) direction of rotation?  \n2. Literature Review  \nDual-coding theory \nThe dual-coding theory is defined as a theory that posits the existence of two interconnected subsystems in \nthe human cognitive system: a verbal system and an imagery system [23]. This dual capacity means the \nlanguage system functions in both communication and symbolic interpretation. Human cognition uniquely \nintegrates both language and nonverbal objects and events, enabling us to process linguistic input (like \nspeech or writing) alongside symbolic representations of nonverbal aspects of the world [24]. Therefore, \nspatial representation should integrate these two systems, where language serves both direct communication \nand a framework for understanding nonverbal phenomena. Integrating sophisticated language and image \nprocessing and response generation of Artificial Intelligence (AI) with immersive three-dimensional \ncapabilities of AR can facilitate a more engaging spatial learning experience [25]. Including Augmented \nReality (AR) in the learning process can improve spatial abilities in the domain of architecture education \n[26]. \nAI Vision Model  \nThe latest development of GPT-4, a large-scale, multimodal model, can accept image and text inputs and \nproduce text outputs. While less capable than humans in many real-world scenarios, GPT-4 Vision can \nanalyze images, provide detailed descriptions, identify objects, and offer explanations of scenes [9]. For \ninstance, it can describe an uploaded photo or explain elements within the image, such as identifying objects \nlike plants, vehicles, or people, and providing contextual information like plant care advice or understanding \nspecific items in complex scenes [27]. AI with vision and spatial understanding have broad applications, \nfor example, in STEM and Architecture/Engineering/Construction (AEC). Integrating GPT AI models into \nconstruction safety management enhances risk assessment, real-time insights, incident reporting, and \ncompliance evaluation [28], e.g. the Personal Protective Equipment (PPE) or helmet usage compliance [11]. \nExperiments have proven GPT-4V to be highly effective in detecting and explaining global and fine-grained \nsemantic patterns in anomaly or risk detection [29], [11].  \nAI’s Spatial and Mathematical Abilities \nSpatial intelligence is described as a skillset that helps us comprehend visual-spatial tasks and spatial \nrelations. As a result, we gain a better orientation of objects in space [30]. In recent years there have been \nseveral studies analyzing spatial and mathematical capabilities of AI. GPT-4 model “hallucinates”, wherein \nthe model returned inaccurate information. The model was unable to accurately return bounding boxes for \nobject detection, suggesting it is not completely fit for computer vision related usage [31]. The SPARE3D \ndataset introduces a series of challenging 2D-3D spatial reasoning tasks, revealing that while convolutional \nneural networks (CNNs) excel in visual learning, their performance on spatial reasoning remains below \nhuman average, highlighting the need for advancements in artificial neural network design to improve 3D \nspatial reasoning for intelligent robots [32]. A recent study has examined the perceived complexity of 3D \n\n \n4 \nshapes from both human and ChatGPT perspectives, revealing that ChatGPT can effectively capture human \nconsensus on shape complexity. The findings suggest that large generative models like ChatGPT could be \nused to automatically generate 3D content with varying complexities, which can be enhanced for spatial \nvisualization skills in VR applications [14]. This research explored how the visual features of some of the \n3D shapes used in the Purdue Spatial Visualization Test are perceived by ChatGPT.  \n[33] introduces a novel pipeline for enhancing the geometric capabilities of Multimodal Large Language \nModels (MLLMs) and Wolfram (a math tool) by generating a dataset of 4.9K simplified, image-text-\nmatching geometric problems combined with 19K open-source data to create a uniformly challenging \ndataset, GeoGPT4V. Training models on GeoGPT4V has demonstrated improved geometric performance \non MathVista and MathVision benchmarks.  \n[34] conducted a pilot study to explore the potential of using ChatGPT to create thematic and mental maps \nby utilizing both public geospatial data and textual descriptions of geographic space. They evaluated \nChatGPT's performance on a GIS (Geographic Information System) exam, comparing GPT-3.5 and GPT-\n4, to assess their strengths and limitations in understanding geospatial concepts and discovered challenges \nin teaching spatial reasoning to AI models. While ChatGPT shows promise in lowering the barrier for map \nproduction, enhancing efficiency, and demonstrating spatial thinking capabilities, it still faces limitations \nsuch as unequal user benefits and reliance on user intervention for maintaining quality control [35].  \nChatGPT-4 and ChatGPT 4o struggle more with geometry and complex math problems compared to \nalgebra, suggesting that while AI can support algebra instruction, teacher intervention is needed in areas \nwhere AI is less effective, such as spatial intelligence and higher-difficulty tasks [36]. After an elaborate \nstudy, [37] observed LLMs resemble sophisticated pattern matching more than true logical reasoning. LLM \nperformance varies highly on different versions of the same question. LLMs’ performance drops \nsubstantially with a minor increase in difficulty, and their sensitivity to inconsequential information \nindicates that their reasoning is fragile.  \nDespite many positive media reports highlighting GPT-4 and ChatGPT's exam-solving abilities, possibly \ninfluenced by a selection bias, their overall mathematical performance falls significantly short of a graduate \nstudent’s level [38]. Educators can leverage AI to create engaging and effective learning environments, \ncultivating a generation skilled in mathematics and prepared to navigate the complexities of the modern \nworld by addressing the associated challenges and adopting a refined approach [39].  \nAR and AI in Education \nIn recent years, AR and AI are demonstrating the potential to reshape education by enhancing interactivity, \npersonalization, and accessibility. The use of AR technology and speech recognition to support productive \nvocabulary among students has shown user satisfaction when parents and teachers were interviewed [40].  \n[41] introduced a novel early education platform that combines smart voice commands and AR technology. \nThe platform is embedded in a storage box, making it convenient for use in homes and kindergartens where \npreschool education is provided. This project enhances the integration of these technologies in educational \nsettings.   \nUsing AR-powered embodied learning for spatial transformations and representation of corresponding \nmatrices has already shown significant improvement in the participants’ math scores [42]. The project AR-\nClassroom uses embodied learning and novel AR features to visualize spatial rotations and their \nmathematical representations, and the usability tests that were completed using the application showed \npromising results. The benchmark test and updated usability test done on 24 participants have shown \nsignificant improvement in usability scores [32]. The AR-Classroom application integrates AR and AI-\nbased multi-sensory assistance to help undergraduate students better understand spatial transformations and \ntheir mathematical representations. A usability study identified four key themes: ease of use, the need for \n\n \n5 \nmore detailed AI responses, a desire for visual information, and improved understanding, highlighting the \npotential for further AI-guided visual cues in AR learning environments [8]. \n3. Methodology \nWhen explaining rotation, we use references like direction, angle, and axis verbally and visually. According \nto the dual-coding theory, two cognitive subsystems, verbal and imagery, work together to process language \nand nonverbal content. Integrating these systems has the potential to enhance spatial understanding, where \nlanguage aids in communication and interpreting nonverbal cues.  \nLLMs rely on descriptive datasets and visual cues, which can enhance spatial descriptions in vision models. \nUsing visual cues to communicate a rotation process could improve interactions between AR and AI \nsystems, providing a more intuitive tool for conveying spatial information, thus facilitating spatial learning. \nThis approach could bridge the gap between text-based AI and spatially oriented AR, making explanations \nof spatial transformations more accessible and effective. Combined AR and AI have the potential of \nbecoming instrumental in assembly and fabrication, offering real-time guidance, error reduction, and \nefficiency improvements. In this paper, we examine generative AI’s vision and language capacity of \nunderstanding and describing spatial rotation using standard test (Revised PSVT:R) vs AR.  \nOur evaluation focused on the AI's capacity to comprehend key aspects of spatial rotations, including the \naxis, angle, and direction of each rotation, as well as its ability to process sequences of multiple rotations. \nWe integrated multiple layers of graphical and textual features into the Revised PSVT:R to analyze their \nimpact on the performance of GPT-4 (ChatGPT 4o or GPT-4o). We also tested GPT-4's capability in an AR \napplication that visualizes the spatial rotation of a hand-held object in 3D space and tested the accuracy \nwith the addition of supplementary textual information (angle, textual information, rotation matrix equation \netc.) \n4. Experiments and Results \nExperiment 1: Evaluate GPT-4 Using Revised PSVT:R (original and with added coordinate axes)  \nThe Revised PSVT:R is a widely used test for assessing spatial visualization skills, consisting of rotation \nproblems. In this test, participants are shown diagrams of 3D objects in both a starting and finishing \norientation after an unknown rotation along one or more axes. The task is to identify the correct diagram of \na second 3D object that has undergone the same rotations as the first. Participants are given the second 3D \nobject in its starting orientation and must choose the correct finishing orientation from five possible options.  \nTest Using Revised PSVT:R (Test 1a) \nUsing ChatGPT 4o (with a vision capability) from June 9, 2024 through Fall 2024, we conducted various \ntests. In the first test, we provided GPT-4 with all the 30 problems from Revised Purdue Spatial \nVisualization Test: Visualization of Rotations (Revised PSVT:R) as a standard test to study its ability to \nanalyze input images and spatial rotation capabilities (Figure 1a). After getting the input (a screenshot of \nthe test questions) uploaded, GPT-4 responded with answering the questions by choosing from (A, B, C, D, \nand E options). Comparing GPT-4’s results with the correct results on Revised PSVT:R, we found that GPT-\n4 only got 5 out of 30 answers correct. The 5 correct answers are for Question 10, 15, 16, 19, and 30. (For \ncopyright of Revised PSVT:R and keeping the test results confidential for future tests with participants, the \n30 questions and results are not revealed in this paper. An example question in the DIRECTIONS section \nof the test that is allowed to publish is shown in Figure 1.) See Appendix A for sample interaction between \nauthors and GPT-4 in Test 1a. \n\n \n6 \nTest Using Revised PSVT:R with Axes added (Test 1b) \nAfter getting unsatisfactory results in the first test, we explored more of its spatial analyzing capabilities by \nadding a coordinate system with x- y- z-axes onto the Revised PSVT:R question images. The motivation \nbehind this is incorporating more visual and textual cues for analyzing images and enhancing \ncommunication with AI. We superimposed the coordinate system on 30 question images of Revised \nPSVT:R where x-, y-, and z-axes are indicated (Figure 1b). See Appendix B for sample interaction between \nauthors and GPT-4 in Test 1b. \n \nFigure 1: (a) The example question from Revised PSVT:R (left), (b) the example question from Revised \nPSVT:R with a coordinate system added (right). In both questions, D is the correct answer. \nIn Experiment 1, both Test 1a and 1b, GPT-4 had the most complete task to identify the correct option from \nthe five 3D object orientations (A to E). To execute the whole task, like a human test-taker, GPT-4 needs to \nidentify the rotation process in Step 1 and then compare it to the five rotation processes in Step 2, in order \nto choose the correct option (Figure 2). For example, in Figure 2, to answer correctly, one should identify \nthe similarities between the axis systems between Step 1 and Step 2, and choose option D. In Test 1a, GPT-\n4 gave correct answers only 5 out of 30 questions which reflects 17% accuracy (Figure 3). In Test 1b, GPT-\n4 had a similar accuracy, which indicates that it is unable to identify and compare rotation processes \ncorrectly even with the visual cues (axes) added. \n\n \n7 \nFigure 2: Example question in Revised PSVT:R with a coordinates system. D is the correct answer. \n \n \nFigure 3: Accuracy of GPT-4 in Experiment 1 using Revised PSVT:R: 5 correct out of 30 answers, and \nRevised PSVT:R with a coordinate system added: 4 correct out of 30 answers. \nExperiment 2: Evaluate GPT-4 Using Step 1 of Revised PSVT:R with a Coordinate System (Test 2) \nGPT-4 showed an unsatisfactory result in answering the questions of Revised PSVT:R in Experiment 1. To \nsimplify the tasks for AI, we continued testing with only Step 1 of the questions of Revised PSVT:R (Figure \n3) to see if AI can identify the rotations of one object in each question. We superimposed a coordinate \nsystem with axes on Step 1 of each question of Revised PSVT:R, and asked GPT-4 to explain the rotation \nprocess with minimum rotation steps (Figure 4). The resulting orientation in 14 questions of Revised \nPSVT:R can be achieved by rotating along a single axis and in the rest 16 questions can be achieved by \nrotating along multiple axes (e.g., rotating around one axis and then around another axis). As the resulting \norientation of a 3D object can be achieved using different sequences of multiple rotations, we asked GPT-\n4 to describe the change of orientation using rotations along a minimum number of axes. We used the \nprompt, “Explain the rotation process using minimum steps of rotation in terms of axis of rotation, angle \nof rotation, and direction of rotation.” GPT-4 performed better in case of the Step 1 rotation process than \n5\n4\n25\n26\n0\n10\n20\n30\nRevised PSVT:R\nRevised PSVT:R w/\nCoordinates\nRevised PSVT:R Test\nCorrect\nIncorrect\n\n \n8 \nin Experiment 1 with both Step 1 and 2. See Appendix C for sample interaction between authors and GPT-\n4 in Test 2. \n \nFigure 4: Diagram of Step 1 of Revised PSVT:R with a coordinate system added. \nIn this test, GPT-4 performed better in case of rotation along a single axis than rotations along multiple axes \n(Figure 5). In case of a single-step rotation, it was correct 5 out of 14 times reflecting 35% accuracy, in case \nof rotations along multiple axes, it was correct 2 out of 16 times showing 12.5% accuracy. The combined \naccuracy of this test is 23.33%. The performance in case of rotation along a single axis is almost 2 times \nmore accurate than the rotations along multiple axes, though in both cases performance by GPT-4 is \nunsatisfactory. This experiment results indicate that even though GPT-4 cannot compare similar rotation \nprocesses, it is somewhat capable of understanding one rotation process. The information we provided is \nnot sufficient for GPT-4 to understand the rotation process. However, there may still be a possibility of \nachieving better results by adding supplementary information and context.  \n \nFigure 5: Rotation process accuracy described by GPT-4 using Step 1 of Revised PSVT:R with axes of a \ncoordinate system \nExperiment 3: Evaluate GPT-4 on Understanding Rotations with Supplementary Information through \nan AR Application \nAR can superimpose 3D models, data, or text over real-world objects, enhancing the user's understanding \nof their surroundings and the physical world. An AR application named AR-Classroom provides an \ninteractive learning environment with better context and visualization for understanding rotations (Figure \n6) [22]. The app aims to run on an AR device, such as an AR-enabled tablet, an AR headset, or a computer \nwith a camera. A user can hold and manipulate a 3D physical model to learn spatial transformations by \nobserving the corresponding mathematical representation and visualization that augment the 3D physical \nmodel with a graphical user interface (GUI) in AR.  \n5\n2\n7\n9\n14\n23\n0\n10\n20\n30\nOne Axis Rotation\nMultiple Axis Rotation\nCombined\nRotation Process Accuracy  \nCorrect\nIncorrect\n\n \n9 \n \nFigure 6: AR-Classroom GUI – augmenting the 3D physical space shuttle model with a 3D virtual space \nshuttle model, rotation axes, rotation arc and angle, and the matrix representation of the rotation.  \nThe X-, Y-, and Z-axes of the models’ coordinate systems and corresponding matrix equations are displayed \nin the GUI. In Figure 6, the following AR scene components are shown. \n1. a physical model, called pre-image of a transformation, showing the state before rotation \n2. a 3D reference frame coordinate with red, green, and blue solid arrows for X-, Y-, and Z-axis, \nattached to the physical model \n3. a 3D wireframe model, called image of a transformation, showing the state after rotation \n4. a 3D body frame coordinate with red, green, and blue dotted arrows for X-, Y-, and Z-axes, attached \nto the 3D wireframe model \nTest Using AR with a Coordinate System (Test 3a): We uploaded to GPT-4 with 12 screenshots of the AR \nenvironment visualizing only the physical and virtual models and the coordinate system. The angle of \nrotation and corresponding rotation matrix equation were hidden (Figure 7a). All screenshots varied in axis \nof rotation, angle of rotation, and direction (clockwise or counterclockwise) of rotation. We provided the \nprompt, “Explain the rotation process with axis of rotation, angle of rotation and direction. Note: All red \nlines represent X-axis, all yellow-green lines represent Y-axis, all blue lines represent Z-axis. The dotted \nline is the orientation of axis after rotation. The axis of rotation does not have a dotted counterpart of the \nsame color. Follow right hand rule for the direction of rotation.” \nTest Using AR with a Coordinate System and Angle of Rotation (Test 3b): We uploaded to GPT-4 with 12 \nscreenshots of the AR environment visualizing the physical and virtual models, the coordinate system, and \nangle of rotation. The corresponding rotation matrix equation was hidden (Figure 7b). All screenshots \nvaried in axis of rotation, angle of rotation, and direction of rotation. We provided the same prompt as in \nTest 3a. \nTest Using AR with Coordinate System, Angle of Rotation, and Rotation Equation (Test 3c): We uploaded \nto GPT-4 with 12 screenshots of AR-Classroom where the coordinate system, rotation angle, and \ncorresponding rotation matrix equation are all visible (Figure 7c). We provided the same prompt as in Test \n3a and GPT-4 gave very satisfactory results each time we tested, as explained below. Furthermore, even \nwith a simpler prompt “Explain the rotation process using minimum steps of rotation in terms of axis of \nrotation, angle of rotation, and direction of rotation”, the results are as accurate as the more detailed prompt \nin Test 3c. \n\n \n10 \n \n(a) \n \n(b) \n \n(c) \nFigure 7: (a) AR rotation visualization with a coordinate system, (b) AR rotation visualization with a \ncoordinate system and angle, (c) AR rotation visualization with a coordinate system, angle, and a rotation \nequation \nIn this experiment, where we tested different versions of AR-Classroom images, we observed that the \nperformance of GPT-4 gets better when more descriptive texts are added to the visual input. AR rotation \nwith only the coordinate system has 25% accuracy (Test 3a), and AR rotation with a coordinate system and \nan angle of rotation (Test 3b) has 75% accuracy. When we provided the whole UI of AR-Classroom (AR \nwith a coordinate system, an angle, and rotation equation) as input, the additional information related to the \nrotation process helped GPT-4 gain 100% accuracy (Test 3c). Here, GPT-4 could identify and connect the \nrotation matrix equation with the rotation activity, and it described the rotation process correctly. We can \nsee from Table 1 or Figure 8 that making the angle of rotation and the rotation matrix equation visible in \n\n \n11 \nthe AR scene each has increased the accuracy significantly. See Appendix D for sample interaction between \nauthors and GPT-4 in Test 3c. \nTable 1: Accuracy of identifying rotation process in Test 3a, Test 3b, and Test 3c \n \nAR with \nCoordinate System \n(Test 3a) \nAR with Coordinate \nSystem & Angle \n(Test 3b) \nAR with Coordinate \nSystem, Angle, & \nRotation Equation \n(Test 3c) \nIdentify Axis of Rotation \n75.0% \n91.7% \n100.0% \nIdentify Direction of Rotation \n58.3% \n100.0% \n100.0% \nIdentify Angle of Rotation \n50.0% \n83.3% \n100.0% \nExplanation of Rotation \nProcess Correctly Using all the \nabove Criteria  \n25.0% \n75.0% \n100.0% \n \n \nFigure 8: Accuracy of identifying rotation process in Test 3a, Test 3b, and Test 3c \n5. Discussions \nIn GPT-4, the language model tends to be more accurate for purely text-based tasks, while the vision model \nis better suited for multimodal tasks involving both images and text [43]. GPT-4V integrates image \nunderstanding with text, making it powerful for multimodal tasks like interpreting images alongside text \ndescriptions. This model performs well in visual question answering, image-to-text tasks, and recognizing \nobjects in images. However, its accuracy for purely image-based tasks, like object detection or more \ncomplex image analysis, may not match specialized vision models and still relies heavily on textual inputs \nto guide its performance. This supports what we observed in our experiments: GPT-4 gave inadequate \nperformance in case of Revised PSVT:R (Test 1a) with a score of 17% (5 out of 30). This indicates that it \ncannot identify complex rotation tasks and cannot identify the similarities between two identical rotation \nprocesses with different objects. The cases when it got the right answers could be coincidental. As a \ncomparison, in 1993, a PSVT:R was administered to 535 first-year engineering students at Michigan Tech \nduring orientation. Approximately 20% of the students (n = 96) scored 60% or less on the PSVT:R, while \n80% of the students scored above 60% accuracy [44]. In another study [20] using Revised PSVT:R, the \nmean score of 1,022 participants was 19.08 out of 30 with a standard deviation of 6.16. Scores ranged from \n3 to 30, and 13 participants (1.27%) achieved a perfect score. \n0.00%\n20.00%\n40.00%\n60.00%\n80.00%\n100.00%\nAR with Coordinate System (Test\n3a)\nAR with Coordinate System &\nAngle (Test 3b)\n AR with Coordinate System, Angle,\n& Rotation Equation (Test 3c)\nAccuracy of Rotation along Single Axis in AR\nIdentify Axis of Rotation\nIdentify Direction of Rotation\nIdentify Angle of Rotation\nExplanation of Rotation Process Using 3 Criteria\n\n \n12 \n \nFurthermore, providing a 3D coordinate system with the original Revised PSVT:R did not show significant \nimprovement (Test 1b). \nTest 2 (Experiment 2) showed that there is an improvement of GPT-4 in recognizing Step 1 only in Revised \nPSVT:R or a simplified rotation process. \nIn the AR environment, we provided better context than the previous experiments. GPT-4 could see pre-\nimage before rotation and image after the rotation in the AR environment (Test 3a). By increasing textual \ninput step by step in the AR environment, we observed improved performance by GPT-4 (Figure 8). Just \nincluding the angle of rotation with the coordinate system showed significant improvement in result (Test \n3b). Providing a rotation matrix equation as additional information enabled GPT-4 to identify the rotation \nprocess correctly (Test 3c). Even with a less detailed prompt and same image as Test 3c, it identified the \nrotation process correctly. The use of the AR environment is for the convenience of adding accurate rotation \ninformation to the testing for GPT-4. A drawing style of PSVT:R may be used in a similar evaluation \nprocess. However, the rendering mode (realistic 3D physical model, wireframe virtual model, colors of \nrotation arc and angle values, etc.) may have affected the understanding of rotations by GPT-4.  \nOverall, this result points out that GPT-4 shows limited spatial intelligence in terms of understanding 3D \nrotations with 2D image inputs. For applications, it is very promising for future development of AR \napplications in the education and STEM sectors, where GPT-4 can guide a user on task performance by \ntaking real-world data and relevant, superimposed textual and graphical data as input. In an AR setting, \nusers can view physical objects, diagrams, or virtual models while AI provides additional information \nlayered over the scene. For example, in a geometry lesson, a 3D model of a shape could be displayed, and \nGPT-4 could guide the student by offering real-time feedback on angles, dimensions, or rotation processes, \nenabling more effective learning. This combination of visual and textual data allows GPT-4 to better \nunderstand spatial relationships and provide meaningful insights.  \nThe result also indicates that the current generative AI doesn’t perform well regarding spatial intelligence \nwithout additional context information (e.g., rotation axis, angle, and equation). The context information \ncan facilitate AI for understanding spatial transformations and can help train AI to improve its spatial \nintelligence, so that in the future the context information may not be needed any more. \nA major limitation of the research is that the sample size (number of each test) is small as this is a very \nearly evaluation of spatial intelligence of AI in terms of 3D rotations. \n6. Conclusions and Future Work \nSpatial knowledge is critical in fields like architecture, computer graphics, civil engineering, and other \nSTEM disciplines. While generative AI-powered tools can assist students in learning, conversational AI \nagents still struggle with spatial abilities. AR can enhance these systems by overlaying textual information \non spatial diagrams, making the data more comprehensible for AI. This paper investigates GPT-4's spatial \ncapabilities, particularly its proficiency in interpreting 3D rotations, as tested through Revised PSVT:R and \nrotation images from an AR application. We improvised Revised PSVT:R with additional graphical and \ntextual features to assess if GPT-4’s performance changes with supplementary information. GPT-4 has \ndifficulty recognizing complex rotation tasks and identifying the similarities when two rotations are \nessentially the same but for different objects, as tested in the Revised PSVT:R. \nIn the AR images, where objects are visualized in 3D space, adding supplementary textual and math \ninformation significantly improved GPT-4’s accuracy. For example, explicitly showing the angle of rotation \n\n \n13 \nincreased accuracy from 25% to 75% in one of the tests. Providing the math equation of rotation in the AR \napplication, along with detailed text, allowed GPT-4 to reach 100% accuracy. This suggests that while GPT-\n4V performs well in multimodal tasks, it relies heavily on textual cues for better performance. Additional \ninformation, as visualized in AR applications, has the potential to enhance the performance of GPT-4. These \nresults indicate that in an AR setting, users can view physical objects, diagrams, or virtual models’ \nadditional information layered over the scene while AI can provide real-time feedback observing the user’s \ninteraction process. \nThe future work will include more thorough testing with an increased sample size and with different AI \ntools, and enhancing the spatial intelligence of AI, so that AI can understand spatial transformations \nincluding rotations, translations, scales, perspectives, etc. based on various styles (e.g., real-world photo or \ndrawing styles) of image or video input of arbitrary 3D physical objects, with or without context information \nof axes, text, math equations, etc. \nAcknowledgement \nThis work is partially supported by National Science Foundation (NSF) Grant #2119549. The authors \nappreciate the support of the Texas A&M University’s team of the NSF RETTL project: Using Augmented \nReality and Artificial Intelligence to Improve Teaching and Learning Spatial Transformations in STEM \nDisciplines, for the AR-Classroom app and discussions on mental rotations and AI. \nReferences \n[1] \nD. Ben-Chaim, G. Lappan, and R. T. Houang, “The Effect of Instruction on Spatial Visualization \nSkills of Middle School Boys and Girls,” American Educational Research Journal, vol. 25, no. 1, \npp. 51–71, 1988, doi: 10.2307/1163159. \n[2] \nR. Fleisig, A. Robertson, and A. Spence, “Improving the Spatial Visualization Skills of First Year \nEngineering Students,” Proceedings of the Canadian Engineering Education Association, Aug. \n2011, doi: 10.24908/pceea.v0i0.4050. \n[3] \nN. Pellas, A. Dengel, and A. Christopoulos, “A Scoping Review of Immersive Virtual Reality in \nSTEM Education,” IEEE Transactions on Learning Technologies, vol. 13, no. 4, pp. 748–761, Oct. \n2020, doi: 10.1109/TLT.2020.3019405. \n[4] \nM. Garmendia, J. Guisasola, and E. Sierra, “First-year engineering students’ difficulties in \nvisualization and drawing tasks,” European Journal of Engineering Education, vol. 32, no. 3, pp. \n315–323, Jun. 2007, doi: 10.1080/03043790701276874. \n[5] \nC. Andrews-Larson, M. Wawro, and M. Zandieh, “A hypothetical learning trajectory for \nconceptualizing matrices as linear transformations,” International Journal of Mathematical \nEducation in Science and Technology, vol. 48, no. 6, pp. 809–829, Aug. 2017, doi: \n10.1080/0020739X.2016.1276225. \n[6] \nJ. Dick and M. Childrey, “Enhancing Understanding of Transformation Matrices,” Mathematics \nTeacher, vol. 105, no. 8, pp. 622–626, Apr. 2012. \n[7] \nC. Potter and E. MERWE, “Perception, imagery, visualization and engineering graphics,” European \nJournal of Engineering Education, vol. 28, pp. 117–133, Mar. 2003, doi: \n10.1080/0304379031000065216. \n[8] \nU. Monjoree et al., “AR-Classroom: Integrating Conversational Artificial Intelligence with \nAugmented Reality Technology for Learning Spatial Transformations and Their Matrix \nRepresentation,” in 2023 IEEE Frontiers in Education Conference (FIE), Oct. 2024. \n[9] \nOpenAI et al., “GPT-4 Technical Report,” Mar. 04, 2024, arXiv: arXiv:2303.08774. doi: \n10.48550/arXiv.2303.08774. \n[10] “OpenAI Platform.” Accessed: Apr. 23, 2024. [Online]. Available: https://platform.openai.com \n\n \n14 \n[11] Z. Yang et al., “The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision),” 2023, doi: \n10.48550/ARXIV.2309.17421. \n[12] R. T. Azuma, “A Survey of Augmented Reality,” Presence: Teleoperators and Virtual Environments, \nvol. 6, no. 4, pp. 355–385, Aug. 1997, doi: 10.1162/pres.1997.6.4.355. \n[13] I. Radu, “Augmented reality in education: a meta-review and cross-media analysis,” Personal \nUbiquitous Comput., vol. 18, no. 6, pp. 1533–1543, Aug. 2014, doi: 10.1007/s00779-013-0747-y. \n[14] L. McShane and C. Lopez, “Perceived Complexity of 3D Shapes for Spatial Visualization Tasks: \nHumans vs Generative Models,” doi: 10.1115/DETC2023-115081. \n[15] B. Regassa Hunde and A. Debebe Woldeyohannes, “Future prospects of computer-aided design \n(CAD) – A review from the perspective of artificial intelligence (AI), extended reality, and 3D \nprinting,” Results in Engineering, vol. 14, p. 100478, Jun. 2022, doi: 10.1016/j.rineng.2022.100478. \n[16] A. Rychert, M. L. Ganuza, and M. N. Selzer, “Integrating GPT as an Assistant for Low-Cost Virtual \nReality Escape-Room Games,” IEEE Computer Graphics and Applications, vol. 44, no. 4, pp. 14–\n25, Jul. 2024, doi: 10.1109/MCG.2024.3426314. \n[17] S. Cunningham-Nelson, W. Boles, L. Trouton, and E. Margerison, “A review of chatbots in \neducation: Practical steps forward,” in 30th Annual Conference for the Australasian Association for \nEngineering Education (AAEE 2019): Educators Becoming Agents of Change: Innovate, Integrate, \nMotivate, Australia: Engineers Australia, 2019, pp. 299–306. Accessed: Feb. 29, 2024. [Online]. \nAvailable: https://search.informit.com.au/documentSummary;dn=068364390172788;res=IELENG \n[18] S. G. Vandenberg and A. R. Kuse, “Mental rotations, a group test of three-dimensional spatial \nvisualization,” Percept Mot Skills, vol. 47, no. 2, pp. 599–604, Oct. 1978, doi: \n10.2466/pms.1978.47.2.599. \n[19] G. M. BODNER and R. B. GUAY, “The Purdue Visualization of Rotations Test,” Chem. Educator, \nvol. 2, no. 4, pp. 1–17, Oct. 1997, doi: 10.1007/s00897970138a. \n[20] S. Y. Yoon, “Psychometric properties of the Revised Purdue Spatial Visualization Tests: \nVisualization of Rotations (the Revised PSVT:R),” Theses and Dissertations Available from \nProQuest, pp. 1–180, Jan. 2011. \n[21] J. M. Loomis, R. L. Klatzky, and N. A. Giudice, “Representing 3D Space in Working Memory: \nSpatial Images from Vision, Hearing, Touch, and Language,” in Multisensory Imagery, S. Lacey and \nR. Lawson, Eds., New York, NY: Springer, 2013, pp. 131–155. doi: 10.1007/978-1-4614-5879-1_8. \n[22] S.-H. Yeh et al., “AR-Classroom: Augmented Reality Technology for Learning 3D Spatial \nTransformations and Their Matrix Representation,” in 2023 IEEE Frontiers in Education \nConference (FIE), Oct. 2023, pp. 1–8. doi: 10.1109/FIE58773.2023.10343049. \n[23] J. M. Clark and A. Paivio, “Dual coding theory and education,” Educ Psychol Rev, vol. 3, no. 3, pp. \n149–210, Sep. 1991, doi: 10.1007/BF01320076. \n[24] A. Paivio, Mental Representations: A Dual Coding Approach. in Oxford Psychology Series. Oxford, \nNew York: Oxford University Press, 1990. \n[25] J. S. Devagiri, S. Paheding, Q. Niyaz, X. Yang, and S. Smith, “Augmented Reality and Artificial \nIntelligence in industry: Trends, tools, and future challenges,” Expert Systems with Applications, \nvol. 207, p. 118002, Nov. 2022, doi: 10.1016/j.eswa.2022.118002. \n[26] A. Dünser, K. Steinbügl, H. Kaufmann, and J. Glück, “Virtual and augmented reality as spatial \nability training tools,” in Proceedings of the 7th ACM SIGCHI New Zealand chapter’s international \nconference on Computer-human interaction: design centered HCI, in CHINZ ’06. New York, NY, \nUSA: Association for Computing Machinery, Jul. 2006, pp. 125–132. doi: \n10.1145/1152760.1152776. \n[27] “GPT-4.” Accessed: Oct. 09, 2024. [Online]. Available: https://openai.com/index/gpt-4-research/ \n[28] A. Saka et al., “GPT models in construction industry: Opportunities, limitations, and a use case \nvalidation,” Developments in the Built Environment, vol. 17, p. 100300, Mar. 2024, doi: \n10.1016/j.dibe.2023.100300. \n\n \n15 \n[29] T. Driessen, D. Dodou, P. Bazilinskyy, and J. de Winter, “Putting ChatGPT vision (GPT-4V) to the \ntest: risk perception in traffic images,” Royal Society Open Science, vol. 11, no. 5, p. 231676, May \n2024, doi: 10.1098/rsos.231676. \n[30] Z. C. K. Hawes, K. A. Gilligan-Lee, and K. S. Mix, “Effects of spatial training on mathematics \nperformance: A meta-analysis,” Dev Psychol, vol. 58, no. 1, pp. 112–137, Jan. 2022, doi: \n10.1037/dev0001281. \n[31] “GPT-4 with Vision: Complete Guide and Evaluation,” Roboflow Blog. Accessed: Oct. 09, 2024. \n[Online]. Available: https://blog.roboflow.com/gpt-4-vision/ \n[32] W. Han, S. Xiang, C. Liu, R. Wang, and C. Feng, “SPARE3D: A Dataset for SPAtial REasoning on \nThree-View Line Drawings,” Sep. 02, 2020, arXiv: arXiv:2003.14034. doi: \n10.48550/arXiv.2003.14034. \n[33] S. Cai, K. Bao, H. Guo, J. Zhang, J. Song, and B. Zheng, “GeoGPT4V: Towards Geometric Multi-\nmodal Large Language Models with Geometric Image Generation,” Jun. 17, 2024, arXiv: \narXiv:2406.11503. doi: 10.48550/arXiv.2406.11503. \n[34] P. Mooney, W. Cui, B. Guan, and L. Juhász, “Towards Understanding the Geospatial Skills of \nChatGPT: Taking a Geographic Information Systems (GIS) Exam,” in Proceedings of the 6th ACM \nSIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery, in GeoAI ’23. \nNew York, NY, USA: Association for Computing Machinery, Nov. 2023, pp. 85–94. doi: \n10.1145/3615886.3627745. \n[35] R. Tao and J. Xu, “Mapping with ChatGPT,” ISPRS International Journal of Geo-Information, vol. \n12, no. 7, Art. no. 7, Jul. 2023, doi: 10.3390/ijgi12070284. \n[36] X. Wei, “Evaluating chatGPT-4 and chatGPT-4o: performance insights from NAEP mathematics \nproblem solving,” Front. Educ., vol. 9, Sep. 2024, doi: 10.3389/feduc.2024.1452570. \n[37] I. Mirzadeh, K. Alizadeh, H. Shahrokhi, O. Tuzel, S. Bengio, and M. Farajtabar, “GSM-Symbolic: \nUnderstanding the Limitations of Mathematical Reasoning in Large Language Models,” Oct. 07, \n2024, arXiv: arXiv:2410.05229. Accessed: Oct. 27, 2024. [Online]. Available: \nhttp://arxiv.org/abs/2410.05229 \n[38] S. Frieder et al., “Mathematical Capabilities of ChatGPT,” Advances in Neural Information \nProcessing Systems, vol. 36, pp. 27699–27744, Dec. 2023. \n[39] N. Rane, “Enhancing Mathematical Capabilities through ChatGPT and Similar Generative Artificial \nIntelligence: Roles and Challenges in Solving Mathematical Problems,” Aug. 20, 2023, Rochester, \nNY: 4603237. doi: 10.2139/ssrn.4603237. \n[40] N. Che Hashim, N. A. Abd Majid, H. Arshad, W. Khalid Obeidy, and T. Mäkilä, “User Satisfaction \nfor an Augmented Reality Application to Support Productive Vocabulary Using Speech \nRecognition,” Adv. MultiMedia, vol. 2018, Jan. 2018, doi: 10.1155/2018/9753979. \n[41] K. Xia, X. Xie, H. Fan, and H. Liu, “An Intelligent Hybrid–Integrated System Using Speech \nRecognition and a 3D Display for Early Childhood Education,” Electronics, vol. 10, no. 15, Art. no. \n15, Jan. 2021, doi: 10.3390/electronics10151862. \n[42] Z. Shaghaghian, H. Burte, D. Song, and W. Yan, “An augmented reality application and experiment \nfor understanding and learning spatial transformation matrices,” Virtual Reality, vol. 28, no. 1, p. 12, \nJan. 2024, doi: 10.1007/s10055-023-00904-x. \n[43] “Gemini vs GPT-4V : A Preliminary Comparison and Combination of Vision-Language Models \nThrough Qualitative Cases,” ar5iv. Accessed: Oct. 10, 2024. [Online]. Available: \nhttps://ar5iv.labs.arxiv.org/html/2312.15011 \n[44] S. A. Sorby, “Educational Research in Developing 3‐D Spatial Skills for Engineering Students,” \nInternational Journal of Science Education, vol. 31, no. 3, pp. 459–480, Feb. 2009, doi: \n10.1080/09500690802595839. \n \n \n \n \n\n \n16 \nAppendices \nSome conversation examples from our experiments are included here.  \nA. GPT-4 Test Using Revised PSVT:R \nSample question from authors (June, 2024):  \n \n[Revised PSVT:R Question 2 screenshot image upload to GPT-4] \n \nSample answer from GPT-4, whose answer is B in the end (not necessarily to be correct): \nTo answer question 2, we need to visualize the rotation applied to the \nobject in the middle line and find the matching option from the bottom line. \n \n**Step-by-Step Analysis:** \n1. **Top Line**: Shows an initial object and its rotated version. \n2. **Middle Line**: Shows another object labeled \"As.\" \n3. **Bottom Line**: Provides five possible rotated versions labeled A, B, C, \nD, and E. \n \n**Top Line Rotation:** \n- The initial object in the top line is rotated such that the front face \n(with the cutout) moves to the left, and the side face is on the top. \n \n**Applying the Same Rotation:** \n- The object labeled \"As\" in the middle line will undergo the same rotation. \n- Visualize the rotation where the curved surface, currently facing forward, \nwill move to the left, and the flat surface will face upward. \n \n**Matching the Rotated Object:** \n- Option A does not match the rotation. \n- **Option B** matches the rotation. \n\n \n17 \n- Option C does not match the rotation. \n- Option D does not match the rotation. \n- Option E does not match the rotation. \n \nThus, the correct answer is **B**. \n \nB. Test Using Revised PSVT:R with a coordinate system axes \nSample prompt by authors into GPT-4 with the uploaded image based on Revised PSVT:R with \naxes added (September, 2024):  \nThe first line shows an object and how it is rotated. The task is to study how the object in the top line of \nthe question is rotated. Picture in your mind what the object shown in the middle line of the question looks \nlike when rotated in exactly the same manner. Select from among the five drawings (A, B, C, D, or E) \ngiven in the bottom line of the question the one that looks like the object rotated in the correct position.  \nObjects in the Example: The top line shows the initial object and its rotated version. The middle line shows \nanother object labeled \"As\". The bottom line provides five possible rotated versions labeled A, B, C, D, \nand E. To solve the example, you would visualize the rotation applied to the middle object (\"As\") and \nchoose the correct rotated version from the options provided. \n \nSample answer from GPT-4: \nTo solve this problem, let’s walk through the steps in detail by analyzing the rotation transformation \napplied to the top object and then applying it to the \"As\" object. \n1. Analyze the Rotation in the Top Row: In the top row, we see that the initial object is rotated to achieve \na new orientation. Observing the axes: \n• \nThe x-axis (green) in the initial object is rotated to the z-axis position (upward). \n• \nThe z-axis (blue) rotates to the x-axis position (forward). \n• \nThe y-axis (red) stays in the same orientation. \n2.Apply the Same Rotation to the \"As\" Object: Now, we apply this rotation to the middle object labeled \n\"As\": \n• \nRotate the x-axis (green) of \"As\" to where the z-axis would point (upward). \n• \nRotate the z-axis (blue) of \"As\" to the position of the x-axis (forward). \n• \nKeep the y-axis (red) in the same orientation. \n3.Identify the Matching Option: \n\n \n18 \n• \nNow, look at each option (A, B, C, D, E) in the bottom row to find the one that matches the rotated \n\"As\" object. \n• \nOption C has the correct orientation of the axes: \nThe green x-axis points upward. \n• \nThe blue z-axis points forward. \n• \nThe red y-axis is unchanged. \nTherefore, the correct answer is Option C. \nC. Test of Step 1of Revised PSVT:R with a coordinate system \nSample prompt by authors into GPT-4 with the uploaded image based on Step 1 of a Revised \nPSVT:R question with axes added (September, 2024): \nExplain the rotation process using minimum steps of rotation in terms of axis of rotation, angle of \nrotation, and direction of rotation. \n \nSample answer from GPT-4: \nTo describe the rotation process shown in this image with minimal steps, let's focus on the changes in \naxis orientation between the initial and rotated object. \nStep-by-Step Rotation Process \n1. Identify the Rotation Needed: \n• \nIn the initial object, the x-axis (green) is pointing to the left, the y-axis (red) is pointing forward, \nand the z-axis (blue) is pointing upward. \n• \nIn the rotated object, the x'-axis (green) is pointing forward, the y'-axis (red) is pointing to the \nright, and the z'-axis (blue) remains pointing upward. \nThis indicates a rotation that affects the x and y axes while keeping the z-axis unchanged. \n2. Rotation Details: \n• \nAxis of Rotation: Z-axis. \n• \nAngle of Rotation: 90 degrees. \n• \nDirection of Rotation: Clockwise when viewed from above. \nExplanation \nBy applying a 90-degree clockwise rotation around the z-axis, the x-axis moves from pointing left to \npointing forward, and the y-axis moves from pointing forward to pointing right, while the z-axis remains \nin its original upward orientation. \nThis minimal rotation successfully transforms the initial object to match the rotated version shown. \nD. Test of AR Rotation with Coordinate, Angle of Rotation, Corresponding Matrix Equation \nSample prompt by authors into GPT-4 with the uploaded image based on AR-Classroom (October, \n2024): \nExplain the rotation process with axis of rotation, angle of rotation and direction. Note: All red lines \nrepresent X-axis, all yellow-green lines represent Y-axis, all blue lines represent Z-axis. The dotted line \nis the orientation of axis after rotation. The axis of rotation does not have a dotted counterpart of the same \ncolor. Follow right hand rule for the direction of rotation. \n\n \n19 \n \nSample Answer from GPT-4: \nIn this image, we see a rotation of the model from an initial position (left) to a rotated position (right) \naround the Z-axis (blue line) by an angle of 106°. \nHere's a detailed breakdown of the rotation process: \n1. Axis of Rotation: \nThe Z-axis (blue line) serves as the axis around which the model rotates. This axis \nremains fixed and does not have a dotted counterpart because it acts as the pivot for the \nrotation. \n2. Angle of Rotation: \nThe rotation angle is 106°, as indicated in purple. This positive angle represents a \ncounterclockwise rotation around the Z-axis. \n3. Direction of Rotation: \nUsing the right-hand rule, if you point your right thumb along the Z-axis, your fingers \ncurl in the direction of the rotation. Since the angle is positive, the rotation proceeds \ncounterclockwise when viewed from the positive direction of the Z-axis. \n4. Resulting Orientation: \nAfter rotating 106° around the Z-axis, the X-axis (red line) and Y-axis (yellow-green \nline) shift orientation. The dotted red and yellow-green lines in the right image show the \nnew positions of the X and Y axes, respectively, after the rotation. The X-axis rotates to \na new position indicated by the dotted red line, and the Y-axis shifts to the dotted yellow-\ngreen line, showing how the model's alignment changes in 3D space. \nIn summary, this transformation represents a 106° rotation around the Z-axis. The Z-axis remains as \nthe fixed axis, while the X and Y axes change orientation, rotating counterclockwise according to the \nright-hand rule for a positive angle.",
    "pdf_filename": "AI's_Spatial_Intelligence_Evaluating_AI's_Understanding_of_Spatial_Transformations_in_PSVTR_and_Augm.pdf"
}