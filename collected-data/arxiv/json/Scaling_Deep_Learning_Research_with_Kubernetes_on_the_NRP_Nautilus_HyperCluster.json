{
    "title": "1",
    "abstract": "Throughout the scientific computing space, deep learning algorithms have shown excellent performance in a wide range of applications. As these deep neural networks (DNNs) continue to mature, the necessary compute required to train them has continuedtogrow.Today,modernDNNsrequiremillionsofFLOPsanddaystoweeksoftrainingtogenerateawell-trainedmodel. The training times required for DNNs are oftentimes a bottleneck in DNN research for a variety of deep learning applications, and as such, accelerating and scaling DNN training enables more robust and accelerated research. To that end, in this work, we exploreutilizingtheNRPNautilusHyperClustertoautomateandscaledeeplearningmodeltrainingforthreeseparateapplications of DNNs, including overhead object detection, burned area segmentation, and deforestation detection. In total, 234 deep neural models are trained on Nautilus, for a total time of 4,040 hours. I. INTRODUCTION Deepconvolutionalneuralnetworks(DCNNs)havebeenestablishedasthestateoftheartincomputervision(CV)andhave shownsuperiorperformanceinvisualtasksformanydomains,includingremotesensing.Withbillionsofpixelsbeingcollected byoverheadsourceslikesatellites,remotesensing(RS)isbecomingevermoreabig-dataproblemdomain,withendlessamounts ofdataavailabletoenableCVapplications.Dueinparttothisdataavailability,thetrainingandoptimizationofdeepnetworks for RS applications has been explored to great lengths in recent years. In 2017, researchers investigated utilizing DCNNs for land-cover classification in overhead imagery along with techniques such as transfer learning and data augmentation[1]. This workwasthenextendedintomulti-networkfusionresearch,wheremultipleDCNNstrainedonoverheadsatelliteimagerywere fused using simple fusion techniques such as voting and arrogance [2] and then compared to more complex fusion algorithms such as the Choquet and Sugeno Fuzzy Integral [3], [4]. While these studies explored utilizing DCNNs to perform classification on overhead RS imagery, further exploration was requiredinbroadareasearch,inwhichDCNNsaretrainedandusednotoncleanpre-processeddatasets,butinsteadappliedto large swaths of overhead imagery with the goal of finding all instances of a given object or terrain. In these applications, large swaths of imagery are investigated, usually on the order of tens or hundreds of thousands of square kilometers, if not more. For example, in [5], researchers utilized a DCNN to locate surface-to-air missile sites in the South China Sea. Surface-to-air missile site detection was then improved in [6], in which researchers employed spatial fusion of component detections. In another application, broad area search was also explored in [7], in which researchers utilized DCNNs to search for aircraft and other classes in a 9,500 square kilometer AOI surrounding Beijing. In each of these applications of DCNNs for Broad Area Search, the compute required is a limiting factor in applying these algorithms at scale. Asidefromclassificationandbroadareasearch,DCNNshavealsobeenutilizedtoperformobjectdetectiononoverheadRS imagery.In[8],theYOLOv3architecturewascomparedwithotherdeepnetworkarchitecturestodetectmilitaryvehiclegroups, and in [9], researchers used overhead imagery to perform maneuverability hazard detection with multiple DCNN-based object detection methodologies. More recently, as DCNNs have evolved and their operational characteristics have become more clear, researchers have investigated utilizing various techniques to address the shortcomings of DCNNs, including utilizing shape extraction to increase model performance, as in [10], [11]. ThroughoutCVandRSresearch,DCNNsarebeingutilizedtoenablemorereal-worldapplications,howeverasDCNNshave maturedandthenumberofconvolutionallayershasgrown,sotoohavethecomputerequirementstotrainmodels.Additionally, asDCNNhavegrowninpopularity,severalframeworkshavebeencreatedtoenablebetterscalingandreproducibilityoftraining deep networks. Among the benefits of these frameworks is the parallelization capabilities built into the training processes, however, these benefits can only be utilized if the appropriate hardware is available. Large numbers of GPUs are required to effectively train networks in a reasonable amount of time and keep batch sizes large enough to minimize the impact of diminishinggradients.Further,CPUparallelizationisoftenusedformanagingGPUresourcesindistributedtrainingparadigms as well as for asynchronous data loading and preprocessing during the training process. Inthiswork,weexploreutilizingtheNationalResearchPlatform(NRP)NautilusHyperClustertoaccelerateandscalethree separatescientificapplicationsutilizingDCNNs.Nautilusisanever-growingKubernetesclustercontainingover1300NVIDIA GPUs and 19,000 CPU Cores that can be utilized for a variety of applications, including teaching, research prototyping and exploration,andscalingofscientificcompute,whichwewillexplorehere.ThethreeDCNNapplicationsdiscussedinthisstudy are all in the overhead RS domain: overhead object detection with visual transformer architectures, burned area segmentation with U-Net, and deforestation detection in the Brazilian Amazon rain forest. Each of these applications requires an intense 4202 voN 81 ]GL.sc[ 1v83021.1142:viXra",
    "body": "1\nScaling Deep Learning Research with Kubernetes\non the NRP Nautilus HyperCluster\nJ. Alex Hurt, Anes Ouadou, Mariam Alshehri, Grant J. Scott\nUniversity of Missouri - Columbia\nAbstract\nThroughout the scientific computing space, deep learning algorithms have shown excellent performance in a wide range\nof applications. As these deep neural networks (DNNs) continue to mature, the necessary compute required to train them has\ncontinuedtogrow.Today,modernDNNsrequiremillionsofFLOPsanddaystoweeksoftrainingtogenerateawell-trainedmodel.\nThe training times required for DNNs are oftentimes a bottleneck in DNN research for a variety of deep learning applications,\nand as such, accelerating and scaling DNN training enables more robust and accelerated research. To that end, in this work, we\nexploreutilizingtheNRPNautilusHyperClustertoautomateandscaledeeplearningmodeltrainingforthreeseparateapplications\nof DNNs, including overhead object detection, burned area segmentation, and deforestation detection. In total, 234 deep neural\nmodels are trained on Nautilus, for a total time of 4,040 hours.\nI. INTRODUCTION\nDeepconvolutionalneuralnetworks(DCNNs)havebeenestablishedasthestateoftheartincomputervision(CV)andhave\nshownsuperiorperformanceinvisualtasksformanydomains,includingremotesensing.Withbillionsofpixelsbeingcollected\nbyoverheadsourceslikesatellites,remotesensing(RS)isbecomingevermoreabig-dataproblemdomain,withendlessamounts\nofdataavailabletoenableCVapplications.Dueinparttothisdataavailability,thetrainingandoptimizationofdeepnetworks\nfor RS applications has been explored to great lengths in recent years. In 2017, researchers investigated utilizing DCNNs for\nland-cover classification in overhead imagery along with techniques such as transfer learning and data augmentation[1]. This\nworkwasthenextendedintomulti-networkfusionresearch,wheremultipleDCNNstrainedonoverheadsatelliteimagerywere\nfused using simple fusion techniques such as voting and arrogance [2] and then compared to more complex fusion algorithms\nsuch as the Choquet and Sugeno Fuzzy Integral [3], [4].\nWhile these studies explored utilizing DCNNs to perform classification on overhead RS imagery, further exploration was\nrequiredinbroadareasearch,inwhichDCNNsaretrainedandusednotoncleanpre-processeddatasets,butinsteadappliedto\nlarge swaths of overhead imagery with the goal of finding all instances of a given object or terrain. In these applications, large\nswaths of imagery are investigated, usually on the order of tens or hundreds of thousands of square kilometers, if not more.\nFor example, in [5], researchers utilized a DCNN to locate surface-to-air missile sites in the South China Sea. Surface-to-air\nmissile site detection was then improved in [6], in which researchers employed spatial fusion of component detections. In\nanother application, broad area search was also explored in [7], in which researchers utilized DCNNs to search for aircraft and\nother classes in a 9,500 square kilometer AOI surrounding Beijing. In each of these applications of DCNNs for Broad Area\nSearch, the compute required is a limiting factor in applying these algorithms at scale.\nAsidefromclassificationandbroadareasearch,DCNNshavealsobeenutilizedtoperformobjectdetectiononoverheadRS\nimagery.In[8],theYOLOv3architecturewascomparedwithotherdeepnetworkarchitecturestodetectmilitaryvehiclegroups,\nand in [9], researchers used overhead imagery to perform maneuverability hazard detection with multiple DCNN-based object\ndetection methodologies. More recently, as DCNNs have evolved and their operational characteristics have become more clear,\nresearchers have investigated utilizing various techniques to address the shortcomings of DCNNs, including utilizing shape\nextraction to increase model performance, as in [10], [11].\nThroughoutCVandRSresearch,DCNNsarebeingutilizedtoenablemorereal-worldapplications,howeverasDCNNshave\nmaturedandthenumberofconvolutionallayershasgrown,sotoohavethecomputerequirementstotrainmodels.Additionally,\nasDCNNhavegrowninpopularity,severalframeworkshavebeencreatedtoenablebetterscalingandreproducibilityoftraining\ndeep networks. Among the benefits of these frameworks is the parallelization capabilities built into the training processes,\nhowever, these benefits can only be utilized if the appropriate hardware is available. Large numbers of GPUs are required\nto effectively train networks in a reasonable amount of time and keep batch sizes large enough to minimize the impact of\ndiminishinggradients.Further,CPUparallelizationisoftenusedformanagingGPUresourcesindistributedtrainingparadigms\nas well as for asynchronous data loading and preprocessing during the training process.\nInthiswork,weexploreutilizingtheNationalResearchPlatform(NRP)NautilusHyperClustertoaccelerateandscalethree\nseparatescientificapplicationsutilizingDCNNs.Nautilusisanever-growingKubernetesclustercontainingover1300NVIDIA\nGPUs and 19,000 CPU Cores that can be utilized for a variety of applications, including teaching, research prototyping and\nexploration,andscalingofscientificcompute,whichwewillexplorehere.ThethreeDCNNapplicationsdiscussedinthisstudy\nare all in the overhead RS domain: overhead object detection with visual transformer architectures, burned area segmentation\nwith U-Net, and deforestation detection in the Brazilian Amazon rain forest. Each of these applications requires an intense\n4202\nvoN\n81\n]GL.sc[\n1v83021.1142:viXra\n2\nRarePlanes DOTA XView\nFig.1. SampleScenesfromtheDatasetsselectedforOverheadDetectionwithTransformers:RarePlanes,DOTA,andXView\namount of compute to efficiently and exhaustively study, thus demonstrating the need for the Nautilus HyperCluster. In total,\ntheresearchperformedonNautilusfortheseapplicationssofartotalsover3,000GPUhoursofcomputeandnearly250trained\ndeep neural models.\nII. SCIENTIFICAPPLICATIONS\nThe compute available in the Nautilus HyperCluster enables a number of applications across the scientific community in a\nvariety of fields. Herein, we present three such applications requiring the vast compute available in Nautilus in the RS space,\nall of which utilize DCNNs as their primary methodology. The first research scaling enabled by Nautilus is an investigation\ninto how visual transformers, which have seen a rise in popularity in ground-based CV research, translate into the RS space.\nThe next application discussed is wildfire-burned area mapping, in which semantic segmentation models are utilized to build\nmodels that can assist with the recovery efforts from wildfires in North America. Finally, Nautilus is used to accelerate the\nresearch into detecting and mapping deforestation of the Brazilian Amazon rainforest. More details of these domains and their\nrespective methodologies are given in the sections below.\nA. Overhead Object Detection with Transformers\nWhile convolutional-based deep neural networks have shown excellent performance in CV applications, with the recent\npublication of Visual Transformers, we have again seen a leap in computational vision capabilities. As such, it is imperative to\nunderstand how various transformer-based neural networks perform on satellite imagery. While transformers have shown high\nlevels of performance in natural language processing and CV applications, they have yet to be compared on a large scale to\nmodern RS data. This comparison is especially necessary because oftentimes, algorithms and methodologies applied in ground\nphoto CV applications do not effectively translate to RS applications, as the unique characteristics of RS imagery present\nchallenges to new techniques. Additionally, transfer learning has been shown to provide an incredible benefit to deep neural\nmodels (see [1]), however, most pretrained weights easily available are trained on ground photo datasets like ImageNet [12]\nandCOCO[13],whichwhilerobust,arenotnecessarilythemostapplicabledatasetfortransferlearningintheRSdomain.By\ntraining models on popular convolutional and transformer-based models on overhead benchmark datasets, we not only enable\na comparison of their performance in the RS domain but also simultaneously generate weights that can be used for better\ntransfer learning for overhead vision applications.\n3\nMeanwhile, to effectively evaluate transformer architectures on overhead imagery and generate pretrained weights for\ntransformer architectures in the RS domain, an incredible amount of compute is also required. Research has shown that\ntransformerarchitecturesrequiremoretrainingtimeanddatatoeffectivelytrainthantheirconvolutionalsiblings.Toeffectively\nevaluatetransformersintheRSspace,wemusttrainabreadthofarchitecturesutilizingdifferingfeatureextractionanddetection\ntechniques.Further,inordertoderiveconclusionsaboutanysinglearchitecture’sperformancecharacteristics,weneedtoutilize\nmultipledatasetsofvaryingsizes.Combiningthesefactorsnecessitatesthousandsofhoursofcomputeonpowerfulhardwarein\norder to generate the number of trained models required to make conclusions on the applicability of transformers on overhead\nimagery. This level of compute is available in Nautilus with its 1,300 NVIDIA GPUs, thousands of CPU cores, and terabytes\nof available memory. Finally, orchestration and repeatability are paramount to ensuring sound scientific results, as well as\nscalingresearchcompute.TechnologiessuchasKubernetesandDockerenablethisreproducibilityandorchestratethesoftware\nneeded to train and test such a vast number of models.\nForthisresearch,wecomparetendistinctbounding-boxdetectionandlocalizationalgorithms,ofwhichsevenwerepublished\nsince2020,andalltensince2015.Theperformanceoffourtransformer-basedarchitecturesiscomparedwithsixconvolutional\nnetworksonthreestate-of-the-artopen-sourcehigh-resolutionRSimagerydatasetsranginginsizeandcomplexity.Descriptions\nof the networks utilized for this research as well as the datasets on which these networks are trained are described below:\n1) Two-StageConvolutionalNetworks: Two-stageconvolutional-basedmethodsperformregionproposalasaninitialfeature\nprocessingstepandthenutilizetheselearnedregionproposalstoperformboundingboxlocalization.Themostpopularmethod\nfor two-stage detection is the Faster R-CNN [14], which utilizes a Region Proposal Network (RPN) to learn the set of region\nproposal from the most course learned feature map. In this research, we utilize a Faster R-CNN as the detection methodology\nfor several backbones, both convolution and transformer. The convolutional backbone used for Faster R-CNN here is the\nConvNeXt [15] backbone.\n2) Single-StageConvolutionalNetworks: Unliketwo-stagedetectors,singlestagenetworkslearntheboundingboxlocaliza-\ntion directly from the learned feature maps without needing prior region proposal. These networks are generally less compute\nintensive and therefore more appropriate for real time applications. The single-stage methods investigated in our comparison\nwith transformers are Single Shot Detector (SSD) [16], RetinaNet [17], Fully Convolutional One-Stage Object Detection\n(FCOS) [18], and YOLOX [19]. The feature extraction architectures used for these networks are VGG16 [20], ResNeXt-101\n[21], ResNet-101 [22], and YOLOX-XL, respectively.\n3) Visual Transformer Networks: We train and evaluate three families of visual transformer networks to compare with\nthe aforementioned convolutional models. The first of these is the original Vision Transformer (ViT) [23], which utilized\ntransformer techniques from Natural Language Processing and modified them for use in CV. The next set of transformer\narchitectures utilized here is the DETR [24] and Deformable DETR [25] networks. These networks, unlike ViT, are end-to-end\ndetection networks utilizng only transformers and have shown promising result in traditional CV applications. The final model\nused in this study is the SWIN Transformer, specifically SWIN-T [26], which is an improvement on the original ViT in terms\nof both network performance and computational efficiency. Since both SWIN-T and ViT are general-use feature extraction\narchitectures rather than end-to-end detection networks, Faster R-CNN will be utilized as the bounding box localization for\neach of these models.\n4) Datasets: Threeopen-sourcehigh-resolutionremotesensingimagery(RSI)datasetsareinvestigatedinthisstudy.Utilizing\nmultiple datasets in our investigation of transformer architectures is crucial for ensuring that any conclusions and analyses are\nnot biased by a models’ performance on a single dataset. To that end, we select three visually and characteristically distinct\ndatasets for this study, as can be seen in Figure 1. The first of these, the RarePlanes dataset [27], is a 7-class aircraft detection\ndataset containing just over 25,000 samples. Next, the DOTA dataset [28], is a general-use overhead RSI dataset containing\naround 250,000 objects belonging to 16 classes. Finally, the XView dataset is a large-scale RSI dataset, containing over a\nmilliontotalobjectsin60classes.Eachofthesedatasetspresentsdistinctchallengestoourobjectdetectionmodelsandshould\nenable a robust comparison. Note that for DOTA and XView, which contain large scenes, preprocessing is applied to chip the\nimagery and labels to 512x512 with overlap to enable more robust GPU acceleration on Nautilus.\nB. Burned Area Segmentation\nWildfires have become common all over the world. Every year multiple fires burn hundreds of thousands of acres in North\nAmericaandworldwide.Thesefirescancoverlargeareasanddifferentlandcovers,whichmakesthetaskofpreciselymapping\nthe burned area (BA) an arduous one. The exact mapping of the BA faces many obstacles, such as difficulty to access the\nremote area where the fire took place, the discrepancy between agencies and parties responsible for accomplishing this task,\nand human error.\nEarth observation (EO) data opens an alternative possibility for field surveys and facilitates collaboration between different\nagencies, reducing discrepancy and missing data.\nSentinel-2 is one of the platforms used for acquiring EO data, launched by the European Space Agency (ESA) through their\nprogram Copernicus.It offers rasters composed of 13 bands with the highest of these medium-resolution sensors at 10m and a\nrevisit time of five days thanks to the twin satellites placed at 180 degrees from each other. This makes Sentinel-2 an ideal,\ncost-effective sensor for BA mapping.\n4\nFig.2. Exampleofground-truthpolygonsforBurnedAreaMapping:redboundingboxesdrawnaroundtheburnedareapolygons\nFig.3. ExamplesofrasterstobedownloadedforBurnedAreaMapping.Eachorangesquarerepresentsaraster.\nAvarietyofapproacheshavebeenutilizedforBAmapping.Temporalstudiesthatusespectralindices(SI)[29]thatcombine\ndifferent bands and compare their outcome before and after the fire were first developed. These methods produced varying\nresults depending on the bands used and land cover. Machine learning techniques (ML), such as random forest, have also been\nusedonSentinelandLandsatdata[30].FollowingrecenttrendsinCV,deeplearninghasbeenemployedforBAmapping,using\napproaches such as Siamese Self-Attention (SSA)[31] and U-Net [32]. This problem is perfect for deep learning techniques,\nbut not many have been employed so far.\nDCNNs have been used for a variety of CV problems, from image classification to object detection. Semantic segmentation\nis the natural next step in the list of CV tasks to be tackled by DCNNs where classification is performed at the pixel level.\nThe first attempt at deep learning semantic segmentation was done using a fully convolutional network (FCN) [33]. Then the\nencoder-decoder structures were used to build the U-Net [34] and U-Net++ [35]. In U-Net architectures, the image first goes\nthrough an encoder that is built using convolutional layers followed by pooling layers. Next, the feature maps pass through a\ndecoder where the features are upsampled using a transpose convolutional layer. U-Net also possesses horizontal connections\nthat connect layers in the encoder part with layers in the decoder part of the network.\nDeepLab is another semantic segmentation network developed by Google, this network has seen many variants, with the\nlatest form DeepLabV3 [36] and DeepLabV3+[37]. DeepLab uses atrous convolutional layers to replace some of the pooling\nlayers and to build a spatial pyramid to account for objects with varying scales.\nWeframetheBAmappingasasemanticsegmentationproblemforwhichwetrainDCNNmodelsonsatelliteimages.These\nsatellite images require a large storage capability as well as computing power, a task for which the Nautilus HyperCluster is\n5\nImages\nMasks\nFig.4. ExamplechipsusedfortrainingBurnedAreaMappingmodels\nTABLEI\nNUMBEROFJOBSRUNANDAMOUNTOFDATAPROCESSEDATEACHSTAGEFORBURNEDAREAMAPPING.\nPhase Download Norm Label Chip Total\nJobs 46 46 46 36 174\nData(GB) 146 808.41 20.43 17.75 992.6\nbest suited. We developed a process to download and process these satellite images to create a dataset. We took advantage of\nthe vast resources of Nautilus to train 144 models to determine the best hyperparameters for our problem. We then used these\nparameters to determine which of the four architectures produces the best results.\n1) Imagery: The EO data we are using is Sentinel-2 L2A, bottom-of-atmosphere reflectance satellite imagery. Each raster\nhas 13 bands distributed over three different resolutions (10m, 20m, and 60m). They are easy to obtain through their portal\nwhere the only requirement is to have an active account. From the 13 bands, we select the Red, Green, and Blue bands at\n10m resolution. These three bands are then stacked together and normalized to create an RGB image. While there are multiple\ndifferentmethodstonormalizethesebands,wechosetonormalizeourbandsusingthe1stand99thpercentilesastheminimum\nand maximum values, respectively, for clamping and stretching the histogram. In addition to the normalized image, We also\nuse the true-colored image (TCI) supplied with the raster bands.\n2) Burned area ground truth: The BA ground truth is in the form of Geo-tagged polygons. These polygons are provided by\nthe Canadian Wildland Fire Information System (CWFIS)1. We start from the ground truth polygons and begin by processing\nthem to get the appropriate coordinates to download Sentinel-2L2A rasters, and for the creation of the semantic segmentation\nground-truth mask. Figure 2 shows the bounding boxes (BB) created around the burned area. The area of each of these boxes\nis equal to or larger than the area of a single raster. We then use these coordinates to download the appropriate rasters, as\nshown in Figure 3.\nSince the rasters we are using are from the year 2019, they are older than one year. In the Copernicus hub, all images older\nthan one year are automatically moved to Long Time Archive (LTA). So each raster needs to be requested individually. Once\nall rasters are online, they can be downloaded together in bulk.\nAsingleSentinel-2L2Arastercoversanareaof100km×100kmwhich,dependingonthebandresolution,couldrangefrom\n10,000×10,000 pixels to 1830×1830 pixels. There is a significant need for storage space that can only be satisfied by a\ncluster such as Nautilus. In addition, these rasters cover different areas at different periods of time, so each group of rasters\nneeds to be downloaded separately and independently from each other.\nRastersarenormalizedasdescribedinII-B1.Next,WeusetherasterizemethodintheRasteriolibraryalongwiththerasters’\ncoordinates reference system and the ground truth polygons to create the semantic segmentation masks. Lastly, we generate\ntraining chips from the larger scenes using a sliding window of size 256×256 pixels with 25% overlap between every two\nconsecutive chips. We choose to include only the chips that contain both classes; we set the threshold to select a chip to be\n1BurnPolygons:https://cwfis.cfs.nrcan.gc.ca/datamart/download/nbac?\ntoken=888fee58cba7b45d7a68ccbcc19dade1\n6\nFig.5. TheAmazonBiome(top)isoutlinedbytheyellowboundaryandtheconservationunitsarehighlightedinred.Color-shiftedinfraredfromchipsfor\n2020(left)and2021(right)highlightdeforestationchangesupclose.\nat least 10% of burned and unburned pixels, and we remove chips that do not meet this threshold. Thus, the number of chips\ngenerated from each raster depends mainly on the size of the burned area. We generated the input chips from both normalized\nrastersandTCIs,andwegeneratedthecorrespondingmaskchipsfromthecreatedrastermasks.Anexampleofthechipsused\nin training is shown in Fig. 4.\n3) Data Acquisition Scaling: We treat each red box in Figure 2 independently from each other, where we get a batch of\nrasters from each box. So we ran the entire data processing pipeline for each batch of rasters in parallel. The coordinates\nof each of the red bounding boxes are used to acquire a batch of rasters, and each batch of rasters is downloaded using a\nKubernetes job on Nautilus, with multiple jobs spun in parallel to download these rasters, enabling us to download our data in\na very short period of time. We then use multiple jobs to process our rasters and normalize them, which allowed us to quickly\nprocess the entire dataset. Next, we generate a label mask for each raster, using the rasterize method from the Rasterio library.\nThese masks are 10,000×10,000 pixels each. Finally, we generate training chips from TCI images, normalized images, and\nlabel masks. Table I shows a breakdown of the number of jobs run at each stage as well as the amount of data processed.\nWedownloaded144rastersthatgeneratedatotalof7273chips.Thereweresomeredundantrastersthatgeneratedredundant\nchips.Soweremovedtheredundantdata.Ourfinaldatasethas63rastersgenerating5762chips.Thenumberofchipsgenerated\nby each raster varies from a few chips per raster to over 700 chips per raster. Instead of blindly splitting our dataset 70%\nfor training, 20% for validation, and 10% for testing. We chose to split our dataset by rasters. We chose to use rasters that\ngenerated a large number of chips in our training and validation sets, and use rasters with a few chips in our test set as this\nwill make our test set more diverse. The training set has 20 rasters in total generating 3928 chips representing 68% of the\ntotal number of chips. The validation set has three rasters in total generating 1120 chips representing 20% of the total number\nof chips. The test set has 40 rasters in total generating 714 chips representing 12% of the total number of chips. The dataset\nis staged on a persistent volume on Nautilus where the entire process is run.\nC. Deforestation Detection in the Brazilian Amazon\nDeforestationposesaseverethreattothenaturalecosystem,causingadepletionofbiodiversity,instabilitywithinecosystems,\nand contributing to climate change. As the largest rainforest in the world, the Brazilian Amazon plays a critical role in\nregulating climate and carbon levels. Nevertheless, the rampant rate of deforestation has led to a host of issues, including\nincreased greenhouse gas emissions, reduced carbon storage capacity, and a higher frequency of forest fires [38]. To address\n7\nTABLEII\nTHETOPFIVECONSERVATIONUNITSBYSIZEOFDEFORESTATIONINTHEBRAZILIANAMAZON\nArea Area Deforestation Deforestation\nID (km2) (km2) (%)\n59 16792 528.33 3.14\n89 13017 214.59 1.63\n55 1974 108.92 5.51\n291 20395 114.70 0.56\n165 9315 91.31 0.98\nthe catastrophic implications of deforestation, it is crucial to implement effective policies that rely on accurate and timely data.\nDeforestation detection (DD) is the primary means of obtaining this critical data to preserve the Brazilian Amazon. Traditional\nDD methods such as map interpretation, field surveys, and ancillary data analysis are labor-intensive and time-consuming due\nto the vast size of the region (covering approximately 5.2 million km2 of land). To combat this obstacle, RSI has emerged\nas a practical and cost-effective solution, providing consistent and repeatable data over large areas. Among the available RSI\nsources, the European Space Agency’s Sentinel-2 is an ideal open-access data source for DD, with its 10 m resolution and\n5-day revisit rate.\nIn RSI changedetection (CD) applications, quantitative analysis isdone to identify surface changes from images captured at\ndifferent timestamps. With its attention mechanism, the transformer model exhibits promise in accommodating multi-temporal\nimages, as it is scalable, captures long-range sequence features, and facilitates efficient parallel processing. This study aims to\ninvestigate the effectiveness of transformer-based networks in detecting deforestation in the Brazilian Amazon.\n1) Data Acquisition: We used Sentinel-2 Level-2A Surface Reflectance images with a maximum cloud cover percentage\nof 20%. We obtained ground truth polygons from the PRODES project datasets, provided by the Brazilian National Space\nAgency (INPE). PRODES has monitored and quantified annual deforestation rates in the Brazilian Amazon rainforest since\n1988, utilizing medium-resolution satellite imagery and a team of experts to visually interpret the data. The PRODES data is\naccessible to everyone on the TerraBrasilis website 2. Figure 5 (top) displays the polygons of the Amazon conservation units,\nas well as the boundary of the Amazon Biome. We selected the top-5 conservation units with the highest deforested land areas\nfor 2020 and 2021 (see Table II). To determine the appropriate date range for each selected area, we identified the earliest\ndate d and the latest date d of all the images used to label that area in PRODES. These dates fall in the Amazon’s dry\n1 2\nseason–typically from late June to early September– when imagery is clearer with fewer clouds. The selected date range for\neach area was then set to [d −30 days,d +30 days]. This range ensures that the images used in our study were as close as\n1 2\npossible to the labeling dates so significant deforestation is less likely to have developed during this interim period between\nimage acquisition and labeling.\n2) Band Combinations: Sentinel-2 images are composed of 13 bands at varying resolutions of 10 m, 20 m, and 60 m. We\nselected the visible and near-infrared bands at 10 m resolution and Scene Classification Layer (SCL) at 20 m resolution. The\nnear-infraredbanddistinguishesvegetationfromotherfeatures,whiletheSCLbandactedasamasktoeliminatecloudorcloud-\nshadow areas. We employed three band combinations, Color-shifted Infrared (NIR-R-G), Normalized Difference Vegetation\nIndex (NDVI), and Enhanced Vegetation Index (EVI), to highlight vegetation spectral signatures. Figure 5 (right) highlights\nthe deforestation in two NIR-R-G images taken in 2020 and 2021.\n3) Dataset Creation: To ensure a representative and unbiased dataset, we applied careful preprocessing procedures. Linear\nnormalization was first applied to facilitate image comparison on a consistent scale, with minimum and maximum values\nselectedfromthevaluehistogram’s1st and99th percentiles.Weappliedfilteringatboththerasterandchiplevels.High-quality\nrasters were selected manually, while at the chip level, we removed single-class chips and those with less than 10% “change”\nclass area. We used rotation augmentation at 90 and 180 degrees to increase dataset size and enhance model robustness. At the\nendofthechipcreationprocessatotalof7,734pairsofchipsin256×256sizeand1,406pairsofchipsin512×512sizewere\ngenerated. The dataset then was split into three subsets: (60%, 20%, 20%) for training, testing, and validation, respectively.\nIII. EXPERIMENTS\nWe now move to the experimental results and analysis for the three aforementioned scientific applications. Each application\nutilizes its own training and testing methodologies and measures separate performance metrics based relevant to each scientific\napplication. Detailed descriptions of these results are given below.\nA. Overhead Object Detection with Transformers\nUsingNautilus,wehavetrainedandtestedtendistinctmodelsonthreeseparateRSIdatasets,generating30separatemodels.\nEachofthesetenarchitecturesistrainedonthethreechosendatasetsasdescribedinSect.II-A.Allmodelsarepretrainedfrom\nCOCOweightsexceptforSWINandConvNext,whichuseImageNet-1kweights,andalllearningisperformedwithStochastic\n2TerrabrasilisDownload:http://terrabrasilis.dpi.inpe.br/en/download-2/\n8\nTABLEIII\nTRANSFORMERCOMPUTESTATISTICS,INCLUDINGPARAMETERSOPTIMIZEDPERNETWORK,ASWELLASTHEPERMODELGPU-TIMEANDGPU\nMEMORYACROSSALL30MODELS.\nRarePlanes DOTA XView\nModel Params(M)\nGPU-hours VRAM(GB) GPU-hours VRAM(GB) GPU-hours VRAM(GB)\nConvNext 67.1 18.4 9.7 63.4 9.0 60.4 11.3\nSSD 36.0 20.6 7.4 53.4 8.6 57.1 8.9\nRetinaNet 95.5 56.3 6.5 56.5 24.2 58.1 13.7\nFCOS 89.8 24.5 5.4 59.8 5.4 56.2 30.5\nYOLOv3 62.0 50.9 9.8 56.7 4.1 55.3 3.7\nYOLOX 99.1 12.1 21.1 56.4 42.2 56.9 40.1\nViT 97.7 8.0 12.6 59.3 24.7 60.3 25.1\nDETR 41.3 11.0 9.7 58.1 2.5 56.2 2.5\nDeformableDETR 40.9 16.7 20.1 59.1 6.1 61.9 17.9\nSWIN 45.2 22.7 19.9 57.7 37.9 58.2 13.7\nTOTAL 674.6 241.2 122.2 580.4 164.7 580.6 167.4\nFig.6. SystemOverviewforOverheadDetectionwithTransformers\nGradient Descent (SGD), except for SWIN and Deformable DETR, which are optimized using AdamW [39]. Hyperparameter\nconfiguration and dataset preprocessing vary by model and are set by mirroring the hyperparameters and image processing\nusedinthegenerationofthepretrainedweights,soastoenablethemosteffectivetransferlearningperformance.Allpretrained\nweights used and all models trained come from the open source MMDetection framework [40].\nExperiments are started in an automation fashion, in which containerized models are trained by Kubernetes jobs on the\ncluster as shown in Fig. 6. Data is staged in persistent volumes on the cluster prior to the beginning of training, and all\nmodels are copied to S3 cloud storage following training to ensure their later availability for evaluation. Training and testing\nare performed utilizing environment variables that are updated using bash automation scripts that take in the set of models and\ndatasets that are to be trained and launch each job on the Nautilus cluster in an automated fashion. Four GPUs are utilized for\neach model’s training, and the batch size is dynamically set based on available GPU memory, as the GPUs on Nautilus range\nfrom as little as the NVIDIA GTX 1080 (11 GB) to as high as the NVIDIA A100 (80GB). In total, 30 models are trained in\nparallel on Nautilus, for a total of 9,000 epochs of training and over 2.02 billion learnable parameters optimized. More than\n137 TB of imagery are processed throughout the training processes and the total wall clock time of this compute is 2,142\nhours. Per-network learnable parameters along with per-model GPU-time and GPU memory are shown in Table III.\n9\nTABLEIV\nCOMPARISONOFTHEPERFORMANCEANDTRAININGTIMEOFTHEFOURMODELS:U-NET,U-NET++,DEEPLABV3,ANDDEEPLABV3+USINGTHE\nBESTHYPER-PARAMETERS\nClass1(BurnedArea) Time(s)\nModel Prec(%) Rec(%) F1 IoU Avg\nU-Net 83.03 80.88 0.82 0.694 22120\nU-Net++ 82.78 81.97 0.824 0.700 31716\nDeepLabV3 83.71 83.78 0.837 0.720 23731\nDeepLabV3+ 84.35 82.29 0.833 0.714 21607\nReviewing the results of transformer model training statistics (Table III), we see that DOTA and XView required similar\namounts of both GPU-time and GPU memory to generate the trained models, which is expected given the similar number of\nforeground images in each dataset. RarePlanes, meanwhile, required less than half the GPU-hours to generate the ten trained\nmodels, but still required over 122 GB of VRAM. Note as well the diversity in VRAM used by each model and dataset\ncombination. The increased VRAM used for YOLOX and FOCS enabled larger batch sizes, but this did not necessarily lead\nto lower training times, due in part to the incredibly high number of learnable parameters (over 90 million).\nIntermsofmodeldetection performance,resultsrangedforeachdatasetand network.However,theoverallresultsindicated\nthat SWIN and YOLOX were the best-performing networks, with AP50 scores exceeding 70% on RarePlanes and 60% on\nDOTA. The results generated from this investigation will enable many insights into how transformers are able to perform\non RSI and serve as a basis for performance expectations in real-world RS applications such as broad area search and other\nmachine-assisted visual analytics applications. Additionally, having these trained models on hand enables transfer learning for\nmore novel RSI datasets in a variety of RS domains, as the performance benefits of transfer learning from relevant pretrained\nweights have been well documented in the CV space.\nB. Burned Area Segmentation\nTraining a DCNN requires the selection of crucial hyperparameters, which can be challenging for new problem domains,\nsuch as Burned Area Segmentation. There exist two potential approaches to making these hyperparameter choices. The first\nis that we can copy the training parameters from what may seem like a similar problem, in hopes that we generate a well-\ntrained, generalizable model for our problem domain. The other approach is to train the model with different sets of these\nhyperparameters until we find the best combination. This approach is both time-consuming and compute-intensive, as the local\nmachine we use may not have sufficient resources to train a model efficiently. Nautilus offers us a solution to this problem,\nwith large amounts of compute resources that can be leveraged to train multiple large models utilizing Kubernetes, which then\nallows us to manage the training of multiple models in parallel. We utilize the resources available in Nautilus to search for\nthe most appropriate combination of hyperparameters to train a DCNN for BA mapping. These parameters can then be used\nto train prominent DCNN architectures to see which network performs best.\nWe identified the set parameters that we want to investigate and select an appropriate set of values for each parameter. We\nevaluated three different learning rates: 1×10−3, 1×10−4, and 1×10−5 as well as three different batch sizes: 8,16 and 32.\nWe also examined the effect of weight initialization on our models, so we initialized our models once with weights pre-trained\non the ImageNet dataset [12], and once randomly. Lastly, we examined two different optimizers Adam and LAMB. In addition\nto the various hyperparameter values, we trained our models with two sets of data. The first set is from the chips we generated\nfromthenormalizedRGBimagesandtheothersetismadeupofchipsgeneratedfromtheTCIimagesprovidedbySentinel-2\nL2A. Each combination of the values of these parameters constitutes an experiment, 72 in total. We trained a U-Net model\nand a DeeplabV3 model with the ResNet-50 backbone for each experiment for 100 epochs. In total 144 models were trained.\nThe training process is run on the same persistent volume where the data is staged. We used a custom semantic segmentation\nframeworkbasedonPytorchtotrainourmodels.ThisframeworktakesinaJSONconfigurationfilewherethespecificsofeach\nexperiment are defined. We autogenerat these configuration files using the open-source Jinja2 library. For each experiment, we\nneed two YAML files, one for training, and one for testing and evaluation. In total, we generate 288 YAML files. Instead of\ncreating these files manually, we again relied upon Jinja2 library to autogenerate these files. With the configuration files and\nthe Kubernetes YAML files ready, we can proceed with the training of our models. Rather than manually submit these jobs,\nwe opted to use a bash file to submit all jobs automatically. We were able to allocate 24GB of memory, four CPUs, and two\nGPUs for each model. We trained and evaluated 144 models in parallel for a total of 14400 epochs with 3.6 billion parameters\noptimized. We processed over five TB of imagery during the training process taking in a total of 518 hours of wall clock time.\nThanks to these experiments we determined that, the learning rate 1e-5, batch size 32, and optimizer LAMB to be the best\nparameters to have smooth and stable decreasing training and validation losses. We also found that the models initialized with\nthe ImageNet dataset weight performed better than a randomly initialized model.\nWeusedtheseparameterstoenhancethetrainingofabroadersetofmodels:U-Net,U-Net++,DeepLabV3,andDeepLabV3+\nWe increased the number of epochs to 200 and introduced a scheduler such that the learning rate decreases by a factor of 0.5\nevery 50 epochs. We were able to achieve the results shown in Table IV.\n10\nTABLEV\nSUMMARYOFCOMPUTEPERFORMEDONNAUTILUS\nScientificApplication Networks Models Parameters(M) Imagery(GB) Epochs Time(h)\nDetectionwithTransformers 10 30 2024 1370 9000 2142\nBurnedAreaSegmentation 4 144 3600 5175 14400 518\nDeforestationDetection 1 60 2460 31200 12000 1380\nTOTAL 15 234 8084 37745 35200 4040\nFig. 7. A simplified overview of the ChangeFormer architecture, showing its three main components: a siamese hierarchical transformer encoder, four\ndifferencemodules,andalightweightMLPdecoder.\nC. Deforestation Detection in the Brazilian Amazon\nChangeFormer [41], a transformer-based network that is specifically designed for change detection, was used in this study.\nThis network has demonstrated excellent performance in detecting both building and general changes in urban areas. Figure 7\npresentsasimplifieddiagramoftheChangeFormerarchitecture.Themodelwasrandomlyinitializedandfine-tunedbyadjusting\nits hyperparameters to optimize its performance. We trained more than 60 model configurations for 200 epochs and evaluated\ntheir performance using overall and change-class-specific metrics such as F1 score, IoU score, precision, and recall. Training\nentailsprocessingover112millionimagesofsize256×256and8millionimagesofsize512×512.Thecumulativewall-clock\ntime for data pipeline and training is estimated to be 1380 hours.\nWe found that the 256×256 chip size outperformed the 512×512 chip size in all metrics, likely due to the availability of\nmore training chips, with the top three results from the former surpassing the best result from the latter. We further determined\nthat the optimal combination for the ChangeFormer model was a learning rate of 0.0001, CE loss, AdamW optimizer, and the\nNIR-R-G band. This achieved an overall accuracy of 94%/92% (256×256/512×512) with mIoU scores of 83%/80% and F1\nscores of 90%/88%, respectively. A previous study [42] also applied deep learning for deforestation detection by comparing\nvarious fully convolutional network architectures. Similar to our work, they used the PRODES dataset for ground truth, and\nthey utilized both Sentinel 2 and Landsat-8 satellites imagery. The best result for their study was achieved by FC-DenseNet\nwith an F1-score of 70.7%. However, ChangeFormer model obtained at least 81% F1-score for the change-class in both chip\nsizes.Similartrends wereobservedinrecall and precision,whereweachieved 80%and86%in ourtrainingprocesscompared\nto 75.1% and 78% reported in the previous study.\n11\nFig.8. Threeimagesampleswith correspondingpredictedandgroundtruth deforestationmaps.Thefirsttwo columnsshowimagesfrom 2020and2021,\nrespectively.Thethirdandfourthcolumnsdisplaythepredictedandgroundtruthmaps,respectively.\nD. Scaling Overhead Vision with Nautilus\nIn each of the above scientific applications, we have demonstrated a clear motivation for scaling deep learning research\napplications. Effectively studying these problems requires an inordinate amount of compute that is not feasible for a single\nGPU-enabled server to complete in a reasonable amount of time. To that end, we have scaled each of these research problems\nutilizing the Nautilus HyperCluster. A summary of the compute performed can be found in Table V.\nIn total, over 4000 hours of compute are performed in parallel on Nautilus, the equivalent of over five and a half months\nif this compute were to be performed on a single server. In performing this compute, over 37 TB of imagery is processed by\nthe nodes on the cluster, and over 8 billion learnable parameters are optimized in the generation of over 230 models. Scaling\nthese research problems has enabled key takeaways and applications as well as opportunities for further research in each of\nthese areas that would not have been possible without the scaling of the training methodologies to Nautilus.\nIV. CONCLUSION\nModerndeeplearningresearchislimitedinmanywaysbythecomputerequiredtotrainandtestmodels,evenmoresowhen\napplied at scale. In this research, we have shown the benefits of utilizing the Nautilus HyperCluster to robustly automate and\nscaledeeplearningforhigh-resolutionRSresearch.Wehavedemonstratedthebenefitsandabilitytoacceleratethetrainingand\ntesting of deep neural models for three separate deep learning applications in RS, including an exploratory investigation into\ndeep neural transformers and the development of models capable of performing humanitarian aid: segmenting burned areas in\nNorthAmericaanddetectingdeforestationintheBrazilianAmazon.Inourtransformerinvestigation,wegeneratedthirtysetsof\nweightsthatcanbeusedtoenablemorerobustoverheadRSapplications.Meanwhile,inburnedareamapping,wewereableto\nautomateandexplorethecostlyhyperparametertuningrequiredtoapplydeepneuralmodelstonewproblemdomains.Finally,\nin applying DCNNs to deforestation detection, our trained models were able to outperform previously published competing\nmethods by more than 10%, due in part to the large grid of parameters that were able to be tested on the Nautilus cluster.\nFuture work for this research involves the development of more robust automation tools, perhaps utilizing the open-source\nKubernetes Python API to build a Python library or application that can more easily and reliably manage jobs for training and\ntesting deep learning models. Additionally, future work will involve continuing to scale and improve model performance in\neach of these RS applications using deep learning on Nautilus. Finally, utilizing Kubernetes to train models across multiple\npods to train large models more effectively, regardless of the available GPU memory on a single pod would enable even more\nscalability, which may be necessary to effectively evaluate the incredibly large models being developed and released for RS\nresearch today.\n12\nACKNOWLEDGEMENT\nComputing resources for this research have been supported by the NSF National Research Platform and NSF OAC Award\n#2322218 (GP-ENGINE).\nREFERENCES\n[1] G.J.Scott,M.R.England,W.A.Starms,R.A.Marcum,andC.H.Davis, “Trainingdeepconvolutionalneuralnetworksforland-coverclassification\nofhigh-resolutionimagery,” IEEEGRSL,vol.14,no.4,pp.549–553,2017.\n[2] G.J.Scott,R.A.Marcum,C.H.Davis,andT.W.Nivin, “Fusionofdeepconvolutionalneuralnetworksforlandcoverclassificationofhigh-resolution\nimagery,” IEEEGRSL,vol.14,no.9,2017.\n[3] GrantJScott,KyleCHagan,RichardAMarcum,JamesAlexHurt,DerekTAnderson,andCurtHDavis, “Enhancedfusionofdeepneuralnetworks\nforclassificationofbenchmarkhigh-resolutionimagedatasets,” IEEEGRSL,vol.15,no.9,pp.1451–1455,2018.\n[4] D.T.Anderson,G.JScott,M.Islam,B.Murray,andR.Marcum, “Fuzzychoquetintegrationofdeepconvolutionalneuralnetworksforremotesensing,”\ninComputationalIntelligenceforPatternRecognition,WitoldandShyi-Ming,Eds.,p.TBD.SpringerBerlinHeidelberg,2018.\n[5] Richard A Marcum, Curt H Davis, Grant J Scott, and Tyler W Nivin, “Rapid broad area search and detection of chinese surface-to-air missile sites\nusingdeepconvolutionalneuralnetworks,” JournalofAppliedRemoteSensing,vol.11,no.4,pp.11–31,2017.\n[6] Alan B Cannaday, Curt H Davis, and Grant J Scott, “Improved search and detection of surface-to-air missile sites using spatial fusion of component\nobjectdetectionsfromdeepneuralnetworks,” inIGARSS2019-2019IEEEInternationalGeoscienceandRemoteSensingSymposium.IEEE,2019,pp.\n9811–9814.\n[7] Grant J Scott, J Alex Hurt, Richard A Marcum, Derek T Anderson, and Curt H Davis, “Aggregating deep convolutional neural network scans of\nbroad-area high-resolution remote sensing imagery,” in IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium. IEEE,\n2018,pp.665–668.\n[8] JAlexHurt,DavidHuangal,CurtHDavis,andGrantJScott, “Acomparisonofdeeplearningvehiclegroupdetectioninsatelliteimagery,” in2019\nIEEEInternationalConferenceonBigData(BigData).IEEE,2019,pp.5419–5427.\n[9] JAlexHurt,DavidHuangal,JeffreyDale,TrevorMBajkowski,JamesMKeller,GrantJScott,andStantonRPrice, “Maneuverabilityhazarddetection\nandlocalizationinlow-altitudeuasimagery,” inArtificialIntelligenceandMachineLearningforMulti-DomainOperationsApplicationsII.SPIE,2020,\nvol.11413,pp.526–535.\n[10] G.J.Scottetal., “Differentialmorphologicalprofileneuralnetworkforobjectdetectioninoverheadimagery,” inIJCNN,2020,pp.1–7.\n[11] J.AlexHurt,TrevorBajkowski,andGrantJ.Scott, “Improvedclassificationofhighresolutionremotesensingimagerywithdifferentialmorphological\nprofileneuralnetwork,” inIGARSS,2021,pp.2823–2826.\n[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet classification with deep convolutional neural networks,” Communications of the\nACM,vol.60,no.6,pp.84–90,2017.\n[13] Tsung-YiLinetal., “Microsoftcoco:Commonobjectsincontext,” inEuropeanconferenceoncomputervision.Springer,2014,pp.740–755.\n[14] S.Renetal., “FasterR-CNN:Towardsreal-timeobjectdetectionwithregionproposalnetworks,” IEEETransactionsonPatternAnalysisandMachine\nIntelligence,vol.39,no.6,pp.1137–1149,2017.\n[15] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie, “Aconvnetforthe2020s,” inProceedingsofthe\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,2022,pp.11976–11986.\n[16] WeiLiuetal., “SSD:Singleshotmultiboxdetector,” inComputerVision–ECCV2016,Cham,2016,pp.21–37,SpringerInternationalPublishing.\n[17] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDolla´r,“Focallossfordenseobjectdetection,”inProceedingsoftheIEEEinternational\nconferenceoncomputervision,2017,pp.2980–2988.\n[18] ZhiTian,ChunhuaShen,HaoChen,andTongHe,“Fcos:Fullyconvolutionalone-stageobjectdetection,”inProceedingsoftheIEEE/CVFinternational\nconferenceoncomputervision,2019,pp.9627–9636.\n[19] ZhengGe,SongtaoLiu,FengWang,ZemingLi,andJianSun, “Yolox:Exceedingyoloseriesin2021,” arXivpreprintarXiv:2107.08430,2021.\n[20] KarenSimonyanandAndrewZisserman, “Verydeepconvolutionalnetworksforlarge-scaleimagerecognition,” arXivpreprintarXiv:1409.1556,2014.\n[21] SainingXie,RossGirshick,PiotrDolla´r,ZhuowenTu,andKaimingHe,“Aggregatedresidualtransformationsfordeepneuralnetworks,”inProceedings\noftheIEEEconferenceoncomputervisionandpatternrecognition,2017,pp.1492–1500.\n[22] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun, “Deepresiduallearningforimagerecognition,” arXivpreprintarXiv:1512.03385,2015.\n[23] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,\nGeorgHeigold,SylvainGelly,etal., “Animageisworth16x16words:Transformersforimagerecognitionatscale,” arXivpreprintarXiv:2010.11929,\n2020.\n[24] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko, “End-to-end object detection with\ntransformers,” inComputerVision–ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartI16.Springer,2020,\npp.213–229.\n[25] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,andJifengDai,“Deformabledetr:Deformabletransformersforend-to-endobjectdetection,”\narXivpreprintarXiv:2010.04159,2020.\n[26] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo, “Swintransformer:Hierarchicalvisiontransformer\nusingshiftedwindows,” inProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,2021,pp.10012–10022.\n[27] Jacob Shermeyer et. al, “Rareplanes: Synthetic data takes flight,” in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision,2021,pp.207–217.\n[28] Gui-SongXiaet.al, “Dota:Alarge-scaledatasetforobjectdetectioninaerialimages,” inCVPR,2018,pp.3974–3983.\n[29] Bo Wu, He Zheng, Zelong Xu, Zhiwei Wu, and Yindi Zhao, “Forest burned area detection using a novel spectral index based on multi-objective\noptimization,” Forests,vol.13,no.11,pp.1787,2022.\n[30] David P Roy, Haiyan Huang, Luigi Boschetti, Louis Giglio, Lin Yan, Hankui H Zhang, and Zhongbin Li, “Landsat-8 and sentinel-2 burned area\nmapping-acombinedsensormulti-temporalchangedetectionapproach,” RemoteSensingofEnvironment,vol.231,pp.111254,2019.\n[31] Qi Zhang, Linlin Ge, Ruiheng Zhang, Graciela Isabel Metternicht, Zheyuan Du, Jianming Kuang, and Min Xu, “Deep-learning-based burned area\nmappingusingthesynergyofsentinel-1&2data,” RemoteSensingofEnvironment,vol.264,pp.112575,2021.\n[32] Lisa Knopp, Marc Wieland, Michaela Ra¨ttich, and Sandro Martinis, “A deep learning approach for burned area segmentation with sentinel-2 data,”\nRemoteSensing,vol.12,no.15,pp.2422,2020.\n[33] JonathanLong,EvanShelhamer,andTrevorDarrell, “Fullyconvolutionalnetworksforsemanticsegmentation,” pp.3431–3440,2015.\n[34] OlafRonneberger,PhilippFischer,andThomasBrox,“U-net:Convolutionalnetworksforbiomedicalimagesegmentation,”inMedicalImageComputing\nandComputer-AssistedIntervention–MICCAI2015,NassirNavab,JoachimHornegger,WilliamM.Wells,andAlejandroF.Frangi,Eds.,Cham,2015,\npp.234–241,SpringerInternationalPublishing.\n[35] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang, “Unet++: A nested u-net architecture for medical image\nsegmentation,” inDeeplearninginmedicalimageanalysisandmultimodallearningforclinicaldecisionsupport,pp.3–11.Springer,2018.\n13\n[36] Liang-ChiehChen,GeorgePapandreou,FlorianSchroff,andHartwigAdam, “Rethinkingatrousconvolutionforsemanticimagesegmentation,” arXiv\npreprintarXiv:1706.05587,2017.\n[37] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam, “Encoder-decoder with atrous separable convolution for\nsemanticimagesegmentation,” inProceedingsoftheEuropeanconferenceoncomputervision(ECCV),2018,pp.801–818.\n[38] Pablo Pozzobon de Bem, Osmar Ab´ılio de Carvalho Junior, Renato Fontes Guimara˜es, and Roberto Arnaldo Trancoso Gomes, “Change Detection of\nDeforestationintheBrazilianAmazonUsingLandsatDataandConvolutionalNeuralNetworks,” RemoteSensing,vol.12,no.6,pp.901,Jan.2020.\n[39] IlyaLoshchilovandFrankHutter, “Decoupledweightdecayregularization,” arXivpreprintarXiv:1711.05101,2017.\n[40] KaiChen,JiaqiWang,JiangmiaoPang,YuhangCao,YuXiong,XiaoxiaoLi,ShuyangSun,WansenFeng,ZiweiLiu,JiaruiXu,ZhengZhang,Dazhi\nCheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChenChangeLoy,andDahuaLin, “MMDetection:Openmmlabdetectiontoolboxandbenchmark,” arXivpreprintarXiv:1906.07155,2019.\n[41] WeleGedaraChamindaBandaraandVishalM.Patel, “ATransformer-BasedSiameseNetworkforChangeDetection,” inIGARSS2022-2022IEEE\nInternationalGeoscienceandRemoteSensingSymposium,July2022,pp.207–210.\n[42] Daliana Lobo Torres, Javier Noa Turnes, Pedro Juan Soto Vega, Raul Queiroz Feitosa, Daniel E. Silva, Jose Marcato Junior, and Claudio Almeida,\n“Deforestation Detection with Fully Convolutional Networks in the Amazon Forest from Landsat-8 and Sentinel-2 Images,” Remote Sensing, vol. 13,\nno.24,pp.5084,Jan.2021.",
    "pdf_filename": "Scaling_Deep_Learning_Research_with_Kubernetes_on_the_NRP_Nautilus_HyperCluster.pdf"
}