{
    "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
    "abstract": "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer’s superior handling of Indic languages, GPT-4o’s advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.",
    "body": "EVALUATING TOKENIZER PERFORMANCE OF LARGE LANGUAGE\nMODELS ACROSS OFFICIAL INDIAN LANGUAGES\nA PREPRINT\nSagar Tamang∗\nDepartment of IT\nThe Assam Kaziranga University\nJorhat, India\ncs22bcagn033@kazirangauniversity.in\nDr. Dibya Jyoti Bora\nDepartment of IT\nThe Assam Kaziranga University\nJorhat, India\ndibyajyotibora@kazirangauniversity.in\nNovember 20, 2024\nABSTRACT\nLarge Language Models (LLMs) based on transformer architectures have revolutionized a variety of\ndomains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In\nmultilingual models, particularly those tailored for Indic languages, effective tokenization is crucial\nfor optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by\n12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their\ntokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including\nseveral Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA\ntokenizer’s superior handling of Indic languages, GPT-4o’s advancement over its predecessor GPT-4\nin processing Indian languages, and the limited performance of Project Indus in certain languages.\nThis study underscores the critical importance of developing targeted tokenization strategies for\nmultilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer\ndesign to enhance linguistic coverage and model efficiency.\nKeywords tokenizer · LLM · tokens · GPT · SUTRA · indic languages.\n1\nIntroduction\n1.1\nBackground\nIn an ever-evolving landscape of Artificial Intelligence (AI), transformers-based generative Large Language Models\n(LLMs) are transforming an increasing number of fields with an ever-increasing number of applications in finance,\nmedicine, education, and many more [1, 2]. Tokenization is an important step for LLMs, especially in pre-processing\nand fine-tuning stages [3].\nMost of the LLMs use either of two types of tokenization algorithms, namely WordPiece and Byte Pair Encoding (BPE).\nFor example, OpenaAi’s GPT-4o model and META’s Llama 3, both employ a modified BPE tokenizer [4]. WordPiece\nwas developed for models like BERT employing a greedy approach. It starts with the longest substring that matches a\ntoken in its vocabulary, allowing it to handle out-of-vocabulary words effectively by breaking them down into known\nsubword units [5, 6]. BPE works by iteratively merging the most frequently occurring pairs of characters or subwords\nin a corpus to create a complete vocabulary [7, 8].\nAgnostic of the tokenization algorithms used, many techniques have been developed to compare tokenizers of LLMs.\nSubword fertility is one such technique which measures the average number of tokens used per word [9]. Normalized\nSequence Length (NSL) is another such metric used to evaluate the efficiency of tokenizers [12].\n∗Correspondance can be addressed to cs22bcagn033@kazirangauniversity.in\narXiv:2411.12240v1  [cs.CL]  19 Nov 2024\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\n      \nઉદાહરણ ટેક્સ્ટ્સ\n      \nउदाहरण पाठ\n      \nExample Texts\nLLM’s Tokenizer\nLLM’s Tokenizer\nTokenized Output\nExample Texts\n(for each language)\nLeaderboard\nEvaluation\nLLM’s Tokenizer\nFigure 1: Evaluation pipeline: (1) We collect example texts for all 22 languages. (2) We send the example texts to the\nLLMs’ tokenizer. (3) Evaluate the tokenized outputs. (4) We construct leaderboards using our evaluation.\nAn effective tokenizer is essential for enabling a model to learn a language efficiently. Improved tokenizer performance\noffers several benefits, including faster token generation and reduced computational resource requirements, enhancing\nboth efficiency and cost-effectiveness [13].\n1.2\nTokenizers in Multilingual & Indic Models\nMany multilingual models, including OpenAI’s ChatGPT [14], Google’s Gemini [16], Meta’s Llama [17], and TWO\nAI’s SUTRA [15], are designed to deliver coherent performance across a wide range of global languages, including\nIndian languages. Achieving this requires tokenizers in these LLMs to handle diverse languages efficiently. This\nstudy evaluates the performance of tokenizers in these multilingual models and Indic-specific models across all official\nlanguages of India.\nThe rest of the paper is organized in the following way: Section 2 highlights related works, Section 3 describes the\nmethodology of this study, Section 4 is where the results are showcased and Section 5 is for discussions with future\noutlook.\n2\nLiterature Review\n2.1\nLarge Language Models and Tokenization\nEver since the release of transformer-based architecture, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in Natual Language Processing (NLP) tasks and beyond [19]. However, some recent studies [18] have also\nproposed promising non-transformer LLMs. LLMs today exhibit a plethora of applications not limited to NLP tasks,\nbut can also perform general tasks performing multi-step reasoning. Thus, LLMs are becoming the basic building block\nfor developing general-purpose AI agents or Artificial General Intelligence (AGI) [18].\nTokenization refers to the process of converting a sequence of texts into smaller parts, known as tokens. In order to\nincrease the coverage of dictionaries and also to deal with words that were unseen in training data, LLMs use sub-words\nbased tokenizers like BPE or Wordpiece [4, 8, 7, 18].\n2.2\nIndic-Specific Language Models\nS. Bhat et al. evaluated the capabilities of generative models like ChatGPT, mT0, and BLOOMZ in generating Indic\nlanguages, and the findings revealed that these models have limited capabilities in generating text in Indic languages\nin a zero-shot setting. While they performed better on manual quality evaluations for English, their performance in\nIndic languages highlighted the need for further development and training specifically tailored to these languages [22].\nSeveral studies have introduced benchmarks to evaluate the performance of LLMs across Indic languages, and their\nresults emphasize the necessity for more focused research and development efforts in LLMs to handle the linguistic\ndiversity of India [23, 24, 25].\nOne notable advancement is SUTRA (Scalable Multilingual Language Model Architecture), introduced by A. Bendale\net al. SUTRA supports over 50 languages, including Indic ones, by decoupling conceptual understanding from language-\n2\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\nspecific processing for scalable multilingual learning. It uses a Mixture of Experts framework, enhancing computational\nefficiency and responsiveness. Evaluations show that SUTRA outperforms models like GPT-3.5 and Llama2 by 20-30%\non MMLU benchmarks. The model provides hallucination-free, factual, and up-to-date responses, with the potential to\ndemocratize AI access, especially in non-English regions [15, 29].\n2.3\nComparative Evaluation of Tokenizers\nTo compare the tokenizers, several metrics and methodologies have been introduced. Subword fertility is one such\nmetric that measures the average number of tokens generated per word [12]. An ideal tokenizer in this case would have\na fertility of 1.0, indicating that most words are represented as single tokens, while a higher fertility score signals that\nmany words are split into multiple tokens [10]. Another metric that is employed is the proportion of continued words\n[10] which indicates the percentage of words that are split into multiple tokens. 0 is an ideal value for the proportion\nof continued words. A lower proportion indicates better performance, as it means fewer words are fragmented [10].\nAnother metric that is used to evaluate and compare tokenizers is the Normalized Sequence Length (NSL). NSL\nmeasures the average length of tokenized sequences produced by a tokenizer relative to a baseline tokenizer [12].\nExisting tokenizer studies are performed at a smaller scale than what is typical for modern LLMs or focus on multilingual\ntasks [12]. Some studies have tried to evaluate the performance of tokenizers but only in some select languages like\nTurkish [21], Arabic [26], or focusing on a few Indian languages [3, 20].\n2.4\nGaps in Existing Research\nThus, we can observe that there exists a research gap to evaluate the tokenizers in other Indian languages as well. Hence,\nin this study, we are conducting an overall evaluation of the tokenizers of LLMs in all 22 official languages of India as\nrecognized by the eighth schedule of the Indian constitution [27].\n3\nMethodology\nOur evaluation step is summarized in Figure 1.\n3.1\nExample Texts\n      \n\"জীৱনৰ পৰিসৰে মোহিত হোৱাটো বাঞ্ছনীয়।\"\nTranslation: \"It is desirable to be captivated by the expanse of\nlife.\"\nSource: \"Miri Jiyori\" by Lakshminath Bezbaroa\nExample Text in Assamese\nFigure 2: Assamese text used for evaluating tokenizer performance.\nWe compiled example texts in all 22 languages to evaluate the performance of the tokenizers. Each language’s text\nwas selected in its primary writing script to ensure an authentic assessment of the tokenizer’s capability to process\nnative scripts accurately. The curated example texts represent diverse linguistic structures and scripts, enabling a\ncomprehensive analysis of tokenization performance.\nAll the example texts used during our study can be found in the Appendix A.2 or Figure 26 and 27. One such example\ntext can be found in Figure 2.\n3.2\nModels\nWe chose 12 models including proprietary multilingual models, as well as Open-weights multilingual and Indic language\nmodels for our study. The list of models can be found in Table 1.\nWhile we acknowledge that some models are not specifically designed for all Indian languages—such as MahaMarathi,\nwhich is tailored for Marathi, and MBZUAI’s Nanda, which is optimized for Hindi and English—we have included\nthem in this study for the sake of comprehensive evaluation.\n3\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\nModels\nLanguages\nTested\nAvailability\nGPT-4o\nAll\nProprietary\nGPT-4\nAll\nProprietary\nTWO/sutra-mlt256-v2\nAll\nProprietary\nmicrosoft/Phi-3.5-MoE-instruct\nAll\nOpen-weights\nmeta-llama/Llama-3.1-405B-FP8\nAll\nOpen-weights\nai4bharat/Airavata\nAll\nOpen-weights\nCohereForAI/aya-23-35B\nAll\nOpen-weights\nMBZUAI/Llama-3-Nanda-10B-Chat\nAll\nOpen-weights\nnickmalhotra/ProjectIndus\nAll\nOpen-weights\nsarvamai/OpenHathi-7B-Hi-v0.1-Base\nAll\nOpen-weights\nTelugu-LLM-Labs/Indic-gemma-7b-finetuned-sft-Navarasa-2.0\nAll\nOpen-weights\nmarathi-llm/MahaMarathi-7B-v24.01-Base\nAll\nOpen-weights\nTable 1: List of tokenizers tested. “All\" refers to all 22 official languages of India as recognized by the Eighth Schedule\nof the Indian Constitution. The official languages include Assamese, Bengali, Bodo, Dogri, Gujarati, Hindi, Kannada,\nKashmiri, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Odia, Punjabi, Sanskrit, Santali, Sindhi, Tamil,\nTelugu, Urdu.\n3.3\nEvaluation Metric\nFor our work is extending the previous works by [3], we have chosen to go with the NSL metric. Formally the NSL is\ndefined by [12] cλβ as the ratio between the length of an encoded sequence from a tokenizer Tλ and a tokenizer Tβ. For\nN examples taken from a dataset D:\ncλβ =\nPN\ni=1 length(Tλ(Di))\nPN\ni=1 length(Tβ(Di))\n4\nResults\nAverage NSL Values Table 2 presents the average NSL values for all tokenizers across the 22 languages, calculated\nusing the examples provided in Appendix A.2. The scores are reported to four decimal places for precision. The bold\ntext indicates the lowest value or the best performance among all the other tokenizers. It can be observed that SUTRA\ntokenizer manages to excel among all the other tokenizers including ChatGPT’s 4-o or other Indic models.\nFigure 3 illustrates the number of languages in which each tokenizer achieved the highest NSL score. For instance,\nTWO AI’s SUTRA outperformed all other tokenizers in 14 languages, while MBZUAI’s Nanda excelled in 6, and\nOpenAI’s GPT-4o in 5. Other notable performers include indic models like Tech Mahindra’s Project Indus with 4,\nSarvam AI’s OpenHathi and MahaMarathi with 2 each, and Indic Gemma, Microsoft Phi, and Airavata with 1 each.\nNumber of Tokens Appendix A.1 provides individual row bar charts for each language, offering a detailed breakdown\nof the number of tokens generated in each language. Lower token counts indicate better outcomes.\n5\nDiscussion\nIn this study, we evaluated the tokenizers from 12 LLMs in all 22 official languages of India and we found that the\nSUTRA tokenizer performed the best among all others, outperforming the 2nd best tokenizer by a large margin. This\nshowcases the multilingual strength of the SUTRA tokenizer to handle the Indic languages.\nMicrosoft’s Phi-3.5-MoE-instruct and Google’s Indic Gemma\nThough both the models were developed for Indic\nlanguages, they did not perform up to the level securing best performances only in one language out of 22 languages.\nObservation between GPT-4 and GPT-4o\nAnother interesting observation is that GPT-4, the predecessor of GPT-4o,\ndid not manage to secure the best tokenizer value in any of the 22 languages, a stark contrast to GPT-4o. Perhaps,\nthis highlights that an important difference between the two models is that the newer GPT-4o is well adept at Indian\nlanguages, increasing the multi-lingual capability.\n4\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma\nMahaMarathi\nMicrosoft Phi\nAiravata\nAya\n0\n2\n4\n6\n8\n10\n12\n14\n5\n14\n6\n4\n2\n1\n2\n1\n1\n0\n0\nFigure 3: Number of Best Performances Achieved by Each Tokenizer Across 22 Languages.\nLanguages\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma\nMahaMarathi\nMicrosoft Phi\nAiravata\nAya\nAssamese\n0.5429\n1.4\n0.4571\n1.4\n1.4\n2.7714\n1.5714\n0.8286\n1.3143\n1.5428\n1.5714\n1.5142\nBengali\n0.25\n1.2307\n0.2115\n1.25\n1.25\n2.8076\n1.3461\n0.5769\n1.0961\n1.3269\n1.3461\n1.2307\nBodo\n0.5675\n1.0540\n0.5405\n0.5945\n0.4594\n0.4864\n0.5405\n0.5675\n0.5405\n1.2432\n0.5405\n0.7567\nDogri\n0.5\n1.0313\n0.4688\n0.5938\n0.3750\n0.4688\n0.4063\n0.4688\n0.4063\n1.0312\n0.4062\n0.7812\nGujarati\n0.47545\n1.6875\n0.4688\n1.7188\n1.7188\n2.75\n2.6875\n0.7188\n1.0938\n2.6562\n2.6875\n1.7187\nHindi\n0.4091\n1.0\n0.4545\n0.5909\n0.3636\n0.3182\n0.4545\n0.5455\n0.3636\n1.0454\n0.4545\n1.0454\nKannada\n0.44\n1.76\n0.44\n1.8\n1.8\n2.84\n2.52\n0.56\n1.12\n2.48\n2.52\n1.72\nKashmiri\n0.6047\n1.093\n0.5814\n0.8837\n0.8837\n1.8605\n1.1628\n0.5814\n1.186\n1.1395\n1.1627\n0.9069\nKonkani\n0.4643\n1.1429\n0.5357\n0.6429\n0.5\n0.4643\n0.6071\n0.5357\n1.1071\n0.6071\n0.6071\n0.8214\nMaithili\n0.4211\n1.0\n0.6316\n0.6316\n0.3684\n0.3684\n0.5789\n0.5789\n1.1578\n0.5789\n0.5789\n0.7368\nMalayalam\n0.5\n1.75\n0.5\n1.8333\n1.8333\n3.0\n1.3333\n0.6667\n1.25\n1.3333\n1.3333\n1.8333\nManipuri\n0.6471\n1.2941\n0.5882\n1.3529\n1.3529\n2.8824\n1.5882\n0.7647\n1.5882\n1.5294\n1.5882\n1.2941\nMarathi\n0.4706\n0.9412\n0.5294\n0.6471\n0.3529\n0.3529\n0.4706\n0.5882\n1.0582\n0.4706\n0.4705\n0.8235\nNepali\n0.4091\n0.9091\n0.3182\n0.6364\n0.3182\n0.3636\n0.3182\n0.4545\n0.4090\n1.1363\n0.3182\n0.8181\nOdia\n1.0\n2.625\n0.625\n2.625\n2.625\n2.875\n2.875\n1.0625\n2.875\n2.8125\n2.875\n2.1875\nPunjabi\n0.6538\n1.6923\n0.4615\n1.7308\n1.7308\n2.7692\n2.3077\n0.7692\n2.3077\n2.2692\n2.3076\n1.7307\nSanskrit\n0.5\n1.0833\n0.6667\n0.5833\n0.5\n0.5\n0.5\n0.6667\n1.08333\n0.5\n0.5\n0.75\nSantali\n2.7647\n2.647\n0.4705\n2.7058\n2.7058\n2.8823\n2.8823\n1.0588\n2.9411\n2.8235\n2.8823\n2.7058\nSindhi\n0.4117\n0.9117\n0.5\n0.6176\n0.6176\n1.8529\n1.0882\n0.5588\n1.1176\n1.0588\n1.0882\n0.5882\nTamil\n0.4411\n1.3823\n0.3823\n1.4117\n1.4117\n2.7647\n1.2352\n0.5294\n1.0882\n1.2058\n1.2352\n1.2058\nTelugu\n0.375\n1.75\n0.2916\n1.7916\n1.7916\n2.8333\n2.6666\n0.625\n1.125\n2.625\n2.6666\n1.7083\nUrdu\n0.3928\n0.7857\n0.3571\n0.5357\n1.8928\n1.8928\n1.1071\n0.4285\n1.1071\n1.0714\n1.1071\n0.5357\nTable 2: Average NSL Values Across Models for 22 Languages (lower is better). The bold values indicate the best-\nperforming tokenizer for each language.\nObservation of Tech Mahindra’s Project Indus\nBoth SUTRA and GPT-4o tokenizers manage to get a consistently\nlow average NSL value (below 1.0) for all the languages but the Project Indus’ tokenizer seems to be getting the same\nfor only a few languages like (1) Bodo, (2) Dogri, (3) Hindi, (4) Konkani, (5) Maithili, (6) Marathi, (7) Nepali, and\n(8) Sanskrit. This is probably because all these 8 languages follow the same Devanagari script of writing, which the\nmodel’s tokenizer was probably trained on. But for the rest of the languages, the tokenizer seems to be struggling,\ngetting an average NSL score of above 1 (the higher the worse).\nNumber of Tokens\nAccording to the results and appendix A.1, tokenizers like SUTRA generate fewer tokens\nacross the 22 Indian languages compared to others. Lower token counts suggest that the tokenizer is more efficient in\n5\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\nprocessing the input text without excessive fragmentation. This is a crucial factor in improving the overall performance\nand computational efficiency of LLMs, particularly for large-scale applications.\nSignificance of tokenization in LLMs\nTokenization plays a vital role in LLMs by breaking down text into smaller\nunits (tokens) that the model can process efficiently. A well-designed tokenizer enables the model to handle complex\nlanguage structures, out-of-vocabulary words, and multi-language contexts effectively. It enhances the model’s ability to\nunderstand and generate language with greater accuracy. Additionally, a good tokenizer leads to reduced computational\ncosts and resource requirements by optimizing token generation. This results in faster training times, lower resource\nconsumption, and overall improved performance, allowing the model to process diverse languages more effectively.\nReal-World Applications and Future Directions\nThe insights from this study have important implications for the\ndevelopment of multilingual models across Indian languages. Future research could focus on enhancing tokenizers to\nbetter handle languages with complex scripts or languages with a high degree of dialectical variation, improving model\nperformance for both high-resource and low-resource languages.\n6\nAcknowledgement\nWe would like to thank the Assam Kaziranga University for assisting us in conducting this research.\nReferences\n[1] F. Chiarello, V. Giordano, I. Spada, S. Barandoni, and G. Fantoni, \"Future applications of generative large\nlanguage models: A data-driven case study on ChatGPT,\" Technovation, vol. 133, p. 103002, 2024. [Online].\nAvailable: https://www.sciencedirect.com/science/article/pii/S016649722400052X. [Accessed:\nNov. 12, 2024].\n[2] Y. Nie, Y. Kong, X. Dong, J. M. Mulvey, H. V. Poor, Q. Wen, and S. Zohren, \"A Survey of Large Language\nModels for Financial Applications: Progress, Prospects and Challenges,\" arXiv preprint arXiv:2406.11903, 2024.\n[Online]. Available: https://arxiv.org/abs/2406.11903. [Accessed: Nov. 12, 2024].\n[3] S. Tamang and D. J. Bora, \"Performance Evaluation of Tokenizers in Large Language Models for the Assamese\nLanguage,\" arXiv preprint arXiv:2410.03718, 2024. [Online]. Available: https://arxiv.org/abs/2410.\n03718\n[4] J. Yang, Z. Wang, Y. Lin, and Z. Zhao, \"Large Language Model Tokenizer Bias: A Case Study and Solution on\nGPT-4o,\" arXiv preprint arXiv:2406.11214, 2024. [Online]. Available: https://arxiv.org/abs/2406.11214.\n[Accessed: Nov. 13, 2024].\n[5] X. Song, A. Salcianu, Y. Song, D. Dopson, and D. Zhou, “Fast WordPiece Tokenization,” 2021. [Online]. Available:\nhttps://arxiv.org/abs/2012.15524.\n[6] O. Ogundepo, X. Zhang, and J. Lin, “Better Than Whitespace: Information Retrieval for Languages without\nCustom Tokenizers,” 2022. [Online]. Available: https://arxiv.org/abs/2210.05481.\n[7] L. Kozma and J. Voderholzer, “Theoretical Analysis of Byte-Pair Encoding,” 2024. [Online]. Available: https:\n//arxiv.org/abs/2411.08671.\n[8] V. Zouhar, C. Meister, J. L. Gastaldi, L. Du, T. Vieira, M. Sachan, and R. Cotterell, “A Formal Perspective on\nByte-Pair Encoding,” 2024. [Online]. Available: https://arxiv.org/abs/2306.16837.\n[9] H. Singh, N. Gupta, S. Bharadwaj, D. Tewari, and P. Talukdar, \"IndicGenBench: A Multilingual Benchmark to\nEvaluate Generation Capabilities of LLMs on Indic Languages,\" arXiv preprint arXiv:2404.16816, 2024. [Online].\nAvailable: https://arxiv.org/abs/2404.16816.\n[10] Occiglot, \"EU Tokenizer Performance,\" [Online]. Available: https://occiglot.eu/posts/eu_tokenizer_\nperfomance/. Accessed: Nov. 17, 2024.\n[11] Occiglot, \"Tokenizer performance on EU languages,\" Occiglot Blog, Sep. 26, 2023. [Online]. Available: https:\n//occiglot.eu/posts/eu_tokenizer_perfomance/.\n[12] G. Dagan, G. Synnaeve, and B. Rozière, \"Getting the most out of your tokenizer for pre-training and domain\nadaptation,\" arXiv preprint arXiv:2402.01035, 2024. [Online]. Available: https://arxiv.org/abs/2402.\n01035.\n6\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\n[13] Microsoft, \"Exploring the New Frontier of AI: OpenAI’s GPT-4-O for Indic Languages,\" Azure AI Blog, Oct. 30,\n2024. [Online]. Available: https://techcommunity.microsoft.com/blog/azure-ai-services-blog/\nexploring-the-new-frontier-of-ai-openais-gpt-4-o-for-indic-languages/4142383.\n[14] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, et al., \"GPT-4 Technical Report,\" arXiv preprint\narXiv:2303.08774, 2024. [Online]. Available: https://arxiv.org/abs/2303.08774.\n[15] A. Bendale, M. Sapienza, S. Ripplinger, S. Gibbs, J. Lee, and P. Mistry, \"SUTRA: Scalable Multilingual Language\nModel Architecture,\" arXiv preprint arXiv:2405.06694, 2024. [Online]. Available: https://arxiv.org/abs/\n2405.06694.\n[16] Gemini Team et al., Gemini: A Family of Highly Capable Multimodal Models, 2024. Available at: https:\n//arxiv.org/abs/2401.12345.\n[17] Dubey, Abhimanyu, et al. The Llama 3 Herd of Models. arXiv preprint, 2024. Available at: https://arxiv.\norg/abs/2407.21783.\n[18] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and\nJianfeng Gao, \"Large Language Models: A Survey,\" arXiv preprint arXiv:2402.06196, 2024. [Online]. Available:\nhttps://arxiv.org/abs/2402.06196\n[19] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed\nAkhtar, Nick Barnes, and Ajmal Mian, \"A Comprehensive Overview of Large Language Models,\" arXiv preprint\narXiv:2307.06435, 2024. [Online]. Available: https://arxiv.org/abs/2307.06435\n[20] AdaSci,\n\"Multilingual\nTokenization\nEfficiency\nin\nLarge\nLanguage\nModels:\nA\nStudy\non\nIndian\nLanguages,\"\n[Online].\nAvailable:\nhttps://adasci.org/\nmultilingual-tokenization-efficiency-in-large-language-models-a-study-on-indian-languages/\n[21] Cagri Toraman, Eyup Halit Yilmaz, Furkan ¸Sahinüç, and Oguzhan Ozcelik, \"Impact of Tokenization on Language\nModels: An Analysis for Turkish,\" ACM Transactions on Asian and Low-Resource Language Information\nProcessing (TALLIP), vol. 22, no. 4, article 116, pp. 1–21, Mar. 2023. [Online]. Available: https://doi.org/\n10.1145/3578707\n[22] Bhat, Savita, Vasudeva Varma, and Niranjan Pedanekar. \"Generative Models For Indic Languages: Evaluating\nContent Generation Capabilities.\" In *Proceedings of the 14th International Conference on Recent Advances\nin Natural Language Processing*, edited by Ruslan Mitkov and Galia Angelova, 187–195. Varna, Bulgaria:\nINCOMA Ltd., September 2023. https://aclanthology.org/2023.ranlp-1.21.\n[23] Singh, Harman, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, and Partha Talukdar. \"IndicGenBench: A\nMultilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages.\" In *Proceedings of the\n62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, edited by Lun-\nWei Ku, Andre Martins, and Vivek Srikumar, 11047–11073. Bangkok, Thailand: Association for Computational\nLinguistics, August 2024. https://aclanthology.org/2024.acl-long.595, DOI: 10.18653/v1/2024.acl-\nlong.595.\n[24] Singh, Abhishek Kumar, Rudra Murthy, Vishwajeet Kumar, Jaydeep Sen, and Ganesh Ramakrishnan. \"Indic\nQA Benchmark: A Multilingual Benchmark to Evaluate Question Answering Capability of LLMs for Indic\nLanguages.\" arXiv preprint, 2024. https://arxiv.org/abs/2407.13522.\n[25] Kumar, Aman, Himani Shrotriya, Prachi Sahu, Amogh Mishra, Raj Dabre, Ratish Puduppully, Anoop Kunchukut-\ntan, Mitesh M. Khapra, and Pratyush Kumar. \"IndicNLG Benchmark: Multilingual Datasets for Diverse NLG\nTasks in Indic Languages.\" In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing*, edited by Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, 5363–5394. Abu Dhabi, United Arab\nEmirates: Association for Computational Linguistics, December 2022. https://aclanthology.org/2022.\nemnlp-main.360, DOI: 10.18653/v1/2022.emnlp-main.360.\n[26] Alyafeai, Zaid, Maged S. Al-shaibani, Mustafa Ghaleb, and Irfan Ahmad. \"Evaluating Various Tokenizers for\nArabic Text Classification.\" arXiv preprint, 2021. https://arxiv.org/abs/2106.07540.\n[27] Government of India, \"Eighth Schedule,\" [Online]. Available: https://www.mea.gov.in/Images/pdf1/S8.\npdf. Accessed: Dec. 5, 2023.\n[28] I. Watts, V. Gumma, A. Yadavalli, V. Seshadri, M. Swaminathan, and S. Sitaram, “PARIKSHA: A Large-Scale\nInvestigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data,” arXiv preprint\narXiv:2406.15053, 2024. [Online]. Available: https://arxiv.org/abs/2406.15053\n[29] DeepLearning.ai,\nStartup\nTWO\nAI\nLaunches\nSUTRA:\nA\nMultilingual\nModel\nfor\nSouth\nAsian\nMarkets,\nhttps://www.deeplearning.ai/the-batch/\nstartup-two-ai-launches-sutra-a-multilingual-model-for-south-asian-markets/, Accessed:\n2024-11-18.\n7\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\nA\nAppendix\nA.1\nBar Charts of Token Counts for Each Language\n0\n20\n40\n60\n80\n100\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n19\n49\n16\n49\n49\n97\n55\n29\n46\n54\n55\n53\nAssamese\nFigure 4: Number of tokens required for a single example text in Assamese. Lower values are better.\n0\n50\n100\n150\n200\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n13\n64\n11\n65\n65\n146\n70\n30\n57\n69\n70\n64\nBengali\nFigure 5: Number of tokens required for a single example text in Bengali. Lower values are better.\n0\n10\n20\n30\n40\n50\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n21\n39\n20\n22\n17\n18\n20\n21\n20\n46\n20\n28\nBodo\nFigure 6: Number of tokens required for a single example text in Bodo. Lower values are better.\n0\n5\n10\n15\n20\n25\n30\n35\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n16\n33\n15\n19\n12\n15\n13\n15\n13\n33\n13\n25\nDogri\nFigure 7: Number of tokens required for a single example text in Dogri. Lower values are better.\n8\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\n0\n20\n40\n60\n80\n100\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n15\n54\n15\n55\n55\n88\n86\n23\n35\n85\n86\n55\nGujarati\nFigure 8: Number of tokens required for a single example text in Gujarati. Lower values are better.\n0\n5\n10\n15\n20\n25\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n9\n22\n10\n13\n8\n7\n10\n12\n8\n23\n10\n23\nHindi\nFigure 9: Number of tokens required for a single example text in Hindi. Lower values are better.\n0\n20\n40\n60\n80\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n11\n44\n11\n45\n45\n71\n63\n14\n28\n62\n63\n43\nKannada\nFigure 10: Number of tokens required for a single example text in Kannada. Lower values are better.\n0\n20\n40\n60\n80\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n26\n47\n25\n38\n38\n80\n50\n25\n51\n49\n50\n39\nKashmiri\nFigure 11: Number of tokens required for a single example text in Kashmiri. Lower values are better.\nA.2\nExample Texts Used for Tokenizer Evaluation\n9\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\n0\n5\n10\n15\n20\n25\n30\n35\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n13\n32\n15\n18\n14\n13\n17\n15\n11\n31\n17\n23\nKonkani\nFigure 12: Number of tokens required for a single example text in Konkani. Lower values are better.\n0\n5\n10\n15\n20\n25\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n8\n19\n12\n12\n7\n7\n11\n11\n12\n22\n11\n14\nMaithili\nFigure 13: Number of tokens required for a single example text in Maithili. Lower values are better.\n0\n10\n20\n30\n40\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n6\n21\n6\n22\n22\n36\n16\n8\n15\n15\n16\n22\nMalayalam\nFigure 14: Number of tokens required for a single example text in Malayalam. Lower values are better.\n0\n10\n20\n30\n40\n50\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n11\n22\n10\n23\n23\n49\n27\n13\n20\n26\n27\n22\nManipuri\nFigure 15: Number of tokens required for a single example text in Manipuri. Lower values are better.\n10\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\n0\n5\n10\n15\n20\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n8\n16\n9\n11\n6\n6\n8\n10\n7\n18\n8\n14\nMarathi\nFigure 16: Number of tokens required for a single example text in Marathi. Lower values are better.\n0\n5\n10\n15\n20\n25\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n9\n20\n7\n13\n7\n8\n7\n10\n9\n25\n7\n18\nNepali\nFigure 17: Number of tokens required for a single example text in Nepali. Lower values are better.\n0\n10\n20\n30\n40\n50\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n16\n42\n10\n42\n42\n46\n46\n17\n29\n45\n46\n35\nOdia\nFigure 18: Number of tokens required for a single example text in Odia. Lower values are better.\n0\n20\n40\n60\n80\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n17\n44\n12\n45\n45\n72\n60\n20\n31\n59\n60\n45\nPunjabi\nFigure 19: Number of tokens required for a single example text in Punjabi. Lower values are better.\n11\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\n0\n2\n4\n6\n8\n10\n12\n14\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n6\n13\n8\n7\n6\n6\n6\n8\n7\n13\n6\n9\nSanskrit\nFigure 20: Number of tokens required for a single example text in Sanskrit. Lower values are better.\n0\n10\n20\n30\n40\n50\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n47\n45\n8\n46\n46\n49\n49\n18\n50\n48\n49\n46\nSantali\nFigure 21: Number of tokens required for a single example text in Santali. Lower values are better.\n0\n10\n20\n30\n40\n50\n60\n70\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n14\n31\n17\n21\n21\n63\n37\n10\n38\n36\n37\n20\nSindhi\nFigure 22: Number of tokens required for a single example text in Sindhi. Lower values are better.\n0\n20\n40\n60\n80\n100\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n15\n47\n13\n48\n48\n94\n42\n18\n37\n41\n42\n41\nTamil\nFigure 23: Number of tokens required for a single example text in Tamil. Lower values are better.\n12\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\n0\n10\n20\n30\n40\n50\n60\n70\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n9\n42\n7\n43\n43\n68\n64\n15\n27\n63\n64\n41\nTelugu\nFigure 24: Number of tokens required for a single example text in Telugu. Lower values are better.\n0\n10\n20\n30\n40\n50\n60\nGPT-4o\nGPT-4\nSUTRA\nLlama 3.1\nNanda\nProject Indus\nOpenHathi\nIndic Gemma 7B\nMahaMarathi\nPhi-3.5-MoE\nAiravata\nAya\n11\n22\n10\n15\n15\n53\n31\n12\n31\n30\n31\n15\nUrdu\nFigure 25: Number of tokens required for a single example text in Urdu. Lower values are better.\n      \n\"জীৱনৰ পৰিসৰে মোহিত হোৱাটো বাঞ্ছনীয়।\"\nTranslation: \"It is desirable to be captivated by the expanse of\nlife.\"\nSource: \"Miri Jiyori\" by Lakshminath Bezbaroa\nExample Text in Assamese\n      \n\"পৃথিবীর বিভিন্ন প্রান্তে মানুষের নানা সংস্কৃতি রয়েছে\"\nTranslation: \"There are various cultures of people across\ndifferent parts of the world.\"\nSource: General expression\nExample Text in Bengali\n      \n\"बोरो जाति हिन्सा नाकुं आनि एकोफ फयलाइ\"\nTranslation: \"The Bodo people believe in peace and unity.\"\nSource: Bodo Cultural Heritage by Ganesh Chandra Boro\n(2008)\nExample Text in Bodo\n      \n\"डोगरी जुबां साडा मान साडा अभिमान\"\nTranslation: \"Dogri language, our pride and honor.\"\nSource: Dogri Literature Society, Dogri Language and\nLiterature\nExample Text in Dogri\n      \n \"જનની જનમભૂમિ સ્વર્ગથી પણ મહાન છે\"\nTranslation: \"Mother and motherland are greater than heaven.\"\nSource: Subramania Bharati, Tamil Nationalism and Poetry\n(20th century)\nExample Text in Gujarati\n      \n\"मधुर मधुर मेरे दीपक जल\"\nTranslation: \"Burn gently, my lamp.\"\nSource: Mahadevi Verma, Madhur Madhur Mere Deepak Jal\n(1930)\nExample Text in Hindi\n      \n\"ಹರಿದ ಹವೆಯಂತೆ ನಿನ್ನ ಪ್ರೀತಿ\"\nTranslation: \"Your love, like the passing wind.\"\nSource: Kuvempu, Sri Ramayana Darshanam (1949)\nExample Text in Kannada\n      \n\" مشإ یچھٖ زمنِ ہتِ َہون زروٕ مہاراز یچھٖ ۍب\"\nTranslation: \"You are my king, you live in my evening and\ndawn.\"\nSource: Kashmirian Sufi Poetry, Kashmiri Shayari\nExample Text in Kashmiri\n      \n\"जेवपाच्या ताटात एक चटणी खासच\"\nTranslation: \"A special condiment on the dining plate.\"\nSource: Konkani Literature Society, Folk Songs and Proverbs of\nGoa\nExample Text in Konkani\n      \n\"हमर मिथिला धरोहर छै\"\nTranslation: \"Our Mithila is a heritage.\"\nSource: Maithili Literature, Mithila and Its Legacy\nExample Text in Maithili\nFigure 26: Example Texts for Assamese, Bengali, Bodo, Dogri, Gujarati, Hindi, Kannada, Kashmiri, Konkani, Maithili.\n13\n\nEvaluating Tokenizer Performance of Large Language Models in Indian Languages\nA PREPRINT\n      \n      \n      \n      \n      \n      \n      \n       \n      \n      \n      \n      \n\"വാർണം പൂക്കൾ\"\nTranslation: \"Colorful flowers.\"\nSource: Changampuzha Krishna Pillai, Varnangal (1940)\nExample Text in Malayalam\n\"ঈশ্বর শংদন চিত্রং\"\nTranslation: \"O Lord, give me strength.\"\nSource: Manipuri Devotional Songs, Manipur Shaiva Literature\nExample Text in Manipuri\n\"स्वप्नातली सोनपरी\"\nTranslation: \"The golden fairy of dreams.\"\nSource: Marathi Poets and Folklore, Marathi Literature\nExample Text in Marathi\n\"मेरा देशको माया माया छ\"\nTranslation: \"My love for my country is immense.\"\nSource: Nepali Literature, Patriotism in Nepali Poetry\nExample Text in Nepali\n\"ବନ୍ଦେ ଉତ୍କଳ ଜନନୀ\"\nTranslation: \"I bow to Mother Utkala.\"\nSource: Laxmibai, Odia Patriotism (Published in 1947)\nExample Text in Odia\n\"ਸੰਤੋਖੀ ਰਖੀਏ ਰੱਬ ਨਾਲ ਮਿੱਤਰੋ\"\nTranslation: \"Be content, my friends, with God.\"\nSource: Punjabi Proverbs and Spirituality, Punjabi Folklore\nExample Text in Punjabi\n\"सत्यमेव जयते\"\nTranslation: \"Truth alone triumphs.\"\nSource: Mundaka Upanishad, Shruti Texts of Hinduism (1500\nBCE)\nExample Text in Sanskrit\nTranslation: \"The moon shines brightly in the sky.\"\nSource: S. S. Pradhan, Santal Folklore (2001)\nExample Text in Santali\n\"آهي ملندو سکون کي دل ،سچائي ۾ محبت\"\nTranslation: \"In love, truth brings peace to the heart.\"\nSource: Sindhi Poetry, Sufism and Love (1982)\nExample Text in Sindhi\n\"காற்றின் இசை காதில் மழைத் துளி போல\"\nTranslation: \"The sound of the wind, like a raindrop in the ear.\"\nSource: Subramania Bharati, Tamil Poems and Songs\nExample Text in Tamil\n\"ప్రతీ రాత్రి తరువాత ఉదయం\"\nTranslation: \"After every night, comes the morning.\"\nSource: Gurajada Appa Rao, Telugu Literature and Hope\nExample Text in Telugu\n\"سکوت کا دریا ،میں رات چاندنی\"\nTranslation: \"The silence of the river on a moonlit night.\"\nSource: Faiz Ahmed Faiz, Whispers of the Night\nExample Text in Urdu\nFigure 27: Example Texts for Maithili, Malayalam, Manipuri, Marathi, Nepali, Odia, Punjabi, Sanskrit, Santali, Sindhi,\nTamil, Telugu, Urdu.\n14",
    "pdf_filename": "Evaluating_Tokenizer_Performance_of_Large_Language_Models_Across_Official_Indian_Languages.pdf"
}