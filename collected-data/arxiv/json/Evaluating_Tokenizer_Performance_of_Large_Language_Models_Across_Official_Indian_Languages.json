{
    "title": "EVALUATING TOKENIZER PERFORMANCE OF LARGE LANGUAGE",
    "abstract": "LargeLanguageModels(LLMs)basedontransformerarchitectureshaverevolutionizedavarietyof domains,withtokenizationplayingapivotalroleintheirpre-processingandfine-tuningstages. In multilingualmodels,particularlythosetailoredforIndiclanguages,effectivetokenizationiscrucial foroptimizingperformance. Thispaperpresentsacomprehensiveevaluationoftokenizersusedby 12LLMsacrossall22officiallanguagesofIndia,withafocusoncomparingtheefficiencyoftheir tokenizationprocesses. WeemployedtheNormalizedSequenceLength(NSL)asakeymetricin ouranalysis. OurfindingsrevealthattheSUTRAtokenizeroutperformsallothermodels,including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer’ssuperiorhandlingofIndiclanguages,GPT-4o’sadvancementoveritspredecessorGPT-4 inprocessingIndianlanguages,andthelimitedperformanceofProjectIndusincertainlanguages. This study underscores the critical importance of developing targeted tokenization strategies for multilingualandIndic-centricmodels,layingthegroundworkforfutureimprovementsintokenizer designtoenhancelinguisticcoverageandmodelefficiency.",
    "body": "EVALUATING TOKENIZER PERFORMANCE OF LARGE LANGUAGE\nMODELS ACROSS OFFICIAL INDIAN LANGUAGES\nAPREPRINT\nSagarTamang∗ Dr. DibyaJyotiBora\nDepartmentofIT DepartmentofIT\nTheAssamKazirangaUniversity TheAssamKazirangaUniversity\nJorhat,India Jorhat,India\ncs22bcagn033@kazirangauniversity.in dibyajyotibora@kazirangauniversity.in\nNovember20,2024\nABSTRACT\nLargeLanguageModels(LLMs)basedontransformerarchitectureshaverevolutionizedavarietyof\ndomains,withtokenizationplayingapivotalroleintheirpre-processingandfine-tuningstages. In\nmultilingualmodels,particularlythosetailoredforIndiclanguages,effectivetokenizationiscrucial\nforoptimizingperformance. Thispaperpresentsacomprehensiveevaluationoftokenizersusedby\n12LLMsacrossall22officiallanguagesofIndia,withafocusoncomparingtheefficiencyoftheir\ntokenizationprocesses. WeemployedtheNormalizedSequenceLength(NSL)asakeymetricin\nouranalysis. OurfindingsrevealthattheSUTRAtokenizeroutperformsallothermodels,including\nseveral Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA\ntokenizer’ssuperiorhandlingofIndiclanguages,GPT-4o’sadvancementoveritspredecessorGPT-4\ninprocessingIndianlanguages,andthelimitedperformanceofProjectIndusincertainlanguages.\nThis study underscores the critical importance of developing targeted tokenization strategies for\nmultilingualandIndic-centricmodels,layingthegroundworkforfutureimprovementsintokenizer\ndesigntoenhancelinguisticcoverageandmodelefficiency.\nKeywords tokenizer·LLM·tokens·GPT·SUTRA·indiclanguages.\n1 Introduction\n1.1 Background\nInanever-evolvinglandscapeofArtificialIntelligence(AI),transformers-basedgenerativeLargeLanguageModels\n(LLMs)aretransforminganincreasingnumberoffieldswithanever-increasingnumberofapplicationsinfinance,\nmedicine,education,andmanymore[1,2]. TokenizationisanimportantstepforLLMs,especiallyinpre-processing\nandfine-tuningstages[3].\nMostoftheLLMsuseeitheroftwotypesoftokenizationalgorithms,namelyWordPieceandBytePairEncoding(BPE).\nForexample,OpenaAi’sGPT-4omodelandMETA’sLlama3,bothemployamodifiedBPEtokenizer[4]. WordPiece\nwasdevelopedformodelslikeBERTemployingagreedyapproach. Itstartswiththelongestsubstringthatmatchesa\ntokeninitsvocabulary,allowingittohandleout-of-vocabularywordseffectivelybybreakingthemdownintoknown\nsubwordunits[5,6]. BPEworksbyiterativelymergingthemostfrequentlyoccurringpairsofcharactersorsubwords\ninacorpustocreateacompletevocabulary[7,8].\nAgnosticofthetokenizationalgorithmsused,manytechniqueshavebeendevelopedtocomparetokenizersofLLMs.\nSubwordfertilityisonesuchtechniquewhichmeasurestheaveragenumberoftokensusedperword[9]. Normalized\nSequenceLength(NSL)isanothersuchmetricusedtoevaluatetheefficiencyoftokenizers[12].\n∗Correspondancecanbeaddressedtocs22bcagn033@kazirangauniversity.in\n4202\nvoN\n91\n]LC.sc[\n1v04221.1142:viXra\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nLLM’s Tokenizer\nઉદાહર ણ ટ\u0000ે \u0000\u0000સ\nउदाह र ण पाठ\nExam p l e Texts\nExample Texts LLM’s Tokenizer Tokenized Output Evaluation Leaderboard\n(for each language)\nLLM’s Tokenizer\nFigure1: Evaluationpipeline: (1)Wecollectexampletextsforall22languages. (2)Wesendtheexampletextstothe\nLLMs’tokenizer. (3)Evaluatethetokenizedoutputs. (4)Weconstructleaderboardsusingourevaluation.\nAneffectivetokenizerisessentialforenablingamodeltolearnalanguageefficiently. Improvedtokenizerperformance\noffersseveralbenefits,includingfastertokengenerationandreducedcomputationalresourcerequirements,enhancing\nbothefficiencyandcost-effectiveness[13].\n1.2 TokenizersinMultilingual&IndicModels\nManymultilingualmodels,includingOpenAI’sChatGPT[14],Google’sGemini[16],Meta’sLlama[17],andTWO\nAI’sSUTRA[15],aredesignedtodelivercoherentperformanceacrossawiderangeofgloballanguages,including\nIndian languages. Achieving this requires tokenizers in these LLMs to handle diverse languages efficiently. This\nstudyevaluatestheperformanceoftokenizersinthesemultilingualmodelsandIndic-specificmodelsacrossallofficial\nlanguagesofIndia.\nTherestofthepaperisorganizedinthefollowingway: Section2highlightsrelatedworks,Section3describesthe\nmethodologyofthisstudy,Section4iswheretheresultsareshowcasedandSection5isfordiscussionswithfuture\noutlook.\n2 LiteratureReview\n2.1 LargeLanguageModelsandTokenization\nEversincethereleaseoftransformer-basedarchitecture,LargeLanguageModels(LLMs)havedemonstratedremarkable\ncapabilitiesinNatualLanguageProcessing(NLP)tasksandbeyond[19]. However,somerecentstudies[18]havealso\nproposedpromisingnon-transformerLLMs. LLMstodayexhibitaplethoraofapplicationsnotlimitedtoNLPtasks,\nbutcanalsoperformgeneraltasksperformingmulti-stepreasoning. Thus,LLMsarebecomingthebasicbuildingblock\nfordevelopinggeneral-purposeAIagentsorArtificialGeneralIntelligence(AGI)[18].\nTokenizationreferstotheprocessofconvertingasequenceoftextsintosmallerparts,knownastokens. Inorderto\nincreasethecoverageofdictionariesandalsotodealwithwordsthatwereunseenintrainingdata,LLMsusesub-words\nbasedtokenizerslikeBPEorWordpiece[4,8,7,18].\n2.2 Indic-SpecificLanguageModels\nS.Bhatetal. evaluatedthecapabilitiesofgenerativemodelslikeChatGPT,mT0,andBLOOMZingeneratingIndic\nlanguages,andthefindingsrevealedthatthesemodelshavelimitedcapabilitiesingeneratingtextinIndiclanguages\ninazero-shotsetting. WhiletheyperformedbetteronmanualqualityevaluationsforEnglish,theirperformancein\nIndiclanguageshighlightedtheneedforfurtherdevelopmentandtrainingspecificallytailoredtotheselanguages[22].\nSeveralstudieshaveintroducedbenchmarkstoevaluatetheperformanceofLLMsacrossIndiclanguages,andtheir\nresultsemphasizethenecessityformorefocusedresearchanddevelopmenteffortsinLLMstohandlethelinguistic\ndiversityofIndia[23,24,25].\nOnenotableadvancementisSUTRA(ScalableMultilingualLanguageModelArchitecture),introducedbyA.Bendale\netal. SUTRAsupportsover50languages,includingIndicones,bydecouplingconceptualunderstandingfromlanguage-\n2\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nspecificprocessingforscalablemultilinguallearning. ItusesaMixtureofExpertsframework,enhancingcomputational\nefficiencyandresponsiveness. EvaluationsshowthatSUTRAoutperformsmodelslikeGPT-3.5andLlama2by20-30%\nonMMLUbenchmarks. Themodelprovideshallucination-free,factual,andup-to-dateresponses,withthepotentialto\ndemocratizeAIaccess,especiallyinnon-Englishregions[15,29].\n2.3 ComparativeEvaluationofTokenizers\nTocomparethetokenizers,severalmetricsandmethodologieshavebeenintroduced. Subwordfertilityisonesuch\nmetricthatmeasurestheaveragenumberoftokensgeneratedperword[12]. Anidealtokenizerinthiscasewouldhave\nafertilityof1.0,indicatingthatmostwordsarerepresentedassingletokens,whileahigherfertilityscoresignalsthat\nmanywordsaresplitintomultipletokens[10]. Anothermetricthatisemployedistheproportionofcontinuedwords\n[10]whichindicatesthepercentageofwordsthataresplitintomultipletokens. 0isanidealvaluefortheproportion\nofcontinuedwords. Alowerproportionindicatesbetterperformance,asitmeansfewerwordsarefragmented[10].\nAnother metric that is used to evaluate and compare tokenizers is the Normalized Sequence Length (NSL). NSL\nmeasurestheaveragelengthoftokenizedsequencesproducedbyatokenizerrelativetoabaselinetokenizer[12].\nExistingtokenizerstudiesareperformedatasmallerscalethanwhatistypicalformodernLLMsorfocusonmultilingual\ntasks[12]. Somestudieshavetriedtoevaluatetheperformanceoftokenizersbutonlyinsomeselectlanguageslike\nTurkish[21],Arabic[26],orfocusingonafewIndianlanguages[3,20].\n2.4 GapsinExistingResearch\nThus,wecanobservethatthereexistsaresearchgaptoevaluatethetokenizersinotherIndianlanguagesaswell. Hence,\ninthisstudy,weareconductinganoverallevaluationofthetokenizersofLLMsinall22officiallanguagesofIndiaas\nrecognizedbytheeighthscheduleoftheIndianconstitution[27].\n3 Methodology\nOurevaluationstepissummarizedinFigure1.\n3.1 ExampleTexts\nExample Text in Assamese\n\"জীৱনৰ পিৰসেৰ \u0000মািহত \u0000হাৱােটা বা\u0000নীয়।\"\nTranslation: \"It is desirable to b e c aptivated by the expanse of\nlife.\"\nSource: \"Miri Jiyori\" by Lakshminath Bezbaroa\nFigure2: Assamesetextusedforevaluatingtokenizerperformance.\nWecompiledexampletextsinall22languagestoevaluatetheperformanceofthetokenizers. Eachlanguage’stext\nwasselectedinitsprimarywritingscripttoensureanauthenticassessmentofthetokenizer’scapabilitytoprocess\nnative scripts accurately. The curated example texts represent diverse linguistic structures and scripts, enabling a\ncomprehensiveanalysisoftokenizationperformance.\nAlltheexampletextsusedduringourstudycanbefoundintheAppendixA.2orFigure26and27. Onesuchexample\ntextcanbefoundinFigure2.\n3.2 Models\nWechose12modelsincludingproprietarymultilingualmodels,aswellasOpen-weightsmultilingualandIndiclanguage\nmodelsforourstudy. ThelistofmodelscanbefoundinTable1.\nWhileweacknowledgethatsomemodelsarenotspecificallydesignedforallIndianlanguages—suchasMahaMarathi,\nwhichistailoredforMarathi,andMBZUAI’sNanda,whichisoptimizedforHindiandEnglish—wehaveincluded\ntheminthisstudyforthesakeofcomprehensiveevaluation.\n3\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nLanguages\nModels Availability\nTested\nGPT-4o All Proprietary\nGPT-4 All Proprietary\nTWO/sutra-mlt256-v2 All Proprietary\nmicrosoft/Phi-3.5-MoE-instruct All Open-weights\nmeta-llama/Llama-3.1-405B-FP8 All Open-weights\nai4bharat/Airavata All Open-weights\nCohereForAI/aya-23-35B All Open-weights\nMBZUAI/Llama-3-Nanda-10B-Chat All Open-weights\nnickmalhotra/ProjectIndus All Open-weights\nsarvamai/OpenHathi-7B-Hi-v0.1-Base All Open-weights\nTelugu-LLM-Labs/Indic-gemma-7b-finetuned-sft-Navarasa-2.0 All Open-weights\nmarathi-llm/MahaMarathi-7B-v24.01-Base All Open-weights\nTable1: Listoftokenizerstested. “All\"referstoall22officiallanguagesofIndiaasrecognizedbytheEighthSchedule\noftheIndianConstitution. TheofficiallanguagesincludeAssamese,Bengali,Bodo,Dogri,Gujarati,Hindi,Kannada,\nKashmiri,Konkani,Maithili,Malayalam,Manipuri,Marathi,Nepali,Odia,Punjabi,Sanskrit,Santali,Sindhi,Tamil,\nTelugu,Urdu.\n3.3 EvaluationMetric\nForourworkisextendingthepreviousworksby[3],wehavechosentogowiththeNSLmetric. FormallytheNSLis\ndefinedby[12]c astheratiobetweenthelengthofanencodedsequencefromatokenizerT andatokenizerT . For\nλβ λ β\nN examplestakenfromadatasetD:\n(cid:80)N\nlength(T (D ))\nc = i=1 λ i\nλβ (cid:80)N\nlength(T (D ))\ni=1 β i\n4 Results\nAverageNSLValuesTable2presentstheaverageNSLvaluesforalltokenizersacrossthe22languages,calculated\nusingtheexamplesprovidedinAppendixA.2. Thescoresarereportedtofourdecimalplacesforprecision. Thebold\ntextindicatesthelowestvalueorthebestperformanceamongalltheothertokenizers. ItcanbeobservedthatSUTRA\ntokenizermanagestoexcelamongalltheothertokenizersincludingChatGPT’s4-oorotherIndicmodels.\nFigure3illustratesthenumberoflanguagesinwhicheachtokenizerachievedthehighestNSLscore. Forinstance,\nTWOAI’sSUTRAoutperformedallothertokenizersin14languages,whileMBZUAI’sNandaexcelledin6,and\nOpenAI’sGPT-4oin5. OthernotableperformersincludeindicmodelslikeTechMahindra’sProjectInduswith4,\nSarvamAI’sOpenHathiandMahaMarathiwith2each,andIndicGemma,MicrosoftPhi,andAiravatawith1each.\nNumberofTokensAppendixA.1providesindividualrowbarchartsforeachlanguage,offeringadetailedbreakdown\nofthenumberoftokensgeneratedineachlanguage. Lowertokencountsindicatebetteroutcomes.\n5 Discussion\nInthisstudy,weevaluatedthetokenizersfrom12LLMsinall22officiallanguagesofIndiaandwefoundthatthe\nSUTRAtokenizerperformedthebestamongallothers,outperformingthe2ndbesttokenizerbyalargemargin. This\nshowcasesthemultilingualstrengthoftheSUTRAtokenizertohandletheIndiclanguages.\nMicrosoft’sPhi-3.5-MoE-instructandGoogle’sIndicGemma ThoughboththemodelsweredevelopedforIndic\nlanguages,theydidnotperformuptothelevelsecuringbestperformancesonlyinonelanguageoutof22languages.\nObservationbetweenGPT-4andGPT-4o AnotherinterestingobservationisthatGPT-4,thepredecessorofGPT-4o,\ndidnotmanagetosecurethebesttokenizervalueinanyofthe22languages,astarkcontrasttoGPT-4o. Perhaps,\nthishighlightsthatanimportantdifferencebetweenthetwomodelsisthatthenewerGPT-4oiswelladeptatIndian\nlanguages,increasingthemulti-lingualcapability.\n4\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\n14\n14\n12\n10\n8\n6\n6\n5\n4\n4\n2 2\n2\n1 1 1\n0 0\n0\nGPT-4o GPT- S4 U TR LlA ama 3.1 PN ra on jed ca t In Od pu es In nH diat\nc\nhi G Me am hama M Ma ir ca rt oh si oft Ph Ai iravata Aya\nFigure3: NumberofBestPerformancesAchievedbyEachTokenizerAcross22Languages.\nLanguages GPT-4o GPT-4 SUTRA Llama3.1 Nanda ProjectIndus OpenHathi IndicGemma MahaMarathi MicrosoftPhi Airavata Aya\nAssamese 0.5429 1.4 0.4571 1.4 1.4 2.7714 1.5714 0.8286 1.3143 1.5428 1.5714 1.5142\nBengali 0.25 1.2307 0.2115 1.25 1.25 2.8076 1.3461 0.5769 1.0961 1.3269 1.3461 1.2307\nBodo 0.5675 1.0540 0.5405 0.5945 0.4594 0.4864 0.5405 0.5675 0.5405 1.2432 0.5405 0.7567\nDogri 0.5 1.0313 0.4688 0.5938 0.3750 0.4688 0.4063 0.4688 0.4063 1.0312 0.4062 0.7812\nGujarati 0.47545 1.6875 0.4688 1.7188 1.7188 2.75 2.6875 0.7188 1.0938 2.6562 2.6875 1.7187\nHindi 0.4091 1.0 0.4545 0.5909 0.3636 0.3182 0.4545 0.5455 0.3636 1.0454 0.4545 1.0454\nKannada 0.44 1.76 0.44 1.8 1.8 2.84 2.52 0.56 1.12 2.48 2.52 1.72\nKashmiri 0.6047 1.093 0.5814 0.8837 0.8837 1.8605 1.1628 0.5814 1.186 1.1395 1.1627 0.9069\nKonkani 0.4643 1.1429 0.5357 0.6429 0.5 0.4643 0.6071 0.5357 1.1071 0.6071 0.6071 0.8214\nMaithili 0.4211 1.0 0.6316 0.6316 0.3684 0.3684 0.5789 0.5789 1.1578 0.5789 0.5789 0.7368\nMalayalam 0.5 1.75 0.5 1.8333 1.8333 3.0 1.3333 0.6667 1.25 1.3333 1.3333 1.8333\nManipuri 0.6471 1.2941 0.5882 1.3529 1.3529 2.8824 1.5882 0.7647 1.5882 1.5294 1.5882 1.2941\nMarathi 0.4706 0.9412 0.5294 0.6471 0.3529 0.3529 0.4706 0.5882 1.0582 0.4706 0.4705 0.8235\nNepali 0.4091 0.9091 0.3182 0.6364 0.3182 0.3636 0.3182 0.4545 0.4090 1.1363 0.3182 0.8181\nOdia 1.0 2.625 0.625 2.625 2.625 2.875 2.875 1.0625 2.875 2.8125 2.875 2.1875\nPunjabi 0.6538 1.6923 0.4615 1.7308 1.7308 2.7692 2.3077 0.7692 2.3077 2.2692 2.3076 1.7307\nSanskrit 0.5 1.0833 0.6667 0.5833 0.5 0.5 0.5 0.6667 1.08333 0.5 0.5 0.75\nSantali 2.7647 2.647 0.4705 2.7058 2.7058 2.8823 2.8823 1.0588 2.9411 2.8235 2.8823 2.7058\nSindhi 0.4117 0.9117 0.5 0.6176 0.6176 1.8529 1.0882 0.5588 1.1176 1.0588 1.0882 0.5882\nTamil 0.4411 1.3823 0.3823 1.4117 1.4117 2.7647 1.2352 0.5294 1.0882 1.2058 1.2352 1.2058\nTelugu 0.375 1.75 0.2916 1.7916 1.7916 2.8333 2.6666 0.625 1.125 2.625 2.6666 1.7083\nUrdu 0.3928 0.7857 0.3571 0.5357 1.8928 1.8928 1.1071 0.4285 1.1071 1.0714 1.1071 0.5357\nTable2: AverageNSLValuesAcrossModelsfor22Languages(lowerisbetter). Theboldvaluesindicatethebest-\nperformingtokenizerforeachlanguage.\nObservationofTechMahindra’sProjectIndus BothSUTRAandGPT-4otokenizersmanagetogetaconsistently\nlowaverageNSLvalue(below1.0)forallthelanguagesbuttheProjectIndus’tokenizerseemstobegettingthesame\nforonlyafewlanguageslike(1)Bodo,(2)Dogri,(3)Hindi,(4)Konkani,(5)Maithili,(6)Marathi,(7)Nepali,and\n(8)Sanskrit. Thisisprobablybecauseallthese8languagesfollowthesameDevanagariscriptofwriting,whichthe\nmodel’stokenizerwasprobablytrainedon. Butfortherestofthelanguages,thetokenizerseemstobestruggling,\ngettinganaverageNSLscoreofabove1(thehighertheworse).\nNumber of Tokens According to the results and appendix A.1, tokenizers like SUTRA generate fewer tokens\nacrossthe22Indianlanguagescomparedtoothers. Lowertokencountssuggestthatthetokenizerismoreefficientin\n5\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nprocessingtheinputtextwithoutexcessivefragmentation. Thisisacrucialfactorinimprovingtheoverallperformance\nandcomputationalefficiencyofLLMs,particularlyforlarge-scaleapplications.\nSignificanceoftokenizationinLLMs TokenizationplaysavitalroleinLLMsbybreakingdowntextintosmaller\nunits(tokens)thatthemodelcanprocessefficiently. Awell-designedtokenizerenablesthemodeltohandlecomplex\nlanguagestructures,out-of-vocabularywords,andmulti-languagecontextseffectively. Itenhancesthemodel’sabilityto\nunderstandandgeneratelanguagewithgreateraccuracy. Additionally,agoodtokenizerleadstoreducedcomputational\ncostsandresourcerequirementsbyoptimizingtokengeneration. Thisresultsinfastertrainingtimes,lowerresource\nconsumption,andoverallimprovedperformance,allowingthemodeltoprocessdiverselanguagesmoreeffectively.\nReal-WorldApplicationsandFutureDirections Theinsightsfromthisstudyhaveimportantimplicationsforthe\ndevelopmentofmultilingualmodelsacrossIndianlanguages. Futureresearchcouldfocusonenhancingtokenizersto\nbetterhandlelanguageswithcomplexscriptsorlanguageswithahighdegreeofdialecticalvariation,improvingmodel\nperformanceforbothhigh-resourceandlow-resourcelanguages.\n6 Acknowledgement\nWewouldliketothanktheAssamKazirangaUniversityforassistingusinconductingthisresearch.\nReferences\n[1] F. Chiarello, V. Giordano, I. Spada, S. Barandoni, and G. Fantoni, \"Future applications of generative large\nlanguagemodels: Adata-drivencasestudyonChatGPT,\"Technovation,vol.133,p.103002,2024.[Online].\nAvailable: https://www.sciencedirect.com/science/article/pii/S016649722400052X. [Accessed:\nNov.12,2024].\n[2] Y.Nie, Y.Kong, X.Dong, J.M.Mulvey, H.V.Poor, Q.Wen, andS.Zohren, \"ASurveyofLargeLanguage\nModelsforFinancialApplications: Progress,ProspectsandChallenges,\"arXivpreprintarXiv:2406.11903,2024.\n[Online].Available: https://arxiv.org/abs/2406.11903.[Accessed: Nov.12,2024].\n[3] S.TamangandD.J.Bora,\"PerformanceEvaluationofTokenizersinLargeLanguageModelsfortheAssamese\nLanguage,\" arXiv preprint arXiv:2410.03718, 2024. [Online]. Available: https://arxiv.org/abs/2410.\n03718\n[4] J.Yang,Z.Wang,Y.Lin,andZ.Zhao,\"LargeLanguageModelTokenizerBias: ACaseStudyandSolutionon\nGPT-4o,\"arXivpreprintarXiv:2406.11214,2024.[Online].Available: https://arxiv.org/abs/2406.11214.\n[Accessed: Nov.13,2024].\n[5] X.Song,A.Salcianu,Y.Song,D.Dopson,andD.Zhou,“FastWordPieceTokenization,”2021.[Online].Available:\nhttps://arxiv.org/abs/2012.15524.\n[6] O.Ogundepo, X.Zhang, andJ.Lin, “BetterThanWhitespace: InformationRetrievalforLanguageswithout\nCustomTokenizers,”2022.[Online].Available: https://arxiv.org/abs/2210.05481.\n[7] L.KozmaandJ.Voderholzer,“TheoreticalAnalysisofByte-PairEncoding,”2024.[Online].Available: https:\n//arxiv.org/abs/2411.08671.\n[8] V.Zouhar,C.Meister,J.L.Gastaldi,L.Du,T.Vieira,M.Sachan,andR.Cotterell,“AFormalPerspectiveon\nByte-PairEncoding,”2024.[Online].Available: https://arxiv.org/abs/2306.16837.\n[9] H.Singh,N.Gupta,S.Bharadwaj,D.Tewari,andP.Talukdar,\"IndicGenBench: AMultilingualBenchmarkto\nEvaluateGenerationCapabilitiesofLLMsonIndicLanguages,\"arXivpreprintarXiv:2404.16816,2024.[Online].\nAvailable: https://arxiv.org/abs/2404.16816.\n[10] Occiglot,\"EUTokenizerPerformance,\"[Online].Available: https://occiglot.eu/posts/eu_tokenizer_\nperfomance/.Accessed: Nov.17,2024.\n[11] Occiglot,\"TokenizerperformanceonEUlanguages,\"OcciglotBlog,Sep.26,2023.[Online].Available: https:\n//occiglot.eu/posts/eu_tokenizer_perfomance/.\n[12] G.Dagan,G.Synnaeve,andB.Rozière,\"Gettingthemostoutofyourtokenizerforpre-traininganddomain\nadaptation,\" arXiv preprint arXiv:2402.01035, 2024. [Online]. Available: https://arxiv.org/abs/2402.\n01035.\n6\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\n[13] Microsoft,\"ExploringtheNewFrontierofAI:OpenAI’sGPT-4-OforIndicLanguages,\"AzureAIBlog,Oct.30,\n2024. [Online].Available: https://techcommunity.microsoft.com/blog/azure-ai-services-blog/\nexploring-the-new-frontier-of-ai-openais-gpt-4-o-for-indic-languages/4142383.\n[14] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, et al., \"GPT-4 Technical Report,\" arXiv preprint\narXiv:2303.08774,2024.[Online].Available: https://arxiv.org/abs/2303.08774.\n[15] A.Bendale,M.Sapienza,S.Ripplinger,S.Gibbs,J.Lee,andP.Mistry,\"SUTRA:ScalableMultilingualLanguage\nModelArchitecture,\"arXivpreprintarXiv:2405.06694,2024.[Online].Available: https://arxiv.org/abs/\n2405.06694.\n[16] Gemini Team et al., Gemini: A Family of Highly Capable Multimodal Models, 2024. Available at: https:\n//arxiv.org/abs/2401.12345.\n[17] Dubey,Abhimanyu,etal.TheLlama3HerdofModels.arXivpreprint,2024.Availableat: https://arxiv.\norg/abs/2407.21783.\n[18] ShervinMinaee,TomasMikolov,NarjesNikzad,MeysamChenaghlu,RichardSocher,XavierAmatriain,and\nJianfengGao,\"LargeLanguageModels: ASurvey,\"arXivpreprintarXiv:2402.06196,2024.[Online].Available:\nhttps://arxiv.org/abs/2402.06196\n[19] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed\nAkhtar,NickBarnes,andAjmalMian,\"AComprehensiveOverviewofLargeLanguageModels,\"arXivpreprint\narXiv:2307.06435,2024.[Online].Available: https://arxiv.org/abs/2307.06435\n[20] AdaSci, \"Multilingual Tokenization Efficiency in Large Language Models: A\nStudy on Indian Languages,\" [Online]. Available: https://adasci.org/\nmultilingual-tokenization-efficiency-in-large-language-models-a-study-on-indian-languages/\n[21] CagriToraman,EyupHalitYilmaz,FurkanS¸ahinüç,andOguzhanOzcelik,\"ImpactofTokenizationonLanguage\nModels: An Analysis for Turkish,\" ACM Transactions on Asian and Low-Resource Language Information\nProcessing(TALLIP),vol.22,no.4,article116,pp.1–21,Mar.2023.[Online].Available: https://doi.org/\n10.1145/3578707\n[22] Bhat,Savita,VasudevaVarma,andNiranjanPedanekar.\"GenerativeModelsForIndicLanguages: Evaluating\nContentGenerationCapabilities.\"In*Proceedingsofthe14thInternationalConferenceonRecentAdvances\nin Natural Language Processing*, edited by Ruslan Mitkov and Galia Angelova, 187–195. Varna, Bulgaria:\nINCOMALtd.,September2023.https://aclanthology.org/2023.ranlp-1.21.\n[23] Singh, Harman, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, and Partha Talukdar. \"IndicGenBench: A\nMultilingualBenchmarktoEvaluateGenerationCapabilitiesofLLMsonIndicLanguages.\"In*Proceedingsofthe\n62ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers)*,editedbyLun-\nWeiKu,AndreMartins,andVivekSrikumar,11047–11073.Bangkok,Thailand: AssociationforComputational\nLinguistics,August2024.https://aclanthology.org/2024.acl-long.595,DOI:10.18653/v1/2024.acl-\nlong.595.\n[24] Singh, AbhishekKumar, RudraMurthy, VishwajeetKumar, Jaydeep Sen, andGanesh Ramakrishnan.\"Indic\nQA Benchmark: A Multilingual Benchmark to Evaluate Question Answering Capability of LLMs for Indic\nLanguages.\"arXivpreprint,2024.https://arxiv.org/abs/2407.13522.\n[25] Kumar,Aman,HimaniShrotriya,PrachiSahu,AmoghMishra,RajDabre,RatishPuduppully,AnoopKunchukut-\ntan,MiteshM.Khapra,andPratyushKumar.\"IndicNLGBenchmark: MultilingualDatasetsforDiverseNLG\nTasksinIndicLanguages.\"In*Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing*,editedbyYoavGoldberg,ZornitsaKozareva,andYueZhang,5363–5394.AbuDhabi,UnitedArab\nEmirates: AssociationforComputationalLinguistics, December2022.https://aclanthology.org/2022.\nemnlp-main.360,DOI:10.18653/v1/2022.emnlp-main.360.\n[26] Alyafeai,Zaid,MagedS.Al-shaibani,MustafaGhaleb,andIrfanAhmad.\"EvaluatingVariousTokenizersfor\nArabicTextClassification.\"arXivpreprint,2021.https://arxiv.org/abs/2106.07540.\n[27] GovernmentofIndia,\"EighthSchedule,\"[Online].Available: https://www.mea.gov.in/Images/pdf1/S8.\npdf.Accessed: Dec.5,2023.\n[28] I.Watts,V.Gumma,A.Yadavalli,V.Seshadri,M.Swaminathan,andS.Sitaram,“PARIKSHA:ALarge-Scale\nInvestigationofHuman-LLMEvaluatorAgreementonMultilingualandMulti-CulturalData,”arXivpreprint\narXiv:2406.15053,2024.[Online].Available: https://arxiv.org/abs/2406.15053\n[29] DeepLearning.ai, Startup TWO AI Launches SUTRA: A Multilingual Model\nfor South Asian Markets, https://www.deeplearning.ai/the-batch/\nstartup-two-ai-launches-sutra-a-multilingual-model-for-south-asian-markets/, Accessed:\n2024-11-18.\n7\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nA Appendix\nA.1 BarChartsofTokenCountsforEachLanguage\nAssamese\nGPT-4o 19\nGPT-4 49\nSUTRA 16\nLlama 3.1 49\nNanda 49\nProject Indus 97\nOpenHathi 55\nIndic Gemma 7B 29\nMahaMarathi 46\nPhi-3.5-MoE 54\nAiravata 55\nAya 53\n0 20 40 60 80 100\nFigure4: NumberoftokensrequiredforasingleexampletextinAssamese. Lowervaluesarebetter.\nBengali\nGPT-4o 13\nGPT-4 64\nSUTRA 11\nLlama 3.1 65\nNanda 65\nProject Indus 146\nOpenHathi 70\nIndic Gemma 7B 30\nMahaMarathi 57\nPhi-3.5-MoE 69\nAiravata 70\nAya 64\n0 50 100 150 200\nFigure5: NumberoftokensrequiredforasingleexampletextinBengali. Lowervaluesarebetter.\nBodo\nGPT-4o 21\nGPT-4 39\nSUTRA 20\nLlama 3.1 22\nNanda 17\nProject Indus 18\nOpenHathi 20\nIndic Gemma 7B 21\nMahaMarathi 20\nPhi-3.5-MoE 46\nAiravata 20\nAya 28\n0 10 20 30 40 50\nFigure6: NumberoftokensrequiredforasingleexampletextinBodo. Lowervaluesarebetter.\nDogri\nGPT-4o 16\nGPT-4 33\nSUTRA 15\nLlama 3.1 19\nNanda 12\nProject Indus 15\nOpenHathi 13\nIndic Gemma 7B 15\nMahaMarathi 13\nPhi-3.5-MoE 33\nAiravata 13\nAya 25\n0 5 10 15 20 25 30 35\nFigure7: NumberoftokensrequiredforasingleexampletextinDogri. Lowervaluesarebetter.\n8\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nGujarati\nGPT-4o 15\nGPT-4 54\nSUTRA 15\nLlama 3.1 55\nNanda 55\nProject Indus 88\nOpenHathi 86\nIndic Gemma 7B 23\nMahaMarathi 35\nPhi-3.5-MoE 85\nAiravata 86\nAya 55\n0 20 40 60 80 100\nFigure8: NumberoftokensrequiredforasingleexampletextinGujarati. Lowervaluesarebetter.\nHindi\nGPT-4o 9\nGPT-4 22\nSUTRA 10\nLlama 3.1 13\nNanda 8\nProject Indus 7\nOpenHathi 10\nIndic Gemma 7B 12\nMahaMarathi 8\nPhi-3.5-MoE 23\nAiravata 10\nAya 23\n0 5 10 15 20 25\nFigure9: NumberoftokensrequiredforasingleexampletextinHindi. Lowervaluesarebetter.\nKannada\nGPT-4o 11\nGPT-4 44\nSUTRA 11\nLlama 3.1 45\nNanda 45\nProject Indus 71\nOpenHathi 63\nIndic Gemma 7B 14\nMahaMarathi 28\nPhi-3.5-MoE 62\nAiravata 63\nAya 43\n0 20 40 60 80\nFigure10: NumberoftokensrequiredforasingleexampletextinKannada. Lowervaluesarebetter.\nKashmiri\nGPT-4o 26\nGPT-4 47\nSUTRA 25\nLlama 3.1 38\nNanda 38\nProject Indus 80\nOpenHathi 50\nIndic Gemma 7B 25\nMahaMarathi 51\nPhi-3.5-MoE 49\nAiravata 50\nAya 39\n0 20 40 60 80\nFigure11: NumberoftokensrequiredforasingleexampletextinKashmiri. Lowervaluesarebetter.\nA.2 ExampleTextsUsedforTokenizerEvaluation\n9\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nKonkani\nGPT-4o 13\nGPT-4 32\nSUTRA 15\nLlama 3.1 18\nNanda 14\nProject Indus 13\nOpenHathi 17\nIndic Gemma 7B 15\nMahaMarathi 11\nPhi-3.5-MoE 31\nAiravata 17\nAya 23\n0 5 10 15 20 25 30 35\nFigure12: NumberoftokensrequiredforasingleexampletextinKonkani. Lowervaluesarebetter.\nMaithili\nGPT-4o 8\nGPT-4 19\nSUTRA 12\nLlama 3.1 12\nNanda 7\nProject Indus 7\nOpenHathi 11\nIndic Gemma 7B 11\nMahaMarathi 12\nPhi-3.5-MoE 22\nAiravata 11\nAya 14\n0 5 10 15 20 25\nFigure13: NumberoftokensrequiredforasingleexampletextinMaithili. Lowervaluesarebetter.\nMalayalam\nGPT-4o 6\nGPT-4 21\nSUTRA 6\nLlama 3.1 22\nNanda 22\nProject Indus 36\nOpenHathi 16\nIndic Gemma 7B 8\nMahaMarathi 15\nPhi-3.5-MoE 15\nAiravata 16\nAya 22\n0 10 20 30 40\nFigure14: NumberoftokensrequiredforasingleexampletextinMalayalam. Lowervaluesarebetter.\nManipuri\nGPT-4o 11\nGPT-4 22\nSUTRA 10\nLlama 3.1 23\nNanda 23\nProject Indus 49\nOpenHathi 27\nIndic Gemma 7B 13\nMahaMarathi 20\nPhi-3.5-MoE 26\nAiravata 27\nAya 22\n0 10 20 30 40 50\nFigure15: NumberoftokensrequiredforasingleexampletextinManipuri. Lowervaluesarebetter.\n10\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nMarathi\nGPT-4o 8\nGPT-4 16\nSUTRA 9\nLlama 3.1 11\nNanda 6\nProject Indus 6\nOpenHathi 8\nIndic Gemma 7B 10\nMahaMarathi 7\nPhi-3.5-MoE 18\nAiravata 8\nAya 14\n0 5 10 15 20\nFigure16: NumberoftokensrequiredforasingleexampletextinMarathi. Lowervaluesarebetter.\nNepali\nGPT-4o 9\nGPT-4 20\nSUTRA 7\nLlama 3.1 13\nNanda 7\nProject Indus 8\nOpenHathi 7\nIndic Gemma 7B 10\nMahaMarathi 9\nPhi-3.5-MoE 25\nAiravata 7\nAya 18\n0 5 10 15 20 25\nFigure17: NumberoftokensrequiredforasingleexampletextinNepali. Lowervaluesarebetter.\nOdia\nGPT-4o 16\nGPT-4 42\nSUTRA 10\nLlama 3.1 42\nNanda 42\nProject Indus 46\nOpenHathi 46\nIndic Gemma 7B 17\nMahaMarathi 29\nPhi-3.5-MoE 45\nAiravata 46\nAya 35\n0 10 20 30 40 50\nFigure18: NumberoftokensrequiredforasingleexampletextinOdia. Lowervaluesarebetter.\nPunjabi\nGPT-4o 17\nGPT-4 44\nSUTRA 12\nLlama 3.1 45\nNanda 45\nProject Indus 72\nOpenHathi 60\nIndic Gemma 7B 20\nMahaMarathi 31\nPhi-3.5-MoE 59\nAiravata 60\nAya 45\n0 20 40 60 80\nFigure19: NumberoftokensrequiredforasingleexampletextinPunjabi. Lowervaluesarebetter.\n11\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nSanskrit\nGPT-4o 6\nGPT-4 13\nSUTRA 8\nLlama 3.1 7\nNanda 6\nProject Indus 6\nOpenHathi 6\nIndic Gemma 7B 8\nMahaMarathi 7\nPhi-3.5-MoE 13\nAiravata 6\nAya 9\n0 2 4 6 8 10 12 14\nFigure20: NumberoftokensrequiredforasingleexampletextinSanskrit. Lowervaluesarebetter.\nSantali\nGPT-4o 47\nGPT-4 45\nSUTRA 8\nLlama 3.1 46\nNanda 46\nProject Indus 49\nOpenHathi 49\nIndic Gemma 7B 18\nMahaMarathi 50\nPhi-3.5-MoE 48\nAiravata 49\nAya 46\n0 10 20 30 40 50\nFigure21: NumberoftokensrequiredforasingleexampletextinSantali. Lowervaluesarebetter.\nSindhi\nGPT-4o 14\nGPT-4 31\nSUTRA 17\nLlama 3.1 21\nNanda 21\nProject Indus 63\nOpenHathi 37\nIndic Gemma 7B 10\nMahaMarathi 38\nPhi-3.5-MoE 36\nAiravata 37\nAya 20\n0 10 20 30 40 50 60 70\nFigure22: NumberoftokensrequiredforasingleexampletextinSindhi. Lowervaluesarebetter.\nTamil\nGPT-4o 15\nGPT-4 47\nSUTRA 13\nLlama 3.1 48\nNanda 48\nProject Indus 94\nOpenHathi 42\nIndic Gemma 7B 18\nMahaMarathi 37\nPhi-3.5-MoE 41\nAiravata 42\nAya 41\n0 20 40 60 80 100\nFigure23: NumberoftokensrequiredforasingleexampletextinTamil. Lowervaluesarebetter.\n12\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nTelugu\nGPT-4o 9\nGPT-4 42\nSUTRA 7\nLlama 3.1 43\nNanda 43\nProject Indus 68\nOpenHathi 64\nIndic Gemma 7B 15\nMahaMarathi 27\nPhi-3.5-MoE 63\nAiravata 64\nAya 41\n0 10 20 30 40 50 60 70\nFigure24: NumberoftokensrequiredforasingleexampletextinTelugu. Lowervaluesarebetter.\nUrdu\nGPT-4o 11\nGPT-4 22\nSUTRA 10\nLlama 3.1 15\nNanda 15\nProject Indus 53\nOpenHathi 31\nIndic Gemma 7B 12\nMahaMarathi 31\nPhi-3.5-MoE 30\nAiravata 31\nAya 15\n0 10 20 30 40 50 60\nFigure25: NumberoftokensrequiredforasingleexampletextinUrdu. Lowervaluesarebetter.\nExample Text in Assamese Example Text in Bengali\n\"জীৱনৰ পিৰসেৰ \u0000মািহত \u0000হাৱােটা বা\u0000নীয়।\" \"পৃিথবীর িবিভ\u0000 \u0000াে\u0000 মানুেষর নানা সং\u0000\u0000িত রেয়েছ\"\nTranslation: \"It is desirable to b e c aptivated by the expanse of Translation: \"There are various c u ltures of people across\nlife.\" different parts of the world.\"\nSource: \"Miri Jiyori\" by Lakshminath Bezbaroa Source: General expression\nExample Text in Bodo Example Text in Dogri\n\"बोरो जा\u0000त \u0000ह\u0000सा नाकुं आ\u0000न एकोफ फयलाइ\" \"डोगरी जुबां साडा मान साडा अ\u0000भमान\"\nTranslation: \"The Bodo people b e lieve in peace and unity.\" Translation: \"Dogri language, o u r pride and honor.\"\nSource: Bodo Cultural Heritage by Ganesh Chandra Boro Source: Dogri Literature Society, Dogri Language and\n(2008) Literature\nExample Text in Gujarati Example Text in Hindi\n\"જનની જનમ\u0000ૂ\u0000મ \u0000વગ\u0000થી પણ મહાન છે\" \"मधुर मधुर मेरे द\u0000पक जल\"\nTranslation: \"Mother and moth e r l and are greater than heaven.\" Translation: \"Burn gently, my l a m p.\"\nSource: Subramania Bharati, Tamil Nationalism and Poetry Source: Mahadevi Verma, Madhur Madhur Mere Deepak Jal\n(20th century) (1930)\nExample Text in Kannada Example Text in Kashmiri\n\" Tಹ ra\u0000 nದ\nsl\naಹ ti\u0000 oಯ\nn:\n\"ಂ Y\u0000\no\nu\u0000 rನ l\u0000\no\n\u0000 ve\u0000ೕ ,\u0000 l\"\nike th e p assing wind.\"\n\" Tمٕ rﺎ aﺷ\nn\nsﯽٖ laﮭ tﭼ\nio\nِﺰ nﻨﻣ\n:\n\"ﮧِ Yﺗ oن uﻮ َﮨ\na\nز rٕو eر\nm\nزا yرﺎ ﮩ kﻣ\ni\nnﯽٖ gﮭ\n,\nﭼ\ny o\n\u0000 uﺑ \"\nlive in my evening and\nSource: Kuvempu, Sri Ramayana Darshanam (1949) dawn.\"\nSource: Kashmirian Sufi Poetry, Kashmiri Shayari\nExample Text in Konkani Example Text in Maithili\n\"जेवपा\u0000या ताटात एक चटणी खासच\" \"हमर \u0000म\u0000थला धरोहर छै\"\nTranslation: \"A special condim e n t on the dining plate.\" Translation: \"Our Mithila is a h e r i tage.\"\nSource: Konkani Literature Society, Folk Songs and Proverbs of Source: Maithili Literature, Mithila and Its Legacy\nGoa\nFigure26: ExampleTextsforAssamese,Bengali,Bodo,Dogri,Gujarati,Hindi,Kannada,Kashmiri,Konkani,Maithili.\n13\nEvaluatingTokenizerPerformanceofLargeLanguageModelsinIndianLanguages APREPRINT\nExample Text in Malayalam Example Text in Manipuri\n\"വാർണം പൂ\u0000ൾ\" \"ঈ\u0000র শংদন িচ\u0000ং\"\nTranslation: \"Colorful flowers. \" Translation: \"O Lord, give me s t r e ngth.\"\nSource: Changampuzha Krishna Pillai, Varnangal (1940) Source: Manipuri Devotional Songs, Manipur Shaiva Literature\nExample Text in Marathi Example Text in Nepali\n\"\u0000व\u0000ातली सोनपरी\" \"मेरा देशको माया माया छ\"\nTranslation: \"The golden fairy o f d reams.\" Translation: \"My love for my c o u n try is immense.\"\nSource: Marathi Poets and Folklore, Marathi Literature Source: Nepali Literature, Patriotism in Nepali Poetry\nExample Text in Odia Example Text in Punjabi\n\"ବେ\u0000 ଉ\u0000ଳ ଜନନୀ\" \"ਸੰਤੋਖੀ ਰਖੀਏ ਰੱਬ ਨਾਲ ਿਮੱਤਰ\"ੋ\nTranslation: \"I bow to Mother U t k ala.\" Translation: \"Be content, my fr i e n ds, with God.\"\nSource: Laxmibai, Odia Patriotism (Published in 1947) Source: Punjabi Proverbs and Spirituality, Punjabi Folklore\nExample Text in Sanskrit Example Text in Santali\n\"स\u0000यमेव जयत\"े\nTranslation: \"Truth alone trium p h s.\" Translation: \"The moon shines b r i ghtly in the sky.\"\nSource: Mundaka Upanishad, Shruti Texts of Hinduism (1500 Source: S. S. Pradhan, Santal Folklore (2001)\nBCE)\nExample Text in Sindhi Example Text in Tamil\n\" ﻲﻫآ وﺪﻨﻠﻣ نﻮﮑﺳ ﻲﮐ لد ،ﻲﺋﺎﭽﺳ ۾ ﺖﺒﺤﻣ\" \"கா\u0000ற\u0000\u0000 இைச காத\u0000\u0000 மைழ\u0000 \u0000ளி ேபால\"\nTranslation: \"In love, truth brin g s peace to the heart.\" Translation: \"The sound of the w i nd, like a raindrop in the ear.\"\nSource: Sindhi Poetry, Sufism and Love (1982) Source: Subramania Bharati, Tamil Poems and Songs\nExample Text in Telugu Example Text in Urdu\n\"ప\u0000\u0000 \u0000\u0000\u0000 తరు\u0000త ఉదయం\" \" تﻮﮑﺳ ﺎﮐ ﺎﯾرد ،ﮟﯿﻣ تار ﯽﻧﺪﻧﺎﭼ\"\nTranslation: \"After every night , c o mes the morning.\" Translation: \"The silence of the r i v er on a moonlit night.\"\nSource: Gurajada Appa Rao, Telugu Literature and Hope Source: Faiz Ahmed Faiz, Whispers of the Night\nFigure27: ExampleTextsforMaithili,Malayalam,Manipuri,Marathi,Nepali,Odia,Punjabi,Sanskrit,Santali,Sindhi,\nTamil,Telugu,Urdu.\n14",
    "pdf_filename": "Evaluating_Tokenizer_Performance_of_Large_Language_Models_Across_Official_Indian_Languages.pdf"
}