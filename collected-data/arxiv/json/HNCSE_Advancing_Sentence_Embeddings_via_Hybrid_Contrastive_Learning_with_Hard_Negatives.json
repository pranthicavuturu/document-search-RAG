{
    "title": "HNCSE Advancing Sentence Embeddings via Hybrid Contrastive Learning with Hard Negatives",
    "abstract": "Unsupervised sentence representation learning remains a critical challenge in mod- ern natural language processing (NLP) research. Recently, contrastive learning techniques have achieved significant success in addressing this issue by effectively capturing textual semantics. Many such approaches prioritize the optimization using negative samples. In fields such as computer vision, hard negative samples (samples that are close to the decision boundary and thus more difficult to distin- guish) have been shown to enhance representation learning. However, adapting hard negatives to contrastive sentence learning is complex due to the intricate syn- tactic and semantic details of text. To address this problem, we propose HNCSE, a novel contrastive learning framework that extends the leading SimCSE approach. The hallmark of HNCSE is its innovative use of hard negative samples to enhance the learning of both positive and negative samples, thereby achieving a deeper semantic understanding. Empirical tests on semantic textual similarity and transfer task datasets validate the superiority of HNCSE. 1 Introduction Sentence representation learning (SRL), or sentence embedding, is a crucial subfield of natural language processing (NLP) that involves encoding sentences into low-dimensional vectors to capture their semantic content. The essence of learning sentence representations is to grasp the full linguistic context, transcending mere word meanings. Sentence representation learning contributes to facilitating a myriad of applications, including information retrieval Liu et al. (2015), conversational AI You et al. (2021), and machine translation systems Zhang et al. (2021). ∗Corresponding author. Preprint. Under review. arXiv:2411.12156v1  [cs.CL]  19 Nov 2024",
    "body": "HNCSE: Advancing Sentence Embeddings via Hybrid\nContrastive Learning with Hard Negatives\nWenxiao Liu\nCollege of Cyberspace Security\nJinan University\nlwx_mailbk@163.com\nZihong Yang\nCollege of Cyberspace Security\nJinan University\nChaozhuo Li\nSchool of Cyberspace Security\nBeijing University of Posts and Telecommunications\nZijin Hong\nBirmingham College\nJinan University\nJianfeng Ma\nCollege of Cyberspace Security\nJinan University\nZhiquan Liu\nCollege of Cyberspace Security\nJinan University\nLitian Zhang\nBeihang University\nFeiran Huang∗\nCollege of Cyberspace Security\nJinan University\nAbstract\nUnsupervised sentence representation learning remains a critical challenge in mod-\nern natural language processing (NLP) research. Recently, contrastive learning\ntechniques have achieved significant success in addressing this issue by effectively\ncapturing textual semantics. Many such approaches prioritize the optimization\nusing negative samples. In fields such as computer vision, hard negative samples\n(samples that are close to the decision boundary and thus more difficult to distin-\nguish) have been shown to enhance representation learning. However, adapting\nhard negatives to contrastive sentence learning is complex due to the intricate syn-\ntactic and semantic details of text. To address this problem, we propose HNCSE, a\nnovel contrastive learning framework that extends the leading SimCSE approach.\nThe hallmark of HNCSE is its innovative use of hard negative samples to enhance\nthe learning of both positive and negative samples, thereby achieving a deeper\nsemantic understanding. Empirical tests on semantic textual similarity and transfer\ntask datasets validate the superiority of HNCSE.\n1\nIntroduction\nSentence representation learning (SRL), or sentence embedding, is a crucial subfield of natural\nlanguage processing (NLP) that involves encoding sentences into low-dimensional vectors to capture\ntheir semantic content. The essence of learning sentence representations is to grasp the full linguistic\ncontext, transcending mere word meanings. Sentence representation learning contributes to facilitating\na myriad of applications, including information retrieval Liu et al. (2015), conversational AI You et al.\n(2021), and machine translation systems Zhang et al. (2021).\n∗Corresponding author.\nPreprint. Under review.\narXiv:2411.12156v1  [cs.CL]  19 Nov 2024\n\nConventional endeavors generally follow the supervised learning paradigm Jordan and Rumelhart\n(2013). However, such supervised learning methods suffer from the substantial cost associated\nwith gathering sufficient training data. Recently, unsupervised sentence learning aims to enhance\nefficiency and cost-effectiveness in training without sacrificing the accuracy or efficacy of the resulting\nsentence representations. Unsupervised sentence representation learning refers to training models\nto understand the semantic content of sentences without explicit human-labeled guidance, often\nusing methods like auto-encoding, predicting surrounding context, or self-supervision techniques\nsuch as contrastive learning Gao et al. (2021). At present, the mainstream methods of unsupervised\nsentence representation learning follow the idea of contrastive learning, that is, to train the sentence\nrepresentation model by maximizing the similarity between positive sample pairs and minimizing the\nsimilarity between negative sample pairs.\nTwo dogs are running.\nA man surfing on the sea.\nE\nTwo dogs are walking.\nE\nEncoder\nOver-Easy Negatives\nOver-Hard Negatives\nHard Negatives\nPositive\nMissing？\nMissing？\nQue\nry\nQue\nry\nFigure 1: An example of a query with its positive, over-easy, over-hard and missing hard negative\nsamples. The figure shows the situation that two too similar sentences may cause the loss of normal\nhard negative samples after passing through the encoder.\nThe core of contrastive learning-based SRL lies in how to construct effective positive samples\nand negative samples. Positive samples refer to the sentence pairs that are semantically similar\nor identical, while negative samples denote the sentence pairs that are semantically unrelated or\ndistinct. Under unsupervised learning conditions, labeled information is not available. Therefore,\nwe need to design some heuristic methods to generate correct positive and negative samples. A\npopular strategy is to use the same sentence to generate positive samples through different data\nenhancement (such as random replacement, deletion, insertion of words, etc. Chuang et al. (2022)),\nand randomly select other sentences as the negative samples. However, such strategies are not a\npanacea to solve the unsupervised SRL task due to the following reasons. Firstly, the positive samples\nand negative samples are generated independently, without considering the semantic distance between\nthem. Such a weak constraint may blur the boundary between positive and negative samples, leading\nto a dilemma such as the similarity between positive samples is lower than the one between negative\nsamples Yang et al. (2022). Secondly, model performance is seriously affected by the number and\ndistribution of positive and negative samples. If the distributions of positive/negative samples are\nuneven, SRL models tend to learn biased or unstable sentence representations. Meanwhile, the\nmainstream unsupervised SRL methods (e.g., SimCSE Gao et al. (2021)) adopt the method of in-\nbatch negative sampling. Although in-batch negative sampling enjoys the advantage of computing\nresource efficiency, it still suffers from severe limitations such as relying on batch size, a limited\nnumber of negative samples, and diversity Li et al. (2021).\nThis paper targets the acquisition of superior, correlated positive-negative sample pairs to enhance\nSRL. A major hurdle is managing confusing, over-hard, and over-easy negative samples effectively.\nConfusing samples, often a byproduct of data augmentation like in SimCSE, can lead to divergent\nembeddings for positive pairs, resulting in subpar learning samples. Over-hard negatives, due to their\nhigh similarity to the query, may cause learning distortion. Conversely, over-easy negatives, marked\nby their extreme dissimilarity, can lead to an unchallenging learning process, inhibiting the acquisition\nof meaningful representations. Figure 1 illustrates an example of a dense embedded distribution\nand the issue of missing hard negatives. Addressing these issues highlights the necessity for careful\nconstruction of positive/negative samples, particularly hard negatives that balance difficulty and\nsimplicity. This balance is crucial for a well-tuned and effective learning path. However, identifying\nthese challenging negatives remains a significant challenge due to computational and storage demands.\nIn this paper, we introduce an innovative unsupervised comparative learning framework that leverages\nhard negative sample mixing to mitigate the adverse effects of confusing samples and enrich the\nreliability of hard negatives within the same batch. Contrasting prior research that solely targets\nnegative samples, our approach underscores the significance of positive samples in unsupervised\n2\n\nSRL, fostering a synergistic learning process between positive and negative samples. Our framework\nnot only harnesses hard negative information to enhance the quality of positive samples but also\ngenerates additional hard negatives through the mixing of existing samples. Furthermore, it efficiently\nexpands the negative sample pool, facilitating convergence in alignment characteristics. Extensive\nevaluations across various tasks have confirmed the superior performance of our proposal against\nSOTA benchmarks. In summary, our key contributions are as follows:\n• We propose the HNCSE framework, which is divided into HNCSE-PM and HNCSE-HNM,\nboth extended based on SimCSE. HNCSE-PM constructs positive samples closer to the\nquery through the hardest negative sample. HNCSE-HNM uses mixup on existing hard\nnegative samples to obtain higher quality hard negative samples.\n• The theoretical analysis of Hard Negative Mixing (HNM) has deeply elucidated its profound\nimpact on the enhancement of sentence representation learning.\n• Through extensive experiments, HNCSE has achieved promising improvements over Sim-\nCSE in terms of semantic textual similarity (STS) and transfer tasks (TR). Additionally,\nHNCSE demonstrates clear advantages over current popular large language models (LLMs)\nin STS tasks.\n2\nRelated Work\nIn recent times, the academic community increasingly focuses on unsupervised sentence representa-\ntion learning. Traditional approaches, such as those demonstrated by Arora et al.Arora et al. (2017)\nand Ethayarajh Ethayarajh (2019), generate sentence embeddings through a weighted average of\nword embeddings. Kiros et al.Kiros et al. (2015) introduce the SkipThought model, which applies the\nskip-gram model at the sentence level, using the encoded sentence to predict its adjacent sentences.\nMany researchers, like Qiao et al Qiao et al. (2019), are adopting outputs from pre-trained language\nmodels like BERT for sentence embedding. However, Ethayarajh Ethayarajh (2019) observes that\ndirectly using BERT for this task yields suboptimal results. Li et al Li et al. (2022) note that BERT in-\nduces anisotropy in the sentence embedding space, adversely impacting the performance on Semantic\nText Similarity (STS) tasks.\nTo address this, Li et al Li et al. (2022) introduce BERT-flow, a method that utilizes a flow-based\ntechnique, as described by Dinh, Krueger, and Bengio Dinh et al. (2014). This technique transforms\nthe anisotropic sentence embedding distribution into a more uniform, isotropic Gaussian distribution,\naiming to improve performance on STS tasks. Concurrently, Su et al Su et al. (2023) present the\nBERT-whitening approach. This method leverages traditional whitening techniques to produce a\nmore consistent sentence embedding distribution. It not only matches the performance of BERT-flow\nbut also reduces the dimensionality of the sentence embeddings, enhancing computational efficiency.\n3\nMethodology\nThe HNCSE model, as outlined in the literature, pioneers a novel training approach that leverages hard\nnegative mixing to refine sentence embeddings. It rectifies misclassified positives by incorporating\nhard negative traits, thereby boosting the model’s discriminative capacity. HNCSE then broadens the\nchallenge set by augmenting existing hard negatives, fostering a more robust learning context that\nadvances sentence representation learning. Following this, we will elucidate the foundation of our\nbase model, SimCSE Gao et al. (2021).\nThe core idea of unsupervised SimCSE is to use a contrastive learning objective to train sentence\nembeddings without any labeled data. The contrastive learning objective is based on the principle of\nmaximizing the agreement between two different views of the same sentence, while minimizing the\nagreement between different sentences. The two views are obtained by applying dropout to the input\nsentence and encoding it with a pre-trained transformer model. The contrastive learning objective\ncan be formulated as follows:\nL(θ) = −1\nN\nN\nX\ni=1\nlog\nexp (sim (fθ(si), fθ(˜si)) /τ)\nPN\nj=1 exp (sim (fθ(si), fθ(˜sj)) /τ)\n(1)\n3\n\nE\nHardest Negative\nOld  Positive\nMixup\nNew Positive\nMixup\n1\nSentence\nSentence2\nEncoder\n𝑑1\n𝑑2\n𝑑3\n𝑑1 ≈𝑑2 > 𝑑3\nQUE\nRY\nQUE\nRY\nFigure 2: Schematic diagram of Positive Mixing.\nwhere fθ is the sentence encoder parameterized by θ, si and ˜si are two views of the i-th sentence,\nsim is a cosine similarity function, τ is a temperature parameter, and N is the batch size. This\nobjective encourages the embeddings of the same sentence to be close to each other, while pushing\naway the embeddings of different sentences. This is a simple but effective method to learn sentence\nembeddings that can capture semantic similarity. Next we will detail each of the two ways to improve\nthe unsupervised SimCSE.\n3.1\nPositive Mixing\nIn text-based contrastive learning, \"hard negatives\" are exemplars that the model nearly misclassifies\nas positive during training. These negatives are erroneously deemed highly similar to positives.\nIntegrating hard negatives with positives offers unique advantages. It mirrors real-world scenarios\nwhere category distinctions can be ambiguous, training the model to navigate such complexities. This\nrefines the model’s class boundaries, enhancing its ability to discern between subtly distinct instances.\nIt also mitigates overfitting by diverting the model’s focus from only the clear and easy examples.\nSimCSE’s positive sampling involves feeding the same sentence into the model twice to yield two\ndistinct eigenvectors as positives, facilitated by dropout layers. Despite dropout, semantically distinct\nsentences may still produce similar positive samples due to random masking.\nFor example, as shown in Figure 2, given two input sentences \"Two dogs are running” standing\nfor Sentence1 and \"Two dogs are walking” standing for Sentence2, after dropout in SimCSE’s\nunsupervised mode, two pairs of query and positive can be achieved, which are respectively shown as\n(Q1, P1) and (Q2, P2). The similarity between Q1 and P2 might be even larger that the one between\nQ1 and P1, which is obviously incorrect.\nIn order to shorten the distance between the query and the positive while increasing the distance\nbetween the positive and the negative, we compute the similarity between the query and the positive\nas well as the hardest negative, denoted as d1 and d2, respectively:\ns+\n2 =\n\n\n\n\n\nw1s+\n1 + w2s−\n1 ,\n0 < d1−d2\nd1\n≤0.1\ns+\n1 ,\nd1−d2\nd1\n> 0.1\nd1−d2\nd1\ns+\n1 + d1−d2\nd2\ns−\n1 ,\nd1 < d2\n(2)\nIn Equation (2), s+\n1 denotes the positive example, while s−\n1 stands for the most similar negative\ninstance. s+\n2 refers to the generated positive instance after the mixup of both positive and negative\ninstances. d1 signifies the similarity between the query and the positive instance, and d2 refers to the\nsimilarity between the query and the hardest negative. w1 and w2 respectively represent the weights\nof s+\n1 and s−\n1 .\n• Contrastive Loss. InfoNCE is adopted as the contrastive loss, which represents the model capability\nto estimate the mutual information. Optimizing InfoNCE loss maximizes the mutual information\nbetween positive samples and minimizes the mutual information between negative samples as:\n4\n\nLi = −1\nB\nB\nX\ni=1\nlog\nesim(s(i),s(i)′)/τ\nP2B\nj=1 esim(s(i),s(j))/τ\n(3)\nwhere s(i) and s(i)′ are embeddings obtained after the same sentence is input to the encoder through\ndifferent dropout masks, which are positive sample pairs; sim(·, ·) is the cosine similarity function; τ\nis a temperature coefficient that regulates similarity scaling; B is the number of sentences in a batch,\nand 2B is the number of vectors in a batch.\n3.2\nHard Negative Mixing\n�1\n�2\n��\n…\n�1\n�2\n��\n…\n�1\n�2\n��\n…\n���1\n���2\n����\n����������:\n② Computing the ���������� with ��\n③ Sort by the ���������� and Get TopK\n① Sample\nSamples From Previous Batch\n���������\n……\n�1\n′\n�1\n�2\n′\n�2\n��′\n��\n����������\n���������\nEncoder\nand\nDropout\n……\nIn-batch Samples\n��������1\n……\n�1\n′\n�1\n�2\n′\n�2\n��′\n��\n�1\n�2\n��\n��������2\n���������\nEncoder\nand\nDropout\n……\n…\n④  Choose � Pairs\n��\n��\n��\nMixup\nNegative Instance\nNew Hard Negative\nPositive Instance\nFigure 3: Schematic diagram of Hard Negative Mixing.\nConsider a scenario in which the embedding of a given query, denoted as q, and the embedding of a\ncorresponding key in the matched pair, denoted as k, is contrasted with every feature n in the bank of\nnegatives (N).\nThe popular loss function for contrastive learning Chen et al. (2020); He et al. (2020) is:\nLq, k, N = −log\nexp(qT k/τ)\nexp(qT k/τ) + P n ∈N exp(qT n/τ)\n(4)\nwhere τ assumes the role of a temperature parameter, and it is worth noting that all embeddings\nundergo ℓ2-normalization.\nThe query and key are two distinct augmentations of an identical sentence. The negative bank,\ndenoted as N, comprises instances that act as negatives for each positive pair. Specifically, it can be\ndefined as encompassing the remaining texts within the present minibatch Chen et al. (2020).\nThe logarithmic likelihood function formulated in Eq (4) operates within the bounds of a probability\ndistribution constructed via the application of the softmax function to each input/query q. If we let\npzi denote the probability of match between the query and the feature fi ∈F = N ∪k, then the\nderivative of the loss concerning the query q, with respect to the gradient, can be expressed:\n∂Lq, k, N\n∂q\n= −1\nτ\n\u0010\n(1 −pk) · k −\nX\nn ∈Npn · n\n\u0011\nwhere pfi =\nexp(qT fi/τ)\nP\nj∈Z exp(qT fj/τ).\n(5)\npk and pn symbolize the likelihood of a match pertaining to the key and the negative feature, denoted\nas fi = k and fi = n respectively. It is discernible that the impacts of the positive and negative logits\nupon the loss function mirror those encountered in a (K + 1)-way cross-entropy classification loss.\n5\n\nTo further expand our understanding, let’s consider the dimensional dynamics of the embeddings.\nAssume that each embedding vector v (whether q, k, or n) is of dimension d. Then the normalization\nprocess can be described as:\nˆv =\nv\n|v|2\n(6)\nwhere |v|2 denotes the L2 norm of vector v. This normalization ensures that the focus is on the\ndirection of the vectors rather than their magnitude.\n3.2.1\nHard Negative Mixing for Contrastive Learning\nIn contrastive learning, particularly within unsupervised learning, the role of hard negative samples is\npivotal for effective model training. The standard practice of using batch negatives often falls short,\nas they may not offer a sufficient challenge or provide the necessary contrast with positive examples,\nas shown by recent studies Kalantidis et al. (2020). To address this, Hard Negative Mining has been\nintroduced to enhance the learning process.\nHard Negative Mining involves deliberately selecting negative samples that are more challenging and\nclosely resemble positive samples in the feature space. This method is more effective in teaching the\nmodel to distinguish between similar-looking but distinct categories, thereby improving the model’s\nrobustness and discriminative capabilities, as evidenced by recent advancements.\nIn response to the need for more effective hard negatives, we have developed an approach inspired\nby Kalantidis et al. (2020) to further enhance the model’s generalization capabilities. Our strategy\ninnovatively applies the mixup technique to generate more relevant and challenging hard negative\nsamples, thus refining the learning process.\nAs shown in Figure 3, this approach begins by identifying a set of top k vectors, denoted as\nG = {G1, G2, ..., Gm}, which closely resemble a given input query x1 from a previous minibatch.\nThese vectors are then utilized as the foundation for crafting new hard negative samples. We employ a\nmethod of linear combination to produce m novel hard negatives. These are subsequently incorporated\ninto our existing negative sample bank (denoted as N in equation 4). The process of generating each\nnew hard negative instance, uo, is methodically formulated as follows:\nuo =\nuo\n||uo||2\n, where uo = αGi + (1 −α)Gj\n(7)\nHere, uo(where o ∈{1, ..., m}) represents each new hard negative instance. The indices (i, j) are\nrandomly selected from the set of top k vectors, and the coefficient α helps balance the contribution\nof each vector in the mixup.\nExpanding further, we introduce an adaptive selection process for α, which varies based on the\nsimilarity between the vectors Gi and Gj. This approach ensures that the resulting hard negative is\nneither too similar nor too dissimilar to the query, maintaining an optimal level of difficulty. The\nadaptive selection is defined as:\nα = f(sim(Gi, Gj))\n(8)\nwhere f is a function that maps the similarity score to an appropriate value for α. The similarity\nfunction sim(·, ·) could be a cosine similarity.\nThis section outlines the Hard Negative Mixing strategy to optimize sentence representation learning.\nBy crafting challenging negative samples, this approach enhances the model’s semantic relationship\nrecognition. The efficacy of this method is demonstrated through experiments detailed in the\nsubsequent chapter, showcasing performance gains and comparisons with other methods.\nThe Appendix A provides a comprehensive mathematical analysis of the Hard Negative Mixing\nstrategy. It defines hard negatives, explains the generation of challenging negatives through linear\ncombinations, and discusses the impact on the model’s optimization and loss function adjustments.\nThe analysis elucidates how this method enhances model performance by increasing sample distinc-\ntiveness and improving discriminative ability. It also addresses the role of regularization and learning\nrate adjustments in stabilizing and optimizing the learning process. This theoretical framework\nsupports the experimental outcomes and offers guidance for applying the strategy effectively.\n6\n\nModel\nSTS12\nSTS13\nSTS14\nSTS15\nSTS16\nSTS-B\nSICK-R\nAvg.\nUnsupervised Models (Base)\nGloVe (avg.)\n55.14\n70.66\n59.73\n68.25\n63.66\n58.02\n53.76\n61.32\nBERT (first-last avg.)\n39.70\n59.38\n49.67\n66.03\n66.19\n53.87\n62.06\n56.70\nBERT-flow\n58.40\n67.10\n60.85\n75.16\n71.22\n68.66\n64.47\n66.55\nBERT-whitening\n57.83\n66.90\n60.90\n75.08\n71.31\n68.24\n63.73\n66.28\nIS-BERT\n56.77\n69.24\n61.21\n75.23\n70.16\n69.21\n64.25\n66.58\nCT-BERT\n61.63\n76.80\n68.47\n77.50\n76.48\n74.31\n69.19\n72.05\nRoBERTa (first-last avg.)\n40.88\n58.74\n49.07\n65.63\n61.48\n58.55\n61.63\n56.57\nRoBERTa-whitening\n46.99\n63.24\n57.23\n71.36\n68.99\n61.36\n62.91\n61.73\nDeCLUTR-RoBERT\n52.41\n75.19\n65.52\n77.12\n78.63\n72.41\n68.62\n69.99\nSIMCSE\n68.40\n82.41\n74.38\n80.91\n78.56\n76.85\n72.23\n76.25\nSIMCSE(reproduce)\n70.82\n82.24\n73.25\n81.38\n77.06\n77.24\n71.16\n76.16\nLLaMA2-7B\n50.66\n73.32\n62.76\n67.00\n70.98\n63.28\n67.40\n65.06\nLLaMA2-7B(P romptEOL)\n58.81\n77.01\n66.34\n73.22\n73.56\n71.66\n69.64\n70.03\nLLaMA2-7B(P retended_CoT )\n67.45\n83.89\n74.14\n79.47\n80.76\n78.95\n73.33\n76.86\nLLaMA2-7B(Konwledge_Enhancement)\n65.60\n82.82\n74.48\n80.75\n80.13\n80.34\n75.89\n77.14\nHNCSE-PM(ours)\n71.02\n83.92\n75.52\n82.93\n81.03\n81.45\n72.76\n78.38\nHNCSE-HNM(ours)\n69.76\n83.97\n75.52\n83.21\n81.63\n81.85\n72.87\n78.27\nUnsupervised Models (Large)\nSIMCSE\n70.88\n84.16\n76.43\n84.50\n79.76\n79.26\n73.88\n78.11\nSIMCSE(reproduce)\n71.02\n83.52\n76.06\n83.83\n78.95\n79.26\n72.24\n77.84\nHNCSE-PM(ours)\n72.94\n84.67\n77.24\n83.97\n79.53\n80.78\n74.79\n79.13\nHNCSE-HNM(ours)\n72.75\n84.54\n77.36\n84.58\n79.92\n80.60\n74.64\n79.20\nTable 1: Spearman’s correlation scores across seven STS benchmarks for various models. HNCSE-\nPM refers to Positive Mixing method. HNCSE-HNM refers to Hard Negative Mixing method.\n4\nExperiments\n4.1\nExperimental Settings\nWe executed experiments across seven established semantic text similarity (STS) benchmarks,\nspanning STS 2012–2016 as delineated by Agirre et al. from 2012 to 2016, in conjunction with the\nSTSBenchmark Cer et al. (2017) and SICKRelatedness Wijnholds and Moortgat (2021). Our model’s\nperformance was assessed using the SentEval toolkit, and the Spearman’s correlation was chosen as\nthe evaluation metric. Our methodology initiated by leveraging pre-existing BERT checkpoints. The\n[CLS] token embedding, extracted from the model’s output, served as the sentence representation.\nBeyond semantic similarity evaluations, our model was subjected to seven distinct transfer learning\ntasks to gauge its capacity for generalization.\n4.2\nImplementation Details\nIn the methodology outlined by Gao, Yao, and Chen Gao et al. (2021), we adopted an identical\ntraining dataset and protocol. This dataset encompasses a million sentences, sourced from Wikipedia.\nUtilizing a BERT model that’s been finely optimized Devlin et al. (2018), we derive an embedding for\neach of these sentences. Subsequent to this, we employ distinct dropout masks to produce augmented\nvariants of the aforementioned sentence embeddings. All models are trained over a single epoch,\nwith a batch size set at 64. Our computational implementation is founded on Python version 3.8.13,\nleveraging the capabilities of Pytorch version 1.12.1. All experimental tasks were executed on an\nNVIDIA GeForce RTX 3090 GPU equipped with 24G memory.\n4.3\nMain Results\n4.3.1\nSTS Task\nIn the study by Zhang et al. (2022), we benchmarked the newly introduced HNCSE against a range\nof conventional unsupervised approaches and the prevailing state-of-the-art contrastive learning\ntechnique for the text semantic similarity (STS) task. This comparison encompasses methods such as\naverage GloVe embeddings Pennington et al. (2014), mean embeddings from BERT or RoBERTa,\n7\n\nModel\nMR\nCR\nSUBJ\nMPQA\nSST\nTREC\nMRPC\nAvg.\nUnsupervised Models(Base)\nGloVe (avg.)\n77.25\n78.30\n91.17\n87.85\n80.18\n83.00\n72.87\n81.52\nBERT\n78.66\n86.25\n94.37\n88.66\n84.40\n92.80\n69.54\n84.94\nIS-BERT\n81.09\n87.18\n94.96\n88.75\n85.96\n88.64\n74.24\n85.83\nSimCSE-RoBERTa\n81.04\n87.74\n93.28\n86.94\n86.60\n84.60\n73.68\n84.84\nHNCSE-PM(ours)\n81.94\n86.99\n95.20\n89.77\n86.81\n85.31\n75.49\n85.93\nHNCSE-HNM(ours)\n81.64\n86.84\n95.11\n89.66\n86.81\n83.90\n75.91\n85.70\nUnsupervised Models(Large)\nSimCSE-RoBERTa\n82.74\n87.87\n93.66\n88.22\n88.58\n92.00\n69.68\n86.11\nHNCSE-PM(ours)\n84.91\n89.32\n95.00\n90.02\n89.68\n86.85\n75.49\n87.32\nHNCSE-HNM(ours)\n85.89\n90.57\n96.06\n89.91\n89.91\n85.91\n76.47\n87.82\nTable 2: Results on the Transfer Task datasets.\nBERT-flow, BERT-whitening, ISBERT Zhang et al. (2020), DeCLUTR Giorgi et al. (2020), CT-BERT\nCarlsson et al. (2020), and SimCSE Gao et al. (2021).\nAs delineated in Table 1, the peak performances achieved by our two variant models of HNCSE\nare 78.38% and 78.27% respectively. This clearly surpasses the unsupervised SimCSE employing\na BERT-base architecture. Specifically, HNCSE exhibits superior performance to SimCSE across\nSTS2012, STS2013, STS2014, STS2015, STS2016, SICK-R and STS-B benchmarks. Moreover, the\nmore robust iterations of our HNCSE, leveraging the BERT-large architecture, consistently outshine\nthe corresponding large model of SimCSE on the majority of STS tasks.\n4.3.2\nLarge language models are used for the STS Task\nThe growing interest in open-source large language models (LLMs) like LLaMA has highlighted\ntheir use in unsupervised sentence similarity tasks (STS). However, studies Li and Li (2023); Zhang\net al. (2024) have shown that these models’ performance in STS has not been up to par. The latest\nresults for LLaMA2-7B and its three prompt engineering strategies—PromptEOL, Pretended Chain\nof Thought (CoT), and Knowledge Enhancement—have demonstrated a shortfall when compared to\nthe methods introduced by HNCSE.\nThe architectures and training objectives of LLaMA2-7B and HNCSE have notably diverged.\nLLaMA2-7B, an autoregressive model, has been optimized for text generation, while HNCSE\nhas been trained using contrastive learning for sentence-level representation. This divergence may\nhave reduced the efficacy of LLaMA2-7B in sentence similarity tasks compared to HNCSE. Addi-\ntionally, prompt engineering for LLaMA2-7B has faced limitations: (1) it heavily relies on carefully\ncrafted templates requiring extensive optimization across models and tasks, increasing complexity in\nresearch and application; (2) these methods, designed primarily for generative models, show limited\neffectiveness for discriminative models, limiting their applicability; (3) as prompts become longer\nand more complex, the computational resource demand escalates, especially with large-scale datasets,\nhighlighting resource consumption. In contrast, HNCSE’s contrastive learning has eliminated the\nneed for complex prompt designs, ensuring stable application across various models and tasks, and\nhas shown improved efficiency in resource consumption.\n4.3.3\nTransfer Task\nIn the comprehensive evaluation of HNCSE’s performance using the seven transfer tasks from\nSentEval Conneau and Kiela (2018), the results delineated in Table 2 are particularly revealing.\nNotably, both the Base and Large configurations of HNCSE surpass SimCSE in six out of the seven\nbenchmark datasets within the TR task. Importantly, a remarkable enhancement in performance is\ndistinctly observed for the MRPC dataset.\n4.3.4\nAblation Study\nTo assess model components’ impact, we run ablation studies using two model variants, HNCSE-\nPMsingle and HNCSE-HNMsingle, with identical setups and hyperparameters as the main experiment.\n8\n\nModel\nSTS(Avg)\nTR(Avg)\nHNCSE-PM-BERTbase\n78.38\n85.93\nHNCSE-HNM-BERTbase\n78.27\n85.70\nHNCSE-PMsingle-BERTbase\n76.43\n85.06\nHNCSE-HNMsingle-BERTbase\n76.64\n84.89\nTable 3: Results of the Ablation Study.\n16\n32\n64\n128\n256\n73\n74\n75\n76\n77\n78\nBatch Size\nAvg, Spearman\nHNCSE-PM\nHNCSE-HNM\n(a) Impact of batch size\n16\n32\n64\n128\n75\n76\n77\n78\nMax Seq Length\nHNCSE-PM\nHNCSE-HNM\n(b) Impact of max sequence length\nFigure 4: The influence of different hyperparameters on HNCSE.\nHNCSE-PMsingle This model is a variant of HNCSE-PM, which means that in the Equation 2 of\nPositive Mixing, we only optimize the case where d1 is greater than d2, and the case where d1 is less\nthan d2 is not considered for optimization.\nHNCSE-HNMsingle This model is a variant of HNCSE-HNM, which refers to the operation of\ndirectly removing Mixup in the Positive Mixing method, that is, directly replacing uo with Gi in\nEquation 7.\nAblation results are shown in Table 3. The experimental results demonstrate that the outcomes of the\ntwo HNCSEsingle methods are, on average, lower than the two HNCSE methods on both the STS\ntask and the TR task. Therefore, the following conclusions can be drawn:\n(1) It is essential to optimize the positive samples by leveraging the hardest negative samples, which\nhelps to prevent these samples from becoming over-hard negative samples. (2) The application of\npairwise Mixup on hard negative samples is necessary, as it can capture beneficial aspects between\ndifferent hard negative samples for the positive samples, thereby constructing more challenging\nnegative samples.\n4.3.5\nAnalysis of Batch Size and Max Sequence Length\nIn our HNCSE model analysis, we’ve assessed the influence of batch size and max sequence length\non model performance. Figure 4(a) demonstrates that optimal performance for both HNCSE-PM and\nHNCSE-HNM has been achieved with a batch size of 64, regardless of the number of hard negatives.\nThe plateau in performance with larger batch sizes suggests that beyond a certain point, increased\nsize no longer provides additional valuable learning information, underscoring the importance of hard\nnegatives in mini-batches. Figure 4(b) shows the highest model value at a maximum sequence length\nof 32. Yet, model performance does not improve with increased sequence length. This may be due to\nlonger sequences introducing greater ambiguities or complexities in semantic relationships, making\nit more challenging for the model to disambiguate and accurately capture nuances, thus reducing\nperformance.\n5\nConclusion and Limitation\nIn this work, we introduce a novel framework, HNCSE, specifically designed to address the multi-\nfaceted challenges posed by the incorporation of hard negative samples in sentence representation\n9\n\nlearning. Unlike conventional approaches, HNCSE innovatively integrates identified hard negative\nsamples to optimize positive samples and construct even harder negative samples, thereby extending\nthe well-established SimCSE methodology. This approach facilitates a deeper and more nuanced\nunderstanding of negative samples. Empirical evaluations of HNCSE on semantic textual similarity\ndatasets and transfer task datasets demonstrate its superiority, indicating that unsupervised learning\nof sentence representations can make promising advancements to existing research.\nDespite its strong performance, we have identified certain limitations associated with HNCSE. Firstly,\nHNCSE relies on large amounts of unannotated text data. If this data contains substantial noise or\nirrelevant information, it may adversely affect the quality of the model’s embeddings. Secondly,\nHNCSE lacks explicit contextual information during training, potentially impeding its ability to\ncapture long-distance dependencies. These issues present challenges for future research.\nReferences\nXiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation\nlearning using multi-task deep neural networks for semantic classification and information retrieval.\n2015.\nChenyu You, Nuo Chen, and Yuexian Zou. Self-supervised contrastive cross-modality representation\nlearning for spoken question answering. arXiv preprint arXiv:2109.03381, 2021.\nYan Zhang, Ruidan He, Zuozhu Liu, Lidong Bing, and Haizhou Li. Bootstrapped unsupervised\nsentence representation learning. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 5168–5180, 2021.\nMichael I Jordan and David E Rumelhart. Forward models: Supervised learning with a distal teacher.\nIn Backpropagation, pages 189–236. Psychology Press, 2013.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\nembeddings. arXiv preprint arXiv:2104.08821, 2021.\nYung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljaˇci´c,\nShang-Wen Li, Wen-tau Yih, Yoon Kim, and James Glass. Diffcse: Difference-based contrastive\nlearning for sentence embeddings. arXiv preprint arXiv:2204.10298, 2022.\nJinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul\nChilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n15671–15680, 2022.\nYangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and\nJunjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training\nparadigm. arXiv preprint arXiv:2110.05208, 2021.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence\nembeddings. In International conference on learning representations, 2017.\nKawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry\nof bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512, 2019.\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. Skip-thought vectors. Advances in neural information processing systems, 28,\n2015.\nYifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. Understanding the behaviors of bert in\nranking. arXiv preprint arXiv:1904.07531, 2019.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence\nembeddings from pre-trained language models. arXiv preprint arXiv:2011.05864, 2022.\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components\nestimation. arXiv preprint arXiv:1410.8516, 2014.\n10\n\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. Whitening sentence representations for better\nsemantics and faster retrieval. arXiv preprint arXiv:2103.15316, 2023.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npages 1597–1607. PMLR, 2020.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 9729–9738, 2020.\nYannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard\nnegative mixing for contrastive learning. Advances in Neural Information Processing Systems, 33:\n21798–21809, 2020.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055, 2017.\nGijs Wijnholds and Michael Moortgat. Sicknl: a dataset for dutch natural language inference. arXiv\npreprint arXiv:2101.05716, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nYanzhao Zhang, Richong Zhang, Samuel Mensah, Xudong Liu, and Yongyi Mao. Unsupervised\nsentence representation via contrastive learning with mixing negatives. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36, pages 11730–11738, 2022.\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), pages 1532–1543, 2014.\nYan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, and Lidong Bing. An unsupervised sentence\nembedding method by mutual information maximization. arXiv preprint arXiv:2009.12061, 2020.\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. Declutr: Deep contrastive learning for\nunsupervised textual representations. arXiv preprint arXiv:2006.03659, 2020.\nFredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylipää Hellqvist, and Magnus\nSahlgren. Semantic re-tuning with contrastive tension. In International conference on learning\nrepresentations, 2020.\nXianming Li and Jing Li. Angle-optimized text embeddings. arXiv preprint arXiv:2309.12871, 2023.\nBowen Zhang, Kehua Chang, and Chunping Li. Simple techniques for enhancing sentence embed-\ndings in generative language models. arXiv preprint arXiv:2404.03921, 2024.\nAlexis Conneau and Douwe Kiela. Senteval: An evaluation toolkit for universal sentence representa-\ntions. arXiv preprint arXiv:1803.05449, 2018.\nA\nTheoretical Analysis of Hard Negative Mixing\nGiven a dataset of sentences S = {s1, s2, . . . , sn}, with a sentence embedding function f : S →Rd\nmapping sentences to a d-dimensional vector space, Hard Negative Mixing (HNM) aims to refine\nthe representation learning process by leveraging challenging negative samples.\n11\n\nA.1\nDefinition of Hard Negatives\nHard negatives are crucial for the effective training of models, especially in tasks that require fine-\ngrained discrimination between semantically close but distinct sentences. A hard negative for a given\nsentence si is defined as a sentence sj that, while semantically different, lies close in the embedding\nspace. This closeness poses a greater challenge for the model to accurately discriminate:\nHN(si) = {sj|sim(f(si), f(sj)) > θ, ∀sj ∈S \\ {si}}\n(9)\nwhere HN(si) denotes the set of hard negatives for the sentence si, sim(·) is a similarity measure\n(cosine similarity), f(·) represents the sentence embedding function mapping sentences to points in a\nhigh-dimensional vector space, S is the set of all sentences, and θ is a predefined threshold indicating\nthe minimum similarity for a sentence to be considered a hard negative.\nA.2\nMixing Strategy\nBuilding upon the foundational concept of hard negatives, as delineated in the preceding section, we\nintroduce a novel approach to further enrich the training dataset and amplify the model’s discriminative\npower. By strategically leveraging the defined hard negatives, we propose a method to generate\nsynthetic samples that embody the nuances and complexities encountered in real-world scenarios.\nThis method, termed Hard Negative Mixing (HNM), involves the innovative creation of synthetic\nhard negatives through the blending of embeddings from identified hard negative pairs. This strategy\nnot only augments the diversity of training data but also introduces a heightened level of challenge,\ncompelling the model to refine its understanding and representation of semantic distinctions.\nTo enhance the diversity and challenge of the training process, we employ a strategy of generating\nsynthetic hard negatives. This is achieved by taking a linear combination of two existing hard\nnegatives, hni and hnj, associated with the same anchor sentence:\nhnmix = λhni + (1 −λ)hnj\n(10)\nwhere λ is a mixing coefficient that lies in the range [0, 1]. This coefficient determines the relative\ncontribution of each hard negative to the synthetic sample. By adjusting λ, we can control the\ndifficulty and variability of the generated hard negatives, enriching the training data and encouraging\nthe model to learn more discriminative features.\nA.3\nOptimization Objective\nThe conceptual groundwork laid by the identification of hard negatives and the subsequent generation\nof synthetic hard negatives through our Hard Negative Mixing strategy paves the way for a refined\ntraining objective. This advanced stage of model training seeks not only to utilize these synthetic\nsamples effectively but also to optimize the model’s sensitivity towards differentiating between\nsemantically similar and dissimilar sentences. By intricately balancing the influences of positive\nsamples and synthetic hard negatives, we aim to craft an optimization objective that embodies the\ncomplexity of natural language, enhancing the model’s ability to discern subtle semantic nuances.\nThe primary goal of incorporating HNM into the training process is to optimize the model’s ability\nto distinguish between positive and negative samples. The optimization objective is formulated to\nminimize the distance between positive pairs while maximizing the distance between the anchor\nsentence and its synthetic hard negatives. This is quantified by the following loss function:\nmin\nf\nn\nX\ni=1\n−log\n \nexp(sim(f(si), f(s+\ni ))/τ)\nexp(sim(f(si), f(s+\ni ))/τ) + P\nhnmix∈HNM(si) exp(sim(f(si), f(hnmix))/τ)\n!\n(11)\nIn this equation, f(·) denotes the embedding function mapping sentences to a high-dimensional\nvector space. The term sim(·) represents a similarity measure (e.g., cosine similarity) between\ntwo embeddings. The variable s+\ni signifies a positive sample that is semantically similar to the\n12\n\nsentence si, and hnmix represents a synthetic hard negative sample generated for si through HNM.\nThe parameter τ, known as the temperature, scales the similarity scores to modulate the softmax\nfunction’s concentration level, impacting the overall differentiation sensitivity.\nThe numerator of the fraction within the logarithm accentuates the affinity between si and its\npositive counterpart s+\ni , compelling the model to cluster semantically similar sentences closer in the\nembedding space. Conversely, the denominator aggregates the exponential similarities between si and\nboth its positive pair and the set of synthetic hard negatives, promoting a spatial distinction between\nsi and the challenging negatives generated via HNM. Through this loss function, the optimization\nobjective delicately orchestrates a balance between attraction and repulsion forces in the embedding\nspace, fostering a nuanced understanding of semantic relationships critical for the advancement of\nnatural language processing capabilities.\nA.4\nOptimization Objective\nIn the realm of representation learning, particularly within the framework of HNM, the essence of\nrefining the model’s capacity to distinguish between semantically proximate yet distinct sentences\nis encapsulated in the formulation of a meticulously crafted optimization objective. This advanced\nstage of model refinement leverages both the synthetic hard negatives generated through HNM and\nthe positive samples to formulate a loss function that embodies the intricate dynamics of natural\nlanguage understanding. The overarching goal is to optimize the model in such a way that it achieves\na heightened sensitivity to the subtle semantic nuances that differentiate positive pairs from their hard\nnegative counterparts.\nTo articulate this concept more concretely, we introduce a loss function designed to minimize the\ndistance between positive sentence pairs while simultaneously maximizing the distance between each\nsentence and its associated synthetic hard negatives. This is achieved through a contrastive learning\napproach, where the model is encouraged to align closely with positive samples in the embedding\nspace and diverge from the synthetic hard negatives. The loss function is formulated as follows:\nmin\nf\nn\nX\ni=1\n−log\n \nexp(sim(f(si), f(s+\ni ))/τ)\nexp(sim(f(si), f(s+\ni ))/τ) + P\nhnmix∈HNM(si) exp(sim(f(si), f(hnmix))/τ)\n!\n(12)\nIn this equation, f(·) denotes the embedding function that maps sentences to a d-dimensional vector\nspace. The function sim(·) represents a measure of similarity (e.g., cosine similarity) between the\nembeddings of two sentences. The term s+\ni refers to a positive sample that is semantically similar\nto si, and hnmix denotes a synthetic hard negative sample generated through the HNM process for\nthe sentence si. The parameter τ, known as the temperature, serves to scale the similarity scores,\ncontrolling the sharpness of the softmax distribution utilized in the denominator.\nThe numerator of the fraction within the logarithm focuses on the similarity between the sentence si\nand its positive counterpart s+\ni , encouraging the model to learn embeddings that bring these pairs\ncloser together in the embedding space. Conversely, the denominator aggregates the similarities\nbetween si and its synthetic hard negatives hnmix, alongside the similarity to the positive pair,\nthereby promoting the model to differentiate si from its hard negatives effectively.\nA.5\nAnalyzing Impact on Embedding Space\nFollowing the in-depth exploration in the \"Optimization Objective\" section, where we discussed\noptimizing the model to distinguish between positive samples and synthetic hard negatives through a\ncarefully designed loss function, this process not only enhances the model’s discriminative capabilities\nbut also suggests a significant impact on the structure of the embedding space. Therefore, our\nsubsequent analysis will focus on the specific effects of the HNMstrategy on the embedding space,\nparticularly on how it alters the average pairwise distance between embeddings, thereby facilitating a\nmore distinct separation of semantic meanings.\nThe structural dynamics of the embedding space are crucial for the effective representation of\nsemantic relationships among sentences. The introduction of mixed hard negatives via HNM alters\nthis structure, potentially leading to a more separable space where similar and dissimilar sentences\n13\n\nare better distinguished. This transformation can be quantified by measuring the average pairwise\ndistance among embeddings before and after the application of HNM:\n∆= 1\n|S|2\nX\nsi,sj∈S\n∥f(si) −f(sj)∥2 −\n1\n|S|2\nX\nsi,sj∈S′\n∥f ′(si) −f ′(sj)∥2\n(13)\nIn this equation, S denotes the original set of sentences, and S′ represents the set of sentences after the\napplication of HNM. The function f maps sentences to their embeddings in the d-dimensional vector\nspace, while f ′ denotes the updated mapping post-HNM application. The norm | · |2 measures the\nEuclidean distance between two embeddings, providing a quantifiable measure of their dissimilarity.\nThe term ∆embodies the net change in the average pairwise distance among all sentence embeddings\nwithin the dataset, serving as an indicator of how the embedding space has evolved in response to\nHNM. A positive value of ∆suggests that, on average, embeddings have become more dispersed,\nimplying that HNM has succeeded in expanding the embedding space. Such an expansion is\nindicative of a model that is better attuned to differentiating between semantically similar and\ndissimilar sentences, potentially leading to improvements in tasks requiring fine-grained semantic\ndiscrimination.\nConversely, a negative value of ∆would suggest a contraction of the embedding space, which could\nhave implications for the model’s ability to distinguish between closely related semantic concepts.\nHowever, the goal of HNM is not merely to expand the embedding space indiscriminately but to do\nso in a manner that enhances the model’s discriminative capacity by fostering a more structured and\nseparable embedding landscape.\nA.6\nEnhancement of Model Discriminability\nFollowing the detailed exploration of HNMstrategies and their profound impact on the embedding\nspace, our focus shifts towards quantitatively assessing the improvements in model discriminabil-\nity. The synthetic hard negatives generated through HNM pose refined challenges, necessitating\nan advanced model capability to distinguish between semantically similar yet distinct sentences.\nThis subsequent section delves into the empirical evaluation of model performance enhancements,\nhighlighting the pivotal role of HNM in advancing the nuanced discernment capabilities of sentence\nembeddings.\nOne of the primary goals of HNM is to enhance the model’s ability to discriminate between se-\nmantically similar and dissimilar sentences, which is pivotal for tasks such as semantic similarity\nmeasurement and classification. This enhancement can be theoretically represented by an increase in\nmodel accuracy, measured as follows:\nA′ −A = 1\n|T|\nX\n(si,s+\ni ,hnmix)∈T\n\u0002\nI[sim(f ′(si), f ′(s+\ni )) > sim(f ′(si), f ′(hnmix))]−\nI[sim(f(si), f(s+\ni )) > sim(f(si), f(hnmix))]\n\u0003\n(14)\nIn this formulation, A and A′ represent the model’s accuracy before and after the application of\nHNM, respectively. The set T encompasses the triplets formed by an anchor sentence si, its positive\ncounterpart s+\ni , and the synthetic hard negative hnmix, generated through HNM. The function I[·]\ndenotes the indicator function, yielding a value of 1 when the condition within the brackets is true,\nand 0 otherwise. The term sim(f(si), f(sj)) measures the similarity between the embeddings of\nsentences si and sj, facilitated by the embedding function f(·), which is refined to f ′(·) post-HNM\napplication.\nThe accuracy measure thus reflects the differential impact of HNM on the model’s performance,\nquantified through the comparison of similarity scores between positive and hard negative pairs,\nbefore and after the integration of HNM. This metric directly illuminates the enhanced discriminative\ncapability of the model, as a result of the nuanced challenges introduced by synthetic hard negatives,\nfostering a deeper understanding and distinction between closely related sentences.\n14\n\nA.7\nImpact on Learning Dynamics\nAfter elucidating the role of HNMin enhancing model discriminability and its substantial effects on\nthe embedding space, it becomes imperative to examine how these strategies influence the learning\ndynamics of the model. The adaptive challenges introduced by synthetic hard negatives necessitate\na deeper understanding of the underlying adjustments in the model’s optimization process. This\nanalysis is critical for optimizing the training strategy and ensuring that the model effectively leverages\nthe nuanced complexities introduced by HNM, thereby achieving a sophisticated balance between\naccuracy and generalization.\nThe introduction of HNM into the training process modifies the learning dynamics by influencing the\ngradient of the loss function with respect to the embeddings. This section delves into the mathematical\nunderpinnings of how HNM alters the embedding updates during training, providing insights into the\noptimization trajectory and the embedding space’s evolution.\nA.7.1\nGradient of Loss Function\nThe loss function’s gradient with respect to the embeddings is a critical factor in the model’s learning\nprocess, dictating how the embeddings are updated in each iteration. Given the loss function L, the\ngradient with respect to the embedding of a sentence si can be expressed as:\n∂L\n∂f(si) = −1\nτ\n \n(1 −pk) · k −\nX\nn∈N\npn · n\n!\n(15)\nwhere:\n• f(si) denotes the embedding of sentence si.\n• τ is the temperature parameter scaling the similarity scores.\n• pk and pn represent the softmax probabilities of the positive and negative samples, respec-\ntively.\n• k is the embedding of the positive sample, and n denotes the embeddings of the negative\nsamples in the set N.\nThis formulation encapsulates the essence of contrastive learning, emphasizing the model’s aim to\npull positive pairs closer while pushing negative pairs apart in the embedding space.\nHNM influences the learning dynamics by adjusting the distribution and characteristics of negative\nsamples, thereby affecting the gradient of the loss function. The modification of the gradient can lead\nto more effective learning, especially in distinguishing closely related but distinct sentences. This\nis achieved by incorporating a mix of hard negatives, which are closer to the query sentence in the\nembedding space, thereby providing a stronger learning signal for the model.\nA.8\nBalancing Diversity and Difficulty\nIn the context of the profound adjustments introduced by HNMin the learning dynamics and its pivotal\nrole in enhancing model discriminability, the strategic balance between the diversity and difficulty of\nhard negatives emerges as a critical consideration. This balance is instrumental in ensuring that the\nsynthetic hard negatives generated through HNM not only challenge the model but also contribute\nto a comprehensive representation of the semantic landscape. Addressing this balance enables the\noptimization of the model’s performance by fine-tuning the difficulty level of training samples to\nmatch the model’s learning stage, thereby facilitating a more nuanced understanding of complex\nsemantic relationships.\nThe efficacy of HNM is not merely a function of introducing hard negatives but also depends on\nbalancing the diversity and difficulty of these negatives. This balance can be formalized as an\noptimization problem, aimed at maximizing the diversity among hard negatives while ensuring they\nremain challenging:\nmax\nHN(si) D(HN(si)) −β\nX\nhn∈HN(si)\nsim(f(si), f(hn))\n(16)\n15\n\nHere, HN(si) denotes the set of hard negatives for the sentence si, and D(HN(si)) quantifies the\ndiversity within this set. The diversity metric is pivotal for ensuring that the model encounters a broad\nspectrum of semantic variations, thereby enhancing its generalizability. The term sim(f(si), f(hn))\nmeasures the similarity between the embeddings of si and its hard negatives hn, with f(·) representing\nthe embedding function. The coefficient β is a balancing parameter that modulates the emphasis\nbetween maximizing diversity and the aggregate similarity of the hard negatives, signifying the\ndifficulty aspect.\nThis equation effectively captures the dual objective of optimizing the set of hard negatives by\nmaximizing their diversity while controlling their difficulty, as measured by their similarity to the\nsentence si. The trade-off, facilitated by β, allows for a nuanced adjustment of the training process,\ntailoring the challenge level to optimally advance the model’s discriminative capabilities.\nA.9\nOptimization Objective with Regularization\nFollowing the exploration of the strategic balance between diversity and difficulty in HNMand its\nsignificant implications for model training dynamics, the introduction of a regularization term in the\noptimization objective becomes a logical progression. This approach seeks to address the potential\nfor overfitting that may arise from the intensified training regimen implied by HNM. Incorporating\nregularization into the optimization framework not only ensures model robustness but also facilitates\nthe generalization of learned embeddings across unseen data, thus safeguarding against the pitfalls of\nover-specialization to the training set.\nIncorporating a regularization term, denoted as R(f), into the optimization objective is a well-\nestablished method for enhancing the robustness of machine learning models. It penalizes complex\nmodels to prevent overfitting, thereby improving generalization:\nmin\nf\n\"\nn\nX\ni=1\n−log\n \nexp(sim(f(si), f(s+\ni ))/τ)\nexp(sim(f(si), f(s+\ni ))/τ) + P\nhnmix∈HNM(si) exp(sim(f(si), f(hnmix))/τ)\n!\n+ λR(f)\n#\n(17)\nIn this expression, f(·) represents the embedding function mapping sentences to their corresponding\nvectors in a d-dimensional space. The sim(·) function denotes a similarity measure (e.g., cosine\nsimilarity) between two sentence embeddings. The symbol s+\ni refers to a positive sample associated\nwith si, and hnmix represents the synthetic hard negatives generated through the HNM process for\nsi. The parameter τ, known as the temperature, is used to scale the similarity scores, influencing\nthe sharpness of the softmax distribution in the denominator. The term λR(f) introduces the\nregularization component, where λ is a coefficient that balances the relative importance of the\nregularization term within the overall optimization objective. The function R(f) quantifies the\ncomplexity of the model, penalizing configurations that may lead to overfitting, thereby encouraging\nthe model to adopt simpler, more generalizable forms.\nThis equation, therefore, encapsulates a dual objective: minimizing the contrastive loss to enhance\ndiscriminability and optimizing model complexity to prevent overfitting. By achieving this balance,\nthe model is not only trained to distinguish effectively between semantically similar and dissimilar\nsentences but also ensured to generalize well to unseen data, embodying the essence of robust machine\nlearning models.\nA.10\nTemperature Scaling Analysis\nFollowing the intricate discussions on the optimization objectives, including the incorporation of reg-\nularization, and the strategic endeavors to balance diversity and difficulty within the HNMframework,\nthe exploration of temperature scaling emerges as a crucial analytical component. This analysis is\nimperative for fine-tuning the model’s sensitivity to distinctions between hard negatives and other\nsentence pairs. Temperature scaling, by adjusting the sharpness of the softmax distribution used\nin the model’s loss function, plays a pivotal role in modulating the learning process. This nuanced\n16\n\nadjustment allows for a more precise calibration of the model’s response to the varying degrees of\nchallenge presented by the hard negatives, thereby optimizing the effectiveness of the HNM strategy.\nThe temperature scaling parameter τ plays a crucial role in controlling the sharpness of the softmax\nfunction, directly impacting the model’s learning dynamics:\nTo further enhance the readability of the long equation, we introduce intermediate variables to\nsimplify its expression. This approach allows for a clearer understanding of each component of the\nequation. Let’s define the following intermediate variables:\n1. sim+\ni = sim(f(si), f(s+\ni )), representing the similarity between the positive pair.\n2. simi,hnmix = sim(f(si), f(hnmix)), representing the similarity between the hard negative mix\npair.\n3. Zi = exp\n\u0010\nsim+\ni\nτ\n\u0011\n+ P\nhnmix∈HNM(si) exp\n\u0010 simi,hnmix\nτ\n\u0011\n, representing the normalization factor.\nWith these definitions, we can rewrite the original equation as follows:\n∂L\n∂τ = −1\nτ 2\nn\nX\ni=1\n\n\nexp\n\u0010\nsim+\ni\nτ\n\u0011\n· sim+\ni\nZi\n\n\n(18)\nThis expression reveals how sensitivity to changes in τ affects the balance between positive and hard\nnegative pairs in the loss gradient, guiding the optimization process.\nA.11\nEmbedding Space Normalization\nBuilding upon the insightful analyses of temperature scaling and its pivotal role in fine-tuning the\nmodel’s discrimination capabilities, the focus now shifts toward the normalization of the embedding\nspace. Embedding space normalization is a critical step in ensuring that the refined embeddings,\ninfluenced by HNM and temperature adjustments, maintain a consistent scale. This normalization\nfacilitates a uniform interpretation of distances and similarities within the embedding space, which is\nessential for the effective application of HNM strategies. By normalizing the embeddings, we aim to\nenhance the model’s generalization ability and ensure that the semantic nuances captured through\nHNM are robustly integrated into the model’s learning framework.\nWhen integrating HNM with embedding normalization to unit length, the process is adapted to\naccommodate the discriminative enhancements introduced by HNM. In the adjusted Equation:\nfnorm−HNM(si) =\nfHNM(si)\n∥fHNM(si)∥2\n(19)\nfHNM(si) is the embedding of sentence si after applying HNM, reflecting the impact of challenging\nmixed negatives on the representation. The denominator ∥fHNM(si)∥2 is the L2 norm of this\nembedding, ensuring normalization to unit length. This normalization focuses on the directional\nattributes of the embedding vectors in the high-dimensional space, crucial for capturing angular\ndifferences indicative of semantic relations. The result, fnorm−HNM(si), is a normalized embedding\nthat retains the rich, discriminative features imbued by HNM, facilitating a precise and nuanced\nseparation of semantic content in the embedding space.\nA.12\nSimilarity Gradient Computation\nTransitioning from the essential task of embedding space normalization, which ensures uniformity\nand comparability of embeddings, we now advance to the intricate process of similarity gradient\ncomputation. This step is pivotal for understanding how the model updates its embeddings in response\nto the learning signal, especially within the HNM framework. The computation of similarity gradients\nis fundamental to the optimization process, as it elucidates the direction and magnitude of adjustments\nneeded for the embeddings to accurately reflect semantic relationships. By calculating these gradients,\nwe aim to further refine the model’s ability to discern subtle semantic differences, leveraging the\ncomplexity and challenges introduced by synthetic hard negatives.\n17\n\nThe gradient of the similarity function with respect to embeddings is essential for understanding how\nthe model updates embeddings in response to the learning signal:\n∇f(si)sim(f(si), f(sj)) =\nf(sj)\n∥f(si)∥∥f(sj)∥−sim(f(si), f(sj))(f(si))\n∥f(si)∥2\n(20)\nIn this expression, ∇f(si)sim(f(si), f(sj)) represents the gradient of the similarity function between\nthe embeddings of sentences si and sj with respect to the embedding of sentence si. The function\nf(·) yields the embedding of a sentence, mapping it to a point in the embedding space. The term\nsim(f(si), f(sj)) quantifies the similarity between the embeddings of si and sj, which is often\ncalculated using cosine similarity or another relevant metric in the embedding space.\nThe gradient is composed of two primary components. The first component,\nf(sj)\n|f(si)||f(sj)|, reflects the\ndirection towards the embedding of sj normalized by the magnitudes of both embeddings, essentially\npointing in the direction to increase similarity. The second component, −sim(f(si),f(sj))(f(si))\n|f(si)|2\n,\nadjusts this direction by considering the current level of similarity and the magnitude of f(si),\nthereby ensuring that the update is proportional to the need to increase or decrease similarity. This\ncomputation effectively guides the updates to embeddings during the learning process, ensuring\nthat they evolve in a manner conducive to maximizing the discriminative capability of the model as\ninfluenced by the semantic relationships embodied in the training data.\nA.13\nAnalyzing Embedding Divergence\nFollowing the computation of similarity gradients, which provides crucial insights into the directional\nadjustments required for embeddings, we pivot towards the analysis of embedding divergence. This\nanalysis is imperative as it quantitatively measures the extent to which the HNMstrategy influences the\nrepresentational space of the embeddings. By evaluating the divergence between embeddings before\nand after the application of HNM, we can gauge the effectiveness of this strategy in enriching the\nmodel’s semantic understanding. This step is not only logical but necessary for validating the impact\nof HNM on enhancing the model’s discrimination capabilities, ensuring that the introduction of\nsynthetic hard negatives leads to a more nuanced and robust representation of semantic relationships.\nThe divergence in embeddings, before and after applying HNM, sheds light on the method’s effec-\ntiveness in altering the representational space:\ndiv(f, f ′) =\nv\nu\nu\nt\nn\nX\ni=1\n∥f(si) −f ′(si)∥2\n2\n(21)\nIn this formulation, div(f, f ′) quantifies the overall divergence between the embeddings produced\nby the model before (f) and after (f ′) the application of HNM. The function f(si) represents the\nembedding of sentence si before the application of HNM, and f ′(si) denotes the embedding of the\nsame sentence after applying HNM. The operation |f(si) −f ′(si)|2 computes the Euclidean distance\nbetween the original and updated embeddings for each sentence si in the dataset, with the square root\nof the sum of these squared distances providing a holistic measure of how the embedding space has\nshifted as a result of HNM.\nThis divergence metric effectively encapsulates the extent of change in the embedding space, offering\na concise measure of the impact of incorporating synthetic hard negatives through HNM on the\nmodel’s semantic representation capabilities.\nA.14\nImpact of Hard Negative Mixing on Similarity Distribution\nBuilding on our analysis of embedding divergence, which quantitatively demonstrates the transforma-\ntive impact of HNMon the model’s representational space, we next turn our attention to the broader\nimplications of HNM on similarity distribution. This analysis is critical for understanding how HNM\nreshapes the landscape of semantic relationships within the embedding space, influencing the model’s\nability to discern and categorize these relationships accurately. By examining the shift in similarity\n18\n\ndistribution before and after the application of HNM, we seek to uncover the nuanced ways in which\nHNM enhances or modifies the model’s perception of semantic similarity, thus providing valuable\ninsights into the effectiveness and strategic value of incorporating hard negatives in the training\nprocess.\nHNM fundamentally alters the similarity distribution between sentence pairs, an effect that can be\nmeasured using the Kullback-Leibler divergence:\nDKL(Psim∥Qsim) =\nX\n(si,sj)∈S×S\nPsim(si, sj) log\n\u0012 Psim(si, sj)\nQsim(si, sj)\n\u0013\n(22)\nIn this equation, DKL(Psim|Qsim) denotes the KL divergence between two probability distributions:\nPsim, representing the distribution of similarities between sentence pairs before the application of\nHNM, and Qsim, representing the distribution after applying HNM. The summation runs over all\npairs of sentences (si, sj) within the set S, encompassing the entire dataset.\nPsim(si, sj) and Qsim(si, sj) are the probabilities associated with the similarity scores between\nsentences si and sj in the respective distributions before and after HNM is applied. The log function\nprovides a measure of the relative difference between these probabilities, with the overall KL\ndivergence summing these differences across all sentence pairs. This formulation effectively captures\nthe divergence in similarity distributions due to HNM, providing a quantitative lens through which\nthe impact of HNM on the embedding space’s structure can be assessed.\nA.15\nEnhanced Discriminative Feature Learning\nBuilding on the comprehensive analyses of embedding divergence and its implications for the\nrepresentational space, we now turn our attention to the enhanced learning of discriminative features\nfacilitated by HNM. This progression is both logical and necessary, as the previous analyses provide\na foundation for understanding how HNM reshapes the embedding landscape. The subsequent\nenhancement in the model’s ability to learn discriminative features is a direct consequence of these\nstructural changes. By focusing on this aspect, we aim to elucidate the direct link between the\nstrategic introduction of synthetic hard negatives and the model’s improved capability to distinguish\nbetween semantically similar but distinct sentences. This step underscores the pivotal role of HNM in\nfostering a deeper, more nuanced comprehension of semantic relations, which is critical for advancing\nthe state of the art in natural language processing tasks.\nThe ability of HNM to encourage the learning of more discriminative features is captured by the\nincreased margin between positive and hard negative pairs:\n∆margin =\nmin\ns+\ni ∈P (si), hnmix∈HNM(si)\n\u0010\nsim(f(si), f(s+\ni )) −sim(f(si), f(hnmix))\n\u0011\n(23)\nIn this formulation, ∆margin quantifies the increment in the separation margin between the embed-\ndings of positive pairs (si, s+\ni ) and the embeddings of sentence si and its synthetic hard negatives\nhnmix. The function sim(f(si), f(sj)) calculates the similarity between the embeddings of sen-\ntences si and sj, with f(·) representing the embedding function. The set P(si) contains positive\nsamples that are semantically close to si, and HNM(si) encompasses the synthetic hard negatives\ngenerated for si through the HNM process.\nThe objective of this measure is to capture the extent to which the model has enhanced its ability to\nembed semantically similar sentences closer in the embedding space, while effectively distancing the\nhard negatives. A larger ∆margin signifies a more pronounced discriminative capability, highlighting\nthe efficacy of HNM in enriching the feature learning process. This metric thus serves as a direct\nindicator of the model’s improved proficiency in parsing and understanding the nuanced semantic\nrelationships inherent in natural language.\nA.16\nLearning Rate Adjustment Strategy\nAfter delving into the nuances of embedding divergence and its implications for the model’s semantic\nrepresentation, the progression towards refining the learning rate adjustment strategy becomes both\n19\n\nlogical and necessary. This refinement is pivotal in accommodating the enhanced discriminative\nfeature learning facilitated by the HNMstrategy. By dynamically adjusting the learning rate in\nresponse to the presence of synthetic hard negatives, the model can more effectively integrate the\ncomplexity and challenge they introduce. This tailored approach to learning rate adjustment ensures\nthat the model remains sensitive to subtle semantic distinctions, optimizing its performance across\nvarying training dynamics. Thus, the strategic modulation of the learning rate emerges as an essential\ncomponent of the optimization framework, enabling the model to fully leverage the benefits of HNM.\nAdapting the learning rate in response to the presence of hard negatives provides a mechanism for\nfine-tuning the model’s sensitivity to challenging samples:\nηnew = η · γI[hnmix∈HNM(si)]\n(24)\nIn this expression, ηnew represents the adjusted learning rate, while η denotes the original learning\nrate prior to adjustment. The term γ is a decay factor, with a value less than 1, which modulates the\nlearning rate. The function I[·] is an indicator function that evaluates to 1 if the condition within\nthe brackets is true—specifically, if the synthetic hard negative hnmix is among the hard negatives\ngenerated for sentence si through HNM—and 0 otherwise.\nThis formulation encapsulates a dynamic adjustment mechanism, wherein the learning rate is selec-\ntively scaled down in the presence of synthetic hard negatives. This scaling is designed to temper\nthe learning step, acknowledging the heightened challenge posed by these negatives, and thereby\nensuring that the model’s updates are both deliberate and measured. The application of this strategy\nensures that the learning process remains robust and responsive to the intricacies introduced by HNM,\noptimizing the model’s trajectory towards achieving superior discriminative performance.\nA.17\nConclusion\nIn conclusion, the theoretical exploration of Hard Negative Mixing (HNM) within this discourse\nelucidates its profound impact on the enhancement of representation learning, particularly in the\ndomain of natural language processing. By rigorously defining hard negatives, devising a synthetic\ngeneration strategy, and optimizing the model through innovative loss functions, HNM has demon-\nstrated significant promise in refining the discriminability and semantic understanding of models. The\nstrategic incorporation of embedding space normalization, similarity gradient computation, and an\nadaptive learning rate adjustment strategy further optimizes the learning process, ensuring the model’s\nsensitivity to nuanced semantic differences. The analysis of embedding divergence and the adjustment\nof similarity distributions underscore the effectiveness of HNM in creating a more nuanced and robust\nrepresentational space. Ultimately, this comprehensive theoretical analysis highlights the paramount\nimportance of HNM in advancing the state of the art in representation learning. Through meticulous\nexamination and optimization, HNM emerges not merely as a technique but as a cornerstone in the\npursuit of more discriminative, nuanced, and contextually aware models, underscoring the indispens-\nability of theoretical analysis in pushing the boundaries of what is achievable in natural language\nprocessing and beyond.\n20",
    "pdf_filename": "HNCSE_Advancing_Sentence_Embeddings_via_Hybrid_Contrastive_Learning_with_Hard_Negatives.pdf"
}