{
    "title": "HNCSE: Advancing Sentence Embeddings via Hybrid",
    "abstract": "Unsupervisedsentencerepresentationlearningremainsacriticalchallengeinmod- ernnaturallanguageprocessing(NLP)research. Recently, contrastivelearning techniqueshaveachievedsignificantsuccessinaddressingthisissuebyeffectively capturing textual semantics. Many such approaches prioritize the optimization usingnegativesamples. Infieldssuchascomputervision,hardnegativesamples (samplesthatareclosetothedecisionboundaryandthusmoredifficulttodistin- guish)havebeenshowntoenhancerepresentationlearning. However,adapting hardnegativestocontrastivesentencelearningiscomplexduetotheintricatesyn- tacticandsemanticdetailsoftext. Toaddressthisproblem,weproposeHNCSE,a novelcontrastivelearningframeworkthatextendstheleadingSimCSEapproach. ThehallmarkofHNCSEisitsinnovativeuseofhardnegativesamplestoenhance the learning of both positive and negative samples, thereby achieving a deeper semanticunderstanding. Empiricaltestsonsemantictextualsimilarityandtransfer taskdatasetsvalidatethesuperiorityofHNCSE. 1 Introduction Sentence representation learning (SRL), or sentence embedding, is a crucial subfield of natural languageprocessing(NLP)thatinvolvesencodingsentencesintolow-dimensionalvectorstocapture theirsemanticcontent. Theessenceoflearningsentencerepresentationsistograspthefulllinguistic context,transcendingmerewordmeanings.Sentencerepresentationlearningcontributestofacilitating amyriadofapplications,includinginformationretrievalLiuetal.(2015),conversationalAIYouetal. (2021),andmachinetranslationsystemsZhangetal.(2021). ∗Correspondingauthor. Preprint.Underreview. 4202 voN 91 ]LC.sc[ 1v65121.1142:viXra",
    "body": "HNCSE: Advancing Sentence Embeddings via Hybrid\nContrastive Learning with Hard Negatives\nWenxiaoLiu ZihongYang\nCollegeofCyberspaceSecurity CollegeofCyberspaceSecurity\nJinanUniversity JinanUniversity\nlwx_mailbk@163.com\nChaozhuoLi\nSchoolofCyberspaceSecurity\nBeijingUniversityofPostsandTelecommunications\nZijinHong JianfengMa ZhiquanLiu\nBirminghamCollege CollegeofCyberspaceSecurity CollegeofCyberspaceSecurity\nJinanUniversity JinanUniversity JinanUniversity\nLitianZhang FeiranHuang∗\nBeihangUniversity CollegeofCyberspaceSecurity\nJinanUniversity\nAbstract\nUnsupervisedsentencerepresentationlearningremainsacriticalchallengeinmod-\nernnaturallanguageprocessing(NLP)research. Recently, contrastivelearning\ntechniqueshaveachievedsignificantsuccessinaddressingthisissuebyeffectively\ncapturing textual semantics. Many such approaches prioritize the optimization\nusingnegativesamples. Infieldssuchascomputervision,hardnegativesamples\n(samplesthatareclosetothedecisionboundaryandthusmoredifficulttodistin-\nguish)havebeenshowntoenhancerepresentationlearning. However,adapting\nhardnegativestocontrastivesentencelearningiscomplexduetotheintricatesyn-\ntacticandsemanticdetailsoftext. Toaddressthisproblem,weproposeHNCSE,a\nnovelcontrastivelearningframeworkthatextendstheleadingSimCSEapproach.\nThehallmarkofHNCSEisitsinnovativeuseofhardnegativesamplestoenhance\nthe learning of both positive and negative samples, thereby achieving a deeper\nsemanticunderstanding. Empiricaltestsonsemantictextualsimilarityandtransfer\ntaskdatasetsvalidatethesuperiorityofHNCSE.\n1 Introduction\nSentence representation learning (SRL), or sentence embedding, is a crucial subfield of natural\nlanguageprocessing(NLP)thatinvolvesencodingsentencesintolow-dimensionalvectorstocapture\ntheirsemanticcontent. Theessenceoflearningsentencerepresentationsistograspthefulllinguistic\ncontext,transcendingmerewordmeanings.Sentencerepresentationlearningcontributestofacilitating\namyriadofapplications,includinginformationretrievalLiuetal.(2015),conversationalAIYouetal.\n(2021),andmachinetranslationsystemsZhangetal.(2021).\n∗Correspondingauthor.\nPreprint.Underreview.\n4202\nvoN\n91\n]LC.sc[\n1v65121.1142:viXra\nConventionalendeavorsgenerallyfollowthesupervisedlearningparadigmJordanandRumelhart\n(2013). However, such supervised learning methods suffer from the substantial cost associated\nwithgatheringsufficienttrainingdata. Recently,unsupervisedsentencelearningaimstoenhance\nefficiencyandcost-effectivenessintrainingwithoutsacrificingtheaccuracyorefficacyoftheresulting\nsentencerepresentations. Unsupervisedsentencerepresentationlearningreferstotrainingmodels\nto understand the semantic content of sentences without explicit human-labeled guidance, often\nusingmethodslikeauto-encoding,predictingsurroundingcontext,orself-supervisiontechniques\nsuchascontrastivelearningGaoetal.(2021). Atpresent,themainstreammethodsofunsupervised\nsentencerepresentationlearningfollowtheideaofcontrastivelearning,thatis,totrainthesentence\nrepresentationmodelbymaximizingthesimilaritybetweenpositivesamplepairsandminimizingthe\nsimilaritybetweennegativesamplepairs.\nMissing？\nTwo dogs are running. Que E Encoder\nry\nHard Negatives\nOver-Hard Negatives\nTwo dogs are walking. E Que Over-Easy Negatives\nry Positive\nA man surfing on the sea.\nMissing？\nFigure1: Anexampleofaquerywithitspositive,over-easy,over-hardandmissinghardnegative\nsamples. Thefigureshowsthesituationthattwotoosimilarsentencesmaycausethelossofnormal\nhardnegativesamplesafterpassingthroughtheencoder.\nThe core of contrastive learning-based SRL lies in how to construct effective positive samples\nand negative samples. Positive samples refer to the sentence pairs that are semantically similar\nor identical, while negative samples denote the sentence pairs that are semantically unrelated or\ndistinct. Underunsupervisedlearningconditions,labeledinformationisnotavailable. Therefore,\nwe need to design some heuristic methods to generate correct positive and negative samples. A\npopular strategy is to use the same sentence to generate positive samples through different data\nenhancement(suchasrandomreplacement,deletion,insertionofwords,etc. Chuangetal.(2022)),\nand randomly select other sentences as the negative samples. However, such strategies are not a\npanaceatosolvetheunsupervisedSRLtaskduetothefollowingreasons. Firstly,thepositivesamples\nandnegativesamplesaregeneratedindependently,withoutconsideringthesemanticdistancebetween\nthem. Suchaweakconstraintmayblurtheboundarybetweenpositiveandnegativesamples,leading\ntoadilemmasuchasthesimilaritybetweenpositivesamplesislowerthantheonebetweennegative\nsamplesYangetal.(2022). Secondly,modelperformanceisseriouslyaffectedbythenumberand\ndistributionofpositiveandnegativesamples. Ifthedistributionsofpositive/negativesamplesare\nuneven, SRL models tend to learn biased or unstable sentence representations. Meanwhile, the\nmainstreamunsupervisedSRLmethods(e.g.,SimCSEGaoetal.(2021))adoptthemethodofin-\nbatchnegativesampling. Althoughin-batchnegativesamplingenjoystheadvantageofcomputing\nresourceefficiency,itstillsuffersfromseverelimitationssuchasrelyingonbatchsize,alimited\nnumberofnegativesamples,anddiversityLietal.(2021).\nThispapertargetstheacquisitionofsuperior,correlatedpositive-negativesamplepairstoenhance\nSRL.Amajorhurdleismanagingconfusing,over-hard,andover-easynegativesampleseffectively.\nConfusingsamples,oftenabyproductofdataaugmentationlikeinSimCSE,canleadtodivergent\nembeddingsforpositivepairs,resultinginsubparlearningsamples. Over-hardnegatives,duetotheir\nhighsimilaritytothequery,maycauselearningdistortion. Conversely,over-easynegatives,marked\nbytheirextremedissimilarity,canleadtoanunchallenginglearningprocess,inhibitingtheacquisition\nofmeaningful representations. Figure 1illustratesan exampleof adenseembedded distribution\nandtheissueofmissinghardnegatives. Addressingtheseissueshighlightsthenecessityforcareful\nconstruction of positive/negative samples, particularly hard negatives that balance difficulty and\nsimplicity. Thisbalanceiscrucialforawell-tunedandeffectivelearningpath. However,identifying\nthesechallengingnegativesremainsasignificantchallengeduetocomputationalandstoragedemands.\nInthispaper,weintroduceaninnovativeunsupervisedcomparativelearningframeworkthatleverages\nhardnegativesamplemixingtomitigatetheadverseeffectsofconfusingsamplesandenrichthe\nreliability of hard negatives within the same batch. Contrasting prior research that solely targets\nnegativesamples, ourapproachunderscoresthesignificanceofpositivesamplesinunsupervised\n2\nSRL,fosteringasynergisticlearningprocessbetweenpositiveandnegativesamples. Ourframework\nnot only harnesses hard negative information to enhance the quality of positive samples but also\ngeneratesadditionalhardnegativesthroughthemixingofexistingsamples. Furthermore,itefficiently\nexpandsthenegativesamplepool,facilitatingconvergenceinalignmentcharacteristics. Extensive\nevaluationsacrossvarioustaskshaveconfirmedthesuperiorperformanceofourproposalagainst\nSOTAbenchmarks. Insummary,ourkeycontributionsareasfollows:\n• WeproposetheHNCSEframework,whichisdividedintoHNCSE-PMandHNCSE-HNM,\nbothextendedbasedonSimCSE.HNCSE-PMconstructspositivesamplesclosertothe\nquerythroughthehardestnegativesample. HNCSE-HNMusesmixuponexistinghard\nnegativesamplestoobtainhigherqualityhardnegativesamples.\n• ThetheoreticalanalysisofHardNegativeMixing(HNM)hasdeeplyelucidateditsprofound\nimpactontheenhancementofsentencerepresentationlearning.\n• Throughextensiveexperiments,HNCSEhasachievedpromisingimprovementsoverSim-\nCSEintermsofsemantictextualsimilarity(STS)andtransfertasks(TR).Additionally,\nHNCSEdemonstratesclearadvantagesovercurrentpopularlargelanguagemodels(LLMs)\ninSTStasks.\n2 RelatedWork\nInrecenttimes,theacademiccommunityincreasinglyfocusesonunsupervisedsentencerepresenta-\ntionlearning. Traditionalapproaches,suchasthosedemonstratedbyAroraetal.Aroraetal.(2017)\nandEthayarajhEthayarajh(2019), generatesentenceembeddingsthroughaweightedaverageof\nwordembeddings. Kirosetal.Kirosetal.(2015)introducetheSkipThoughtmodel,whichappliesthe\nskip-grammodelatthesentencelevel,usingtheencodedsentencetopredictitsadjacentsentences.\nManyresearchers,likeQiaoetalQiaoetal.(2019),areadoptingoutputsfrompre-trainedlanguage\nmodelslikeBERTforsentenceembedding. However,EthayarajhEthayarajh(2019)observesthat\ndirectlyusingBERTforthistaskyieldssuboptimalresults. LietalLietal.(2022)notethatBERTin-\nducesanisotropyinthesentenceembeddingspace,adverselyimpactingtheperformanceonSemantic\nTextSimilarity(STS)tasks.\nToaddressthis,LietalLietal.(2022)introduceBERT-flow,amethodthatutilizesaflow-based\ntechnique,asdescribedbyDinh,Krueger,andBengioDinhetal.(2014). Thistechniquetransforms\ntheanisotropicsentenceembeddingdistributionintoamoreuniform,isotropicGaussiandistribution,\naimingtoimproveperformanceonSTStasks. Concurrently,SuetalSuetal.(2023)presentthe\nBERT-whiteningapproach. Thismethodleveragestraditionalwhiteningtechniquestoproducea\nmoreconsistentsentenceembeddingdistribution. ItnotonlymatchestheperformanceofBERT-flow\nbutalsoreducesthedimensionalityofthesentenceembeddings,enhancingcomputationalefficiency.\n3 Methodology\nTheHNCSEmodel,asoutlinedintheliterature,pioneersanoveltrainingapproachthatleverageshard\nnegativemixingtorefinesentenceembeddings. Itrectifiesmisclassifiedpositivesbyincorporating\nhardnegativetraits,therebyboostingthemodel’sdiscriminativecapacity. HNCSEthenbroadensthe\nchallengesetbyaugmentingexistinghardnegatives,fosteringamorerobustlearningcontextthat\nadvancessentencerepresentationlearning. Followingthis,wewillelucidatethefoundationofour\nbasemodel,SimCSEGaoetal.(2021).\nThecoreideaofunsupervisedSimCSEistouseacontrastivelearningobjectivetotrainsentence\nembeddingswithoutanylabeleddata. Thecontrastivelearningobjectiveisbasedontheprincipleof\nmaximizingtheagreementbetweentwodifferentviewsofthesamesentence,whileminimizingthe\nagreementbetweendifferentsentences. Thetwoviewsareobtainedbyapplyingdropouttotheinput\nsentenceandencodingitwithapre-trainedtransformermodel. Thecontrastivelearningobjective\ncanbeformulatedasfollows:\nN\nL(θ)=−\n1 (cid:88)\nlog\nexp(sim(f θ(s i),f θ(s˜ i))/τ)\n(1)\nN (cid:80)N exp(sim(f (s ),f (s˜ ))/τ)\ni=1 j=1 θ i θ j\n3\n𝑑\nSentence QUE 2\n1 RY\nE Old Positive Mixup\n𝑑\n3\nSentence\n2 QUE\nRY\nHardest Negative Mixup\nEncoder\n𝑑\n1\n𝑑 ≈𝑑 >𝑑\n1 2 3\nNew Positive\nFigure2: SchematicdiagramofPositiveMixing.\nwheref isthesentenceencoderparameterizedbyθ,s ands˜ aretwoviewsofthei-thsentence,\nθ i i\nsim is a cosine similarity function, τ is a temperature parameter, and N is the batch size. This\nobjectiveencouragestheembeddingsofthesamesentencetobeclosetoeachother,whilepushing\nawaytheembeddingsofdifferentsentences. Thisisasimplebuteffectivemethodtolearnsentence\nembeddingsthatcancapturesemanticsimilarity. Nextwewilldetaileachofthetwowaystoimprove\ntheunsupervisedSimCSE.\n3.1 PositiveMixing\nIntext-basedcontrastivelearning,\"hardnegatives\"areexemplarsthatthemodelnearlymisclassifies\naspositiveduringtraining. Thesenegativesareerroneouslydeemedhighlysimilartopositives.\nIntegratinghardnegativeswithpositivesoffersuniqueadvantages. Itmirrorsreal-worldscenarios\nwherecategorydistinctionscanbeambiguous,trainingthemodeltonavigatesuchcomplexities. This\nrefinesthemodel’sclassboundaries,enhancingitsabilitytodiscernbetweensubtlydistinctinstances.\nItalsomitigatesoverfittingbydivertingthemodel’sfocusfromonlytheclearandeasyexamples.\nSimCSE’spositivesamplinginvolvesfeedingthesamesentenceintothemodeltwicetoyieldtwo\ndistincteigenvectorsaspositives,facilitatedbydropoutlayers. Despitedropout,semanticallydistinct\nsentencesmaystillproducesimilarpositivesamplesduetorandommasking.\nFor example, as shown in Figure 2, given two input sentences \"Two dogs are running” standing\nfor Sentence and \"Two dogs are walking” standing for Sentence , after dropout in SimCSE’s\n1 2\nunsupervisedmode,twopairsofqueryandpositivecanbeachieved,whicharerespectivelyshownas\n(Q ,P )and(Q ,P ). ThesimilaritybetweenQ andP mightbeevenlargerthattheonebetween\n1 1 2 2 1 2\nQ andP ,whichisobviouslyincorrect.\n1 1\nInordertoshortenthedistancebetweenthequeryandthepositivewhileincreasingthedistance\nbetweenthepositiveandthenegative,wecomputethesimilaritybetweenthequeryandthepositive\naswellasthehardestnegative,denotedasd andd ,respectively:\n1 2\n\nw 1s+\n1\n+w 2s− 1, 0< d1 d− 1d2 ≤0.1\ns+ = s+, d1−d2 >0.1 (2)\n2 d1\n1 d− 1d2s+ 1 + d1 d− 2d2s− 1, d\n1d <1\nd 2\nIn Equation (2), s+ denotes the positive example, while s− stands for the most similar negative\n1 1\ninstance. s+referstothegeneratedpositiveinstanceafterthemixupofbothpositiveandnegative\n2\ninstances. d signifiesthesimilaritybetweenthequeryandthepositiveinstance,andd referstothe\n1 2\nsimilaritybetweenthequeryandthehardestnegative. w andw respectivelyrepresenttheweights\n1 2\nofs+ands−.\n1 1\n• ContrastiveLoss.InfoNCEisadoptedasthecontrastiveloss,whichrepresentsthemodelcapability\ntoestimatethemutualinformation. OptimizingInfoNCElossmaximizesthemutualinformation\nbetweenpositivesamplesandminimizesthemutualinformationbetweennegativesamplesas:\n4\n1\n(cid:88)B esim(s(i),s(i)′)/τ\nL =− log (3)\ni B (cid:80)2B esim(s(i),s(j))/τ\ni=1 j=1\nwheres(i)ands(i)′areembeddingsobtainedafterthesamesentenceisinputtotheencoderthrough\ndifferentdropoutmasks,whicharepositivesamplepairs;sim(·,·)isthecosinesimilarityfunction;τ\nisatemperaturecoefficientthatregulatessimilarityscaling;Bisthenumberofsentencesinabatch,\nand2Bisthenumberofvectorsinabatch.\n3.2 HardNegativeMixing\nSamples From Previous Batch\n① Sample …\n′  1  2  \nEncoder  1  1 ② Computing the with\nand\n…   …      Dropout  2′ ……  2            \n…\n′  1  2  \n\nIn-batch Samples ③  S   ort   b  y  t  h  e  :    1   a  nd2 Get Top   K  \n…\n′\n1 Encoder  1  1  1  1  2  \nand\n…   …    2 Dropout  2′ ……  2 …  2 ④ Choose Pairs Mixup\n\n′\n\nNew Hard Negative Positive Instance Negative Instance\nFigure3: SchematicdiagramofHardNegativeMixing.\nConsiderascenarioinwhichtheembeddingofagivenquery,denotedasq,andtheembeddingofa\ncorrespondingkeyinthematchedpair,denotedask,iscontrastedwitheveryfeatureninthebankof\nnegatives(N).\nThepopularlossfunctionforcontrastivelearningChenetal.(2020);Heetal.(2020)is:\nexp(qTk/τ)\nLq,k,N =−log (4)\nexp(qTk/τ)+(cid:80) n∈Nexp(qTn/τ)\nwhere τ assumes the role of a temperature parameter, and it is worth noting that all embeddings\nundergoℓ -normalization.\n2\nThe query and key are two distinct augmentations of an identical sentence. The negative bank,\ndenotedasN,comprisesinstancesthatactasnegativesforeachpositivepair. Specifically,itcanbe\ndefinedasencompassingtheremainingtextswithinthepresentminibatchChenetal.(2020).\nThelogarithmiclikelihoodfunctionformulatedinEq(4)operateswithintheboundsofaprobability\ndistributionconstructedviatheapplicationofthesoftmaxfunctiontoeachinput/queryq. Ifwelet\np denotetheprobabilityofmatchbetweenthequeryandthefeaturef ∈ F = N ∪k,thenthe\nzi i\nderivativeofthelossconcerningthequeryq,withrespecttothegradient,canbeexpressed:\n∂Lq,k,N 1 (cid:16) (cid:88) (cid:17)\n=− (1−p )·k− n∈Np ·n\n∂q τ k n\n(5)\nexp(qTf /τ)\nwherep = i .\nfi (cid:80) exp(qTf /τ)\nj∈Z j\np andp symbolizethelikelihoodofamatchpertainingtothekeyandthenegativefeature,denoted\nk n\nasf =kandf =nrespectively. Itisdiscerniblethattheimpactsofthepositiveandnegativelogits\ni i\nuponthelossfunctionmirrorthoseencounteredina(K+1)-waycross-entropyclassificationloss.\n5\nTofurtherexpandourunderstanding,let’sconsiderthedimensionaldynamicsoftheembeddings.\nAssumethateachembeddingvectorv(whetherq,k,orn)isofdimensiond. Thenthenormalization\nprocesscanbedescribedas:\nv\nvˆ= (6)\n|v|\n2\nwhere |v| denotes the L norm of vector v. This normalization ensures that the focus is on the\n2 2\ndirectionofthevectorsratherthantheirmagnitude.\n3.2.1 HardNegativeMixingforContrastiveLearning\nIncontrastivelearning,particularlywithinunsupervisedlearning,theroleofhardnegativesamplesis\npivotalforeffectivemodeltraining. Thestandardpracticeofusingbatchnegativesoftenfallsshort,\nastheymaynotofferasufficientchallengeorprovidethenecessarycontrastwithpositiveexamples,\nasshownbyrecentstudiesKalantidisetal.(2020). Toaddressthis,HardNegativeMininghasbeen\nintroducedtoenhancethelearningprocess.\nHardNegativeMininginvolvesdeliberatelyselectingnegativesamplesthataremorechallengingand\ncloselyresemblepositivesamplesinthefeaturespace. Thismethodismoreeffectiveinteachingthe\nmodeltodistinguishbetweensimilar-lookingbutdistinctcategories,therebyimprovingthemodel’s\nrobustnessanddiscriminativecapabilities,asevidencedbyrecentadvancements.\nInresponsetotheneedformoreeffectivehardnegatives,wehavedevelopedanapproachinspired\nbyKalantidisetal.(2020)tofurtherenhancethemodel’sgeneralizationcapabilities. Ourstrategy\ninnovativelyappliesthemixuptechniquetogeneratemorerelevantandchallenginghardnegative\nsamples,thusrefiningthelearningprocess.\nAs shown in Figure 3, this approach begins by identifying a set of top k vectors, denoted as\nG={G ,G ,...,G },whichcloselyresembleagiveninputqueryx fromapreviousminibatch.\n1 2 m 1\nThesevectorsarethenutilizedasthefoundationforcraftingnewhardnegativesamples. Weemploya\nmethodoflinearcombinationtoproducemnovelhardnegatives.Thesearesubsequentlyincorporated\nintoourexistingnegativesamplebank(denotedasN inequation4). Theprocessofgeneratingeach\nnewhardnegativeinstance,u ,ismethodicallyformulatedasfollows:\no\nu\nu = o , whereu =αG +(1−α)G (7)\no ||u || o i j\no 2\nHere,u (whereo∈{1,...,m})representseachnewhardnegativeinstance. Theindices(i,j)are\no\nrandomlyselectedfromthesetoftopkvectors,andthecoefficientαhelpsbalancethecontribution\nofeachvectorinthemixup.\nExpanding further, we introduce an adaptive selection process for α, which varies based on the\nsimilaritybetweenthevectorsG andG . Thisapproachensuresthattheresultinghardnegativeis\ni j\nneithertoosimilarnortoodissimilartothequery,maintaininganoptimallevelofdifficulty. The\nadaptiveselectionisdefinedas:\nα=f(sim(G ,G )) (8)\ni j\nwheref isafunctionthatmapsthesimilarityscoretoanappropriatevalueforα. Thesimilarity\nfunctionsim(·,·)couldbeacosinesimilarity.\nThissectionoutlinestheHardNegativeMixingstrategytooptimizesentencerepresentationlearning.\nBycraftingchallengingnegativesamples,thisapproachenhancesthemodel’ssemanticrelationship\nrecognition. The efficacy of this method is demonstrated through experiments detailed in the\nsubsequentchapter,showcasingperformancegainsandcomparisonswithothermethods.\nThe Appendix A provides a comprehensive mathematical analysis of the Hard Negative Mixing\nstrategy. Itdefineshardnegatives,explainsthegenerationofchallengingnegativesthroughlinear\ncombinations,anddiscussestheimpactonthemodel’soptimizationandlossfunctionadjustments.\nTheanalysiselucidateshowthismethodenhancesmodelperformancebyincreasingsampledistinc-\ntivenessandimprovingdiscriminativeability. Italsoaddressestheroleofregularizationandlearning\nrate adjustments in stabilizing and optimizing the learning process. This theoretical framework\nsupportstheexperimentaloutcomesandoffersguidanceforapplyingthestrategyeffectively.\n6\nModel STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.\nUnsupervisedModels(Base)\nGloVe(avg.) 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32\nBERT(first-lastavg.) 39.70 59.38 49.67 66.03 66.19 53.87 62.06 56.70\nBERT-flow 58.40 67.10 60.85 75.16 71.22 68.66 64.47 66.55\nBERT-whitening 57.83 66.90 60.90 75.08 71.31 68.24 63.73 66.28\nIS-BERT 56.77 69.24 61.21 75.23 70.16 69.21 64.25 66.58\nCT-BERT 61.63 76.80 68.47 77.50 76.48 74.31 69.19 72.05\nRoBERTa(first-lastavg.) 40.88 58.74 49.07 65.63 61.48 58.55 61.63 56.57\nRoBERTa-whitening 46.99 63.24 57.23 71.36 68.99 61.36 62.91 61.73\nDeCLUTR-RoBERT 52.41 75.19 65.52 77.12 78.63 72.41 68.62 69.99\nSIMCSE 68.40 82.41 74.38 80.91 78.56 76.85 72.23 76.25\nSIMCSE 70.82 82.24 73.25 81.38 77.06 77.24 71.16 76.16\n(reproduce)\nLLaMA2-7B 50.66 73.32 62.76 67.00 70.98 63.28 67.40 65.06\nLLaMA2-7B 58.81 77.01 66.34 73.22 73.56 71.66 69.64 70.03\n(PromptEOL)\nLLaMA2-7B 67.45 83.89 74.14 79.47 80.76 78.95 73.33 76.86\n(Pretended_CoT)\nLLaMA2-7B 65.60 82.82 74.48 80.75 80.13 80.34 75.89 77.14\n(Konwledge_Enhancement)\nHNCSE-PM(ours) 71.02 83.92 75.52 82.93 81.03 81.45 72.76 78.38\nHNCSE-HNM(ours) 69.76 83.97 75.52 83.21 81.63 81.85 72.87 78.27\nUnsupervisedModels(Large)\nSIMCSE 70.88 84.16 76.43 84.50 79.76 79.26 73.88 78.11\nSIMCSE 71.02 83.52 76.06 83.83 78.95 79.26 72.24 77.84\n(reproduce)\nHNCSE-PM(ours) 72.94 84.67 77.24 83.97 79.53 80.78 74.79 79.13\nHNCSE-HNM(ours) 72.75 84.54 77.36 84.58 79.92 80.60 74.64 79.20\nTable1: Spearman’scorrelationscoresacrosssevenSTSbenchmarksforvariousmodels. HNCSE-\nPMreferstoPositiveMixingmethod. HNCSE-HNMreferstoHardNegativeMixingmethod.\n4 Experiments\n4.1 ExperimentalSettings\nWe executed experiments across seven established semantic text similarity (STS) benchmarks,\nspanningSTS2012–2016asdelineatedbyAgirreetal. from2012to2016,inconjunctionwiththe\nSTSBenchmarkCeretal.(2017)andSICKRelatednessWijnholdsandMoortgat(2021). Ourmodel’s\nperformancewasassessedusingtheSentEvaltoolkit,andtheSpearman’scorrelationwaschosenas\ntheevaluationmetric. Ourmethodologyinitiatedbyleveragingpre-existingBERTcheckpoints. The\n[CLS]tokenembedding,extractedfromthemodel’soutput,servedasthesentencerepresentation.\nBeyondsemanticsimilarityevaluations,ourmodelwassubjectedtosevendistincttransferlearning\ntaskstogaugeitscapacityforgeneralization.\n4.2 ImplementationDetails\nIn the methodology outlined by Gao, Yao, and Chen Gao et al. (2021), we adopted an identical\ntrainingdatasetandprotocol. Thisdatasetencompassesamillionsentences,sourcedfromWikipedia.\nUtilizingaBERTmodelthat’sbeenfinelyoptimizedDevlinetal.(2018),wederiveanembeddingfor\neachofthesesentences. Subsequenttothis,weemploydistinctdropoutmaskstoproduceaugmented\nvariantsoftheaforementionedsentenceembeddings. Allmodelsaretrainedoverasingleepoch,\nwithabatchsizesetat64. OurcomputationalimplementationisfoundedonPythonversion3.8.13,\nleveragingthecapabilitiesofPytorchversion1.12.1. Allexperimentaltaskswereexecutedonan\nNVIDIAGeForceRTX3090GPUequippedwith24Gmemory.\n4.3 MainResults\n4.3.1 STSTask\nInthestudybyZhangetal.(2022),webenchmarkedthenewlyintroducedHNCSEagainstarange\nof conventional unsupervised approaches and the prevailing state-of-the-art contrastive learning\ntechniqueforthetextsemanticsimilarity(STS)task. Thiscomparisonencompassesmethodssuchas\naverageGloVeembeddingsPenningtonetal.(2014),meanembeddingsfromBERTorRoBERTa,\n7\nModel MR CR SUBJ MPQA SST TREC MRPC Avg.\nUnsupervisedModels(Base)\nGloVe(avg.) 77.25 78.30 91.17 87.85 80.18 83.00 72.87 81.52\nBERT 78.66 86.25 94.37 88.66 84.40 92.80 69.54 84.94\nIS-BERT 81.09 87.18 94.96 88.75 85.96 88.64 74.24 85.83\nSimCSE-RoBERTa 81.04 87.74 93.28 86.94 86.60 84.60 73.68 84.84\nHNCSE-PM(ours) 81.94 86.99 95.20 89.77 86.81 85.31 75.49 85.93\nHNCSE-HNM(ours) 81.64 86.84 95.11 89.66 86.81 83.90 75.91 85.70\nUnsupervisedModels(Large)\nSimCSE-RoBERTa 82.74 87.87 93.66 88.22 88.58 92.00 69.68 86.11\nHNCSE-PM(ours) 84.91 89.32 95.00 90.02 89.68 86.85 75.49 87.32\nHNCSE-HNM(ours) 85.89 90.57 96.06 89.91 89.91 85.91 76.47 87.82\nTable2: ResultsontheTransferTaskdatasets.\nBERT-flow,BERT-whitening,ISBERTZhangetal.(2020),DeCLUTRGiorgietal.(2020),CT-BERT\nCarlssonetal.(2020),andSimCSEGaoetal.(2021).\nAsdelineatedinTable1, thepeakperformancesachievedbyourtwovariantmodelsofHNCSE\nare78.38%and78.27%respectively. ThisclearlysurpassestheunsupervisedSimCSEemploying\naBERT-basearchitecture. Specifically,HNCSEexhibitssuperiorperformancetoSimCSEacross\nSTS2012,STS2013,STS2014,STS2015,STS2016,SICK-RandSTS-Bbenchmarks. Moreover,the\nmorerobustiterationsofourHNCSE,leveragingtheBERT-largearchitecture,consistentlyoutshine\nthecorrespondinglargemodelofSimCSEonthemajorityofSTStasks.\n4.3.2 LargelanguagemodelsareusedfortheSTSTask\nThegrowinginterestinopen-sourcelargelanguagemodels(LLMs)likeLLaMAhashighlighted\ntheiruseinunsupervisedsentencesimilaritytasks(STS).However,studiesLiandLi(2023);Zhang\netal.(2024)haveshownthatthesemodels’performanceinSTShasnotbeenuptopar. Thelatest\nresultsforLLaMA2-7Banditsthreepromptengineeringstrategies—PromptEOL,PretendedChain\nofThought(CoT),andKnowledgeEnhancement—havedemonstratedashortfallwhencomparedto\nthemethodsintroducedbyHNCSE.\nThe architectures and training objectives of LLaMA2-7B and HNCSE have notably diverged.\nLLaMA2-7B, an autoregressive model, has been optimized for text generation, while HNCSE\nhasbeentrainedusingcontrastivelearningforsentence-levelrepresentation. Thisdivergencemay\nhavereducedtheefficacyofLLaMA2-7BinsentencesimilaritytaskscomparedtoHNCSE.Addi-\ntionally,promptengineeringforLLaMA2-7Bhasfacedlimitations: (1)itheavilyreliesoncarefully\ncraftedtemplatesrequiringextensiveoptimizationacrossmodelsandtasks,increasingcomplexityin\nresearchandapplication;(2)thesemethods,designedprimarilyforgenerativemodels,showlimited\neffectivenessfordiscriminativemodels,limitingtheirapplicability;(3)aspromptsbecomelonger\nandmorecomplex,thecomputationalresourcedemandescalates,especiallywithlarge-scaledatasets,\nhighlightingresourceconsumption. Incontrast,HNCSE’scontrastivelearninghaseliminatedthe\nneedforcomplexpromptdesigns,ensuringstableapplicationacrossvariousmodelsandtasks,and\nhasshownimprovedefficiencyinresourceconsumption.\n4.3.3 TransferTask\nIn the comprehensive evaluation of HNCSE’s performance using the seven transfer tasks from\nSentEval Conneau and Kiela (2018), the results delineated in Table 2 are particularly revealing.\nNotably,boththeBaseandLargeconfigurationsofHNCSEsurpassSimCSEinsixoutoftheseven\nbenchmarkdatasetswithintheTRtask. Importantly,aremarkableenhancementinperformanceis\ndistinctlyobservedfortheMRPCdataset.\n4.3.4 AblationStudy\nToassessmodelcomponents’impact,werunablationstudiesusingtwomodelvariants,HNCSE-\nPM andHNCSE-HNM ,withidenticalsetupsandhyperparametersasthemainexperiment.\nsingle single\n8\nModel STS(Avg) TR(Avg)\nHNCSE-PM-BERT 78.38 85.93\nbase\nHNCSE-HNM-BERT 78.27 85.70\nbase\nHNCSE-PM -BERT 76.43 85.06\nsingle base\nHNCSE-HNM -BERT 76.64 84.89\nsingle base\nTable3: ResultsoftheAblationStudy.\n78 78\n77\n77\n76\n75 76\n74 HNCSE-PM 75 HNCSE-PM\nHNCSE-HNM HNCSE-HNM\n73\n16 32 64 128 256 16 32 64 128\nBatchSize MaxSeqLength\n(a) Impactofbatchsize (b) Impactofmaxsequencelength\nFigure4: TheinfluenceofdifferenthyperparametersonHNCSE.\nHNCSE-PM ThismodelisavariantofHNCSE-PM,whichmeansthatintheEquation2of\nsingle\nPositiveMixing,weonlyoptimizethecasewhered isgreaterthand ,andthecasewhered isless\n1 2 1\nthand isnotconsideredforoptimization.\n2\nHNCSE-HNM This model is a variant of HNCSE-HNM, which refers to the operation of\nsingle\ndirectlyremovingMixupinthePositiveMixingmethod,thatis,directlyreplacingu withG in\no i\nEquation7.\nAblationresultsareshowninTable3. Theexperimentalresultsdemonstratethattheoutcomesofthe\ntwoHNCSE methodsare,onaverage,lowerthanthetwoHNCSEmethodsonboththeSTS\nsingle\ntaskandtheTRtask. Therefore,thefollowingconclusionscanbedrawn:\n(1)Itisessentialtooptimizethepositivesamplesbyleveragingthehardestnegativesamples,which\nhelpstopreventthesesamplesfrombecomingover-hardnegativesamples. (2)Theapplicationof\npairwiseMixuponhardnegativesamplesisnecessary,asitcancapturebeneficialaspectsbetween\ndifferent hard negative samples for the positive samples, thereby constructing more challenging\nnegativesamples.\n4.3.5 AnalysisofBatchSizeandMaxSequenceLength\nInourHNCSEmodelanalysis,we’veassessedtheinfluenceofbatchsizeandmaxsequencelength\nonmodelperformance. Figure4(a)demonstratesthatoptimalperformanceforbothHNCSE-PMand\nHNCSE-HNMhasbeenachievedwithabatchsizeof64,regardlessofthenumberofhardnegatives.\nTheplateauinperformancewithlargerbatchsizessuggeststhatbeyondacertainpoint,increased\nsizenolongerprovidesadditionalvaluablelearninginformation,underscoringtheimportanceofhard\nnegativesinmini-batches. Figure4(b)showsthehighestmodelvalueatamaximumsequencelength\nof32. Yet,modelperformancedoesnotimprovewithincreasedsequencelength. Thismaybedueto\nlongersequencesintroducinggreaterambiguitiesorcomplexitiesinsemanticrelationships,making\nitmorechallengingforthemodeltodisambiguateandaccuratelycapturenuances,thusreducing\nperformance.\n5 ConclusionandLimitation\nInthiswork,weintroduceanovelframework,HNCSE,specificallydesignedtoaddressthemulti-\nfacetedchallengesposedbytheincorporationofhardnegativesamplesinsentencerepresentation\n9\nnamraepS,gvA\nlearning. Unlikeconventionalapproaches,HNCSEinnovativelyintegratesidentifiedhardnegative\nsamplestooptimizepositivesamplesandconstructevenhardernegativesamples,therebyextending\nthewell-establishedSimCSEmethodology. Thisapproachfacilitatesadeeperandmorenuanced\nunderstandingofnegativesamples. EmpiricalevaluationsofHNCSEonsemantictextualsimilarity\ndatasetsandtransfertaskdatasetsdemonstrateitssuperiority,indicatingthatunsupervisedlearning\nofsentencerepresentationscanmakepromisingadvancementstoexistingresearch.\nDespiteitsstrongperformance,wehaveidentifiedcertainlimitationsassociatedwithHNCSE.Firstly,\nHNCSEreliesonlargeamountsofunannotatedtextdata. Ifthisdatacontainssubstantialnoiseor\nirrelevant information, it may adversely affect the quality of the model’s embeddings. Secondly,\nHNCSE lacks explicit contextual information during training, potentially impeding its ability to\ncapturelong-distancedependencies. Theseissuespresentchallengesforfutureresearch.\nReferences\nXiaodongLiu,JianfengGao,XiaodongHe,LiDeng,KevinDuh,andYe-YiWang. Representation\nlearningusingmulti-taskdeepneuralnetworksforsemanticclassificationandinformationretrieval.\n2015.\nChenyuYou,NuoChen,andYuexianZou. Self-supervisedcontrastivecross-modalityrepresentation\nlearningforspokenquestionanswering. arXivpreprintarXiv:2109.03381,2021.\nYan Zhang, Ruidan He, Zuozhu Liu, Lidong Bing, and Haizhou Li. Bootstrapped unsupervised\nsentencerepresentationlearning. InProceedingsofthe59thAnnualMeetingoftheAssociation\nforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguage\nProcessing(Volume1: LongPapers),pages5168–5180,2021.\nMichaelIJordanandDavidERumelhart. Forwardmodels: Supervisedlearningwithadistalteacher.\nInBackpropagation,pages189–236.PsychologyPress,2013.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\nembeddings. arXivpreprintarXiv:2104.08821,2021.\nYung-SungChuang,RumenDangovski,HongyinLuo,YangZhang,ShiyuChang,MarinSoljacˇic´,\nShang-WenLi,Wen-tauYih,YoonKim,andJamesGlass. Diffcse: Difference-basedcontrastive\nlearningforsentenceembeddings. arXivpreprintarXiv:2204.10298,2022.\nJinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul\nChilimbi,andJunzhouHuang. Vision-languagepre-trainingwithtriplecontrastivelearning. In\nProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages\n15671–15680,2022.\nYangguangLi,FengLiang,LichenZhao,YufengCui,WanliOuyang,JingShao,FengweiYu,and\nJunjieYan.Supervisionexistseverywhere:Adataefficientcontrastivelanguage-imagepre-training\nparadigm. arXivpreprintarXiv:2110.05208,2021.\nSanjeevArora,YingyuLiang,andTengyuMa. Asimplebuttough-to-beatbaselineforsentence\nembeddings. InInternationalconferenceonlearningrepresentations,2017.\nKawinEthayarajh. Howcontextualarecontextualizedwordrepresentations?comparingthegeometry\nofbert,elmo,andgpt-2embeddings. arXivpreprintarXiv:1909.00512,2019.\nRyanKiros,YukunZhu,RussRSalakhutdinov,RichardZemel,RaquelUrtasun,AntonioTorralba,\nandSanjaFidler. Skip-thoughtvectors. Advancesinneuralinformationprocessingsystems,28,\n2015.\nYifanQiao,ChenyanXiong,ZhenghaoLiu,andZhiyuanLiu. Understandingthebehaviorsofbertin\nranking. arXivpreprintarXiv:1904.07531,2019.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence\nembeddingsfrompre-trainedlanguagemodels. arXivpreprintarXiv:2011.05864,2022.\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components\nestimation. arXivpreprintarXiv:1410.8516,2014.\n10\nJianlinSu,JiarunCao,WeijieLiu,andYangyiwenOu. Whiteningsentencerepresentationsforbetter\nsemanticsandfasterretrieval. arXivpreprintarXiv:2103.15316,2023.\nTingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor\ncontrastivelearningofvisualrepresentations. InInternationalconferenceonmachinelearning,\npages1597–1607.PMLR,2020.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on\ncomputervisionandpatternrecognition,pages9729–9738,2020.\nYannisKalantidis,MertBulentSariyildiz,NoePion,PhilippeWeinzaepfel,andDianeLarlus. Hard\nnegativemixingforcontrastivelearning. AdvancesinNeuralInformationProcessingSystems,33:\n21798–21809,2020.\nDanielCer,MonaDiab,EnekoAgirre,InigoLopez-Gazpio,andLuciaSpecia. Semeval-2017task\n1: Semantictextualsimilarity-multilingualandcross-lingualfocusedevaluation. arXivpreprint\narXiv:1708.00055,2017.\nGijsWijnholdsandMichaelMoortgat. Sicknl: adatasetfordutchnaturallanguageinference. arXiv\npreprintarXiv:2101.05716,2021.\nJacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeep\nbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.\nYanzhaoZhang,RichongZhang,SamuelMensah,XudongLiu,andYongyiMao. Unsupervised\nsentencerepresentationviacontrastivelearningwithmixingnegatives. InProceedingsoftheAAAI\nConferenceonArtificialIntelligence,volume36,pages11730–11738,2022.\nJeffreyPennington,RichardSocher,andChristopherDManning. Glove: Globalvectorsforword\nrepresentation. InProceedingsofthe2014conferenceonempiricalmethodsinnaturallanguage\nprocessing(EMNLP),pages1532–1543,2014.\nYanZhang,RuidanHe,ZuozhuLiu,KwanHuiLim,andLidongBing. Anunsupervisedsentence\nembeddingmethodbymutualinformationmaximization. arXivpreprintarXiv:2009.12061,2020.\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. Declutr: Deep contrastive learning for\nunsupervisedtextualrepresentations. arXivpreprintarXiv:2006.03659,2020.\nFredrikCarlsson,AmaruCubaGyllensten,EvangeliaGogoulou,ErikYlipääHellqvist,andMagnus\nSahlgren. Semanticre-tuningwithcontrastivetension. InInternationalconferenceonlearning\nrepresentations,2020.\nXianmingLiandJingLi. Angle-optimizedtextembeddings. arXivpreprintarXiv:2309.12871,2023.\nBowenZhang,KehuaChang,andChunpingLi. Simpletechniquesforenhancingsentenceembed-\ndingsingenerativelanguagemodels. arXivpreprintarXiv:2404.03921,2024.\nAlexisConneauandDouweKiela. Senteval: Anevaluationtoolkitforuniversalsentencerepresenta-\ntions. arXivpreprintarXiv:1803.05449,2018.\nA TheoreticalAnalysisofHardNegativeMixing\nGivenadatasetofsentencesS ={s ,s ,...,s },withasentenceembeddingfunctionf :S →Rd\n1 2 n\nmappingsentencestoad-dimensionalvectorspace,HardNegativeMixing(HNM)aimstorefine\ntherepresentationlearningprocessbyleveragingchallengingnegativesamples.\n11\nA.1 DefinitionofHardNegatives\nHardnegativesarecrucialfortheeffectivetrainingofmodels,especiallyintasksthatrequirefine-\ngraineddiscriminationbetweensemanticallyclosebutdistinctsentences. Ahardnegativeforagiven\nsentences isdefinedasasentences that,whilesemanticallydifferent,liescloseintheembedding\ni j\nspace. Thisclosenessposesagreaterchallengeforthemodeltoaccuratelydiscriminate:\nHN(s )={s |sim(f(s ),f(s ))>θ,∀s ∈S\\{s }} (9)\ni j i j j i\nwhereHN(s )denotesthesetofhardnegativesforthesentences ,sim(·)isasimilaritymeasure\ni i\n(cosinesimilarity),f(·)representsthesentenceembeddingfunctionmappingsentencestopointsina\nhigh-dimensionalvectorspace,S isthesetofallsentences,andθisapredefinedthresholdindicating\ntheminimumsimilarityforasentencetobeconsideredahardnegative.\nA.2 MixingStrategy\nBuildinguponthefoundationalconceptofhardnegatives,asdelineatedintheprecedingsection,we\nintroduceanovelapproachtofurtherenrichthetrainingdatasetandamplifythemodel’sdiscriminative\npower. By strategically leveraging the defined hard negatives, we propose a method to generate\nsyntheticsamplesthatembodythenuancesandcomplexitiesencounteredinreal-worldscenarios.\nThismethod,termedHardNegativeMixing(HNM),involvestheinnovativecreationofsynthetic\nhardnegativesthroughtheblendingofembeddingsfromidentifiedhardnegativepairs. Thisstrategy\nnotonlyaugmentsthediversityoftrainingdatabutalsointroducesaheightenedlevelofchallenge,\ncompellingthemodeltorefineitsunderstandingandrepresentationofsemanticdistinctions.\nToenhancethediversityandchallengeofthetrainingprocess,weemployastrategyofgenerating\nsynthetic hard negatives. This is achieved by taking a linear combination of two existing hard\nnegatives,hn andhn ,associatedwiththesameanchorsentence:\ni j\nhn =λhn +(1−λ)hn (10)\nmix i j\nwhereλisamixingcoefficientthatliesintherange[0,1]. Thiscoefficientdeterminestherelative\ncontribution of each hard negative to the synthetic sample. By adjusting λ, we can control the\ndifficultyandvariabilityofthegeneratedhardnegatives,enrichingthetrainingdataandencouraging\nthemodeltolearnmorediscriminativefeatures.\nA.3 OptimizationObjective\nTheconceptualgroundworklaidbytheidentificationofhardnegativesandthesubsequentgeneration\nofsynthetichardnegativesthroughourHardNegativeMixingstrategypavesthewayforarefined\ntrainingobjective. Thisadvancedstageofmodeltrainingseeksnotonlytoutilizethesesynthetic\nsamples effectively but also to optimize the model’s sensitivity towards differentiating between\nsemanticallysimilaranddissimilarsentences. Byintricatelybalancingtheinfluencesofpositive\nsamplesandsynthetichardnegatives,weaimtocraftanoptimizationobjectivethatembodiesthe\ncomplexityofnaturallanguage,enhancingthemodel’sabilitytodiscernsubtlesemanticnuances.\nTheprimarygoalofincorporatingHNMintothetrainingprocessistooptimizethemodel’sability\ntodistinguishbetweenpositiveandnegativesamples. Theoptimizationobjectiveisformulatedto\nminimizethedistancebetweenpositivepairswhilemaximizingthedistancebetweentheanchor\nsentenceanditssynthetichardnegatives. Thisisquantifiedbythefollowinglossfunction:\nmin(cid:88)n −log(cid:32) exp(sim(f(s i),f(s+\ni\n))/τ) (cid:33)\nf\ni=1\nexp(sim(f(s i),f(s+\ni\n))/τ)+(cid:80) hnmix∈HNM(si)exp(sim(f(s i),f(hn mix))/τ)\n(11)\nIn this equation, f(·) denotes the embedding function mapping sentences to a high-dimensional\nvector space. The term sim(·) represents a similarity measure (e.g., cosine similarity) between\ntwo embeddings. The variable s+ signifies a positive sample that is semantically similar to the\ni\n12\nsentences ,andhn representsasynthetichardnegativesamplegeneratedfors throughHNM.\ni mix i\nTheparameterτ, knownasthetemperature, scalesthesimilarityscorestomodulatethesoftmax\nfunction’sconcentrationlevel,impactingtheoveralldifferentiationsensitivity.\nThe numerator of the fraction within the logarithm accentuates the affinity between s and its\ni\npositivecounterparts+,compellingthemodeltoclustersemanticallysimilarsentencescloserinthe\ni\nembeddingspace.Conversely,thedenominatoraggregatestheexponentialsimilaritiesbetweens and\ni\nbothitspositivepairandthesetofsynthetichardnegatives,promotingaspatialdistinctionbetween\ns andthechallengingnegativesgeneratedviaHNM.Throughthislossfunction,theoptimization\ni\nobjectivedelicatelyorchestratesabalancebetweenattractionandrepulsionforcesintheembedding\nspace,fosteringanuancedunderstandingofsemanticrelationshipscriticalfortheadvancementof\nnaturallanguageprocessingcapabilities.\nA.4 OptimizationObjective\nIntherealmofrepresentationlearning,particularlywithintheframeworkofHNM,theessenceof\nrefiningthemodel’scapacitytodistinguishbetweensemanticallyproximateyetdistinctsentences\nisencapsulatedintheformulationofameticulouslycraftedoptimizationobjective. Thisadvanced\nstageofmodelrefinementleveragesboththesynthetichardnegativesgeneratedthroughHNMand\nthepositivesamplestoformulatealossfunctionthatembodiestheintricatedynamicsofnatural\nlanguageunderstanding. Theoverarchinggoalistooptimizethemodelinsuchawaythatitachieves\naheightenedsensitivitytothesubtlesemanticnuancesthatdifferentiatepositivepairsfromtheirhard\nnegativecounterparts.\nToarticulatethisconceptmoreconcretely,weintroducealossfunctiondesignedtominimizethe\ndistancebetweenpositivesentencepairswhilesimultaneouslymaximizingthedistancebetweeneach\nsentenceanditsassociatedsynthetichardnegatives. Thisisachievedthroughacontrastivelearning\napproach,wherethemodelisencouragedtoaligncloselywithpositivesamplesintheembedding\nspaceanddivergefromthesynthetichardnegatives. Thelossfunctionisformulatedasfollows:\nmin(cid:88)n −log(cid:32) exp(sim(f(s i),f(s+\ni\n))/τ) (cid:33)\nf\ni=1\nexp(sim(f(s i),f(s+\ni\n))/τ)+(cid:80) hnmix∈HNM(si)exp(sim(f(s i),f(hn mix))/τ)\n(12)\nInthisequation,f(·)denotestheembeddingfunctionthatmapssentencestoad-dimensionalvector\nspace. Thefunctionsim(·)representsameasureofsimilarity(e.g.,cosinesimilarity)betweenthe\nembeddingsoftwosentences. Theterms+ referstoapositivesamplethatissemanticallysimilar\ni\ntos ,andhn denotesasynthetichardnegativesamplegeneratedthroughtheHNMprocessfor\ni mix\nthesentences . Theparameterτ,knownasthetemperature,servestoscalethesimilarityscores,\ni\ncontrollingthesharpnessofthesoftmaxdistributionutilizedinthedenominator.\nThenumeratorofthefractionwithinthelogarithmfocusesonthesimilaritybetweenthesentences\ni\nanditspositivecounterparts+,encouragingthemodeltolearnembeddingsthatbringthesepairs\ni\ncloser together in the embedding space. Conversely, the denominator aggregates the similarities\nbetween s and its synthetic hard negatives hn , alongside the similarity to the positive pair,\ni mix\ntherebypromotingthemodeltodifferentiates fromitshardnegativeseffectively.\ni\nA.5 AnalyzingImpactonEmbeddingSpace\nFollowing the in-depth exploration in the \"Optimization Objective\" section, where we discussed\noptimizingthemodeltodistinguishbetweenpositivesamplesandsynthetichardnegativesthrougha\ncarefullydesignedlossfunction,thisprocessnotonlyenhancesthemodel’sdiscriminativecapabilities\nbut also suggests a significant impact on the structure of the embedding space. Therefore, our\nsubsequentanalysiswillfocusonthespecificeffectsoftheHNMstrategyontheembeddingspace,\nparticularlyonhowitalterstheaveragepairwisedistancebetweenembeddings,therebyfacilitatinga\nmoredistinctseparationofsemanticmeanings.\nThe structural dynamics of the embedding space are crucial for the effective representation of\nsemanticrelationshipsamongsentences. TheintroductionofmixedhardnegativesviaHNMalters\nthisstructure,potentiallyleadingtoamoreseparablespacewheresimilaranddissimilarsentences\n13\narebetterdistinguished. Thistransformationcanbequantifiedbymeasuringtheaveragepairwise\ndistanceamongembeddingsbeforeandaftertheapplicationofHNM:\n1 (cid:88) 1 (cid:88)\n∆= ∥f(s )−f(s )∥ − ∥f′(s )−f′(s )∥\n|S|2 i j 2 |S|2 i j 2 (13)\nsi,sj∈S si,sj∈S′\nInthisequation,Sdenotestheoriginalsetofsentences,andS′representsthesetofsentencesafterthe\napplicationofHNM.Thefunctionf mapssentencestotheirembeddingsinthed-dimensionalvector\nspace,whilef′denotestheupdatedmappingpost-HNMapplication. Thenorm|·| measuresthe\n2\nEuclideandistancebetweentwoembeddings,providingaquantifiablemeasureoftheirdissimilarity.\nTheterm∆embodiesthenetchangeintheaveragepairwisedistanceamongallsentenceembeddings\nwithinthedataset,servingasanindicatorofhowtheembeddingspacehasevolvedinresponseto\nHNM.Apositivevalueof∆suggeststhat,onaverage,embeddingshavebecomemoredispersed,\nimplying that HNM has succeeded in expanding the embedding space. Such an expansion is\nindicative of a model that is better attuned to differentiating between semantically similar and\ndissimilarsentences,potentiallyleadingtoimprovementsintasksrequiringfine-grainedsemantic\ndiscrimination.\nConversely,anegativevalueof∆wouldsuggestacontractionoftheembeddingspace,whichcould\nhaveimplicationsforthemodel’sabilitytodistinguishbetweencloselyrelatedsemanticconcepts.\nHowever,thegoalofHNMisnotmerelytoexpandtheembeddingspaceindiscriminatelybuttodo\nsoinamannerthatenhancesthemodel’sdiscriminativecapacitybyfosteringamorestructuredand\nseparableembeddinglandscape.\nA.6 EnhancementofModelDiscriminability\nFollowingthedetailedexplorationofHNMstrategiesandtheirprofoundimpactontheembedding\nspace,ourfocusshiftstowardsquantitativelyassessingtheimprovementsinmodeldiscriminabil-\nity. The synthetic hard negatives generated through HNM pose refined challenges, necessitating\nan advanced model capability to distinguish between semantically similar yet distinct sentences.\nThissubsequentsectiondelvesintotheempiricalevaluationofmodelperformanceenhancements,\nhighlightingthepivotalroleofHNMinadvancingthenuanceddiscernmentcapabilitiesofsentence\nembeddings.\nOne of the primary goals of HNM is to enhance the model’s ability to discriminate between se-\nmanticallysimilaranddissimilarsentences,whichispivotalfortaskssuchassemanticsimilarity\nmeasurementandclassification. Thisenhancementcanbetheoreticallyrepresentedbyanincreasein\nmodelaccuracy,measuredasfollows:\nA′−A= 1 (cid:88) (cid:2)I[sim(f′(s ),f′(s+))>sim(f′(s ),f′(hn ))]−\n|T| i i i mix\n(si,s+ i,hnmix)∈T (14)\nI[sim(f(s ),f(s+))>sim(f(s ),f(hn ))](cid:3)\ni i i mix\nIn this formulation, A and A′ represent the model’s accuracy before and after the application of\nHNM,respectively. ThesetT encompassesthetripletsformedbyananchorsentences ,itspositive\ni\ncounterparts+,andthesynthetichardnegativehn ,generatedthroughHNM.ThefunctionI[·]\ni mix\ndenotestheindicatorfunction,yieldingavalueof1whentheconditionwithinthebracketsistrue,\nand0otherwise. Thetermsim(f(s ),f(s ))measuresthesimilaritybetweentheembeddingsof\ni j\nsentencess ands ,facilitatedbytheembeddingfunctionf(·),whichisrefinedtof′(·)post-HNM\ni j\napplication.\nThe accuracy measure thus reflects the differential impact of HNM on the model’s performance,\nquantified through the comparison of similarity scores between positive and hard negative pairs,\nbeforeandaftertheintegrationofHNM.Thismetricdirectlyilluminatestheenhanceddiscriminative\ncapabilityofthemodel,asaresultofthenuancedchallengesintroducedbysynthetichardnegatives,\nfosteringadeeperunderstandinganddistinctionbetweencloselyrelatedsentences.\n14\nA.7 ImpactonLearningDynamics\nAfterelucidatingtheroleofHNMinenhancingmodeldiscriminabilityanditssubstantialeffectson\ntheembeddingspace,itbecomesimperativetoexaminehowthesestrategiesinfluencethelearning\ndynamicsofthemodel. Theadaptivechallengesintroducedbysynthetichardnegativesnecessitate\na deeper understanding of the underlying adjustments in the model’s optimization process. This\nanalysisiscriticalforoptimizingthetrainingstrategyandensuringthatthemodeleffectivelyleverages\nthenuancedcomplexitiesintroducedbyHNM,therebyachievingasophisticatedbalancebetween\naccuracyandgeneralization.\nTheintroductionofHNMintothetrainingprocessmodifiesthelearningdynamicsbyinfluencingthe\ngradientofthelossfunctionwithrespecttotheembeddings.Thissectiondelvesintothemathematical\nunderpinningsofhowHNMalterstheembeddingupdatesduringtraining,providinginsightsintothe\noptimizationtrajectoryandtheembeddingspace’sevolution.\nA.7.1 GradientofLossFunction\nThelossfunction’sgradientwithrespecttotheembeddingsisacriticalfactorinthemodel’slearning\nprocess,dictatinghowtheembeddingsareupdatedineachiteration. GiventhelossfunctionL,the\ngradientwithrespecttotheembeddingofasentences canbeexpressedas:\ni\n(cid:32) (cid:33)\n∂L 1 (cid:88)\n=− (1−p )·k− p ·n (15)\n∂f(s ) τ k n\ni\nn∈N\nwhere:\n• f(s )denotestheembeddingofsentences .\ni i\n• τ isthetemperatureparameterscalingthesimilarityscores.\n• p andp representthesoftmaxprobabilitiesofthepositiveandnegativesamples,respec-\nk n\ntively.\n• kistheembeddingofthepositivesample,andndenotestheembeddingsofthenegative\nsamplesinthesetN.\nThisformulationencapsulatestheessenceofcontrastivelearning,emphasizingthemodel’saimto\npullpositivepairscloserwhilepushingnegativepairsapartintheembeddingspace.\nHNMinfluencesthelearningdynamicsbyadjustingthedistributionandcharacteristicsofnegative\nsamples,therebyaffectingthegradientofthelossfunction. Themodificationofthegradientcanlead\ntomoreeffectivelearning,especiallyindistinguishingcloselyrelatedbutdistinctsentences. This\nisachievedbyincorporatingamixofhardnegatives,whichareclosertothequerysentenceinthe\nembeddingspace,therebyprovidingastrongerlearningsignalforthemodel.\nA.8 BalancingDiversityandDifficulty\nInthecontextoftheprofoundadjustmentsintroducedbyHNMinthelearningdynamicsanditspivotal\nroleinenhancingmodeldiscriminability,thestrategicbalancebetweenthediversityanddifficultyof\nhardnegativesemergesasacriticalconsideration. Thisbalanceisinstrumentalinensuringthatthe\nsynthetichardnegativesgeneratedthroughHNMnotonlychallengethemodelbutalsocontribute\ntoacomprehensiverepresentationofthesemanticlandscape. Addressingthisbalanceenablesthe\noptimizationofthemodel’sperformancebyfine-tuningthedifficultyleveloftrainingsamplesto\nmatchthemodel’slearningstage, therebyfacilitatingamorenuancedunderstandingofcomplex\nsemanticrelationships.\nTheefficacyofHNMisnotmerelyafunctionofintroducinghardnegativesbutalsodependson\nbalancing the diversity and difficulty of these negatives. This balance can be formalized as an\noptimizationproblem,aimedatmaximizingthediversityamonghardnegativeswhileensuringthey\nremainchallenging:\n(cid:88)\nmax D(HN(s ))−β sim(f(s ),f(hn)) (16)\ni i\nHN(si)\nhn∈HN(si)\n15\nHere,HN(s )denotesthesetofhardnegativesforthesentences ,andD(HN(s ))quantifiesthe\ni i i\ndiversitywithinthisset. Thediversitymetricispivotalforensuringthatthemodelencountersabroad\nspectrumofsemanticvariations,therebyenhancingitsgeneralizability. Thetermsim(f(s ),f(hn))\ni\nmeasuresthesimilaritybetweentheembeddingsofs anditshardnegativeshn,withf(·)representing\ni\ntheembeddingfunction. Thecoefficientβ isabalancingparameterthatmodulatestheemphasis\nbetween maximizing diversity and the aggregate similarity of the hard negatives, signifying the\ndifficultyaspect.\nThis equation effectively captures the dual objective of optimizing the set of hard negatives by\nmaximizingtheirdiversitywhilecontrollingtheirdifficulty,asmeasuredbytheirsimilaritytothe\nsentences . Thetrade-off,facilitatedbyβ,allowsforanuancedadjustmentofthetrainingprocess,\ni\ntailoringthechallengeleveltooptimallyadvancethemodel’sdiscriminativecapabilities.\nA.9 OptimizationObjectivewithRegularization\nFollowingtheexplorationofthestrategicbalancebetweendiversityanddifficultyinHNMandits\nsignificantimplicationsformodeltrainingdynamics,theintroductionofaregularizationterminthe\noptimizationobjectivebecomesalogicalprogression. Thisapproachseekstoaddressthepotential\nforoverfittingthatmayarisefromtheintensifiedtrainingregimenimpliedbyHNM.Incorporating\nregularizationintotheoptimizationframeworknotonlyensuresmodelrobustnessbutalsofacilitates\nthegeneralizationoflearnedembeddingsacrossunseendata,thussafeguardingagainstthepitfallsof\nover-specializationtothetrainingset.\nIncorporating a regularization term, denoted as R(f), into the optimization objective is a well-\nestablishedmethodforenhancingtherobustnessofmachinelearningmodels. Itpenalizescomplex\nmodelstopreventoverfitting,therebyimprovinggeneralization:\nmin(cid:34) (cid:88)n −log(cid:32) exp(sim(f(s i),f(s+\ni\n))/τ) (cid:33)\nf\ni=1\nexp(sim(f(s i),f(s+\ni\n))/τ)+(cid:80) hnmix∈HNM(si)exp(sim(f(s i),f(hn mix))/τ)\n(cid:35)\n+λR(f)\n(17)\nInthisexpression,f(·)representstheembeddingfunctionmappingsentencestotheircorresponding\nvectorsinad-dimensionalspace. Thesim(·)functiondenotesasimilaritymeasure(e.g., cosine\nsimilarity)betweentwosentenceembeddings. Thesymbols+referstoapositivesampleassociated\ni\nwiths ,andhn representsthesynthetichardnegativesgeneratedthroughtheHNMprocessfor\ni mix\ns . Theparameterτ,knownasthetemperature,isusedtoscalethesimilarityscores,influencing\ni\nthe sharpness of the softmax distribution in the denominator. The term λR(f) introduces the\nregularization component, where λ is a coefficient that balances the relative importance of the\nregularization term within the overall optimization objective. The function R(f) quantifies the\ncomplexityofthemodel,penalizingconfigurationsthatmayleadtooverfitting,therebyencouraging\nthemodeltoadoptsimpler,moregeneralizableforms.\nThisequation,therefore,encapsulatesadualobjective: minimizingthecontrastivelosstoenhance\ndiscriminabilityandoptimizingmodelcomplexitytopreventoverfitting. Byachievingthisbalance,\nthemodelisnotonlytrainedtodistinguisheffectivelybetweensemanticallysimilaranddissimilar\nsentencesbutalsoensuredtogeneralizewelltounseendata,embodyingtheessenceofrobustmachine\nlearningmodels.\nA.10 TemperatureScalingAnalysis\nFollowingtheintricatediscussionsontheoptimizationobjectives,includingtheincorporationofreg-\nularization,andthestrategicendeavorstobalancediversityanddifficultywithintheHNMframework,\ntheexplorationoftemperaturescalingemergesasacrucialanalyticalcomponent. Thisanalysisis\nimperativeforfine-tuningthemodel’ssensitivitytodistinctionsbetweenhardnegativesandother\nsentence pairs. Temperature scaling, by adjusting the sharpness of the softmax distribution used\ninthemodel’slossfunction,playsapivotalroleinmodulatingthelearningprocess. Thisnuanced\n16\nadjustmentallowsforamoreprecisecalibrationofthemodel’sresponsetothevaryingdegreesof\nchallengepresentedbythehardnegatives,therebyoptimizingtheeffectivenessoftheHNMstrategy.\nThetemperaturescalingparameterτ playsacrucialroleincontrollingthesharpnessofthesoftmax\nfunction,directlyimpactingthemodel’slearningdynamics:\nTo further enhance the readability of the long equation, we introduce intermediate variables to\nsimplifyitsexpression. Thisapproachallowsforaclearerunderstandingofeachcomponentofthe\nequation. Let’sdefinethefollowingintermediatevariables:\n1. sim+ =sim(f(s ),f(s+)),representingthesimilaritybetweenthepositivepair.\ni i i\n2. sim =sim(f(s ),f(hn )),representingthesimilaritybetweenthehardnegativemix\ni,hnmix i mix\npair.\n3. Z i =exp(cid:16) sim τ+ i (cid:17) +(cid:80) hnmix∈HNM(si)exp(cid:16) simi, τhnmix(cid:17) ,representingthenormalizationfactor.\nWiththesedefinitions,wecanrewritetheoriginalequationasfollows:\n∂L 1 (cid:88)n  exp(cid:16) sim τ+ i (cid:17) ·sim+ i \n=−   (18)\n∂τ τ2 Z\ni\ni=1\nThisexpressionrevealshowsensitivitytochangesinτ affectsthebalancebetweenpositiveandhard\nnegativepairsinthelossgradient,guidingtheoptimizationprocess.\nA.11 EmbeddingSpaceNormalization\nBuildingupontheinsightfulanalysesoftemperaturescalinganditspivotalroleinfine-tuningthe\nmodel’sdiscriminationcapabilities,thefocusnowshiftstowardthenormalizationoftheembedding\nspace. Embeddingspacenormalizationisacriticalstepinensuringthattherefinedembeddings,\ninfluencedbyHNMandtemperatureadjustments,maintainaconsistentscale. Thisnormalization\nfacilitatesauniforminterpretationofdistancesandsimilaritieswithintheembeddingspace,whichis\nessentialfortheeffectiveapplicationofHNMstrategies. Bynormalizingtheembeddings,weaimto\nenhancethemodel’sgeneralizationabilityandensurethatthesemanticnuancescapturedthrough\nHNMarerobustlyintegratedintothemodel’slearningframework.\nWhen integrating HNM with embedding normalization to unit length, the process is adapted to\naccommodatethediscriminativeenhancementsintroducedbyHNM.IntheadjustedEquation:\nf (s )\nf (s )= HNM i (19)\nnorm−HNM i ∥f (s )∥\nHNM i 2\nf (s )istheembeddingofsentences afterapplyingHNM,reflectingtheimpactofchallenging\nHNM i i\nmixed negatives on the representation. The denominator ∥f (s )∥ is the L2 norm of this\nHNM i 2\nembedding, ensuringnormalizationtounitlength. Thisnormalizationfocusesonthedirectional\nattributes of the embedding vectors in the high-dimensional space, crucial for capturing angular\ndifferencesindicativeofsemanticrelations. Theresult,f (s ),isanormalizedembedding\nnorm−HNM i\nthat retains the rich, discriminative features imbued by HNM, facilitating a precise and nuanced\nseparationofsemanticcontentintheembeddingspace.\nA.12 SimilarityGradientComputation\nTransitioningfromtheessentialtaskofembeddingspacenormalization,whichensuresuniformity\nandcomparabilityofembeddings,wenowadvancetotheintricateprocessofsimilaritygradient\ncomputation.Thisstepispivotalforunderstandinghowthemodelupdatesitsembeddingsinresponse\ntothelearningsignal,especiallywithintheHNMframework.Thecomputationofsimilaritygradients\nisfundamentaltotheoptimizationprocess,asitelucidatesthedirectionandmagnitudeofadjustments\nneededfortheembeddingstoaccuratelyreflectsemanticrelationships.Bycalculatingthesegradients,\nweaimtofurtherrefinethemodel’sabilitytodiscernsubtlesemanticdifferences,leveragingthe\ncomplexityandchallengesintroducedbysynthetichardnegatives.\n17\nThegradientofthesimilarityfunctionwithrespecttoembeddingsisessentialforunderstandinghow\nthemodelupdatesembeddingsinresponsetothelearningsignal:\nf(s ) sim(f(s ),f(s ))(f(s ))\n∇ sim(f(s ),f(s ))= j − i j i (20)\nf(si) i j ∥f(s )∥∥f(s )∥ ∥f(s )∥2\ni j i\nInthisexpression,∇ sim(f(s ),f(s ))representsthegradientofthesimilarityfunctionbetween\nf(si) i j\ntheembeddingsofsentencess ands withrespecttotheembeddingofsentences . Thefunction\ni j i\nf(·)yieldstheembeddingofasentence,mappingittoapointintheembeddingspace. Theterm\nsim(f(s ),f(s )) quantifies the similarity between the embeddings of s and s , which is often\ni j i j\ncalculatedusingcosinesimilarityoranotherrelevantmetricintheembeddingspace.\nThegradientiscomposedoftwoprimarycomponents. Thefirstcomponent, f(sj) ,reflectsthe\n|f(si)||f(sj)|\ndirectiontowardstheembeddingofs normalizedbythemagnitudesofbothembeddings,essentially\nj\npointing in the direction to increase similarity. The second component, −sim(f(si),f(sj))(f(si)),\n|f(si)|2\nadjusts this direction by considering the current level of similarity and the magnitude of f(s ),\ni\ntherebyensuringthattheupdateisproportionaltotheneedtoincreaseordecreasesimilarity. This\ncomputation effectively guides the updates to embeddings during the learning process, ensuring\nthattheyevolveinamannerconducivetomaximizingthediscriminativecapabilityofthemodelas\ninfluencedbythesemanticrelationshipsembodiedinthetrainingdata.\nA.13 AnalyzingEmbeddingDivergence\nFollowingthecomputationofsimilaritygradients,whichprovidescrucialinsightsintothedirectional\nadjustmentsrequiredforembeddings,wepivottowardstheanalysisofembeddingdivergence. This\nanalysisisimperativeasitquantitativelymeasurestheextenttowhichtheHNMstrategyinfluencesthe\nrepresentationalspaceoftheembeddings. Byevaluatingthedivergencebetweenembeddingsbefore\nandaftertheapplicationofHNM,wecangaugetheeffectivenessofthisstrategyinenrichingthe\nmodel’ssemanticunderstanding. Thisstepisnotonlylogicalbutnecessaryforvalidatingtheimpact\nof HNM on enhancing the model’s discrimination capabilities, ensuring that the introduction of\nsynthetichardnegativesleadstoamorenuancedandrobustrepresentationofsemanticrelationships.\nThedivergenceinembeddings,beforeandafterapplyingHNM,shedslightonthemethod’seffec-\ntivenessinalteringtherepresentationalspace:\n(cid:118)\n(cid:117) n\n(cid:117)(cid:88)\ndiv(f,f′)=(cid:116) ∥f(s i)−f′(s i)∥2\n2\n(21)\ni=1\nInthisformulation,div(f,f′)quantifiestheoveralldivergencebetweentheembeddingsproduced\nbythemodelbefore(f)andafter(f′)theapplicationofHNM.Thefunctionf(s )representsthe\ni\nembeddingofsentences beforetheapplicationofHNM,andf′(s )denotestheembeddingofthe\ni i\nsamesentenceafterapplyingHNM.Theoperation|f(s )−f′(s )| computestheEuclideandistance\ni i 2\nbetweentheoriginalandupdatedembeddingsforeachsentences inthedataset,withthesquareroot\ni\nofthesumofthesesquareddistancesprovidingaholisticmeasureofhowtheembeddingspacehas\nshiftedasaresultofHNM.\nThisdivergencemetriceffectivelyencapsulatestheextentofchangeintheembeddingspace,offering\na concise measure of the impact of incorporating synthetic hard negatives through HNM on the\nmodel’ssemanticrepresentationcapabilities.\nA.14 ImpactofHardNegativeMixingonSimilarityDistribution\nBuildingonouranalysisofembeddingdivergence,whichquantitativelydemonstratesthetransforma-\ntiveimpactofHNMonthemodel’srepresentationalspace,wenextturnourattentiontothebroader\nimplicationsofHNMonsimilaritydistribution. ThisanalysisiscriticalforunderstandinghowHNM\nreshapesthelandscapeofsemanticrelationshipswithintheembeddingspace,influencingthemodel’s\nabilitytodiscernandcategorizetheserelationshipsaccurately. Byexaminingtheshiftinsimilarity\n18\ndistributionbeforeandaftertheapplicationofHNM,weseektouncoverthenuancedwaysinwhich\nHNMenhancesormodifiesthemodel’sperceptionofsemanticsimilarity,thusprovidingvaluable\ninsights into the effectiveness and strategic value of incorporating hard negatives in the training\nprocess.\nHNMfundamentallyaltersthesimilaritydistributionbetweensentencepairs,aneffectthatcanbe\nmeasuredusingtheKullback-Leiblerdivergence:\n(cid:18) (cid:19)\nD (P ∥Q )=\n(cid:88)\nP (s ,s )log\nP sim(s i,s j)\n(22)\nKL sim sim sim i j Q (s ,s )\nsim i j\n(si,sj)∈S×S\nInthisequation,D (P |Q )denotestheKLdivergencebetweentwoprobabilitydistributions:\nKL sim sim\nP ,representingthedistributionofsimilaritiesbetweensentencepairsbeforetheapplicationof\nsim\nHNM,andQ ,representingthedistributionafterapplyingHNM.Thesummationrunsoverall\nsim\npairsofsentences(s ,s )withinthesetS,encompassingtheentiredataset.\ni j\nP (s ,s ) and Q (s ,s ) are the probabilities associated with the similarity scores between\nsim i j sim i j\nsentencess ands intherespectivedistributionsbeforeandafterHNMisapplied. Thelogfunction\ni j\nprovides a measure of the relative difference between these probabilities, with the overall KL\ndivergencesummingthesedifferencesacrossallsentencepairs. Thisformulationeffectivelycaptures\nthedivergenceinsimilaritydistributionsduetoHNM,providingaquantitativelensthroughwhich\ntheimpactofHNMontheembeddingspace’sstructurecanbeassessed.\nA.15 EnhancedDiscriminativeFeatureLearning\nBuilding on the comprehensive analyses of embedding divergence and its implications for the\nrepresentationalspace,wenowturnourattentiontotheenhancedlearningofdiscriminativefeatures\nfacilitatedbyHNM.Thisprogressionisbothlogicalandnecessary,asthepreviousanalysesprovide\na foundation for understanding how HNM reshapes the embedding landscape. The subsequent\nenhancementinthemodel’sabilitytolearndiscriminativefeaturesisadirectconsequenceofthese\nstructural changes. By focusing on this aspect, we aim to elucidate the direct link between the\nstrategicintroductionofsynthetichardnegativesandthemodel’simprovedcapabilitytodistinguish\nbetweensemanticallysimilarbutdistinctsentences. ThisstepunderscoresthepivotalroleofHNMin\nfosteringadeeper,morenuancedcomprehensionofsemanticrelations,whichiscriticalforadvancing\nthestateoftheartinnaturallanguageprocessingtasks.\nTheabilityofHNMtoencouragethelearningofmorediscriminativefeaturesiscapturedbythe\nincreasedmarginbetweenpositiveandhardnegativepairs:\n(cid:16) (cid:17)\n∆margin= min sim(f(s i),f(s+ i ))−sim(f(s i),f(hn mix)) (23)\ns+ i∈P(si),hnmix∈HNM(si)\nInthisformulation,∆marginquantifiestheincrementintheseparationmarginbetweentheembed-\ndingsofpositivepairs(s ,s+)andtheembeddingsofsentences anditssynthetichardnegatives\ni i i\nhn . Thefunctionsim(f(s ),f(s ))calculatesthesimilaritybetweentheembeddingsofsen-\nmix i j\ntencess ands ,withf(·)representingtheembeddingfunction. ThesetP(s )containspositive\ni j i\nsamplesthataresemanticallyclosetos ,andHNM(s )encompassesthesynthetichardnegatives\ni i\ngeneratedfors throughtheHNMprocess.\ni\nTheobjectiveofthismeasureistocapturetheextenttowhichthemodelhasenhanceditsabilityto\nembedsemanticallysimilarsentencescloserintheembeddingspace,whileeffectivelydistancingthe\nhardnegatives.Alarger∆marginsignifiesamorepronounceddiscriminativecapability,highlighting\ntheefficacyofHNMinenrichingthefeaturelearningprocess. Thismetricthusservesasadirect\nindicatorofthemodel’simprovedproficiencyinparsingandunderstandingthenuancedsemantic\nrelationshipsinherentinnaturallanguage.\nA.16 LearningRateAdjustmentStrategy\nAfterdelvingintothenuancesofembeddingdivergenceanditsimplicationsforthemodel’ssemantic\nrepresentation,theprogressiontowardsrefiningthelearningrateadjustmentstrategybecomesboth\n19\nlogical and necessary. This refinement is pivotal in accommodating the enhanced discriminative\nfeature learning facilitated by the HNMstrategy. By dynamically adjusting the learning rate in\nresponsetothepresenceofsynthetichardnegatives,themodelcanmoreeffectivelyintegratethe\ncomplexityandchallengetheyintroduce. Thistailoredapproachtolearningrateadjustmentensures\nthatthemodelremainssensitivetosubtlesemanticdistinctions,optimizingitsperformanceacross\nvaryingtrainingdynamics. Thus,thestrategicmodulationofthelearningrateemergesasanessential\ncomponentoftheoptimizationframework,enablingthemodeltofullyleveragethebenefitsofHNM.\nAdaptingthelearningrateinresponsetothepresenceofhardnegativesprovidesamechanismfor\nfine-tuningthemodel’ssensitivitytochallengingsamples:\nη\n=η·γI[hnmix∈HNM(si)]\n(24)\nnew\nInthisexpression,η representstheadjustedlearningrate,whileηdenotestheoriginallearning\nnew\nratepriortoadjustment. Thetermγ isadecayfactor,withavaluelessthan1,whichmodulatesthe\nlearningrate. ThefunctionI[·]isanindicatorfunctionthatevaluatesto1iftheconditionwithin\nthebracketsistrue—specifically,ifthesynthetichardnegativehn isamongthehardnegatives\nmix\ngeneratedforsentences throughHNM—and0otherwise.\ni\nThisformulationencapsulatesadynamicadjustmentmechanism,whereinthelearningrateisselec-\ntivelyscaleddowninthepresenceofsynthetichardnegatives. Thisscalingisdesignedtotemper\nthelearningstep,acknowledgingtheheightenedchallengeposedbythesenegatives,andthereby\nensuringthatthemodel’supdatesarebothdeliberateandmeasured. Theapplicationofthisstrategy\nensuresthatthelearningprocessremainsrobustandresponsivetotheintricaciesintroducedbyHNM,\noptimizingthemodel’strajectorytowardsachievingsuperiordiscriminativeperformance.\nA.17 Conclusion\nInconclusion, thetheoreticalexplorationofHardNegativeMixing(HNM)withinthisdiscourse\nelucidatesitsprofoundimpactontheenhancementofrepresentationlearning, particularlyinthe\ndomainofnaturallanguageprocessing. Byrigorouslydefininghardnegatives,devisingasynthetic\ngenerationstrategy,andoptimizingthemodelthroughinnovativelossfunctions,HNMhasdemon-\nstratedsignificantpromiseinrefiningthediscriminabilityandsemanticunderstandingofmodels. The\nstrategicincorporationofembeddingspacenormalization,similaritygradientcomputation,andan\nadaptivelearningrateadjustmentstrategyfurtheroptimizesthelearningprocess,ensuringthemodel’s\nsensitivitytonuancedsemanticdifferences.Theanalysisofembeddingdivergenceandtheadjustment\nofsimilaritydistributionsunderscoretheeffectivenessofHNMincreatingamorenuancedandrobust\nrepresentationalspace. Ultimately,thiscomprehensivetheoreticalanalysishighlightstheparamount\nimportanceofHNMinadvancingthestateoftheartinrepresentationlearning. Throughmeticulous\nexaminationandoptimization,HNMemergesnotmerelyasatechniquebutasacornerstoneinthe\npursuitofmorediscriminative,nuanced,andcontextuallyawaremodels,underscoringtheindispens-\nabilityoftheoreticalanalysisinpushingtheboundariesofwhatisachievableinnaturallanguage\nprocessingandbeyond.\n20",
    "pdf_filename": "HNCSE_Advancing_Sentence_Embeddings_via_Hybrid_Contrastive_Learning_with_Hard_Negatives.pdf"
}