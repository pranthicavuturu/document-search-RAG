{
    "title": "REINFORCEMENT LEARNING WITH ACTION SEQUENCE",
    "abstract": "Trainingreinforcementlearning(RL)agentsonrobotictaskstypicallyrequiresa largenumberoftrainingsamples. Thisisbecausetrainingdataoftenconsistsof noisytrajectories,whetherfromexplorationorhuman-collecteddemonstrations, makingitdifficulttolearnvaluefunctionsthatunderstandtheeffectoftakingeach action. Ontheotherhand,recentbehavior-cloning(BC)approacheshaveshown thatpredictingasequenceofactionsenablespoliciestoeffectivelyapproximate noisy,multi-modaldistributionsofexpertdemonstrations.Canweuseasimilaridea forimprovingRLonrobotictasks?Inthispaper,weintroduceanovelRLalgorithm thatlearnsacriticnetworkthatoutputsQ-valuesoverasequenceofactions. By explicitly training the value functions to learn the consequence of executing a seriesofcurrentandfutureactions,ouralgorithmallowsforlearningusefulvalue functionsfromnoisytrajectories. Westudyouralgorithmacrossvarioussetups with sparse and dense rewards, and with or without demonstrations, spanning mobilebi-manualmanipulation,whole-bodycontrol,andtabletopmanipulation tasksfromBiGym,HumanoidBench,andRLBench. Wefindthat,bylearningthe criticnetworkwithactionsequences,ouralgorithmoutperformsvariousRLand BCbaselines,inparticularonchallenginghumanoidcontroltasks. Projectwebsite: younggyo.me/cqn-as. BiGym (25 Tasks) HumanoidBench (8 Tasks) RLBench (20 Tasks) 100 1000 100 75 750 75 50 500 50 25 250 25 0 0 0 0 2e4 4e4 6e4 8e4 1e5 0 2e6 4e6 6e6 8e6 1e7 0 1e4 2e4 3e4 Environment Steps Environment Steps Environment Steps RL: CQN-AS (Ours) CQN DrQ-v2+ SAC BC: ACT Figure1: Summaryofresults. Coarse-to-fineQ-NetworkwithActionSequence(CQN-AS)isa value-basedRLalgorithmthatlearnsacriticnetworkwithactionsequence. WestudyCQN-ASon53 robotictasksfromBiGym(Chernyadevetal.,2024),HumanoidBench(Sferrazzaetal.,2024),and RLBench(Jamesetal.,2020),wherepriormodel-freeRLalgorithmsstruggletoachievecompetitive performance. WeshowthatCQN-ASoutperformsvariousRLandBCbaselinessuchasCQN(Seo etal.,2024),DrQ-v2+(Yaratsetal.,2022),SAC(Haarnojaetal.,2018),andACT(Zhaoetal.,2023). 1 INTRODUCTION Reinforcementlearning(RL)holdsthepromiseofcontinuallyimprovingpoliciesthroughonlinetrial- and-errorexperiences(Sutton&Barto,2018),makingitanidealchoicefordevelopingrobotsthat canadapttovariousenvironments. However,despitethispromise,trainingRLagentsonrobotictasks typicallyrequiresaprohibitivelylargenumberoftrainingsamples(Kalashnikovetal.,2018;Herzog etal.,2023),whichbecomesproblematicasdeployingrobotsoftenincursahugecost. Therefore manyoftherecentsuccessfulapproachesonrobotlearninghavebeenbasedonbehavior-cloning (BC;Pomerleau1988),whichcanlearnstrongpoliciesfromofflineexpertdemonstrations(Brohan etal.,2023b;a;Zhaoetal.,2023;Chietal.,2023;Teametal.,2024;Fuetal.,2024a). 1 4202 voN 91 ]GL.sc[ 1v55121.1142:viXra )%( etaR sseccuS nruteR edosipE )%( etaR sseccuS",
    "body": "REINFORCEMENT LEARNING WITH ACTION SEQUENCE\nFOR DATA-EFFICIENT ROBOT LEARNING\nYounggyoSeo PieterAbbeel\nUCBerkeley UCBerkeley\nyounggyo.seo@berkeley.edu pabbeel@cs.berkeley.edu\nABSTRACT\nTrainingreinforcementlearning(RL)agentsonrobotictaskstypicallyrequiresa\nlargenumberoftrainingsamples. Thisisbecausetrainingdataoftenconsistsof\nnoisytrajectories,whetherfromexplorationorhuman-collecteddemonstrations,\nmakingitdifficulttolearnvaluefunctionsthatunderstandtheeffectoftakingeach\naction. Ontheotherhand,recentbehavior-cloning(BC)approacheshaveshown\nthatpredictingasequenceofactionsenablespoliciestoeffectivelyapproximate\nnoisy,multi-modaldistributionsofexpertdemonstrations.Canweuseasimilaridea\nforimprovingRLonrobotictasks?Inthispaper,weintroduceanovelRLalgorithm\nthatlearnsacriticnetworkthatoutputsQ-valuesoverasequenceofactions. By\nexplicitly training the value functions to learn the consequence of executing a\nseriesofcurrentandfutureactions,ouralgorithmallowsforlearningusefulvalue\nfunctionsfromnoisytrajectories. Westudyouralgorithmacrossvarioussetups\nwith sparse and dense rewards, and with or without demonstrations, spanning\nmobilebi-manualmanipulation,whole-bodycontrol,andtabletopmanipulation\ntasksfromBiGym,HumanoidBench,andRLBench. Wefindthat,bylearningthe\ncriticnetworkwithactionsequences,ouralgorithmoutperformsvariousRLand\nBCbaselines,inparticularonchallenginghumanoidcontroltasks.\nProjectwebsite: younggyo.me/cqn-as.\nBiGym (25 Tasks) HumanoidBench (8 Tasks) RLBench (20 Tasks)\n100 1000 100\n75 750 75\n50 500 50\n25 250 25\n0 0 0\n0 2e4 4e4 6e4 8e4 1e5 0 2e6 4e6 6e6 8e6 1e7 0 1e4 2e4 3e4\nEnvironment Steps Environment Steps Environment Steps\nRL: CQN-AS (Ours) CQN DrQ-v2+ SAC BC: ACT\nFigure1: Summaryofresults. Coarse-to-fineQ-NetworkwithActionSequence(CQN-AS)isa\nvalue-basedRLalgorithmthatlearnsacriticnetworkwithactionsequence. WestudyCQN-ASon53\nrobotictasksfromBiGym(Chernyadevetal.,2024),HumanoidBench(Sferrazzaetal.,2024),and\nRLBench(Jamesetal.,2020),wherepriormodel-freeRLalgorithmsstruggletoachievecompetitive\nperformance. WeshowthatCQN-ASoutperformsvariousRLandBCbaselinessuchasCQN(Seo\netal.,2024),DrQ-v2+(Yaratsetal.,2022),SAC(Haarnojaetal.,2018),andACT(Zhaoetal.,2023).\n1 INTRODUCTION\nReinforcementlearning(RL)holdsthepromiseofcontinuallyimprovingpoliciesthroughonlinetrial-\nand-errorexperiences(Sutton&Barto,2018),makingitanidealchoicefordevelopingrobotsthat\ncanadapttovariousenvironments. However,despitethispromise,trainingRLagentsonrobotictasks\ntypicallyrequiresaprohibitivelylargenumberoftrainingsamples(Kalashnikovetal.,2018;Herzog\netal.,2023),whichbecomesproblematicasdeployingrobotsoftenincursahugecost. Therefore\nmanyoftherecentsuccessfulapproachesonrobotlearninghavebeenbasedonbehavior-cloning\n(BC;Pomerleau1988),whichcanlearnstrongpoliciesfromofflineexpertdemonstrations(Brohan\netal.,2023b;a;Zhaoetal.,2023;Chietal.,2023;Teametal.,2024;Fuetal.,2024a).\n1\n4202\nvoN\n91\n]GL.sc[\n1v55121.1142:viXra\n)%(\netaR\nsseccuS\nnruteR\nedosipE\n)%(\netaR\nsseccuS\nQ-Values over Action Sequence from Level L\nAction Sequence\nGRU\nCoarse-to-fine Critic with\nMLP MLP MLP Action Sequence\nRepeat\nProprio States\nL Times\nLevel Index Actions from Actions from\nInputs\n[0, 1, …, 0] Previous Level Previous Level\nInputs & Encoding Coarse-to-fine Critic with Action Sequence Action Inference\nFigure2: Coarse-to-fineQ-networkwithactionsequence. (Left)Ourkeyideaistotrainacritic\nnetworktooutputQ-valuesoverasequenceofactions. Wedesignourarchitecturetofirstobtain\nfeaturesforeachsequencestepandaggregatefeaturesfrommultiplesequencestepswitharecurrent\nnetwork. WethenprojecttheseoutputsintoQ-valuesatlevell. (Right)Foractioninference,we\nrepeattheprocedureofcomputingQ-valuesforlevell∈{1,...,L}. Wethenfindtheactionsequence\nwiththehighestQ-valuesfromthelastlevelL,anduseitforcontrollingrobotsateachtimestep.\nOnecauseforthepoordata-efficiencyofRLalgorithmsonrobotictasksisthattrainingdataconsists\nofnoisytrajectories. WhencollectingdatafortrainingRLagents,wetypicallyinjectsomenoiseinto\nactionsforexploration(Sehnkeetal.,2010;Lillicrapetal.,2016)thatmayinducetrajectorieswith\njerkymotions. Moreover,weofteninitializetrainingwithhuman-collecteddemonstrationsthatcan\nconsistofnoisymulti-modaltrajectories(Chernyadevetal.,2024). Suchnoisydatadistributions\nmake it difficult to learn value functions that should understand the consequence of taking each\naction. WeindeedfindthatpriorRLalgorithmsperformmuchworsethantheBCbaselineonmobile\nbi-manualmanipulationtaskswithhuman-collecteddemonstrationswhencomparedtoatabletop\nmanipulationsetupwithsyntheticdemonstrationscollectedviamotion-planning(seeFigure1).\nOntheotherhand,recentBCapproacheshaveshownthatpredictingasequenceofactionsenables\npoliciestoeffectivelyapproximatethenoisy,multi-modaldistributionofexpertdemonstrations(Zhao\netal.,2023;Chietal.,2023). Inspiredbythis, inthispaper, weinvestigatehowtouseasimilar\nideaforimprovingthedata-efficiencyofRLalgorithmsonrobotictasks. Inparticular,wepresent\nanovelRLalgorithmthatlearnsacriticnetworkthatoutputsQ-valuesoverasequenceofactions\n(seeFigure2). Bytrainingthecriticnetworktoexplicitlylearntheconsequenceoftakingaseries\nofcurrentandfutureactions,ouralgorithmenablestheRLagentstoeffectivelylearnusefulvalue\nfunctionsfromnoisytrajectories. Webuildthisalgorithmuponarecentvalue-basedRLalgorithm\nthatlearnsRLagentstozoom-intocontinuousactionspaceinacoarse-to-finemanner(Seoetal.,\n2024),thuswerefertoouralgorithmasCoarse-to-fineQ-NetworkwithActionSequence(CQN-AS).\nToevaluatethegeneralityandcapabilitiesofCQN-AS,westudyCQN-ASonvarioussetupswith\nsparseanddenserewards,andwithorwithoutdemonstrations. InBiGymbenchmark(Chernyadev\netal.,2024),whichprovideshuman-collecteddemonstrationsformobilebi-manualmanipulation\ntasks, CQN-AS outperforms various model-free RL and BC baselines (Yarats et al., 2022; Zhao\netal.,2023;Seoetal.,2024). Moreover,inHumanoidBench(Sferrazzaetal.,2024),whichconsists\nofdensely-rewardedhumanoidcontroltasks,weshowthatCQN-AScanalsobeeffectivewithout\ndemonstrations,outperformingpriormodel-freeRLbaselines(Haarnojaetal.,2018;Seoetal.,2024).\nFinally,inRLBench(Jamesetal.,2020),whichprovidessyntheticdemonstrationsgeneratedvia\nmotion-planning, CQN-ASachievessimilarperformanceasmodel-freeRLandBCbaselineson\nmosttasks,butsignificantlybetterperformanceonseverallong-horizonmanipulationtasks.\n2 BACKGROUND\nProblemsetup Wemainlyconsideraroboticcontrolproblemwhichweformulateasapartially\nobservableMarkovdecisionprocess(Kaelblingetal.,1998;Sutton&Barto,2018). Ateachtime\nstep t, an RL agent encounters an observation o , executes an action a , receives a reward r ,\nt t t+1\nandencountersanewobservationo fromtheenvironment. Becausetheobservationo doesnot\nt+1 t\ncontainfullinformationabouttheinternalstateoftheenvironment,inthiswork,weuseastackof\npastobservationsasinputstotheRLagentbyfollowingthecommonpracticeinMnihetal.(2015).\n2\nCNN\nCNN\nConcat\n&\nLinear\nForsimplicity,weomitthenotationforthesestackedobservations. Whentheenvironmentisfully\nobservable,wesimplyuseo asinputs. Ourgoalinthisworkistotrainapolicyπthatmaximizesthe\nt\nexpectedsumofrewardsthroughRLwhileusingasfewsamplesaspossible,optionallywithaccess\ntoamodestamountofexpertdemonstrationscollectedeitherbymotion-plannersorbyhumans.\nInputsandencoding Givenvisualobservationsov ={ov1,...,ovM}fromM cameras,weencode\nt t t\neachovi usingconvolutionalneuralnetworks(CNN)intohvi. Wethenprocessthemthroughaseries\nt t\noflinearlayerstofusethemintohv. Iflow-dimensionalobservationsolowareavailablealongwith\nt t\nvisualobservations,weprocessthemthroughaseriesoflinearlayerstoobtainhlow. Wethenuse\nt\nconcatenatedfeatures h = [hv,hlow] asinputs tothe criticnetwork. Indomains withoutvision\nt t t\nsensors,wesimplyuseolowash withoutencodingthelow-dimensionalobservations.\nt t\nCoarse-to-fineQ-Network Coarse-to-fineQ-Network(CQN;Seoetal.2024)isavalue-basedRL\nalgorithmforcontinuouscontrolthattrainsRLagentstozoom-intothecontinuousactionspaceina\ncoarse-to-finemanner. Inparticular,CQNiteratestheproceduresof(i)discretizingthecontinuous\nactionspaceintomultiplebinsand(ii)selectingthebinwiththehighestQ-valuetofurtherdiscretize.\nThisreformulatesthecontinuouscontrolproblemasamulti-leveldiscretecontrolproblem,allowing\nfortheuseofideasfromsample-efficientvalue-basedRLalgorithms(Mnihetal.,2015;Silveretal.,\n2017;Schrittwieseretal.,2020),designedtobeusedwithdiscreteactions,forcontinuouscontrol.\nFormally,letal beanactionatlevellwitha0beingthezerovector1.Wethendefinethecoarse-to-fine\nt t\ncritictoconsistofmultipleQ-networkswhichcomputeQ-valuesforactionsateachlevelal,given\nt\nthefeaturesh andactionsfromthepreviouslevelal−1,asfollows:\nt t\nQl(h ,al,al−1) for l∈{1,...,L} (1)\nθ t t t\nWeoptimizeeachQ-networkatlevellwiththefollowingobjective:\n(cid:16) (cid:17)\nLl = Ql(h ,al,al−1)−r −γmaxQl(h ,a′,πl(h ) , (2)\nθ t t t t+1\na′\nθ¯ t+1 t+1\nwhereθ¯aredelayedparametersforatargetnetwork(Polyak&Juditsky,1992)andπl isapolicy\nthat outputs the action al at each level l via the inference steps with our critic, i.e., πl(h ) = al.\nt t t\nSpecifically,tooutputactionsattimesteptwiththecritic,CQNfirstinitializesconstantsalowand\nt\nahighwith−1and1. Thenthefollowingstepsarerepeatedforl∈{1,...,L}:\nt\n• Step1(Discretization): Discretizeaninterval[alow,ahigh]intoBuniformintervals,andeach\nt t\noftheseintervalsbecomeanactionspaceforQl\nθ\n• Step2(Binselection): FindabinwiththehighestQ-valueandsetal tothecentroidofthebin.\nt\n• Step3(Zoom-in): Setalowandahightotheminimumandmaximumoftheselectedbin,which\nt t\nintuitivelycanbeseenaszooming-intoeachbin.\nWethenusethelastlevel’sactionaL astheactionattimestept. Formoredetails,includingthe\nt\ninferenceprocedureforcomputingQ-values,wereferreaderstoAppendixB.\n3 COARSE-TO-FINE Q-NETWORK WITH ACTION SEQUENCE\nWepresentCoarse-to-fineQ-NetworkwithActionSequence(CQN-AS),avalue-basedRLalgorithm\nthatlearnsacriticnetworkthatoutputsQ-valuesforasequenceofactionsa ={a ,...,a }\nt:t+K t t+K−1\nforagivenobservationo . Ourmainmotivationforthisdesigncomesfromoneofthekeyideas\nt\ninrecentbehavior-cloning(BC)approaches,i.e.,predictingactionsequences,whichhelpsresolve\nambiguitywhenapproximatingnoisy,multi-modaldistributionsofexpertdemonstrations(Zhaoetal.,\n2023;Chietal.,2023). Similarly,byexplicitlylearningQ-valuesofbothcurrentandfutureactions\nfromthegivenstate,ourapproachaimstomitigatethechallengeoflearningQ-valueswithnoisy\ntrajectoriesfromexploratorybehaviorsorhuman-collecteddemonstrations.\nThissectiondescribeshowwedesignourcriticnetworkwithactionsequence(seeSection3.1)and\nhowweutilizeactionsequenceoutputstocontrolrobotsateachtimestep(seeSection3.2). The\noverviewofouralgorithmisavailableinFigure2.\n1Forsimplicity,wedescribeCQNandCQN-ASwithasingle-dimensionalactioninthemainsection.See\nAppendixBforfulldescriptionwithN-dimensionalactions,whichisstraightforwardbutrequiresmoreindices.\n3\n3.1 COARSE-TO-FINECRITICWITHACTIONSEQUENCE\nOurkeyideaistodesignacriticnetworktoexplicitlylearnQ-valuesforcurrentactionandfuture\nactionsfromthecurrenttimestept,i.e.,{Q(o ,a ),Q(o ,a ),...,Q(o ,a )},toenablethe\nt t t t+1 t t+K−1\ncritictounderstandtheconsequenceofexecutingaseriesofactionsfromthegivenstate.\nFormulationandobjective Letal ={al,...,al }beanactionsequenceatlevelland\nt:t+K t t+K−1\na0 beazerovector.Wedesignourcoarse-to-finecriticnetworktoconsistofmultipleQ-networks\nt:t+K\nthatcomputeQ-valuesforeachactionatsequencestepkandlevell:\nQl,k(h ,al ,al−1 )for l∈{1,...,L}andk ∈{1,...,K} (3)\nθ t t+k−1 t:t+K\nWeoptimizeourcriticnetworkwiththefollowingobjective:\n(cid:88) (cid:88) (cid:16) (cid:17)2\nQl,k(h ,al ,al−1 )−r −γmaxQl,k(h ,a′,πl (h ) , (4)\nk l θ t t+k−1 t:t+K t+1 a′ θ¯ t+1 K t+1\nwhere πl is an action sequence policy that outputs the action sequence al . In practice, we\nK t:t+K\ncomputeQ-valuesforallsequencestepk ∈{1,...,K}inparallel,whichispossiblebecauseQ-values\nforfutureactionsdependonlyoncurrentfeaturesh butnotonQ-valuesforpreviousactions. We\nt\nfindthissimpledesign,withindependenceacrossactionsequence,workswellevenonchallenging\nhumanoidcontroltaskswithhigh-dimensionalactionspaces(Sferrazzaetal.,2024). Weexpectour\nideacanbestrengthenedbyexploitingthesequentialstructure,i.e.,Q-valuesatsubsequentsteps\ndependonpreviousQ-values(Metzetal.,2017;Chebotaretal.,2023),butweleaveitasfuturework.\nArchitecture Weimplementourcriticnetworktoinitiallyextractfeaturesforeachsequencestepk\nandaggregatefeaturesfrommultiplestepswitharecurrentnetwork(seeFigure2). Thisarchitecture\nisoftenhelpfulincaseswhereasingle-stepactionisalreadyhigh-dimensionalsothatconcatenating\nthemmakeinputstoohigh-dimensional. Specifically,lete denoteanone-hotencodingfork. At\nk\neachlevell,weconstructfeaturesforeachsequencestepk ashl = (cid:2) h ,al−1 ,e (cid:3) . Wethen\nt,k t t+k−1 k\nencodeeachhl withasharedMLPnetworkandprocessthemthroughGRU(Choetal.,2014)to\nt,k\nobtainsl = fGRU(fMLP(hl ),...,fMLP(hl )). Wethenuseasharedprojectionlayertomapeach\nt,k θ θ t,1 θ t,k\nsl intoQ-valuesateachsequencestepk,i.e.,Ql,k(o ,al ,al−1 )=fproj(sl ).\nt,k θ t t+k−1 t:t+K θ t,k\n3.2 ACTIONEXECUTIONANDTRAININGDETAILS\nWhile the idea of using action sequence is simple, there are two important yet small details for\neffectivelytrainingRLagentswithactionsequence: (i)howweexecuteactionsateachtimestepto\ncontrolrobotsand(ii)howwestoretrainingdataandsamplebatchesfortraining.\nExecutingactionwithtemporalensemble Withthepolicythatoutputsanactionsequencea ,\nt:t+K\noneimportantquestionishowtoexecuteactionsattimestepi∈{t,...,t+K−1}. Forthis,weuse\ntheideaofZhaoetal.(2023)thatutilizestemporalensemble,whichcomputesa everytimestep,\nt:t+K\n(cid:80) (cid:80)\nsavesittoabuffer,andexecutesaweightedaverage w a / w wherew = exp(−m∗i)\ni i t−i i i\ndenotesaweightthatassignshighervaluetomorerecentactions. Wefindthisschemeoutperforms\nthealternativeofcomputinga everyK stepsandexecutingeachactionforsubsequentK steps\nt:t+K\nonmosttasksweconsidered,exceptonseveraltasksthatneedreactivecontrol.\nStoringtrainingdatafromenvironmentinteraction Whenstoringsamplesfromonlineenvi-\nronmentinteraction,westoreatransition(o ,aˆ ,r ,o )whereaˆ denotesanactionexecutedat\nt t t+1 t+1 t\ntimestept. Forinstance,ifweusetemporalensembleforactionexecution,aˆ isaweightedaverage\nt\nofactionoutputsobtainedfrompreviousK timesteps.\nSamplingtrainingdatafromareplaybuffer Whensamplingtrainingdatafromthereplaybuffer,\nwesampleatransitionwithactionsequence,i.e.,(o ,aˆ ,r ,o ). Ifwesampletimestept\nt t:t+K t+1 t+1\nneartheendofepisodesothatwedonothaveenoughdatatoconstructafullactionsequence,wefill\ntheactionsequencewithnullactions. Inparticular,inpositioncontrolwherewespecifytheposition\nofjointsorendeffectors,werepeattheactionfromthelaststepsothattheagentlearnsnottochange\ntheposition. Ontheotherhand,intorquecontrolwherewespecifytheforcetoapplytojoints,we\nsettheactionafterthelaststeptozerosothatagentlearnstonottoapplyforce.\n4\nFigure3: Examplesofrobotictasks. WestudyCQN-ASon53robotictasksspanningmobilebi-\nmanualmanipulation,whole-bodycontrol,andtabletopmanipulationtasksfromBiGym(Chernyadev\netal.,2024),HumanoidBench(Sferrazzaetal.,2024),andRLBench(Jamesetal.,2020).\n4 EXPERIMENT\nWestudyCQN-ASon53robotictasksspanningmobilebi-manualmanipulation,whole-bodycontrol,\nandtabletopmanipulationtasksfromBiGym(Chernyadevetal.,2024),HumanoidBench(Sferrazza\netal.,2024),andRLBench(Jamesetal.,2020)environments(seeFigure3forexamplesofrobotic\ntasks). Thesetaskswithsparseanddenserewards,withorwithoutvisionsensors,andwithorwithout\ndemonstrations,allowforevaluatingthecapabilitiesandlimitationsofouralgorithm. Inparticular,\nourexperimentsaredesignedtoinvestigatethefollowingquestions:\n• CanCQN-ASquicklymatchtheperformanceofarecentBCalgorithm(Zhaoetal.,2023)\nandsurpassitthroughonlinelearning? HowdoesCQN-AScomparetopreviousmodel-free\nRLalgorithms(Haarnojaetal.,2018;Yaratsetal.,2022;Seoetal.,2024)?\n• WhatisthecontributionofeachcomponentinCQN-AS?\n• UnderwhichconditionsisCQN-ASeffective? WhendoesCQN-ASfail?\nBaselinesforfine-grainedcontroltaskswithdemonstrations Fortasksthatneedhigh-precision\ncontrol,e.g.,manipulationtasksfromBiGymandRLBench,weconsidermodel-freeRLbaselines\nthat aim to learn deterministic policies, as we find that stochastic policies struggle to solve such\nfine-grainedcontroltasks. Specifically,weconsider(i)Coarse-to-fineQ-Network(CQN;Seoetal.\n2024),avalue-basedRLalgorithmthatlearnstozoom-intocontinuousactionspaceinacoarse-to-fine\nmanner,and(ii)DrQ-v2+,anoptimizeddemo-drivenvariantofamodel-freeactor-criticalgorithm\nDrQ-v2(Yaratsetal.,2022)thatusesadeterministicpolicyalgorithmanddataaugmentation. We\nfurtherconsider(iii)ActionChunkingTransformer(ACT;Zhaoetal.2023),aBCalgorithmthat\ntrainsatransformer(Vaswanietal.,2017)policytopredictactionsequenceandutilizestemporal\nensembleforexecutingactions,asourhighly-optimizedBCbaseline.\nBaselines for whole-body control tasks with dense reward For locomotion tasks with dense\nreward,weconsider(i)SoftActor-Critic(SAC;Haarnojaetal.2018),amodel-freeactor-criticRL\nalgorithmthatmaximizesactionentropy,and(ii)Coarse-to-fineQ-Network(CQN;Seoetal.2024).\nMoreover,althoughitisnotthegoalofthispapertocompareagainstmodel-basedRLalgorithms,\nwealsoconsidertwomodel-basedbaselines: (iii)DreamerV3(Hafneretal.,2023),amodel-based\nRLalgorithmthatlearnsalatentdynamicsmodelandapolicyusingimaginedtrajectoriesand(iv)\nTD-MPC2(Hansenetal.,2024),amodel-basedRLalgorithmthatlearnsalatentdynamicsmodel\nandutilizeslocaltrajectoryoptimizationinimaginedlatenttrajectories.\nImplementationdetails Fortrainingwithexpertdemonstrations,wefollowthesetupofSeoetal.\n(2024). Specifically,wekeepaseparatereplaybufferthatstoresdemonstrationsandsamplehalf\noftrainingdatafromdemonstrations. Wealsorelabelsuccessfulonlineepisodesasdemonstrations\nandstoretheminthedemonstrationreplaybuffer. ForCQN-AS,weuseanauxiliaryBClossfrom\nSeo et al. (2024) based on large margin loss (Hester et al., 2018). For actor-critic baselines, we\nuseanauxiliaryBClossthatminimizesL2lossbetweenthepolicyoutputsandexpertactions. We\nimplementCQN-ASbasedonapubliclyavailablesourcecodeofCQN2basedonPyTorch(Paszke\netal.,2019). Wewillreleasethefullsourcecodeuponpublication.\n2https://github.com/younggyoseo/CQN\n5\nMove Plate Move Two Plates Saucepan To Hob Sandwich Flip Sandwich Remove\n100 100 100 100 100\n75 75 75 75 75\n50 50 50 50 50\n25 25 25 25 25\n0 0 0 0 0\n0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5\nDishwasher Load Plates Dishwasher Load Cups Dishwasher Unload Cutlery Take Cups Put Cups\n100 100 100 100 100\n75 75 75 75 75\n50 50 50 50 50\n25 25 25 25 25\n0 0 0 0 0\n0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5\nFlip Cup Flip Cutlery Dishwasher Close Trays Cupboards Close All Reach Target Single\n100 100 100 100 100\n75 75 75 75 75\n50 50 50 50 50\n25 25 25 25 25\n0 0 0 0 0\n0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5\nReach Target Multi Modal Reach Target Dual Dishwasher Close Wall Cupboard Open Drawers Open All\n100 100 100 100 100\n75 75 75 75 75\n50 50 50 50 50\n25 25 25 25 25\n0 0 0 0 0\n0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 1e4 2e4 3e4 4e4 5e4 0 1e4 2e4 3e4 4e4 5e4 0 1e4 2e4 3e4 4e4 5e4\nWall Cupboard Close Dishwasher Open Trays Drawers Close All Drawer Top Open Drawer Top Close\n100 100 100 100 100\n75 75 75 75 75\n50 50 50 50 50\n25 25 25 25 25\n0 0 0 0 0\n0 0.5e41e41.5e42e42.5e4 0 0.5e41e41.5e42e42.5e4 0 0.5e41e41.5e42e42.5e4 0 0.5e41e41.5e42e42.5e4 0 0.5e41e41.5e42e42.5e4\nEnvironment Steps Environment Steps Environment Steps Environment Steps Environment Steps\nRL: CQN-AS (Ours) CQN DrQ-v2+ BC: ACT\nFigure4: BiGymresultson25sparsely-rewardedmobilebi-manualmanipulationtasks(Chernyadev\net al.,2024). All experiments are initialized with17 to 60 human-collected demonstrations, and\nRLmethodsaretrainedwithanauxiliaryBCobjective. Onmanyofthechallenginglong-horizon\ntasks,CQN-ASquicklylearnstomatchtheperformanceofACT(Zhaoetal.,2023)andsurpassit\nthroughonlinelearning,whileotherRLbaselinesfailtoeffectivelyacceleratetrainingwithnoisy\nhuman-collecteddemonstrations. Wereportthesuccessrateover25episodes. Thesolidlineand\nshadedregionsrepresentthemeanandconfidenceintervals,respectively,across4runs.\n4.1 BIGYMEXPERIMENTS\nWestudyCQN-ASonmobilebi-manualmanipulationtasksfromBiGym(Chernyadevetal.,2024).\nBiGym’shuman-collecteddemonstrationsareoftennoisyandmulti-modal,posingchallengestoRL\nalgorithmswhichshouldeffectivelyleveragedemonstrationsforsolvingsparsely-rewardedtasks.\nSetup BecausewefindthatnotalldemonstrationsfromBiGymbenchmarkcanbesuccessfully\nreplayed3, we replay all the demonstrations and only use the successful ones as demonstrations.\nWedonotdiscardonesthatfailtobereplayed,butweusethemastrainingdatawithzeroreward\nbecause they can still be useful as failure experiences. To avoid training with too few demon-\nstrations, we exclude the tasks where the ratio of successful demonstrations is below 50%. This\nleavesuswith25tasks,eachwith17to60demonstrations. Forvisualobservations,weuseRGB\nobservationswith84×84resolutionfromhead,left wrist,andright wristcameras. Wealso\nuselow-dimensionalproprioceptivestatesfromproprioception,proprioception grippers,and\nproprioception floating basesensors. Weuse(i)absolutejointpositioncontrolactionmode\nand (ii) floating base that replaces locomotion with classic controllers. We use the same set of\nhyperparametersforallthetasks,inparticular,weuseactionsequenceoflength16. Moredetailson\nBiGymexperimentsareavailableinAppendixA.\n3WeusedemonstrationsavailableatthedateofOct1stwiththecommit018f8b2.\n6\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\nStand Walk Run Reach\n1000 1000 800\n8000\n800 800 600\n6000\n600 600\n400 400 400 4000\n200 200 200 2000\n0 0 0 0\n0 2e6 4e6 6e6 8e6 1e7 0 2e6 4e6 6e6 8e6 1e7 0 2e6 4e6 6e6 8e6 1e7 0 2e6 4e6 6e6 8e6 1e7\nHurdle Crawl Maze Sit Simple\n150 1000 400 1000\n800 300 800\n100\n600 600\n200\n400 400\n50\n200 100 200\n0 0 0 0\n0 2e6 4e6 6e6 8e6 1e7 0 2e6 4e6 6e6 8e6 1e7 0 2e6 4e6 6e6 8e6 1e7 0 2e6 4e6 6e6 8e6 1e7\nEnvironment Steps Environment Steps Environment Steps Environment Steps\nModel-free RL: CQN-AS (Ours) CQN SAC Model-based RL: TD-MPC2 DreamerV3\nFigure5: HumanoidBenchresultson8densely-rewardedhumanoidcontroltasks(Sferrazzaetal.,\n2024). AlltheexperimentsstartfromscratchandRLmethodsdonothaveanauxiliaryBCobjective.\nCQN-ASsignificantlyoutperformsothermodel-freeRLbaselinesonmosttasks. CQN-ASoften\nachievescompetitiveperformancetomodel-basedRLbaselines,whichisintriguingbutnotthemain\ngoalofthispaper. ForCQN-ASandCQN,wereporttheresultsaggregatedover4runs. Forother\nbaselines,wereporttheresultsaggregatedover3runsavailablefrompublicwebsite. Thesolidline\nandshadedregionsrepresentthemeanandconfidenceintervals.\nComparisontobaselines Figure4showstheexperimentalresultsonBiGymbenchmark. Wefind\nthatCQN-ASquicklymatchestheperformanceofACTandoutperformsitthroughonlinelearning\non20/25tasks,whileotherRLalgorithmsfailtodosoespeciallyonchallenginglong-horizontasks\nsuchasMovePlateandSaucepanToHob. AnotableresulthereisthatCQN-ASenablessolving\nthesechallengingBiGymtaskswhileotherRLbaselines,i.e.,CQNandDrQ-v2+,completelyfail\nastheyachieve0%successrate. ThisresulthighlightsthecapabilityofCQN-AStoaccelerateRL\ntrainingfromnoisy,multi-modaldemonstrationscollectedbyhumans.\nLimitation However,wefindthatCQN-ASstrugglestoachievemeaningfulsuccessrateonsome\nofthelong-horizontasksthatrequireinteractionwithdelicateobjectssuchascuporcutlery,leaving\nroom for future work to incorporate advanced vision encoders (He et al., 2016; 2022) or critic\narchitectures(Kapturowskietal.,2023;Chebotaretal.,2023;Springenbergetal.,2024).\n4.2 HUMANOIDBENCHEXPERIMENTS\nToshowthatCQN-AScanbegenerallyapplicabletotaskswithoutdemonstrations,westudyCQN-AS\nondensely-rewardedhumanoidcontroltasksfromHumanoidBench(Sferrazzaetal.,2024).\nSetup ForHumanoidBench,wefollowastandardsetupthattrainsRLagentsfromscratch,whichis\nalsousedinoriginalbenchmark(Sferrazzaetal.,2024). Specifically,weuselow-dimensionalstates\nconsistingofproprioceptionandprivilegedtaskinformationasinputs. Fortasks,wesimplyselectthe\nfirst8locomotiontasksinthebenchmark. FollowingtheoriginalbenchmarkthattrainsRLagents\nforenvironmentstepsthatroughlyrequires48hoursoftraining,wereporttheresultsofCQN-AS\nandCQNfor7millionsteps. Forbaselines,weusetheresultsavailablefromthepublicrepository,\nwhichareevaluatedontaskswithdexteroushands,andwealsoevaluateouralgorithmontaskswith\nhands. Weusethesamesetofhyperparametersforallthetasks,inparticular,weuseactionsequence\noflength4. MoredetailsonHumanoidBenchexperimentsareavailableinAppendixA.\nComparisontomodel-freeRLbaselines Figure5showstheresultsononHumanoidBench. We\nfindthat,bylearningthecriticnetworkwithactionsequence,CQN-ASoutperformsothermodel-free\nRLbaselines,i.e.,CQNandSAC,onmosttasks. Inparticular,thedifferencebetweenCQN-ASand\nbaselinesbecomeslargerasthetaskgetsmoredifficult,e.g.,baselinesfailtoachievehighepisode\nreturnonWalkandRuntasksbutCQN-ASachievesstrongperformance. Thisresultshowsthatour\nideaofusingactionsequencecanbeapplicabletogenericsetupwithoutdemonstrations.\n7\nnruteR\nedosipE\nnruteR\nedosipE\nTake Lid Off Saucepan Open Drawer Stack Wine Toilet Seat Up Open Microwave\n100 100 100 100 100\n75 75 75 75 75\n50 50 50 50 50\n25 25 25 25 25\n0 0 0 0 0\n0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4\nOpen Oven Take Plate Off Colored Dish Rack Turn Tap Put Money In Safe Phone On Base\n100 100 100 100 100\n75 75 75 75 75\n50 50 50 50 50\n25 25 25 25 25\n0 0 0 0 0\n0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4\nPut Books On Bookshelf Sweep To Dustpan Pick Up Cup Open Door Meat On Grill\n100 100 100 100 100\n75 75 75 75 75\n50 50 50 50 50\n25 25 25 25 25\n0 0 0 0 0\n0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4\nBasketball In Hoop Lamp On Press Switch Put Rubbish In Bin Insert Usb In Computer\n100 100 100 100 100\n75 75 75 75 75\n50 50 50 50 50\n25 25 25 25 25\n0 0 0 0 0\n0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4 0 1e4 2e4 3e4\nEnvironment Steps Environment Steps Environment Steps Environment Steps Environment Steps\nRL: CQN-AS (Ours) CQN DrQ-v2+ BC: ACT\nFigure6: RLBenchresultson20sparsely-rewardedtabletopmanipulationtasksfromRLBench\n(Jamesetal.,2020). Allexperimentsareinitializedwith100syntheticdemonstrationsgenerated\nviamotion-planningandRLmethodsaretrainedwithanauxiliaryBCobjective. Asexpected,with\nsyntheticdemonstrations,CQN-ASachievessimilarperformancetoCQNonmosttasks. However,\nCQN-ASoftensignificantlyoutperformsbaselinesonseveralchallenging,long-horizontaskssuchas\nOpenOven. Wereportthesuccessrateover25episodes. Thesolidlineandshadedregionsrepresent\nthemeanandconfidenceintervals,respectively,across4runs.\nCQN-ASoftenachievescompetitiveperformancetomodel-basedRLbaselines Whileoutper-\nformingmodel-basedRLalgorithmsisnotthegoalofthispaper,wefindthatCQN-ASoftenachieves\ncompetitiveperformancetomodel-basedRLbaselines,i.e.,DreamerV3andTD-MPC2,ontasks\nsuchasRunorSit Simple. ThisresultshowsthepotentialofourideatoenableRLagentstolearn\nusefulvaluefunctionsonchallengingtasks,withouttheneedtoexplicitlylearndynamicsmodel. We\nalsonotethatincorporatingourideaintoworldmodellearningcouldbeaninterestingdirection.\n4.3 RLBENCHEXPERIMENTS\nToinvestigatewhetherCQN-AScanalsobeeffectiveinleveragingcleandemonstrations,westudy\nCQN-ASonRLBench(Jamesetal.,2020)withsyntheticdemonstrations.\nSetup ForRLBenchexperiments,weusetheofficialCQNimplementationforcollectingdemon-\nstrationsandreproducingthebaselineresultsonthesamesetoftasks. Specifically,weuseRGB\nobservationswith84×84resolutionfromfront,wrist,left shoulder,andright shouldercam-\neras. Wealsouselow-dimensionalproprioceptivestatesconsistingof7-dimensionaljointpositions\nand a binary value for gripper open. We use 100 demonstrations and delta joint position control\nactionmode. Weusethesamesetofhyperparametersforallthetasks,inparticular,weuseaction\nsequenceoflength4. MoredetailsonRLBenchexperimentsareavailableinAppendixA.\nCQN-ASisalsoeffectivewithcleandemonstrations BecauseRLBenchprovidessyntheticclean\ndemonstrations,asweexpected,Figure6showsthatCQN-ASachievessimilarperformancetoCQN\nonmanyofthetasks,except2/25taskswhereithurtstheperformance. ButwestillfindthatCQN-AS\nachievesquitesuperiorperformancetoCQNonsomechallenginglong-horizontaskssuchasOpen\nOvenorTakePlateOffColoredDishRack. Theseresults, alongwithresultsfromBiGymand\nHumanoidBench,showthatCQN-AScanbeusedinvariousbenchmarkwithdifferentcharacteristics.\n8\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\nMove Plate Saucepan To Hob Move Plate Saucepan To Hob\n100 100 100 100\n75 75 75 75\n50 50 50 50\n25 25 25 25\n0 0 0 0\n0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5\nEnvironment Steps Environment Steps Environment Steps Environment Steps\nCQN-AS CQN-AS8 CQN-AS4 CQN-AS2 CQN-AS CQN-AS (No RL)\n(a)Effectofactionsequencelength (b)EffectofRLobjective\nSaucepan To Hob Reach Target Single Walker Run Cheetah Run\n100 100 1000 1000\n75 75 750 750\n50 50 500 500\n25 25 250 250\n0 0 0 0\n0 2e4 4e4 6e4 8e4 1e5 0 2e4 4e4 6e4 8e4 1e5 0 2e5 4e5 6e5 8e5 1e6 0 2e5 4e5 6e5 8e5 1e6\nEnvironment Steps Environment Steps Environment Steps Environment Steps\nCQN-AS CQN-AS (No Temporal Ensemble) CQN-AS8 CQN-AS4 CQN-AS2 CQN\n(c)Effectoftemporalensemble (d)Failuremode:Torquecontrol\nFigure7: Ablationstudiesandanalysisontheeffectof(a)actionsequence,(b)RLobjective,and\n(c)temporalensemble. (d)WealsoprovideresultsonlocomotiontasksfromDeepMindControl\nSuite(Tassaetal.,2020),whereCQN-ASfailstoimproveperformance. Thesolidlineandshaded\nregionsrepresentthemeanandconfidenceintervals,respectively,across4runs.\n4.4 ABLATIONSTUDIES,ANALYSIS,FAILURECASES\nEffectofactionsequencelength Figure7ashowstheperformanceofCQN-ASwithdifferent\nactionsequencelengthsontwoBiGymtasks. Wefindthattrainingthecriticnetworkwithlonger\nactionsequencesimprovesandstabilizesperformance.\nRLobjectiveiscrucialforstrongperformance Figure7bshowstheperformanceofCQN-AS\nwithoutRLobjectivethattrainsthemodelonlywithBCobjectiveonsuccessfuldemonstrations. We\nfindthisbaselinesignificantlyunderperformsCQN-AS,whichshowsthatRLobjectiveisindeed\nenablingtheagenttolearnfromonlinetrial-and-errorexperiences.\nEffectoftemporalensemble Figure7cshowsthatperformancelargelydegradeswithouttemporal\nensembleonSaucepanToHopastemporalensembleinducesasmoothmotionandthusimproves\nperformanceinfine-grainedcontroltasks. Butwealsofindthattemporalensemblecanbeharmful\nonReachTargetSingle. Wehypothesizethisisbecausetemporalensembleoftenmakesitdifficult\ntorefinebehaviorsbasedonrecentvisualobservations. Nonetheless,weusetemporalensemblefor\nallthetasksaswefindithelpsonmosttasksandweaimtousethesamesetofhyperparameters.\nFailurecase: Torquecontrol Figure7dshowsthatCQN-ASunderperformsCQNonlocomotion\ntaskswithtorquecontrol. Wehypothesizethisisbecauseasequenceofjointpositionsusuallyhasa\nsemanticmeaninginjointspaces,makingiteasiertolearnwith,whencomparedtolearninghowto\napplyasequenceoftorques. Addressingthisfailurecaseisaninterestingfuturedirection.\n5 RELATED WORK\nBehavior cloning with action sequence Recent behavior cloning approaches have shown that\npredictingasequenceofactionsenablesthepolicytoeffectivelyimitatenoisyexperttrajectoriesand\nhelpsindealingwithidleactionsfromhumanpausesduringdatacollection(Zhaoetal.,2023;Chi\netal.,2023). Inparticular,Zhaoetal.(2023)trainatransformermodel(Vaswanietal.,2017)that\npredictsactionsequenceandChietal.(2023)trainadenoisingdiffusionmodel(Hoetal.,2020)that\napproximatestheactiondistributions. Thisideahasbeenextendedtomulti-tasksetup(Bharadhwaj\netal.,2024),mobilemanipulation(Fuetal.,2024b)andhumanoidcontrol(Fuetal.,2024a). Our\nworkisinspiredbythislineofworkandproposedtolearnRLagentswithactionsequence.\n9\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\n)%(\netaR\nsseccuS\nnruteR\nedosipE\nReinforcementlearningwithactionsequence Inthecontextofreinforcementlearning,Medini&\nShrivastava(2019)proposedtopre-computefrequentactionsequencesfromexpertdemonstrations\nand augment the action space with these sequences. However, this idea introduces additional\ncomplexityandisnotscalabletosetupswithoutdemonstrations. Onerecentworkrelevanttoours\nis Saanum et al. (2024) that encourages a sequence of actions from RL agents to be predictable\nandsmooth. Butthisdiffersfromourworkinthatitusestheconceptofactionsequenceonlyfor\ncomputingthepenaltyterm. Recently,Ankileetal.(2024)pointoutthatRLwithactionsequence\nischallengingandinsteadproposestouseRLforlearningasingle-steppolicythatcorrectsaction\nsequence predictions from BC. In contrast, our work shows that training RL agents with action\nsequenceisfeasibleandleadstoimprovedperformancecomparedtopriorRLalgorithms.\n6 CONCLUSION\nWepresentedCoarse-to-fineQ-NetworkwithActionSequence(CQN-AS),avalue-basedRLalgo-\nrithmthattrainsacriticnetworkthatoutputsQ-valuesoveractionsequences. Extensiveexperiments\ninbenchmarkswithvarioussetupsshowthatourideanotonlyimprovestheperformanceofthebase\nalgorithmbutalsoallowsforsolvingcomplextaskswherepriorRLalgorithmscompletelyfail.\nWebelieveourworkwillbestrongevidencethatshowsRLcanrealizeitspromisetodeveloprobots\nthatcancontinuallyimprovethroughonlinetrial-and-errorexperiences,surpassingtheperformance\nofBCapproaches. Weareexcitedaboutfuturedirections,includingreal-worldRLwithhumanoid\nrobots,incorporatingadvancedcriticarchitectures(Kapturowskietal.,2023;Chebotaretal.,2023;\nSpringenbergetal.,2024),bootstrappingRLagentsfromimitationlearning(Huetal.,2023;Xing\netal.,2024)orofflineRL(Nairetal.,2020;Leeetal.,2021),extendingtheideatorecentmodel-based\nRLapproaches(Hafneretal.,2023;Hansenetal.,2024),tonamebutafew.\nACKNOWLEDGEMENTS\nWethankStephenJamesandRichieLoforthediscussionontheinitialideaofthisproject. Pieter\nAbbeelholdsconcurrentappointmentsasaProfessoratUCBerkeleyandasanAmazonScholar.\nThispaperdescribesworkperformedatUCBerkeleyandisnotassociatedwithAmazon.\nREFERENCES\nLarsAnkile,AnthonySimeonov,IdanShenfeld,MarcelTorne,andPulkitAgrawal. Fromimitation\ntorefinement–residualrlforprecisevisualassembly. arXivpreprintarXiv:2407.16677,2024.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450,2016.\nMarcGBellemare,WillDabney,andRe´miMunos. Adistributionalperspectiveonreinforcement\nlearning. InInternationalConferenceonMachineLearning,2017.\nHomangaBharadhwaj,JayVakil,MohitSharma,AbhinavGupta,ShubhamTulsiani,andVikashKu-\nmar. Roboagent: Generalizationandefficiencyinrobotmanipulationviasemanticaugmentations\nandactionchunking. In2024IEEEInternationalConferenceonRoboticsandAutomation(ICRA),\n2024.\nAnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,XiChen,KrzysztofChoromanski,\nTianliDing, DannyDriess, AvinavaDubey, ChelseaFinn, etal. Rt-2: Vision-language-action\nmodelstransferwebknowledgetoroboticcontrol. InConferenceonRobotLearning,2023a.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthanaGopalakrishnan, KarolHausman, AlexHerzog, JasmineHsu, etal. Rt-1: Robotics\ntransformerforreal-worldcontrolatscale. InRobotics: ScienceandSystems,2023b.\nYevgenChebotar,QuanVuong,KarolHausman,FeiXia,YaoLu,AlexIrpan,AviralKumar,Tianhe\nYu,AlexanderHerzog,KarlPertsch,etal. Q-transformer: Scalableofflinereinforcementlearning\nviaautoregressiveq-functions. InConferenceonRobotLearning,2023.\n10\nNikitaChernyadev,NicholasBackshall,XiaoMa,YunfanLu,YounggyoSeo,andStephenJames.\nBigym: A demo-driven mobile bi-manual manipulation benchmark. In Conference on Robot\nLearning,2024.\nChengChi,SiyuanFeng,YilunDu,ZhenjiaXu,EricCousineau,BenjaminBurchfiel,andShuran\nSong. Diffusionpolicy: Visuomotorpolicylearningviaactiondiffusion. InRobotics: Scienceand\nSystems,2023.\nKyunghyunCho,BartVanMerrie¨nboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,Holger\nSchwenk,andYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderfor\nstatisticalmachinetranslation. arXivpreprintarXiv:1406.1078,2014.\nZipengFu,QingqingZhao,QiWu,GordonWetzstein,andChelseaFinn. Humanplus: Humanoid\nshadowingandimitationfromhumans. InConferenceonRobotLearning,2024a.\nZipengFu,TonyZZhao,andChelseaFinn. Mobilealoha: Learningbimanualmobilemanipulation\nwithlow-costwhole-bodyteleoperation. InConferenceonRobotLearning,2024b.\nTuomasHaarnoja,AurickZhou,KristianHartikainen,GeorgeTucker,SehoonHa,JieTan,Vikash\nKumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and\napplications. arXivpreprintarXiv:1812.05905,2018.\nDanijarHafner, JurgisPasukonis, JimmyBa, andTimothyLillicrap. Masteringdiversedomains\nthroughworldmodels. arXivpreprintarXiv:2301.04104,2023.\nNicklasHansen,HaoSu,andXiaolongWang.Td-mpc2:Scalable,robustworldmodelsforcontinuous\ncontrol. InInternationalConferenceonLearningRepresentations,2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,\n2016.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dolla´r, and Ross Girshick. Masked\nautoencodersarescalablevisionlearners.InProceedingsoftheIEEE/CVFconferenceoncomputer\nvisionandpatternrecognition,2022.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415,2016.\nAlexanderHerzog,KanishkaRao,KarolHausman,YaoLu,PaulWohlhart,MengyuanYan,Jessica\nLin,MontserratGonzalezArenas,TedXiao,DanielKappler,etal. Deeprlatscale: Sortingwaste\ninofficebuildingswithafleetofmobilemanipulators. arXivpreprintarXiv:2305.03270,2023.\nToddHester,MatejVecerik,OlivierPietquin,MarcLanctot,TomSchaul,BilalPiot,DanHorgan,\nJohn Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In\nProceedingsoftheAAAIconferenceonartificialintelligence,2018.\nJonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin\nneuralinformationprocessingsystems,2020.\nHengyuanHu,SuvirMirchandani,andDorsaSadigh. Imitationbootstrappedreinforcementlearning.\narXivpreprintarXiv:2311.02198,2023.\nStephenJames,MarcFreese,andAndrewJDavison. Pyrep: Bringingv-reptodeeprobotlearning.\narXivpreprintarXiv:1906.11176,2019.\nStephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot\nlearningbenchmark&learningenvironment. IEEERoboticsandAutomationLetters,5(2):3019–\n3026,2020.\nLesliePackKaelbling,MichaelLLittman,andAnthonyRCassandra.Planningandactinginpartially\nobservablestochasticdomains. Artificialintelligence,1998.\n11\nDmitryKalashnikov,AlexIrpan,PeterPastor,JulianIbarz,AlexanderHerzog,EricJang,Deirdre\nQuillen,EthanHolly,MrinalKalakrishnan,VincentVanhoucke,etal. Scalabledeepreinforcement\nlearningforvision-basedroboticmanipulation. InConferenceonrobotlearning,2018.\nStevenKapturowski,V´ıctorCampos,RayJiang,NemanjaRakic´evic´,HadovanHasselt,Charles\nBlundell, and Adria` Puigdome`nech Badia. Human-level atari 200x faster. In International\nConferenceonLearningRepresentations,2023.\nSeunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online\nreinforcementlearningviabalancedreplayandpessimisticq-ensemble. InConferenceonRobot\nLearning,2021.\nTimothyPLillicrap, JonathanJHunt, AlexanderPritzel, NicolasHeess, TomErez, YuvalTassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In\nInternationalConferenceonLearningRepresentations,2016.\nIlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternationalConfer-\nenceonLearningRepresentations,2019.\nTharunMediniandAnshumaliShrivastava. Mimickingactionsisagoodstrategyforbeginners: Fast\nreinforcement learning with expert action sequences, 2019. URL https://openreview.net/\nforum?id=HJfxbhR9KQ.\nLuke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of\ncontinuousactionsfordeeprl. arXivpreprintarXiv:1705.05035,2017.\nVolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBellemare,\nAlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal. Human-levelcontrol\nthroughdeepreinforcementlearning. Nature,2015.\nAshvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online\nreinforcementlearningwithofflinedatasets. arXivpreprintarXiv:2006.09359,2020.\nAdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,\nhigh-performancedeeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,\n2019.\nBorisTPolyakandAnatoliBJuditsky. Accelerationofstochasticapproximationbyaveraging. SIAM\njournaloncontrolandoptimization,1992.\nDeanAPomerleau. Alvinn: Anautonomouslandvehicleinaneuralnetwork. InAdvancesinneural\ninformationprocessingsystems,1988.\nEricRohmer,SuryaPNSingh,andMarcFreese. V-rep: Aversatileandscalablerobotsimulation\nframework. InIEEE/RSJinternationalconferenceonintelligentrobotsandsystems,2013.\nTankredSaanum,Noe´miE´lteto˝,PeterDayan,MarcelBinz,andEricSchulz. Reinforcementlearning\nwithsimplesequencepriors. AdvancesinNeuralInformationProcessingSystems,2024.\nJulianSchrittwieser,IoannisAntonoglou,ThomasHubert,KarenSimonyan,LaurentSifre,Simon\nSchmitt,ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,etal. Masteringatari,\ngo,chessandshogibyplanningwithalearnedmodel. Nature,2020.\nFrankSehnke,ChristianOsendorfer,ThomasRu¨ckstieß,AlexGraves,JanPeters,andJu¨rgenSchmid-\nhuber. Parameter-exploringpolicygradients. NeuralNetworks,23(4):551–559,2010.\nYounggyoSeo,JafarUruc¸,andStephenJames. Continuouscontrolwithcoarse-to-finereinforcement\nlearning. InConferenceonRobotLearning,2024.\nTimSeyde,PeterWerner,WilkoSchwarting,IgorGilitschenski,MartinRiedmiller,DanielaRus,and\nMarkusWulfmeier. Solvingcontinuouscontrolviaq-learning. InInternationalConferenceon\nLearningRepresentations,2023.\n12\nCarmeloSferrazza,Dun-MingHuang,XingyuLin,YoungwoonLee,andPieterAbbeel. Humanoid-\nbench:Simulatedhumanoidbenchmarkforwhole-bodylocomotionandmanipulation.InRobotics:\nScienceandSystems,2024.\nDavidSilver,JulianSchrittwieser,KarenSimonyan,IoannisAntonoglou,AjaHuang,ArthurGuez,\nThomasHubert,LucasBaker,MatthewLai,AdrianBolton,etal.Masteringthegameofgowithout\nhumanknowledge. nature,2017.\nJost Tobias Springenberg, Abbas Abdolmaleki, Jingwei Zhang, Oliver Groth, Michael Bloesch,\nThomas Lampe, Philemon Brakel, Sarah Bechtle, Steven Kapturowski, Roland Hafner, et al.\nOfflineactor-criticreinforcementlearningscalestolargemodels. InInternationalConferenceon\nMachineLearning,2024.\nRichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.\nYuvalTassa,SaranTunyasuvunakool,AlistairMuldal,YotamDoron,SiqiLiu,StevenBohez,Josh\nMerel, Tom Erez, Timothy Lillicrap, and Nicolas Heess. dm control: Software and tasks for\ncontinuouscontrol. arXivpreprintarXiv:2006.12983,2020.\nOcto Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep\nDasari,JoeyHejna,TobiasKreiman,CharlesXu,etal. Octo: Anopen-sourcegeneralistrobot\npolicy. InRobotics: ScienceandSystems,2024.\nEmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-basedcontrol.\nIn2012IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems.IEEE,2012.\nAshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessingSystems,2017.\nZiyuWang,TomSchaul,MatteoHessel,HadoHasselt,MarcLanctot,andNandoFreitas. Dueling\nnetworkarchitecturesfordeepreinforcementlearning. InInternationalconferenceonmachine\nlearning,2016.\nJiaxuXing,AngelRomero,LeonardBauersfeld,andDavideScaramuzza. Bootstrappingreinforce-\nment learning with imitation for vision-based agile flight. In Conference on Robot Learning,\n2024.\nDenisYarats,RobFergus,AlessandroLazaric,andLerrelPinto. Masteringvisualcontinuouscontrol:\nImproved data-augmented reinforcement learning. In International Conference on Learning\nRepresentations,2022.\nTonyZZhao, VikashKumar, SergeyLevine, andChelseaFinn. Learningfine-grainedbimanual\nmanipulationwithlow-costhardware. InRobotics: ScienceandSystems,2023.\n13\nA EXPERIMENTAL DETAILS\nBiGym BiGym4(Chernyadevetal.,2024)isbuiltuponMuJoCo(Todorovetal.,2012). Weuse\nUnitreeH1withtwoparallelgrippers. Wefindthatdemonstrationsavailableintherecentversionof\nBiGymarenotallsuccessful. Thereforeweadoptthestrategyofreplayingallthedemonstrationsand\nonlyusethesuccessfulonesasdemonstrations. insteadofdiscardingthefaileddemonstrations,we\nstillstoretheminareplaybufferasfailureexperiences.Toavoidtrainingwithtoofewdemonstrations,\nweexcludethetaskswheretheratioofsuccessfuldemonstrationsisbelow50%. Table1showsthe\nlistof25sparsely-rewardedmobilebi-manualmanipulationtasksusedinourexperiments.\nTable1: BiGymtaskswiththeirmaximumepisodelengthandnumberofsuccessfuldemonstrations.\nTask Length Demos Task Length Demos\nMovePlate 300 51 CupboardsCloseAll 620 53\nMoveTwoPlates 550 30 ReachTargetSingle 100 30\nSaucepanToHob 440 28 ReachTargetMultiModal 100 60\nSandwichFlip 620 34 ReachTargetDual 100 50\nSandwichRemove 540 24 DishwasherClose 375 44\nDishwasherLoadPlates 560 17 WallCupboardOpen 300 44\nDishwasherLoadCups 750 58 DrawersOpenAll 480 45\nDishwasherUnloadCutlery 620 29 WallCupboardClose 300 60\nTakeCups 420 32 DishwasherOpenTrays 380 57\nPutCups 425 43 DrawersCloseAll 200 59\nFlipCup 550 45 DrawerTopOpen 200 40\nFlipCutlery 500 43 DrawerTopClose 120 51\nDishwasherCloseTrays 320 62\nHumanoidBench HumanoidBench5(Sferrazzaetal.,2024)isbuiltuponMuJoCo(Todorovetal.,\n2012). WeuseUnitreeH1withtwodexteroushands. Weconsiderthefirst8locomotiontasksinthe\nbenchmark: Stand,Walk,Run,Reach,Hurdle,Crawl,Maze,SitSimple. Weuseproprioceptive\nstatesandprivilegedtaskinformationinsteadofvisualobservations. UnlikeBiGymandRLBench\nexperiments,wedonotutilizeduelingnetwork(Wangetal.,2016)anddistributionalcritic(Bellemare\netal.,2017)inHumanoidBenchforfasterexperimentation.\nRLBench RLBench6 (Jamesetal.,2020)isbuiltuponCoppeliaSim(Rohmeretal.,2013)and\nPyRep(Jamesetal.,2019).Weusea7-DoFFrankaPandarobotarmandaparallelgripper.Following\nthesetupofSeoetal.(2024),weincreasethevelocityandaccelerationofthearmby2times. Forall\nexperiments,weuse100demonstrationsgeneratedviamotion-planning. Table2showsthelistof20\nsparsely-rewardedvisualmanipulationtasksusedinourexperiments.\nTable2: RLBenchtaskswiththeirmaximumepisodelengthusedinourexperiments.\nTask Length Task Length\nTakeLidOffSaucepan 100 PutBooksOnBookshelf 175\nOpenDrawer 100 SweepToDustpan 100\nStackWine 150 PickUpCup 100\nToiletSeatUp 150 OpenDoor 125\nOpenMicrowave 125 MeatOnGrill 150\nOpenOven 225 BasketballInHoop 125\nTakePlateOff\n150 LampOn 100\nColoredDishRack\nTurnTap 125 PressSwitch 100\nPutMoneyInSafe 150 PutRubbishInBin 150\nPhoneonBase 175 InsertUsbInComputer 100\n4https://github.com/chernyadev/bigym\n5https://github.com/carlosferrazza/humanoid-bench\n6https://github.com/stepjam/RLBench\n14\nHyperparameters Weusethesamesetofhyperparametersacrossthetasksineachdomain. For\nhyperparameters shared across CQN and CQN-AS, we use the same hyperparameters for both\nalgorithmsforafaircomparison. WeprovidedetailedhyperparametersforBiGymandRLBench\nexperimentsinTable3andhyperparametersforHumanoidBenchexperimentsinTable4\nTable3: Hyperparametersfordemo-drivenvision-basedexperimentsinBiGymandRLBench\nHyperparameter Value\nImageresolution 84×84×3\nImageaugmentation RandomShift(Yaratsetal.,2022)\nFramestack 4(BiGym)/8(RLBench)\nCNN-Architecture Conv(c=[32,64,128,256],s=2,p=1)\nLinear(c=[512,512,64,512,512],bias=False)(BiGym)\nMLP-Architecture\nLinear(c=[64,512,512],bias=False)(RLBench)\nCNN&MLP-Activation SiLU(Hendrycks&Gimpel,2016)andLayerNorm(Baetal.,2016)\nGRU-Architecture GRU(c=[512],bidirectional=False)\nDuelingnetwork True\nC51-Atoms 51\nC51-v ,v -2,2\nmin max\nActionsequence 16(BiGym)/4(RLBench)\nTemporalensembleweightm 0.01\nLevels 3\nBins 5\nBCloss(L )scale 1.0\nBC\nRLloss(L )scale 0.1\nRL\nRelabelingasdemonstrations True\nData-drivenactionscaling True\nActionmode AbsoluteJoint(BiGym),DeltaJoint(RLBench)\nExplorationnoise ϵ∼N(0,0.01)\nTargetcriticupdateratio(τ) 0.02\nN-stepreturn 1\nBatchsize 128(BiGym)/256(RLBench)\nDemobatchsize 128(BiGym)/256(RLBench)\nOptimizer AdamW(Loshchilov&Hutter,2019)\nLearningrate 5e-5\nWeightdecay 0.1\nTable4: Hyperparametersforstate-basedexperimentsinHumanoidBench\nHyperparameter Value\nMLP-Architecture Linear(c=[512,512],bias=False)\nCNN&MLP-Activation SiLU(Hendrycks&Gimpel,2016)andLayerNorm(Baetal.,2016)\nGRU-Architecture GRU(c=[512],bidirectional=False)\nDuelingnetwork False\nActionsequence 4\nTemporalensembleweightm 0.01\nLevels 3\nBins 5\nRLloss(L )scale 1.0\nRL\nActionmode AbsoluteJoint\nExplorationnoise ϵ∼N(0,0.01)\nTargetcriticupdateratio(τ) 1.0\nTargetcriticupdateinterval(τ) 1000\nN-stepreturn 3\nBatchsize 128\nOptimizer AdamW(Loshchilov&Hutter,2019)\nLearningrate 5e-5\nWeightdecay 0.1\n15\nComputinghardware Forallexperiments,weuseconsumer-grade11GBGPUssuchasNVIDIA\nGTX1080Ti,NVIDIATitanXP,andNVIDIARTX2080Tiwith11or12GBVRAM.With2080Ti\nGPU,eachBiGymexperimentwith100Kenvironmentstepstake9.5hours,eachRLBenchexperi-\nmentwith30Kenvironmentstepstake6.5hours,andeachHumanoidBenchexperimentwith7M\nenvironmentstepstake48hours. WefindthatCQN-ASisaround33%slowerthanrunningCQN\nbecauselargerarchitectureslowsdownbothtrainingandinference.\nBaseline implementation For CQN (Seo et al., 2024) and DrQ-v2+ (Yarats et al., 2022), we\nuse the implementation available from the official CQN implementation7. For ACT (Zhao et al.,\n2023), weusetheimplementationfromRoboBaserepository8. ForSAC(Haarnojaetal.,2018),\nDreamerV3(Hafneretal.,2023),andTD-MPC2(Hansenetal.,2024),weuseresultsprovidedin\nHumanoidBench9repository(Sferrazzaetal.,2024).\nB FULL DESCRIPTION OF CQN AND CQN-AS\nThissectionprovidestheformulationofCQNandCQN-ASwithn-dimensionalactions.\nB.1 COARSE-TO-FINEQ-NETWORK\nLetal,nbeanactionatlevellanddimensionnandal ={al,1,...,al,N}beactionsatlevellwitha0\nt t t t t\nbeingzerovector. Wethendefinecoarse-to-finecritictoconsistofmultipleQ-networks:\nQl,n(h ,al,n,al−1)forl∈{1,...,L}andn∈{1,...,N} (5)\nθ t t t\nWeoptimizethecriticnetworkwiththefollowingobjective:\n(cid:88) (cid:88) (cid:16) (cid:17)2\nQl,n(h ,al,n,al−1)−r −γmaxQl,n(h ,a′,πl(h ) , (6)\nn l θ t t t t+1 a′ θ¯ t+1 t+1\nwhereθ¯aredelayedparametersforatargetnetwork(Polyak&Juditsky,1992)andπlisapolicythat\noutputstheactional ateachlevellviatheinferencestepswithourcritic,i.e.,πl(h )=al.\nt t t\nActioninference Tooutputactionsattimesteptwiththecritic,CQNfirstinitializesconstants\nan,lowandan,highwith−1and1foreachn. Thenthefollowingstepsarerepeatedforl∈{1,...,L}:\nt t\n• Step1(Discretization): Discretizeaninterval[an,low,an,high]intoBuniformintervals,and\nt t\neachoftheseintervalsbecomeanactionspaceforQl,n\nθ\n• Step2(Binselection): FindthebinwiththehighestQ-value,setal,ntothecentroidofthe\nt\nselectedbin,andaggregateactionsfromalldimensionstoal\nt\n• Step3(Zoom-in): Setan,lowandan,hightotheminimumandmaximumoftheselectedbin,\nt t\nwhichintuitivelycanbeseenaszooming-intoeachbin.\nWethenusethelastlevel’sactionaLastheactionattimestept.\nt\nComputingQ-values TocomputeQ-valuesforgivenactionsa ,CQNfirstinitializesconstants\nt\nan,lowandan,highwith−1and1foreachn. Wethenrepeatthefollowingstepsforl∈{1,...,L}:\nt t\n• Step1(Discretization): Discretizeaninterval[an,low,an,high]intoBuniformintervals,and\nt t\neachoftheseintervalsbecomeanactionspaceforQl,n\nθ\n• Step 2 (Bin selection): Find the bin that contains input action a , compute al,n for the\nt t\nselectedinterval,andcomputeQ-valuesQl,n(h ,al,n,al−1).\nθ t t t\n• Step3(Zoom-in): Setan,lowandan,hightotheminimumandmaximumoftheselectedbin,\nt t\nwhichintuitivelycanbeseenaszooming-intoeachbin.\nWethenuseasetofQ-values{Ql,n(h ,al,n,al−1)}L forgivenactionsa .\nθ t t t l=1 t\n7https://github.com/younggyoseo/CQN\n8https://github.com/robobase-org/robobase\n9https://github.com/carlosferrazza/humanoid-bench\n16\nB.2 COARSE-TO-FINECRITICWITHACTIONSEQUENCE\nLetal ={al,...,al }beanactionsequenceatlevellanda0 bezerovector. Ourcritic\nt:t+K t t+K−1 t:t+K\nnetworkconsistsofmultipleQ-networksforeachlevell,dimensionn,andsequencestepk:\nQl,n,k(h ,al,n ,al−1 )for l∈{1,...,L}, n∈{1,...,N}andk ∈{1,...,K} (7)\nθ t t+k−1 t:t+K\nWeoptimizethecriticnetworkwiththefollowingobjective:\n(cid:88)(cid:88)(cid:88)(cid:16) (cid:17)2\nQl,n,k(h ,al,n,al−1 )−r −γmaxQl,n,k(h ,a′,πl (h ) , (8)\nθ t t t:t+K t+1\na′\nθ¯ t+1 K t+1\nn l k\nwhere πl is an action sequence policy that outputs the action sequence al . In practice, we\nK t:t+K\ncomputeQ-valuesforallsequencestepk ∈{1,...,K}andallactiondimensionn∈{1,...,N}in\nparallel. ThiscanbeseenasextendingtheideaofSeydeetal.(2023),whichlearnsdecentralized\nQ-networksforactiondimensions,intoactionsequencedimension. AswementionedinSection3.1,\nwefindthissimpleschemeworkswellonchallengingtaskswithhigh-dimensionalactionspaces.\nArchitecture Lete denoteanone-hotencodingfork. Foreachlevell,weconstructfeaturesfor\nk\neachsequencestepkashl =(cid:2) h ,al−1 ,e (cid:3) . Weencodeeachhl withasharedMLPnetwork\nt,k t t+k−1 k t,k\nandprocessthemthroughGRU(Choetal.,2014)toobtainsl =fGRU(fMLP(hl ),...,fMLP(hl )).\nt,k θ θ t,1 θ t,k\nWe use a shared projection layer to map each sl into Q-values at each sequence step k, i.e.,\nt,k\n{Ql,k(o ,al,n ,al−1 )}N =fproj(sl ).WenotethatwecomputeQ-valuesforalldimensions\nθ t t+k−1 t:t+K n=1 θ t,k\nn∈{1,...,N}atthesametimewithabiglinearlayer,whichfollowsthedesignofSeoetal.(2024).\n17",
    "pdf_filename": "Reinforcement_Learning_with_Action_Sequence_for_Data-Efficient_Robot_Learning.pdf"
}