{
    "title": "Reinforcement Learning with Action Sequence for Data-Efficient Robot Learning",
    "context": "Training reinforcement learning (RL) agents on robotic tasks typically requires a large number of training samples. This is because training data often consists of noisy trajectories, whether from exploration or human-collected demonstrations, making it difficult to learn value functions that understand the effect of taking each action. On the other hand, recent behavior-cloning (BC) approaches have shown that predicting a sequence of actions enables policies to effectively approximate noisy, multi-modal distributions of expert demonstrations. Can we use a similar idea for improving RL on robotic tasks? In this paper, we introduce a novel RL algorithm that learns a critic network that outputs Q-values over a sequence of actions. By explicitly training the value functions to learn the consequence of executing a series of current and future actions, our algorithm allows for learning useful value functions from noisy trajectories. We study our algorithm across various setups with sparse and dense rewards, and with or without demonstrations, spanning mobile bi-manual manipulation, whole-body control, and tabletop manipulation tasks from BiGym, HumanoidBench, and RLBench. We find that, by learning the critic network with action sequences, our algorithm outperforms various RL and BC baselines, in particular on challenging humanoid control tasks. Project website: younggyo.me/cqn-as. 0 2e4 4e4 6e4 8e4 1e5 Environment Steps 0 25 50 75 100 Success Rate (%) BiGym (25 Tasks) 0 2e6 4e6 6e6 8e6 1e7 Environment Steps 0 250 500 750 1000 Episode Return HumanoidBench (8 Tasks) 0 1e4 2e4 3e4 Environment Steps 0 25 50 75 100 Success Rate (%) RLBench (20 Tasks) RL: CQN-AS (Ours) CQN DrQ-v2+ SAC BC: ACT Figure 1: Summary of results. Coarse-to-fine Q-Network with Action Sequence (CQN-AS) is a value-based RL algorithm that learns a critic network with action sequence. We study CQN-AS on 53 robotic tasks from BiGym (Chernyadev et al., 2024), HumanoidBench (Sferrazza et al., 2024), and RLBench (James et al., 2020), where prior model-free RL algorithms struggle to achieve competitive performance. We show that CQN-AS outperforms various RL and BC baselines such as CQN (Seo et al., 2024), DrQ-v2+ (Yarats et al., 2022), SAC (Haarnoja et al., 2018), and ACT (Zhao et al., 2023). 1 Reinforcement learning (RL) holds the promise of continually improving policies through online trial- and-error experiences (Sutton & Barto, 2018), making it an ideal choice for developing robots that can adapt to various environments. However, despite this promise, training RL agents on robotic tasks typically requires a prohibitively large number of training samples (Kalashnikov et al., 2018; Herzog et al., 2023), which becomes problematic as deploying robots often incurs a huge cost. Therefore many of the recent successful approaches on robot learning have been based on behavior-cloning (BC; Pomerleau 1988), which can learn strong policies from offline expert demonstrations (Brohan et al., 2023b;a; Zhao et al., 2023; Chi et al., 2023; Team et al., 2024; Fu et al., 2024a). 1 arXiv:2411.12155v1  [cs.LG]  19 Nov 2024",
    "body": "REINFORCEMENT LEARNING WITH ACTION SEQUENCE\nFOR DATA-EFFICIENT ROBOT LEARNING\nYounggyo Seo\nUC Berkeley\nyounggyo.seo@berkeley.edu\nPieter Abbeel\nUC Berkeley\npabbeel@cs.berkeley.edu\nABSTRACT\nTraining reinforcement learning (RL) agents on robotic tasks typically requires a\nlarge number of training samples. This is because training data often consists of\nnoisy trajectories, whether from exploration or human-collected demonstrations,\nmaking it difficult to learn value functions that understand the effect of taking each\naction. On the other hand, recent behavior-cloning (BC) approaches have shown\nthat predicting a sequence of actions enables policies to effectively approximate\nnoisy, multi-modal distributions of expert demonstrations. Can we use a similar idea\nfor improving RL on robotic tasks? In this paper, we introduce a novel RL algorithm\nthat learns a critic network that outputs Q-values over a sequence of actions. By\nexplicitly training the value functions to learn the consequence of executing a\nseries of current and future actions, our algorithm allows for learning useful value\nfunctions from noisy trajectories. We study our algorithm across various setups\nwith sparse and dense rewards, and with or without demonstrations, spanning\nmobile bi-manual manipulation, whole-body control, and tabletop manipulation\ntasks from BiGym, HumanoidBench, and RLBench. We find that, by learning the\ncritic network with action sequences, our algorithm outperforms various RL and\nBC baselines, in particular on challenging humanoid control tasks.\nProject website: younggyo.me/cqn-as.\n0\n2e4\n4e4\n6e4\n8e4\n1e5\nEnvironment Steps\n0\n25\n50\n75\n100\nSuccess Rate (%)\nBiGym (25 Tasks)\n0\n2e6\n4e6\n6e6\n8e6\n1e7\nEnvironment Steps\n0\n250\n500\n750\n1000\nEpisode Return\nHumanoidBench (8 Tasks)\n0\n1e4\n2e4\n3e4\nEnvironment Steps\n0\n25\n50\n75\n100\nSuccess Rate (%)\nRLBench (20 Tasks)\nRL: \nCQN-AS (Ours)\nCQN\nDrQ-v2+\nSAC\nBC: \nACT\nFigure 1: Summary of results. Coarse-to-fine Q-Network with Action Sequence (CQN-AS) is a\nvalue-based RL algorithm that learns a critic network with action sequence. We study CQN-AS on 53\nrobotic tasks from BiGym (Chernyadev et al., 2024), HumanoidBench (Sferrazza et al., 2024), and\nRLBench (James et al., 2020), where prior model-free RL algorithms struggle to achieve competitive\nperformance. We show that CQN-AS outperforms various RL and BC baselines such as CQN (Seo\net al., 2024), DrQ-v2+ (Yarats et al., 2022), SAC (Haarnoja et al., 2018), and ACT (Zhao et al., 2023).\n1\nINTRODUCTION\nReinforcement learning (RL) holds the promise of continually improving policies through online trial-\nand-error experiences (Sutton & Barto, 2018), making it an ideal choice for developing robots that\ncan adapt to various environments. However, despite this promise, training RL agents on robotic tasks\ntypically requires a prohibitively large number of training samples (Kalashnikov et al., 2018; Herzog\net al., 2023), which becomes problematic as deploying robots often incurs a huge cost. Therefore\nmany of the recent successful approaches on robot learning have been based on behavior-cloning\n(BC; Pomerleau 1988), which can learn strong policies from offline expert demonstrations (Brohan\net al., 2023b;a; Zhao et al., 2023; Chi et al., 2023; Team et al., 2024; Fu et al., 2024a).\n1\narXiv:2411.12155v1  [cs.LG]  19 Nov 2024\n\nCNN\nCNN\nProprio States\nMLP\nActions from\nPrevious Level\nMLP\nMLP\nInputs & Encoding\nLevel Index\n[0, 1, …, 0]\nConcat & Linear\nGRU\nCoarse-to-fine Critic with Action Sequence\nQ-Values over \nAction Sequence\nCoarse-to-fine Critic with\nAction Sequence\nInputs\nActions from \nPrevious Level\nRepeat\nL Times\nAction Sequence from Level L\nAction Inference\nFigure 2: Coarse-to-fine Q-network with action sequence. (Left) Our key idea is to train a critic\nnetwork to output Q-values over a sequence of actions. We design our architecture to first obtain\nfeatures for each sequence step and aggregate features from multiple sequence steps with a recurrent\nnetwork. We then project these outputs into Q-values at level l. (Right) For action inference, we\nrepeat the procedure of computing Q-values for level l ∈{1, ..., L}. We then find the action sequence\nwith the highest Q-values from the last level L, and use it for controlling robots at each time step.\nOne cause for the poor data-efficiency of RL algorithms on robotic tasks is that training data consists\nof noisy trajectories. When collecting data for training RL agents, we typically inject some noise into\nactions for exploration (Sehnke et al., 2010; Lillicrap et al., 2016) that may induce trajectories with\njerky motions. Moreover, we often initialize training with human-collected demonstrations that can\nconsist of noisy multi-modal trajectories (Chernyadev et al., 2024). Such noisy data distributions\nmake it difficult to learn value functions that should understand the consequence of taking each\naction. We indeed find that prior RL algorithms perform much worse than the BC baseline on mobile\nbi-manual manipulation tasks with human-collected demonstrations when compared to a tabletop\nmanipulation setup with synthetic demonstrations collected via motion-planning (see Figure 1).\nOn the other hand, recent BC approaches have shown that predicting a sequence of actions enables\npolicies to effectively approximate the noisy, multi-modal distribution of expert demonstrations (Zhao\net al., 2023; Chi et al., 2023). Inspired by this, in this paper, we investigate how to use a similar\nidea for improving the data-efficiency of RL algorithms on robotic tasks. In particular, we present\na novel RL algorithm that learns a critic network that outputs Q-values over a sequence of actions\n(see Figure 2). By training the critic network to explicitly learn the consequence of taking a series\nof current and future actions, our algorithm enables the RL agents to effectively learn useful value\nfunctions from noisy trajectories. We build this algorithm upon a recent value-based RL algorithm\nthat learns RL agents to zoom-into continuous action space in a coarse-to-fine manner (Seo et al.,\n2024), thus we refer to our algorithm as Coarse-to-fine Q-Network with Action Sequence (CQN-AS).\nTo evaluate the generality and capabilities of CQN-AS, we study CQN-AS on various setups with\nsparse and dense rewards, and with or without demonstrations. In BiGym benchmark (Chernyadev\net al., 2024), which provides human-collected demonstrations for mobile bi-manual manipulation\ntasks, CQN-AS outperforms various model-free RL and BC baselines (Yarats et al., 2022; Zhao\net al., 2023; Seo et al., 2024). Moreover, in HumanoidBench (Sferrazza et al., 2024), which consists\nof densely-rewarded humanoid control tasks, we show that CQN-AS can also be effective without\ndemonstrations, outperforming prior model-free RL baselines (Haarnoja et al., 2018; Seo et al., 2024).\nFinally, in RLBench (James et al., 2020), which provides synthetic demonstrations generated via\nmotion-planning, CQN-AS achieves similar performance as model-free RL and BC baselines on\nmost tasks, but significantly better performance on several long-horizon manipulation tasks.\n2\nBACKGROUND\nProblem setup\nWe mainly consider a robotic control problem which we formulate as a partially\nobservable Markov decision process (Kaelbling et al., 1998; Sutton & Barto, 2018). At each time\nstep t, an RL agent encounters an observation ot, executes an action at, receives a reward rt+1,\nand encounters a new observation ot+1 from the environment. Because the observation ot does not\ncontain full information about the internal state of the environment, in this work, we use a stack of\npast observations as inputs to the RL agent by following the common practice in Mnih et al. (2015).\n2\n\nFor simplicity, we omit the notation for these stacked observations. When the environment is fully\nobservable, we simply use ot as inputs. Our goal in this work is to train a policy π that maximizes the\nexpected sum of rewards through RL while using as few samples as possible, optionally with access\nto a modest amount of expert demonstrations collected either by motion-planners or by humans.\nInputs and encoding\nGiven visual observations ov\nt = {ov1\nt , ..., ovM\nt\n} from M cameras, we encode\neach ovi\nt using convolutional neural networks (CNN) into hvi\nt . We then process them through a series\nof linear layers to fuse them into hv\nt . If low-dimensional observations olow\nt\nare available along with\nvisual observations, we process them through a series of linear layers to obtain hlow\nt\n. We then use\nconcatenated features ht = [hv\nt , hlow\nt\n] as inputs to the critic network. In domains without vision\nsensors, we simply use olow\nt\nas ht without encoding the low-dimensional observations.\nCoarse-to-fine Q-Network\nCoarse-to-fine Q-Network (CQN; Seo et al. 2024) is a value-based RL\nalgorithm for continuous control that trains RL agents to zoom-into the continuous action space in a\ncoarse-to-fine manner. In particular, CQN iterates the procedures of (i) discretizing the continuous\naction space into multiple bins and (ii) selecting the bin with the highest Q-value to further discretize.\nThis reformulates the continuous control problem as a multi-level discrete control problem, allowing\nfor the use of ideas from sample-efficient value-based RL algorithms (Mnih et al., 2015; Silver et al.,\n2017; Schrittwieser et al., 2020), designed to be used with discrete actions, for continuous control.\nFormally, let al\nt be an action at level l with a0\nt being the zero vector1. We then define the coarse-to-fine\ncritic to consist of multiple Q-networks which compute Q-values for actions at each level al\nt, given\nthe features ht and actions from the previous level al−1\nt\n, as follows:\nQl\nθ(ht, al\nt, al−1\nt\n) for l ∈{1, ..., L}\n(1)\nWe optimize each Q-network at level l with the following objective:\nLl =\n\u0010\nQl\nθ(ht, al\nt, al−1\nt\n) −rt+1 −γ max\na′ Ql\n¯θ(ht+1, a′, πl(ht+1)\n\u0011\n,\n(2)\nwhere ¯θ are delayed parameters for a target network (Polyak & Juditsky, 1992) and πl is a policy\nthat outputs the action al\nt at each level l via the inference steps with our critic, i.e., πl(ht) = al\nt.\nSpecifically, to output actions at time step t with the critic, CQN first initializes constants alow\nt\nand\nahigh\nt\nwith −1 and 1. Then the following steps are repeated for l ∈{1, ..., L}:\n• Step 1 (Discretization): Discretize an interval [alow\nt\n, ahigh\nt\n] into B uniform intervals, and each\nof these intervals become an action space for Ql\nθ\n• Step 2 (Bin selection): Find a bin with the highest Q-value and set al\nt to the centroid of the bin.\n• Step 3 (Zoom-in): Set alow\nt\nand ahigh\nt\nto the minimum and maximum of the selected bin, which\nintuitively can be seen as zooming-into each bin.\nWe then use the last level’s action aL\nt as the action at time step t. For more details, including the\ninference procedure for computing Q-values, we refer readers to Appendix B.\n3\nCOARSE-TO-FINE Q-NETWORK WITH ACTION SEQUENCE\nWe present Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a value-based RL algorithm\nthat learns a critic network that outputs Q-values for a sequence of actions at:t+K = {at, ..., at+K−1}\nfor a given observation ot. Our main motivation for this design comes from one of the key ideas\nin recent behavior-cloning (BC) approaches, i.e., predicting action sequences, which helps resolve\nambiguity when approximating noisy, multi-modal distributions of expert demonstrations (Zhao et al.,\n2023; Chi et al., 2023). Similarly, by explicitly learning Q-values of both current and future actions\nfrom the given state, our approach aims to mitigate the challenge of learning Q-values with noisy\ntrajectories from exploratory behaviors or human-collected demonstrations.\nThis section describes how we design our critic network with action sequence (see Section 3.1) and\nhow we utilize action sequence outputs to control robots at each time step (see Section 3.2). The\noverview of our algorithm is available in Figure 2.\n1For simplicity, we describe CQN and CQN-AS with a single-dimensional action in the main section. See\nAppendix B for full description with N-dimensional actions, which is straightforward but requires more indices.\n3\n\n3.1\nCOARSE-TO-FINE CRITIC WITH ACTION SEQUENCE\nOur key idea is to design a critic network to explicitly learn Q-values for current action and future\nactions from the current time step t, i.e., {Q(ot, at), Q(ot, at+1), ..., Q(ot, at+K−1)}, to enable the\ncritic to understand the consequence of executing a series of actions from the given state.\nFormulation and objective\nLet al\nt:t+K = {al\nt, ..., al\nt+K−1} be an action sequence at level l and\na0\nt:t+K be a zero vector. We design our coarse-to-fine critic network to consist of multiple Q-networks\nthat compute Q-values for each action at sequence step k and level l:\nQl,k\nθ (ht, al\nt+k−1, al−1\nt:t+K) for l ∈{1, ..., L} and k ∈{1, ..., K}\n(3)\nWe optimize our critic network with the following objective:\nX\nk\nX\nl\n\u0010\nQl,k\nθ (ht, al\nt+k−1, al−1\nt:t+K) −rt+1 −γ max\na′ Ql,k\n¯θ (ht+1, a′, πl\nK(ht+1)\n\u00112\n,\n(4)\nwhere πl\nK is an action sequence policy that outputs the action sequence al\nt:t+K. In practice, we\ncompute Q-values for all sequence step k ∈{1, ..., K} in parallel, which is possible because Q-values\nfor future actions depend only on current features ht but not on Q-values for previous actions. We\nfind this simple design, with independence across action sequence, works well even on challenging\nhumanoid control tasks with high-dimensional action spaces (Sferrazza et al., 2024). We expect our\nidea can be strengthened by exploiting the sequential structure, i.e., Q-values at subsequent steps\ndepend on previous Q-values (Metz et al., 2017; Chebotar et al., 2023), but we leave it as future work.\nArchitecture\nWe implement our critic network to initially extract features for each sequence step k\nand aggregate features from multiple steps with a recurrent network (see Figure 2). This architecture\nis often helpful in cases where a single-step action is already high-dimensional so that concatenating\nthem make inputs too high-dimensional. Specifically, let ek denote an one-hot encoding for k. At\neach level l, we construct features for each sequence step k as hl\nt,k =\n\u0002\nht, al−1\nt+k−1, ek\n\u0003\n. We then\nencode each hl\nt,k with a shared MLP network and process them through GRU (Cho et al., 2014) to\nobtain sl\nt,k = f GRU\nθ\n(f MLP\nθ\n(hl\nt,1), ..., f MLP\nθ\n(hl\nt,k)). We then use a shared projection layer to map each\nsl\nt,k into Q-values at each sequence step k, i.e., Ql,k\nθ (ot, al\nt+k−1, al−1\nt:t+K) = f proj\nθ\n(sl\nt,k).\n3.2\nACTION EXECUTION AND TRAINING DETAILS\nWhile the idea of using action sequence is simple, there are two important yet small details for\neffectively training RL agents with action sequence: (i) how we execute actions at each time step to\ncontrol robots and (ii) how we store training data and sample batches for training.\nExecuting action with temporal ensemble\nWith the policy that outputs an action sequence at:t+K,\none important question is how to execute actions at time step i ∈{t, ..., t + K −1}. For this, we use\nthe idea of Zhao et al. (2023) that utilizes temporal ensemble, which computes at:t+K every time step,\nsaves it to a buffer, and executes a weighted average P\ni wiat−i/ P wi where wi = exp(−m ∗i)\ndenotes a weight that assigns higher value to more recent actions. We find this scheme outperforms\nthe alternative of computing at:t+K every K steps and executing each action for subsequent K steps\non most tasks we considered, except on several tasks that need reactive control.\nStoring training data from environment interaction\nWhen storing samples from online envi-\nronment interaction, we store a transition (ot, ˆat, rt+1, ot+1) where ˆat denotes an action executed at\ntime step t. For instance, if we use temporal ensemble for action execution, ˆat is a weighted average\nof action outputs obtained from previous K time steps.\nSampling training data from a replay buffer\nWhen sampling training data from the replay buffer,\nwe sample a transition with action sequence, i.e., (ot, ˆat:t+K, rt+1, ot+1). If we sample time step t\nnear the end of episode so that we do not have enough data to construct a full action sequence, we fill\nthe action sequence with null actions. In particular, in position control where we specify the position\nof joints or end effectors, we repeat the action from the last step so that the agent learns not to change\nthe position. On the other hand, in torque control where we specify the force to apply to joints, we\nset the action after the last step to zero so that agent learns to not to apply force.\n4\n\nFigure 3: Examples of robotic tasks. We study CQN-AS on 53 robotic tasks spanning mobile bi-\nmanual manipulation, whole-body control, and tabletop manipulation tasks from BiGym (Chernyadev\net al., 2024), HumanoidBench (Sferrazza et al., 2024), and RLBench (James et al., 2020).\n4\nEXPERIMENT\nWe study CQN-AS on 53 robotic tasks spanning mobile bi-manual manipulation, whole-body control,\nand tabletop manipulation tasks from BiGym (Chernyadev et al., 2024), HumanoidBench (Sferrazza\net al., 2024), and RLBench (James et al., 2020) environments (see Figure 3 for examples of robotic\ntasks). These tasks with sparse and dense rewards, with or without vision sensors, and with or without\ndemonstrations, allow for evaluating the capabilities and limitations of our algorithm. In particular,\nour experiments are designed to investigate the following questions:\n• Can CQN-AS quickly match the performance of a recent BC algorithm (Zhao et al., 2023)\nand surpass it through online learning? How does CQN-AS compare to previous model-free\nRL algorithms (Haarnoja et al., 2018; Yarats et al., 2022; Seo et al., 2024)?\n• What is the contribution of each component in CQN-AS?\n• Under which conditions is CQN-AS effective? When does CQN-AS fail?\nBaselines for fine-grained control tasks with demonstrations\nFor tasks that need high-precision\ncontrol, e.g., manipulation tasks from BiGym and RLBench, we consider model-free RL baselines\nthat aim to learn deterministic policies, as we find that stochastic policies struggle to solve such\nfine-grained control tasks. Specifically, we consider (i) Coarse-to-fine Q-Network (CQN; Seo et al.\n2024), a value-based RL algorithm that learns to zoom-into continuous action space in a coarse-to-fine\nmanner, and (ii) DrQ-v2+, an optimized demo-driven variant of a model-free actor-critic algorithm\nDrQ-v2 (Yarats et al., 2022) that uses a deterministic policy algorithm and data augmentation. We\nfurther consider (iii) Action Chunking Transformer (ACT; Zhao et al. 2023), a BC algorithm that\ntrains a transformer (Vaswani et al., 2017) policy to predict action sequence and utilizes temporal\nensemble for executing actions, as our highly-optimized BC baseline.\nBaselines for whole-body control tasks with dense reward\nFor locomotion tasks with dense\nreward, we consider (i) Soft Actor-Critic (SAC; Haarnoja et al. 2018), a model-free actor-critic RL\nalgorithm that maximizes action entropy, and (ii) Coarse-to-fine Q-Network (CQN; Seo et al. 2024).\nMoreover, although it is not the goal of this paper to compare against model-based RL algorithms,\nwe also consider two model-based baselines: (iii) DreamerV3 (Hafner et al., 2023), a model-based\nRL algorithm that learns a latent dynamics model and a policy using imagined trajectories and (iv)\nTD-MPC2 (Hansen et al., 2024), a model-based RL algorithm that learns a latent dynamics model\nand utilizes local trajectory optimization in imagined latent trajectories.\nImplementation details\nFor training with expert demonstrations, we follow the setup of Seo et al.\n(2024). Specifically, we keep a separate replay buffer that stores demonstrations and sample half\nof training data from demonstrations. We also relabel successful online episodes as demonstrations\nand store them in the demonstration replay buffer. For CQN-AS, we use an auxiliary BC loss from\nSeo et al. (2024) based on large margin loss (Hester et al., 2018). For actor-critic baselines, we\nuse an auxiliary BC loss that minimizes L2 loss between the policy outputs and expert actions. We\nimplement CQN-AS based on a publicly available source code of CQN2 based on PyTorch (Paszke\net al., 2019). We will release the full source code upon publication.\n2https://github.com/younggyoseo/CQN\n5\n\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nSuccess Rate (%)\nMove Plate\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nMove Two Plates\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nSaucepan To Hob\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nSandwich Flip\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nSandwich Remove\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nSuccess Rate (%)\nDishwasher Load Plates\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nDishwasher Load Cups\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nDishwasher Unload Cutlery\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nTake Cups\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nPut Cups\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nSuccess Rate (%)\nFlip Cup\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nFlip Cutlery\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nDishwasher Close Trays\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nCupboards Close All\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nReach Target Single\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nSuccess Rate (%)\nReach Target Multi Modal\n0\n2e4 4e4 6e4 8e4 1e5\n0\n25\n50\n75\n100\nReach Target Dual\n0\n1e4 2e4 3e4 4e4 5e4\n0\n25\n50\n75\n100\nDishwasher Close\n0\n1e4 2e4 3e4 4e4 5e4\n0\n25\n50\n75\n100\nWall Cupboard Open\n0\n1e4 2e4 3e4 4e4 5e4\n0\n25\n50\n75\n100\nDrawers Open All\n0\n0.5e4 1e4 1.5e4 2e4 2.5e4\nEnvironment Steps\n0\n25\n50\n75\n100\nSuccess Rate (%)\nWall Cupboard Close\n0\n0.5e4 1e4 1.5e4 2e4 2.5e4\nEnvironment Steps\n0\n25\n50\n75\n100\nDishwasher Open Trays\n0\n0.5e4 1e4 1.5e4 2e4 2.5e4\nEnvironment Steps\n0\n25\n50\n75\n100\nDrawers Close All\n0\n0.5e4 1e4 1.5e4 2e4 2.5e4\nEnvironment Steps\n0\n25\n50\n75\n100\nDrawer Top Open\n0\n0.5e4 1e4 1.5e4 2e4 2.5e4\nEnvironment Steps\n0\n25\n50\n75\n100\nDrawer Top Close\nRL: \nCQN-AS (Ours)\nCQN\nDrQ-v2+\nBC: \nACT\nFigure 4: BiGym results on 25 sparsely-rewarded mobile bi-manual manipulation tasks (Chernyadev\net al., 2024). All experiments are initialized with 17 to 60 human-collected demonstrations, and\nRL methods are trained with an auxiliary BC objective. On many of the challenging long-horizon\ntasks, CQN-AS quickly learns to match the performance of ACT (Zhao et al., 2023) and surpass it\nthrough online learning, while other RL baselines fail to effectively accelerate training with noisy\nhuman-collected demonstrations. We report the success rate over 25 episodes. The solid line and\nshaded regions represent the mean and confidence intervals, respectively, across 4 runs.\n4.1\nBIGYM EXPERIMENTS\nWe study CQN-AS on mobile bi-manual manipulation tasks from BiGym (Chernyadev et al., 2024).\nBiGym’s human-collected demonstrations are often noisy and multi-modal, posing challenges to RL\nalgorithms which should effectively leverage demonstrations for solving sparsely-rewarded tasks.\nSetup\nBecause we find that not all demonstrations from BiGym benchmark can be successfully\nreplayed3, we replay all the demonstrations and only use the successful ones as demonstrations.\nWe do not discard ones that fail to be replayed, but we use them as training data with zero reward\nbecause they can still be useful as failure experiences. To avoid training with too few demon-\nstrations, we exclude the tasks where the ratio of successful demonstrations is below 50%. This\nleaves us with 25 tasks, each with 17 to 60 demonstrations. For visual observations, we use RGB\nobservations with 84×84 resolution from head, left wrist, and right wrist cameras. We also\nuse low-dimensional proprioceptive states from proprioception, proprioception grippers, and\nproprioception floating base sensors. We use (i) absolute joint position control action mode\nand (ii) floating base that replaces locomotion with classic controllers. We use the same set of\nhyperparameters for all the tasks, in particular, we use action sequence of length 16. More details on\nBiGym experiments are available in Appendix A.\n3We use demonstrations available at the date of Oct 1st with the commit 018f8b2.\n6\n\n0\n2e6\n4e6\n6e6\n8e6\n1e7\n0\n200\n400\n600\n800\n1000\nEpisode Return\nStand\n0\n2e6\n4e6\n6e6\n8e6\n1e7\n0\n200\n400\n600\n800\n1000\nWalk\n0\n2e6\n4e6\n6e6\n8e6\n1e7\n0\n200\n400\n600\n800\nRun\n0\n2e6\n4e6\n6e6\n8e6\n1e7\n0\n2000\n4000\n6000\n8000\nReach\n0\n2e6\n4e6\n6e6\n8e6\n1e7\nEnvironment Steps\n0\n50\n100\n150\nEpisode Return\nHurdle\n0\n2e6\n4e6\n6e6\n8e6\n1e7\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nCrawl\n0\n2e6\n4e6\n6e6\n8e6\n1e7\nEnvironment Steps\n0\n100\n200\n300\n400\nMaze\n0\n2e6\n4e6\n6e6\n8e6\n1e7\nEnvironment Steps\n0\n200\n400\n600\n800\n1000\nSit Simple\nModel-free RL: \nCQN-AS (Ours)\nCQN\nSAC\nModel-based RL: \nTD-MPC2\nDreamerV3\nFigure 5: HumanoidBench results on 8 densely-rewarded humanoid control tasks (Sferrazza et al.,\n2024). All the experiments start from scratch and RL methods do not have an auxiliary BC objective.\nCQN-AS significantly outperforms other model-free RL baselines on most tasks. CQN-AS often\nachieves competitive performance to model-based RL baselines, which is intriguing but not the main\ngoal of this paper. For CQN-AS and CQN, we report the results aggregated over 4 runs. For other\nbaselines, we report the results aggregated over 3 runs available from public website. The solid line\nand shaded regions represent the mean and confidence intervals.\nComparison to baselines\nFigure 4 shows the experimental results on BiGym benchmark. We find\nthat CQN-AS quickly matches the performance of ACT and outperforms it through online learning\non 20/25 tasks, while other RL algorithms fail to do so especially on challenging long-horizon tasks\nsuch as Move Plate and Saucepan To Hob. A notable result here is that CQN-AS enables solving\nthese challenging BiGym tasks while other RL baselines, i.e., CQN and DrQ-v2+, completely fail\nas they achieve 0% success rate. This result highlights the capability of CQN-AS to accelerate RL\ntraining from noisy, multi-modal demonstrations collected by humans.\nLimitation\nHowever, we find that CQN-AS struggles to achieve meaningful success rate on some\nof the long-horizon tasks that require interaction with delicate objects such as cup or cutlery, leaving\nroom for future work to incorporate advanced vision encoders (He et al., 2016; 2022) or critic\narchitectures (Kapturowski et al., 2023; Chebotar et al., 2023; Springenberg et al., 2024).\n4.2\nHUMANOIDBENCH EXPERIMENTS\nTo show that CQN-AS can be generally applicable to tasks without demonstrations, we study CQN-AS\non densely-rewarded humanoid control tasks from HumanoidBench (Sferrazza et al., 2024).\nSetup\nFor HumanoidBench, we follow a standard setup that trains RL agents from scratch, which is\nalso used in original benchmark (Sferrazza et al., 2024). Specifically, we use low-dimensional states\nconsisting of proprioception and privileged task information as inputs. For tasks, we simply select the\nfirst 8 locomotion tasks in the benchmark. Following the original benchmark that trains RL agents\nfor environment steps that roughly requires 48 hours of training, we report the results of CQN-AS\nand CQN for 7 million steps. For baselines, we use the results available from the public repository,\nwhich are evaluated on tasks with dexterous hands, and we also evaluate our algorithm on tasks with\nhands. We use the same set of hyperparameters for all the tasks, in particular, we use action sequence\nof length 4. More details on HumanoidBench experiments are available in Appendix A.\nComparison to model-free RL baselines\nFigure 5 shows the results on on HumanoidBench. We\nfind that, by learning the critic network with action sequence, CQN-AS outperforms other model-free\nRL baselines, i.e., CQN and SAC, on most tasks. In particular, the difference between CQN-AS and\nbaselines becomes larger as the task gets more difficult, e.g., baselines fail to achieve high episode\nreturn on Walk and Run tasks but CQN-AS achieves strong performance. This result shows that our\nidea of using action sequence can be applicable to generic setup without demonstrations.\n7\n\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nSuccess Rate (%)\nTake Lid Off Saucepan\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nOpen Drawer\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nStack Wine\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nToilet Seat Up\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nOpen Microwave\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nSuccess Rate (%)\nOpen Oven\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100Take Plate Off Colored Dish Rack\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nTurn Tap\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nPut Money In Safe\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nPhone On Base\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nSuccess Rate (%)\nPut Books On Bookshelf\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nSweep To Dustpan\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nPick Up Cup\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nOpen Door\n0\n1e4\n2e4\n3e4\n0\n25\n50\n75\n100\nMeat On Grill\n0\n1e4\n2e4\n3e4\nEnvironment Steps\n0\n25\n50\n75\n100\nSuccess Rate (%)\nBasketball In Hoop\n0\n1e4\n2e4\n3e4\nEnvironment Steps\n0\n25\n50\n75\n100\nLamp On\n0\n1e4\n2e4\n3e4\nEnvironment Steps\n0\n25\n50\n75\n100\nPress Switch\n0\n1e4\n2e4\n3e4\nEnvironment Steps\n0\n25\n50\n75\n100\nPut Rubbish In Bin\n0\n1e4\n2e4\n3e4\nEnvironment Steps\n0\n25\n50\n75\n100\nInsert Usb In Computer\nRL: \nCQN-AS (Ours)\nCQN\nDrQ-v2+\nBC: \nACT\nFigure 6: RLBench results on 20 sparsely-rewarded tabletop manipulation tasks from RLBench\n(James et al., 2020). All experiments are initialized with 100 synthetic demonstrations generated\nvia motion-planning and RL methods are trained with an auxiliary BC objective. As expected, with\nsynthetic demonstrations, CQN-AS achieves similar performance to CQN on most tasks. However,\nCQN-AS often significantly outperforms baselines on several challenging, long-horizon tasks such as\nOpen Oven. We report the success rate over 25 episodes. The solid line and shaded regions represent\nthe mean and confidence intervals, respectively, across 4 runs.\nCQN-AS often achieves competitive performance to model-based RL baselines\nWhile outper-\nforming model-based RL algorithms is not the goal of this paper, we find that CQN-AS often achieves\ncompetitive performance to model-based RL baselines, i.e., DreamerV3 and TD-MPC2, on tasks\nsuch as Run or Sit Simple. This result shows the potential of our idea to enable RL agents to learn\nuseful value functions on challenging tasks, without the need to explicitly learn dynamics model. We\nalso note that incorporating our idea into world model learning could be an interesting direction.\n4.3\nRLBENCH EXPERIMENTS\nTo investigate whether CQN-AS can also be effective in leveraging clean demonstrations, we study\nCQN-AS on RLBench (James et al., 2020) with synthetic demonstrations.\nSetup\nFor RLBench experiments, we use the official CQN implementation for collecting demon-\nstrations and reproducing the baseline results on the same set of tasks. Specifically, we use RGB\nobservations with 84×84 resolution from front, wrist, left shoulder, and right shoulder cam-\neras. We also use low-dimensional proprioceptive states consisting of 7-dimensional joint positions\nand a binary value for gripper open. We use 100 demonstrations and delta joint position control\naction mode. We use the same set of hyperparameters for all the tasks, in particular, we use action\nsequence of length 4. More details on RLBench experiments are available in Appendix A.\nCQN-AS is also effective with clean demonstrations\nBecause RLBench provides synthetic clean\ndemonstrations, as we expected, Figure 6 shows that CQN-AS achieves similar performance to CQN\non many of the tasks, except 2/25 tasks where it hurts the performance. But we still find that CQN-AS\nachieves quite superior performance to CQN on some challenging long-horizon tasks such as Open\nOven or Take Plate Off Colored Dish Rack. These results, along with results from BiGym and\nHumanoidBench, show that CQN-AS can be used in various benchmark with different characteristics.\n8\n\n0\n2e4\n4e4\n6e4\n8e4\n1e5\nEnvironment Steps\n0\n25\n50\n75\n100\nSuccess Rate (%)\nMove Plate\n0\n2e4\n4e4\n6e4\n8e4\n1e5\nEnvironment Steps\n0\n25\n50\n75\n100\nSaucepan To Hob\nCQN-AS\nCQN-AS8\nCQN-AS4\nCQN-AS2\n(a) Effect of action sequence length\n0\n2e4\n4e4\n6e4\n8e4\n1e5\nEnvironment Steps\n0\n25\n50\n75\n100\nSuccess Rate (%)\nMove Plate\n0\n2e4\n4e4\n6e4\n8e4\n1e5\nEnvironment Steps\n0\n25\n50\n75\n100\nSaucepan To Hob\nCQN-AS\nCQN-AS (No RL)\n(b) Effect of RL objective\n0\n2e4\n4e4\n6e4\n8e4\n1e5\nEnvironment Steps\n0\n25\n50\n75\n100\nSuccess Rate (%)\nSaucepan To Hob\n0\n2e4\n4e4\n6e4\n8e4\n1e5\nEnvironment Steps\n0\n25\n50\n75\n100\nReach Target Single\nCQN-AS\nCQN-AS (No Temporal Ensemble)\n(c) Effect of temporal ensemble\n0\n2e5\n4e5\n6e5\n8e5\n1e6\nEnvironment Steps\n0\n250\n500\n750\n1000\nEpisode Return\nWalker Run\n0\n2e5\n4e5\n6e5\n8e5\n1e6\nEnvironment Steps\n0\n250\n500\n750\n1000\nCheetah Run\nCQN-AS8\nCQN-AS4\nCQN-AS2\nCQN\n(d) Failure mode: Torque control\nFigure 7: Ablation studies and analysis on the effect of (a) action sequence, (b) RL objective, and\n(c) temporal ensemble. (d) We also provide results on locomotion tasks from DeepMind Control\nSuite (Tassa et al., 2020), where CQN-AS fails to improve performance. The solid line and shaded\nregions represent the mean and confidence intervals, respectively, across 4 runs.\n4.4\nABLATION STUDIES, ANALYSIS, FAILURE CASES\nEffect of action sequence length\nFigure 7a shows the performance of CQN-AS with different\naction sequence lengths on two BiGym tasks. We find that training the critic network with longer\naction sequences improves and stabilizes performance.\nRL objective is crucial for strong performance\nFigure 7b shows the performance of CQN-AS\nwithout RL objective that trains the model only with BC objective on successful demonstrations. We\nfind this baseline significantly underperforms CQN-AS, which shows that RL objective is indeed\nenabling the agent to learn from online trial-and-error experiences.\nEffect of temporal ensemble\nFigure 7c shows that performance largely degrades without temporal\nensemble on Saucepan To Hop as temporal ensemble induces a smooth motion and thus improves\nperformance in fine-grained control tasks. But we also find that temporal ensemble can be harmful\non Reach Target Single. We hypothesize this is because temporal ensemble often makes it difficult\nto refine behaviors based on recent visual observations. Nonetheless, we use temporal ensemble for\nall the tasks as we find it helps on most tasks and we aim to use the same set of hyperparameters.\nFailure case: Torque control\nFigure 7d shows that CQN-AS underperforms CQN on locomotion\ntasks with torque control. We hypothesize this is because a sequence of joint positions usually has a\nsemantic meaning in joint spaces, making it easier to learn with, when compared to learning how to\napply a sequence of torques. Addressing this failure case is an interesting future direction.\n5\nRELATED WORK\nBehavior cloning with action sequence\nRecent behavior cloning approaches have shown that\npredicting a sequence of actions enables the policy to effectively imitate noisy expert trajectories and\nhelps in dealing with idle actions from human pauses during data collection (Zhao et al., 2023; Chi\net al., 2023). In particular, Zhao et al. (2023) train a transformer model (Vaswani et al., 2017) that\npredicts action sequence and Chi et al. (2023) train a denoising diffusion model (Ho et al., 2020) that\napproximates the action distributions. This idea has been extended to multi-task setup (Bharadhwaj\net al., 2024), mobile manipulation (Fu et al., 2024b) and humanoid control (Fu et al., 2024a). Our\nwork is inspired by this line of work and proposed to learn RL agents with action sequence.\n9\n\nReinforcement learning with action sequence\nIn the context of reinforcement learning, Medini &\nShrivastava (2019) proposed to pre-compute frequent action sequences from expert demonstrations\nand augment the action space with these sequences. However, this idea introduces additional\ncomplexity and is not scalable to setups without demonstrations. One recent work relevant to ours\nis Saanum et al. (2024) that encourages a sequence of actions from RL agents to be predictable\nand smooth. But this differs from our work in that it uses the concept of action sequence only for\ncomputing the penalty term. Recently, Ankile et al. (2024) point out that RL with action sequence\nis challenging and instead proposes to use RL for learning a single-step policy that corrects action\nsequence predictions from BC. In contrast, our work shows that training RL agents with action\nsequence is feasible and leads to improved performance compared to prior RL algorithms.\n6\nCONCLUSION\nWe presented Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a value-based RL algo-\nrithm that trains a critic network that outputs Q-values over action sequences. Extensive experiments\nin benchmarks with various setups show that our idea not only improves the performance of the base\nalgorithm but also allows for solving complex tasks where prior RL algorithms completely fail.\nWe believe our work will be strong evidence that shows RL can realize its promise to develop robots\nthat can continually improve through online trial-and-error experiences, surpassing the performance\nof BC approaches. We are excited about future directions, including real-world RL with humanoid\nrobots, incorporating advanced critic architectures (Kapturowski et al., 2023; Chebotar et al., 2023;\nSpringenberg et al., 2024), bootstrapping RL agents from imitation learning (Hu et al., 2023; Xing\net al., 2024) or offline RL (Nair et al., 2020; Lee et al., 2021), extending the idea to recent model-based\nRL approaches (Hafner et al., 2023; Hansen et al., 2024), to name but a few.\nACKNOWLEDGEMENTS\nWe thank Stephen James and Richie Lo for the discussion on the initial idea of this project. Pieter\nAbbeel holds concurrent appointments as a Professor at UC Berkeley and as an Amazon Scholar.\nThis paper describes work performed at UC Berkeley and is not associated with Amazon.\nREFERENCES\nLars Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne, and Pulkit Agrawal. From imitation\nto refinement–residual rl for precise visual assembly. arXiv preprint arXiv:2407.16677, 2024.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nMarc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement\nlearning. In International Conference on Machine Learning, 2017.\nHomanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, and Vikash Ku-\nmar. Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations\nand action chunking. In 2024 IEEE International Conference on Robotics and Automation (ICRA),\n2024.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski,\nTianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action\nmodels transfer web knowledge to robotic control. In Conference on Robot Learning, 2023a.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics\ntransformer for real-world control at scale. In Robotics: Science and Systems, 2023b.\nYevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe\nYu, Alexander Herzog, Karl Pertsch, et al. Q-transformer: Scalable offline reinforcement learning\nvia autoregressive q-functions. In Conference on Robot Learning, 2023.\n10\n\nNikita Chernyadev, Nicholas Backshall, Xiao Ma, Yunfan Lu, Younggyo Seo, and Stephen James.\nBigym: A demo-driven mobile bi-manual manipulation benchmark. In Conference on Robot\nLearning, 2024.\nCheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran\nSong. Diffusion policy: Visuomotor policy learning via action diffusion. In Robotics: Science and\nSystems, 2023.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for\nstatistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\nZipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, and Chelsea Finn. Humanplus: Humanoid\nshadowing and imitation from humans. In Conference on Robot Learning, 2024a.\nZipeng Fu, Tony Z Zhao, and Chelsea Finn. Mobile aloha: Learning bimanual mobile manipulation\nwith low-cost whole-body teleoperation. In Conference on Robot Learning, 2024b.\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash\nKumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al.\nSoft actor-critic algorithms and\napplications. arXiv preprint arXiv:1812.05905, 2018.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nNicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous\ncontrol. In International Conference on Learning Representations, 2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n2016.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked\nautoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2022.\nDan Hendrycks and Kevin Gimpel.\nGaussian error linear units (gelus).\narXiv preprint\narXiv:1606.08415, 2016.\nAlexander Herzog, Kanishka Rao, Karol Hausman, Yao Lu, Paul Wohlhart, Mengyuan Yan, Jessica\nLin, Montserrat Gonzalez Arenas, Ted Xiao, Daniel Kappler, et al. Deep rl at scale: Sorting waste\nin office buildings with a fleet of mobile manipulators. arXiv preprint arXiv:2305.03270, 2023.\nTodd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan,\nJohn Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In\nProceedings of the AAAI conference on artificial intelligence, 2018.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 2020.\nHengyuan Hu, Suvir Mirchandani, and Dorsa Sadigh. Imitation bootstrapped reinforcement learning.\narXiv preprint arXiv:2311.02198, 2023.\nStephen James, Marc Freese, and Andrew J Davison. Pyrep: Bringing v-rep to deep robot learning.\narXiv preprint arXiv:1906.11176, 2019.\nStephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot\nlearning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019–\n3026, 2020.\nLeslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially\nobservable stochastic domains. Artificial intelligence, 1998.\n11\n\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre\nQuillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement\nlearning for vision-based robotic manipulation. In Conference on robot learning, 2018.\nSteven Kapturowski, V´ıctor Campos, Ray Jiang, Nemanja Raki´cevi´c, Hado van Hasselt, Charles\nBlundell, and Adri`a Puigdom`enech Badia. Human-level atari 200x faster. In International\nConference on Learning Representations, 2023.\nSeunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online\nreinforcement learning via balanced replay and pessimistic q-ensemble. In Conference on Robot\nLearning, 2021.\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In\nInternational Conference on Learning Representations, 2016.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2019.\nTharun Medini and Anshumali Shrivastava. Mimicking actions is a good strategy for beginners: Fast\nreinforcement learning with expert action sequences, 2019. URL https://openreview.net/\nforum?id=HJfxbhR9KQ.\nLuke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of\ncontinuous actions for deep rl. arXiv preprint arXiv:1705.05035, 2017.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. Nature, 2015.\nAshvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online\nreinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in neural information processing systems, 32,\n2019.\nBoris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM\njournal on control and optimization, 1992.\nDean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural\ninformation processing systems, 1988.\nEric Rohmer, Surya PN Singh, and Marc Freese. V-rep: A versatile and scalable robot simulation\nframework. In IEEE/RSJ international conference on intelligent robots and systems, 2013.\nTankred Saanum, No´emi ´Eltet˝o, Peter Dayan, Marcel Binz, and Eric Schulz. Reinforcement learning\nwith simple sequence priors. Advances in Neural Information Processing Systems, 2024.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,\ngo, chess and shogi by planning with a learned model. Nature, 2020.\nFrank Sehnke, Christian Osendorfer, Thomas R¨uckstieß, Alex Graves, Jan Peters, and J¨urgen Schmid-\nhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551–559, 2010.\nYounggyo Seo, Jafar Uruc¸, and Stephen James. Continuous control with coarse-to-fine reinforcement\nlearning. In Conference on Robot Learning, 2024.\nTim Seyde, Peter Werner, Wilko Schwarting, Igor Gilitschenski, Martin Riedmiller, Daniela Rus, and\nMarkus Wulfmeier. Solving continuous control via q-learning. In International Conference on\nLearning Representations, 2023.\n12\n\nCarmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, and Pieter Abbeel. Humanoid-\nbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. In Robotics:\nScience and Systems, 2024.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without\nhuman knowledge. nature, 2017.\nJost Tobias Springenberg, Abbas Abdolmaleki, Jingwei Zhang, Oliver Groth, Michael Bloesch,\nThomas Lampe, Philemon Brakel, Sarah Bechtle, Steven Kapturowski, Roland Hafner, et al.\nOffline actor-critic reinforcement learning scales to large models. In International Conference on\nMachine Learning, 2024.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nYuval Tassa, Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh\nMerel, Tom Erez, Timothy Lillicrap, and Nicolas Heess. dm control: Software and tasks for\ncontinuous control. arXiv preprint arXiv:2006.12983, 2020.\nOcto Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep\nDasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot\npolicy. In Robotics: Science and Systems, 2024.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems, 2017.\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling\nnetwork architectures for deep reinforcement learning. In International conference on machine\nlearning, 2016.\nJiaxu Xing, Angel Romero, Leonard Bauersfeld, and Davide Scaramuzza. Bootstrapping reinforce-\nment learning with imitation for vision-based agile flight. In Conference on Robot Learning,\n2024.\nDenis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control:\nImproved data-augmented reinforcement learning. In International Conference on Learning\nRepresentations, 2022.\nTony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual\nmanipulation with low-cost hardware. In Robotics: Science and Systems, 2023.\n13\n\nA\nEXPERIMENTAL DETAILS\nBiGym\nBiGym4 (Chernyadev et al., 2024) is built upon MuJoCo (Todorov et al., 2012). We use\nUnitree H1 with two parallel grippers. We find that demonstrations available in the recent version of\nBiGym are not all successful. Therefore we adopt the strategy of replaying all the demonstrations and\nonly use the successful ones as demonstrations. instead of discarding the failed demonstrations, we\nstill store them in a replay buffer as failure experiences. To avoid training with too few demonstrations,\nwe exclude the tasks where the ratio of successful demonstrations is below 50%. Table 1 shows the\nlist of 25 sparsely-rewarded mobile bi-manual manipulation tasks used in our experiments.\nTable 1: BiGym tasks with their maximum episode length and number of successful demonstrations.\nTask\nLength\nDemos\nTask\nLength\nDemos\nMove Plate\n300\n51\nCupboards Close All\n620\n53\nMove Two Plates\n550\n30\nReach Target Single\n100\n30\nSaucepan To Hob\n440\n28\nReach Target Multi Modal\n100\n60\nSandwich Flip\n620\n34\nReach Target Dual\n100\n50\nSandwich Remove\n540\n24\nDishwasher Close\n375\n44\nDishwasher Load Plates\n560\n17\nWall Cupboard Open\n300\n44\nDishwasher Load Cups\n750\n58\nDrawers Open All\n480\n45\nDishwasher Unload Cutlery\n620\n29\nWall Cupboard Close\n300\n60\nTake Cups\n420\n32\nDishwasher Open Trays\n380\n57\nPut Cups\n425\n43\nDrawers Close All\n200\n59\nFlip Cup\n550\n45\nDrawer Top Open\n200\n40\nFlip Cutlery\n500\n43\nDrawer Top Close\n120\n51\nDishwasher Close Trays\n320\n62\nHumanoidBench\nHumanoidBench5 (Sferrazza et al., 2024) is built upon MuJoCo (Todorov et al.,\n2012). We use Unitree H1 with two dexterous hands. We consider the first 8 locomotion tasks in the\nbenchmark: Stand, Walk, Run, Reach, Hurdle, Crawl, Maze, Sit Simple. We use proprioceptive\nstates and privileged task information instead of visual observations. Unlike BiGym and RLBench\nexperiments, we do not utilize dueling network (Wang et al., 2016) and distributional critic (Bellemare\net al., 2017) in HumanoidBench for faster experimentation.\nRLBench\nRLBench6 (James et al., 2020) is built upon CoppeliaSim (Rohmer et al., 2013) and\nPyRep (James et al., 2019). We use a 7-DoF Franka Panda robot arm and a parallel gripper. Following\nthe setup of Seo et al. (2024), we increase the velocity and acceleration of the arm by 2 times. For all\nexperiments, we use 100 demonstrations generated via motion-planning. Table 2 shows the list of 20\nsparsely-rewarded visual manipulation tasks used in our experiments.\nTable 2: RLBench tasks with their maximum episode length used in our experiments.\nTask\nLength\nTask\nLength\nTake Lid Off Saucepan\n100\nPut Books On Bookshelf\n175\nOpen Drawer\n100\nSweep To Dustpan\n100\nStack Wine\n150\nPick Up Cup\n100\nToilet Seat Up\n150\nOpen Door\n125\nOpen Microwave\n125\nMeat On Grill\n150\nOpen Oven\n225\nBasketball In Hoop\n125\nTake Plate Off\nColored Dish Rack\n150\nLamp On\n100\nTurn Tap\n125\nPress Switch\n100\nPut Money In Safe\n150\nPut Rubbish In Bin\n150\nPhone on Base\n175\nInsert Usb In Computer\n100\n4https://github.com/chernyadev/bigym\n5https://github.com/carlosferrazza/humanoid-bench\n6https://github.com/stepjam/RLBench\n14\n\nHyperparameters\nWe use the same set of hyperparameters across the tasks in each domain. For\nhyperparameters shared across CQN and CQN-AS, we use the same hyperparameters for both\nalgorithms for a fair comparison. We provide detailed hyperparameters for BiGym and RLBench\nexperiments in Table 3 and hyperparameters for HumanoidBench experiments in Table 4\nTable 3: Hyperparameters for demo-driven vision-based experiments in BiGym and RLBench\nHyperparameter\nValue\nImage resolution\n84 × 84 × 3\nImage augmentation\nRandomShift (Yarats et al., 2022)\nFrame stack\n4 (BiGym) / 8 (RLBench)\nCNN - Architecture\nConv (c=[32, 64, 128, 256], s=2, p=1)\nMLP - Architecture\nLinear (c=[512, 512, 64, 512, 512], bias=False) (BiGym)\nLinear (c=[64, 512, 512], bias=False) (RLBench)\nCNN & MLP - Activation\nSiLU (Hendrycks & Gimpel, 2016) and LayerNorm (Ba et al., 2016)\nGRU - Architecture\nGRU (c=[512], bidirectional=False)\nDueling network\nTrue\nC51 - Atoms\n51\nC51 - vmin, vmax\n-2, 2\nAction sequence\n16 (BiGym) / 4 (RLBench)\nTemporal ensemble weight m\n0.01\nLevels\n3\nBins\n5\nBC loss (LBC) scale\n1.0\nRL loss (LRL) scale\n0.1\nRelabeling as demonstrations\nTrue\nData-driven action scaling\nTrue\nAction mode\nAbsolute Joint (BiGym), Delta Joint (RLBench)\nExploration noise\nϵ ∼N(0, 0.01)\nTarget critic update ratio (τ)\n0.02\nN-step return\n1\nBatch size\n128 (BiGym) / 256 (RLBench)\nDemo batch size\n128 (BiGym) / 256 (RLBench)\nOptimizer\nAdamW (Loshchilov & Hutter, 2019)\nLearning rate\n5e-5\nWeight decay\n0.1\nTable 4: Hyperparameters for state-based experiments in HumanoidBench\nHyperparameter\nValue\nMLP - Architecture\nLinear (c=[512, 512], bias=False)\nCNN & MLP - Activation\nSiLU (Hendrycks & Gimpel, 2016) and LayerNorm (Ba et al., 2016)\nGRU - Architecture\nGRU (c=[512], bidirectional=False)\nDueling network\nFalse\nAction sequence\n4\nTemporal ensemble weight m\n0.01\nLevels\n3\nBins\n5\nRL loss (LRL) scale\n1.0\nAction mode\nAbsolute Joint\nExploration noise\nϵ ∼N(0, 0.01)\nTarget critic update ratio (τ)\n1.0\nTarget critic update interval (τ)\n1000\nN-step return\n3\nBatch size\n128\nOptimizer\nAdamW (Loshchilov & Hutter, 2019)\nLearning rate\n5e-5\nWeight decay\n0.1\n15\n\nComputing hardware\nFor all experiments, we use consumer-grade 11GB GPUs such as NVIDIA\nGTX 1080Ti, NVIDIA Titan XP, and NVIDIA RTX 2080Ti with 11 or 12GB VRAM. With 2080Ti\nGPU, each BiGym experiment with 100K environment steps take 9.5 hours, each RLBench experi-\nment with 30K environment steps take 6.5 hours, and each HumanoidBench experiment with 7M\nenvironment steps take 48 hours. We find that CQN-AS is around 33% slower than running CQN\nbecause larger architecture slows down both training and inference.\nBaseline implementation\nFor CQN (Seo et al., 2024) and DrQ-v2+ (Yarats et al., 2022), we\nuse the implementation available from the official CQN implementation7. For ACT (Zhao et al.,\n2023), we use the implementation from RoboBase repository8. For SAC (Haarnoja et al., 2018),\nDreamerV3 (Hafner et al., 2023), and TD-MPC2 (Hansen et al., 2024), we use results provided in\nHumanoidBench9 repository (Sferrazza et al., 2024).\nB\nFULL DESCRIPTION OF CQN AND CQN-AS\nThis section provides the formulation of CQN and CQN-AS with n-dimensional actions.\nB.1\nCOARSE-TO-FINE Q-NETWORK\nLet al,n\nt\nbe an action at level l and dimension n and al\nt = {al,1\nt , ..., al,N\nt\n} be actions at level l with a0\nt\nbeing zero vector. We then define coarse-to-fine critic to consist of multiple Q-networks:\nQl,n\nθ (ht, al,n\nt , al−1\nt\n) for l ∈{1, ..., L} and n ∈{1, ..., N}\n(5)\nWe optimize the critic network with the following objective:\nX\nn\nX\nl\n\u0010\nQl,n\nθ (ht, al,n\nt , al−1\nt\n) −rt+1 −γ max\na′ Ql,n\n¯θ (ht+1, a′, πl(ht+1)\n\u00112\n,\n(6)\nwhere ¯θ are delayed parameters for a target network (Polyak & Juditsky, 1992) and πl is a policy that\noutputs the action al\nt at each level l via the inference steps with our critic, i.e., πl(ht) = al\nt.\nAction inference\nTo output actions at time step t with the critic, CQN first initializes constants\nan,low\nt\nand an,high\nt\nwith −1 and 1 for each n. Then the following steps are repeated for l ∈{1, ..., L}:\n• Step 1 (Discretization): Discretize an interval [an,low\nt\n, an,high\nt\n] into B uniform intervals, and\neach of these intervals become an action space for Ql,n\nθ\n• Step 2 (Bin selection): Find the bin with the highest Q-value, set al,n\nt\nto the centroid of the\nselected bin, and aggregate actions from all dimensions to al\nt\n• Step 3 (Zoom-in): Set an,low\nt\nand an,high\nt\nto the minimum and maximum of the selected bin,\nwhich intuitively can be seen as zooming-into each bin.\nWe then use the last level’s action aL\nt as the action at time step t.\nComputing Q-values\nTo compute Q-values for given actions at, CQN first initializes constants\nan,low\nt\nand an,high\nt\nwith −1 and 1 for each n. We then repeat the following steps for l ∈{1, ..., L}:\n• Step 1 (Discretization): Discretize an interval [an,low\nt\n, an,high\nt\n] into B uniform intervals, and\neach of these intervals become an action space for Ql,n\nθ\n• Step 2 (Bin selection): Find the bin that contains input action at, compute al,n\nt\nfor the\nselected interval, and compute Q-values Ql,n\nθ (ht, al,n\nt , al−1\nt\n).\n• Step 3 (Zoom-in): Set an,low\nt\nand an,high\nt\nto the minimum and maximum of the selected bin,\nwhich intuitively can be seen as zooming-into each bin.\nWe then use a set of Q-values {Ql,n\nθ (ht, al,n\nt , al−1\nt\n)}L\nl=1 for given actions at.\n7https://github.com/younggyoseo/CQN\n8https://github.com/robobase-org/robobase\n9https://github.com/carlosferrazza/humanoid-bench\n16\n\nB.2\nCOARSE-TO-FINE CRITIC WITH ACTION SEQUENCE\nLet al\nt:t+K = {al\nt, ..., al\nt+K−1} be an action sequence at level l and a0\nt:t+K be zero vector. Our critic\nnetwork consists of multiple Q-networks for each level l, dimension n, and sequence step k:\nQl,n,k\nθ\n(ht, al,n\nt+k−1, al−1\nt:t+K) for l ∈{1, ..., L}, n ∈{1, ..., N} and k ∈{1, ..., K}\n(7)\nWe optimize the critic network with the following objective:\nX\nn\nX\nl\nX\nk\n\u0010\nQl,n,k\nθ\n(ht, al,n\nt , al−1\nt:t+K) −rt+1 −γ max\na′ Ql,n,k\n¯θ\n(ht+1, a′, πl\nK(ht+1)\n\u00112\n,\n(8)\nwhere πl\nK is an action sequence policy that outputs the action sequence al\nt:t+K. In practice, we\ncompute Q-values for all sequence step k ∈{1, ..., K} and all action dimension n ∈{1, ..., N} in\nparallel. This can be seen as extending the idea of Seyde et al. (2023), which learns decentralized\nQ-networks for action dimensions, into action sequence dimension. As we mentioned in Section 3.1,\nwe find this simple scheme works well on challenging tasks with high-dimensional action spaces.\nArchitecture\nLet ek denote an one-hot encoding for k. For each level l, we construct features for\neach sequence step k as hl\nt,k =\n\u0002\nht, al−1\nt+k−1, ek\n\u0003\n. We encode each hl\nt,k with a shared MLP network\nand process them through GRU (Cho et al., 2014) to obtain sl\nt,k = f GRU\nθ\n(f MLP\nθ\n(hl\nt,1), ..., f MLP\nθ\n(hl\nt,k)).\nWe use a shared projection layer to map each sl\nt,k into Q-values at each sequence step k, i.e.,\n{Ql,k\nθ (ot, al,n\nt+k−1, al−1\nt:t+K)}N\nn=1 = f proj\nθ\n(sl\nt,k). We note that we compute Q-values for all dimensions\nn ∈{1, ..., N} at the same time with a big linear layer, which follows the design of Seo et al. (2024).\n17",
    "pdf_filename": "Reinforcement_Learning_with_Action_Sequence_for_Data-Efficient_Robot_Learning.pdf"
}