{
    "title": "DLBacktrace A Model Agnostic Explainability for any Deep Learning Models",
    "abstract": "The rapid advancement of artificial intelligence has led to increasingly sophisticated deep learning models, which frequently operate as opaque “black boxes” with limited transparency in their decision- making processes. This lack of interpretability presents considerable challenges, especially in high-stakes applications where understanding the rationale behind a model’s outputs is as essential as the outputs themselves. This study addresses the pressing need for interpretability in AI systems, emphasizing its role in fostering trust, ensuring accountability, and promoting responsible deployment in mission-critical fields. To address the interpretability challenge in deep learning, we introduce DLBacktrace, an innovative technique developed by the AryaXAI team to illuminate model decisions across a wide array of domains, including simple Multi Layer Perceptron (MLPs), Convolutional Neural Networks (CNNs), Large Language Models (LLMs), Computer Vision Models, and more. We provide a comprehensive overview of the DLBacktrace algorithm and present benchmarking results, comparing its performance against established interpretability methods, such as SHAP, LIME, GradCAM, Integrated Gradients, SmoothGrad, and Attention Rollout, using diverse task-based metrics. The proposed DLBacktrace technique is compatible with various model architectures built in PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP architectures such as BERT and LSTMs, computer vision models like ResNet and U-Net, as well as custom deep neural network (DNN) models for tabular data. This flexibility underscores DLBacktrace’s adaptability and effectiveness in enhancing model transparency across a broad spectrum of applications. The library is open-sourced and available at https://github.com/AryaXAI/DLBacktrace. 1 Introduction Despite remarkable advancements in artificial intelligence, particularly with the evolution of deep learning architectures, even the most sophisticated models face a persistent challenge: they often operate as \"black boxes,\" with internal processes that remain opaque and difficult to interpret. These models produce highly accurate predictions, yet provide limited insights into how and why they make specific decisions. This opacity raises significant concerns, especially in high-stakes applications like healthcare, finance, and law enforcement, where understanding the rationale behind model outputs is critical. For example, in the healthcare sector, AI-driven diagnostics must be interpretable to ensure that medical professionals can trust and act on recommendations for patient treatment. Similarly, in finance, regulations such as the European Union’s GDPR mandate a \"right to explanation\" for automated decisions affecting individuals, making explainability not only an ethical imperative but also a regulatory requirement. The growing demand for explainability and transparency in AI systems is often eclipsed by the prevailing focus on maximizing raw performance. This trend is especially evident with the increasing use of models like OpenAI’s arXiv:2411.12643v1  [cs.LG]  19 Nov 2024",
    "body": "DLBACKTRACE: A MODEL AGNOSTIC EXPLAINABILITY FOR\nANY DEEP LEARNING MODELS\nVinay Kumar Sankarapu, Chintan Chitroda, Yashwardhan Rathore\n{vinay, chintan.chitroda, yashwardhan.rathore}@aryaxai.com\nNeeraj Kumar Singh, Pratinav Seth\n{neeraj.singh, pratinav.seth}@aryaxai.com\nAryaXAI\nNovember 20, 2024\nABSTRACT\nThe rapid advancement of artificial intelligence has led to increasingly sophisticated deep learning\nmodels, which frequently operate as opaque “black boxes” with limited transparency in their decision-\nmaking processes. This lack of interpretability presents considerable challenges, especially in\nhigh-stakes applications where understanding the rationale behind a model’s outputs is as essential\nas the outputs themselves. This study addresses the pressing need for interpretability in AI systems,\nemphasizing its role in fostering trust, ensuring accountability, and promoting responsible deployment\nin mission-critical fields. To address the interpretability challenge in deep learning, we introduce\nDLBacktrace, an innovative technique developed by the AryaXAI team to illuminate model decisions\nacross a wide array of domains, including simple Multi Layer Perceptron (MLPs), Convolutional\nNeural Networks (CNNs), Large Language Models (LLMs), Computer Vision Models, and more.\nWe provide a comprehensive overview of the DLBacktrace algorithm and present benchmarking\nresults, comparing its performance against established interpretability methods, such as SHAP, LIME,\nGradCAM, Integrated Gradients, SmoothGrad, and Attention Rollout, using diverse task-based\nmetrics. The proposed DLBacktrace technique is compatible with various model architectures built\nin PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP architectures such as\nBERT and LSTMs, computer vision models like ResNet and U-Net, as well as custom deep neural\nnetwork (DNN) models for tabular data. This flexibility underscores DLBacktrace’s adaptability and\neffectiveness in enhancing model transparency across a broad spectrum of applications. The library is\nopen-sourced and available at https://github.com/AryaXAI/DLBacktrace.\n1\nIntroduction\nDespite remarkable advancements in artificial intelligence, particularly with the evolution of deep learning architectures,\neven the most sophisticated models face a persistent challenge: they often operate as \"black boxes,\" with internal\nprocesses that remain opaque and difficult to interpret. These models produce highly accurate predictions, yet provide\nlimited insights into how and why they make specific decisions. This opacity raises significant concerns, especially\nin high-stakes applications like healthcare, finance, and law enforcement, where understanding the rationale behind\nmodel outputs is critical. For example, in the healthcare sector, AI-driven diagnostics must be interpretable to ensure\nthat medical professionals can trust and act on recommendations for patient treatment. Similarly, in finance, regulations\nsuch as the European Union’s GDPR mandate a \"right to explanation\" for automated decisions affecting individuals,\nmaking explainability not only an ethical imperative but also a regulatory requirement.\nThe growing demand for explainability and transparency in AI systems is often eclipsed by the prevailing focus\non maximizing raw performance. This trend is especially evident with the increasing use of models like OpenAI’s\narXiv:2411.12643v1  [cs.LG]  19 Nov 2024\n\nA PREPRINT - NOVEMBER 20, 2024\nChatGPT[1], Meta’s LLaMA [2], Google’s Gemini, and similar large language models (LLMs) that are widely adopted\nfor tasks such as language generation, classification, and complex question answering. These models frequently operate\nas ‘black boxes,’ offering limited insight into their decision-making processes. Many of their architectures are also\nproprietary and closed-source, adding to their opacity. However, due to their high accuracy and utility in various\napplications, their lack of transparency is often accepted by users and stakeholders. Consequently, interpretability\nis frequently overlooked or considered secondary to model performance in deployment. This disregard can lead to\nchallenges, such as increased uncertainty in responses to out-of-domain questions and potential regulatory concerns.\nOver the years, various efforts have been made to make machine learning models more transparent, leading to the\ndevelopment of interpretability methods. Popular approaches like Local Interpretable Model-agnostic Explanations\n(LIME) [3] and SHapley Additive exPlanations (SHAP) [4] aim to clarify model decisions by assigning feature\nimportance scores. These methods work well in tabular data contexts but encounter challenges with complex data\ntypes such as images and text, where feature interactions and contextual nuances are intricate. Both LIME and SHAP\nrequire repeated evaluations and data perturbations to estimate importance scores, which can increase computational\nload significantly, especially in high-dimensional contexts. They generate explanations by analyzing selected subsets\nof data, which leads to instance-specific (local) feature importance. This method may hinder their ability to offer\na comprehensive, model-wide perspective, impacting both interpretability and reliability. This issue is particularly\npronounced when real-time insights are needed, as the time required to generate explanations increases exponentially\nwith the size of the data subset.\nFor complex data like images and text, interpretability methods such as Grad-CAM, Integrated Gradients [5], and\nSmoothGrad help highlight influential regions or tokens in model predictions but come with limitations: Grad-CAM\nis restricted to CNNs, Integrated Gradients requires carefully chosen baselines, and SmoothGrad’s perturbations add\ncomputational cost. For transformer-based models like Vision Transformers (ViTs), techniques such as Attention\nRollout aggregate attention across layers to trace information flow, and tools like BertViz [6] visualize attention\nheads, providing insights into the model’s decision-making. However, interpreting attention alone can be challenging,\nas attention weights don’t always correlate directly with feature importance, requiring careful analysis for a fuller\nunderstanding.\nIn response to the pressing challenges of interpretability in deep learning, we present, DLBacktrace a model-agnostic\nmethod for deep learning interpretability by tracing relevance from output to input, assigning relevance scores across\nlayers to reveal feature importance, information flow, and bias within predictions. Operating independently of auxiliary\nmodels or baselines, DLBacktrace ensures deterministic, consistent interpretations across various architectures and\ndata types, including images, text, and tabular data. This approach supports both local (instance-specific) and global\n(aggregate) analysis, enhancing transparency for models such as LLMs (e.g., BERT, Llama) and computer vision\narchitectures (e.g., ResNet, U-Net), making it a reliable tool for detailed model interpretation and validation. This\ntechnique addresses limitations in current interpretability frameworks by delivering stable, reliable explanations that\naccurately reflect decision-making pathways within a model, with insights at both local (instance-specific) and global\n(feature-aggregate) levels.\nIn this work, we make the following contributions:\n• Introduction of DLBacktrace: A detailed methodology outlining the model-agnostic and deterministic\napproach of DLBacktrace for achieving enhanced interpretability in AI systems.\n• Comprehensive Benchmarking: We benchmark DLBacktrace against widely used interpretability methods\n(e.g., LIME, SHAP, Grad-CAM, Integrated Gradients and more) across different tasks.\n• Cross-Modality Applications: DLBacktrace’s adaptability is illustrated across various data types, including\ntabular, image, and text, addressing limitations in current interpretability methods within these domains.\n• Framework for Reliable Interpretability: By providing consistent relevance scores, DLBacktrace contributes\nto more reliable, regulatory-compliant AI systems, supporting ethical and responsible AI deployment.\n2\nRelevant Literature\n2.1\nImportance of eXplainable AI (XAI)\n2.1.1\nXAI for Responsible and Trustworthy AI\nResponsible AI is essential for deploying systems that align with ethical standards and societal values, especially in\ncritical sectors like healthcare, finance, and law enforcement, where AI decisions can profoundly impact individuals\nand communities. Responsible AI emphasizes fairness, transparency, accountability, privacy, and ethical alignment\n2\n\nA PREPRINT - NOVEMBER 20, 2024\nto prevent bias, protect individual rights, and foster public trust. Fairness ensures that AI models do not discriminate\nand treat individuals equitably, while transparency provides clear explanations of AI decision-making to build trust.\nAccountability involves defining responsibility for AI outcomes and enabling human oversight. Privacy and security\nfocus on protecting personal data and ensuring system resilience against security threats, and ethical alignment ensures\nAI development respects human rights and societal norms.\nThere has been substantial progress toward responsible AI, supported by regulatory frameworks like the EU’s General\nData Protection Regulation (GDPR), which enforces transparency and accountability in automated decision-making.\nFurther advancements include interpretability methods like SHAP, LIME, and Grad-CAM, which aim to make complex\nmodels more accessible to stakeholders. Despite this, challenges remain. Current interpretability tools often provide\nlimited insights for complex models, display inconsistencies in high-dimensional data, and are not always applicable\nin real-time settings. Additionally, the rapid evolution of AI technologies outpaces regulatory responses, creating\noversight gaps, especially with advanced models such as large language models (LLMs). These challenges underscore\nthe ongoing need for innovations in responsible AI practices and supportive policy development.\nIn their work, Madsen et al. [7] advocate for moving beyond traditional interpretability methods, introducing three\nparadigms that focus on faithful model explanations: models with inherent faithfulness measures, those trained\nto provide faithful explanations, and self-explaining models. Their work highlights that trustworthy AI requires\nexplanations closely aligned with the model’s actual decision-making to prevent misunderstandings and misplaced\ntrust. Whereas, Singh et al. [8] explore interpretability challenges in large language models (LLMs), proposing tailored\nmethods to address their scale and complexity, enabling clearer insights into model behavior. Tull et al. [9] present a\ncategory theory-based framework to unify interpretability approaches, aiming for a cohesive understanding of model\nbehavior crucial to responsible AI. Dinu et al. [10] critically examine assumptions in feature attribution, finding that\nsome methods lack reliability, thus stressing the need for rigorous evaluation of interpretability techniques. While,\nKaur et al. [11] apply sensemaking theory to AI, promoting explanations that align with human cognitive processes to\nenhance trust and accountability by making model reasoning more accessible.\n2.1.2\nXAI for Safe AI\nAI safety aims to ensure AI systems are predictable, controllable, and aligned with human values, especially in\ncritical areas like healthcare, autonomous vehicles, and infrastructure. A key aspect of AI safety is explainability, as\ntransparent models enable developers, users, and regulators to understand system behavior, detect risks, and implement\nsafeguards. Core pillars of AI safety include robustness, reliability, alignment with human intentions, and the use of\nexplainability to clarify decision processes. Explainability is essential for risk mitigation in dynamic environments,\nwhere understanding AI behavior allows for safe intervention. For example, [12] discusses challenges like reward\nhacking and exploration hazards in reinforcement learning. Explainability techniques help analyze reward structures\nand behavior patterns, enabling developers to detect and correct unintended actions. Catastrophic forgetting, where\nmodels lose prior knowledge when learning new tasks, poses risks in sequential learning [13]. Explainable AI (XAI)\nmethods can identify model areas vulnerable to forgetting, supporting memory mechanisms that preserve safety-critical\ninformation. [14] introduces reward modeling to align AI with human preferences through feedback. Explainability tools\nprovide insights into reward structures’ impact on agent behavior, aiding iterative refinement to align model goals with\nhuman values. Explainability also supports adversarial robustness by revealing patterns in model vulnerability, guiding\ndefenses that enhance safety and reliability. These examples underscore explainability’s role in AI safety, enhancing\ntransparency, accountability, and risk mitigation. Embedding explainable practices within AI safety frameworks helps\ndevelopers control AI behavior, reducing risks in diverse operational contexts.\n2.1.3\nXAI for Regulatory AI\nExplainable AI (XAI) is crucial for regulatory compliance, promoting transparency, fairness, and accountability in AI-\ndriven decisions in sectors like finance, healthcare, and law. Regulatory frameworks increasingly mandate interpretable\nmodels to ensure oversight, protect user rights, and uphold ethical standards. Key elements of XAI in regulatory\ncontexts include transparent decision processes, model auditability, and mechanisms to mitigate bias. In finance, XAI\nhelps institutions clarify decisions on credit scoring, loan approvals, and fraud detection, reducing regulatory risks and\nfostering public trust. For instance, [15] systematically reviews XAI applications in finance, illustrating transparency’s\nrole in regulatory compliance. As large language models (LLMs) become ubiquitous, interpretability in NLP has gained\nimportance. [16] examines alignment of interpretability methods with stakeholder needs, categorizing techniques\nand identifying differences between developer and non-developer requirements. Stakeholder-centered frameworks\nhelp ensure more responsible AI deployment. In healthcare, XAI is vital for patient safety and ethical standards.\nExplainable models enable healthcare providers to interpret AI-driven diagnoses, treatments, and risk assessments,\naligning these with medical regulations. [17] addresses the challenge of conflicting post hoc explanations, often resolved\n3\n\nA PREPRINT - NOVEMBER 20, 2024\nby practitioners ad hoc. This study calls for standardized metrics to enhance explanation reliability, especially in\nhigh-stakes fields like healthcare and finance. Specific to LLMs, [8] explores their potential for interactive, natural\nlanguage explanations that can improve comprehension of complex behaviors. The authors address interpretability\nchallenges, such as hallucinations and computational cost, recommending LLM-based methods to improve transparency\nand nuanced insights in high-accountability domains. Finally, [18] critiques common interpretability techniques,\nhighlighting limitations in methods like Layer-wise Relevance Propagation (LRP) [19] and proposing Cosine Similarity\nConvergence (CSC) as a metric to improve explanation accuracy. These studies emphasize XAI’s role in enhancing\nregulatory compliance by fostering transparency, accountability, and fairness. Embedding explainable practices in\nregulatory frameworks ensures ethical AI use, facilitates oversight, and builds trust across regulated sectors.\n2.2\nExplainability Methods\n2.2.1\nTabular Data\nIn high-stakes applications such as finance and healthcare, machine learning models like regression and probabilistic\nalgorithms (e.g., decision trees and their variants) are often preferred. This is because deep learning models are\nfrequently described as \"black boxes,\" making it challenging to interpret how they arrive at their conclusions. This lack\nof transparency can be particularly problematic in critical contexts where accountability and trust are essential.\nTo enhance interpretability, explainable algorithms like LIME [3] and SHAP [4] are increasingly used. LIME (Local\nInterpretable Model-Agnostic Explanations) builds simple, interpretable models around specific data points to highlight\nthe most influential features for each prediction. SHAP (SHapley Additive exPlanations) assigns importance scores\nto each feature, providing both global explanations (by ranking features based on overall importance) and local\nexplanations (by illustrating how individual features contribute to specific predictions).\nHowever, these methods have limitations. LIME’s reliance on random sampling can lead to inconsistent explanations\nfor the same data point, and its effectiveness can be sensitive to the choice of perturbation method. SHAP, while\ncomprehensive, can be computationally expensive for large datasets and complex models. Additionally, SHAP’s\nmodel-agnostic nature may result in less accurate explanations for highly intricate models, like deep neural networks.\nAs a result, both LIME and SHAP may face challenges in providing precise, interpretable explanations for complex\ndeep learning models.\n2.2.2\nImage Data\nIn explainable AI (XAI) for image modality-based tasks, gradient-based methods such as GradCAM [20], Vanilla\nGradient [21], SmoothGrad [22], and Integrated Gradients [5] are widely used for interpreting model predictions.\nGradCAM generates heatmaps by calculating the gradient of the target class with respect to convolutional layer\nactivations, but may miss fine details and is sensitive to input noise. Vanilla Gradient directly computes gradients\non the input, though it faces the \"saturation problem,\" where gradients become too small for clear interpretation.\nSmoothGrad improves clarity by averaging gradients with added noise, albeit at a computational cost, while Integrated\nGradients addresses saturation by calculating an integral of gradients from a baseline to the input, though it also\ndemands significant computation.\nIn recent advances, Vision Transformers (ViTs) require specific interpretability approaches due to their reliance on\nattention mechanisms. In the paper [23], authors introduces TokenTM, a method designed for ViTs that considers both\ntoken transformations (changes in token length and direction) and attention weights across layers. By aggregating these\nfactors, TokenTM provides more focused and reliable explanations, addressing unique interpretability challenges in\ntransformer models. These developments reflect a shift in XAI, where interpretability techniques are tailored to the\nunique demands of model architectures, like CNNs and transformers, enhancing transparency and reliability across\ndifferent models.\n2.2.3\nTextual Data\nIn the text modality, quite a few explanation methods are employed to enhance the interpretability of machine learning\nmodels. Among these, LIME [3] and SHAP [4] are baseline for interpreting text classification models. Gradient-based\nmethods, such as GradCAM [20], Integrated Gradients [5], and Attention Rollout [24], also play a significant role\nacross diverse model architectures.\nFor text generation tasks, 17 challenges were identified by [25], such as tokenization effects and randomness, and\nadvocates for probabilistic explanations and perturbed benchmarks to address these issues. Furthermore, LACOAT\nintroduced by [26], which clusters word representations to produce context-aware explanations. It maps test features\nto latent clusters and translates these into natural language summaries, improving interpretability for complex NLP\n4\n\nA PREPRINT - NOVEMBER 20, 2024\ntasks. While [27] draws attention to the noise in explanations generated by large language models (LLMs), noting\nsignificant randomness. The study suggests that more sophisticated interpretability techniques, beyond simple word-level\nexplanations, are needed to achieve reliability. Lastly, [28] proposes a hybrid approach that combines counterfactual\nexplanations with domain knowledge. This method generates context-specific counterfactuals and incorporates user\nfeedback, enhancing BERT’s interpretability through expert insights and interactive elements, thus fostering a more\ntransparent framework.\nIn addition to the previously discussed methods, Mechanistic Interpretability has become a cornerstone of research\naimed at unraveling the internal mechanisms of large language models (LLMs). This field focuses on dissecting how\nspecific components, such as neurons and attention heads, contribute to a model’s functionality and decision-making\nprocesses. For example, Olah et al. [29] performed an in-depth analysis of individual neurons in LLMs, revealing\nhow certain neurons specialize in detecting specific linguistic features. Their work highlights the modularity and\nspecialization inherent in these models. Building on this foundation, Elhage et al. [30] introduced the concept of\n\"induction heads\" in transformer architectures. They demonstrated that these attention heads play a critical role in tasks\nsuch as sequence copying, offering insight into the mechanisms underlying certain model behaviors. Further advancing\nthe field, Nanda et al. [31] investigated the phenomenon of \"grokking\" in LLMs, where models suddenly exhibit strong\ngeneralization capabilities after extended training. Their analysis traced the internal changes leading to this abrupt\nperformance improvement, shedding light on this intriguing aspect of model behavior. These advancements underscore\nthe significance of mechanistic interpretability in enhancing our understanding of the intricate operations of LLMs. By\nmaking AI systems more transparent and reliable, this research is vital for fostering trust, particularly as LLMs are\nscaled and deployed in high-stakes applications.\n2.2.4\nMetrics for Benchmarking Explainability\nRecent advancements in Explainable AI (XAI) emphasize the need for robust evaluation frameworks, benchmarks,\nand specialized toolkits to enhance transparency and trust in machine learning systems. Quantus [32], provides a\nmodular toolkit with metrics such as faithfulness, robustness, and completeness, promoting reproducible and responsible\nevaluation of neural network explanations. BEExAI [33] addresses the underexplored domain of tabular data by\nbenchmarking explanation methods using tailored datasets and metrics like local fidelity and human interpretability,\nenabling systematic comparisons in real-world scenarios. In a survey over 30 XAI toolkits by [34], including Quantus,\nhighlighting challenges such as inconsistent metrics, lack of fairness assessments, and insufficient focus on human-\ncentered evaluation, calling for standardized benchmarks and unified platforms. PyXAI [35] complements these efforts\nby focusing on tree-based models, introducing efficient tools and unique metrics such as path importance and rule-level\ninterpretability for domains like healthcare and finance. Despite these advancements, key gaps remain, including\nstandardization across tools, evaluation for diverse data types, human-centered usability, and fairness and robustness\nassessments, underscoring the need for continued research to achieve actionable and responsible XAI.\n3\nBacktrace\n3.1\nIntroduction\nBacktrace is a technique for analyzing neural networks that involves tracing the relevance of each component from the\noutput back to the input.\nThis approach clarifies how each element contributes to the final prediction. By distributing relevance scores across\nvarious layers, Backtrace provides insights into feature importance, information flow, and potential biases, which\nfacilitates improved model interpretation and validation without relying on external dependencies.\nBacktrace has the following advantages over other available tools:\n• No dependence on a sample selection algorithm :\nThe relevance is calculated using just the sample in focus. This avoids deviations in importance due to varying\ntrends in sample datasets.\n• No dependence on a secondary white-box algorithm :\nThe relevance is calculated directly from the network itself. This prevents any variation in importance due to\ntype, hyperparameters and assumptions of secondary algorithms.\n• Deterministic in nature\nThe relevance scores won’t change on repeated calculations on the same sample. Hence, can be used in live\nenvironments or training workflows as a result of its independence from external factors.\n5\n\nA PREPRINT - NOVEMBER 20, 2024\n(a) Sample Network\n(b) Relevance Ouptput for Sample Network\nFigure 1: Illustration Depecting Backtrace Calculation for a Sample Network\n3.2\nMethodology\nBacktrace operates in two modes: Default Mode and Contrast Mode.\nFirst we describe the Basic Methodology of Backtrace in Default Mode as follows :\n3.2.1\nBasic Methodology\nEvery neural network consists of multiple layers. Each layer has a variation of the following basic operation:\ny = Φ(Wx + b)\nwhere,\n• Φ = activation function\n• W = weight matrix of the layer\n• b = bias\n• x = input\n• y = output\nThis can be further organized as:\ny = Φ(Xp + Xn + b)\nwhere,\n6\n\nA PREPRINT - NOVEMBER 20, 2024\n• Xp = P Wixi\n∀Wixi > 0\n• Xn = P Wixi\n∀Wixi < 0\nActivation functions can be categorized into monotonic and non-monotonic functions.\n• Non-Monotonic functions: The relevance is propagated as is.\n• Monotonic functions: The relevance is switched off for positive or negative components based on saturation.\n3.2.2\nRelevance Propagation\nThe aforementioned modes represent the basic operations at each source layer for propagating relevance to the\ndestination layer. The procedure for relevance calculation is as follows:\n1. Construct a graph from the model weights and architecture with output nodes as root and input nodes as leaves.\n2. Propagate relevance in a breadth-first manner, starting at the root.\n3. The propagation completes when all leaves (input nodes) have been assigned relevance.\nNote: Any loss of relevance during propagation is due to network bias.\nThe relevance of a single sample represents local importance. For global importance, the relevance of each feature can\nbe aggregated after normalization at the sample level.\n3.3\nAlgorithm\nThe algorithm has two modes of operation:\n• Default Mode\n• Contrastive Mode\n3.3.1\nDefault Mode\nIn this mode, a single relevance is associated with each unit. The relevance is propagated by proportionately distributing\nit between positive and negative components. If the relevance associated with y is ry and with x is rx, then for the jth\nunit in y, we compute:\nTj = Xpj + |Xnj| + |bj|\n(1)\nRpj = Xpj\nTj\nryj,\nRnj = Xnj\nTj\nryj,\nRbj = bj\nTj\nryj\n(2)\nRpj and Rnj are distributed among x in the following manner:\nrxij =\n\n\n\n\n\n\n\n\n\n\n\n\n\nWijxij\nXpj Rpj\nif Wijxij > 0\n0\nif Wijxij > 0 and Φ is saturated on negative end\n−Wijxij\nXnj\nRnj\nif Wijxij < 0\n0\nif Wijxij < 0 and Φ is saturated on positive end\n0\nif Wijxij = 0\n(3)\nThe total relevance at layer x is:\nrx =\nX\ni\nrxi\n(4)\n7\n\nA PREPRINT - NOVEMBER 20, 2024\n3.3.2\nContrastive Mode:\nIn this mode, each unit is assigned dual relevance, distributed between positive and negative components. This\napproach facilitates separate analyses of supporting and detracting influences. Unlike single-mode propagation, which\ncombines relevance into aggregated scores, dual-mode propagation provides clarity by isolating favorable and adverse\ncontributions. This separation enhances interpretability, enabling deeper insights into features that negatively impact\npredictions a capability essential for identifying counterfactuals or assessing model biases in high-stakes scenarios.\nIf the relevance associated with y are ryp, ryn and with x are rxp, rxn, then for the jth unit in y, we compute:\nTj = Xpj + Xnj + bj\n(5)\nWe then calculate Determine Rpj, Rnj, and Relevance Polarity as described in Algorithm 1.\nAlgorithm 1 Determine Rpj, Rnj, and relevance polarity in Contrastive Mode\n1: if Tj > 0 then\n2:\nif rypj > rynj then\n3:\nRpj ←rypj\n4:\nRnj ←rynj\n5:\nrelevance_polarity ←1\n6:\nelse\n7:\nRpj ←rynj\n8:\nRnj ←rypj\n9:\nrelevance_polarity ←−1\n10:\nend if\n11: else\n12:\nif rypj > rynj then\n13:\nRpj ←rynj\n14:\nRnj ←rypj\n15:\nrelevance_polarity ←−1\n16:\nelse\n17:\nRpj ←rypj\n18:\nRnj ←rynj\n19:\nrelevance_polarity ←1\n20:\nend if\n21: end if\nAfterwards, Rpj and Rnj are distributed among x as described in Algorithm 2.\nAlgorithm 2 Computation of rxp,ij and rxn,ij based on relevance polarity\n1: if relevance_polarity > 0 then\n2:\nrxp,ij ←Wijxij\nXpj Rpj\n∀Wijxij > 0\n3:\nrxn,ij ←−Wijxij\nXnj\nRnj\n∀Wijxij < 0\n4: else\n5:\nrxp,ij ←−Wijxij\nXnj\nRnj\n∀Wijxij < 0\n6:\nrxn,ij ←Wijxij\nXpj Rpj\n∀Wijxij > 0\n7: end if\nThe total positive and negative relevance at layer x are:\nrxp =\nX\ni\nrxp,i,\nrxn =\nX\ni\nrxn,i\n(6)\n8\n\nA PREPRINT - NOVEMBER 20, 2024\n3.4\nRelevance for Attention Layers:\nCurrently, the majority of AI models across various applications are primarily based on the attention mechanism [36].\nAccordingly, we have extended our Backtrace algorithm to provide support for this attention model [37].\nAttention mechanism allows the model to focus on specific parts of the input sequence, dynamically weighting the\nimportance of different elements when making predictions. The attention function employs the equation:\nAttention(Q, K, V ) = softmax\n\u0012QKT\n√dk\n\u0013\nV\n(7)\nFor Multi-Head Attention, the implementation is as follows:\nMultiHead(Q, K, V ) = Concat (head1, head2, ..., headn) W O\n(8)\nwhere each head is computed as\nheadi = Attention\n\u0010\nQW Q\ni , KW K\ni , V W V\ni\n\u0011\n(9)\nsuch that\n• Q, K, V: Query, Key, Value Matrices\n• W Q\ni , W K\ni , W V\ni : Weight matrices for the i-th head\n• W O: Weight matrix for combining all the heads after concatenation\n• Concat: Concatenation of the outputs from all attention heads\n3.4.1\nRelevance Propagation for Attention Layers\nSuppose the input to the attention layer is x and the output is y. The relevance associated with y is ry. To compute the\nrelevance using the Backtrace, we use the steps as indicated in Algorithm 3 below:\nAlgorithm 3 Relevance Propagation for Attention Layers\n1: Input: x (input to attention layer)\n2: Output: ry (relevance associated with y)\n3: We calculate the relevance rO of Concat (head1, head2, . . . , headn).Where rO represents the relevance from the\nlinear projection layer of the Attention module.\n4: To compute the relevance of QKT and V , use the following formulas:\nrQK = (rO · xV ) · xQK\n(10)\nrV = (xQK · rO) · xV\n(11)\nHere, xQK and xV are the outputs of QKT and V , respectively.\n5: Now that we have rQK, compute the relevance of rQ and rK as:\nrQ = (rQK · xQ) · xK\n(12)\nrK = (xK · rQK) · xQ\n(13)\nHere, xQ and xK are the outputs of Q and K, respectively.\n6: To compute rAttn, sum up rQ, rK, and rV :\nrAttn = rQ + rK + rV\n(14)\n4\nBenchmarking\nIn this section, we present a comparative study to benchmark our proposed Backtrace algorithm against various existing\nexplainability methods. The goal of this evaluation is to assess the effectiveness, robustness, and interpretability of\nBacktrace in providing meaningful insights into model predictions across different data modalities, including tabular,\n9\n\nA PREPRINT - NOVEMBER 20, 2024\nimage, and text data. By systematically comparing our approach with established methods, we aim to highlight the\nadvantages and potential limitations of Backtrace in the context of explainable artificial intelligence (XAI).\n4.1\nSetup\nThe experimental setup consists of three distinct data modalities: tabular, image, and text. Each modality is associated\nwith specific tasks, datasets, and model architectures tailored to effectively evaluate the explainability methods.\n4.1.1\nTabular Modality\nFor the tabular data modality, we focus on a binary classification task utilizing the Lending Club dataset. This dataset\nis representative of financial applications, containing features that capture various attributes of borrower profiles. We\nemploy a four-layer Multi-Layer Perceptron (MLP) neural network, which is well-suited for learning from structured\ndata and provides a foundation for assessing the performance of explainability techniques.\n4.1.2\nImage Modality\nIn the image data modality, we conduct a multi-class classification task using the CIFAR-10 dataset. This benchmark\ndataset consists of images across 10 different classes, making it ideal for evaluating image classification algorithms.\nFor this experiment, we utilize a fine-tuned ResNet-34 model, known for its deep residual learning capabilities, which\nenhances the model’s ability to learn intricate patterns and features within the images.\n4.1.3\nText Modality\nThe text data modality involves a binary classification task using the SST-2 dataset, which is focused on sentiment\nanalysis. The dataset consists of movie reviews labeled as positive or negative, allowing for a nuanced evaluation of\nsentiment classification models. We employ a pre-trained BERT model, which leverages transformer-based architectures\nto capture contextual relationships in text. This approach facilitates the generation of high-quality explanations for\nthe model’s predictions, enabling a thorough assessment of explainability methods in the realm of natural language\nprocessing.\n4.2\nMetrics\nTo assess the effectiveness of explanation methods across various modalities, we utilize different metrics tailored to\nspecific use cases. Further details are provided below:\n4.2.1\nTabular Modality\n• Maximal Perturbation Robustness Test (MPRT): This metric assesses the extent of perturbation that can be\napplied to an input before there are significant changes in the model’s explanation of its decision. It evaluates\nthe stability and robustness of the model’s explanations rather than solely its predictions.\n• Complexity Metric: This metric quantifies the level of detail in a model’s explanation by analyzing the\ndistribution of feature contributions.\n4.2.2\nImage Modality\n• Faithfulness Correlation: Faithfulness Correlation [38] metric evaluates the degree to which an explanation\naligns with the model’s behavior by calculating the correlation between feature importance and changes in\nmodel output resulting from perturbations of key features.\n• Max Sensitivity: Max-Sensitivity [39], is a robustness metric for explainability methods that evaluates\nhow sensitive explanations are to small perturbations in the input. Using a Monte Carlo sampling-based\napproximation, it measures the maximum change in the explanation when slight random modifications are\napplied to the input. Formally, it computes the maximum distance (e.g., using L1, L2, or L∞norms) between\nthe original explanation and those derived from perturbed inputs.\n• Pixel Flipping: Pixel Flipping method involves perturbing significant pixels and measuring the degradation in\nthe model’s prediction, thereby testing the robustness of the generated explanation.\n10\n\nA PREPRINT - NOVEMBER 20, 2024\n4.2.3\nText Modality\nTo evaluate the textual modality, we use the Token Perturbation for Explanation Quality (ToPEQ) metric, which assesses\nthe robustness of model explanations by analyzing the impact of token perturbations. We employ the Least Relevant\nFirst AUC (LeRF AUC) and Most Relevant First AUC (MoRF AUC) to measure sensitivity to the least and most\nimportant tokens, respectively. Additionally, we calculate Delta AUC, the difference between LeRF AUC and MoRF\nAUC, to further indicate the model’s ability to distinguish between important and unimportant features.\n• LeRF AUC (Least Relevant First AUC): This metric evaluates how gradually perturbing the least important\nfeatures (tokens) affects the model’s confidence. The AUC measures the model’s response as the least relevant\nfeatures are replaced with a baseline (e.g., [UNK]), indicating the degree to which the model relies on these\nfeatures.\n• MoRF AUC (Most Relevant First AUC): This metric measures how quickly the model’s performance\ndeteriorates when the most important features are perturbed first. The AUC represents the decrease in the\nmodel’s confidence as the most relevant tokens are removed, revealing the impact of these key features on the\nprediction.\n• Delta AUC: This metric represents the difference between LeRF AUC and MoRF AUC. It reflects the model’s\nsensitivity to the removal of important features (MoRF) compared to less important ones (LeRF). A larger delta\nsuggests that the explanation method effectively distinguishes between important and unimportant features.\n4.3\nExperiments\n4.3.1\nTabular Modality\nWe evaluated 1,024 samples from the test set of the Lending Club dataset, using a fine-tuned MLP checkpoint that\nattained an accuracy of 0.89 and a weighted average F1 score of 0.87. We assessed Backtrace against widely used\nmetrics for tabular data, specifically LIME and SHAP [4], and employed MPRT for comparison, along with Complexity\nto examine the simplicity of the model explanations as illustrated in Appendix A.1.1.\nTable 1: Explaination Performance metrics for explanation methods - LIME,SHAP and Backtrace, including Mean\nvalues and feature contributions across different layers (fc1 to fc4). Lower values in MPRT and Complexity indicate\nbetter performance.\nMethod\nMPRT (↓)\nComplexity\n(↓)\nMean\nfc1\nfc2\nfc3\nfc4\nLIME\n0.933\n0.934\n0.933\n0.933\n0.933\n2.57\nSHAP\n0.684\n0.718\n0.65\n0.699\n0.669\n1.234\nBacktrace\n0.562\n0.579\n0.561\n0.557\n0.552\n2.201\nThe proposed method, Backtrace, as demonstrated in Table 1, achieves superior performance compared to both\nLIME and SHAP, evidenced by lower Maximal Perturbation Robustness Test (MPRT) values across various layers.\nThis highlights its improved interpretability and robustness in explainability. However, Backtrace exhibits higher\ncomputational complexity, reflecting the fine-grained, higher entropy of its explanations. This trade-off suggests the\nneed to balance the quality of interpretability with the simplicity of model explanations.\n4.3.2\nImage Modality\nWe conducted an evaluation on 500 samples from the CIFAR-10 test set using a supervised, fine-tuned ResNet-34\nmodel, which achieved a test accuracy of 75.85%. We compared Backtrace against several methods as illustrated in\nAppendix A.1.2, including Grad-CAM, vanilla gradient, smooth gradient, and integrated gradient. The comparison\nutilized metrics such as Faithfulness Correlation, Max Sensitivity, and Pixel Flipping.\n11\n\nA PREPRINT - NOVEMBER 20, 2024\nTable 2: Performance metrics of various explanation methods for a subset of CIFAR10 test set samples. Higher values\n(↑) of Faithfulness Correlation indicate better performance, while lower values (↓) of Max Sensitivity and Pixel Flipping\nsuggest improved robustness. (*) - Indicates the presence of infinite values in some batches, for which a non-infinite\nmean was used to calculate the final value.\nExplanation\nMethod\nFaithfulness\nCorrelation (↑)\nMax\nSensitivity (↓)\nPixel\nFlipping (↓)\nGradCAM\n0.010\n1070(*)\n0.249\nVanilla Gradient\n0.011\n154(*)\n0.253\nSmooth Grad\n0.018\n158(*)\n0.252\nIntegrated Gradient\n0.009\n169(*)\n0.253\nBacktrace\n0.199\n0.617\n0.199\nThe Evaluations conducted as shown in Table 2 reveals that Backtrace significantly outperforms traditional methods\nlike Grad-CAM, Vanilla Gradient, Smooth Gradient, and Integrated Gradient across all key metrics. Backtrace achieves\na superior Faithfulness Correlation score of (0.199), indicating a stronger alignment between its explanations and the\nmodel’s behavior. Additionally, it demonstrates robust performance with much lower Max Sensitivity (0.617) and Pixel\nFlipping (0.199) scores, highlighting its stability against input perturbations and better robustness in preserving the\nmodel’s predictive integrity under pixel modifications. Overall, Backtrace establishes itself as a more reliable and robust\nexplainability technique for image modality tasks.\n4.3.3\nText Modality\nFor the text modality, evaluation was performed on the evaluation set of the SST-2 dataset using a fine-tuned BERT\nmodel1, achieving an F1 score of 0.926. Since explainable AI (XAI) for BERT and other transformer-based models is\nrelatively new, we employed metrics based on token perturbation for explanation quality, specifically LeRF, MoRF, and\nDelta AUC, as introduced in [37]. We used methods like IG, SmoothGrad, AttnRoll, GradCAM and Input Grad as\nillustrated in Appendix A.1.5.\nTable 3: Token Perturbation for Explanation Quality metrics for various explanation methods. Lower MoRF AUC\nvalues indicate better performance, while higher LeRF AUC and Delta AUC values suggest greater robustness and\nbetter differentiation between relevant and irrelevant features.\nMethod\nMoRF\nAUC (↓)\nLeRF\nAUC (↑)\nDelta\nAUC (↑)\nIG\n-6.723\n46.756\n53.479\nSmooth Grad\n14.568\n38.264\n23.696\nAttn Roll\n16.123\n37.937\n21.814\nBacktrace\n15.431\n30.69\n15.259\nGradCAM\n19.955\n21.714\n1.759\nRandom\n25.068\n25.684\n0.616\nInput Grad\n27.784\n19.34\n-8.444\nAs shown in Table 3, Integrated Gradients (IG) delivered the strongest performance, achieving the lowest MoRF AUC\nand the highest LeRF and Delta AUC, underscoring the robustness of its explanations and precise feature attribution.\nSmooth Grad and Attn Roll also demonstrated commendable performance. Backtrace exhibited balanced results across\nLeRF and Delta AUC metrics, with a MoRF AUC of 15.431, showcasing its ability to provide meaningful explanations\nwhile allowing scope for further enhancement. These findings highlight the effectiveness of IG in explainability for\ntransformer-based models while recognizing Backtrace’s potential as a promising and competitive approach.\n4.4\nObservations\nThe quality of explanations is influenced by both the input data and the model weights. The impact of model performance\nis significant; low model performance tends to result in unstable explanations characterized by high entropy, while good\nmodel performance is associated with stable explanations that are more sparse. Additionally, the inference time for a\nsample is proportional to both the size of the model and the computational infrastructure used.\n1Model checkpoint used https://huggingface.co/textattack/bert-base-uncased-SST-2\n12\n\nA PREPRINT - NOVEMBER 20, 2024\n5\nDiscussion\nAdditional illustrations of Backtrace for various use cases are provided in Appendix A.\n5.1\nAdvantages of Backtrace\n5.1.1\nNetwork Analysis\n• Existing solutions involve distribution graphs and heatmaps for any network node based on node activation.\n• These are accurate for that specific node but don’t represent the impact of that node on the final prediction.\n• Existing solutions are also unable to differentiate between the impact of input sources versus the internal\nnetwork biases.\n5.1.2\nFeature Importance\n• With each input source being assigned a fraction of the overall weightage, we can now quantify the dependence\nof the final prediction on each input source.\n• We can also evaluate the dependence within the input source as the weight assignment happens on a per unit\nbasis.\n• Integrated Gradients and Shapley values are other methods available for calculating feature importance from\nDeep Learning Models. Both come with caveats and give approximate values:\n– Integrated Gradients depends on a baseline sample which needs to be constructed for the dataset and\naltered as the dataset shifts. This is extremely difficult for high-dimensional datasets.\n– Shapley Values are calculated on a sample set selected from the complete dataset. This makes those\nvalues highly dependent on the selection of data.\n5.1.3\nUncertainty\n• Instead of just relying on the final prediction score for decision-making, the validity of the decision can now\nbe determined based on the weight distribution of any particular node with respect to the prior distribution of\ncorrect and incorrect predictions.\n5.2\nApplicability\nThe Backtrace framework is applicable in the following use-cases:\n5.2.1\nInterpreting the model outcomes using the local and global importance of each feature\nThe local importance is directly inferred from the relevance associated with input data layers. For inferring global\nimportance, the local importance of each sample is normalized with respect to the model outcome of that sample. The\nnormalized local importance from all samples is then averaged to provide global importance. The averaging can be\nfurther graded based on the various outcomes and binning of the model outcome.\n5.2.2\nNetwork analysis based on the relevance attributed to each layer in the network\nThe two modes together provide a lot of information for each layer, such as:\n• Bias to input ratio\n• Activation Saturation\n• Positive and negative relevance (unit-wise and complete layer)\nUsing this information, layers can be modified to increase or decrease variability and reduce network bias. Major\nchanges to the network architecture via complete shutdown of nodes or pathways are also possible based on the total\ncontribution of that component.\n5.2.3\nFairness and bias analysis using the feature-wise importance\nThis is in continuation of the global importance of features. Based on the global importance of sensitive features (e.g.\ngender, age, etc.) and their alignment with the data, it can be inferred whether the model or data have undue bias\ntowards any feature value.\n13\n\nA PREPRINT - NOVEMBER 20, 2024\n5.2.4\nProcess Compliance based on the ranking of features on local and global levels\nUsing the local and global importance of features and ranking them accordingly, it can be determined whether the\nmodel is considering the features in the same manner as in the business process it is emulating. This also helps in\nevaluating the solution’s alignment with various business and regulatory requirements.\n5.2.5\nValidating the model outcome\nEvery model is analyzed based on certain performance metrics which are calculated over a compiled validation dataset.\nThis doesn’t represent the live deployment scenario.\nDuring deployment, validation of outcomes is extremely important for complete autonomous systems. The layer-wise\nrelevance can be used for accomplishing this. The relevance for each layer is mapped in the vector space of the same\ndimension as the layer outcome, yet it is linearly related to the model outcome.\nSince the information changes as it passes through the network, the relevance from lower layers, even input layers, can\nbe used to get different outcomes. These outcomes can be used to validate the model outcome. The layers are generally\nmulti-dimensional, for which either proximity-based methods or white-box regression algorithms can be used to derive\noutcomes.\n6\nConclusion\nIn this paper, we introduced the DLBacktrace, a new method that significantly improves model interpretability for\ndeep learning. DLBacktrace traces relevance from output back to input, giving clear and consistent insights into which\nfeatures are important and how information flows through the model. Unlike existing methods, which often rely on\nchanging inputs or using other algorithms, DLBacktrace is stable and reliable, making it especially useful in fields that\nneed high transparency, like finance, healthcare, and regulatory compliance. Our benchmarking results demonstrate\nthat DLBacktrace performs better in terms of robustness and accuracy across various model types, proving it can\nprovide practical insights. Overall, DLBacktrace contributes to the growing field of explainable AI by enhancing model\ntransparency and trustworthiness, promoting responsible AI deployment in critical applications.\n7\nFuture Works\nFuture research on DLBacktrace will aim to broaden its use and improve how it scores relevance. Key areas for\ndevelopment include adapting DLBacktrace for complex and evolving model architectures, like advanced transformers\nand multimodal models to ensure it remains effective across different AI applications. Additionally, we aim to reduce\nthe inference time making DLBacktrace more suitable for real-time applications in production environments such as\nautonomous systems and dynamic decision-making scenarios.\nFuture development will also explore the use of DLBacktrace for specific model improvements, including diagnosis\nand targeted editing. For example, DLBacktrace could assist in model pruning by identifying less critical components,\ntehreby optimizing model performance and efficiency. In addition, DLBacktrace’s potential for targeted model\nimprovements will be explored. It can assist in model pruning, especially for Mixtures of Experts (MoEs), by\nidentifying underutilized components or redundant experts to optimize performance and efficiency. It can also help\nin facilitate model merging, providing insights for seamless integration of multiple models, and layer swapping,\nenabling selective replacement of layers to enhance adaptability or performance. We also plan to apply DLBacktrace to\nout-of-distribution (OOD) detection, where it can help distinguish instances that fall outside the model’s training data,\nenhancing the robustness and reliability of AI systems.\nFurthermore, extending DLBacktrace’s support for model-agnostic explainability will allow it to be seamlessly applied\nacross various architectures, making it a versatile tool in explainable AI. These improvements will make DLBacktrace\nmore useful and establish it as an important tool for understanding and improving models across a wide range of AI\napplications and tasks.\n14\n\nA PREPRINT - NOVEMBER 20, 2024\nA\nIllustrations\nA.1\nIllustrations for Various Tasks\nA.1.1\nTabular Modality : Binary Classification\n(a) Backtrace\n(b) LIME\n(c) SHAP\nFigure 2: Illustration of Explanations of a Correctly Classified Sample from the Lending Club Dataset where Loan was\nFully Paid and was predicted by MLP as Fully Paid.\n(a) Backtrace\n(b) LIME\n(c) SHAP\nFigure 3: Illustration of Explanations of a Incorrectly Classified Sample from the Lending Club Dataset where Loan\nwas Fully Paid and was predicted by MLP as Charged Off.\nA.1.2\nImage Modality : Multi-Class Classification\n(a) Original Image\n(b) Backtrace\n(c) Smooth Grad\n(d) Vanilla Gradient\n(e) IG\n(f) GradCAM\nFigure 4: Visualizing ResNet’s decisions on a Horse image of CIFAR10 Dataset using various explanation methods.\n(a) Original Image\n(b) Backtrace\n(c) Smooth Grad\n(d) Vanilla Gradient\n(e) IG\n(f) GradCAM\nFigure 5: Visualizing ResNet’s decisions on a Truck image of CIFAR10 Dataset using various explanation methods.\n15\n\nA PREPRINT - NOVEMBER 20, 2024\nA.1.3\nImage Modality : Object Segmentation\n(a) Original Image\n(b) Model Prediction\n(c) Label\n(d) GradCAM\n(e) Backtrace Default\n(f) Backtrace Contrastive (Pos)\n(g) Backtrace Contrastive (Neg)\nFigure 6: Analysis of a U-Net segmentation model’s decision-making on a CamVid Dataset Sample. The figure shows\nthe original Image, Model Prediction, and Label, alongside Explanations of GradCAM and Backtrace visualizations in\nDefault and Contrastive modes.\n(a) Original Image\n(b) Prediction\n(c) Label\n(d) GradCAM\n(e) Backtrace Default\n(f) Backtrace Contrastive (Pos) (g) Backtrace Contrastive (Neg)\nFigure 7: Analysis of a Tumour Segmentation Model’s decision-making on a ClinicdB Dataset Sample. The figure shows\nthe original Image, Model Prediction, and Label, alongside Explanations of GradCAM and Backtrace visualizations in\nDefault and Contrastive modes.\n16\n\nA PREPRINT - NOVEMBER 20, 2024\nA.1.4\nImage Modality : Object Detection\n(a) Original Image\n(b) Prediction & Label\n(c) GradCAM\n(d) Backtrace\nFigure 8: Explanations of the model’s decision-making process on a Bird image from the CUB-200 dataset, using\nGrad-CAM and Backtrace to highlight the key regions influencing the prediction.\n(a) Original Image\n(b) Prediction & Label\n(c) GradCAM\n(d) Backtrace\nFigure 9: Explanations of the model’s decision-making process on a duck image from the CUB-200 dataset, using\nGrad-CAM and Backtrace to highlight the key regions influencing the prediction.\nA.1.5\nText Modality : BERT Sentiment Classification\n-1.000\n-0.500\n0.000\n0.500\n1.000\n\"[CLS]\",\n\"the\",\n\"emotions\",\n\"are\",\n\"raw\",\n\"and\",\n\"will\",\n\"strike\",\n\"a\",\n\"nerve\",\n\"with\",\n\"anyone\",\n\"who\",\n\"'\",\n\"s\",\n\"ever\",\n\"had\",\n\"family\",\n\"trauma\",\n\".\",\n\"[SEP]\"\nBacktrace\nSmoothGrad\nRandom\ninputgrad\nIG\ngradcam\nattnroll\nBacktrace, SmoothGrad, Random, inputgrad, IG…\nFigure 10: Explanations by different methods for model decision making for Sentiment Analysis for a sample from\nSST Dataset. Input Text: The emotions are raw and will strike a nerve with anyone who ever had family trauma.\nPrediction: 1 and Label: 1\n17\n\nA PREPRINT - NOVEMBER 20, 2024\nA.1.6\nText Modality : Multi-Class Classification\n(a) Original Image\n(b) Prediction & Label\n(c) Backtrace\nFigure 11: Explanations of the model’s decision-making process for multi-class topic detection for incorrect classi-\nfication on a Model using Pre-Trained Glove Word Embedding and 1D CNN, using LIME, SHAP and Backtrace to\nhighlight the key regions influencing the prediction.\n(a) LIME\n(b) SHAP\n(c) Backtrace\nFigure 12: Explanations of the model’s decision-making process for multi-class topic detection for correct classification\non a Model using Pre-Trained Glove Word Embedding and 1D CNN, using LIME, SHAP and Backtrace to highlight\nthe key regions influencing the prediction.\n18\n\nA PREPRINT - NOVEMBER 20, 2024\nA.1.7\nText Modality: Llama3.2\nFigure 13:\nInput Prompt: Classify the text into neutral, negative, or positive. Text: The product mentioned full coverage, but my\nclaim wasn’t accepted, and the experience was not something I would recommend to others. Sentiment of the text is:\nOuput : Negative. Reason: The text expresses dissatisfaction with the product and the experience, stating that the\nclaim was not accepted and the experience was not something the author would recommend. This indicates a negative\nsentiment. Note: The text does not contain any explicit negative\nFigure 14: Input Prompt: Provide a function that calculates the n-th fibonacci number.\nOutput: Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones, usually\nstarting with 0 and 1.Fibonacci Function. Here’s a Python function that calculates the n-th Fibonacci number using\nmemoization to improve performance.\ndef fibonacci(n, memo=):\nCalculate the n-th Fibonacci number.\nArgs: n (int): The position of the Fibonacci number to calculate. memo (dict): A dictionary to store previously\ncalculated Fibonacci numbers.\nReturns: int: The n-th Fibonacci number.\nif n <= 0:\nreturn 0\nelif n == 1:\nreturn 1\nelif n not in memo:\nmemo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)\nreturn memo[n]\nExample usage: print(fibonacci(10))\nOutput: 55\n19\n\nA PREPRINT - NOVEMBER 20, 2024\nFigure 15:\nInput Prompt: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her\nfriends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much\nin dollars does she make daily at the farmers’ market?\nOutput: Step 1: Calculate the number of eggs laid per day Janet’s ducks lay 16 eggs per day. Step 2: Calculate the\nnumber of eggs eaten per day. Janet eats 3 eggs for breakfast every morning.\n20\n\nA PREPRINT - NOVEMBER 20, 2024\nReferences\n[1] OpenAI. OpenAI ChatGPT. https://openai.com/index/chatgpt/. [Accessed 12-11-2024].\n[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.\n[3] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “why should i trust you?”: Explaining the predictions\nof any classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining, 2016.\n[4] Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Neural Information\nProcessing Systems, 2017.\n[5] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International\nConference on Machine Learning, 2017.\n[6] Jesse Vig. A multiscale visualization of attention in the transformer model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations, pages 37–42, Florence, Italy,\nJuly 2019. Association for Computational Linguistics.\n[7] Andreas Madsen, Himabindu Lakkaraju, Siva Reddy, and Sarath Chandar. Interpretability needs a new paradigm.\narXiv preprint arXiv:2405.05386, 2024.\n[8] Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. Rethinking interpretability\nin the era of large language models. ArXiv, abs/2402.01761, 2024.\n[9] Sean Tull, Robin Lorenz, Stephen Clark, Ilyas Khan, and Bob Coecke. Towards compositional interpretability for\nxai. ArXiv, abs/2406.17583, 2024.\n[10] Jonathan Dinu, Jeffrey P. Bigham, J. Zico Kolter Unaffiliated, and Carnegie Mellon University. Challenging\ncommon interpretability assumptions in feature attribution explanations. ArXiv, abs/2012.02748, 2020.\n[11] Harmanpreet Kaur, Eytan Adar, Eric Gilbert, and Cliff Lampe. Sensible ai: Re-imagining interpretability and\nexplainability using sensemaking theory. Proceedings of the 2022 ACM Conference on Fairness, Accountability,\nand Transparency, 2022.\n[12] Dario Amodei, Christopher Olah, Jacob Steinhardt, Paul Francis Christiano, John Schulman, and Dandelion Mané.\nConcrete problems in ai safety. ArXiv, abs/1606.06565, 2016.\n[13] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan\nKumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National\nAcademy of Sciences, 114(13):3521–3526, 2017.\n[14] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment\nvia reward modeling: a research direction. ArXiv, abs/1811.07871, 2018.\n[15] Patrick Maximilian Weber, Kim Valerie Carl, and Oliver Hinz. Applications of explainable artificial intelligence\nin finance—a systematic review of finance, information systems, and computer science literature. Management\nReview Quarterly, 74:867–907, 2023.\n[16] Nitay Calderon and Roi Reichart. On behalf of the stakeholders: Trends in nlp model interpretability in the era of\nllms. ArXiv, abs/2407.19200, 2024.\n[17] Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and Himabindu Lakkaraju.\nThe disagreement problem in explainable machine learning: A practitioner’s perspective. Trans. Mach. Learn.\nRes., 2024, 2022.\n[18] Leon Sixt, Maximilian Granz, and Tim Landgraf. When explanations lie: Why many modified bp attributions fail.\nIn International Conference on Machine Learning, 2019.\n[19] Grégoire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus Müller. Layer-wise\nrelevance propagation: An overview. In Explainable AI, 2019.\n[20] Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv\nBatra. Grad-cam: Visual explanations from deep networks via gradient-based localization. International Journal\nof Computer Vision, 128:336 – 359, 2016.\n[21] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising\nimage classification models and saliency maps. CoRR, abs/1312.6034, 2013.\n21\n\nA PREPRINT - NOVEMBER 20, 2024\n[22] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B. Viégas, and Martin Wattenberg. Smoothgrad: removing\nnoise by adding noise. ArXiv, abs/1706.03825, 2017.\n[23] Junyi Wu, Bin Duan, Weitai Kang, Hao Tang, and Yan Yan. Token transformation matters: Towards faithful\npost-hoc explanation for vision transformer. 2024 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10926–10935, 2024.\n[24] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Annual Meeting of the\nAssociation for Computational Linguistics, 2020.\n[25] Kenza Amara, Rita Sevastjanova, and Mennatallah El-Assady. Challenges and opportunities in text generation\nexplainability. In xAI, 2024.\n[26] Xuemin Yu, Fahim Dalvi, Nadir Durrani, and Hassan Sajjad. Latent concept-based explanation of nlp models.\nArXiv, abs/2404.12545, 2024.\n[27] Jérémie Bogaert and François-Xavier Standaert. A question on the explainability of large language models and\nthe word-level univariate first-order plausibility assumption. ArXiv, abs/2403.10275, 2024.\n[28] Aroua Hedhili Sbaï and Islem Bouallagui. Hybrid approach to explain bert model: Sentiment analysis case. In\nInternational Conference on Agents and Artificial Intelligence, 2024.\n[29] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An\nintroduction to circuits. Distill, 2020. https://distill.pub/2020/circuits/zoom-in.\n[30] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny\nHernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,\nJared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer\nCircuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html.\n[31] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking\nvia mechanistic interpretability. ArXiv, abs/2301.05217, 2023.\n[32] Anna Hedström, Leander Weber, Dilyara Bareeva, Franz Motzkus, Wojciech Samek, Sebastian Lapuschkin, and\nMarina M.-C. Höhne. Quantus: An explainable ai toolkit for responsible evaluation of neural network explanations.\nArXiv, abs/2202.06861, 2022.\n[33] Samuel Sithakoul, Sara Meftah, and Clément Feutry. Beexai: Benchmark to evaluate explainable ai. In xAI, 2024.\n[34] Phuong Quynh Le, Meike Nauta, Van Bach Nguyen, Shreyasi Pathak, Jörg Schlötterer, and Christin Seifert.\nBenchmarking explainable ai - a survey on available toolkits and open challenges. In International Joint Conference\non Artificial Intelligence, 2023.\n[35] Gilles Audemard, Jean-Marie Lagniez, Pierre Marquis, and Nicolas Szczepanski. PyXAI: An XAI Library for\nTree-Based Models. In The 33rd International Joint Conference on Artificial Intelligence, pages 8601–8605, Jeju\nIsland (South Korea), South Korea, August 2024.\n[36] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017.\n[37] Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand,\nSebastian Lapuschkin, and Wojciech Samek. Attnlrp: Attention-aware layer-wise relevance propagation for\ntransformers. ArXiv, abs/2402.05602, 2024.\n[38] Umang Bhatt, Adrian Weller, and José M. F. Moura. Evaluating and aggregating feature-based model explanations.\nIn International Joint Conference on Artificial Intelligence, 2020.\n[39] Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Sai Suggala, David I. Inouye, and Pradeep Ravikumar. On the (in)fidelity\nand sensitivity for explanations. arXiv: Learning, 2019.\n22",
    "pdf_filename": "DLBacktrace_A_Model_Agnostic_Explainability_for_any_Deep_Learning_Models.pdf"
}