{
    "title": "DLBACKTRACE: A MODEL AGNOSTIC EXPLAINABILITY FOR",
    "abstract": "Therapidadvancementofartificialintelligencehasledtoincreasinglysophisticateddeeplearning models,whichfrequentlyoperateasopaque“blackboxes”withlimitedtransparencyintheirdecision- making processes. This lack of interpretability presents considerable challenges, especially in high-stakesapplicationswhereunderstandingtherationalebehindamodel’soutputsisasessential astheoutputsthemselves. ThisstudyaddressesthepressingneedforinterpretabilityinAIsystems, emphasizingitsroleinfosteringtrust,ensuringaccountability,andpromotingresponsibledeployment inmission-criticalfields. Toaddresstheinterpretabilitychallengeindeeplearning,weintroduce DLBacktrace,aninnovativetechniquedevelopedbytheAryaXAIteamtoilluminatemodeldecisions acrossawidearrayofdomains,includingsimpleMultiLayerPerceptron(MLPs),Convolutional NeuralNetworks(CNNs),LargeLanguageModels(LLMs),ComputerVisionModels,andmore. WeprovideacomprehensiveoverviewoftheDLBacktracealgorithmandpresentbenchmarking results,comparingitsperformanceagainstestablishedinterpretabilitymethods,suchasSHAP,LIME, GradCAM, Integrated Gradients, SmoothGrad, and Attention Rollout, using diverse task-based metrics. TheproposedDLBacktracetechniqueiscompatiblewithvariousmodelarchitecturesbuilt in PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP architectures such as BERTandLSTMs,computervisionmodelslikeResNetandU-Net,aswellascustomdeepneural network(DNN)modelsfortabulardata. ThisflexibilityunderscoresDLBacktrace’sadaptabilityand effectivenessinenhancingmodeltransparencyacrossabroadspectrumofapplications. Thelibraryis open-sourcedandavailableathttps://github.com/AryaXAI/DLBacktrace. 1 Introduction Despiteremarkableadvancementsinartificialintelligence,particularlywiththeevolutionofdeeplearningarchitectures, even the most sophisticated models face a persistent challenge: they often operate as \"black boxes,\" with internal processesthatremainopaqueanddifficulttointerpret. Thesemodelsproducehighlyaccuratepredictions,yetprovide limitedinsightsintohowandwhytheymakespecificdecisions. Thisopacityraisessignificantconcerns,especially inhigh-stakesapplicationslikehealthcare,finance,andlawenforcement,whereunderstandingtherationalebehind modeloutputsiscritical. Forexample,inthehealthcaresector,AI-drivendiagnosticsmustbeinterpretabletoensure thatmedicalprofessionalscantrustandactonrecommendationsforpatienttreatment. Similarly,infinance,regulations suchastheEuropeanUnion’sGDPRmandatea\"righttoexplanation\"forautomateddecisionsaffectingindividuals, makingexplainabilitynotonlyanethicalimperativebutalsoaregulatoryrequirement. The growing demand for explainability and transparency in AI systems is often eclipsed by the prevailing focus onmaximizingrawperformance. ThistrendisespeciallyevidentwiththeincreasinguseofmodelslikeOpenAI’s 4202 voN 91 ]GL.sc[ 1v34621.1142:viXra",
    "body": "DLBACKTRACE: A MODEL AGNOSTIC EXPLAINABILITY FOR\nANY DEEP LEARNING MODELS\nVinayKumarSankarapu,ChintanChitroda,YashwardhanRathore\n{vinay, chintan.chitroda, yashwardhan.rathore}@aryaxai.com\nNeerajKumarSingh,PratinavSeth\n{neeraj.singh, pratinav.seth}@aryaxai.com\nAryaXAI\nNovember20,2024\nABSTRACT\nTherapidadvancementofartificialintelligencehasledtoincreasinglysophisticateddeeplearning\nmodels,whichfrequentlyoperateasopaque“blackboxes”withlimitedtransparencyintheirdecision-\nmaking processes. This lack of interpretability presents considerable challenges, especially in\nhigh-stakesapplicationswhereunderstandingtherationalebehindamodel’soutputsisasessential\nastheoutputsthemselves. ThisstudyaddressesthepressingneedforinterpretabilityinAIsystems,\nemphasizingitsroleinfosteringtrust,ensuringaccountability,andpromotingresponsibledeployment\ninmission-criticalfields. Toaddresstheinterpretabilitychallengeindeeplearning,weintroduce\nDLBacktrace,aninnovativetechniquedevelopedbytheAryaXAIteamtoilluminatemodeldecisions\nacrossawidearrayofdomains,includingsimpleMultiLayerPerceptron(MLPs),Convolutional\nNeuralNetworks(CNNs),LargeLanguageModels(LLMs),ComputerVisionModels,andmore.\nWeprovideacomprehensiveoverviewoftheDLBacktracealgorithmandpresentbenchmarking\nresults,comparingitsperformanceagainstestablishedinterpretabilitymethods,suchasSHAP,LIME,\nGradCAM, Integrated Gradients, SmoothGrad, and Attention Rollout, using diverse task-based\nmetrics. TheproposedDLBacktracetechniqueiscompatiblewithvariousmodelarchitecturesbuilt\nin PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP architectures such as\nBERTandLSTMs,computervisionmodelslikeResNetandU-Net,aswellascustomdeepneural\nnetwork(DNN)modelsfortabulardata. ThisflexibilityunderscoresDLBacktrace’sadaptabilityand\neffectivenessinenhancingmodeltransparencyacrossabroadspectrumofapplications. Thelibraryis\nopen-sourcedandavailableathttps://github.com/AryaXAI/DLBacktrace.\n1 Introduction\nDespiteremarkableadvancementsinartificialintelligence,particularlywiththeevolutionofdeeplearningarchitectures,\neven the most sophisticated models face a persistent challenge: they often operate as \"black boxes,\" with internal\nprocessesthatremainopaqueanddifficulttointerpret. Thesemodelsproducehighlyaccuratepredictions,yetprovide\nlimitedinsightsintohowandwhytheymakespecificdecisions. Thisopacityraisessignificantconcerns,especially\ninhigh-stakesapplicationslikehealthcare,finance,andlawenforcement,whereunderstandingtherationalebehind\nmodeloutputsiscritical. Forexample,inthehealthcaresector,AI-drivendiagnosticsmustbeinterpretabletoensure\nthatmedicalprofessionalscantrustandactonrecommendationsforpatienttreatment. Similarly,infinance,regulations\nsuchastheEuropeanUnion’sGDPRmandatea\"righttoexplanation\"forautomateddecisionsaffectingindividuals,\nmakingexplainabilitynotonlyanethicalimperativebutalsoaregulatoryrequirement.\nThe growing demand for explainability and transparency in AI systems is often eclipsed by the prevailing focus\nonmaximizingrawperformance. ThistrendisespeciallyevidentwiththeincreasinguseofmodelslikeOpenAI’s\n4202\nvoN\n91\n]GL.sc[\n1v34621.1142:viXra\nAPREPRINT-NOVEMBER20,2024\nChatGPT[1],Meta’sLLaMA[2],Google’sGemini,andsimilarlargelanguagemodels(LLMs)thatarewidelyadopted\nfortaskssuchaslanguagegeneration,classification,andcomplexquestionanswering. Thesemodelsfrequentlyoperate\nas‘blackboxes,’offeringlimitedinsightintotheirdecision-makingprocesses. Manyoftheirarchitecturesarealso\nproprietary and closed-source, adding to their opacity. However, due to their high accuracy and utility in various\napplications, theirlackoftransparencyisoftenacceptedbyusersandstakeholders. Consequently, interpretability\nisfrequentlyoverlookedorconsideredsecondarytomodelperformanceindeployment. Thisdisregardcanleadto\nchallenges,suchasincreaseduncertaintyinresponsestoout-of-domainquestionsandpotentialregulatoryconcerns.\nOvertheyears, variouseffortshavebeenmadetomakemachinelearningmodelsmoretransparent, leadingtothe\ndevelopmentofinterpretabilitymethods. PopularapproacheslikeLocalInterpretableModel-agnosticExplanations\n(LIME) [3] and SHapley Additive exPlanations (SHAP) [4] aim to clarify model decisions by assigning feature\nimportancescores. Thesemethodsworkwellintabulardatacontextsbutencounterchallengeswithcomplexdata\ntypessuchasimagesandtext,wherefeatureinteractionsandcontextualnuancesareintricate. BothLIMEandSHAP\nrequirerepeatedevaluationsanddataperturbationstoestimateimportancescores,whichcanincreasecomputational\nloadsignificantly,especiallyinhigh-dimensionalcontexts. Theygenerateexplanationsbyanalyzingselectedsubsets\nof data, which leads to instance-specific (local) feature importance. This method may hinder their ability to offer\nacomprehensive,model-wideperspective,impactingbothinterpretabilityandreliability. Thisissueisparticularly\npronouncedwhenreal-timeinsightsareneeded,asthetimerequiredtogenerateexplanationsincreasesexponentially\nwiththesizeofthedatasubset.\nForcomplexdatalikeimagesandtext, interpretabilitymethodssuchasGrad-CAM,IntegratedGradients[5], and\nSmoothGradhelphighlightinfluentialregionsortokensinmodelpredictionsbutcomewithlimitations: Grad-CAM\nisrestrictedtoCNNs,IntegratedGradientsrequirescarefullychosenbaselines,andSmoothGrad’sperturbationsadd\ncomputational cost. For transformer-based models like Vision Transformers (ViTs), techniques such as Attention\nRollout aggregate attention across layers to trace information flow, and tools like BertViz [6] visualize attention\nheads,providinginsightsintothemodel’sdecision-making. However,interpretingattentionalonecanbechallenging,\nas attention weights don’t always correlate directly with feature importance, requiring careful analysis for a fuller\nunderstanding.\nInresponsetothepressingchallengesofinterpretabilityindeeplearning,wepresent,DLBacktraceamodel-agnostic\nmethodfordeeplearninginterpretabilitybytracingrelevancefromoutputtoinput,assigningrelevancescoresacross\nlayerstorevealfeatureimportance,informationflow,andbiaswithinpredictions. Operatingindependentlyofauxiliary\nmodelsorbaselines,DLBacktraceensuresdeterministic,consistentinterpretationsacrossvariousarchitecturesand\ndatatypes,includingimages,text,andtabulardata. Thisapproachsupportsbothlocal(instance-specific)andglobal\n(aggregate) analysis, enhancing transparency for models such as LLMs (e.g., BERT, Llama) and computer vision\narchitectures(e.g.,ResNet,U-Net),makingitareliabletoolfordetailedmodelinterpretationandvalidation. This\ntechniqueaddresseslimitationsincurrentinterpretabilityframeworksbydeliveringstable,reliableexplanationsthat\naccuratelyreflectdecision-makingpathwayswithinamodel,withinsightsatbothlocal(instance-specific)andglobal\n(feature-aggregate)levels.\nInthiswork,wemakethefollowingcontributions:\n• Introduction of DLBacktrace: A detailed methodology outlining the model-agnostic and deterministic\napproachofDLBacktraceforachievingenhancedinterpretabilityinAIsystems.\n• ComprehensiveBenchmarking: WebenchmarkDLBacktraceagainstwidelyusedinterpretabilitymethods\n(e.g.,LIME,SHAP,Grad-CAM,IntegratedGradientsandmore)acrossdifferenttasks.\n• Cross-ModalityApplications: DLBacktrace’sadaptabilityisillustratedacrossvariousdatatypes,including\ntabular,image,andtext,addressinglimitationsincurrentinterpretabilitymethodswithinthesedomains.\n• FrameworkforReliableInterpretability:Byprovidingconsistentrelevancescores,DLBacktracecontributes\ntomorereliable,regulatory-compliantAIsystems,supportingethicalandresponsibleAIdeployment.\n2 RelevantLiterature\n2.1 ImportanceofeXplainableAI(XAI)\n2.1.1 XAIforResponsibleandTrustworthyAI\nResponsibleAIisessentialfordeployingsystemsthatalignwithethicalstandardsandsocietalvalues,especiallyin\ncriticalsectorslikehealthcare,finance,andlawenforcement,whereAIdecisionscanprofoundlyimpactindividuals\nandcommunities. ResponsibleAIemphasizesfairness,transparency,accountability,privacy,andethicalalignment\n2\nAPREPRINT-NOVEMBER20,2024\ntopreventbias,protectindividualrights,andfosterpublictrust. FairnessensuresthatAImodelsdonotdiscriminate\nandtreatindividualsequitably,whiletransparencyprovidesclearexplanationsofAIdecision-makingtobuildtrust.\nAccountabilityinvolvesdefiningresponsibilityforAIoutcomesandenablinghumanoversight. Privacyandsecurity\nfocusonprotectingpersonaldataandensuringsystemresilienceagainstsecuritythreats,andethicalalignmentensures\nAIdevelopmentrespectshumanrightsandsocietalnorms.\nTherehasbeensubstantialprogresstowardresponsibleAI,supportedbyregulatoryframeworksliketheEU’sGeneral\nDataProtectionRegulation(GDPR),whichenforcestransparencyandaccountabilityinautomateddecision-making.\nFurtheradvancementsincludeinterpretabilitymethodslikeSHAP,LIME,andGrad-CAM,whichaimtomakecomplex\nmodelsmoreaccessibletostakeholders. Despitethis,challengesremain. Currentinterpretabilitytoolsoftenprovide\nlimitedinsightsforcomplexmodels,displayinconsistenciesinhigh-dimensionaldata,andarenotalwaysapplicable\nin real-time settings. Additionally, the rapid evolution of AI technologies outpaces regulatory responses, creating\noversightgaps,especiallywithadvancedmodelssuchaslargelanguagemodels(LLMs). Thesechallengesunderscore\ntheongoingneedforinnovationsinresponsibleAIpracticesandsupportivepolicydevelopment.\nIntheirwork,Madsenetal.[7]advocateformovingbeyondtraditionalinterpretabilitymethods,introducingthree\nparadigms that focus on faithful model explanations: models with inherent faithfulness measures, those trained\nto provide faithful explanations, and self-explaining models. Their work highlights that trustworthy AI requires\nexplanationscloselyalignedwiththemodel’sactualdecision-makingtopreventmisunderstandingsandmisplaced\ntrust. Whereas,Singhetal.[8]exploreinterpretabilitychallengesinlargelanguagemodels(LLMs),proposingtailored\nmethodstoaddresstheirscaleandcomplexity,enablingclearerinsightsintomodelbehavior. Tulletal.[9]presenta\ncategorytheory-basedframeworktounifyinterpretabilityapproaches,aimingforacohesiveunderstandingofmodel\nbehaviorcrucialtoresponsibleAI.Dinuetal.[10]criticallyexamineassumptionsinfeatureattribution,findingthat\nsomemethodslackreliability,thusstressingtheneedforrigorousevaluationofinterpretabilitytechniques. While,\nKauretal.[11]applysensemakingtheorytoAI,promotingexplanationsthatalignwithhumancognitiveprocessesto\nenhancetrustandaccountabilitybymakingmodelreasoningmoreaccessible.\n2.1.2 XAIforSafeAI\nAI safety aims to ensure AI systems are predictable, controllable, and aligned with human values, especially in\ncriticalareaslikehealthcare,autonomousvehicles,andinfrastructure. AkeyaspectofAIsafetyisexplainability,as\ntransparentmodelsenabledevelopers,users,andregulatorstounderstandsystembehavior,detectrisks,andimplement\nsafeguards. CorepillarsofAIsafetyincluderobustness,reliability,alignmentwithhumanintentions,andtheuseof\nexplainabilitytoclarifydecisionprocesses. Explainabilityisessentialforriskmitigationindynamicenvironments,\nwhereunderstandingAIbehaviorallowsforsafeintervention. Forexample, [12]discusseschallengeslikereward\nhackingandexplorationhazardsinreinforcementlearning. Explainabilitytechniqueshelpanalyzerewardstructures\nandbehaviorpatterns,enablingdeveloperstodetectandcorrectunintendedactions. Catastrophicforgetting,where\nmodelslosepriorknowledgewhenlearningnewtasks,posesrisksinsequentiallearning[13]. ExplainableAI(XAI)\nmethodscanidentifymodelareasvulnerabletoforgetting,supportingmemorymechanismsthatpreservesafety-critical\ninformation.[14]introducesrewardmodelingtoalignAIwithhumanpreferencesthroughfeedback.Explainabilitytools\nprovideinsightsintorewardstructures’impactonagentbehavior,aidingiterativerefinementtoalignmodelgoalswith\nhumanvalues. Explainabilityalsosupportsadversarialrobustnessbyrevealingpatternsinmodelvulnerability,guiding\ndefensesthatenhancesafetyandreliability. Theseexamplesunderscoreexplainability’sroleinAIsafety,enhancing\ntransparency,accountability,andriskmitigation. EmbeddingexplainablepracticeswithinAIsafetyframeworkshelps\ndeveloperscontrolAIbehavior,reducingrisksindiverseoperationalcontexts.\n2.1.3 XAIforRegulatoryAI\nExplainableAI(XAI)iscrucialforregulatorycompliance,promotingtransparency,fairness,andaccountabilityinAI-\ndrivendecisionsinsectorslikefinance,healthcare,andlaw. Regulatoryframeworksincreasinglymandateinterpretable\nmodels to ensure oversight, protect user rights, and uphold ethical standards. Key elements of XAI in regulatory\ncontextsincludetransparentdecisionprocesses,modelauditability,andmechanismstomitigatebias. Infinance,XAI\nhelpsinstitutionsclarifydecisionsoncreditscoring,loanapprovals,andfrauddetection,reducingregulatoryrisksand\nfosteringpublictrust. Forinstance,[15]systematicallyreviewsXAIapplicationsinfinance,illustratingtransparency’s\nroleinregulatorycompliance. Aslargelanguagemodels(LLMs)becomeubiquitous,interpretabilityinNLPhasgained\nimportance. [16] examines alignment of interpretability methods with stakeholder needs, categorizing techniques\nandidentifyingdifferencesbetweendeveloperandnon-developerrequirements. Stakeholder-centeredframeworks\nhelp ensure more responsible AI deployment. In healthcare, XAI is vital for patient safety and ethical standards.\nExplainablemodelsenablehealthcareproviderstointerpretAI-drivendiagnoses,treatments,andriskassessments,\naligningthesewithmedicalregulations.[17]addressesthechallengeofconflictingposthocexplanations,oftenresolved\n3\nAPREPRINT-NOVEMBER20,2024\nby practitioners ad hoc. This study calls for standardized metrics to enhance explanation reliability, especially in\nhigh-stakesfieldslikehealthcareandfinance. SpecifictoLLMs,[8]explorestheirpotentialforinteractive,natural\nlanguageexplanationsthatcanimprovecomprehensionofcomplexbehaviors. Theauthorsaddressinterpretability\nchallenges,suchashallucinationsandcomputationalcost,recommendingLLM-basedmethodstoimprovetransparency\nand nuanced insights in high-accountability domains. Finally, [18] critiques common interpretability techniques,\nhighlightinglimitationsinmethodslikeLayer-wiseRelevancePropagation(LRP)[19]andproposingCosineSimilarity\nConvergence(CSC)asametrictoimproveexplanationaccuracy. ThesestudiesemphasizeXAI’sroleinenhancing\nregulatorycompliancebyfosteringtransparency, accountability, andfairness. Embeddingexplainablepracticesin\nregulatoryframeworksensuresethicalAIuse,facilitatesoversight,andbuildstrustacrossregulatedsectors.\n2.2 ExplainabilityMethods\n2.2.1 TabularData\nInhigh-stakesapplicationssuchasfinanceandhealthcare,machinelearningmodelslikeregressionandprobabilistic\nalgorithms (e.g., decision trees and their variants) are often preferred. This is because deep learning models are\nfrequentlydescribedas\"blackboxes,\"makingitchallengingtointerprethowtheyarriveattheirconclusions. Thislack\noftransparencycanbeparticularlyproblematicincriticalcontextswhereaccountabilityandtrustareessential.\nToenhanceinterpretability,explainablealgorithmslikeLIME[3]andSHAP[4]areincreasinglyused. LIME(Local\nInterpretableModel-AgnosticExplanations)buildssimple,interpretablemodelsaroundspecificdatapointstohighlight\nthemostinfluentialfeaturesforeachprediction. SHAP(SHapleyAdditiveexPlanations)assignsimportancescores\nto each feature, providing both global explanations (by ranking features based on overall importance) and local\nexplanations(byillustratinghowindividualfeaturescontributetospecificpredictions).\nHowever,thesemethodshavelimitations. LIME’srelianceonrandomsamplingcanleadtoinconsistentexplanations\nfor the same data point, and its effectiveness can be sensitive to the choice of perturbation method. SHAP, while\ncomprehensive, can be computationally expensive for large datasets and complex models. Additionally, SHAP’s\nmodel-agnosticnaturemayresultinlessaccurateexplanationsforhighlyintricatemodels,likedeepneuralnetworks.\nAsaresult,bothLIMEandSHAPmayfacechallengesinprovidingprecise,interpretableexplanationsforcomplex\ndeeplearningmodels.\n2.2.2 ImageData\nInexplainableAI(XAI)forimagemodality-basedtasks, gradient-basedmethodssuchasGradCAM[20], Vanilla\nGradient [21], SmoothGrad [22], and Integrated Gradients [5] are widely used for interpreting model predictions.\nGradCAM generates heatmaps by calculating the gradient of the target class with respect to convolutional layer\nactivations, but may miss fine details and is sensitive to input noise. Vanilla Gradient directly computes gradients\non the input, though it faces the \"saturation problem,\" where gradients become too small for clear interpretation.\nSmoothGradimprovesclaritybyaveraginggradientswithaddednoise,albeitatacomputationalcost,whileIntegrated\nGradients addresses saturation by calculating an integral of gradients from a baseline to the input, though it also\ndemandssignificantcomputation.\nInrecentadvances,VisionTransformers(ViTs)requirespecificinterpretabilityapproachesduetotheirrelianceon\nattentionmechanisms. Inthepaper[23],authorsintroducesTokenTM,amethoddesignedforViTsthatconsidersboth\ntokentransformations(changesintokenlengthanddirection)andattentionweightsacrosslayers. Byaggregatingthese\nfactors,TokenTMprovidesmorefocusedandreliableexplanations,addressinguniqueinterpretabilitychallengesin\ntransformermodels. ThesedevelopmentsreflectashiftinXAI,whereinterpretabilitytechniquesaretailoredtothe\nuniquedemandsofmodelarchitectures,likeCNNsandtransformers,enhancingtransparencyandreliabilityacross\ndifferentmodels.\n2.2.3 TextualData\nInthetextmodality,quiteafewexplanationmethodsareemployedtoenhancetheinterpretabilityofmachinelearning\nmodels. Amongthese,LIME[3]andSHAP[4]arebaselineforinterpretingtextclassificationmodels. Gradient-based\nmethods, suchasGradCAM[20], IntegratedGradients[5], andAttentionRollout[24], alsoplayasignificantrole\nacrossdiversemodelarchitectures.\nFortextgenerationtasks, 17challengeswereidentifiedby[25], suchastokenizationeffectsandrandomness, and\nadvocatesforprobabilisticexplanationsandperturbedbenchmarkstoaddresstheseissues. Furthermore,LACOAT\nintroducedby[26],whichclusterswordrepresentationstoproducecontext-awareexplanations. Itmapstestfeatures\ntolatentclustersandtranslatestheseintonaturallanguagesummaries,improvinginterpretabilityforcomplexNLP\n4\nAPREPRINT-NOVEMBER20,2024\ntasks. While[27]drawsattentiontothenoiseinexplanationsgeneratedbylargelanguagemodels(LLMs),noting\nsignificantrandomness.Thestudysuggeststhatmoresophisticatedinterpretabilitytechniques,beyondsimpleword-level\nexplanations,areneededtoachievereliability. Lastly,[28]proposesahybridapproachthatcombinescounterfactual\nexplanationswithdomainknowledge. Thismethodgeneratescontext-specificcounterfactualsandincorporatesuser\nfeedback,enhancingBERT’sinterpretabilitythroughexpertinsightsandinteractiveelements,thusfosteringamore\ntransparentframework.\nInadditiontothepreviouslydiscussedmethods,MechanisticInterpretabilityhasbecomeacornerstoneofresearch\naimedatunravelingtheinternalmechanismsoflargelanguagemodels(LLMs). Thisfieldfocusesondissectinghow\nspecificcomponents,suchasneuronsandattentionheads,contributetoamodel’sfunctionalityanddecision-making\nprocesses. Forexample,Olahetal. [29]performedanin-depthanalysisofindividualneuronsinLLMs,revealing\nhow certain neurons specialize in detecting specific linguistic features. Their work highlights the modularity and\nspecialization inherent in these models. Building on this foundation, Elhage et al. [30] introduced the concept of\n\"inductionheads\"intransformerarchitectures. Theydemonstratedthattheseattentionheadsplayacriticalroleintasks\nsuchassequencecopying,offeringinsightintothemechanismsunderlyingcertainmodelbehaviors. Furtheradvancing\nthefield,Nandaetal. [31]investigatedthephenomenonof\"grokking\"inLLMs,wheremodelssuddenlyexhibitstrong\ngeneralizationcapabilitiesafterextendedtraining. Theiranalysistracedtheinternalchangesleadingtothisabrupt\nperformanceimprovement,sheddinglightonthisintriguingaspectofmodelbehavior. Theseadvancementsunderscore\nthesignificanceofmechanisticinterpretabilityinenhancingourunderstandingoftheintricateoperationsofLLMs. By\nmakingAIsystemsmoretransparentandreliable,thisresearchisvitalforfosteringtrust,particularlyasLLMsare\nscaledanddeployedinhigh-stakesapplications.\n2.2.4 MetricsforBenchmarkingExplainability\nRecentadvancementsinExplainableAI(XAI)emphasizetheneedforrobustevaluationframeworks,benchmarks,\nand specialized toolkits to enhance transparency and trust in machine learning systems. Quantus [32], provides a\nmodulartoolkitwithmetricssuchasfaithfulness,robustness,andcompleteness,promotingreproducibleandresponsible\nevaluation of neural network explanations. BEExAI [33] addresses the underexplored domain of tabular data by\nbenchmarkingexplanationmethodsusingtailoreddatasetsandmetricslikelocalfidelityandhumaninterpretability,\nenablingsystematiccomparisonsinreal-worldscenarios. Inasurveyover30XAItoolkitsby[34],includingQuantus,\nhighlightingchallengessuchasinconsistentmetrics,lackoffairnessassessments,andinsufficientfocusonhuman-\ncenteredevaluation,callingforstandardizedbenchmarksandunifiedplatforms. PyXAI[35]complementstheseefforts\nbyfocusingontree-basedmodels,introducingefficienttoolsanduniquemetricssuchaspathimportanceandrule-level\ninterpretability for domains like healthcare and finance. Despite these advancements, key gaps remain, including\nstandardizationacrosstools,evaluationfordiversedatatypes,human-centeredusability,andfairnessandrobustness\nassessments,underscoringtheneedforcontinuedresearchtoachieveactionableandresponsibleXAI.\n3 Backtrace\n3.1 Introduction\nBacktraceisatechniqueforanalyzingneuralnetworksthatinvolvestracingtherelevanceofeachcomponentfromthe\noutputbacktotheinput.\nThisapproachclarifieshoweachelementcontributestothefinalprediction. Bydistributingrelevancescoresacross\nvarious layers, Backtrace provides insights into feature importance, information flow, and potential biases, which\nfacilitatesimprovedmodelinterpretationandvalidationwithoutrelyingonexternaldependencies.\nBacktracehasthefollowingadvantagesoverotheravailabletools:\n• Nodependenceonasampleselectionalgorithm:\nTherelevanceiscalculatedusingjustthesampleinfocus. Thisavoidsdeviationsinimportanceduetovarying\ntrendsinsampledatasets.\n• Nodependenceonasecondarywhite-boxalgorithm:\nTherelevanceiscalculateddirectlyfromthenetworkitself. Thispreventsanyvariationinimportancedueto\ntype,hyperparametersandassumptionsofsecondaryalgorithms.\n• Deterministicinnature\nTherelevancescoreswon’tchangeonrepeatedcalculationsonthesamesample. Hence,canbeusedinlive\nenvironmentsortrainingworkflowsasaresultofitsindependencefromexternalfactors.\n5\nAPREPRINT-NOVEMBER20,2024\n(a)SampleNetwork (b)RelevanceOuptputforSampleNetwork\nFigure1: IllustrationDepectingBacktraceCalculationforaSampleNetwork\n3.2 Methodology\nBacktraceoperatesintwomodes: DefaultModeandContrastMode.\nFirstwedescribetheBasicMethodologyofBacktraceinDefaultModeasfollows:\n3.2.1 BasicMethodology\nEveryneuralnetworkconsistsofmultiplelayers. Eachlayerhasavariationofthefollowingbasicoperation:\ny =Φ(Wx+b)\nwhere,\n• Φ=activationfunction\n• W =weightmatrixofthelayer\n• b=bias\n• x=input\n• y=output\nThiscanbefurtherorganizedas:\ny =Φ(X +X +b)\np n\nwhere,\n6\nAPREPRINT-NOVEMBER20,2024\n(cid:80)\n• X = W x ∀W x >0\np i i i i\n(cid:80)\n• X = W x ∀W x <0\nn i i i i\nActivationfunctionscanbecategorizedintomonotonicandnon-monotonicfunctions.\n• Non-Monotonicfunctions: Therelevanceispropagatedasis.\n• Monotonicfunctions: Therelevanceisswitchedoffforpositiveornegativecomponentsbasedonsaturation.\n3.2.2 RelevancePropagation\nThe aforementioned modes represent the basic operations at each source layer for propagating relevance to the\ndestinationlayer. Theprocedureforrelevancecalculationisasfollows:\n1. Constructagraphfromthemodelweightsandarchitecturewithoutputnodesasrootandinputnodesasleaves.\n2. Propagaterelevanceinabreadth-firstmanner,startingattheroot.\n3. Thepropagationcompleteswhenallleaves(inputnodes)havebeenassignedrelevance.\nNote: Anylossofrelevanceduringpropagationisduetonetworkbias.\nTherelevanceofasinglesamplerepresentslocalimportance. Forglobalimportance,therelevanceofeachfeaturecan\nbeaggregatedafternormalizationatthesamplelevel.\n3.3 Algorithm\nThealgorithmhastwomodesofoperation:\n• DefaultMode\n• ContrastiveMode\n3.3.1 DefaultMode\nInthismode,asinglerelevanceisassociatedwitheachunit. Therelevanceispropagatedbyproportionatelydistributing\nitbetweenpositiveandnegativecomponents. Iftherelevanceassociatedwithyisr andwithxisr ,thenforthejth\ny x\nunitiny,wecompute:\nT =X +|X |+|b | (1)\nj pj nj j\nX X b\nR = pjr , R = njr , R = jr (2)\npj T yj nj T yj bj T yj\nj j j\nR andR aredistributedamongxinthefollowingmanner:\npj nj\nWijxijR\nifW x >0\n0Xpj pj\nifW\nii jj\nx\nii jj\n>0andΦissaturatedonnegativeend\nr xij = 00−W Xi nj jxijR nj iii fff WWW ii jj xxx ii jj\n<\n=< 00\n0andΦissaturatedonpositiveend\n(3)\nij ij\nThetotalrelevanceatlayerxis:\n(cid:88)\nr = r (4)\nx xi\ni\n7\nAPREPRINT-NOVEMBER20,2024\n3.3.2 ContrastiveMode:\nIn this mode, each unit is assigned dual relevance, distributed between positive and negative components. This\napproachfacilitatesseparateanalysesofsupportinganddetractinginfluences. Unlikesingle-modepropagation,which\ncombinesrelevanceintoaggregatedscores,dual-modepropagationprovidesclaritybyisolatingfavorableandadverse\ncontributions. Thisseparationenhancesinterpretability,enablingdeeperinsightsintofeaturesthatnegativelyimpact\npredictionsacapabilityessentialforidentifyingcounterfactualsorassessingmodelbiasesinhigh-stakesscenarios.\nIftherelevanceassociatedwithyarer ,r andwithxarer ,r ,thenforthejthunitiny,wecompute:\nyp yn xp xn\nT =X +X +b (5)\nj pj nj j\nWethencalculateDetermineR ,R ,andRelevancePolarityasdescribedinAlgorithm1.\npj nj\nAlgorithm1DetermineR ,R ,andrelevancepolarityinContrastiveMode\npj nj\n1: ifT j >0then\n2: ifr ypj >r ynj then\n3: R pj ←r ypj\n4: R nj ←r ynj\n5: relevance_polarity←1\n6: else\n7: R pj ←r ynj\n8: R nj ←r ypj\n9: relevance_polarity←−1\n10: endif\n11: else\n12: ifr ypj >r ynj then\n13: R pj ←r ynj\n14: R nj ←r ypj\n15: relevance_polarity←−1\n16: else\n17: R pj ←r ypj\n18: R nj ←r ynj\n19: relevance_polarity←1\n20: endif\n21: endif\nAfterwards,R andR aredistributedamongxasdescribedinAlgorithm2.\npj nj\nAlgorithm2Computationofr andr basedonrelevancepolarity\nxp,ij xn,ij\n1: ifrelevance_polarity>0then\n2: r xp,ij ← W Xij px jijR pj ∀W ijx ij >0\n3: r xn,ij ← −W Xi nj jxijR nj ∀W ijx ij <0\n4: else\n5: r xp,ij ← −W Xi nj jxijR nj ∀W ijx ij <0\n6: r xn,ij ← W Xij px jijR pj ∀W ijx ij >0\n7: endif\nThetotalpositiveandnegativerelevanceatlayerxare:\n(cid:88) (cid:88)\nr = r , r = r (6)\nxp xp,i xn xn,i\ni i\n8\nAPREPRINT-NOVEMBER20,2024\n3.4 RelevanceforAttentionLayers:\nCurrently,themajorityofAImodelsacrossvariousapplicationsareprimarilybasedontheattentionmechanism[36].\nAccordingly,wehaveextendedourBacktracealgorithmtoprovidesupportforthisattentionmodel[37].\nAttentionmechanismallowsthemodeltofocusonspecificpartsoftheinputsequence,dynamicallyweightingthe\nimportanceofdifferentelementswhenmakingpredictions. Theattentionfunctionemploystheequation:\n(cid:18) QKT(cid:19)\nAttention(Q,K,V)=softmax √ V (7)\nd\nk\nForMulti-HeadAttention,theimplementationisasfollows:\nMultiHead(Q,K,V)=Concat(head ,head ,...,head )WO (8)\n1 2 n\nwhereeachheadiscomputedas\n(cid:16) (cid:17)\nhead =Attention QWQ,KWK,VWV (9)\ni i i i\nsuchthat\n• Q,K,V:Query,Key,ValueMatrices\n• WQ,WK,WV: Weightmatricesforthei-thhead\ni i i\n• WO: Weightmatrixforcombiningalltheheadsafterconcatenation\n• Concat: Concatenationoftheoutputsfromallattentionheads\n3.4.1 RelevancePropagationforAttentionLayers\nSupposetheinputtotheattentionlayerisxandtheoutputisy. Therelevanceassociatedwithyisr . Tocomputethe\ny\nrelevanceusingtheBacktrace,weusethestepsasindicatedinAlgorithm3below:\nAlgorithm3RelevancePropagationforAttentionLayers\n1: Input: x(inputtoattentionlayer)\n2: Output: r y (relevanceassociatedwithy)\n3: Wecalculatetherelevancer O ofConcat(head 1,head 2,...,head n).Wherer O representstherelevancefromthe\nlinearprojectionlayeroftheAttentionmodule.\n4: TocomputetherelevanceofQKT andV,usethefollowingformulas:\nr =(r ·x )·x (10)\nQK O V QK\nr =(x ·r )·x (11)\nV QK O V\nHere,x andx aretheoutputsofQKT andV,respectively.\nQK V\n5: Nowthatwehaver QK,computetherelevanceofr Qandr K as:\nr =(r ·x )·x (12)\nQ QK Q K\nr =(x ·r )·x (13)\nK K QK Q\nHere,x andx aretheoutputsofQandK,respectively.\nQ K\n6: Tocomputer Attn,sumupr Q,r K,andr V:\nr =r +r +r (14)\nAttn Q K V\n4 Benchmarking\nInthissection,wepresentacomparativestudytobenchmarkourproposedBacktracealgorithmagainstvariousexisting\nexplainabilitymethods. Thegoalofthisevaluationistoassesstheeffectiveness,robustness,andinterpretabilityof\nBacktraceinprovidingmeaningfulinsightsintomodelpredictionsacrossdifferentdatamodalities,includingtabular,\n9\nAPREPRINT-NOVEMBER20,2024\nimage,andtextdata. Bysystematicallycomparingourapproachwithestablishedmethods,weaimtohighlightthe\nadvantagesandpotentiallimitationsofBacktraceinthecontextofexplainableartificialintelligence(XAI).\n4.1 Setup\nTheexperimentalsetupconsistsofthreedistinctdatamodalities: tabular,image,andtext. Eachmodalityisassociated\nwithspecifictasks,datasets,andmodelarchitecturestailoredtoeffectivelyevaluatetheexplainabilitymethods.\n4.1.1 TabularModality\nForthetabulardatamodality,wefocusonabinaryclassificationtaskutilizingtheLendingClubdataset. Thisdataset\nisrepresentativeoffinancialapplications,containingfeaturesthatcapturevariousattributesofborrowerprofiles. We\nemployafour-layerMulti-LayerPerceptron(MLP)neuralnetwork,whichiswell-suitedforlearningfromstructured\ndataandprovidesafoundationforassessingtheperformanceofexplainabilitytechniques.\n4.1.2 ImageModality\nIntheimagedatamodality,weconductamulti-classclassificationtaskusingtheCIFAR-10dataset. Thisbenchmark\ndatasetconsistsofimagesacross10differentclasses,makingitidealforevaluatingimageclassificationalgorithms.\nForthisexperiment,weutilizeafine-tunedResNet-34model,knownforitsdeepresiduallearningcapabilities,which\nenhancesthemodel’sabilitytolearnintricatepatternsandfeatureswithintheimages.\n4.1.3 TextModality\nThetextdatamodalityinvolvesabinaryclassificationtaskusingtheSST-2dataset,whichisfocusedonsentiment\nanalysis. Thedatasetconsistsofmoviereviewslabeledaspositiveornegative,allowingforanuancedevaluationof\nsentimentclassificationmodels.Weemployapre-trainedBERTmodel,whichleveragestransformer-basedarchitectures\ntocapturecontextualrelationshipsintext. Thisapproachfacilitatesthegenerationofhigh-qualityexplanationsfor\nthemodel’spredictions,enablingathoroughassessmentofexplainabilitymethodsintherealmofnaturallanguage\nprocessing.\n4.2 Metrics\nToassesstheeffectivenessofexplanationmethodsacrossvariousmodalities,weutilizedifferentmetricstailoredto\nspecificusecases. Furtherdetailsareprovidedbelow:\n4.2.1 TabularModality\n• MaximalPerturbationRobustnessTest(MPRT):Thismetricassessestheextentofperturbationthatcanbe\nappliedtoaninputbeforetherearesignificantchangesinthemodel’sexplanationofitsdecision. Itevaluates\nthestabilityandrobustnessofthemodel’sexplanationsratherthansolelyitspredictions.\n• Complexity Metric: This metric quantifies the level of detail in a model’s explanation by analyzing the\ndistributionoffeaturecontributions.\n4.2.2 ImageModality\n• FaithfulnessCorrelation: FaithfulnessCorrelation[38]metricevaluatesthedegreetowhichanexplanation\nalignswiththemodel’sbehaviorbycalculatingthecorrelationbetweenfeatureimportanceandchangesin\nmodeloutputresultingfromperturbationsofkeyfeatures.\n• Max Sensitivity: Max-Sensitivity [39], is a robustness metric for explainability methods that evaluates\nhow sensitive explanations are to small perturbations in the input. Using a Monte Carlo sampling-based\napproximation,itmeasuresthemaximumchangeintheexplanationwhenslightrandommodificationsare\nappliedtotheinput. Formally,itcomputesthemaximumdistance(e.g.,usingL ,L ,orL norms)between\n1 2 ∞\ntheoriginalexplanationandthosederivedfromperturbedinputs.\n• PixelFlipping: PixelFlippingmethodinvolvesperturbingsignificantpixelsandmeasuringthedegradationin\nthemodel’sprediction,therebytestingtherobustnessofthegeneratedexplanation.\n10\nAPREPRINT-NOVEMBER20,2024\n4.2.3 TextModality\nToevaluatethetextualmodality,weusetheTokenPerturbationforExplanationQuality(ToPEQ)metric,whichassesses\ntherobustnessofmodelexplanationsbyanalyzingtheimpactoftokenperturbations. WeemploytheLeastRelevant\nFirst AUC (LeRF AUC) and Most Relevant First AUC (MoRF AUC) to measure sensitivity to the least and most\nimportanttokens,respectively. Additionally,wecalculateDeltaAUC,thedifferencebetweenLeRFAUCandMoRF\nAUC,tofurtherindicatethemodel’sabilitytodistinguishbetweenimportantandunimportantfeatures.\n• LeRFAUC(LeastRelevantFirstAUC):Thismetricevaluateshowgraduallyperturbingtheleastimportant\nfeatures(tokens)affectsthemodel’sconfidence. TheAUCmeasuresthemodel’sresponseastheleastrelevant\nfeaturesarereplacedwithabaseline(e.g.,[UNK]),indicatingthedegreetowhichthemodelreliesonthese\nfeatures.\n• MoRF AUC (Most Relevant First AUC): This metric measures how quickly the model’s performance\ndeteriorateswhenthemostimportantfeaturesareperturbedfirst. TheAUCrepresentsthedecreaseinthe\nmodel’sconfidenceasthemostrelevanttokensareremoved,revealingtheimpactofthesekeyfeaturesonthe\nprediction.\n• DeltaAUC:ThismetricrepresentsthedifferencebetweenLeRFAUCandMoRFAUC.Itreflectsthemodel’s\nsensitivitytotheremovalofimportantfeatures(MoRF)comparedtolessimportantones(LeRF).Alargerdelta\nsuggeststhattheexplanationmethodeffectivelydistinguishesbetweenimportantandunimportantfeatures.\n4.3 Experiments\n4.3.1 TabularModality\nWeevaluated1,024samplesfromthetestsetoftheLendingClubdataset,usingafine-tunedMLPcheckpointthat\nattainedanaccuracyof0.89andaweightedaverageF1scoreof0.87. WeassessedBacktraceagainstwidelyused\nmetricsfortabulardata,specificallyLIMEandSHAP[4],andemployedMPRTforcomparison,alongwithComplexity\ntoexaminethesimplicityofthemodelexplanationsasillustratedinAppendixA.1.1.\nTable1: ExplainationPerformancemetricsforexplanationmethods-LIME,SHAPandBacktrace,includingMean\nvaluesandfeaturecontributionsacrossdifferentlayers(fc1tofc4). LowervaluesinMPRTandComplexityindicate\nbetterperformance.\nMPRT(↓) Complexity\nMethod\nMean fc1 fc2 fc3 fc4 (↓)\nLIME 0.933 0.934 0.933 0.933 0.933 2.57\nSHAP 0.684 0.718 0.65 0.699 0.669 1.234\nBacktrace 0.562 0.579 0.561 0.557 0.552 2.201\nThe proposed method, Backtrace, as demonstrated in Table 1, achieves superior performance compared to both\nLIMEandSHAP,evidencedbylowerMaximalPerturbationRobustnessTest(MPRT)valuesacrossvariouslayers.\nThis highlights its improved interpretability and robustness in explainability. However, Backtrace exhibits higher\ncomputationalcomplexity,reflectingthefine-grained,higherentropyofitsexplanations. Thistrade-offsuggeststhe\nneedtobalancethequalityofinterpretabilitywiththesimplicityofmodelexplanations.\n4.3.2 ImageModality\nWeconductedanevaluationon500samplesfromtheCIFAR-10testsetusingasupervised,fine-tunedResNet-34\nmodel,whichachievedatestaccuracyof75.85%. WecomparedBacktraceagainstseveralmethodsasillustratedin\nAppendixA.1.2,includingGrad-CAM,vanillagradient,smoothgradient,andintegratedgradient. Thecomparison\nutilizedmetricssuchasFaithfulnessCorrelation,MaxSensitivity,andPixelFlipping.\n11\nAPREPRINT-NOVEMBER20,2024\nTable2: PerformancemetricsofvariousexplanationmethodsforasubsetofCIFAR10testsetsamples. Highervalues\n(↑)ofFaithfulnessCorrelationindicatebetterperformance,whilelowervalues(↓)ofMaxSensitivityandPixelFlipping\nsuggestimprovedrobustness. (*)-Indicatesthepresenceofinfinitevaluesinsomebatches,forwhichanon-infinite\nmeanwasusedtocalculatethefinalvalue.\nExplanation Faithfulness Max Pixel\nMethod Correlation(↑) Sensitivity(↓) Flipping(↓)\nGradCAM 0.010 1070(*) 0.249\nVanillaGradient 0.011 154(*) 0.253\nSmoothGrad 0.018 158(*) 0.252\nIntegratedGradient 0.009 169(*) 0.253\nBacktrace 0.199 0.617 0.199\nTheEvaluationsconductedasshowninTable2revealsthatBacktracesignificantlyoutperformstraditionalmethods\nlikeGrad-CAM,VanillaGradient,SmoothGradient,andIntegratedGradientacrossallkeymetrics. Backtraceachieves\nasuperiorFaithfulnessCorrelationscoreof(0.199),indicatingastrongeralignmentbetweenitsexplanationsandthe\nmodel’sbehavior. Additionally,itdemonstratesrobustperformancewithmuchlowerMaxSensitivity(0.617)andPixel\nFlipping(0.199)scores,highlightingitsstabilityagainstinputperturbationsandbetterrobustnessinpreservingthe\nmodel’spredictiveintegrityunderpixelmodifications. Overall,Backtraceestablishesitselfasamorereliableandrobust\nexplainabilitytechniqueforimagemodalitytasks.\n4.3.3 TextModality\nForthetextmodality,evaluationwasperformedontheevaluationsetoftheSST-2datasetusingafine-tunedBERT\nmodel1,achievinganF1scoreof0.926. SinceexplainableAI(XAI)forBERTandothertransformer-basedmodelsis\nrelativelynew,weemployedmetricsbasedontokenperturbationforexplanationquality,specificallyLeRF,MoRF,and\nDeltaAUC,asintroducedin[37]. WeusedmethodslikeIG,SmoothGrad,AttnRoll,GradCAMandInputGradas\nillustratedinAppendixA.1.5.\nTable3: TokenPerturbationforExplanationQualitymetricsforvariousexplanationmethods. LowerMoRFAUC\nvaluesindicatebetterperformance,whilehigherLeRFAUCandDeltaAUCvaluessuggestgreaterrobustnessand\nbetterdifferentiationbetweenrelevantandirrelevantfeatures.\nMoRF LeRF Delta\nMethod\nAUC(↓) AUC(↑) AUC(↑)\nIG -6.723 46.756 53.479\nSmoothGrad 14.568 38.264 23.696\nAttnRoll 16.123 37.937 21.814\nBacktrace 15.431 30.69 15.259\nGradCAM 19.955 21.714 1.759\nRandom 25.068 25.684 0.616\nInputGrad 27.784 19.34 -8.444\nAsshowninTable3,IntegratedGradients(IG)deliveredthestrongestperformance,achievingthelowestMoRFAUC\nandthehighestLeRFandDeltaAUC,underscoringtherobustnessofitsexplanationsandprecisefeatureattribution.\nSmoothGradandAttnRollalsodemonstratedcommendableperformance. Backtraceexhibitedbalancedresultsacross\nLeRFandDeltaAUCmetrics,withaMoRFAUCof15.431,showcasingitsabilitytoprovidemeaningfulexplanations\nwhileallowingscopeforfurtherenhancement. ThesefindingshighlighttheeffectivenessofIGinexplainabilityfor\ntransformer-basedmodelswhilerecognizingBacktrace’spotentialasapromisingandcompetitiveapproach.\n4.4 Observations\nThequalityofexplanationsisinfluencedbyboththeinputdataandthemodelweights.Theimpactofmodelperformance\nissignificant;lowmodelperformancetendstoresultinunstableexplanationscharacterizedbyhighentropy,whilegood\nmodelperformanceisassociatedwithstableexplanationsthataremoresparse. Additionally,theinferencetimefora\nsampleisproportionaltoboththesizeofthemodelandthecomputationalinfrastructureused.\n1Modelcheckpointusedhttps://huggingface.co/textattack/bert-base-uncased-SST-2\n12\nAPREPRINT-NOVEMBER20,2024\n5 Discussion\nAdditionalillustrationsofBacktraceforvarioususecasesareprovidedinAppendixA.\n5.1 AdvantagesofBacktrace\n5.1.1 NetworkAnalysis\n• Existingsolutionsinvolvedistributiongraphsandheatmapsforanynetworknodebasedonnodeactivation.\n• Theseareaccurateforthatspecificnodebutdon’trepresenttheimpactofthatnodeonthefinalprediction.\n• Existing solutions are also unable to differentiate between the impact of input sources versus the internal\nnetworkbiases.\n5.1.2 FeatureImportance\n• Witheachinputsourcebeingassignedafractionoftheoverallweightage,wecannowquantifythedependence\nofthefinalpredictiononeachinputsource.\n• Wecanalsoevaluatethedependencewithintheinputsourceastheweightassignmenthappensonaperunit\nbasis.\n• IntegratedGradientsandShapleyvaluesareothermethodsavailableforcalculatingfeatureimportancefrom\nDeepLearningModels. Bothcomewithcaveatsandgiveapproximatevalues:\n– IntegratedGradientsdependsonabaselinesamplewhichneedstobeconstructedforthedatasetand\nalteredasthedatasetshifts. Thisisextremelydifficultforhigh-dimensionaldatasets.\n– ShapleyValuesarecalculatedonasamplesetselectedfromthecompletedataset. Thismakesthose\nvalueshighlydependentontheselectionofdata.\n5.1.3 Uncertainty\n• Insteadofjustrelyingonthefinalpredictionscorefordecision-making,thevalidityofthedecisioncannow\nbedeterminedbasedontheweightdistributionofanyparticularnodewithrespecttothepriordistributionof\ncorrectandincorrectpredictions.\n5.2 Applicability\nTheBacktraceframeworkisapplicableinthefollowinguse-cases:\n5.2.1 Interpretingthemodeloutcomesusingthelocalandglobalimportanceofeachfeature\nThelocalimportanceisdirectlyinferredfromtherelevanceassociatedwithinputdatalayers. Forinferringglobal\nimportance,thelocalimportanceofeachsampleisnormalizedwithrespecttothemodeloutcomeofthatsample. The\nnormalizedlocalimportancefromallsamplesisthenaveragedtoprovideglobalimportance. Theaveragingcanbe\nfurthergradedbasedonthevariousoutcomesandbinningofthemodeloutcome.\n5.2.2 Networkanalysisbasedontherelevanceattributedtoeachlayerinthenetwork\nThetwomodestogetherprovidealotofinformationforeachlayer,suchas:\n• Biastoinputratio\n• ActivationSaturation\n• Positiveandnegativerelevance(unit-wiseandcompletelayer)\nUsingthisinformation, layerscanbemodifiedtoincreaseordecreasevariabilityandreducenetworkbias. Major\nchangestothenetworkarchitectureviacompleteshutdownofnodesorpathwaysarealsopossiblebasedonthetotal\ncontributionofthatcomponent.\n5.2.3 Fairnessandbiasanalysisusingthefeature-wiseimportance\nThisisincontinuationoftheglobalimportanceoffeatures. Basedontheglobalimportanceofsensitivefeatures(e.g.\ngender, age, etc.) and their alignment with the data, it can be inferred whether the model or data have undue bias\ntowardsanyfeaturevalue.\n13\nAPREPRINT-NOVEMBER20,2024\n5.2.4 ProcessCompliancebasedontherankingoffeaturesonlocalandgloballevels\nUsingthelocalandglobalimportanceoffeaturesandrankingthemaccordingly,itcanbedeterminedwhetherthe\nmodelisconsideringthefeaturesinthesamemannerasinthebusinessprocessitisemulating. Thisalsohelpsin\nevaluatingthesolution’salignmentwithvariousbusinessandregulatoryrequirements.\n5.2.5 Validatingthemodeloutcome\nEverymodelisanalyzedbasedoncertainperformancemetricswhicharecalculatedoveracompiledvalidationdataset.\nThisdoesn’trepresentthelivedeploymentscenario.\nDuringdeployment,validationofoutcomesisextremelyimportantforcompleteautonomoussystems. Thelayer-wise\nrelevancecanbeusedforaccomplishingthis. Therelevanceforeachlayerismappedinthevectorspaceofthesame\ndimensionasthelayeroutcome,yetitislinearlyrelatedtothemodeloutcome.\nSincetheinformationchangesasitpassesthroughthenetwork,therelevancefromlowerlayers,eveninputlayers,can\nbeusedtogetdifferentoutcomes. Theseoutcomescanbeusedtovalidatethemodeloutcome. Thelayersaregenerally\nmulti-dimensional,forwhicheitherproximity-basedmethodsorwhite-boxregressionalgorithmscanbeusedtoderive\noutcomes.\n6 Conclusion\nInthispaper,weintroducedtheDLBacktrace,anewmethodthatsignificantlyimprovesmodelinterpretabilityfor\ndeeplearning. DLBacktracetracesrelevancefromoutputbacktoinput,givingclearandconsistentinsightsintowhich\nfeaturesareimportantandhowinformationflowsthroughthemodel. Unlikeexistingmethods,whichoftenrelyon\nchanginginputsorusingotheralgorithms,DLBacktraceisstableandreliable,makingitespeciallyusefulinfieldsthat\nneedhightransparency,likefinance,healthcare,andregulatorycompliance. Ourbenchmarkingresultsdemonstrate\nthat DLBacktrace performs better in terms of robustness and accuracy across various model types, proving it can\nprovidepracticalinsights. Overall,DLBacktracecontributestothegrowingfieldofexplainableAIbyenhancingmodel\ntransparencyandtrustworthiness,promotingresponsibleAIdeploymentincriticalapplications.\n7 FutureWorks\nFuture research on DLBacktrace will aim to broaden its use and improve how it scores relevance. Key areas for\ndevelopmentincludeadaptingDLBacktraceforcomplexandevolvingmodelarchitectures,likeadvancedtransformers\nandmultimodalmodelstoensureitremainseffectiveacrossdifferentAIapplications. Additionally,weaimtoreduce\ntheinferencetimemakingDLBacktracemoresuitableforreal-timeapplicationsinproductionenvironmentssuchas\nautonomoussystemsanddynamicdecision-makingscenarios.\nFuturedevelopmentwillalsoexploretheuseofDLBacktraceforspecificmodelimprovements,includingdiagnosis\nandtargetedediting. Forexample,DLBacktracecouldassistinmodelpruningbyidentifyinglesscriticalcomponents,\ntehreby optimizing model performance and efficiency. In addition, DLBacktrace’s potential for targeted model\nimprovements will be explored. It can assist in model pruning, especially for Mixtures of Experts (MoEs), by\nidentifyingunderutilizedcomponentsorredundantexpertstooptimizeperformanceandefficiency. Itcanalsohelp\nin facilitate model merging, providing insights for seamless integration of multiple models, and layer swapping,\nenablingselectivereplacementoflayerstoenhanceadaptabilityorperformance. WealsoplantoapplyDLBacktraceto\nout-of-distribution(OOD)detection,whereitcanhelpdistinguishinstancesthatfalloutsidethemodel’strainingdata,\nenhancingtherobustnessandreliabilityofAIsystems.\nFurthermore,extendingDLBacktrace’ssupportformodel-agnosticexplainabilitywillallowittobeseamlesslyapplied\nacrossvariousarchitectures,makingitaversatiletoolinexplainableAI.TheseimprovementswillmakeDLBacktrace\nmoreusefulandestablishitasanimportanttoolforunderstandingandimprovingmodelsacrossawiderangeofAI\napplicationsandtasks.\n14\nAPREPRINT-NOVEMBER20,2024\nA Illustrations\nA.1 IllustrationsforVariousTasks\nA.1.1 TabularModality: BinaryClassification\n(a)Backtrace (b)LIME (c)SHAP\nFigure2: IllustrationofExplanationsofaCorrectlyClassifiedSamplefromtheLendingClubDatasetwhereLoanwas\nFullyPaidandwaspredictedbyMLPasFullyPaid.\n(a)Backtrace (b)LIME (c)SHAP\nFigure3: IllustrationofExplanationsofaIncorrectlyClassifiedSamplefromtheLendingClubDatasetwhereLoan\nwasFullyPaidandwaspredictedbyMLPasChargedOff.\nA.1.2 ImageModality: Multi-ClassClassification\n(a)OriginalImage (b)Backtrace (c)SmoothGrad (d)VanillaGradient (e)IG (f)GradCAM\nFigure4: VisualizingResNet’sdecisionsonaHorseimageofCIFAR10Datasetusingvariousexplanationmethods.\n(a)OriginalImage (b)Backtrace (c)SmoothGrad (d)VanillaGradient (e)IG (f)GradCAM\nFigure5: VisualizingResNet’sdecisionsonaTruckimageofCIFAR10Datasetusingvariousexplanationmethods.\n15\nAPREPRINT-NOVEMBER20,2024\nA.1.3 ImageModality: ObjectSegmentation\n(a)OriginalImage (b)ModelPrediction (c)Label (d)GradCAM\n(e)BacktraceDefault (f)BacktraceContrastive(Pos) (g)BacktraceContrastive(Neg)\nFigure6: AnalysisofaU-Netsegmentationmodel’sdecision-makingonaCamVidDatasetSample. Thefigureshows\ntheoriginalImage,ModelPrediction,andLabel,alongsideExplanationsofGradCAMandBacktracevisualizationsin\nDefaultandContrastivemodes.\n(a)OriginalImage (b)Prediction (c)Label (d)GradCAM\n(e)BacktraceDefault (f)BacktraceContrastive(Pos)(g)BacktraceContrastive(Neg)\nFigure7:AnalysisofaTumourSegmentationModel’sdecision-makingonaClinicdBDatasetSample.Thefigureshows\ntheoriginalImage,ModelPrediction,andLabel,alongsideExplanationsofGradCAMandBacktracevisualizationsin\nDefaultandContrastivemodes.\n16\nAPREPRINT-NOVEMBER20,2024\nA.1.4 ImageModality: ObjectDetection\n(a)OriginalImage (b)Prediction & Label (c)GradCAM (d)Backtrace\nFigure8: Explanationsofthemodel’sdecision-makingprocessonaBirdimagefromtheCUB-200dataset,using\nGrad-CAMandBacktracetohighlightthekeyregionsinfluencingtheprediction.\n(a)OriginalImage (b)Prediction & Label (c)GradCAM (d)Backtrace\nFigure9: Explanationsofthemodel’sdecision-makingprocessonaduckimagefromtheCUB-200dataset,using\nGrad-CAMandBacktracetohighlightthekeyregionsinfluencingtheprediction.\nA.1.5 TextModality: BERTSentimentClassification\nBacktrace, SmoothGrad, Random, inputgrad, IG…\n1.000 Backtrace\nSmoothGrad\nRandom\ninputgrad\n0.500\nIG\ngradcam\nattnroll\n0.000\n-0.500\n-1.000\n\"[CLS]\", \"t \"h ee\" m, otions\", \"are\", \"raw\", \"and\", \"will\", \"strike\", \"a\", \"nerve\", \"with \"\", anyone\", \"who\", \"'\", \"s\", \"ever\", \"had\", \"family \"\" t, rauma\", \".\", \"[SEP]\"\nFigure10: ExplanationsbydifferentmethodsformodeldecisionmakingforSentimentAnalysisforasamplefrom\nSSTDataset. InputText: Theemotionsarerawandwillstrikeanervewithanyonewhoeverhadfamilytrauma.\nPrediction: 1andLabel: 1\n17\nAPREPRINT-NOVEMBER20,2024\nA.1.6 TextModality: Multi-ClassClassification\n(a)OriginalImage (b)Prediction & Label\n(c)Backtrace\nFigure11: Explanationsofthemodel’sdecision-makingprocessformulti-classtopicdetectionforincorrectclassi-\nficationonaModelusingPre-TrainedGloveWordEmbeddingand1DCNN,usingLIME,SHAPandBacktraceto\nhighlightthekeyregionsinfluencingtheprediction.\n(a)LIME (b)SHAP\n(c)Backtrace\nFigure12: Explanationsofthemodel’sdecision-makingprocessformulti-classtopicdetectionforcorrectclassification\nonaModelusingPre-TrainedGloveWordEmbeddingand1DCNN,usingLIME,SHAPandBacktracetohighlight\nthekeyregionsinfluencingtheprediction.\n18\nAPREPRINT-NOVEMBER20,2024\nA.1.7 TextModality: Llama3.2\nFigure13:\nInputPrompt: Classifythetextintoneutral,negative,orpositive. Text: Theproductmentionedfullcoverage,butmy\nclaimwasn’taccepted,andtheexperiencewasnotsomethingIwouldrecommendtoothers. Sentimentofthetextis:\nOuput: Negative. Reason: Thetextexpressesdissatisfactionwiththeproductandtheexperience,statingthatthe\nclaimwasnotacceptedandtheexperiencewasnotsomethingtheauthorwouldrecommend. Thisindicatesanegative\nsentiment. Note: Thetextdoesnotcontainanyexplicitnegative\nFigure14: InputPrompt: Provideafunctionthatcalculatesthen-thfibonaccinumber.\nOutput: Fibonaccisequenceisaseriesofnumbersinwhicheachnumberisthesumofthetwoprecedingones,usually\nstartingwith0and1.FibonacciFunction. Here’saPythonfunctionthatcalculatesthen-thFibonaccinumberusing\nmemoizationtoimproveperformance.\ndeffibonacci(n,memo=):\nCalculatethen-thFibonaccinumber.\nArgs: n (int): The position of the Fibonacci number to calculate. memo (dict): A dictionary to store previously\ncalculatedFibonaccinumbers.\nReturns: int: Then-thFibonaccinumber.\nifn<=0:\nreturn0\nelifn==1:\nreturn1\nelifnnotinmemo:\nmemo[n]=fibonacci(n-1,memo)+fibonacci(n-2,memo)\nreturnmemo[n]\nExampleusage: print(fibonacci(10))\nOutput: 55\n19\nAPREPRINT-NOVEMBER20,2024\nFigure15:\nInputPrompt: Janet’sduckslay16eggsperday. Sheeatsthreeforbreakfasteverymorningandbakesmuffinsforher\nfriendseverydaywithfour. Shesellstheremainderatthefarmers’marketdailyfor$2perfreshduckegg. Howmuch\nindollarsdoesshemakedailyatthefarmers’market?\nOutput: Step1: CalculatethenumberofeggslaidperdayJanet’sduckslay16eggsperday. Step2: Calculatethe\nnumberofeggseatenperday. Janeteats3eggsforbreakfasteverymorning.\n20\nAPREPRINT-NOVEMBER20,2024\nReferences\n[1] OpenAI. OpenAIChatGPT. https://openai.com/index/chatgpt/. [Accessed12-11-2024].\n[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,EdouardGrave,\nandGuillaumeLample. Llama: Openandefficientfoundationlanguagemodels. ArXiv,abs/2302.13971,2023.\n[3] MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin. “whyshoulditrustyou?”: Explainingthepredictions\nofanyclassifier. Proceedingsofthe22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryand\nDataMining,2016.\n[4] ScottM.LundbergandSu-InLee. Aunifiedapproachtointerpretingmodelpredictions. InNeuralInformation\nProcessingSystems,2017.\n[5] MukundSundararajan,AnkurTaly,andQiqiYan. Axiomaticattributionfordeepnetworks. InInternational\nConferenceonMachineLearning,2017.\n[6] JesseVig. Amultiscalevisualizationofattentioninthetransformermodel. InProceedingsofthe57thAnnual\nMeetingoftheAssociationforComputationalLinguistics: SystemDemonstrations,pages37–42,Florence,Italy,\nJuly2019.AssociationforComputationalLinguistics.\n[7] AndreasMadsen,HimabinduLakkaraju,SivaReddy,andSarathChandar. Interpretabilityneedsanewparadigm.\narXivpreprintarXiv:2405.05386,2024.\n[8] ChandanSingh,JeevanaPriyaInala,MichelGalley,RichCaruana,andJianfengGao. Rethinkinginterpretability\nintheeraoflargelanguagemodels. ArXiv,abs/2402.01761,2024.\n[9] SeanTull,RobinLorenz,StephenClark,IlyasKhan,andBobCoecke. Towardscompositionalinterpretabilityfor\nxai. ArXiv,abs/2406.17583,2024.\n[10] JonathanDinu, Jeffrey P.Bigham, J.Zico KolterUnaffiliated, and CarnegieMellonUniversity. Challenging\ncommoninterpretabilityassumptionsinfeatureattributionexplanations. ArXiv,abs/2012.02748,2020.\n[11] HarmanpreetKaur,EytanAdar,EricGilbert,andCliffLampe. Sensibleai: Re-imagininginterpretabilityand\nexplainabilityusingsensemakingtheory. Proceedingsofthe2022ACMConferenceonFairness,Accountability,\nandTransparency,2022.\n[12] DarioAmodei,ChristopherOlah,JacobSteinhardt,PaulFrancisChristiano,JohnSchulman,andDandelionMané.\nConcreteproblemsinaisafety. ArXiv,abs/1606.06565,2016.\n[13] JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,AndreiA.Rusu,Kieran\nMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,DemisHassabis,ClaudiaClopath,Dharshan\nKumaran,andRaiaHadsell. Overcomingcatastrophicforgettinginneuralnetworks. ProceedingsoftheNational\nAcademyofSciences,114(13):3521–3526,2017.\n[14] JanLeike,DavidKrueger,TomEveritt,MiljanMartic,VishalMaini,andShaneLegg. Scalableagentalignment\nviarewardmodeling: aresearchdirection. ArXiv,abs/1811.07871,2018.\n[15] PatrickMaximilianWeber,KimValerieCarl,andOliverHinz. Applicationsofexplainableartificialintelligence\ninfinance—asystematicreviewoffinance,informationsystems,andcomputerscienceliterature. Management\nReviewQuarterly,74:867–907,2023.\n[16] NitayCalderonandRoiReichart. Onbehalfofthestakeholders: Trendsinnlpmodelinterpretabilityintheeraof\nllms. ArXiv,abs/2407.19200,2024.\n[17] SatyapriyaKrishna,TessaHan,AlexGu,JavinPombra,ShahinJabbari,StevenWu,andHimabinduLakkaraju.\nThedisagreementprobleminexplainablemachinelearning: Apractitioner’sperspective. Trans.Mach.Learn.\nRes.,2024,2022.\n[18] LeonSixt,MaximilianGranz,andTimLandgraf. Whenexplanationslie: Whymanymodifiedbpattributionsfail.\nInInternationalConferenceonMachineLearning,2019.\n[19] GrégoireMontavon,AlexanderBinder,SebastianLapuschkin,WojciechSamek,andKlausMüller. Layer-wise\nrelevancepropagation: Anoverview. InExplainableAI,2019.\n[20] RamprasaathR.Selvaraju,AbhishekDas,RamakrishnaVedantam,MichaelCogswell,DeviParikh,andDhruv\nBatra. Grad-cam: Visualexplanationsfromdeepnetworksviagradient-basedlocalization. InternationalJournal\nofComputerVision,128:336–359,2016.\n[21] KarenSimonyan, AndreaVedaldi, andAndrewZisserman. Deepinsideconvolutionalnetworks: Visualising\nimageclassificationmodelsandsaliencymaps. CoRR,abs/1312.6034,2013.\n21\nAPREPRINT-NOVEMBER20,2024\n[22] DanielSmilkov,NikhilThorat,BeenKim,FernandaB.Viégas,andMartinWattenberg. Smoothgrad: removing\nnoisebyaddingnoise. ArXiv,abs/1706.03825,2017.\n[23] JunyiWu, BinDuan, WeitaiKang, HaoTang, andYanYan. Tokentransformationmatters: Towardsfaithful\npost-hoc explanation for vision transformer. 2024 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition(CVPR),pages10926–10935,2024.\n[24] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Annual Meeting of the\nAssociationforComputationalLinguistics,2020.\n[25] KenzaAmara,RitaSevastjanova,andMennatallahEl-Assady. Challengesandopportunitiesintextgeneration\nexplainability. InxAI,2024.\n[26] XueminYu,FahimDalvi,NadirDurrani,andHassanSajjad. Latentconcept-basedexplanationofnlpmodels.\nArXiv,abs/2404.12545,2024.\n[27] JérémieBogaertandFrançois-XavierStandaert. Aquestionontheexplainabilityoflargelanguagemodelsand\ntheword-levelunivariatefirst-orderplausibilityassumption. ArXiv,abs/2403.10275,2024.\n[28] ArouaHedhiliSbaïandIslemBouallagui. Hybridapproachtoexplainbertmodel: Sentimentanalysiscase. In\nInternationalConferenceonAgentsandArtificialIntelligence,2024.\n[29] ChrisOlah,NickCammarata,LudwigSchubert,GabrielGoh,MichaelPetrov,andShanCarter. Zoomin: An\nintroductiontocircuits. Distill,2020. https://distill.pub/2020/circuits/zoom-in.\n[30] NelsonElhage,NeelNanda,CatherineOlsson,TomHenighan,NicholasJoseph,BenMann,AmandaAskell,\nYuntaoBai,AnnaChen,TomConerly,NovaDasSarma,DawnDrain,DeepGanguli,ZacHatfield-Dodds,Danny\nHernandez,AndyJones,JacksonKernion,LianeLovitt,KamalNdousse,DarioAmodei,TomBrown,JackClark,\nJaredKaplan,SamMcCandlish,andChrisOlah. Amathematicalframeworkfortransformercircuits. Transformer\nCircuitsThread,2021. https://transformer-circuits.pub/2021/framework/index.html.\n[31] NeelNanda,LawrenceChan,TomLieberum,JessSmith,andJacobSteinhardt. Progressmeasuresforgrokking\nviamechanisticinterpretability. ArXiv,abs/2301.05217,2023.\n[32] AnnaHedström,LeanderWeber,DilyaraBareeva,FranzMotzkus,WojciechSamek,SebastianLapuschkin,and\nMarinaM.-C.Höhne.Quantus:Anexplainableaitoolkitforresponsibleevaluationofneuralnetworkexplanations.\nArXiv,abs/2202.06861,2022.\n[33] SamuelSithakoul,SaraMeftah,andClémentFeutry. Beexai: Benchmarktoevaluateexplainableai. InxAI,2024.\n[34] Phuong Quynh Le, Meike Nauta, Van Bach Nguyen, Shreyasi Pathak, Jörg Schlötterer, and Christin Seifert.\nBenchmarkingexplainableai-asurveyonavailabletoolkitsandopenchallenges.InInternationalJointConference\nonArtificialIntelligence,2023.\n[35] GillesAudemard,Jean-MarieLagniez,PierreMarquis,andNicolasSzczepanski. PyXAI:AnXAILibraryfor\nTree-BasedModels. InThe33rdInternationalJointConferenceonArtificialIntelligence,pages8601–8605,Jeju\nIsland(SouthKorea),SouthKorea,August2024.\n[36] AshishVaswani,NoamM.Shazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,\nandIlliaPolosukhin. Attentionisallyouneed. InNeuralInformationProcessingSystems,2017.\n[37] Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand,\nSebastian Lapuschkin, and Wojciech Samek. Attnlrp: Attention-aware layer-wise relevance propagation for\ntransformers. ArXiv,abs/2402.05602,2024.\n[38] UmangBhatt,AdrianWeller,andJoséM.F.Moura. Evaluatingandaggregatingfeature-basedmodelexplanations.\nInInternationalJointConferenceonArtificialIntelligence,2020.\n[39] Chih-KuanYeh,Cheng-YuHsieh,ArunSaiSuggala,DavidI.Inouye,andPradeepRavikumar. Onthe(in)fidelity\nandsensitivityforexplanations. arXiv: Learning,2019.\n22",
    "pdf_filename": "DLBacktrace_A_Model_Agnostic_Explainability_for_any_Deep_Learning_Models.pdf"
}