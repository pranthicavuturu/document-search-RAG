{
    "title": "Continual Learning with Deep Learning Methods in an Application-Oriented Context",
    "context": "",
    "body": "Continual Learning with Deep Learning Methods in an\nApplication-Oriented Context\nDissertation\nfor the attainment of the degree\nDoctor rerum naturalium (Dr. rer. nat.)\nfrom\nBenedikt Simon Pfülb, M. Sc.\nDate of submission:\nNovember 8, 2021, Fulda\nDate of disputation:\nJune 2, 2022, Darmstadt\nPublishing place and year:\nFulda, 2022\n\nAccepted as dissertation from the Promotionszentrum Angewandte Informatik\nof the Hessian Universities of Applied Sciences\nSupervisor:\nProf. Dr. Alexander Gepperth\nFulda University of Applied Sciences\nLeipziger Straße 123\n36037 Fulda\nGermany\nCo-Supervisor:\nProf. Dr. Sebastian Rieger\nFulda University of Applied Sciences\nLeipziger Straße 123\n36037 Fulda\nGermany\nFirst Reviewer:\nProf. Dr. Ute Bauer-Wersing\nFrankfurt University of Applied Sciences\nNibelungenplatz 1\n60318 Frankfurt am Main\nGermany\nSecond Reviewer:\nProf. Dr. David Filliat\nU2IS - ENSTA Paris\n828, boulevard des Maréchaux\n91762 PALAISEAU CEDEX\nFrance\nPartner:\nHochschule Fulda\nFrankfurt University of Applied Sciences\nHochschule Darmstadt\nHochschule RheinMain\n\nAbstract\nAbstract knowledge is deeply grounded in many computer-based applications. An important research\narea of Artiﬁcial Intelligence (AI) deals with the automatic derivation of knowledge from data. Machine\nlearning oﬀers the according algorithms. One area of research focuses on the development of biologically\ninspired learning algorithms. The respective machine learning methods are based on neurological\nconcepts so that they can systematically derive knowledge from data and store it. One type of machine\nlearning algorithms that can be categorized as deep learning model is referred to as Deep Neural\nNetworks (DNNs). DNNs consist of multiple artiﬁcial neurons arranged in layers that are trained by\nusing the backpropagation algorithm. These deep learning methods exhibit amazing capabilities for\ninferring and storing complex knowledge from high-dimensional data.\nHowever, DNNs are aﬀected by a problem that prevents new knowledge from being added to\nan existing base. The ability to continuously accumulate knowledge is an important factor that\ncontributed to evolution and is therefore a prerequisite for the development of strong AIs. The so-called\n“catastrophic forgetting” (CF) eﬀect causes DNNs to immediately loose already derived knowledge\nafter a few training iterations on a new data distribution. Only an energetically expensive retraining\nwith the joint data distribution of past and new data enables the abstraction of the entire new set of\nknowledge. In order to counteract the eﬀect, various techniques have been and are still being developed\nwith the goal to mitigate or even solve the CF problem. These published CF avoidance studies usually\nimply the eﬀectiveness of their approaches for various continual learning tasks.\nThis dissertation is set in the context of continual machine learning with deep learning methods. The\nﬁrst part deals with the development of an application-oriented real-world evaluation protocol which\ncan be used to investigate diﬀerent machine learning models with regard to the suppression of the\nCF eﬀect. In the second part, a comprehensive study indicates that under the application-oriented\nrequirements none of the investigated models can exhibit satisfactory continual learning results. In\nthe third part, a novel deep learning model is presented which is referred to as Deep Convolutional\nGaussian Mixture Models (DCGMMs). DCGMMs build upon the unsupervised approach of Gaussian\nMixture Models (GMMs). GMMs cannot be considered as deep learning method and they have to be\ninitialized in a data-driven manner before training. These aspects limit the use of GMMs in continual\nlearning scenarios.\nThe training procedure proposed in this work enables the training of GMMs by using Stochastic\nGradient Descent (SGD) (as applied to DNNs). The integrated annealing scheme solves the problem\nof a data-driven initialization, which has been a prerequisite for GMM training. It is experimentally\nproven that the novel training method enables equivalent results compared to conventional methods\nwithout iterating their disadvantages. Another innovation is the arrangement of GMMs in form of\nlayers, which is similar to DNNs. The transformation of GMMs into layers enables the combination\nwith existing layer types and thus the construction of deep architectures, which can derive more\ncomplex knowledge with less resources.\nIn the ﬁnal part of this work, the DCGMM model is examined with regard to its continual learning\ncapabilities. In this context, a replay approach referred to as Gaussian Mixture Replay (GMR) is\nintroduced. GMR describes the generation and replay of data samples by utilizing the DCGMM\nfunctionalities. Comparisons with existing CF avoidance models show that similar continual learning\nresults can be achieved by using GMR under application-oriented conditions. All in all, the presented\nwork implies that the identiﬁed application-oriented requirements are still an open issue with respect to\n“applied” continual learning research approaches. In addition, the novel deep learning model provides\nan interesting starting point for many other research areas.\n\n\nZusammenfassung\nAbstraktes Wissen ist in vielen computergestützten Anwendungen fest verankert.\nEin wichtiger\nForschungsbereich der Künstlichen Intelligenz (KI) beschäftigt sich mit dem automatischen Ableiten\nvon Wissen aus Daten. Maschinelle Lernverfahren bieten diesbezüglich die grundlegenden Algorithmen\nan. Ein Forschungsbereich befasst sich mit der Entwicklung von biologisch inspirierten Lernverfahren.\nDerartige maschinelle Lernverfahren basieren auf neurologischen Konzepten, um Wissen aus Daten\nsystematisch ableiten und speichern zu können. Eine Art davon, die unter die Kategorie der deep\nlearning Modelle fällt, wird als Künstliches Neuronales Netz (KNN) bezeichnet. KNNs bestehen aus\nmehreren künstlichen Neuronen, die in Schichten angeordnet sind und mit Hilfe des Backpropagation\nAlgorithmus trainiert werden. Derartige deep learning Verfahren weisen erstaunliche Fähigkeiten auf,\num komplexes Wissen aus hochdimensionalen Daten ableiten und speichern zu können.\nNichtsdestotrotz sind KNNs von einem Problem betroﬀen, welches das Hinzufügen von neuem\nWissen zu bestehendem verhindert. Die Fähigkeit, kontinuierlich Wissen anhäufen zu können, war ein\nwichtiger Faktor für die menschliche Evolution und demnach eine Voraussetzung für die Entwicklung\nstarker KIs. Der sogenannte “catastrophic forgetting” (CF) Eﬀekt führt bei KNNs dazu, dass bereits\nabgeleitetes Wissen nach wenigen Trainingsiterationen auf einer neuen Datenverteilung unmittelbar\nverloren geht. Lediglich das energetisch aufwendige erneute Training mit der vereinten Datenverteilung\nvon vergangenen und neuen Daten ermöglicht die Abstraktion des gesamten Wissens. Um dem Eﬀekt\ndes Vergessens entgegenzuwirken, wurden und werden verschiedeneartige Ansätze vorgeschlagen, die das\nkatastrophale Vergessen abmildern oder sogar lösen sollen. Veröﬀentlichte Studien derartiger Modelle\nbestärken die Wirksamkeit der Lernverfahren für unterschiedliche kontinuierliche Lernaufgaben.\nDiese Dissertation steht im Kontext des kontinuierlichen maschinellen Lernens mit deep learning\nVerfahren. Der erste Teil befasst sich mit der Entwicklung eines anwendungsorientierten Evaluation-\nsprotokolls, mit dessen Hilfe verschiedene Modelle auf die Unterdrückung des CF Eﬀekt untersucht\nwerden können. Im zweiten Teil folgt eine umfassende Untersuchung, die zeigt, dass unter den anwen-\ndungsorientierten Anforderungen keines der untersuchten Modelle zufriedenstellende kontinuierliche\nLernergebnisse vorweisen kann. Im dritten Teil wird ein neuartiges deep learning Modell vorgestellt, was\nals Deep Convolutional Gaussian Mixture Models (DCGMMs) bezeichnet wird. Grundsätzlich bauen\nDCGMMs auf dem unüberwachten Ansatz der Gaussian Mixture Models (GMMs) auf. GMMs zählen\nnicht zu den deep learning Modellen und müssen für das Training zunächst datengetrieben initialisiert\nwerden. Diese Nachteile erschweren den Einsatz von GMMs in kontinuierlichen Lernszenarien.\nDas vorgeschlagene Trainingsverfahren ermöglicht das Training von GMMs mittels Stochastic\nGradient Descent (SGD) (wie bei KNNs).\nZudem löst die annealing-Methode das Problem der\ndatengetriebenen Initialisierung, welche bisher für das Training von GMMs vorausgesetzt wird. Es wird\nexplorativ gezeigt, dass das neuartige Trainingsverfahren im Vergleich zu herkömmlichen Methoden\ngleichwertige Ergebnisse ermöglicht, ohne dessen Nachteile.\nEin weiteres Novum besteht in der\nAnordnung von GMMs in Schichten, ähnlich wie sie bei KNNs aufzuﬁnden sind. Die Transformation\nvon GMMs in Schichten ermöglicht die Kombination mit existierenden Schichtarten und somit die\nKonstruktion von tiefen Modellen, welche komplexeres Wissen mit weniger Ressourcen ableiten können.\nIm abschließenden Teil der Arbeit wird das DCGMM Modell auf die kontinuierlichen Lernfähigkeiten\nhin untersucht. Hierfür wird ein replay-Ansatz vorgeschlagen, der als Gaussian Mixture Replay (GMR)\nbezeichnet wird. GMR beschreibt das Generieren und Wiedereinspielen von Datenpunkten durch\ndie Nutzung der DCGMM Funktionalitäten. Vergleiche mit existierenden CF Unterdrückungsmod-\nellen zeigen, dass mittels GMR ähnliche kontinuierliche Lernergebnisse unter anwendungsorientierten\nBedingungen erzielt werden können.\nDie vorliegende Dissertation verweist abschließend darauf,\ndass die identiﬁzierten anwendungsorientierten Anforderungen noch ein oﬀenes Thema in Bezug auf\nForschungsansätze zum kontinuierlichen Lernen darstellen. Darüber hinaus bietet das neuartige Modell\neinen wesentlichen Ansatzpunkt für weitere Anwendungsbereiche und zukünftige Forschungsfragen.\n\n\nTable of Contents\n1\nIntroduction .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n1\n1.1\nProblem Statement and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nResearch Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.3\nInnovative Aspects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.4\nStructure of the Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2\nFoundations of Continual Learning.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n7\n2.1\nMathematical Notation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nDeep Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.2.1\nDeep Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2.2\nDeep Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.3\nDeﬁnitions of Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.3.1\nChanging Data Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.3.2\nIncremental Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2.3.3\nContinual Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.3.4\nCatastrophic Forgetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2.3.5\nContinual Learning Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3\nRelated Work\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n27\n3.1\nTraining and Evaluation Schemes for CL Scenarios . . . . . . . . . . . . . . . . . . . .\n27\n3.2\nContinual Learning Models/Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n3.3.1\nEvaluation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n3.3.2\nExisting Catastrophic Forgetting Avoidance Models\n. . . . . . . . . . . . . . .\n32\n4\nResearch Design\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n33\n4.1\nResearch Questions in Detail\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2\nResearch Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.3\nExhaustive Grid Search\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5\nExemplary Real-World Scenario for Continual Learning .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n39\n5.1\nProblem Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.1.1\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.2\nFlow Data Stream Pipeline\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n5.2.1\nFlow Data Stream Server\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n5.2.2\nFlow Data Stream Client\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n5.2.3\nData Flow of Network Flow Data . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n5.3\nTraﬃc Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n5.3.1\nLabel-based Data Distribution\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n5.3.2\nStructural Data Patterns\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n5.4\nFlow Prediction Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n5.4.1\nDeep Neural Network Architectural Experiments . . . . . . . . . . . . . . . . .\n51\n5.4.2\nConcept Drift/Shift Experiments . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.4.3\nStreaming Experiments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.4.4\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n5.5\nReal-World Continual Learning Requirements . . . . . . . . . . . . . . . . . . . . . . .\n58\n5.6\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\nBenedikt Pfülb\npage I\n\n6\nApplication-Oriented Continual Learning Protocol .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n61\n6.1\nUsed Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n6.1.1\nGround-Truth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n6.2\nSequential Learning Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n6.3\nRealistic Application-Oriented Evaluation Protocol . . . . . . . . . . . . . . . . . . . .\n66\n6.3.1\nReal-World Continual Learning Requirements . . . . . . . . . . . . . . . . . . .\n67\n6.3.2\nHyper-Parameter Selection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n6.3.3\nModel Training and Re-Training . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n6.3.4\nModel Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.4\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n6.5\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n7\nEvaluation of Continual Learning Methods .\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n77\n7.1\nExperimental Setup\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n7.1.1\nModels\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n7.1.2\nHyper-Parameter Selection\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n7.1.3\nReproduction of Previous Results by the Prescient Evaluation Protocol\n. . . .\n83\n7.1.4\nRealistic Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n7.2\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n7.3\nConclusion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n8\nNovel Deep Learning Model for Continual Learning.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n95\n8.1\nFundamentals of Gaussian Mixture Models\n. . . . . . . . . . . . . . . . . . . . . . . .\n96\n8.1.1\nThe Gaussian Distribution\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n8.1.2\nMultivariate Gaussian Distribution . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n8.1.3\nTypes of Covariance Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n8.1.4\nGaussian Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n8.2\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n103\n8.3\nSGD for GMM Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n104\n8.3.1\nGMM Constraint Enforcement\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n105\n8.3.2\nMax-Component Approximation . . . . . . . . . . . . . . . . . . . . . . . . . .\n106\n8.3.3\nAnnealing Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n106\n8.3.4\nTraining Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n108\n8.4\nDeep Convolutional Gaussian Mixture Models . . . . . . . . . . . . . . . . . . . . . . .\n109\n8.4.1\nTypes of Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n110\n8.4.2\nDCGMM Functionalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n115\n8.5\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n119\n8.5.1\nSGD Experiments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n119\n8.5.2\nDCGMM Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n122\n8.6\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n126\n8.7\nConclusion and Future Work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n129\n9\nContinual Learning with DCGMMs\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. 131\n9.1\nGaussian Mixture Replay\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n132\n9.1.1\nPseudo-Rehearsal Procedure\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n133\n9.2\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n134\n9.2.1\nExperimental Setup\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n134\n9.2.2\nTask Boundary Detection Experiments . . . . . . . . . . . . . . . . . . . . . . .\n139\n9.2.3\nContinual Learning Experiments . . . . . . . . . . . . . . . . . . . . . . . . . .\n140\n9.3\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n142\n9.4\nConclusion and Future Work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n145\npage II\nBenedikt Pfülb\n\nTable of Contents\n10\nFindings and General Discussion\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. 147\n10.1 Application-Oriented CL Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . . .\n147\n10.2 Continual Learning Investigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n149\n10.3 Novel Deep Learning Model DCGMM . . . . . . . . . . . . . . . . . . . . . . . . . . .\n150\n10.4 Gaussian Mixture Replay\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n152\n11\nConclusion and Outlook.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. 155\n11.1 Summary and Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n155\n11.2 Prospects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n156\nA\nListings.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. 159\nList of Figures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n161\nList of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n163\nList of Algorithms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n165\nList of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n167\nList of Abbreviatons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n169\nList of References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n171\nB\nStatutory Declaration\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n. 183\nBenedikt Pfülb\npage III\n\nTable of Contents\npage IV\nBenedikt Pfülb\n\n1.\nIntroduction\nChapter Contents\n1.1\nProblem Statement and Motivation\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nResearch Questions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.3\nInnovative Aspects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n1.4\nStructure of the Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\nHuman’s ability to learn has been one of the decisive factors for evolution. Men and women learned to\nspeak, use tools, invented writing systems and used punched cards to process data. In today’s society\nwhich is coined by technology, the challenge is to transfer this ability to the computer. Computers are\nused in almost all areas of our daily life: They have to make decisions in the shortest time possible\non the basis of constantly increasing amounts of data. However, several questions arise regarding the\nknowledge basis of decisions and how knowledge is obtained. In the past, teams of experts developed\nproblem-speciﬁc algorithms as a solution, whereas the algorithms were based on selected criteria.\nIncreasingly complex problems and huge amounts of data result in the economic ineﬃciency of this\napproach. This includes, for instance, biological vision and interpretation, genome sequencing and\nmore. Even though it may be easy for humans to distinguish the entities illustrated in ﬁgure 1.1,\ndescribing them in terms of an algorithm by hand can be diﬃcult.\nFigure 1.1: Example of visual classiﬁcation problems.\nToday’s machine learning (ML) methods are predestined for this task as they can automatically derive\nknowledge with the help of exemplary data. The ML approach is part of the artiﬁcial intelligence (AI)\nresearch area. ML partially replaces the expert teams by deriving abstract models from data. Thus,\nknowledge is no longer encoded by hand in the algorithms but instead via an (abstract) knowledge\nbase. It does not matter what the data looks like initially and what problem it is supposed to represent.\nHuge amounts of data need to be processed in form of numbers which can be considered too big a\nchallenge for humans.\nOne area of AI research is Artiﬁcial Neural Networks (ANNs), which has been subject to research\nsince the early 1940s. However, it was the development of faster and highly parallel hardware that\nenabled the eﬃcient use of this technology. The idea of ANN results from observations in nature.\nANNs mimic the biological nervous system of animals (including humans) and usually consist of\nBenedikt Pfülb\npage 1\n\nProblem Statement and Motivation\nIntroduction\nmany individual connected artiﬁcial neurons. These types of ML models have many advantages over\nconventional methods and are particularly powerful. They take their performance from stacking\nmultiple layers which they refer to as “deep” learning methods. Deep Neural Networks (DNNs), for\nexample, are among this method. The eﬃciency of DNNs has already been proven in a wide range of\napplications. Due to their structure, highly complex functions can be approximated. At the same time,\nthe used training procedure (Stochastic Gradient Descent (SGD)) results in its attractiveness for many\napplication scenarios.\nYet another crucial factor for evolution has been the ability to accumulate knowledge. Learning\nstep by step and thus the gradual accumulation of knowledge without forgetting prior concepts is\nfundamental. This is one of the foundations of the present work: The so called continual learning (CL)\nor life-long learning paradigm. In addition to reproducing biological mechanisms for learning, the\nchallenge for many technological applications is to adapt existing and add new knowledge. The same\napplies to ML models and in particular DNNs.\n1.1\nProblem Statement and Motivation\nA well-known and common problem with some ML models is the gradual addition of knowledge. Once a\nlearning process with a set of data is complete, it is diﬃcult for many models to add new and somehow\ndiﬀerent knowledge. A simpliﬁed example is a student who learns the letter A in the ﬁrst lesson and\nthen the number 1 in the second. In a ﬁgurative sense, the brain as one and the same model ﬁrst\nconstructs the knowledge of the letter. As a next step, the knowledge regarding the number is added.\nIt must be taken into account that 1) The knowledge is stored in one and the same medium – the\nbrain – and 2) The previous knowledge is available after having learning the second piece of knowledge.\nTransferring this learning process to ML, the following aspects can be noted: 1) The existing (not\nmore) memory must be used to store all knowledge and 2) The required computational capacity must\nnot be dependent on prior knowledge. The described scenario is known as CL or life-long learning\nscenario.\nMany deep learning models such as DNNs can address very complex problems. However, models\nbased on DNNs suﬀer from various disadvantages. They are aﬀected by the so called catastrophic\nforgetting (CF) eﬀect, which describes the loss of all existing knowledge as soon as new knowledge is\nadded (see French 1997; McCloskey and Cohen 1989). The CF eﬀect is often related to the stability-\nplasticity dilemma. The latter describes the necessity of plasticity to be able to add new knowledge.\nAt the same time, the ML model must provide stability in order to preserve existing knowledge. As\nan example, the ﬁrst sub-problem of ﬁgure 1.1 is reconsidered. Here, objects in images should be\ndiscriminated. Chihuahuas and muﬃns are depicted and it is interesting to note that they have\namazing similarities from the human perspective. It is, therefore, the ﬁrst task to train an ML model\nwith regard to the distinction of these two types of objects. Figure 1.2 shows an initially empty ML\nmodel on the left-hand side. The model is trained with labeled data, i.e., the Chihuahua class in red\nand muﬃn class in blue. The result of the training process is a trained model that should be able to\ndistinguish these two types of entities.\nFigure 1.2: Initial training of an ML model.\nIn order to evaluate the functionality of an ML model, test data is used. Test data are often taken\nfrom the original set of training data, but are separated for testing. As a result, the model can be\nveriﬁed with previously unknown but authentic data. As illustrated in ﬁgure 1.3, the object type\nassigned by the ML model matches the reference value. Thus, the model can distinguish Chihuahuas\nfrom muﬃns.\npage 2\nBenedikt Pfülb\n\nIntroduction\nProblem Statement and Motivation\nFigure 1.3: Evaluation of the initially trained ML model.\nEven though the present model works very well, two additional object types should be distinguishable.\nThis is comparable to the step by step addition of new knowledge in the human brain. For this purpose,\nnew data is collected and again, the pre-trained model is re-trained with the new data (see ﬁgure 1.4).\nImages of sheepdogs are supposed to be distinguished from the presented images of mobs.\nFigure 1.4: Adding additional knowledge to a pre-trained ML model.\nAgain, the model is evaluated with test data showing that the model has learned the diﬀerence.\nFigure 1.5 depicts the test results. It is obvious that the model works well, which is consistent with\nthe CL paradigm. Consequently, it is assumed that knowledge can be accumulated task by task within\nthe same ML model.\nFigure 1.5: Evaluation of the re-trained ML model.\nHowever, when the performance of the fully trained model is re-evaluated with regard to the ﬁrst task,\nall test samples are incorrectly classiﬁed (see ﬁgure 1.6). This is also true when using the original\ntraining data for testing. In fact, the retrospective addition of new knowledge during the second\ntraining process leads to the loss of the previously learned knowledge. For this reason, continual\nlearning processes become impossible.\nFigure 1.6: Re-evaluation of the re-trained ML model.\nBenedikt Pfülb\npage 3\n\nProblem Statement and Motivation\nIntroduction\nThis problem can be circumvented by combining the training data of both tasks in the given example\nand (re-)training the model (see ﬁgure 1.7 and ﬁgure 1.8). Interestingly, a high quality ML model can\nbe derived without adding additional resources, i.e., memory. However, the joint training approach has\nsome signiﬁcant disadvantages. First of all, the initially learned data has to be available and it needs\nto be combined with the new data. Furthermore, the time- and energy-consuming training process\nmust be carried out from scratch again. This process can take up to several weeks, depending on the\ncomplexity of the ML model, the amount and structure of the data.\nFigure 1.7: Joint training of an ML model.\nThe joint training approach becomes even more problematic when the size of the datasets grows\ninﬁnitely, which usually applies to data streams. Devices such as embedded systems with very limited\nmemory are particularly aﬀected. The same is true for very large amounts of data that would otherwise\nrequire excessive memory capacities. In many application scenarios, the joint training is impossible\nor undesirable for data protection reasons. One example of a future application is an autonomous\nrobot that is equipped with a human/pet recognizer by default. After being delivered to the customer\nthe robot should then learn to distinguish recognize family members. However, privacy issues may\nalso prohibit the joint training of an ML model. Thus, knowledge needs to be added in a consecutive\ntraining step.\nFigure 1.8: Testing of joint trained ML model.\nA large scientiﬁc community has been addressing this very problem of the CF eﬀect for several decades.\nCF is often evaluated from a theoretical perspective, resulting in the presentation of new approaches\nto circumvent the CF eﬀect. Unfortunately, “clean-room conditions” are often assumed, under which\nthe eﬀect is greatly simpliﬁed or easily bypassed. This is especially true for the evaluation of the\ntheoretically CF-immune models. The main problem that arises is that requirements from real-world\nscenarios are often neglected or ignored. Looking into future data is, for example, not possible in a\nreal-world application. Furthermore, so called oracles are sometimes used as integral part of CF-immune\nmodels. Oracles simply provide additional information that is unavailable or not existent in certain\nscenarios. Consequently, the CF eﬀect cannot be avoided without the application of an oracle.\nTo conclude, it is the fundamental motivation of the present work to investigate the CF eﬀect\nin ML models such as DNNs under real-world conditions. Avoiding the CF eﬀect can result in the\napplication of these ML models in more real-world CL scenarios. In addition to new application\nareas, training time and the associated amount of energy can also be reduced. Yet another beneﬁt\nof intentionally controlling the forgetting is related to ethical aspects of ML. This way, challenging,\noutdated or incorrect knowledge can be removed in a systematic manner.\npage 4\nBenedikt Pfülb\n\nIntroduction\nResearch Questions\n1.2\nResearch Questions\nThe following research questions result from the initial problem description and motivation stated\nin section 1.1. Some MLs models, and DNNs in particular, are subject to the CF eﬀect. DNNs and\ntheir variants are very powerful, and they can model very complex problems. They do, however, fail\nas soon as new knowledge is added after an initial training phase. The consequences of this eﬀect\nare particularly serious in real-world applications. The so called continual learning (CL) scenarios\nare among them, as new knowledge is usually added step by step. Considering this scenario and the\nrelated challenges, the following research questions can be derived.\nRQ 1: How can an application-oriented validation protocol be modeled to detect catastrophic forgetting?\nThe answer to this research question aims at providing a valid CL investigation protocol, which\nmakes the CF eﬀect detectable under real-world conditions. As a next step, diﬀerent deep learning\nmodels are examined with regard to the occurrence of the CF eﬀect by using one and the same\nprocedure. This ensures comparability between the models and allows for a ranking.\nRQ 2: To what extent can existing machine learning models avoid the catastrophic forgetting eﬀect?\nThe second research question examines the validity of existing CF avoidance models by using the\nevaluation method developed as a result of RQ 1. In order to answer RQ 2, empirical research is\nrequired to investigate the specialized CF avoidance models under the CL protocol. In this context,\nvarious learning problems will be investigated by using diﬀerent datasets. Conducting numerous\nexperiments and parameter variations is assumed to prevent model parameters from aﬀecting the\nCL performance. At the same time, all of the presented learning problems have to be “solved” in\norder to ensure that this particular procedure avoids the CF eﬀect.\nRQ 3: How can a novel deep learning model be designed so that it avoids catastrophic forgetting?\nThis question aims to extend/adapt an existing model or develop a novel ML model so that the\nCF eﬀect becomes controllable. The ﬁndings from RQ 2 can be used as a basis for the further\ndevelopment of extensions or enhancements. Alternatively, a new approach may emerge as an\nanswer to RQ 3. In this respect, existing non deep learning models are considered helpful. The\ncrucial question is whether they can be eﬀectively layered/stacked in order to develop a true deep\nlearning method with an appropriate CL performance.\nRQ 4: How does the novel model perform when compared to existing continual learning models?\nThe ﬁnal question is dedicated to the impact of the resulting/new model on the CF eﬀect. In\nthis context, the CL performance is compared with those of other models. For this purpose, an\n(re-)evaluation will be performed by using the evaluation protocol developed for RQ 1.\n1.3\nInnovative Aspects\nAn important part of this work is the development of a universal and uniﬁed protocol to detect the\nCF eﬀect. The main focus is on the uniformity and traceability of the protocol and the resulting\ncomparability of individual procedures. At the same time, potential weaknesses in existing detection\nmethods are considered for real-world applications. Evaluations of novel CF avoidance models often\npresent results indicating that the CF problem is “solved”. While this may be true for individual\ninvestigations, it does not mean that the problem is generally solved, or solved under application-\noriented conditions. This distinction of scenarios makes the comparison of models and the associated\nareas of applications very diﬃcult, if not impossible. This work aims to address these requirements\nfor real-world applications resulting in a framework to investigate further CF avoidance models. At\nthe same time, the theoretical investigation is shifted towards a more application-oriented scenario\nand the respective requirements. The goal is to derive implications and solutions of the CF problem\nin real-world scenarios. This perspective may be considered crucial for industry or other application\ncontexts in which ML methods are relevant for CL scenarios. Likewise, a set of requirements can be\nbeneﬁcial in order to align certain capabilities to the respective ML models. This overview can help\nincrease the comparability among the various CF avoidance models and, if necessary, support the\nselection of an adequate model.\nThe novel deep learning model is yet another innovation which is supposed to address the CF eﬀect.\nWith a higher or equal CL performance compared to other models, the newly developed model may be\nsuitable for other and new application areas. Moreover, this model can be used as a basis for further\nimprovements and adjustments. A novel deep learning model generally oﬀers many starting points for\nBenedikt Pfülb\npage 5\n\nStructure of the Work\nIntroduction\nnew research projects of, both, theoretical and practical nature. Even though a direct comparison of\nthe new model’s performance with DNNs may be out of proportion, the research potential is quite\nconsiderable. The possible applications in non-CL scenarios may be of particular interest.\n1.4\nStructure of the Work\nThe ﬁrst part of this dissertation is the chapter on fundamentals (chapter 2). This chapter introduces\nthe generally used notations in this work. Moreover, relevant deﬁnitions of terms are brieﬂy elaborated.\nThe fundamentals are followed by chapter 3 which addresses related work. Current research is presented\nwhich is dedicated to “catastrophic forgetting” (CF) and “continual learning” (CL). This includes, for\nexample, research with corresponding machine learning models addressing these problems. Chapter 4\nclariﬁes the general procedure used to answer the four main research questions. It also identiﬁes the\nconnections between the research questions stated in section 1.2 in detail. Accordingly, the main\nquestions are speciﬁed. In addition, the tools used for the conduction of the experiments are described.\nAs part of chapter 5, an exemplary real-world scenario is presented. This exemplary real-world scenario\nis taken from the research ﬁeld of computer communication networks. As a result, the requirements of\nCL applications in this scenario will be derived. Upon the introductory chapters, the speciﬁc research\nquestions are examined. Chapter 6 discusses the ﬁrst research question (RQ 1). The ﬁrst question\nto be answered focuses on how the CF eﬀect can be detected and conﬁrmed. In order to develop the\ninvestigation protocol, other protocols from related works are considered (chapter 3). In addition,\nthe requirements from the real world scenario (chapter 5) serve as basis. The result is a valid test\nprotocol for detecting and comparing the CF eﬀect. Chapter 7 addresses the second research question\n(RQ 2). For this purpose, a large-scale investigation is carried out based on the test protocol described\nin chapter 6. Some of the models presented in chapter 3 are examined with this evaluation protocol.\nThe aim of this study is to draw conclusions about the individual model’s performance with respect to\nthe CF eﬀect. At the same time, the study may identify promising approaches or techniques that can\nbe used for further improvements. Chapter 8 serves as an answer to the third research question (RQ 3),\nTherefore, the novel model or technique that is supposed to control the CF eﬀect is introduced in its\nbasic version. That is due to the fact that the model is initially not reviewed in the context of CL. In\nchapter 9 an extension of the newly developed model in the context of CL is presented. Moreover,\nthe novel model, among others, is re-evaluated using the investigation method from chapter 6. As the\nresults of each research step are presented and discussed chapter wise, chapter 10 serves as a ﬁnal\ndiscussion. In addition, the ﬁndings are summarized in order to answer the research questions. The\ndissertation is closed with a brief summary of contents in chapter 11. In addition, ﬁnal conclusions from\nthe entire work are presented. Last but not least, emerging aspects for future research are described.\nThese open issues can be used for the continuation of the present work.\npage 6\nBenedikt Pfülb\n\n2.\nFoundations of Continual Learning\nChapter Contents\n2.1\nMathematical Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.2.1\nDeep Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2.2\nDeep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.3\nDeﬁnitions of Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.3.1\nChanging Data Distributions\n. . . . . . . . . . . . . . . . . . . . . . . . .\n19\n2.3.2\nIncremental Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2.3.3\nContinual Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.3.4\nCatastrophic Forgetting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2.3.5\nContinual Learning Approaches . . . . . . . . . . . . . . . . . . . . . . . .\n24\nThis chapter introduces the fundamental concepts used in the present work, as several disciplines\nand research areas prefer diﬀerent notations.\nFurthermore, the basic principles of Deep Neural\nNetworks (DNNs) and deep learning are introduced.\nThe chapter concludes with the deﬁnition\nof relevant terminology. Changing data distributions, incremental learning, the continual learning\nparadigm, catastrophic forgetting (CF) eﬀect, and continual learning approaches are among these basic\nconcepts.\n2.1\nMathematical Notation\nIn order to ensure a uniform mathematical notation, the following representations are used in this\nwork. The deﬁnitions correspond to a subset from Goodfellow, Bengio, et al. (2016).\nNumber and Vectors\na\na scalar (integer or real)\na\na vector\nA\na matrix\nA\na tensor\nI\nidentity matrix\ndiag(a)\na square, diagonal matrix with diagonal entries given by a\ntr(A)\ntrace of a square matrix A\nSets\nA\na set\nR\na set of real numbers\n[a, b]\ninterval including a and b\n(a, b]\ninterval excluding a but including b\n(0, 1)\na tuple containing 0 and 1\n{0, 1}\na set containing 0 and 1\n{0, 1, . . . , n}\na set of all integers between 0 and n\nBenedikt Pfülb\npage 7\n\nDeep Learning\nFoundations of Continual Learning\nIndexing\nai\nelement i of vector a\nAij\nelement i, j of matrix A\nAi,:\nrow i of matrix A\nA:,j\ncolumn j of matrix A\nAi,j,k\nelement (i, j, k) of a 3D tensor A\nLinear Algebra Operations\nA⊤\ntranspose of matrix A\nA−1\ninverse of A\ndet(A)\ndeterminant of A\nProbability Theory\np(a)\na probability distribution over a continuous variable\na ∼P\nrandom variable a has distribution P\nVar(f(x))\nvariance of f(x) under P(x)\nCov(f(x), g(x))\ncovariance of f(x) and g(x) under P(x)\nN(x; µ, Σ)\nGaussian distribution over x with mean µ and covariance Σ\nCalculus\n∂y\n∂x\npartial derivative of y with respect to x\n∇xy\ngradients of y with respect to x\nFunctions\nf : A →B\nfunction f with domain A and range B\nlog(x)\nnatural logarithm of x\nDatasets and Distributions\npdata\nthe data generating distribution\nˆpdata\nthe empirical distribution deﬁned by the training set\npmodel\nthe distribution deﬁned by the model\nXtrain\na set of training examples\nXtest\na set of test examples\nxn\nthe n-th example (input) from a dataset\nyn\nthe target associated with xn for supervised learning\n2.2\nDeep Learning\nOur today’s world is increasingly dominated by the support of computers, algorithms and programs.\nHumanity is outsourcing tasks to machines, which are supposed to be either faster and/or better.\nThe available resources and capacities keep growing, and with them the amount of available data.\nHowever, the enormous quantity of data contains information, pattern or knowledge that cannot\nbe extracted without machines, as it would be unfeasible. Machine learning (ML) as a sub-ﬁeld of\nartiﬁcial intelligence (AI) is concerned with techniques that allow for the extraction of knowledge from\ndata. Mitchell (1997) deﬁnes ML as an automated learning process that is able to identify rules within\nthe data:\n“Machine Learning is the study of computer algorithms that improve automatically through\nexperience. Applications range from datamining programs that discover general rules in large\ndata sets, to information ﬁltering systems that automatically learn users’ interests.” Mitchell\n1997\nGenerally speaking, ML algorithms try to highlight generic statistical features of the analyzed data.\nThis process usually results in a model that can recognize the patterns and regularities of the input\ndata. The data should not be learned by memorizing them. Instead, derived rules are used to predict\nunknown samples. Currently, various approaches are used, depending on the application area.\nThe ﬁrst categorization of ML models refers to the type of knowledge representation. One of the\nclasses comprises models that represent the knowledge by means of symbols (also called symbolic or\npage 8\nBenedikt Pfülb\n\nFoundations of Continual Learning\nDeep Learning\nknowledge-based models). The derived and stored information of this type of model is comprehensible\nto humans. This includes, for example, the storage and representation of rule sets derived from the data.\nWith the help of such models, decisions can be reconstructed, whereas the algorithm derives the rules.\nAnother type of model is based on statistical probabilities. In contrast to knowledge-based models, it\nis not possible to immediately comprehend why a decision was made. The group of models based on\nstatistical probabilities includes, for example, DNNs, which can appear in numerous variations.\nFurther types of ML algorithms can be distinguished based on the data’s properties. In the\nfollowing, a selection of the most relevant ones (for this work) is brieﬂy introduced: Supervised learning,\nunsupervised learning (and reinforcement learning).\nSupervised Learning\nIn supervised learning scenarios, the training algorithm is presented with the\ndata X={(x1, y1), ..., (xN, yN)} with N denoting the number of samples. The dataset X consists of\nboth, the input signal xn and the expected output yn. The expected output is referred to as target\nvalues or labels. These can take a single value of the continuous range (yn ∈R), e.g., a real number.\nConsidering the described circumstances, a regression problem is assumed (Arbib and Bonaiuto 2016;\nFausett 1994).\nCategorical values, however, do assign one or multiple classes to a sample (classiﬁcation problem).\nAssigning values to multiple classes indicates a multi-class classiﬁcation problem. A single class\nmapping is a special case. The so-called one-hot encoding is usually used for classiﬁcation problems.\nSee equation 2.1 where C constitutes the number of available classes. For a problem with 10 classes\nto choose from, a label takes the form y ∈{0, 1}C. A sample xn of class 4 is represented by the\nvector yn =(0, 0, 0, 0, 1, 0, 0, 0, 0, 0). In addition to these types, the binary classiﬁcation is used for the\ndistinction of precisely two classes (Heaton 2015).\n1yn :=\n(\n1\nif yn ∈C\n0\nif yn /∈C\n(2.1)\nEquation 2.1: One-hot encoding for single-class classiﬁcation problems.\nUnsupervised Learning\nIn contrast to supervised learning, no labels yn are available in unsu-\npervised (or self-supervised) learning scenarios (Trappenberg 2009). Therefore, it is not the goal to\npredict a target value, but rather to learn the structure of the data. Clustering can be considered as\nan example. In order to cluster the data, the dataspace is divided into areas. Subsequently, a cluster\ncenter can be assigned (hard or gradual) to each sample. At the same time, measuring the group\nmembership of data points can be the goal. A well-known example is the k-means algorithm.\nA combination of supervised and unsupervised learning is referred to as semi-supervised learning.\nIn this scenario, many unlabeled and a few labeled samples are available.\nReinforcement Learning\nReinforcement learning is deﬁned by an agent’s actions in a given\nenvironment. Labeled samples are not required for reinforcement learning. Instead, a reward function\nassigns values to the outcomes of actions. Reinforcement learning is not further discussed in this work.\nNevertheless, it may be relevant for continual learning (CL) scenarios (Keller, Liu, et al. 2016).\n2.2.1\nDeep Neural Networks\nDeep Neural Networks (DNNs) (also known as Neural Networks (NNs) with multiple hidden layers)\nare one of the research areas of ML. This type describes a machine learning model inspired by biology.\nDNNs consist of artiﬁcial neurons that are arranged in a network. In the following, a simpliﬁed version\nof a biological neuron is presented. Artiﬁcial neurons in DNNs are derived from this biological blueprint.\nIn addition, the perceptron is introduced as a basis for DNNs.\nBenedikt Pfülb\npage 9\n\nDeep Learning\nFoundations of Continual Learning\n2.2.1.1\nBiological Neuron\nThe simpliﬁed biological neuron as depicted in ﬁgure 2.1 illustrates the dendrites that can transmit\ninput signals to the cell body (the soma). The soma is responsible for processing the input signals and\ngenerating the output signal, which is transmitted by the axon. At the end of the axon, one or several\nterminals can be used to transmit the output signal to other neurons. The nerve terminals (displayed\nas circles in ﬁgure 2.1) can be connected to dendrites and serve as connectors. The representation of\nknowledge is determined by the processing of signals and their output. Moreover, several other types\nof nerve cells with diﬀerent properties and purposes exist (Alberts, Bray, et al. 2002).\nFigure 2.1: A simpliﬁed visualization of a biological neuron.\n2.2.1.2\nArtiﬁcial Neuron\nIf the concept of the biological neuron is transferred to information technology operations, a model as\nin ﬁgure 2.2 can be derived. It is fundamentally based on the McCulloch-Pitts neuron proposed by\nMcCulloch and Pitts (1943). The input signals xi (left, i∈{1, ..., n}) are comparable to the dendrites\nof the biological template. The two functions displayed in the center (activation P and output fϕ)\nrepresent the logic contained in the nerve cell body (soma). These functions are described in greater\ndetail below. The output y of the artiﬁcial neuron is displayed on the right-hand side of ﬁgure 2.2.\ny corresponds to the axon including the nerve terminals and can also be passed on to several other\nartiﬁcial neurons. The latter reﬂects connections between nerve terminals and dendrites of the biological\nmodel.\nFigure 2.2: Illustration of the components of a single artiﬁcial neuron.\nActivation Function\nThe illustration of the components of an artiﬁcial neuron represents the\nactivation function P for pre-processing the input signals. As there is no consensus on the activation-\nand output function in the literature, the following deﬁnition is used in this work. Duch and Jankowski\npage 10\nBenedikt Pfülb\n\nFoundations of Continual Learning\nDeep Learning\n(1999) describe the activation function as follows: “The activation function determines the total signal\na neuron receives.”. Hence, the activation function aggregates all input signals and results in a single\nscalar value (Hagan, Demuth, et al. 1997; Fausett 1994).\nThe most common activation function is indicated in equation 2.2. In this example, the weighted\nsum of all input signals is calculated and a bias b is added. Equation 2.2 may be replaced with other\ntypes of activation functions (e.g., distance-based). However, equation 2.2 is frequently used due to its\nsimple calculation of the derivative. Moreover, all customizable parameters of an artiﬁcial neuron are\nincluded in this function and correspond to the weight vector w and the bias b.\nX\n(x)\nn\nX\ni=1\nxi · wi + b\n(2.2)\nEquation 2.2: Example activation function.\nOutput Function\nThe second component of the artiﬁcial neuron is the output function fϕ, also\nreferred to as transfer function (Hagan, Demuth, et al. 1997). These two terms are often mixed or\ninterchanged in the literature. In order to prevent this misunderstanding, the deﬁnition by Duch and\nJankowski (1999) is used.\nIn general, the output function tries to imitate the behavior of a biological neuron: Only if the\ninput intensity of all inputs exceeds a certain threshold value, the neuron forwards an output signal. If\nthe function fϕ corresponds to the identity function (fϕ(x)=x), the neuron is considered a linear unit.\nThe explicit addition of the output function is due to the fact that several linear functions (like the\nactivation function P) can be combined to a single linear function by transformation. It is therefore\nthe goal to prevent this combination by adding a (partially) non-linear monotone output function.\nSimilarly to the activation function, various output functions can be applied.\nOutput functions should fulﬁll a number of properties. Some of them are brieﬂy presented in the\nfollowing. First of all, the output function should be non-linear. Second, the universal approximation\ntheorem states that at least one approximation of the function is possible (see Cybenko 1989). By\nlimiting the range of values, the training process becomes more stable. This can be traced back to,\namong other things, the value ranges that the parameters can assume. Moreover, the output function\nshould be continuously diﬀerentiable. Otherwise, problems can arise when calculating the derivative.\nIn the following, two frequently used output functions are represented: The sigmoid and the rectiﬁer\noutput function.\nSigmoid Function\nThe sigmoid function is one of the commonly used output functions. By\ndeﬁnition (equation 2.3), it is a monotone non-linear function which is continuously diﬀerentiable. At\nthe same time, it can be parameterized, as illustrated in ﬁgure 2.3. By adjusting the parameter a, it is\npossible to achieve an approximately similar step function. Additionally, the range of output values\ny ∈[0, 1], which has further advantages with regard to computability. The output can be evaluated as\nthe probability of class membership in a two-class problem (Hagan, Demuth, et al. 1997).\nfϕa(x) =\n1\n1 + exp(−ax)\n(2.3)\nEquation 2.3: Sigmoid output function.\nFigure 2.3: Plot of sigmoid output function.\nBenedikt Pfülb\npage 11\n\nDeep Learning\nFoundations of Continual Learning\nRectiﬁer Function\nAnother frequently used output function is the rectiﬁer function. Neurons\nequipped with this function are also referred to as Rectiﬁer Linear Units (ReLUs). As deﬁned in\nequation 2.4 and indicated by its name, this output function is non-linear and not continuously\ndiﬀerentiable. This can cause neurons to enter an inactive state during the training process. However,\nthis can be compensated with diﬀerent variants (e.g., Leaky ReLU). At the same time, the range of\nvalues is not bounded from above, which can lead to new problems. Nevertheless, the rectiﬁer function\nis successfully applied to many problems (e.g., computer vision or speech recognition) and network\ntypes (e.g., DNNs or Convolutional Neural Networks (CNNs)). As shown by Glorot, Bordes, et al.\n(2011), better gradients result from the use of ReLU for visual problems (Heaton 2015).\nfϕ(x) = max(0, x)\n(2.4)\nEquation 2.4: Rectiﬁer output function.\nFigure 2.4: Plot of rectiﬁer output function.\n2.2.1.3\nPerceptron\nDue to the historical development, the perceptron will be introduced before DNNs at this point (Mc-\nCulloch and Pitts 1943; Graupe 2013). While the perceptron is technically classiﬁed as a subset of\nDNNs, it is also used as a synonym. In this work, a perceptron is deﬁned as a network consisting of\none or multiple artiﬁcial neurons. In fact, the term deﬁnes properties of the networks. One of them is\nthat the perceptron is always a feed-forward network. In terms of graphs, it is a ﬁnite acyclic graph\nwithout loops.\nOne of the most simple perceptrons consists of multiple neurons arranged in a single layer (single\nlayer perceptron). This special form can only solve linear problems. The function that can be used for\nits representation is in the form of y=x1w1 + x2w2 + . . . + xnwn, where xi is the input feature and wi\nis the corresponding weight of the i-th neuron. Therefore, it is often referred to as a linear layer.\nThe far more common and also more powerful variant of a perceptron is the Multi Layer Perceptron\n(MLP) (Graupe 2013). As suggested by its name, the artiﬁcial neurons are arranged in multiple layers\n(see ﬁgure 2.5), which eliminates the problem of linearity among other things. In general, an MLP\nconsists of an input layer (highlighted in blue), one or more so-called hidden layers (green) and an\noutput layer (red).\nFigure 2.5: Layer structure of an MLP.\n2.2.2\nDeep Learning\nDeep Learning is commonly used, and it is often associated with DNNs and supervised learning.\nHowever, the basic technologies, developments or algorithms used in this context have been known for\npage 12\nBenedikt Pfülb\n\nFoundations of Continual Learning\nDeep Learning\nmany years or even decades. In general, deep learning is understood as the training of DNNs (mostly\nvariants of MLPs, see section 2.2.1.3) by means of the “backpropagation” algorithm. The adjective\n“deep” refers to the number of hidden layers of the DNN (Goodfellow, Bengio, et al. 2016; Fausett\n1994).\nDeep learning and the associated models and algorithms derive a function from data.\nThis\nhighly complex function contains knowledge about the provided data, for example, in form of labels\n(supervised). The data dimensionality can result in a complexity that makes it challenging, if not\nimpossible, for humans to develop a function from scratch. Visual problems related to images or videos\ncan reveal this high-dimensionality. In this case, deep learning algorithms adjust the set of free model\nparameters. The resulting function is supposed to describe either the problem in a generic way, or a\nconsiderable solution for the problem (Heaton 2015).\n2.2.2.1\nLoss Function\nThe loss function L (also known as error-, cost-, or objective function) provides a measure of how well\nthe model describes the data. It is used in a later step to optimize the parameters of a DNN. Similarly\nto the other introduced functions, there is no suitable one for all problems. In general, a distinction\ncan be made between two types of supervised learning problems: loss functions for classiﬁcation, or\nregression problems. In the following, two frequently used loss functions are brieﬂy introduced (Heaton\n2015).\nMean Squared Error\nThe Mean Squared Error (MSE) (also known as quadratic- or L2 loss) is a\ncommonly used loss function for regression problems. It penalizes large errors by squaring them. At\nthe same time, outliers in the data lead to increased error values. This loss function can also be used\nfor classiﬁcation problems. In equation 2.5, the loss is calculated for N samples, whereby ˆyn is the\noutput of the model (Heaton 2015).\nLMSE(X) =\nPN\nn=1(yn −ˆyn)2\nN\n(2.5)\nEquation 2.5: MSE loss function.\nCross-Entropy Loss\nOne of the most commonly used loss functions for multi-class problems is\nthe Cross-Entropy (CE) (or negative log-likelihood). It can also be applied to binary classiﬁcation\nproblems, which is facilitated by an additional simpliﬁcation (omitted here). A prerequisite for their\nuse is that the labels are available in a one-hot format (see section 2.2). In equation 2.6, C represents\nthe number of classes, y is the given label vector and p of y is always 1 for the correct label, otherwise\nzero. Before the loss is determined by cross-entropy, the output of the network is usually normalized\nby the softmax function ς (see following section 2.2.2.1). Therefore, pj (output of the DNN) behaves\nlike a probability distribution (Brownlee 2019; Heaton 2015).\nLCE(X) =\nN\nX\nn=1\n−\nC\nX\nj=1\np(ynj) log(ˆynj)\n(2.6)\nEquation 2.6: CE loss function.\nSoftmax\nThe softmax function ς is a normalization function that transforms a K-dimensional vector\nz into the value range (0, 1] (Dayan, Abbott, et al. 2003). Similarly to probability distributions, the\ntransformed components sum up to 1.0 (see equation 2.7).\nBenedikt Pfülb\npage 13\n\nDeep Learning\nFoundations of Continual Learning\nς : RK →\nn\nz ∈RK|zi ≥0,\nK\nX\ni=1\nzi = 1\no\nς(z)j =\nezj\nPK\nk=1 ezk\nfor j = {1, . . . , K}\n(2.7)\nEquation 2.7: Softmax function and its properties.\n2.2.2.2\nGradient-Based Optimization\nGradient-based optimization is often used in conjunction with deep learning algorithms. The function\nthat needs to be optimized is the loss function (see section 2.2.2.1, Goodfellow, Bengio, et al. 2016;\nHeaton 2015). The objective of the optimization is to adjust the parameters x of the function f(x) so\nthat the result is smaller or larger. The function can be used for maximization as well, but it has to be\nnegated, i.e., −f(x).\nGradient Descent\nGradient descent is based on the derivative of a function. The general derivative\nof a function shows how changing the parameters aﬀects the output. This is represented as either f ′(x)\nor df\ndx. So, the assumption is that for a suﬃciently small change of ϵ, f(x + ϵ) ≈f(x) + ϵf ′(x) (see\nCauchy et al. 1847; Goodfellow, Bengio, et al. 2016; Heaton 2015).\nThe following is a simple example with the function f(x)=x2 (see ﬁgure 2.6). By applying the\nderivative rules, the gradient can be determined for any value of x for this quadratic function. The\nﬁrst derivative of f(x) is f ′(x)= df\ndx(x2)=2x (visualized in ﬁgure 2.7). Based on the sign of the ﬁrst\nderivative, a direction can now be determined at which the result y becomes smaller or larger for a\ncertain value x. In ﬁgure 2.7, a positive sign (indicated by the green arrows) is used to make the x\nsmaller, or vice versa (red arrow).\nFigure 2.6: Plot of f(x) = x2.\nFigure 2.7: Plot of f ′(x) = 2x.\nIn both of these simple illustrations (ﬁgures 2.6 and 2.7) one optimum exists (in this example x=0).\nThe best possible value for x should be located from an arbitrary starting point or initialization a. For\nthis purpose, the parameter value x is adjusted step by step so that the optimum is (nearly) reached.\nThe step size is given by the ϵ where the direction is determined by the sign of the derivative f ′(x)\n(see ﬁgure 2.7). Figure 2.8 illustrates the iteration steps resulting in an algorithm. It becomes obvious\nthat the optimum cannot be reached with this ﬁxed ϵ and that a circulation between two points may\noccur. The only way is to decrease the step size ϵ≈0, but this is impractical in terms of the number of\ngradient descent steps that need to be performed. More intelligent strategies for adjusting the step\nsize ϵ can be applied. This is realized by diﬀerent optimizers, as described in section 2.2.2.4. If a cycle\nis detected as shown in ﬁgure 2.8, the step size can be reduced (Heaton 2015).\nHowever, the method also poses challenges, e.g., with regard to more complex functions. If the derivative\nequals 0 (vanishing gradients), no optimization direction can be derived from it. This especially applies\nto saddle points. Likewise, the ﬁrst derivative cannot be used to determine whether a local or global\nminimum has been reached. Figure 2.9 shows a simple function f(x) with a poor local minimum, the\nglobal minimum and an acceptable local minimum. Usually, the functions to be optimized have many\nof these local minima and saddle points. It is often satisfactory to ﬁnd an acceptable local minimum,\nas indicated by the minimum on the right-hand side in ﬁgure 2.9 (Goodfellow, Bengio, et al. 2016;\nWani, Bhat, et al. 2020).\npage 14\nBenedikt Pfülb\n\nFoundations of Continual Learning\nDeep Learning\nFigure 2.8: Visualization of gradient descent steps.\nFigure 2.9: Exemplary function f(x) and its diﬀerent types of minima (Goodfellow, Bengio, et al. 2016).\nNonetheless, functions rarely depend on a single parameter. The same is true for the preceding example,\nwhereby the result of the function is still a scalar (f : Rn →R). Since the input vector x consists of\nseveral parameters, the derivative of the function must be determined for each component xi. This\nconcept is known as partial derivative and it is represented by\n∂\n∂xi f(x). All resulting gradients for\nf(x) are combined and denoted as ∇xf(x) (Goodfellow, Bengio, et al. 2016).\nAs a next step, a descent direction must be determined from the gradient vector. The most trivial\nprocedure is to subtract the determined gradients (∇xf(x)) from the parameters x. Many other\nmethods and optimization steps are available for the determination of the descent direction, even\nthough they are not discussed in this work. Anyway, the increment must be speciﬁed for each gradient\nstep. A scalar is used for this purpose, which is also referred to as learning rate ϵ. The value is usually\nproblem-dependent and can be determined, for example, by trial-and-error. Again, many diﬀerent\nmethods for ﬁnding and adjusting the learning rate can be used (Heaton 2015).\nFor the sake of completeness, a trivial example is provided. Let the function f(x, y)=x4+sin(y)\nbe deﬁned by the corresponding two parameters x and y. The resulting face is roughly rasterized in\nﬁgure 2.10. For the determination of the partial derivative, other parameters are assumed as constants.\nThus, they are omitted by the derivative 0. Taking into account the derivation rules, the derivation\nto\n∂\n∂xf(x, y) = 4x3 and\n∂\n∂yf(x, y) = cos(y) is determined. On this basis, the corresponding descent\ndirection can be deﬁned for both directions, x and y. A randomly chosen point a (here a=(2.2, 1.0))\nis used as a starting point for the gradient descent. Putting the point into the parameters of the\nfunction f(x=2.2, y=1.0), this yields the value ≈24.2. By means of this point, the descent direction\nfor x and y (blue respectively red line, see ﬁgure 2.10) can be determined. As for the one-dimensional\ngradient descent, a learning rate is required, which is ϵ = 0.4 (corresponds to the grid step size) in\nthe present example. If the step size is applied while taking into account the direction, a new point\na′ =(x=1.8, y=0.6) on the surface is obtained. The new function value for a′ is f(x=1.8, y=0.6)≈11.0,\nwhich corresponds to a minimization.\nStochastic Gradient Descent\nA further development of gradient descent (see section 2.2.2.2) is\nStochastic Gradient Descent (SGD). SGD adapts the gradient descent method for large amounts\nof data by determining the gradient for a single or a subset of samples. Since the gradient for a\nsingle sample is not necessarily representative, e.g., for an outlier, several samples are used. This\nmeans that the data needs to be divided into smaller chunks, so-called (mini-)batches. Due to the\nsmaller chunks, the expression “mini-batch gradient descent” is sometimes used to refer to SGD. In\nthis work, the processing of mini-batches is always assumed when referring to SGD. This is due to\nmore representative gradients and the processing speed. A step by step solution can be derived from\nBenedikt Pfülb\npage 15\n\nDeep Learning\nFoundations of Continual Learning\nFigure 2.10: Visualization of a 2D gradient descent step.\nthe determined gradient batches. This allows the method to process enormous amounts of data. The\nlow amount of required memory is especially advantageous, as it does not take up the entire storage\nfor all data samples. At the same time, this allows for the processing of potentially inﬁnite data\nstreams. Another advantage of this method is that the cost of a training step is constant, regardless of\na dataset’s size. In theory, an inﬁnite amount of data can be processed (Keller, Liu, et al. 2016; Hagan,\nDemuth, et al. 1997; Wani, Bhat, et al. 2020).\nDespite the advantages, several disadvantages are related to SGD. On the one hand, the size of the\nbatches (B) has to be speciﬁed. It is often set to a few hundred samples. The larger B is chosen, the\nmore representative gradients are derived. On the other hand, a step size ϵ must be deﬁned, which\ndetermines to what extent the gradients from a batch are weighted. Thus, the parameters of a function\nare adjusted step by step, depending on ϵ: x′ =x−ϵ∇xf(x). The selection of ϵ is problem-dependent\nand diﬃcult to generalize (Flach 2012).\n2.2.2.3\nBackpropagation\nThe backpropagation of error or short backpropagation (see Fausett 1994; Haykin 2009; Graupe 2013) is\nan algorithm for the adjustment of free parameters in an Artiﬁcial Neural Network (ANN) (or DNN).\nThis algorithm brings all the pieces of the puzzle together, which is why the term deep learning is often\nused in this context. A DNN is used as the basis for backpropagation, as presented in section 2.2.1.3.\nThe DNN consists of several artiﬁcial neurons (see section 2.2.1.2), which are arranged in one (NN) or\nmultiple layers (DNN). Each neuron receives an input signal, which is at the same time the output\nsignal of the neurons of the previous layer. Similarly, each neuron provides an activation function whose\nparameters are free but initialized with small random values. In addition, the neuron is terminated by\nan output function, which ensures the non-linearity of the model. Finally, the quality of the model has\nto be measured by a loss function (see section 2.2.2.1).\nThe backpropagation algorithm is summarized in algorithm 2.1. As a ﬁrst step, the data is forwarded\nthrough the DNN. Thus, a prediction of the dataset or a batch of data is performed. Based on the\noutcome, the current quality of the model is determined by the loss function. The speciﬁed target\nvalues of the data are used for this purpose. The result of the loss function is a single scalar which\ncan be considered as quality measure. As a next step, the “backward” part of the algorithm follows.\nHence, the free parameters of the DNN are adjusted. In this context, the state of all model parameters\nθ is considered a starting point for the optimization. Subsequently, the partial derivatives of all free\nnetwork parameters are determined. The chain rule is applied in order to help calculate the derivatives\nfor all neurons in all layers. The goal is to minimize the loss by adjusting the parameters. Thereby, the\nquality of the model is supposed to improve. The adjustment intensity of each parameter is determined\nby the gradients and the learning rate ϵ. Thus, θ∗←θ−ϵ∇θ is obtained. The described steps are\nusually repeated several times until either a ﬁxed number of training iterations or a convergence\ncriterion is fulﬁlled. Various criteria can be used related to the desired convergence, e.g., gradients,\naccuracy scores or the value of the loss function. Often, optimizers take over this process (Haykin\n2009; Wani, Bhat, et al. 2020). The concept of optimizers will be brieﬂy described in the following.\npage 16\nBenedikt Pfülb\n\nFoundations of Continual Learning\nDeep Learning\nAlgorithm 2.1: Steps of backpropagation algorithm.\nData: training data: X={(x1, y1), . . . , (xN, yN)}, model parameter: θ, learning rate ϵ\nResult: trained model\n1\nadd input data (xn) to feed forward through the network\n2\ndetermine the network output ˆyn\n3\ncalculate the loss-value L from real yn to predicted label ˆyn\n4\nadapt model parameters θ based on error value by gradient descent\n2.2.2.4\nOptimizers\nOptimizers are an enhancement of the plain SGD process. They are intended to address the challenges\nassociated with SGD. These well-known problems include, for example, locating unsatisfactory local\nminima or adjusting an inappropriate learning rate.\nA commonly used optimization technique is derived from physics – the momentum optimizer\nproposed by Qian (1999). The idea of this technique can be compared to a ball rolling down a hill.\nDepending on the gradient, the ball becomes faster and faster. In the real world, the increase of\nthe ball’s speed is limited by air resistance. In the context of optimizers, the parameters serve the\nsame purpose. This allows for faster runs in steeper regions and small step searches in ﬂat areas.\nMomentum and another optimized version of it, e.g., Nesterov accelerated gradient (NAG), are well-\nknown optimizers (Nesterov 2013). Above all, the convergence time is accelerated by these optimizers.\nIn general, the same result can be achieved by plain SGD with a constant learning rate (depending on\nthe initialization and other local minima).\nThe Adagrad optimizer (Duchi, Hazan, et al. 2010) and yet another version of it, Adadelta (Zeiler\n2012), adjust the learning rate individually for each parameter. The frequency and degree of change\nwithin each parameter is tracked. Thus, individual parameters are adjusted more strongly in case they\ndo not substantially change. This can be advantageous if the data is unevenly distributed. RMSprop\n(Hinton, Srivastava, et al. 2012a) and Adaptive Moment Estimation (Adam) (Kingma and Ba 2015)\noperate in a similar manner.\nGenerally speaking, optimizers should lead to either a faster (local) optimum or a better one. The\nadvantage is that this reduces the inﬂuence of a ﬁxed learning rate which may be unfavorable. One\nof the disadvantages is the addition of complexity. Moreover, new hyper-parameters are introduced.\nAlthough default values are available, hyper-parameters need to be selected according to the problem\n(Wani, Bhat, et al. 2020).\n2.2.2.5\nDeep Neural Networks\nThis section presents the classiﬁcation of DNNs along with various details (see Heaton 2015; Kelleher\n2019; Wani, Bhat, et al. 2020). First of all, the expression DNN is composed of “Deep” and “Neural\nNetwork”. As described in section 2.2.1, the underlying concept is constituted by ANNs. The addition\n“deep” points out that ANNs consist of multiple hidden layers (i.e., MLPs).\nDNNs are used in many application-oriented areas due to their excellent performance. These include,\nfor example, visual object-, speech-recognition, translation and many others. The underlying reason\nis the ability to approximate particularly complex functions, which includes non-linear relationships.\nMany diﬀerent variations are used for this purpose. They are often summarized under the term DNN.\nAn overview of the related advantages and disadvantages is presented below.\nDNNs are considered suﬃcient function approximators, as long as they provide enough capacity\n(neurons and layers). In fact, the underlying problem is irrelevant under the condition that the input\nformat is adapted. Two-dimensional image data can, for example, be converted into a one-dimensional\ninput vector. When compared to other models, DNNs often achieve equal or even better results. At the\nsame time, the advantages provided by the training procedure SGD remain. This is particularly true for\nthe processing of immense amounts of data, as required in the context of big data or streaming. Another\nadvantage is the high degree of parallelization as a result of the current developments in the ﬁeld of\nparallel processing. In addition to highly specialized hardware like Tensor Processing Units (TPUs),\ncommercial graphics cards with Graphic Processing Units (GPUs) can also be used to accelerate\ntraining and inference. This allows for the eﬃcient use of very large and computationally intensive\nmodels. Additionally, the simplicity of DNNs is an important advantage. By now, many diﬀerent\nBenedikt Pfülb\npage 17\n\nDeep Learning\nFoundations of Continual Learning\nML frameworks are available for diﬀerent platforms and programming languages. These frameworks\nfacilitate the implementation and reduce the respective workload. The usability of DNNs is simpliﬁed\nby the adapted abstraction of parallel programming and knowledge of special hardware. Available\nframeworks include, for example, PyTorch, Keras, TensorFlow and Caﬀe. In order to implement a\nmodel, it is often suﬃcient to deﬁne its structure at an abstract level by means of a few lines of code\n(Goodfellow, Bengio, et al. 2016; Heaton 2015; Nguyen, Dlugolinsky, et al. 2019).\nAt the same time, the use of DNNs involves many challenges, for example with regard to the\nso-called hyper-parameters. These include the network architecture among other things, and thus\nthe number of layers and their neurons. If a DNN is “too small”, the function describing the problem\nmay not be approximated. If the network consists of too many layers or neurons, it may cause the\neﬀect that the data is simply memorized. This eﬀect is known as overﬁtting. Figures 2.11 and 2.12\nillustrate an exaggerated example of an adequate function approximation (left, blue) and an overﬁtted\nfunction (right, red). The overﬁtting eﬀect becomes particularly obvious with outliers arising from\nmeasurement errors or corrupt data. This eﬀect can be avoided by various approaches, such as the\napplication of dropout (Hinton, Srivastava, et al. 2012b; Wani, Bhat, et al. 2020). Based on ﬁxed\nrandom variables, connections within the network are intentionally switched oﬀin order to achieve an\nimproved generalization.\nFigure 2.11: Example of generalization.\nFigure 2.12: Example of overﬁtting.\nAnother decisive parameter is the learning rate ϵ which is used for the training of a DNN. If the\nlearning rate is not adequately adapted to the problem, it becomes impossible to ﬁnd a reasonable\nparameter conﬁguration during the training process. The problem of hyper-parameter determination\nis often solved by an exhaustive grid-search in the parameter space. This procedure can be compared\nto a brute-force search for an adequate parameter conﬁguration. At the same time, the ﬁnal result\ndepends on the initialization of the model, respectively its weights. Various heuristics are oﬀered for the\nchoice of initialization. The proposal of Glorot and Bengio (2010) is, for example, based on a normal\ndistribution that is parameterized by the number of inputs of the previous layer (Xavier initialization).\nNevertheless, random initialization can also lead to worse model performances (Goodfellow, Bengio,\net al. 2016).\nAnother disadvantage can be constituted by the data or its properties. If the data distribution for\na multi-class problem is uneven, it can cause eﬀects that distort a realistic evaluation of the model\n(Japkowicz and Stephen 2002; Buda, Maki, et al. 2018; Kubat, Matwin, et al. 1997; Li, Hu, et al. 2017).\nAn extreme example of this eﬀect is a binary classiﬁcation problem where 9 999 examples come from\nthe ﬁrst class and only 1 originates from the second class. Without an appropriate weighting, the ML\nmodel would only learn to predict the ﬁrst class, regardless of the actual input. Thus, the error rate\nwould be very low and the single example would hardly have any inﬂuence due to the training method\n(e.g., SGD).\nOne more problem that aﬀects all ML models is whether the available data is representing the\nentire problem (Karahoca 2012). Even though a well-generalizing function is derived for the training\ndata, a ML model may fail in real-world applications. This challenge especially arises when synthetic\ndata is used instead of “natural” data. If synthetic data is used, the model merely learns how the data\npage 18\nBenedikt Pfülb\n\nFoundations of Continual Learning\nDeﬁnitions of Terms\ngenerator works, which is insuﬃcient. The same eﬀect can occur during the collection of real-world\ndata, for example, related to seasonal events. Further problems can arise when converting the data\ninto an appropriate machine-compatible format (not a DNN speciﬁc problem). This requires looking at\nhow the data can be structured into a valid format. The question here is how to represent an inﬁnite\ncontinuous number, e.g., milliseconds since 1970. Alternatively, such a number can be deconstructed\ninto its components, as otherwise the information may be lost. A further problem can arise when the\ndata originates from diﬀerent sensors and has very diﬀering value ranges. These ranges can have a\nnegative eﬀect on the training process, as convergence times may increase. If values suddenly become\nsigniﬁcantly larger than the average, the gradients abruptly increase. In this context, the used data\ntypes along with their properties can cause numerical problems (Najafabadi, Villanustre, et al. 2015).\nFurthermore, DNNs and their parameter conﬁgurations can be diﬃcult to interpret. They are often\nreferred to as “black box”. In fact, a slightly diﬀerent initialization of the model parameters hardly ever\nresults in the same parameter conﬁguration. There are approaches that allow for an interpretation\nof model parameters. But these are only applicable to data that can be interpreted by humans, e.g.,\nimages. It is crucial for certain use cases to clearly state why the model classiﬁed a certain sample.\nIn this context, the present research is connected to the issue of adversarial attacks. This research\narea is dedicated to the manipulation of input data and its eﬀects. If the model is not robust against\nadversarial attacks, the application in critical areas should be avoided. An example is a seemingly\ninsigniﬁcant sticker on a stop sign which causes the entire traﬃc sign to be recognized as a right-of-way\nsign (Eykholt, Evtimov, et al. 2017). Another example shows that adding noise can also change the\nclassiﬁcation result of images (Goodfellow, Shlens, et al. 2015).\nTo name a last challenge, DNNs are subject to the eﬀect of catastrophic forgetting (CF), as already\ndescribed in the introduction (see section 1.1). Trained DNNs can achieve excellent classiﬁcation\naccuracies. However, as soon as a new data distribution is presented and knowledge is supposed to be\nextracted, prior knowledge is lost. The knowledge vanishes so quickly (only a few gradient descent\nsteps) that the eﬀect is denoted as catastrophic. This particular problem will be addressed in detail in\nsection 2.3.4 of the present work.\n2.3\nDeﬁnitions of Terms\nIn order to assure a consistent use of terminology, relevant terms for this work are deﬁned in this\nsection. First of all, the relevance of changing data distributions is discussed. Second, a deﬁnition for\nincremental learning is provided, as it is often used diﬀerently in related work. Moreover, detailed\ndeﬁnitions of continual learning (CL) and catastrophic forgetting (CF) are presented. Last, some\ngeneral categories for the classiﬁcation of CF approaches are introduced.\n2.3.1\nChanging Data Distributions\nChanging data distributions may occur in many problems, especially in real-world scenarios. This\nmeans that the source generating the data changes over time. The detection and handling of changing\ndata distributions is in theory a diﬀerent subject area, but it can also be transferred into the context of\nCL. Thus, CL scenarios can also be described by various changes in data. In the literature, technical\nterms are often not deﬁned in the same way (e.g., Wang, Schlobach, et al. 2010; Tsymbal 2004; Lesort,\nCaccia, et al. 2021; Widmer and Kubát 2004; Žliobait˙e 2010; Wang and Abraham 2015). The following\nexpressions are often combined in order to specify the changing data distributions: real, virtual, domain,\ncriterion, population, concept, data(set), model, covariate, drift, shift, and more.\nConcept Change\nThe ﬁrst type of change distinguishes various changes within the data (concept).\nThese changes can refer to both the features and/or the labels. The problem is deﬁned as follows and\nit is visualized in ﬁgure 2.13. At time t, samples x : xt ∼ˆpt\ndata from the generating data distribution\npt\ndata are used to derive a model pt\nmodel (see ﬁgure 2.13a). If new samples xt+1 ∼ˆpt+1\ndata are drawn at\ntime t + 1 from a changed data distribution pt+1\ndata, the derived function of pt\nmodel does not ﬁt. This\ncan be due to either the features or the labels having changed. In case the features changed, this is\nreferred to as concept drift in this work, because the data points move away and keep their label (see\nﬁgure 2.13b). If the labels (in this example all of them) change, it is referred to as concept shift. In\naddition, a combination of both cases can occur.\nBenedikt Pfülb\npage 19\n\nDeﬁnitions of Terms\nFoundations of Continual Learning\n(a) Data distribution pt\ndata\n(b) Concept drift pt+1\ndata\n(c) Concept shift pt+1\ndata\nFigure 2.13: Changes in the data distribution (concept drift/shift).\nContext Change\nThe second type distinguishes changes in the context of the observed data\ndistribution ˆpdata (see ﬁgure 2.14). This type refers to diﬀerent views on the data as a whole. Assume a\nﬁxed data distribution pdata (of the entire data distribution) that is independent of t and thus constant.\nThe initial observed distribution ˆpt\ndata is represented in ﬁgure 2.14a. If a change does not lead to a new\nconcept, this is referred to as virtual concept drift, as illustrated in ﬁgure 2.14b. In case the change\ncauses a completely new concept, which is represented in ﬁgure 2.14c, it is known as real context drift.\nA context shift is represented in ﬁgure 2.14d, including the old and new observed distribution.\n(a) Constant pdata with an\nobserved ˆpt\ndata\n(b) Virtual concept drift with\nˆpt+1\ndata\n(c) Real context drift with\nˆpt+1\ndata\n(d)\nContext\nshift\nwith\nempirical ˆpt+1\ndata\nFigure 2.14: Changes in the empirical distribution (context drift/shift).\nChanges over Time\nIn the context of real-world applications it is helpful to detect statistical\nchanges within the data. However, the two variants (concept and context change) can occur in diﬀerent\ncombinations and temporal progressions (see ﬁgure 2.15). A sudden change is usually caused by\na speciﬁc event, e.g., by an occurring hardware defect (see ﬁgure 2.15a). This does not apply to\nincremental changes, as represented in ﬁgure 2.15b. Incremental changes can be detected when values\nslowly change over time, for example, related to a deteriorating machine or tool. It is similar to the\ngradual change (see ﬁgure 2.15c), but the transition is not as smooth. Gradual changes keep jumping\nback and forth until the change remains constant at some point. Figure 2.15d depicts the last type of\nchange over time – the re-occurring concept change. This type is due to a rhythm, such as the days\nof the week, the calendar, etc. A common problem that aﬀects the detection of these events is the\noccurrence of outliers. These outliers can be found in real-world applications and lead to an unwanted\ndetection of concept or context changes.\n(a) Abrupt change\n(b) Incremental change\n(c) Gradual change\n(d) Re-occurring concepts\nFigure 2.15: Types of changes over time.\n2.3.2\nIncremental Learning\nIncremental learning is often used as a synonym for continual learning (CL) (see related work in\nchapter 3). In this work, however, these terms are clearly distinguished from each other. When\nreferring to incremental learning, it is important to note the ability of an ML model to be trained in\nan online manner. Online learning hereby describes the successive provision of individual samples or\nsubsets of training data. Thus, one sample after the other is available for training and cannot/does\npage 20\nBenedikt Pfülb\n\nFoundations of Continual Learning\nDeﬁnitions of Terms\nnot have to be stored (Goodfellow, Bengio, et al. 2016; Bishop 1995).\nEven though several models and techniques do not support CL, they do allow for incremental\nlearning (as introduced in section 2.3.3). DNNs, for example, are already able to incrementally learn\non a stream of data. At the same time, they are subject to the condition of (catastrophic) forgetting.\nTherefore, the most simple form of DNNs can be referred to as incremental learners. However, they\nare not automatically continual learners.\nNonetheless, not every type of model can be trained in an incremental manner. These ML models\nrequire intensive modiﬁcation before they can process streaming data. The standard variants of\nSupport Vector Machines (SVMs) (Scholkopf and Smola 2001) are among them. Usually, adding a\nsingle sample requires a complete training of SVMs (Schlag, Schmitt, et al. 2019). Furthermore, the\ntraining of large datasets is challenging. This is due to the fact that distances between all data points\nmust be determined in order to separate the support vectors (Graf, Cosatto, et al. 2005). As a result,\nthe standard version of SVMs becomes unsuitable for incremental learning.\n2.3.3\nContinual Learning\nThe following sections describe the diﬀerence between what we as humans perceive as continual\nlearning (CL) and what we, in contrast, expect from machines. Accordingly, the CL paradigm is\ndeﬁned as a basis for this work (Chen and Liu 2016).\n2.3.3.1\nBiological Continual Learning\nHumans have the ability to continuously acquire knowledge without (or at least linear) losing previously\nlearned knowledge (Shadmehr and Mussa-Ivaldi 2012). The eﬀort to learn continuously is represented\nby the survival of the species until today. Adaptability, including the accumulation of experience and\nknowledge, is the key to success. Even in today’s post-knowledge society, CL is applied every single day.\nIn a student’s school career, for example, knowledge is accumulated (see ﬁgure 2.16). However, this\nlearning process is not a sequence of facts without context. Instead, knowledge, skills and dispositions\nare interrelated in the context of a task (Raj, Sabin, et al. 2021) and thus enable the construction of\nnew knowledge or ideas. At the same time, only one organ is available for the processing of data and\nits capacity is limited. In summary, the human being is able to learn continuously by creating and\nseparating connections in the brain.\nFigure 2.16: The life-long learning paradigm.\nInterestingly, the stability-plasticity dilemma is also recognizable in humans. At a young age, the\nhuman brain is very receptive to new knowledge, which is due to its plasticity. At an older age, learning\nnew things is usually much more diﬃcult, indicating stability.\nBenedikt Pfülb\npage 21\n\nDeﬁnitions of Terms\nFoundations of Continual Learning\n2.3.3.2\nContinual Machine Learning\nIn the context of continual learning, ML is an attempt to imitate the biologically inspired abilities.\nCL is also referred to as life-long learning or sequential learning (or even incremental learning). Even\nthough the CL is biologically inspired, current models are not yet able to accumulate and combine\nknowledge. Instead, the goal is to ﬁnd a model and its parameters, which represents a generic function\nof the CL problem. Some work related to continual or lifelong learning will be examined in more detail\nbelow. All of the presented works represent a similar deﬁnition of the CL paradigm (e.g, Mermillod,\nBugaiska, et al. 2013).\nThrun (1996a), for instance, describes the learning process inspired by the human being. Thrun\ndeﬁnes the ability to learn as a “stream”. Moreover, the learning process is lifelong. Thrun’s work\n(Thrun 1996b) describes a “simple version” of lifelong learning problems as “concept learning tasks”.\nTasks are functions (f : fn →{0, 1}) which must be learned sequentially based on the presented data.\nMoreover, Thrun views functions as a binary distinction. Chen and Liu (2016) deﬁne the “lifelong\nsupervised learning” process based on successive tasks T1, T2, ..., TN. Knowledge must be stored in a\n“knowledge base” and it needs to be updated based on new data. Some machine learning methods only\nhave this ability under certain conditions, or not at all.\nThe work of Parisi, Kemker, et al. (2019) comprehensively presents challenges and requirements of\nthe CL paradigm. Lifelong learning is viewed from diﬀerent perspectives, such as the human brain as\nbio-inspiration. Despite the many advances in CL, current models are still far away from the capabilities\nof biological systems. In this context, the CL problem can also be represented as a concept or context\nchange (see section 2.3.1). In terms of the student’s school career and learning process, a virtual concept\ndrift takes place. The generated data distribution usually remains constant, whereas the view of the\ndata ˆpdata changes constantly (comparable to an abrupt change). Nevertheless, knowledge accumulates\nand assembles into a whole, even if individual parts are independent. Delange, Aljundi, et al. (2021)\npresent a taxonomy in the context of the “task incremental learning setting”. Moreover, Delange,\nAljundi, et al. (2021) deﬁne requirements for the CL scenario that occur in real-world applications.\nTheir approach also considers the CL paradigm from the CF eﬀect’s perspective, which is particularly\neﬃcient in sequentially trained tasks. The deﬁnition is based on a potential stream of data that\nrepeatedly originates from diﬀerent domains. Similarly, Hayes, Kemker, et al. (2018) provide a deﬁnition\nof continual learning (CL) as “streaming learning”. A dataset D is partitioned into individual tasks T,\nassuming that the individual tasks T are independent and identically distributed. The classes of the\nindividual data points are represented as discrete labels. In addition, various constraints are deﬁned,\nwhich result from real-world scenarios, such as the use of robots.\nDeﬁnition of a CL task:\nThe following deﬁnition of CL is used in this work. A CL task is deﬁned\nby a sequence of sub-tasks Ti. Each sub-task Ti deﬁnes a subset of samples from a dataset X, where\nTi ⊆X. The index i deﬁnes the order of the sequence i ∈(1, ..., I), where I is the last task. Each\ntask Ti consists of N individual samples, which are deﬁned by the input vector x and by its class\nlabel y (see section 2.2). Likewise, the data within a task is divided into training T train\ni\nand test T test\ni\ndata. Each sub-task is sequentially presented to a model m. It is assumed that each task contains only\nindividual classes (non-overlapping classes).\nDeﬁnition of the CL goal:\nA model m should be able to reproduce all derived knowledge from\nall sub-tasks after the completion of the last sub-task TI. For the supervised learning context the\nuniﬁed test dataset T test\nAll =∪iT test\ni\nis the benchmark. Even though this benchmark does not have to\nbe applied to all scenarios, it is an initial quality measure that should be achieved.\n2.3.4\nCatastrophic Forgetting\nCatastrophic forgetting (CF) or catastrophic interference is an eﬀect that occurs in DNNs (Chen and\nLiu 2016). It has been the focus of research since the early 1990s. CF describes the eﬀect of a model\nlosing all previous knowledge, as soon as new knowledge is added. Although ML models are biologically\ninspired, they are aﬀected by the problem. This seems true for every model based on artiﬁcial neurons.\nThe work of McCloskey and Cohen (1989) identiﬁed the problem by conducting the following\nexperiment. Their training scheme was inspired by elementary school students who should learn the\naddition of two numbers. As a ﬁrst task, the model should learn how to add 1 to another single-digit\npage 22\nBenedikt Pfülb\n\nFoundations of Continual Learning\nDeﬁnitions of Terms\nnumber (i.e., 1+1, 1+2, ..., 1+9 and commutative variants). The second task was similar, but with\nthe number 2 (i.e., 2+1, 2+2, ..., 2+9 and commutative variants). Both tasks were trained until the\nmodel could solve them. An interesting eﬀect appeared during testing on the test data of the ﬁrst task\nand having trained the second task: In each case, the model was oﬀby 1 whenever calculations from\nthe ﬁrst task were performed, e.g., 5+1=7 or 6+1=8. Although both tasks included the operations\n1+2 and 2+1, a high error rate was observed. To be precise, the errors occurred at the beginning of\nthe second training sequence.\nRatcliﬀ(1990) started to address the CF problem and investigated further models. In addition to\nan extended test scheme, ﬁrst solution approaches were presented. In this context, a simple buﬀer is\nused as a solution to enable “rehearsal” or “replay”. For this purpose, samples from the ﬁrst task are\nretained and included in the next training session.\nA visualization of the CF eﬀect is depicted in ﬁgure 2.17. The ﬁgures display the training and\ntesting of a ML model on two sequential tasks. During the ﬁrst training phase, knowledge is extracted\nfrom the data of the ﬁrst task (task 1, white background). In the second training phase, the knowledge\nfrom the second task has to be learned (task 2, gray background). Both tasks are mutually disjunctive,\nwhich means they contain diﬀerent knowledge. The blue lines represent the test performance on the\ntest data of task 1. The test performance of task 2 is presented in green and starts with the training of\nthe second task. Performance is deﬁned by the degree to which individual test samples are correctly\nassigned to their origin/class. As depicted in ﬁgure 2.17a, the performance on task 1 catastrophically\ndrops as soon as learning on the second task begins.\nThe trend displayed in ﬁgure 2.17b is diﬀerent and shows an ML model that is slowly losing\nknowledge. After extracting the knowledge from task 1 (as before), samples from task 2 are presented\nin the second training phase. But this time, the prior knowledge from the previous task disappears in\na linear manner. In theory, the linear forgetting of knowledge from previous tasks is considered an\nattractive solution. Obviously, the complete elimination of the forgetting eﬀects is desired.\nIn general, the ML model must be able to derive the joint knowledge from task 1 and task 2. The\njoint training trend is depicted in ﬁgure 2.17c. The red line represents the performance of the ML\nmodel while it is trained on a combination of task 1 and 2. The so-called “baseline” is thus the reference\nvalue and represents the upper limit that should be achieved even in CL scenarios.\n(a) Catastrophic forgetting\n(b) Linear forgetting\n(c) Baseline\nFigure 2.17: Visualization of the CF eﬀect, a linear forgetting and the baseline.\n2.3.4.1\nReason for Catastrophic Forgetting\nThe main reason for the occurrence of the CF eﬀect in connectionist models is the way the information\nis stored – namely distributed (French 1997). However, this distribution of knowledge is the reason\nwhy these models have their beneﬁcial properties. Although DNNs are biologically inspired (see\nsection 2.2.1), their simple abstraction does not seem to result in the same properties as the more\ncomplex original. This raises the question whether the brain itself may be subject to this eﬀect. Studies\nwith animals show that although a forgetting eﬀect occurs, it is not considered catastrophic and rather\nlinear (McClelland, McNaughton, et al. 1995). Nevertheless, this eﬀect seems to be less problematic\nin highly developed species. This is probably due to the diﬀerent areas of the brain such as the\nhippocampus, which is particularly responsible for memory consolidation (McClelland, McNaughton,\net al. 1995). Consequently, some ML methods are trying to address the CF problem based on a similar\nmechanism.\nThe brain seems to be able to accumulate knowledge over time through multiple mechanisms. One\nof the basic conditions is that there is a limited capacity available to integrate knowledge. Due to this\nintegration, existing knowledge is not immediately lost. Moreover, the brain is particularly adaptive in\nBenedikt Pfülb\npage 23\n\nDeﬁnitions of Terms\nFoundations of Continual Learning\ncertain phases of human development, although this characteristic slowly decreases over time.\n2.3.5\nContinual Learning Approaches\nIn this section, several known categories are presented that intend to mitigate or remedy the CF eﬀect.\nAccordingly, all ML models can be grouped into a category. It is, however, possible that a model can\nbe classiﬁed into more than one category. In addition, the challenges of the various CL approaches are\nindicated in this section. Likewise, the basic concepts of the methods are brieﬂy introduced. Further\nmethods can be used for CL, the presented methods thus do not represent a complete overview, but a\nselection. Transfer learning is a corresponding example, which transfers knowledge from one model\ninto another.\nRegularization Methods\nThe goal of regularization methods is depicted in ﬁgure 2.18 (Solinas,\nRousset, et al. 2021; Delange, Aljundi, et al. 2021; Chen and Liu 2016). The ΘT1 (yellow area) represents\nall parameter conﬁgurations of the model which are acceptable for possible solutions of the ﬁrst task\nT1. ΘT2 (blue area) denotes acceptable solutions for the problem in T2. The points θ∗\nT1 and θ∗\nT2\nillustrate parameter conﬁgurations that provide a solution for the corresponding task. It is the goal\nof regularization to ﬁnd a parameter conﬁguration that is in the green range, e.g., inﬂuenced by the\nloss function. Therefore, it is not the solution θ∗\nT2 (red dashed line) that should be obtained, but a\nparameter conﬁguration which is valid as a solution for T1 ∪T2 (black dashed line). The existence of\nthe joint parameter space can be shown by training the model with a joint dataset from T1 and T2.\nFigure 2.18: Visualization of the objective of regularization methods.\nThe regularization approach tries to keep certain parameters from changing by determining their\n“importance” (Farquhar and Gal 2018). As there are usually many parameters available in an DNN, the\ndetermination of their dependencies is usually very complex. Depending on the size, the determination\nis too computationally complex so that it becomes unfeasible. Therefore, the parameters are often\nindividually considered. Other challenges can arise, such as the determination of the importance\nof individual parameters. At the same time, the inﬂuence on the re-training process needs to be\ndetermined.\nReplay Approaches\nReplay approaches avoid the CF eﬀect by simulating a joint model training.\nFor the training of the second task, samples of the previous task are added in order to rule out the CF\neﬀect (Solinas, Rousset, et al. 2021; Delange, Aljundi, et al. 2021; Chen and Liu 2016). The general\nidea is equivalent to a joint training. The diﬀerence to an actual joint training is the selection and\nquantity of samples used for the replay. In some application areas, however, the question of data\nprivacy issues arises. Three basic replay methods are distinguished: rehearsal, pseudo-rehearsal and\nconstrained optimization.\nThe rehearsal method uses samples that are stored in a separate memory. The basic rehearsal\nconcept is illustrated in ﬁgure 2.19. The challenge is to use as little memory as possible, preferably a\nconstant amount. Models determine samples that are particularly representative and thus have a high\nadded value for the replay. However, this approach has limitations and challenges. Open questions are\nrelated to the representative nature of samples, the scope of available memory and how to keep enough\nsamples for all future tasks. If there are only a few samples available, it remains unclear how to avoid\noverﬁtting (Solinas, Rousset, et al. 2021; Delange, Aljundi, et al. 2021; Chen and Liu 2016).\npage 24\nBenedikt Pfülb\n\nFoundations of Continual Learning\nDeﬁnitions of Terms\nFigure 2.19: Outline of rehearsal methods (replay).\nPseudo-rehearsal follows a similar procedure, except that the samples do not have to be explicitly\nselected and stored. Instead, they are generated (see ﬁgure 2.20). As a consequence, the problem\nof the selection and the storage can be ignored. Nevertheless, the question how to generate data\narises. In this context, it is important how the generating model derives the distribution from the data.\nMoreover, the approaches need to describe in which ratio old data is generated and how the generated\nsamples are labeled. Another problem for pseudo-rehearsal methods is the quality and variability of\nthe data (Chen and Liu 2016).\nFigure 2.20: Delineation of pseudo-rehearsal methods (replay).\nAnother methodology of this category is the constrained optimization. Re-training is not directly\ninﬂuenced by replaying (generated) samples. Instead, their inﬂuence on the training process and thus\non the model and its parameters is analyzed and stored. For re-training, the inﬂuencing factors are\ncombined with the factors inﬂuencing the current training process. In order to provide an example,\nthe following process is assumed: The gradients generated during the ﬁrst training process are used to\nmodify the gradients generated in the re-training process.\nParameter Isolation\nApproaches using parameter isolation exploit the structures or architecture\nof the underlying model (Solinas, Rousset, et al. 2021; Parisi, Kemker, et al. 2019; Delange, Aljundi,\net al. 2021). As part of this strategy, certain components within the network are reserved for sub-tasks.\nBasically, a distinction is made between two types of architecture: dynamic and static.\nDynamic architectures are able to store the new knowledge in additional components of the network.\nThus, the knowledge for a corresponding sub-task is stored separately. Challenges that can arise\ninclude choosing a component for a task or sample. In terms of storage technology, the scalability of\nsuch a system remains unclear. Assuming that a CL problem consists of an inﬁnite number of tasks,\nBenedikt Pfülb\npage 25\n\nDeﬁnitions of Terms\nFoundations of Continual Learning\nmemory limits are restrictive (Solinas, Rousset, et al. 2021; Parisi, Kemker, et al. 2019).\nThis is in contrast to the static models that do not add new parameters, but “freeze” existing\nones. For re-training, parameters from the previous tasks are not changed. The decisive factor is\nwhich component of the network is responsible for which task. Furthermore, questions related to each\nparameter’s role for a particular task remain open (Solinas, Rousset, et al. 2021; Parisi, Kemker, et al.\n2019; Delange, Aljundi, et al. 2021).\npage 26\nBenedikt Pfülb\n\n3.\nRelated Work\nChapter Contents\n3.1\nTraining and Evaluation Schemes for CL Scenarios . . . . . . . . . . . . . . . . . .\n27\n3.2\nContinual Learning Models/Methods\n. . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.3\nDiscussion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n3.3.1\nEvaluation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n3.3.2\nExisting Catastrophic Forgetting Avoidance Models . . . . . . . . . . . . .\n32\nThis chapter presents the current state of research with regard to deep learning in context of continual\nlearning (CL). First of all, catastrophic forgetting (CF) evaluation schemes are introduced. However,\nrelated work that merely constructs a new machine learning (ML) model is not included. Instead,\nthe focus is only on research that presents an evaluation strategy for the CF eﬀect and, if necessary,\nconducts the respective investigations. Related work is usually presented by providing the deﬁnition\nof its evaluation methodology, the metrics, datasets and the description of the investigated scenarios.\nMoreover, related work with regard to CF avoidance models are presented. The number of available\nML models and algorithms for this purpose is extensive, and new proposals are constantly being added.\nThe respective papers usually include an evaluation scheme. In order to summarize the related works,\na brief discussion is provided.\n3.1\nTraining and Evaluation Schemes for CL Scenarios\nIn this section, the training and evaluation schemes used for evaluations in the CL context are presented.\nThe challenge is that related works usually introduce an evaluation scheme along with a new ML\napproach or model. As the evaluation results tend to show the superiority of the newly developed\nmodel, these works are excluded from a detailed review. Moreover, this decision is due to the lack of\ndetailed speciﬁcations of the evaluation scheme.\nChen and Liu (2016) describe the used evaluation methodology as a four-step procedure: 1)\nsequential training on the data of old tasks; 2) training on the data of the current/new task; 3) creating\nbenchmarks by running baseline experiments on the merged data or comparing with other algorithms;\nand 4) the ﬁnal analysis and interpretation of the results. Besides, three “kinds of evaluations” are\ndescribed, ﬁrst of all, the number of tasks and the information content for the current task. For the\nevaluation, the already determined previous knowledge in relation to the eﬀect on the new task is\ntaken into account. Second, the eﬀect of the task’s order is addressed, which can lead to diﬀerent\nresults. The third kind of evaluation is “progressive experiments”. This type determines the inﬂuence\nof the number of previous tasks, assuming that more knowledge could already be derived.\nHsu, Liu, et al. (2018) criticize the methods used for evaluation in the context of CL. Their\nwork introduces requirements that apply to real-world applications: Determining constant memory\nusage, prohibiting the use of oracles at test time, and not specifying task limits. Considering these\nrequirements, 10 of the 14 CF avoidance models are excluded from the investigation of Hsu, Liu, et al.\n(2018). In addition, various types of tasks are described. Task Incremental Learning (Task-IL) and\nClass Incremental Learning (Class-IL) show how datasets are divided into tasks. Overlaps with regard\nto the contained classes are not used. The diﬀerence is that Task-IL speciﬁes from which task the\nsample originates. This is diﬀerent compared to Class-IL, as this particular information is missing.\nBenedikt Pfülb\npage 27\n\nContinual Learning Models/Methods\nRelated Work\nThe latter task type is therefore represented as “hardest scenario”. Furthermore, Domain Incremental\nLearning (Domain-IL) is described, which applies a “task-dependent transformation” to the input. In\nthis context, a permutation and rotation is applied once. The same types of tasks are described in other\nworks, such as Ven and Tolias (2019). Fixed models with 2 hidden layers and 400 neurons (including\nRectiﬁer Linear Unit (ReLU)) are used for the evaluation (Hsu, Liu, et al. 2018). The selection of the\nhyper-parameters is realized by a grid-search where the highest accuracy measured at the end is used\nas criterion.\nFarquhar and Gal (2018) denounce that conventional CL evaluation schemes do not reﬂect the\nfundamental challenges. Accordingly, a separate evaluation procedure is presented based on the\nexample of the Mars rover. The following constraints are assumed. Cross-task resemblances describes\nthe distinctness/diﬃculty of the tasks. Shared output head must be speciﬁed, which describes whether\na separate output vector is used depending on the task (also known as multi-head). No test-time\nassumed task labels prohibits the use of oracles, which denotes the speciﬁcation of the task from which\nthe sample is taken. No unconstrained retraining on old tasks restricts random access to data from all\ntasks. More than two tasks are proposed to be investigated. Even though the basic feasibility can be\nshown with two tasks, this does not have to apply to several tasks.\nIn addition to these constraints, other requirements are imposed on the models (Farquhar and Gal\n2018). Regarding the tasks and their limits, three types are distinguished: Unclear task demarcation –\nit is often assumed to know these task boundaries; continuous tasks – assumption is that there are\nhard task boundaries that are not subject to natural variability; overlapping tasks – denotes that single\nor disjoint tasks are examined. In addition, Farquhar and Gal (2018) outline time/compute/memory\nconstraints. In this context, ﬁxed memory is assumed to be more useful than large computational\noverheads. The last requirement is strict privacy guarantees with special emphasis on intermediate\nindividual tasks.\nSeveral evaluations are presented as a critical analysis (Farquhar and Gal 2018): Pixel permutation\ndescribes the shuﬄing of features based on a random but ﬁxed pattern. In the case of an image, this\nmeans that pixels are exchanged with others. After performing this process, images usually become\nunrecognizable by means of visual inspection. In contrast to that, split classiﬁcation tasks split the\ndataset into several partial datasets. These tasks are associated with the requirements and constraints,\nwhere the impact is enormous.\nLesort, Caselles-Dupré, et al. (2019) describe a number of diﬀerent CL scenarios. Unlike other\nworks, the focus is not on disjoint class scenarios, but drift scenarios. Accordingly, diﬀerent types of\ndrift are described at ﬁrst: Real concept drift, virtual drift, virtual concept drift and domain drift.\nThe evaluation protocol describes two distinct objectives. The maximum performance at the end of\nthe training (“ﬁnal performance”) process and the performance measured over the entire course of\nthe training. In general, both of them are associated with the current performance. In contrast to\nthat, the “cumulative performance” describes the performance during the training process and is thus\nrelated to the currently possible level of knowledge. Furthermore, various stationarities that occur\nin CL scenarios are described, e.g., concept stationarity, target/reward stationarity, ﬁnite world, etc.\nMoreover, drift intensity and patterns are discussed. Besides, incremental learning (virtual concept\ndrift) is distinguished from lifelong learning (domain drifts) and real-concept drift scenario (real concept\ndrifts).\nKemker, McClure, et al. (2017) introduces three metrics that allow a comparison between datasets.\nAccordingly, the performance of the model after the ﬁrst session Ωbase has to be determined. Ωnew\nindicates how models directly react to new tasks. In addition, Ωall indicates to what extent a model\ncan represent both, old and new knowledge. Furthermore, various experiments are described, such\nas the permuted, incremental class learning and multi-modal experiments. The latter describes tasks\nfrom diﬀerent datasets with diverse dimensions. A hyper-parameter investigation is also carried out\nfor the studied models. One of the conclusions is as follows: “We demonstrate that despite popular\nclaims, catastrophic forgetting is not solved.” (Kemker, McClure, et al. 2017).\n3.2\nContinual Learning Models/Methods\nThis section presents related models claiming to mitigate or avoid the CF eﬀect. All approaches use\ndiﬀerent techniques to mitigate the eﬀect and should therefore be suitable for the use in CL scenarios.\nThe following summary represents only an subset of all existing models addressing the CF eﬀect. This\npage 28\nBenedikt Pfülb\n\nRelated Work\nContinual Learning Models/Methods\nis especially true, as novel and better methods are being presented on an almost daily basis. Thus,\nthe importance of the CL paradigm is highlighted. In this work, the categorization of the approaches\naccording to the method used to address the CF problem is attempted (see section 2.3.5).\nParameter Isolation\nThe basic idea of the subsequent approaches is to modify the network\narchitecture so that the CF eﬀect does not occur. In any approach the Deep Neural Network (DNN) is\nextended or only certain parts of the DNN are used for a given task.\nSrivastava, Masci, et al. (2013) present a model referred to as Local Winner Takes All (LWTA),\nwhich addresses the CF eﬀect. The idea is based on the fact that every neuron consists of two or\nmore units, so-called blocks. Depending on the input, one of the corresponding neurons “wins”. This\nprocedure is realized as an activation function. Thus, sub-networks are created which can be selected\nand are responsible for diﬀerent tasks. This model is described in section 7.1.1 in more detail.\nGoodfellow, Mirza, et al. (2013) illustrate in their empirical study that the application of dropout\nis “beneﬁcial” in relation to the CF eﬀect. Depending on a factor, dropout leads to the random\nnon-forwarding (or setting to 0) of neurons activities. Smaller DNNs, which can be trained by using\nthe dropout mechanism, are more resistant to CF than large networks. Furthermore, it is observed\nthat diﬀerent activation functions, such as sigmoid, ReLU, LWTA and maxout, have fewer eﬀects with\nregard to CF. This proposal will be described in section 7.1.1 in more detail.\nRusu, Rabinowitz, et al. (2016) propose Progressive Neural Networks (PNN), a model that coun-\nteracts the CF eﬀect at the architectural level. The model extracts useful features for the diﬀerent\ntasks from several models. Thus, a new DNN (or column) is created for each task. The transfer is\nthen achieved via connections to other networks. Each column is connected to its predecessor columns\n(lateral connections), where parameters are “frozen” after training.\nAljundi, Chakravarty, et al. (2017) propose Expert Gate, a life-long learning system. The procedure\nis based on a network of experts. Thus, a corresponding model is derived for each task. For the selection\nof the respective expert, a “gating” function is used, which decides on the basis of the characteristics of\neach task. The encoder part of an autoencoder is used for this purpose. The autoencoder takes over\nthe task of the oracle, which selects the expert.\nPathNet is suggested by Fernando, Banarse, et al. 2017. The idea is to reuse parts of the network for\nspeciﬁc tasks. The selection of parameters is based on a genetic algorithm characterized by replication\nand mutation. Path determination is initially working with a random selection, where the “ﬁtness”\n(negative classiﬁcation error) is calculated. The same is true for other random paths, whenever the\nbetter/best path wins. Finally, the best path is mutated by a random selection of elements.\nThe model proposed by Mallya and Lazebnik (2018) is referred as PackNet and follows the approach\nof iterative pruning and re-training, Thus, knowledge from several tasks is to be “packed” sequentially\ninto a DNN. A ﬁxed percentage of neurons is selected for pruning. The selection is based on the\nmethod suggested by Han, Mao, et al. (2017), which deﬁnes neurons with small weights as “unimportant\nconnections”. After releasing the unimportant neurons, the remaining ones are “frozen”.\nSerra, Suris, et al. (2018) describe the Hard Attention to the Task (HAT) procedure in order\nto overcome the CF problem. The mechanism referred to as “task-based hard attention” maintains\ntask-speciﬁc relevant information from others. This is realized by a gating mechanism, which activates\nor deactivates neurons depending on the task. Similarly to PathNet (Fernando, Banarse, et al. 2017),\nthis results in diﬀerent paths through the neural network for individual tasks. The binary attention\nmasks (gates) are already parameterized by the training process, whereby the selection is realized by\nthe task ID.\nJoseph and Balasubramanian (2020) propose the Meta-Consolidation for Continual Learning\n(MERLIN). The underlying assumption is that a meta-distribution (namely the latent space) exists for\neach task t. This distribution is learned and consolidated as an online variant. Several base networks\nare trained (for each task). The learned weights are used to learn task-speciﬁc parameters (VAE-like\nstrategy). In the consolidation phase, “task-speciﬁc prior” is generated to “reﬁne” the VAE.\nRegularization Methods\nRegularization methods aﬀect the parameters of a model so that the CF\neﬀect is mitigated. In the following, some well-known methods are brieﬂy presented.\nElastic Weight Consolidation (EWC), as proposed by Kirkpatrick, Pascanu, et al. (2017), is based\non the idea of penalizing changes in parameter that are important for previous tasks. For this purpose,\na Gaussian distribution of the information over the parameters is approximated using the Fisher\nBenedikt Pfülb\npage 29\n\nContinual Learning Models/Methods\nRelated Work\ninformation matrix. EWC is described in section 7.1.1 in greater detail. Huszár (2018) presents an\nadaptation of the method based on a Laplace approximation, which makes it a “multi-penalty version”\nof EWC. This adaption is used as the basis for the Progress & Compress (P&C) framework (Schwarz,\nLuketina, et al. 2018).\nZenke, Poole, et al. (2017) present the Synaptic Intelligence (SI) approach in their work. Each unit,\nor “synapse”, is trained to be “important” for a speciﬁc task. Unlike EWC, this method is an online\nvariant and determined on the basis of the “entire learning trajectory in parameter space” (Zenke,\nPoole, et al. 2017).\nLee, Kim, et al. 2017 introduce Incremental Moment Matching (IMM) as a transfer technique. For\neach task, Bayesian Neural Networks (BNNs, Bishop 1995) are trained one after another. The distinct\nfeature of IMM is that the parameters θ are speciﬁed according to a certain distribution, depending on\nthe data (posterior parameter distribution). Merging the networks is based on the calculated posterior\ndistributions of the weights. For this purpose, IMM uses an approximation of mixture of Gaussian\nposteriors. Two variants are distinguished: Mean, which averages the parameters while minimizing the\nKL-divergence; and mode, which tries to maximize the mixture of Gaussian posteriors.\nThe approach Riemannian Walk (RWalk) was introduced by Chaudhry, Dokania, et al. (2018). It\n“generalizes” Elastic Weight Consolidation (EWC) (Kirkpatrick, Pascanu, et al. 2017) and Synaptic\nIntelligence (SI) (Zenke, Poole, et al. 2017). Moreover, an eﬃcient version of EWC is presented – namely\nEWC++. The KL-divergence as a distance measure for the Riemannian Manifold is fundamental for\nthis method. The combination of the Fisher information matrix and KL divergence determines the\nparameter importance. However, a single-head output leads to “poor test accuracy”. An additional\nreplay procedure is used to counteract this disadvantage.\nLearning without Forgetting (LwF), as proposed by Li and Hoiem (2018) is based on Convolutional\nNeural Networks (CNNs). It can be classiﬁed as a knowledge distillation approach. The LwF approach\nuses some model parameters for all tasks θs, special ones for individual old tasks θo, and θn for\nthe current one.\nFor the training process, θn are initially adjusted.\nThe knowledge distillation\nloss determines how the other parameters are adjusted. Precisely, network outputs (pseudo labels)\nare speciﬁed as previous network states. Kim, Kim, et al. (2018) suggest a combination of EWC\n(Kirkpatrick, Pascanu, et al. 2017) and LwF (Li and Hoiem 2018), known as EWCLwF. An adapted\nloss function is introduced for this purpose.\nReplay-Based Approaches\nReplay-based approaches use stored or generated samples to bypass\nthe CF eﬀect by joint training. Ratcliﬀ(1990) describes the Experience Replay (ER) model as a very\nearly version of the replay process. Since then, many variations have been introduced. However, the\nprinciple is similar in all replay approaches (see section 2.3.5). Samples from past tasks are stored and\nreplayed for training with samples of the new task. Self-generated samples can also be used, which is\nreferred to as pseudo-rehearsal. Further approaches do not directly replay the samples but compute\ntheir inﬂuence on the model.\nGradient Episodic Memory (GEM) is suggested by Lopez-Paz and Ranzato (2017). GEM stores\nsamples for each task in an episodic memory (e.g., Nagy and Orban 2016). However, the samples\nare not replayed, but the loss is inﬂuenced by their loss gradients (depending on the angle between\ngradients). Chaudhry, Marc’Aurelio, et al. (2019) propose further developments, namely Averaged\nGEM (A-GEM). The crucial optimization is realized via the determination and replay of the averaged\ngradients from the previous tasks.\nThe work of Chaudhry, Gordo, et al. (2021) presents the Hindsight Anchor Learning (HAL)\nprocedure.\nThe approach is based on so-called “anchors”, which are used as representatives for\nindividual classes per task. These anchor points are determined by gradient descent and used to\nestimate the level of forgetting for further training.\nThe anchors are usually positioned on the\ntask borders, which allows for a better protection. Likewise, the anchors can be used to enable\n“experience-replay”.\nThe procedure coined by Aljundi, Lin, et al. 2019 is labeled Gradient based Sample Selection\n(GSS). It describes a procedure to create the best possible selection of samples representing all past\nproblems/tasks. For the determination of the set, the gradients or their angles to each other are used.\nFor this purpose, new samples in a “recent” buﬀer are compared with those in the “replay” buﬀer.\nIf necessary, they are exchanged. Furthermore, the greedy version of the algorithms represents the\nproblem of quadratic dependency of the buﬀer size. With this variant, the re-processing of all stored\npage 30\nBenedikt Pfülb\n\nRelated Work\nDiscussion\nsamples is omitted.\nRebuﬃ, Kolesnikov, et al. (2017) propose the incremental Classiﬁer and Representation Learning\n(iCaRL) model. Their approach includes a classiﬁer which works with the nearest-mean-of-exemplars\nclassiﬁcation. This is based on a compressed set of sample data. A learner uses these samples in\ncombination with a knowledge distillation process. Furthermore, a herding procedure is suggested,\nwhich is responsible for the selection of samples.\nShin, Lee, et al. (2017) introduce a pseudo-rehearsal procedure based on short-term memory referred\nto as Generative Replay (GR). A Generative Adverserial Network (GAN) (Goodfellow, Pouget-Abadie,\net al. 2014) is applied as generator, whereas the special variant Wasserstein GAN (Arjovsky, Chintala,\net al. 2017) is selected. In addition to the generator a solver is implemented. For each task, both parts\nare (re-)trained. After the completion of a task, the generator creates samples, which are then replayed\nfor a joint training. A detailed representation is given in section 9.2.1.2.\nMeta-Experience Replay (MER) by Riemer, Cases, et al. (2019) enforces gradient alignment using\nsamples. By means of this customization, the new gradients are appropriately adjusted via the old\nones. As a result, the interference between tasks decreases.\nAnother rehearsal method including knowledge distillation mechanisms is known as Dark Experience\nReplay (DER) (Buzzega, Boschini, et al. 2020). DER can be used in streaming scenarios, because it\naddresses real-world requirements. These include constant memory consumption, the lack of a test time\noracle and task boundaries. The procedure is based on a hyper-parameter α, which is consolidated\nbased on the “teacher-student approach”. In order to achieve that goal, data from a replay buﬀer is\nused. In addition, another version is speciﬁed that selects the data in the buﬀer based on a higher\nlikelihood.\nInterpretable Continual Learning (ICL) is a CL approach presented by Adel, Nguyen, et al. (2019).\nIt is based on a framework called Variational Continuous Learning (VCL) (Nguyen, Li, et al. 2018)\nwhich uses a replay buﬀer to gain CL performance. This model is extended by the introduction of\n“saliency map”. Saliency maps deﬁne the most relevant properties (parts) of an image (Zintgraf, Cohen,\net al. 2017) (also known as Prediction Diﬀerence Analysis (PDA)). For each sample, a saliency map is\ncreated. Moreover, it is used for the training of the learner, which in turn controls the focus.\nKamra, Gupta, et al. (2017) present the architecture referred to as Deep Generative Dual Memory\nNetwork (DGDMN). Like the work of Shin, Lee, et al. (2017), the DGDMN is based on a generator and\nsolver. The main diﬀerence is the provision of several generators. Generators are labeled Short-Term\nTask Memory (STTM) and realized by Variational Autoencoders (VAEs) (Kingma and Welling 2013).\nIn contrast, the solver represents the Long-Term Memory (LTM). Similarly to the processes during\nhuman sleep, the knowledge of the STTMs is transferred to the LTM.\n3.3\nDiscussion\nInitially, the discussion of related work focuses on the presented evaluation strategies. Subsequently, a\ncritical discussion with regard to the emerging CF avoidance models follows.\n3.3.1\nEvaluation Strategies\nFirst of all, the presented evaluation strategies describe the evaluation process insuﬃciently and with\nfew details. Instead, the type of data/tasks used for the evaluation is described. To be precise, related\nwork usually explains how the data is re-structured for CL tasks. Other important aspects concern\nthe real-world requirements and constraints, which are represented in several diﬀerent works. The\npresented real-world scenarios, however, tend to be rather ﬁctional than relevant use cases. In general,\nthe related works do not describe the entire CL training process including hyper-parameter selection\nand the training/evaluation process. This can be considered as an open issue that is often neglected\nwhen describing the experimental setup. The selection and justiﬁcation of hyper-parameters is often\nstated. In contrast to that, the process of determining the parameters is not. These details of the\nprocess can be interpreted very diﬀerently, and the implementation leads to very diﬀerent outcomes.\nThus, the evaluation scheme is crucial for the review of ML models in CL scenarios. Even though the\nobtained results are correct, the speciﬁc circumstances remain unspeciﬁed. To conclude, the presented\ncriticism of other evaluation schemes constitutes one of the main desiderata and, thus, the basis for\nthis work.\nBenedikt Pfülb\npage 31\n\nDiscussion\nRelated Work\n3.3.2\nExisting Catastrophic Forgetting Avoidance Models\nCL as a research area oﬀers a great number of applications by now and therefore great opportunities.\nIn the last few years, this trend has been on the increase. The gained traction is clearly illustrated\nby the number of newly developed deep learning models. More and more of them emerge, and every\nsingle one is improving the CL performance.\nIn each of the related works, the reviewed procedure is almost the same: A novel model is introduced,\nfollowed by an experimental setup with the evaluation scheme, and ﬁnally it is shown that the CF eﬀect\ndecreases compared to other models. Many related works refer to the CF problem as “resolved” (Lee,\nKim, et al. 2017), “controlled” (Serra, Suris, et al. 2018), “mitigated” (Chaudhry, Dokania, et al. 2018),\nor “avoided” (Nguyen, Li, et al. 2018) – just to provide a few examples. Accordingly, the fundamental\nquestion is why should the CF problem still be relevant at all? Interestingly, new models are being\nintroduced, as mentioned. Every one of them claims to improve the CL performance compared to\nprevious models. But why does the work on new, improved models continue, if the CF problem is\nalready solved and the “inﬁnite accumulation of knowledge” is possible? Accordingly, the existence of\nsuch a model or mechanism would result in the application in numerous scenarios.\nAnother issue with many of the related works does not concern the presentation of the models or new\nmechanisms, but often the poorly detailed circumstances under which the models are evaluated. The\nreproducibility of results is diﬃcult in many cases – or simply impossible. Without the speciﬁcation of\nthe exact requirements/constraints/circumstances, a statement concerning the CF behavior is diﬃcult\nto judge. In addition, the code base, if presented at all, only includes the model, but not the training\nor evaluation scheme. This may be due to a lack of the community’s interest, or to space limitations.\nUnfortunately, the seemingly unimportant, precise description of the evaluation procedure, is too often\nomitted in the literature.\npage 32\nBenedikt Pfülb\n\n4.\nResearch Design\nChapter Contents\n4.1\nResearch Questions in Detail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.2\nResearch Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n4.3\nExhaustive Grid Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nThis chapter speciﬁes the research procedures applied to achieve the objectives of this work. The\nstarting point is constituted by four research questions, which are divided into more speciﬁc sub-\nquestions. They deﬁne the structure of this dissertation. In the following, the detailed steps and\nprocesses of the investigation are presented. The same is true for the interrelation of the research\nquestions. As several conclusions are based on empirically validated results, the setup of the conducted\nexperiments is presented. This includes a brief description of the experimental environment and how\nthe experiments are performed.\n4.1\nResearch Questions in Detail\nThe previously introduced research questions (see section 1.2) are presented in the following along\nwith the respective sub-questions. The research design is derived from the research question and it is\nrepresented in this work’s structure.\nThis research examines the continual learning (CL) paradigm for machine learning (ML) models,\nespecially Deep Neural Networks (DNNs). In this context, the catastrophic forgetting (CF) eﬀect in\ndeep learning methods is of special interest. The CF eﬀect describes that all stored knowledge is lost, as\nsoon as new knowledge is tried to be added to an already trained ML model. Some works claim to have\nsolved this problem, but this only accounts for special conditions. However, these conditions do not\nreﬂect the real-world, application-oriented requirements of CL scenarios. As a result, the application\nof available approaches may cause problems in real-world scenarios.\nTherefore, one of the goals of this work is to develop an evaluation protocol that reﬂects real-world\nrequirements. This will be used to investigate ML models that are intended to prevent the CF eﬀect. In\nresponse to the problem, a novel method is to be developed. This method will be evaluated according\nto the application-oriented protocol. Taking these goals into account, the following detailed research\nquestions are derived:\nRQ 1: How can an application-oriented validation protocol be modeled to detect catastrophic forgetting?\nThe aim of this ﬁrst research question is to develop a veriﬁcation method to detect the avoidance\nof the CF eﬀect. Requirements and constraints derived from a real-world CL scenario are included.\nThrough the use of a uniform and application-oriented evaluation protocol, a consistent and\ncomprehensive investigation is targeted. In order to answer this ﬁrst question and develop the\napplication-oriented validation protocol, the following sub-questions are raised:\nRQ 1.1: Which requirements can be derived from a real-world application scenario?\nIn order to answer this question, requirements are identiﬁed by examining a real-world scenario.\nThe applicability of deep learning models will be investigated along with the emerging restrictions\nrelated to CL behavior.\nRQ 1.2: How do machine learning models behave that are aﬀected by catastrophic forgetting?\nBenedikt Pfülb\npage 33\n\nResearch Structure\nResearch Design\nThe ﬁrst step involves an investigation of how models respond when presented with a CL task.\nThe goal is to make the occurrence of the CF eﬀect detectable and measurable. In order to\nachieve this, a metric has to be deﬁned. The metric is supposed to measure what happens\nonce new knowledge is added to models that already derive existing knowledge from data. The\nquestion can be answered by conducting ﬁrst experiments and by considering the literature.\nRQ 1.3: How are existing models examined for the catastrophic forgetting eﬀect?\nThe investigation of other evaluation methods from related works begins with their identiﬁcation\nand classiﬁcation. As a part of this process, promising methods or ideas are extracted and\nadapted for further steps of this research.\nRQ 2: To what extent can existing machine learning models avoid the catastrophic forgetting eﬀect?\nThe second question has the goal to examine existing CF avoidance models for their suitability\nin CL scenarios. For this purpose, the application-oriented protocol is applied, which has been\ndeveloped as an answer to RQ 1. Well-known models are evaluated by means of diﬀerent learning\nproblems in various variants. Since the models and the resulting performance usually depend on\na wide range of parameters, a large-scale investigation is conducted. The aim of the study is to\nshow to what extent the CF eﬀect is suppressed by one or more models. Again, the following\nsub-questions are supposed to specify the main RQ 2:\nRQ 2.1: Which adapted deep learning models avoid the catastrophic forgetting problem?\nThis question will be answered by a systematic literature investigation in the context of CF\n(see chapter 3). A prerequisite for a balanced and reproducible evaluation is, however, the\nprovision of a code base by the authors of the respective research. Hence, all provided data and\ninformation needs to be processed in the same, or comparable way, by all investigated models.\nRQ 2.2: How can machine learning models be benchmarked under real-world conditions?\nThis research question aims at the development of an evaluation framework. The goal is to\ninvestigate existing ML models in a consistent manner and environment. By developing a new\ninterface for training, testing and evaluation, all of the respective models are examined for the\noccurrence of the CF eﬀect and its CL performance.\nRQ 3: How can a novel deep learning model be designed so that it avoids catastrophic forgetting?\nThe aim of this question is to present a new or adapted ML method, if none of the investigated\nmodels provides satisfactory results under real-world conditions. Elements and methodologies of\nexisting models (RQ 2) can serve as a basis. Alternatively, a completely new method in the ﬁeld of\ndeep learning may be developed. RQ 3 is composed of two sub-questions:\nRQ 3.1: How do existing deep learning models attempt to control the catastrophic forgetting eﬀect?\nAfter the evaluation of models in RQ 2, the best ones are examined in greater depth with regard\nto the used techniques and methods. The goal is to identify the most eﬀective methods and, if\nnecessary, transfer them to other models.\nRQ 3.2: Which machine learning models are not deep, but still avoid catastrophic forgetting?\nSince CF results from the distributed representation of knowledge, alternative models are\nexamined. This aims at ﬁnding an answer to the question whether parts or entire models can be\ntransferred into a layered structure, which allows them to be considered deep learning models.\nRQ 4: How does the novel model perform when compared to existing continual learning models?\nThis question aims at the evaluation and comparison of the novel model and the already investigated\nmodels. The CF detection protocol from RQ 1 is applied for this purpose.\n4.2\nResearch Structure\nThe research structure reiterates the relation between research questions and the structure of the work.\nFor this purpose, the questions including speciﬁed sub-questions are summarized and displayed in\nﬁgure 4.1.\nThe ﬁrst research question RQ 1 focuses on the development of an application-oriented evaluation\nprotocol. With the help of this protocol, existing models are going to be examined with respect to\nthe CF eﬀect. The goal is to show whether certain models can control the eﬀect and if they can be\ncompared.\nSince the focus of this work is on real-world applications, requirements of realistic CL scenarios need\nto be gathered at ﬁrst. In order to understand CL scenarios, an exemplary application is presented.\nThe identiﬁed requirements of the real-world scenario constitute the answer to RQ 1.1 (see chapter 5).\npage 34\nBenedikt Pfülb\n\nResearch Design\nResearch Structure\nRQ 1: Uniform method for detecting and evaluating the catastrophic forgetting effect\napply\nconsider\nRQ 4: Evaluate with uniform protocol\nRQ 1.2: Typical behavior induced by the catastrophic forgetting effect\nand\nand\nRQ 1.3: Investigation of existing catastrophic forgetting detection methods \nRQ 1.1: Continual learning requirements based on an exemplary real-world scenario\nRQ 2: Evaluation of existing models which address the catastrophic forgetting problem\nRQ 2.1: Extract relevant catastrophic forgetting avoidance models\nRQ 2.2: Investigation and evaluation within a uniform framework\nand\nRQ 3.1: Advancement of existing catastrophic forgetting avoidance models \nRQ 3.2: Investigation of non-deep catastrophic forgetting avoidance models\nor\nRQ 3: Development of a novel method or model to control the catastrophic forgetting\nFigure 4.1: Research structure and interrelations of research questions.\nFor the development of a research protocol, CF has to be quantiﬁed. Although the eﬀect is\ngenerally known, the question arises how CF can be measured and which conditions are required for\nits measurement. While an experiment illustrates how the CF eﬀect manifests (RQ 1.2), the eﬀect\nis described in the literature. CF avoidance models are usually evaluated in the respective research\npapers. However, a detailed description of how these models are evaluated is often unavailable. This\nmakes it diﬃcult to compare the existing results, which is why this work argues for a standardization\nof evaluation protocols.\nThe respective models along with the corresponding evaluation methodologies are presented in\nchapter 3 (RQ 1.3). As a result for RQ 1, a detailed evaluation protocol is proposed in chapter 6. It is\none of the important contributions of the present work.\nThe second research question RQ 2 aims at the investigation of a subset of the presented CF\navoidance models from chapter 3. The objective is to evaluate the CL capabilities of the models under\na uniform protocol. For this purpose, the application-oriented protocol from chapter 6 is applied.\nRQ 2.1 and RQ 2.2 address whether the models are compatible with the evaluation protocol and how\na uniﬁed validation can be implemented. Results of the study and lessons learned are presented in\nchapter 7 as an answer to the second research question.\nResearch question RQ 3 focuses on the development of a novel model in order to address the CF\neﬀect. Based on the results of the previous study (RQ 2, see chapter 7), an existing method may\nbe developed further (RQ 3.1). Alternatively, the development of a novel ML model may be more\nbeneﬁcial (RQ 3.2). Presumably, models and techniques that inherently do not exhibit the CF eﬀect\ncan be used. The transfer of an existing model into the deep learning context is a prerequisite for\nfulﬁlling the given requirements. This should result in a new model (chapter 8) answering RQ 3.\nThe present work is concluded by answering the last research question RQ 4 in chapter 9. For this\npurpose, the newly developed deep learning model is transferred into the CL context. At the same\ntime, the evaluation of the new model (chapter 8) is performed by means of the CF detection protocol\nas described in chapter 6. In order to answer RQ 4, the results are compared to the evaluation of\nother CF avoidance models.\nAll research questions are revisited and answered in chapter 10. In addition, the contents and\nﬁndings of the individual chapters are brieﬂy presented. An analysis of the individual components of\nthe overall work concludes the discussion.\nBenedikt Pfülb\npage 35\n\nExhaustive Grid Search\nResearch Design\n4.3\nExhaustive Grid Search\nEmpirical experiments are conducted to draw conclusions about the investigated ML models. Since\nmany parameters inﬂuence the results of ML methods, the signiﬁcance of single experiments requires\na critical assessment. One crucial factor refers to hyper-parameters, another one to the random\ninitialization states. Both of them can heavily inﬂuence the results. In order to guarantee the validity\nof statements, a large number of parameter conﬁgurations has to be evaluated. This in turn requires\na corresponding amount of computing capacity, so that a suﬃcient number of experiments can be\nconducted. Compliance with a realistic time scale is assumed, as a 20 year duration exceeds the limit.\nOne of the prerequisites is the independence of experiments, which allows for parallelization. Another\nprerequisite is the availability of suﬃcient computing capacities, as it is a large-scale investigation.\nDue to the limited conditions, a special system was developed that allows for the computing of massive\nexperiments by using further computational resources.\nThe software developed for this purpose is specially designed for the performance of distributed\nexperiments and the collection of results.\nAdditionally, a ﬂexible option for the generation and\nevaluation of experiments was implemented. A brief description of the environment is presented in\nthe following. It outlines the selected experiments and the distribution strategy. Finally, the module\nproviding the diﬀerent datasets is presented.\nExperiment Distributor\nAs the present research requires immense processing capacities, the\nparallel processing of experiments seems an adequate solution. The basis for the parallel execution\nof experiments is referred to as experiment distributor in this work. The individually parameterized\nexperiments are distributed to available compute nodes. Interestingly, the used nodes are regular\ncomputers located in the university’s laboratories. These computer labs are usually used by students\nand lecturers during week days. On weekends and at night, these resources are available and can\ntherefore be used for research.\nThe experiment distributor is a lightweight tool and follows a simple server/client architecture.\nThe server starts a web server via Secure Shell (SSH) on the client side, which is used for further\ncommunication. Subsequently, experiments can be launched in the form of independent processes by\na Representational State Transfer (ReST) interface. The state of an experiment is monitored by its\nprocess ID. Once an experiment is completed, the resulting data/ﬁles are collected. Since the available\nnodes contain diﬀerent hardware components (see table 4.1), the number of experiments can be limited\nindividually per node.\nTable 4.1: Used computational resources.\n#\nGPU Type\nCUDA\nMemory\nClocking\nCPU\nRAM\nCores\n(GB)\n(MHz)\n(# × GHz)\n(GB)\n1\nTITAN Xp\n3840\n12\n1582\n24 × 2.3\n64\n1\nGTX 1080 Ti\n3584\n11\n1480\n24 × 2.3\n64\n10\nRTX 2080\n2944\n8\n1515\n8 × 3.4\n16\n2\nGTX 980 Ti\n2816\n6\n1000\n24 × 2.3\n64\n20\nQuadro P5000\n2560\n16\n1600\n16 × 3.2\n32\n40\nRTX 2060 Super\n2176\n8\n1470\n8 × 3.6\n16\n20\nQuadro K2200\n640\n4\n1046\n24 × 2.4\n32\n90\nQuadro P620\n512\n2\n1354\n16 × 3.6\n48\nDespite the heterogeneous hardware, various operating systems are used (i.e., Windows and Linux)\nDue to the tailored interface, the invocation of a remote method is generic. Therefore, heterogeneous\noperating systems can be utilized. As some operating system-speciﬁc diﬀerences need to be considered,\nthey are implemented individually by the distribution tool. Moreover, there is another prerequisite for\nthe use of a node. This is due to the used ML library TensorFlow, which is designed for a speciﬁc\nhardware manufacturer. Thus, an Nvidia GPU and the ML framework need to be available.\nBefore an experiment can be executed on a node, all prerequisites have to be met. This includes the\nrequired software modules (e.g., Python), libraries (e.g., TensorFlow) and, of course, the experimental\ncode. These requirements are checked and resolved before the actual experiments are executed. In\npage 36\nBenedikt Pfülb\n\nResearch Design\nExhaustive Grid Search\norder to perform an individual execution of the experiments, it is assumed that the underlying code\nbase can be controlled by command line parameters. These can be transferred easily to the individual\nnodes/clients and ﬁnally be executed. Result ﬁles can be traced back from an experiment ID and\nmapped to the command line parameters. Additionally, the parameter conﬁgurations are recorded as\npart of the result ﬁles.\nThe distribution proceeds as follows: Potential nodes are identiﬁed on the basis of a conﬁguration\nﬁle, which checks for accessibility and requirements. Likewise, the experiments are imported and\nprepared speciﬁcally for diﬀerent operating systems by adjusting the command line parameters. If the\nprocessing time of a node is within a free time slot, one or more experiments are started depending on\nthe node deﬁnition. In order to do this, the selected parameters are transmitted and a corresponding\nprocess is started. As soon as the processing ends or a timeout is triggered, the result ﬁles are collected\nand stored. However, a node may fail during its designated time slot, or it may not work due to the\nchoice of parameters. In both of these cases, the experiment is repeated or marked as failed.\nExperiment Generator\nAnother component of the experiment distribution tool is the experiment\ngenerator. It allows for the ﬂexible creation of experiments and their command line parameters. On\nthe one hand, the experiment generator combines single possible hyper-parameters. On the other\nhand, it implements additional features. Accordingly, dependent parameters can be added, where the\nparameter b is set depending on parameter a. Furthermore, experiment IDs are assigned, the result\nﬁles are speciﬁed and the number of experiment repetitions can be determined.\nExperiment Evaluator\nAnother component of the developed software is responsible for the\nevaluation. In this context, command line parameters can be speciﬁed according to the aggregated\nand evaluated experiments. The repeated execution of individual parameter conﬁgurations can, for\nexample, be used for an aggregation. In this case, the Comma-separated values (CSV) or JavaScript\nObject Notation (JSON) ﬁles are imported, converted, aggregated and evaluated according to the\nspeciﬁed method. Finally, plots can be generated or results can be compiled for the generation of\ntables.\nExperiment Dataset\nVarious types of benchmark datasets are used for evaluation. These are\nusually available in the form of public datasets. For the distributed application itself, the datasets need\nto be available on each computation node. This is provided by the experiment dataset component. If a\ndataset does not exist on the node, it is automatically downloaded and stored in a pre-processed format\non the local node. After these pre-processing steps, the pre-processed datasets can be loaded into\ndiﬀerent experiments via an interface. As a last step, they are further processed/adjusted according to\nthe speciﬁed command line parameters. The command line parameters can be used to deﬁne diﬀerent\ntypes of dataset divisions for each individual experiment.\nBenedikt Pfülb\npage 37\n\nExhaustive Grid Search\nResearch Design\npage 38\nBenedikt Pfülb\n\n5.\nExemplary Real-World Scenario\nChapter Contents\n5.1\nProblem Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.1.1\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n5.2\nFlow Data Stream Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n5.2.1\nFlow Data Stream Server\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n5.2.2\nFlow Data Stream Client . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n5.2.3\nData Flow of Network Flow Data . . . . . . . . . . . . . . . . . . . . . . .\n47\n5.3\nTraﬃc Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n5.3.1\nLabel-based Data Distribution . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n5.3.2\nStructural Data Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n5.4\nFlow Prediction Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n5.4.1\nDeep Neural Network Architectural Experiments\n. . . . . . . . . . . . . .\n51\n5.4.2\nConcept Drift/Shift Experiments . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.4.3\nStreaming Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.4.4\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n5.5\nReal-World Continual Learning Requirements\n. . . . . . . . . . . . . . . . . . . .\n58\n5.6\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\nIn this chapter, an applied scenario of continual learning (CL) is introduced. It will be used to derive\nrealistic requirements that are imposed on a typical application (based on the publications Pfülb,\nHardegen, et al. 2019; Hardegen, Pfülb, et al. 2019; Hardegen, Pfülb, et al. 2020).\nThe scenario represents a section of today’s standard computer networks where routing algorithms\nare responsible for path ﬁnding. When a network packet is sent, the computer behaves as if sending a\nletter, a package or even something bigger. Likewise, a number of decisions has to be made in order to\nsuccessfully deliver the packet. Moreover, the context needs to be considered. In this example, the\nsender is not concerned with the transportation route of the package. This decision is made by a carrier\nor a delivery chain. The transportation route is optimized based on various criteria. If the package is\ntoo large, e.g., a wind turbine, a heavy load transport needs to be organized. This in turn limits the\nselection of the route, e.g., with regard to the maximum street width or the load capacity of bridges.\nTransferring these concepts to the computer network scenario, the following challenge arises. The\nscope of a communication (e.g., number of packages or duration) between computers of a network is\nunknown prior to its initiation. In conventional networks, this information cannot be used to determine\nthe path. Therefore, as a rule, the shortest path is selected, which is not necessarily the best/fastest\none. The shortest path, however, does not take into account whether the route is suitable, for example\ndue to bottlenecks or traﬃc jams (depicted in ﬁgure 5.1). Thus, a signiﬁcant opportunity to improve\nthe process is constituted by the prediction of a communication’s scope.\nNevertheless, the explained scenario is subject to temporal changes. Various events may inﬂuence\nthe behavior of transmitters, e.g., a pandemic or recurring events like holidays. New applications,\nupdates or simply the users’ behavioral changes transfer this problem into the CL problem.\nContributions\nThis chapter presents an application-oriented scenario that helps derive requirements\nfor CL scenarios. The resulting requirements serve as an answer to research question RQ 1.1 (see\nsection 4.1).\nBenedikt Pfülb\npage 39\n\nProblem Description\nExemplary Real-World Scenario\nFigure 5.1: Outline of the exemplary real-world problem.\nThe most important contribution is the deﬁnition of real-world requirements and constraints. These\nhave to be addressed by machine learning (ML) applications in the context of CL scenarios. Accordingly,\nit is crucial to include them into the evaluation protocol for measuring the CL performance. The\nresulting protocol therefore attempts to guarantee the validity of the investigated models in an\napplication-oriented context.\nStructure\nThe structure of the chapter is as follows: First, the detailed problem description is\npresented including a simpliﬁed use-case (see section 5.1). Moreover, problem-speciﬁc related work\nwith regard to traﬃc-classiﬁcation is summarized. Subsequently, an overview of the data acquisition\nand pre-processing steps is provided (see section 5.2). In section 5.3 a data analysis is performed with\nregard to the exemplary CL problem. Also, this chapter outlines the results of all related experiments\nin section 5.4. The present chapter is concluded by deriving and summarizing requirements (see\nsection 5.5) from this real-world scenario for the CL context.\n5.1\nProblem Description\nComputer networks are used to exchange information between connected nodes. In order to enable\ncommunication within a computer network, systems must be identiﬁable so that a message exchange is\npossible. The connections between individual components of a network are known as links, where the\ncomplete route represents the path. The path is determined via the destination address. For the sake\nof simplicity, the term path is used in this work, even though technically a link is denoted (path of\nlength 1). As a metric for determining the “best” path, the number of hops (routers on the way to the\ndestination) is generally used.\nProblems may occur in case a path is already utilized or even overloaded. This challenge can\nbe compared to a traﬃc jam. Typically, it is advisable to consider bypassing the concerned section\nof the route. The same solution is usually suggested by navigation systems. However, this requires\nthe deviation from the original route. The pitfall in terms of network communication is that the\ndecision about the path is made as soon as the connection is established. Thus, redirecting the\ncommunication is hardly possible. This lack of ﬂexibility is due to the stateful network components\n(so-called middleboxes) on the way to the target, such as ﬁrewalls (Brim and Carpenter 2002; Iyengar,\nRaiciu, et al. 2011). The described phenomenon is particularly true for connection-oriented network\nprotocols such as Transmission Control Protocol (TCP).\nOne approach to address the problem is determining the size of an emerging communication in\nadvance. However, two pieces of information need to be available as a prerequisite: First, the maximum\npossible bandwidth and the current utilization of the path is required. The utilization information\ncan be obtained from the network components by means of telemetry data. Second, the bandwidth\nrequired for the network communication has to be available. The bandwidth (bits per second) is\nconstituted by the ratio of transmitted data volume (bits) to their transmission time (duration in\nseconds). Part of the challenge is that meta-data of a communication are usually available after its\ntermination. A collection of meta-data is referred to as network ﬂows or in short – ﬂows.\nThe prediction of information, such as the bit rate of a ﬂow, is not trivial. This determination can\npage 40\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nProblem Description\nbe addressed by ML techniques. By using an ML-based prediction, an optimal path can be selected\nbefore the connection is established. The determination of the path can be realized by using Software\nDeﬁned Network (SDN) techniques (Kreutz, Ramos, et al. 2015). These allow for speciﬁc paths to be\nprogrammed for individual ﬂows based on the underlying network components. Besides the eﬃcient\nutilization of the network resources, other application scenarios may be considered.\nConventional computer networks consist of a large number of diﬀerent types of components and\nautonomous systems. Such systems usually follow ﬁxed rules so that predictions are somewhat easier.\nBackups are, for example, often scheduled during night-time, resulting in a load on the network. A\nmore complex load of the network is caused by humans. The respective traﬃc results from applications\nor other online activities. The latter are inﬂuenced by many factors, e.g., time of the day and year, the\nused applications, new trends, hashtags, habits, etc. Especially the load caused by humans leads to a\nhigh variance and constantly changing ﬂow patterns. Due to these continuous changes, this exemplary\nscenario is set in the context of CL.\nIn order to train a supervised ML model, data needs to be collected for training. Meta-data of\nterminated/closed network connections can be used for this purpose, in this case the training of Deep\nNeural Networks (DNNs). The relevant meta-data primarily includes the so-called 5-tuple consisting\nof a source/destination IP address, the source/destination port and the protocol. Since the ﬂows are\nalready completed, additional meta-data is available, such as the required bandwidth of the ﬂow. The\ngoal is to use an ML model to predict the target values (e.g., bit rate) only based on the data stored in\nthe 5-tuple. A visualization of the prediction problem is available in ﬁgure 5.2. The ﬁgure displays a\ntransmitter on the left, as well as a receiver on the right-hand side. The ﬂow meta-data (the 5-tuple),\nwhich is available once a communication is initiated, is classiﬁed using an DNN. In this example, the\nbit rate serves as the label which has to be predicted.\n192.168.44.22\nSrc IP\n193.174.29.11\nDst IP\nDst Port\n80\nSrc Port\n55555\nProtocol\n6\n193.174.29.11:80\n192.168.44.22:55555\nBit Rate\nDNN\nMedium\nLow\nHigh\nFigure 5.2: Use of ML to enable the classiﬁcation of network ﬂows.\n5.1.0.1\nUse Case\nIn the following, the outlined application scenario is illustrated in form of a use case. The use case\nindicates the disadvantage of path determination with the standard decision metric, i.e., the number of\nhops. Using only the number of hops can, however, lead to the overloading of a path, so that other\npaths are slightly utilized. The uneven utilization of the network can cause interferences which aﬀect\nall ﬂows on the route. This in turn can lead to an increased latency or even to packet loss/drop.\nUltimately, the application and user experience can be aﬀected.\nFigure 5.3 shows a simple computer network that consists of 7 routers {R1, . . . , R7}. There are\n3 paths (Px) between R1 and R7, which serve as source and destination. P1 consists of routers R1,\nR2 and R7. The second path P2 = (R1, R3, R4, R7) and the third P3 is deﬁned by (R1, R5, R6, R7).\nAt path P2 and P3, four routers have to be traversed. The path P1 is the shortest one, with three\nrouter hops and therefore the preferred path in a classic routing scenario. It is assumed that each path\n(link) oﬀers the same bandwidth (capacity of 100 %). In this scenario, three diﬀerent load levels are\ndistinguished: low (<10 %, green), medium (10 - 90 %, orange) and high (>90 %, red). The challenge is\nto route three ﬂows from source R1 to destination R7. Flow f1 and f3 requires a bandwidth capacity of\n40% each, whereas ﬂow f2 only requires 30%. Since P1 is the shortest path, all three ﬂows are routed\nvia P1. This results in a congestion of P1 with 110% (over)load. At the same time, the paths P2 and\nP3 are not utilized at all. The scenario displayed in ﬁgure 5.3 shows that routing one ﬂow via a free\npath would solve the congestion. However, this is not possible due to the used metric (minimum hops).\nIn order to solve this problem, the ahead-of-time estimation of the required bandwidth of a ﬂow can\nBenedikt Pfülb\npage 41\n\nProblem Description\nExemplary Real-World Scenario\nbe used as a more “intelligent” metric.\nFigure 5.3: Use case scenario with congestion on a single shortest path.\nAssuming that it is possible to predict the bit rate of a ﬂow in advance, the scenario can be modiﬁed.\nIn this case, the structure of the network, as shown in 5.3, does not change. Based on the prediction,\nhowever, the expected bit rate of the ﬂows is (perfectly) known: f1, f3 =40% and f2 =30%. The three\nﬂows occur sequentially, so that a path for f1 is determined ﬁrst. Since the utilization for the shortest\npath P1 (R1, R2, R7) is not yet at full capacity (0% + 40% < 100%), f1 is routed over it. The same\napplies to f2. For both paths, the current load is unproblematic (40% + 30% < 100%). Only the\nattempt to route the last ﬂow f3 over the shortest path can lead to a predicted overload and detection.\nThus, an alternative path is selected, which is longer but less utilized. P2 and P3 are available for\nselection. A random choice between both options is suﬃcient, since both are equally utilized and have\nthe same length.\nThe result of the prediction-based routing can be seen in ﬁgure 5.4. It is obvious that no congestion\noccurs on the shortest path and that the network is therefore equally utilized. Similarly, the ﬂows\nare not suﬀering from congestion, which can result in a lower latency. To conclude, the application of\nprediction-based routing can lead to a higher quality of an application.\nFigure 5.4: Preventing path congestion by throughput prediction.\n5.1.1\nRelated Work\nThe scenario is located in the research area of computer networks, especially traﬃc engineering. This\nspeciﬁc area has been the focus of research for many years (Awduche 1999; Agarwal, Kodialam, et al.\n2013). In recent years, the application of ML models has been established (Wang, Cui, et al. 2018;\nFadlullah, Tang, et al. 2017a; Yao, Mai, et al. 2019; Rusek, Suárez-Varela, et al. 2019; Zhuang, Wang,\net al. 2019). This is due to the fact that the hardware required for machine learning has become\nso powerful. Another aspect is the performance that can be achieved with ML models. Since most\ncomputer networks are still routed in a destination-oriented manner, the use of ML is a promising area\nof research and advancements. An interesting application is the prediction of traﬃc characteristics, e.g.,\nthe classiﬁcation into “mice” and “elephant” network ﬂows (Mori, Uchida, et al. 2004). This approach\nmay achieve an optimized utilization of capacities (Valadarsky, Schapira, et al. 2017). The same is true\nfor predicting the duration of a ﬂow (Jurkiewicz, Rzym, et al. 2018). Basic summaries of the entire\nsubject area are outlined in the literature ( Nguyen and Armitage 2008; Fadlullah, Tang, et al. 2017b;\nBoutaba, Salahuddin, et al. 2018; Mohammed, Mohammed, et al. 2019; Zhang, Huang, et al. 2019 and\nWang, Cui, et al. 2018).\nRecent developments in the ML area have advanced their application in the research area of\ncomputer networks. However, there is very few related work that addresses network ﬂow prediction\nin terms of continual learning. Another issue of related work is the type of data being used – either\nsynthetic, or real-world data. For synthetic data, a generator is required. The problem of generating\nprocesses is that ML models may easily derive the generating function. This is mostly due to the\nlacking complexity of the generator so that it cannot necessarily represent a real-world problem.\npage 42\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nFlow Data Stream Pipeline\nData Gathering\nThe viable alternative is to collect real-world data, which in turn entails challenges.\nA comprehensive collection of real-world data is supposed to include at which points within the network\nthe data is collected. Moreover, the question arises whether the data is representative at all. Another\ntechnical challenge during data acquisition concerns the used components. Even though data has to\nbe captured by the hardware components, information extraction is not their primary function. At\nthe same time, the amount of generated data is too large to store. For this reason, processing the\ndata in a stream is the only option. Furthermore, it is often diﬃcult to obtain datasets from network\ntraﬃc because they are either outdated, contain too few data elements, or are simply not published\nfor privacy reasons. In the works of Poupart, Chen, et al. (2016); Azzouni, Boutaba, et al. (2017) and\nXiao, Qu, et al. (2015), data collection is performed by using OpenFlow (McKeown, Anderson, et al.\n2008). Poupart, Chen, et al. (2016) criticize the scaling of the collection strategy, as it does “not scale\nwell”. This is mainly due to the available memory of the used hardware (switches or routers). The\nhardware limitation sometimes leads to packages not being included in the statistical analysis. As a\nresult, the data may contain a bias that represents a distortion of reality.\nOther works capture all or parts of the network traﬃc and select “elephant” ﬂows, which are labeled\nwith Quality of Service (QoS) class labels in the second step (Wang, Cui, et al. 2018). Deep packet\ninspection is used for this purpose. In the work of Shi, Li, et al. (2017) real-world data is used. However,\nthe data was extracted from a PC room with about 20 computers. Moreover, the resulting dataset is\nnot accessible. In contrast to the already presented works, simulators are applied (Rastegarfar, Glick,\net al. 2016; Valadarsky, Schapira, et al. 2017). Benson, Akella, et al. (2010) analyze network traﬃc\nfrom multiple data centers and conclude that this data is very complex and data center speciﬁc.\nBinary Classiﬁcation\nA technical detail presented in almost every paper is the classiﬁcation between\n“mice” and “elephant” ﬂows. The regression problem is thus transformed into a binary classiﬁcation\nproblem (Poupart, Chen, et al. 2016; Xiao, Qu, et al. 2015; Valadarsky, Schapira, et al. 2017). In this\ncontext, the question of the ground truth of data arises (see section 6.1.1, Wang, Cui, et al. 2018). The\nchoice of the threshold is problem-speciﬁc and must be adapted to the scenario. A study by Poupart,\nChen, et al. (2016) examines diﬀerent threshold ranges. Likely, Rastegarfar, Glick, et al. (2016) and\nother authors (e.g., Jurkiewicz, Rzym, et al. 2018) encounter the large imbalance in the distribution\nof data within these two classes. Benson, Akella, et al. (2010) highlight this as a problem in their\nresearch. In contrast to other works, no size of the ﬂow is determined by Reis, Rocha, et al. (2019),\nbut an additional step is used to directly make a path-based routing decision.\nFeatures for ML\nLikewise, related work deals with the question of which features/attributes are\ncollected and how they are transformed. The question which features are used is often answered with\nthe 5-tuple (source/destination IP, source/destination port and transport protocol) (Xiao, Qu, et al.\n2015). However, there are studies focusing on the ﬁrst three transmitted packets, such as Poupart,\nChen, et al. 2016. The work of Wang, Cui, et al. (2018) considers as many as 20 packets. It is their\ngoal to determine whether the ﬂow is a client request or a server response. In order to evaluate the\nfeature transformation performed by related works, it is often necessary to study the program code\n(Poupart, Chen, et al. 2016) – if it is published (Xiao, Qu, et al. 2015). Although the normalization of\ndata is mentioned in Azzouni, Boutaba, et al. 2017 and Reis, Rocha, et al. 2019, it is not clear how\nexactly the individual features are converted into the value range [0, 1].\nOﬄine vs. Online Learning\nA key factor for the online or oﬄine training of the ML models is\nthe data processing strategy. In this context, the memory (M) and time complexity (O) for inference\nand training is the subject of discussion. This especially refers to the used ML models. A study\nby Poupart, Chen, et al. 2016 uses online methods (DNNs, Gaussian Process Regression and Online\nBayesian Moment Matching), allowing for the processing of theoretically inﬁnite large data streams.\nThis ability mainly concerns the incremental training of models. Other works do not address this issue\nor directly refer to the oﬄine learning paradigm (e.g., Wang, Cui, et al. 2018).\n5.2\nFlow Data Stream Pipeline\nThe example scenario presented in section 5.1 is implemented by means of an experimental application.\nThe application’s core is constituted by the so-called Flow Data Stream Pipeline (see ﬁgure 5.5). This\nBenedikt Pfülb\npage 43\n\nFlow Data Stream Pipeline\nExemplary Real-World Scenario\npipeline describes the processing stages of the application step by step. It includes the collection, the\npre-processing as well as the learning and evaluation process of the data by an ML model. In order to\nincrease ﬂexibility, processing components are logically separated. As a result, all processes that do\nnot depend on parameters are implemented as a server application (Flow Data Stream Server). The\nFlow Data Stream Client can be adapted to diﬀerent conditions by parameterization. This allows for\nthe performance of a large number of experiments without the necessity of major adjustments.\nFigure 5.5: Flow Data Stream Pipeline.\n5.2.1\nFlow Data Stream Server\nThe ﬁrst half of the Flow Data Stream pipeline is implemented as a server application. It is responsible\nfor collecting data (as described in section 5.2.1.1) and performing initial processing steps (detailed\nin section 5.2.1.2). The division into a server-client architecture has several advantages. On the one\nhand, processing steps that apply to all clients receiving data from the Flow Data Stream Server\nare combined. On the other hand, the executed processing steps are subject to data protection. As\na consequence, the Flow Data Stream Server needs to be hosted by an authorized institution (e.g.,\nautomated processing of IP addresses).\n5.2.1.1\nData Collection\nThe data collection is one of the most challenging tasks with the goal of providing valid data. Since\nmost of the publicly available datasets are either outdated, synthetic, or unrealistic, a new dataset is\nconsidered useful (see section 5.1.1). Therefore, real-world data from a productive university network\nis collected.\nNetwork Architecture\nThe selected campus network is a commonly used network structure, namely\na core distribution access model. The network connects approximately 30 buildings. Each building\nrepresents an element of the distribution layer, which is connected to the core routers (see ﬁgure 5.6).\nThe network connects sub-nets: The data center, laboratories, administration, research facilities and\nWiFi. The network provides access to approximately 1 000 staﬀmembers and 10 000 students. Essential\nIT services, such as mail or Domain Name System (DNS) services are provided by the university data\ncenter. The presented network structure model allows for the collection of network ﬂow data at central\npoints (at the two core routers: CatE and CatM). Thus, the resulting dataset consists of realistic\nsamples of real-world network traﬃc. However, ﬂows that are routed within one and the same building\ncannot be included into the analysis.\nInternet\nCore\nDistribution\nBuilding \nData Center\nAccess\nCatM\nCatE\n...\n...\nNetFlow\nCollector\nFigure 5.6: Infrastructure model of the campus network in which ﬂow data is collected.\nA NetFlow collector is used for the collection of the ﬂow data. It is integrated into the Flow Data\nStream Server and allows for the reception of exported ﬂow data from the two primary routers in the\nCisco Flexible NetFlow format (Cisco 2008; Claise 2004).\npage 44\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nFlow Data Stream Pipeline\nFlow Export\nAs previously mentioned, the two core routers continuously export the ﬂow data\nvia the NetFlow protocol. Thus, it is possible to collect realistic network traﬃc within the campus\nnetwork structure. Individual packages of a ﬂow are identiﬁed by matching criteria. The matching\ncriteria comprise all elements of the 5-tuple. This includes the information if a ﬂow is complete (TCP\nconnection terminated, Claise 2004) and exported. If this is the case, additional meta-data is provided.\nThe so-called collection criteria include timestamps for the start and end of each ﬂow, the number\nof packets and bytes transferred, and, if present, the TCP ﬂags. Moreover, two available timeouts\nare used to trigger a ﬂow export. If no packets are forwarded for a ﬂow within a certain period, the\ninactive timeout (e.g., 30 s) triggers the export of the ﬂow. An active timeout (in this case 600 s) is\nresponsible for longer lasting ﬂows. The latter may lead to incomplete ﬂow exports. A device’s cache\nsize, however, may limit the maximum of the currently traced ﬂow records. Due to the cache size\nlimitation and timeouts, exported ﬂows may not describe a complete communication. Accordingly,\nunﬁnished ﬂows are referred to as ﬂow records. Completed ﬂows are labeled as ﬂow entries.\nThe export of the corresponding devices is conﬁgured for incoming traﬃc only. This setup prevents\nﬂows from being exported twice by one component. However, ﬂows that are routed through both\ncomponents (switches) can still be exported twice. Only IPv4 and unicast ﬂows are investigated, as\nthe amount of other traﬃc (e.g., multicasts or broadcasts) is very low in this network. Disregarding\nthe diﬀerences between daytime and nighttime leads to an average of about 2 000 processed ﬂows per\nsecond.\n5.2.1.2\nServer Side Data Preparation\nBefore the exported ﬂows can be processed by the ML model, they have to be prepared. Parameter-\nindependent pre-processing steps are implemented in a server application. As a ﬁrst step, ﬂows are\ncollected until a certain number (100 000) is reached. The collection of 100 000 ﬂows is referred to as\nblock. In case a block is completed, the ﬂow records are pre-processed in the following steps: Data\naggregation, enrichment and anonymization.\nData Aggregation\nDue to hardware limitations (timeouts and cache sizes), potentially non-\ncompleted ﬂow records are exported.\nFlow records need to be combined into ﬂow entries prior\nto further processing steps. This is performed individually for each block (100 000 ﬂow records). The\nmeta-data of each ﬂow (5-tuple and timestamp, as well as ﬂags) are used for this purpose. The goal of\nthis procedure is to recalculate the meta-data such as duration, number of packets and bytes, as well\nas the bit rate of completed ﬂows (ﬂow entries). The disadvantage of this method is that long-lasting\nﬂows crossing the boundaries of a block are dropped. Approximately 2 500 ﬂows per block are excluded.\nAt the same time, the aggregation step removes duplicate entries of both exporters (∅≈4 200 per\nblock). This mechanism reduces the number of ﬂows to approximately 75% within a block.\nData Enrichment\nA data enrichment step is performed in order to add internal and external\ninformation to the ﬂows. This allows, for example, the addition of a private or public preﬁx to an IP\naddress. As an example of internal information, the Virtual Local Area Network (VLAN) tag is added.\nThis is realized by using a lookup table and the address preﬁx of the ﬂow. Using the same principle, the\nAutonomous System Number (ASN) tag is added among others. A lookup service (MaxMind 2019) is\nused to add external (public) information. As a result, more information related to a communication’s\ncontext and level can be added (e.g., the country).\nData Anonymization\nData anonymity is an important aspect in the present real-world scenario.\nFirst of all, the processing of personal data is sensitive. The collected IP addresses are considered as\npersonal data, as they are assigned to a person at least for a certain period of time. Second, personal\ndata should be protected before the dataset can be published. Furthermore, the internal network\nstructure should not be revealed for security reasons.\nThe anonymization of IP addresses is realized by parameterized substitution tables. A cryptograph-\nically hashed password (seed), which is deﬁned by the data center administrators of the university, is\nused for this purpose. An individual substitution table is applied to each octet of an IP address. Thus,\nthe semantics of the addresses remain the same despite their adjacency. Additionally, the relation to\nthe network address remains, even though the neighboring addresses are separated. This is the last\nserver-side pre-processing step before the data is transferred to a client.\nBenedikt Pfülb\npage 45\n\nFlow Data Stream Pipeline\nExemplary Real-World Scenario\n5.2.2\nFlow Data Stream Client\nThe second part of the Flow Data Stream Pipeline (see ﬁgure 5.5) is implemented as a client. Hence,\nthe client receives the pre-processed data from the server application. The client is responsible for\nthe parameterized part of the data processing steps (section 5.2.2.1) and the machine learning stage\n(section 5.2.2.2).\n5.2.2.1\nClient Side Data Preparation\nBefore the data ﬂows into the machine learning model, further pre-processing steps need to be performed.\nSome of them are optional. In this work, optional means that several diﬀerent parameter conﬁgurations\nare used for the investigation.\nData Filtering\nThe (optional) ﬁlter mechanism allows to exclude ﬂows that match one or more ﬁlter\ncriteria. These may represent sub-problems, which might be interesting for a corresponding real-world\napplication. This allows, for example, the exclusion of User Datagram Protocol (UDP) traﬃc, so that\nonly TCP ﬂows are learned and predicted.\nData Selection\nThe (optional) data selection stage reduces the number of features. This constitutes\na contrast to the enrichment stage, but supports the investigation of an individual feature’s inﬂuence.\nThus, the quality improvement due to the enrichment process (5-tuple vs. all additional features)\nbecomes measurable.\nData Labeling\nTarget values (labels) are required for supervised machine learning (see section 2.2).\nIn this study, most of the target values for the routing scenario are real-valued numbers. Due to the\nimbalance of data, the implementation of a regression problem is challenging. Therefore, the regression\nproblem is transformed into a classiﬁcation problem.\nAs part of the data labeling, each ﬂow is assigned to a speciﬁc class by means of parameterizable\npredeﬁned class boundaries. The selection of class boundaries is described in section 5.3.1. The\nfollowing labels are useful for the given routing scenario: number of bytes or packets, a ﬂow’s duration\nor bit rate. Due to the assignment of class labels, a critical discussion of the ground truth of data (see\nsection 6.1.1) is required. Nevertheless, an approximately equal distribution of the ﬂows within the\nclass is possible (and recommendable, see section 2.2.2.5). All in all, the routing problem shown in\nsection 5.1 can be addressed by the classiﬁcation of ﬂows. The result of the data labeling is a one-hot\nencoded (see section 2.2) class label.\nData Normalization\nBefore the ML model can be fed with ﬂows, two last pre-processing steps are\nperformed: The normalization of value ranges and a feature encoding. The normalization is supposed\nto ensure that no extreme gradients arise and that a convergence of the model is achieved more quickly.\nMoreover, the appropriate encoding should help ensure the independence of features (LeCun, Bottou,\net al. 2012; Bishop 1995).\nWith regard to the coding, 3 diﬀerent formats are oﬀered: ﬂoat, bit pattern, one-hot (e.g., table 5.1).\nSimple ﬂoating point values are transformed into a given range of values ([0.0, 1.0]) by a min-max\nnormalization. This can be realized easily, as the value ranges for all features are known. The bit\npattern format is used to convert real values into their binary representation. It is important to know\nthe range of values the numbers can assume in advance (e.g., for IP addresses). Categorical values are\nrepresented by means of one-hot encoding (also known as 1-of-c coding). This avoids proximity, which\nwould be the case with continuous variables. In addition to these standardized transformations of the\ndata, problem-speciﬁc modiﬁcations are executed. The replacement of dynamic port numbers (≥215)\nby 0 belongs to this data-speciﬁc modiﬁcation.\nTable 5.1: Flow feature normalization and encoding examples.\nFeature\nRaw Data\nData Type Output Data\nIP address 81.169.238.182 Float\n0.317, 0.662, 0.933, 0.713\nProtocol\n6\nBit pattern 0, 0, 0, 0, 0, 1, 1, 0\nLocality\nPrivate | Public One-hot\n0, 1 | 1, 0\npage 46\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nFlow Data Stream Pipeline\nTable 5.2 represents the possible ﬂow features. Additionally, the three data formats plus the size of\noutput vectors for the individual features are included into the table. The used formats for the analyses\nand experiments are highlighted in gray. Features marked with the symbol ⇄do exist twice in the\ndata. In the table, they are used to describe the source (src) and the destination (dst).\nTable 5.2: Details of the ﬂow features in the data stream.\nFeature\nData Format\nSrc Feature\nData Format\nSrc\nFloat\nBit\nOne-hot\nFloat\nBit\nOne-hot\nmonth\n1\n4\n12\nData Collection\nnetwork\n⇄\n4\n32\n\u0017\nData Enrichment\nday\n1\n5\n31\npreﬁx len\n⇄\n1\n5\n\u0017\nhour\n1\n5\n24\nASN\n⇄\n1\n16\n\u0017\nminute\n1\n6\n60\nlongitude\n⇄\n1\n\u0017\n\u0017\nsecond\n1\n6\n60\nlatitude\n⇄\n1\n\u0017\n\u0017\nprotocol\n1\n8\n\u0017\ncountry code ⇄\n1\n8\n240\nIP address\n⇄\n4\n32\n\u0017\nVLAN\n⇄\n1\n12\n\u0017\nport\n⇄\n1\n16\n\u0017\nlocality\n⇄\n1\n1\n2\n5.2.2.2\nMachine Learning\nThe machine learning module is responsible for the training and inference process of a given model.\nThe concrete ML model can be exchanged easily due to an interface. This allows for a quick and easy\nimplementation of diﬀerent models, which might be an interesting approach for further research. In\nthe present work, DNNs as a type of ML model are used for network ﬂow characteristic prediction.\nFirst of all, a block of data is provided to the ML module, which consists of a chronologically\nordered set of ﬂow entries. In order to perform an evaluation, the block is divided into 90 percent\ntraining and 10 percent test data. The chronological order of the ﬂow entries before the division is\npreserved in order to allow an approximately realistic evaluation. Thus, the test data are part of the\nfuture, as far as the model is concerned. The training on a data block takes place until a new block\narrives, which depends on the speed of the exported ﬂows. The prediction output of the model depends\non the selected label (see section 5.2.2.1).\nDepending on the class boundaries, a range for the classiﬁcation of ﬂows (e.g., duration or bit rate)\nis speciﬁed. A detailed speciﬁcation of the used ML model (i.e., DNNs) is presented in conjunction\nwith the experiments (see section 5.4).\n5.2.3\nData Flow of Network Flow Data\nThe block diagram in ﬁgure 5.7 summarizes the ﬂow of data through the individual stages of the\nFlow Data Stream Pipeline (see ﬁgure 5.5). Network ﬂows exported by multiple switches/routers are\ncollected centrally in the Flow Data Stream Server by the NetFlow Collector unit.\nThis part of the application is based on network sockets, which are used to transmit or receive the\nﬂow data\n. Flows are collected until a certain number is reached. The collection is referred to as\nblock, whereas a block size of 100 000 is chosen. This approximately equals 1 min if many ﬂows are\nreceived. A block (see section 5.2.1.2) consists of ﬂow records, which represent potentially unﬁnished\nnetwork ﬂows.\nAs a next step, the block is forwarded to the Flow Processor\n. It divides the block into several\napproximately equal chunks based on the 5-tuple\n. Thus, corresponding ﬂow records are placed in the\nsame chunk. The number of chunks n depends on the number of ﬂow processors. These, in turn, are\nimplemented as independent processes in order to take advantage of multi-core processors\n. Thereby,\nthe complex processing steps become scalable and ensure that processing ends before a new block is\navailable.\nEach of the Flow Processors performs the same three processing steps\n. First of all, an aggregation\nis executed so that the potential ﬂow records are converted into ﬂow entries. In this context, the 5-tuple\nis used to aggregate multiple ﬂow records into a single ﬂow entry by evaluating timers and ﬂags. This\nis how the meta-data can be recalculated, e.g., the duration. The second step is the data enrichment.\nBenedikt Pfülb\npage 47\n\nFlow Data Stream Pipeline\nExemplary Real-World Scenario\nBoth, internal and external information resources are used for this purpose. As an example for internal\nadditional information, the network preﬁx is added by using the IP address and a lookup table. It is\nalso possible to determine the VLAN tag. The latter helps separate a physical network into several\nlogical networks, e.g., data center, WiFi, administration network. A lookup service is used to add\nfurther external information (MaxMind 2019). Additional external information includes, for example,\nthe ASN or the country code (see table 5.2). The download of the provided database enables very\nshort query times.\nThe third and ﬁnal pre-processing step of the Flow Processors is anonymization. Accordingly,\npersonal data, i.e., IP and network addresses, are masked. An individual substitution table is available\nfor every single octet of an address. The table is initialized based on a password. Thus, the semantics\nof an address remains while the adjacency is dissolved.\nThe results of the ﬂow processors are chunks of ﬂow entries\n. They are choronologically merged\nback into a block. The block is then passed to the connection handler\nand transmitted to all\nregistered Flow Data Stream Clients in a compressed form\n. It is optional to export the pre-processed\nblock as a dataset ﬁle\n.\nFlow Data Stream Server\nFlow\nProcessor\nCompressed Block\nFile Export\nFlow Entries\nFlow Records\nn Flow\nProcessors\naggregation\nanonymization\nenrichment\nFlow Data Stream Clients\nFlows\nDNN Inputs\nFlow\nProcessor\nFC-DNN\nClient\nTraining/Test Dataset\nm Flow\nProcessors\nfiltering\nlabeling\nnormalization\nselection\nConnection\nHandler\nExported Flows\nNetFlow\nCollector\nBlock \nFlow Records\nStream\nHandler\nDecompressed\nBlock\nFile Export\n \nBlock\nFlow\nEntries\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n6a\n13\nFigure 5.7: Data ﬂow diagram of network ﬂow data.\nAs illustrated in ﬁgure 5.7, the pre-processed compressed block is unpacked on the client side by the\nStream Handler and passed on to a client Flow Processor\n. The client’s Flow Processor divides the\nblock into chunks\n, again, to perform processing steps by m Flow Processors\n. The number of\nprocesses is adapted to the used hardware in order to minimize the pre-processing time. Filtering is the\nﬁrst optional step. It allows for the removal of individual ﬂow entries from the dataset based on ﬁlter\ncriteria. This is how a type of protocol, e.g., DNS, can be removed from the data. A second optional\nstep is the feature selection. By means of this mechanism, individual features can be removed from the\ndata. Likewise, the inﬂuence of the individual features on the prediction results can be investigated.\nRemoving all features except the 5-tuple, for example, helps measuring the inﬂuence of enrichment.\nAs a third step, the data is categorized into classes. Two decisions are required for this step. First\nof all, label criteria need to be selected. The used ﬂow data contains various meta-information (possible\nlabels) relevant for routing: duration, number of bytes or packets and the bit rate. Second, class\nboundaries have to be speciﬁed. At the current processing step, ﬂows are divided into classes based on\npredeﬁned boundaries. How to determine boundaries is explained in more detail in section 5.3.1.\nThe last processing step of the Flow Processors is normalization and encoding. The goal of this step is\nto specify the format of each feature (see section 5.2.2.1). At the same time, a min-max normalization is\napplied. All of the performed steps can be parameterized individually. Therefore, diﬀerent experiments\ncan be performed simultaneously. Each experiment is represented by an individually parameterized\nFlow Data Stream Client.\nAs a result, the Flow Processor receives several chunks of data, which are merged into one block\n.\nThe block is divided into training (90%) and test data (10%), while preserving the chronological order\nof the ﬂows\n. Thus, future data needs to be predicted by the ML model. This data-driven time\nshift merely simulates the real time diﬀerence of the prediction of a new ﬂow and the current state of\nthe model. A data block can be used until a new block of data is available. As shown in ﬁgure 5.7,\na DNN consisting of fully-connected artiﬁcial neurons (Fully-Connected (FC)-DNN) is used for the\nexperiments\n. At this point, both, the evaluation results and the pre-processed data (optional) can\npage 48\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nTraﬃc Analysis\nbe exported.\nAn advantage of this architecture is the full implementation of the parameterized part on the\nclient side. This way, several clients can simultaneously connect to the server. Thus, many diﬀerent\nexperiments can be performed. Another beneﬁt is that experiments can be repeated, assuming that\nthe pre-processed dataset can be stored.\n5.3\nTraﬃc Analysis\nBefore describing experiments with ﬂow data, the results of a data analysis are summarized. The\nentire dataset contains ≈480 million ﬂow entries which are distributed over 6 800 blocks. The dataset\nrepresents one week of network ﬂow data. Diﬀerent analyses are performed on a single extracted block\nof 100 000 ﬂow records, whereas blocks that are temporally close to each other produce similar results.\nIn order to use representative data, a block at 2:00 p.m. is selected. At that time, the peak of\nstudents on-campus (and within the network) is assumed. It has to be noted that the distribution\nwithin the collected blocks diﬀers depending on the daily rhythm or the days of the week. The\ncorresponding eﬀects are illustrated by the experiments later (see section 5.4). To help ensure the\nvalidity of the processing steps and the analyses, network ﬂows are manually generated, e.g., in form of\ndownloads, and traced back.\n5.3.1\nLabel-based Data Distribution\nThe ﬁrst step is to analyze the data distribution in relation to the most interesting labels, e.g., bit rate,\nduration, number of packets and number of bytes. To be precise, a single block of 100 000 ﬂows records\nis investigated. A histogram (25 bins, note the logarithmic scale) for each type of label is shown in\nﬁgure 5.8. As depicted in ﬁgure 5.8a, most ﬂows are very short while transferring very few bytes or\nconsisting of few packets (see ﬁgures 5.8c and 5.8d). This result indicates a low bit rate for most ﬂows\n(see ﬁgure 5.8b).\n0\n200\n400\n600\n100\n101\n102\n103\n104\n105\nfrequency\nMedian < 1\n(a) duration in seconds\n0\n50\n100\n150\n200\n100\n101\n102\n103\n104\n105\nfrequency\nMedian: 0.006\n(b) bit rate in Mbit/sec\n0\n20 M\n40 M\n60 M\n100\n101\n102\n103\n104\n105\nfrequency\nMedian: 156\n(c) number of bytes\n0\n40 k\n80 k 120 k 160 k\n100\n101\n102\n103\n104\n105\nfrequency\nMedian: 2\n(d) number of packets\nFigure 5.8: Histograms of a block for all ﬂow labels (log scale).\nThe histograms clearly present the uneven distribution based on the possible labels. The same result\nis implied by the median values given shown in ﬁgure 5.8. At this point, converting a regression\ninto a classiﬁcation problem reveals a challenge, namely the deﬁnition of the class boundaries. The\nchoice of class boundaries depends on the given problem, in this case it is the routing decisions. This\nproblem has been simpliﬁed in many related works (section 5.1.1) by making it a binary classiﬁcation\nproblem (distinction between “mice” and “elephant” ﬂows). In the present work, however, the data is\ndivided into three classes, even though the deﬁnition of speciﬁc boundaries remains a challenge. This\nis particularly due to the used ML model (here DNNs) and its diﬃculties with unbalanced data. Even\nif the chosen class boundaries are data-driven, it becomes obvious that a classiﬁcation with several\nclasses is possible.\nThe determination of class boundaries is realized as follows: First, the division into three classes is\nchosen in order to show that a ﬁne-grained classiﬁcation of ﬂows is feasible. Second, the uniform class\ndistribution, which is required by the used DNNs models, is increasingly diﬃcult to achieve with a\nhigher number of classes. In order to determine class borders, the complete dataset is exposed to an\nequal-depth frequency partitioning method (also known as equal frequency binning). The derived class\nboundaries allow for a maximum balance of the data distribution for the chosen label in the dataset.\nThis procedure is, however, only applicable in an oﬄine scenario where the data is available in advance.\nFor online variants, a block-wise adjustment of the boundaries is a better option. This oﬄine approach\nis, however, not problem-speciﬁc, but data-driven.\nBenedikt Pfülb\npage 49\n\nTraﬃc Analysis\nExemplary Real-World Scenario\nAn example of using the bit rate in bit/sec as a label is presented below. The class boundaries derived\nfrom the entire dataset are as follows: class 0 = [0, 4169[, class 1 = [4169, 12 288[, class 2 =[12 288, ∞].\nApplying these bounds results in the data distribution within the classes as shown in ﬁgure 5.9. The\nblack line illustrates the number of all ﬂows after pre-processing within the Flow Data Stream Server\n(see section 5.2.1). The reduction of 100 000 to about 75 percent is mainly due to the aggregation\n(see section 5.2.1.2). In addition, ﬂow records that are not completed within a block are removed.\nThe three colored lines in ﬁgure 5.9 indicate the data distribution for the three classes (red, green\nand blue). It is approximately the same for ≈6 800 blocks. The slightly unequal distribution can be\ncompensated more easily by a weighting method (e.g., by adjusting the learning rate) within the ML\nmodel. Alternatively, over- or under-sampling could be used as a balancing method.\n12-03\nTue\n12-04\nWed\n12-05\nThu\n12-06\nFri\n12-07\nSat\n12-08\nSun\n12-09\nMon\ndate\n0\n50 k\n100 k\nﬂow entries\nall\nclass 0\nclass 1\nclass 2\nFigure 5.9: Class distribution for all blocks of the dataset.\n5.3.2\nStructural Data Patterns\nRecognizing structures in unknown data is diﬃcult. This is particularly true if their properties cannot\nbe represented in a visual manner. In the present scenario, the 5-tuple is one of them. In order to\nvisualize the structures within the data, t-distributed Stochastic Neighbor Embedding (t-SNE) (Maaten\nand Hinton 2008) is used. Based on the Euclidean distance, t-SNE reduces high-dimensional data to\ntwo-dimensional points. This procedure tries to preserve the neighborhood of the individual 2D data\npoints. Therefore, the absolute position of individual points is meaningless.\nThe runtime behavior (O(n2)) of the standard method does not allow to reduce the dimension\nof all data points in an adequate time. The same is true for an entire block or online visualization.\nTherefore, an optimized tree-based approximation proposed by Maaten (2014) is used to process 10 000\nﬂows. For a better understanding of the correlations for one and the same t-SNE output within the\ndata, diﬀerent properties are tagged. For the application of t-SNE, 2 000 iterations are performed at a\nperplexity of 150.\nThe tags in ﬁgure 5.10 refer to the used transport protocol. Flows belonging to UDP ( ) are\ndisproportionately frequent (85.5 %). Only 13.5 % of 100 000 ﬂows are part of the TCP traﬃc ( ).\nThe remaining ﬂows take about 1.0 % ( ) including, for example, ICMP. Figure 5.10 clearly illustrates\nthe symmetric separations emerging between the used protocols.\nTCP\nUDP\nother\nFigure 5.10: Transport protocol-based network ﬂow tagging of the t-SNE results.\nAnother type of tagging that refers to the locality of a communication is presented in ﬁgure 5.11. The\nIP is used to distinguish between local and public addresses. In ﬁgure 5.11, the outward and return\ndirection becomes visible. Only 3.4 % of the communication takes place exclusively between local\nsystems ( ). This is due to the network architecture (core-distribution-access model, see section 5.2.1.1),\nwhich also means that ﬂows within buildings cannot be captured. That is the reason why the number\npage 50\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nFlow Prediction Experiments\nof exclusively internal ﬂows is lower.\nThe largest proportion of ﬂows (36.6 %\nrespectively 34.8 %\n) is constituted by communication\nwith one public system (Internet traﬃc). Since the data center oﬀers IT services with public addresses,\nthe respective number of ﬂows is considerably high with 25.2 % ( ). The direction of communication\nis clearly visible through the symmetry. This fact is strengthened by the highlighted WiFi (46.3 %)\ntraﬃc.\npublic/public\npublic/private\nprivate/public\nprivate/private\nFigure 5.11: Locality-based network ﬂow tagging of the t-SNE results.\nConsidering the application protocol, it is clear that the majority (79.8 %) of ﬂows are assigned to\nDNS ( ). 13.2 % are HTTP ﬂows ( ) and the remaining 7.0 % are assigned to other application\nprotocols ( ). The high proportion of DNS traﬃc is due to two facts. First, almost every type of\nnetwork communication requires address resolution. A web page request, for example, does not only\ntrigger a single name resolution, but any dynamically content that is loaded. The second reason is\nthe structure of the DNS resolver arrangement. Resolving an address comprises the request of an\ninternal DNS server which, if necessary, forwards the request to an external server. Both ﬂows are\ncaptured, causing an increase in DNS traﬃc. The present analysis reinforces the statement concerning\nthe uneven distribution of data presented in section 5.3.1. One of the reasons is that DNS requests\ncomprise very few packets. This results in the highly unequal distribution in terms of duration and\ndata volume.\nHTTP(S)\nDNS\nother\nFigure 5.12: Application-based network ﬂow tagging of the t-SNE results.\n5.4\nFlow Prediction Experiments\nIn this section, the conducted ﬂow data experiments are presented. As described in section 5.4.1, a\nDNN architecture for the used model needs to be determined ﬁrst. Since the DNN architecture has to\nbe deﬁned in advance, a parameter search is performed on a small portion of the collected data. Based\non these results, further experiments related to diﬀerent contexts within the network ﬂow data are\nperformed (stated in section 5.4.3). As a next step, experiments are conducted with the full dataset.\nIt consists of one week of ﬂow data.\n5.4.1\nDeep Neural Network Architectural Experiments\nThe research approach is based on DNNs, as applied in other related works (see section 5.1.1). Many\nproperties of DNNs cause them to be interesting for ﬂow prediction application scenarios. This includes\nthe ability to process batches of data with a constant runtime O(1) for both, training and inference.\nThis is in contrast to other models, such as Gaussian Process Regression (GPR). For the latter, a\nBenedikt Pfülb\npage 51\n\nFlow Prediction Experiments\nExemplary Real-World Scenario\nruntime behavior of O(n3) is expected without further optimization (Poupart, Chen, et al. 2016).\nEven though approximations or heuristics may be able to improve the runtime behavior, additional\nhyper-parameters are introduced (Rasmussen and Williams 2006).\nIn general, the comparatively good performance of DNNs for higher dimensional data is crucial.\nAdditionally, DNNs are suitable for the processing of inﬁnitely long data streams. This is due to\ntheir ability to train/learn incrementally (see section 2.3.2). Other methods, such as vanilla Support\nVector Machines (SVMs), are not applicable as they lack the incremental learning capability. Some\nproperties of DNNs are, however, disadvantageous. These include, for example, the diﬃculties related\nto unbalanced data distributions in regression and classiﬁcation problems. Although various techniques\ncan mitigate the eﬀects, the method remains vulnerable.\nHyper-Parameter Grid Search\nBefore a detailed investigation of the data can be executed, the\narchitecture of the DNNs needs to be deﬁned. For this purpose, a grid-search (see section 4.3) is\nperformed on a subset of the collected data. The subset consists of 10 blocks (each about 75 000 ﬂow\nrecords), which are pre-processed by the Flow Data Stream Pipeline. In addition to the learning rate\nϵ, the standard DNN hyper-parameters are varied. The latter include the number of layers L and the\ncontained artiﬁcial neurons S. More complex functions can be approximated, if DNNs are deeper and\nhave more neurons. However, larger DNNs increase the probability of the data being learned by heart,\nwhich is known as overﬁtting eﬀect (see section 2.2.2.5).\nA commonly used technique for the avoidance of overﬁtting is the application of dropout (Hinton,\nSrivastava, et al. 2012b). Dropout speciﬁes that input signals are randomly passed (or set to 0) to the\nnext layer based on a probability. One probability is deﬁned for the input layer di and one for the\nhidden layers dh (a factor of 1.0 corresponds to dropout being disabled). The application of dropout is\nsupposed to result in an improved generalizability of DNNs.\nIn order to address the unbalanced data problem, two class balancing methods W are applied.\nThis includes the standard method of proportionally adjusting the learning rate for a class, as well as\nunder-sampling. The latter leads to the reduction of the samples in all classes, so that all of them\nmaintain an equal number. The class with the fewest samples serves as a basis.\nThe ﬂow data are used with and without the enrichment (indicated by F). The goal is to avoid\nthat the enrichment inﬂuences the choice of a network architecture. Table 5.3 summarizes the available\nvalues for the hyper-parameters. Combining the hyper-parameters results in the performance of 5 400\nexperiments. The last step is the determination of the best DNN architecture.\nTable 5.3: Overview of the varied hyper-parameters for the initial DNN architecture grid-search.\nParameter\nVariable\nValues\nDropout (input, hidden) (di, dh)\n{(1.0, 1.0), (0.9, 0.6), (0.8, 0.5)}\nLayers\nL\n{3, 4, 5}\nNeurons per layer\nS\n{200, 400, 600, 800, 1 000, 1 500}\nLearning Rate\nϵ\n{0.01, 0.001, 0.0001}\nFeatures\nF\n{5-tuple, all}\nClass balancing method\nW\n{0 (under-sampling),\n1 (class weighting)}\nIn addition to the tuning of hyper-parameters, the DNN structure is deﬁned as follows. For the\nexperiments, fully-connected DNNs are used. In contrast to sparsely connected models, the complex\npruning of DNNs can be omitted. Rectiﬁer Linear Units (ReLUs) are used (see section 2.2.1.2) as\nan output function fϕ. Weights are updated by the Adam optimizer (see section 2.2.2.4). Cross-\nEntropy (CE) loss with softmax is optimized (see section 2.2.2.1). In this application context, the\nexamination of the bit rate only is used as an initial label. It is assumed that this value is the most\ninteresting for potential applications. Both, the batch size B=100 and the number of training epochs\nE =10 are ﬁxed for each training iteration and block. Performance in terms of accuracy is measured\nafter every 50th training iteration. For testing purposes, each block is divided into 90 % training and\n10 % test data while adhering to the chronological order. Accordingly, a complete epoch is evaluated\non the test data.\npage 52\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nFlow Prediction Experiments\nResult of the Architectural Experiments\nThe goal of the grid-search is to determine an\nDNN architecture with a reasonable performance. The respective architecture should be used to\nexecute experiments on the entire dataset in a subsequent step. The collected accuracy values at the\nmeasurement points constitute the basis for the evaluation of the 5 400 experiments. The maximum\nmeasured accuracy within an experiment is used as a metric to determine the best architecture.\nFor long-term experiments, this criterion is questionable, but suﬃcient to derive an adequate DNN\narchitecture. The maximum measured accuracy is 86.7 %, if all features are included in the training\nprocess. Without the enrichment (using only the 5-tuple), the maximum is about 2 % lower. Table 5.4\nshows the hyper-parameters used for the best experiments.\nTable 5.4: Results of the architectural hyper-parameters experiments.\nFeatures FLayers LSizes S Learning Rate ϵDropout Probability (di, dh)Weighting W\nall\n3\n1 000\n0.0001\n(1.0, 1.0)\n1\n5-tuple\n5\n1 000\n0.001\n(0.9, 0.6)\n0\n5.4.2\nConcept Drift/Shift Experiments\nMany problems do not need to be considered within a speciﬁed time interval or treated as CL problem.\nIn general, an ML model’s parameter conﬁguration is derived from the training data. Then it is\nﬁne-tuned and deployed, e.g., to recognize faces in images. The question arises whether this approach\ncan be applied to the prediction of network ﬂow data. Accordingly, conducting the following experiment\naims at detecting a changing data distribution (see ﬁgure 5.13) in this scenario.\nIn order to investigate the changing distribution, an ML model is trained on a single block for 20\nepochs (blue line). Subsequently, the quality of the model is measured at regular intervals (each 50th\nblock, red line). The model remains unchanged after the initial training.\n0\n1\n2\n4\n6\n8\n10\n12\n14\n16\n18\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntest accuracy\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n550\nblock\nepoch\ntrain and test\ntest only\nFigure 5.13: Concept drift/shift experiment.\nThe diﬀerent test accuracies of blocks reveals that the data change over time (concept drift and/or\nshift, see section 2.3.1). It should be noted that a three class problem, as well as an equal distribution\nwithin the class, has a 33 % guessing chance. This limit is almost reached for block number 150 and\n200, which indicated that the model does not ﬁt the data anymore. For the subsequent blocks, e.g.,\nblock number 350, similar accuracies are achieved as during training. The following conclusion for\nfurther experiments can be drawn: The training should be conducted incrementally for each incoming\nblock in order to counteract the changing data distribution.\n5.4.3\nStreaming Experiments\nNew challenges arise for a wider investigation of diﬀerent parameters and contexts. Other useful\ncommunication contexts are enabled by the enrichment of the data, among other things. Considering\nthem leads to 10 possible combinations. At the same time, 4 labels can be assigned for each context:\nBytes, packages, duration and bit rate. The inﬂuence of the used features or feature groups is signiﬁcant\n(6). To conclude, 240 experiments can be performed on the dataset with the resulting architecture\ndescribed in 5.4.1. On a single machine, this amount would take 4.6 years of computation time.\nBenedikt Pfülb\npage 53\n\nFlow Prediction Experiments\nExemplary Real-World Scenario\nSince each experiment requires exactly one week of computing time, the processing is performed in\nparallel. The used hardware components are speciﬁed in table 4.1. Diﬀerent hardware speciﬁcations\n(number and clocking of CUDA cores), however, cause diﬃculties when comparing parallel online\nexperiments. These diﬀerences would lead to more training iterations on faster hardware. Figure 5.14\nrepresents the comparison between the number of training iterations/epochs. The slower Nvidia\nQuadro P620 is depicted in green and the faster Nvidia RTX 2080 in blue. The day-night rhythm also\nbecomes visible with signiﬁcantly fewer ﬂows at night, and more training iterations per block. During\ndaytime it is vice versa.\n12-03\nTue\n12-04\nWed\n12-05\nThu\n12-06\nFri\n12-07\nSat\n12-08\nSun\n12-09\nMon\ndate\n0\n100\ntraining epochs\nmin = 3\nmin = 9\nQuadro P620\nRTX 2080\nFigure 5.14: Training epochs comparison of slowest and fastest GPU.\nThe number of training iterations with the slowest available hardware is recorded ﬁrst. It is used\nsubsequently for all further experiments. This choice is supposed to rule out the advantages of the\nfaster hardware components. The factor between the slowest and fastest hardware amounts to the\nconstant number 2.75. Comparing the high number of training iterations to the lower number of\niterations, a small impact is indicated. The diﬀerence between the faster and slower hardware causes a\nminor diﬀerence of 0.7 percent accuracy on average.\nAs shown in section 5.3.1, the best possible class boundaries are determined for each context prior to\nthe experiments. For this purpose, the data must be available as an oﬄine dataset. As a result of this\npre-processing step, the boundaries in table 5.5 are derived for the assignment of class labels. Only the\nintermediate values are given, where the lower limit is 0 and the upper limited is ∞. These limits for\nall contexts and labels cause the distribution of data within the dataset to be less application-oriented\n(also see section 5.3.1). Nevertheless, the distribution within the classes is more even and therefore\nbeneﬁcial for the training of DNNs.\nTable 5.5: Context-speciﬁc label boundaries for ﬂow streaming experiments.\nCommunication\nContext\nLabel Boundaries\nKBytes\nPackets\nDuration (s)\nBit Rate (Kbit/s)\nall contexts\n≤\n0.11\n≤\n0.35\n≤≤\n1\n≤\n3\n≤≤\n0.14\n≤\n0.53\n≤≤\n4.1\n≤\n12.2\n≤\nonly WiFi\n0.15 0.45\n2\n3\n0.21 1.67\n4.2 10.8\nexclude WiFi\n0.09 0.33\n1\n2\n0.11 0.24\n4.1 12.9\nexclude DNS\n0.51 2.94\n5 12\n0.18 2.38\n3.5 24.1\nonly TCP\n1.50 4.49\n9 15\n0.25 4.92\n6.3 45.8\nonly UDP\n0.09 0.17\n1\n2\n0.12 0.57\n4.0\n9.7\nmulti-packet ﬂows\n0.28 1.41\n2\n8\n0.21 1.39\n5.6 20.8\npublic/public\n0.09 0.28\n1\n2\n0.11 0.23\n4.4 12.0\nprivate/private\n0.09 0.44\n1\n4\n0.10 0.28\n3.9 14.4\nprivate/public\n0.01 0.50\n1\n3\n0.17 0.92\n4.0 12.8\nIn addition to considering the contexts, the impact of individual features is examined. In order to\nminimize the number of performed experiments, they are divided into groups. The various groups along\nwith their number of features are represented in table 5.6. For the investigation of the experiments,\nthe Adam optimizer is not used for the results presented in the following. Instead, a plain Stochastic\nGradient Descent (SGD) is applied. During the ﬁrst experiments, several side eﬀects were observed,\nwhich is why the optimizer was changed. Figure 5.15 presents the measured accuracies of an entire\nexperiment. Moreover, the learning rate is speciﬁed. It becomes clear that strong ﬂuctuations occur\nduring the experiment when Adam is used (blue line). Sometimes, after several hundreds of processed\nblocks, the training process stabilizes. This eﬀect was observed in almost all experiments and is\npage 54\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nFlow Prediction Experiments\nTable 5.6: Feature groups for the ﬂow data experiments.\nFeature Group\nFeatures in the Data Stream\nInputs\nall\neach ﬂow feature from table 5.2\n247\n5-tuple\nIP address ⇄, port ⇄, protocol, timestamp\n109\ninternal\nnetwork + preﬁx length ⇄, VLAN ⇄\n98\nexternal\nnetwork + preﬁx length ⇄, ASN ⇄,\n112\ncountry code ⇄, geo coordinates ⇄\n5-tuple + internal\n207\n5-tuple + external\n221\nindependent of time or respectively training iteration. However, the eﬀect cannot be reproduced by\nrepeating the experiment, as at least the ﬁrst and last block changes. Moreover, the accuracy within\nindividual blocks changes dramatically when using Adam. This is similar to the eﬀect of a too high\nlearning rate. The respective blocks, however, do not reveal any irregularity. An investigation regarding\nthe internal variables of the Adam optimizer remains without new ﬁndings. The present work does not\nfollow this thread any further.\n12-03\nTue\n12-04\nWed\n12-05\nThu\n12-06\nFri\n12-07\nSat\n12-08\nSun\n12-09\nMon\ndate\n0\n50\n100\naccuracy in %\nSGD 0.001\nSGD 0.0001\nAdam 0.0001\nFigure 5.15: Visualization of the optimizer’s side eﬀect (plain SGD vs. Adam).\nTable 5.7 (left) summarizes the results of the 240 one-week-lasting experiments. Combinations of\ncommunication context, used feature groups and the predicted label are distinguished. All collected\nmeasurement points are reduced per experiment to the maximum (white), the average (light gray)\nand the median (dark gray) of the accuracy. As a consequence, many details are lost as part of\nthe information compression. The trend, however, is still recognizable. Accordingly, the challenging\nprediction of the bit rate and the duration is clearly visible. The same is true for the prediction of\nthe number of transmitted packages and bytes, even though it is easier to predict. The right part of\ntable 5.7 illustrates the proportions of the distribution within the collected ﬂows. To be precise, the\npercentage of TCP, UDP and, in particular, DNS ﬂows is presented. In addition, the median of the\nfour potential labels is stated. The reason for appropriately selected class boundaries (see table 5.5)\nbecomes obvious: Smaller median values illustrate an uneven distribution of target values.\nIn the following, eight individual experiments are selected. All of them represent the trend for\nthe entire week (see section 5.4.3). Only TCP ﬂows are selected as context for all experiments. Each\nsub-ﬁgure of section 5.4.3 illustrates two experiments by using all features (blue) and the 5-tuple as\ninput features (orange). During the training on a single block, the accuracy is measured repeatedly\nafter 50 training iterations.\nIn order to visualize the results, all test values measured on a block have to be combined. For this\npurpose, the maximum accuracy value is used for the representation of a block. The day and night\nrhythm is evident, while the performance improves at night. This is due to an increased variability\nof ﬂows during the day, as more students are present on the campus. The same applies for the\nweekend. Enrichment further inﬂuences the result, which is clearly visible in section 5.4.3. All of the\nvisualizations indicate the following ranking. Using only the 5-tuple (orange) hardly ever leads to\nbetter results than using all features (blue). The lower limit supports this conclusion, especially during\nworking hours. The horizontal line in the ﬁgures, which represents the average of the measured values,\nindicates a relatively small improvement of about 2 %.\nBenedikt Pfülb\npage 55\n\nFlow Prediction Experiments\nExemplary Real-World Scenario\nTable 5.7: Overview of the ﬂow prediction experiment results (left) and ﬂow volume metrics (right) for\neach context.\nCommunication\nContext\nAccuracy for Feature Groups and Labels in %\nFlow Volume\nMetrics (Median)\nall\n5-tuple\ninternal\nexternal\n5-tuple + internal5-tuple + external\nBytes\nPackets\nDuration\nBit Rate\nBytes\nPackets\nDuration\nBit Rate\nByte\nPackets\nDuration\nBit Rate\nBytes\nPackets\nDuration\nBit Rate\nBytes\nPackets\nDuration\nBit Rate\nBytes\nPackets\nDuration\nBit Rate\nProportion\n(%)\nKBytes\nPackets\nDuration (s)\nBit\nRate\n(Kbit/s)\nall\ncontexts\nmax\n100\n100\n90\n93 100 100\n90\n93\n98\n99\n99\n98\n98\n99\n95\n91 100 100\n90\n94 100 100\n90\n92\nTCP 21 2.6 12 1.0 14.9\nmean\nalso 96\n98\n67\n74\n95\n97\n67\n73\n85\n88\n61\n67\n85\n88\n61\n67\n95\n97\n67\n73\n95\n97\n67\n74\nUDP 79 0.1\n1 0.2\n6.1\nmedian\n97\n98\n68\n75\n95\n98\n68\n73\n85\n88\n61\n67\n85\n88\n61\n67\n95\n98\n68\n74\n95\n98\n68\n74\nDNS 71 0.1\n1 0.2\n6.0\nonly\nWiFi\nmax\n100\n100\n96\n95 100 100\n94\n95\n98 100\n93\n95\n98 100\n97\n95 100 100\n96\n95 100 100\n94\n95\nTCP 24 3.3 13 3.3\n9.0\nmean\n96\n99\n67\n69\n94\n98\n64\n66\n76\n81\n58\n62\n76\n81\n59\n62\n94\n98\n65\n68\n94\n98\n66\n68\nUDP 75 0.2\n2 0.2\n6.2\nmedian\n97\n99\n69\n69\n95\n98\n66\n66\n82\n86\n60\n62\n82\n86\n60\n62\n95\n98\n67\n68\n95\n98\n68\n68\nDNS 73 0.2\n2 0.2\n5.9\nexclude\nWiFi\nmax\n100\n100\n94\n96 100 100\n93\n95\n98\n99\n88\n90\n98\n99\n88\n92 100 100\n93\n96 100 100\n93\n96\nTCP 19 2.2 11 0.8 17.7\nmean\n97\n98\n71\n77\n96\n98\n71\n76\n86\n88\n65\n68\n86\n88\n65\n69\n96\n98\n71\n76\n96\n98\n71\n76\nUDP 81 0.1\n1 0.1\n6.1\nmedian\n98\n99\n72\n77\n97\n99\n72\n76\n86\n89\n66\n69\n87\n89\n66\n69\n97\n99\n72\n76\n97\n99\n72\n76\nDNS 70 0.1\n1 0.1\n6.0\nexclude\nDNS\nmax\n96\n95\n91\n90\n95\n95\n90\n89\n93\n92\n88\n88\n93\n92\n88\n88\n95\n95\n91\n90\n95\n95\n90\n90\nTCP 70 2.6 12 1.0 14.9\nmean\n85\n84\n72\n72\n84\n82\n71\n71\n74\n71\n64\n65\n74\n71\n65\n65\n84\n82\n71\n71\n84\n82\n71\n71\nUDP 27 0.1\n1 0.1\n4.9\nmedian\n85\n84\n73\n73\n83\n81\n72\n71\n74\n70\n64\n65\n74\n71\n65\n65\n83\n81\n72\n71\n83\n82\n72\n72\nDNS\n0 0.0\n0 0.0\n0.0\nonly\nTCP\nmax\n95\n93\n87\n89\n94\n92\n87\n89\n92\n89\n83\n89\n93\n90\n83\n90\n94\n92\n87\n90\n94\n92\n87\n89\nTCP 100 2.6 12 1.0 14.9\nmean\n82\n78\n76\n72\n80\n76\n74\n71\n70\n65\n67\n65\n71\n65\n68\n65\n80\n76\n75\n71\n80\n76\n75\n71\nUDP\n0 0.0\n0 0.0\n0.0\nmedian\n80\n77\n76\n72\n79\n75\n74\n71\n70\n64\n67\n64\n70\n64\n67\n65\n79\n75\n75\n71\n79\n75\n75\n71\nDNS\n0 0.0\n0 0.0\n0.0\nonly\nUDP\nmax\n97\n100 100\n99\n97 100 100\n99\n96 100 100\n97\n97 100\n98\n97\n97 100 100\n99\n97 100 100 100\nTCP\n0 0.0\n0 0.0\n0.0\nmean\n75\n83\n56\n54\n75\n83\n56\n55\n71\n80\n54\n52\n71\n80\n54\n53\n75\n83\n56\n54\n75\n83\n56\n54\nUDP 100 0.1\n1 0.2\n6.1\nmedian\n74\n82\n56\n53\n74\n83\n56\n53\n71\n80\n54\n51\n71\n80\n54\n51\n74\n82\n56\n53\n74\n82\n56\n53\nDNS 90 0.1\n1 0.2\n6.2\nmulti-\npacket-\nﬂows\nmax\n98\n97\n93\n93\n97\n96\n92\n94\n95\n93\n90\n91\n94\n94\n91\n91\n97\n96\n92\n94\n97\n97\n93\n94\nTCP 40 2.6 12 1.1 16.5\nmean\n90\n91\n76\n76\n88\n89\n75\n74\n78\n78\n67\n67\n78\n78\n68\n68\n88\n89\n75\n74\n88\n89\n76\n75\nUDP 60 0.2\n2 0.2\n6.7\nmedian\n89\n91\n77\n76\n88\n89\n76\n74\n78\n77\n68\n67\n78\n78\n68\n67\n88\n89\n76\n75\n88\n89\n76\n75\nDNS 54 0.2\n2 0.3\n6.0\npublic/\npublic\nmax\n100\n100\n99\n98 100 100\n99\n98\n98 100\n96\n95\n99 100\n97\n95 100 100\n99\n98 100 100\n99\n98\nTCP 11 2.2 11 0.9 18.0\nmean\n97\n99\n78\n74\n96\n99\n78\n72\n86\n94\n72\n61\n87\n94\n73\n62\n96\n99\n78\n73\n96\n99\n78\n73\nUDP 88 0.1\n1 0.1\n6.2\nmedian\n98\n100\n79\n75\n97 100\n79\n73\n88\n95\n72\n61\n89\n95\n73\n62\n97 100\n79\n73\n97 100\n79\n74\nDNS 87 0.1\n1 0.1\n6.2\nprivate/\nprivate\nmax\n100\n100\n97\n98 100 100\n97\n97\n99\n99\n97\n97\n99\n99\n97\n97 100 100\n97\n98 100 100\n97\n98\nTCP 25 1.2\n7 0.2 29.0\nmean\n93\n96\n63\n76\n92\n95\n62\n76\n78\n80\n56\n68\n78\n80\n56\n68\n92\n95\n63\n76\n92\n95\n63\n76\nUDP 71 0.1\n1 0.1\n4.9\nmedian\n94\n97\n63\n77\n93\n96\n62\n76\n79\n80\n55\n68\n79\n80\n55\n68\n93\n96\n63\n76\n93\n96\n63\n76\nDNS\n7 0.7\n6 5.7\n2.9\nprivate/\npublic\nmax\n100\n100\n94\n94 100 100\n91\n93\n96\n99\n90\n91\n96\n99\n88\n91 100 100\n90\n97 100 100\n98\n94\nTCP 25 3.1 13 2.5 10.1\nmean\n95\n98\n68\n71\n94\n97\n67\n70\n82\n86\n59\n64\n82\n87\n59\n65\n94\n97\n68\n71\n94\n97\n68\n71\nUDP 74 0.2\n2 0.2\n6.1\nmedian\n96\n99\n70\n72\n95\n98\n69\n70\n83\n87\n60\n64\n83\n87\n60\n64\n95\n98\n69\n71\n95\n98\n69\n71\nDNS 71 0.2\n2 0.2\n6.0\n12-03\nTue\n12-04\nWed\n12-05\nThu\n12-06\nFri\n12-07\nSat\n12-08\nSun\n12-09\nMon\ndate\n0\n50\n100\naccuracy in %\nall features\nmean = 77.6\n5-tuple\nmean = 76.0\n(a) Packets\n12-03\nTue\n12-04\nWed\n12-05\nThu\n12-06\nFri\n12-07\nSat\n12-08\nSun\n12-09\nMon\ndate\n0\n50\n100\naccuracy in %\nall features\nmean = 81.7\n5-tuple\nmean = 80.0\n(b) Bytes\n12-03\nTue\n12-04\nWed\n12-05\nThu\n12-06\nFri\n12-07\nSat\n12-08\nSun\n12-09\nMon\ndate\n0\n50\n100\naccuracy in %\nall features\nmean = 75.7\n5-tuple\nmean = 74.4\n(c) Duration\n12-03\nTue\n12-04\nWed\n12-05\nThu\n12-06\nFri\n12-07\nSat\n12-08\nSun\n12-09\nMon\ndate\n0\n50\n100\naccuracy in %\nall features\nmean = 72.0\n5-tuple\nmean = 70.6\n(d) Bit rate\nFigure 5.16: Trend for eight selected experiments (only TCP ﬂows and diﬀerent labels).\n5.4.4\nDiscussion\nThis section discusses the real-world application scenario. For this purpose, the previously outlined\nscenario is brieﬂy summarized. An evaluation of the presented scenario in the context of CL will be\ndiscussed in the next section (see section 5.5).\nIn the current chapter, a real-world application scenario was presented. The scenario involves ML\ntechniques, i.e., DNNs, in order to predict meta-data of network ﬂows. The corresponding meta-data\ncomprise information of terminated network communications, e.g., source, destination, duration, or\ntransmitted bytes/packets. However, it is a fundamental problem of network communication that\nthese meta-data are unknown when a communication starts. Thus, meta-information cannot be used\npage 56\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nFlow Prediction Experiments\nin applications, e.g., for determining an optimal path through the network (routing). The use of ML\ntechniques should enable the prediction of a ﬂows’ meta-data.\nA supervised ML scenario is derived from this problem deﬁnition. Studying the real-world scenario\ninvolves a large-scale computer network. Thus, realistic data and requirements can be considered. In\norder to predict ﬂow meta-information, an ML model needs to be derived from representative data. For\nthis purpose, a step by step approach is presented and evaluated. Moreover, the collection of meta-data,\nthe pre-processing and training of an ML is described. Data analyses along with the empirical results\nshow that predicting ﬂows’ meta-data is a viable option with DNNs.\nGeneralizability of the Results\nThe ﬁrst aspect of the discussion concerns the generalizability of\nthe results. The implication is that a prediction of ﬂow meta-data is possible. An investigation of the\nproposed approach in other computer networks could address this aspect. But without the possibility\nto perform a comparable investigation, generalization is very diﬃcult.\nUnfortunately, real-world data of (productive) computer networks are often inaccessible. This is\ndue to the sensitivity of the data and the respective legal issues. At the same time, continuous service,\nwithout interference, must be ensured. Thus, access to comparable network data is challenging. This\nis also the reason why synthetically generated or outdated data are often used in the literature (see\nsection 5.1.1). Statements based on generated data, however, can be generalized even less. This is due\nto the fact that ML models can approximate very complex functions. However, generating functions\ncan never reproduce real-world conditions on a large scale.\nThe underlying network of the exemplary scenario seems to be suﬃciently complex to prevent a\nperfect classiﬁcation rate. This trend is particularly evident in the day and night rhythm. During\nthe day, prediction becomes much more diﬃcult, as more dynamics are created by staﬀmembers and\nstudents. Therefore, the approach can be considered as a valid procedure, even though the exact\nperformance depends on the particular scenario.\nQuality of Results\nThe obtained classiﬁcation accuracy is the second aspect up for discussion. In\ngeneral, an accuracy of 33 percent can be assumed as a baseline/chance for an evenly distributed\nthree-class problem. Therefore, a classiﬁcation rate above the guessed value is an improvement. Of\ncourse, the extraction of knowledge always depends on the quality of data itself. This real-world scenario\nis particularly interesting, because the labels for a supervised learning scenario are automatically\nprovided. Nevertheless, the obtained data is inﬂuenced by the network structure and the exporting\ndevices (hardware and software).\nHowever, the question whether the achieved accuracy is suﬃcient for real applications remains\nunanswered. In order to make a valid statement, further aspects need to be considered. First, an\napplication needs to be developed, which is based on the predictions. Second, measuring the impact\nof the application has to be possible. Achieving these criteria is very diﬃcult in productive network\nenvironments. A ﬁrst attempt to measure the inﬂuence of the prediction accuracy is presented by\nHardegen and Rieger (2020).\nOn average, the empirical experiments result in accuracies above 60 percent. However, this number\nstrongly depends on the chosen communication context. The temporal oﬀset of training and test data\nis assumed to be a representative, realistic oﬀset. Nevertheless, it can be inferred that a DNN is able\nto extract knowledge from the data. The possibility to learn knowledge from the data is supported\nby the recognized patterns as part of the conducted data analysis. Therefore, it is assumed that the\nprediction of network ﬂows’ meta-data is feasible with DNNs.\nApplicability of the Predictions\nThe deﬁnition of class boundaries is yet another aspect that\nrequires a discussion. As shown in section 5.3.1, network ﬂow data is very unevenly distributed with\nrespect to the labels. However, this unequal distribution can become problematic depending on the\nML model. Accordingly, DNNs are aﬀected.\nTo counteract this challenge, the available labels are used for a data-driven class division. The\nderived value ranges of the individual classes are, however, questionable. This in turn aﬀects the areas\nof application. As a consequence, the equal-depth frequency partitioning method is merely used to\ndemonstrate the feasibility.\nAnother issue requiring further research concerns the application of a replay buﬀer. This buﬀer\ncould include hold-out samples that are continuously updated and used for under-represented classes.\nBenedikt Pfülb\npage 57\n\nReal-World Continual Learning Requirements\nExemplary Real-World Scenario\nThus, application-oriented class boarders could be implemented without a data-driven analysis.\n5.5\nReal-World Continual Learning Requirements\nReal-world requirements for CL scenarios can be manifold. In this section, requirements derived from\nthe exemplary scenario are summarized. A collection of requirements will serve as an answer to the ﬁrst\nresearch question, precisely RQ 1.1. In the course of this work, the identiﬁed requirements are used to\ndevelop an evaluation protocol. The protocol is going to implement the presented application-oriented\nCL requirements. At the same time, the dynamics and constraints of the scenario reveal that it is very\ndiﬃcult to implement a corresponding evaluation.\nThe presented real-world scenario focuses on the prediction of meta-data in the context of computer\nnetwork communications by using DNNs. Respective meta-data of terminated network connections\nare collected from a productive campus network. This meta-data is referred to as network ﬂows or in\nshort ﬂows. Flow data comprise both, the 5-tuple (source/destination IP address, source/destination\nport and the transmission protocol), and additional meta-information such as the duration or number\nof transmitted bytes. Thus, a potentially inﬁnite, long stream of ﬂow data needs to be processed. The\ngoal is to predict certain meta-data that may be of interest for applications, for example, intelligent\nrouting. In particular, the prediction concerns information that is not available at the beginning of a\ncommunication, e.g., its duration.\nSeveral one-week lasting experiments validate the capability of DNNs to predict ﬂow data. At the\nsame time, the results show that one static ML model cannot serve as a solution for the very problem.\nOn the contrary, it becomes obvious that the exemplary scenario needs to be classiﬁed in the area of\nCL. This is due to the constantly changing and dynamic context of computer networks. These changes\ninclude the inﬂuence of human behavior as well as changing applications. Based on this real-world\nscenario, the following constraints and requirements can be derived.\nConstraints:\nThe presented constraints specify challenges implied by the exemplary scenario. The\nindividual constraints illustrate why an evaluation within this real-world scenario is challenging.\nParadoxically, an evaluation of CL models’ capabilities is hard to realize in a real-world CL scenario.\nBased on the exemplary scenario and the conducted experiments, the following constraints are identiﬁed.\nUntraceable changing data distribution: The dynamic nature of real-world CL scenarios partic-\nularly restrict the traceability of changing data distributions. In this exemplary scenario, data is\ncollected in a campus network with many diﬀerent components and subscribers. The unknown,\nchanging distributions of the data result in unexpected behavior, which is diﬃcult or impossible to\nestimate. Thus, the expected CL performance of a ML method cannot be estimated either. These\ndynamics are observable in basic experiments, as shown in section 5.4.2. The lack of traceability is\nan essential factor for the evaluation and comparison of ML models in the context of a CL scenario.\nComparability of results: Another constraint is the transfer of the present results and thus the\ncomparability to other CL scenarios. In general, an oﬄine dataset is created and repeatedly\nevaluated for the purpose of research. Accordingly, it is diﬃcult to deﬁne a CL performance metric\nin a real-world scenario and to generalize it.\nImpact of prediction results: A particular challenge in the exemplary scenario is the dependency\nof ﬂows and their application. If ﬂow prediction is used, e.g., to make intelligent routing decisions,\noptimizations inﬂuence future ﬂows and therefore the subsequent training of the ML model. This\nresults in a consecutive concatenation of eﬀects, as the following example illustrates. Suppose that\nan emergent communication is correctly predicted if no optimized routing is applied. Based on the\nrouting optimization, the resulting behavior of the communication is inﬂuenced. This could prove\nthe approximately correct prediction to be wrong. At the same time, the exported ﬂow with the\ncorresponding meta-data would be used for training.\nValidation in applications: Furthermore, the inﬂuence of an applied ﬂow prediction should be\nmeasurable within a productive system. Intelligent routing will, again, serve as application. A\nchallenge regards the impossible generation of a comparative quality value. The reason for this is\nthat the data generating function cannot be reused. Notably, the generating function comprises all\nclients within the network including the interactions with applications. However, without an exact\ncomparison value, only derived quality values, such as the average latency within the network can\npage 58\nBenedikt Pfülb\n\nExemplary Real-World Scenario\nReal-World Continual Learning Requirements\nbe used. Alternatively, a reduced synthetic scenario (including a controllable generating function)\ncould be used to measure the impact of a prediction based application.\nMeaning of the classes boundaries: The ground-truth of the data is yet another challenge (see\nsection 6.1.1). Even if the exported ﬂow information seems correct, details are lost due to the\nconversion of a regression into a classiﬁcation problem. The reason for this transformation is the\nnon-uniform distribution of the data in conjunction with the ML model. Even if this transformation\nconverts the data into a roughly uniform distribution, its inherent meaning is questionable. At the\nsame time, data generated by sensors or measurements are always dependent on environmental\ninﬂuences. In this scenario, the quality of the exported data is inﬂuenced by the resolution of\nhardware timers. The timer resolution of 50 ms, for example, may not be suﬃciently precise.\nInterpretability of the data: A fundamental diﬃculty is the interpretability of data. As part of\nthe exemplary scenario, network ﬂows described by 5-tuples are used. In general, it is impossible\nto assess the assignment of a ﬂow to a class. The following is an exemplary 5-tuple: Source\n23.53.173.33, destination 172.18.8.226, source port 443, destination port 43236, protocol 6 (TCP).\nEven though the label is given after the ﬂows’ termination, it can depend on various factors, e.g.,\nthe components on the path or the application. Traceability, veriﬁcation and interpretation by\nhumans are therefore questionable. This is in contrast to image data which is intended to represent\na scene or a particular object, such as an image of a handwritten digit. This particularly limits\nthe evaluability of generated data. Thus, the extent of a generated sample’s validity can no longer\nbe determined by “looking” at a sample. Accordingly, other mechanisms have to be used (e.g., see\nLesort, Stoian, et al. 2019). As a consequence, the complexity of a respective evaluation increases.\nImportance of the past: Another important constraint concerns the essential or decisive nature of\nknowledge. It is diﬃcult to determine the corresponding extent. An answer or estimation cannot\nbe derived from the data or from the classiﬁcation accuracy. Basically, trends can repeat, whereby\nretroactive knowledge should not be lost. At the same time, however, this can be the opposite for\ncertain knowledge. Old knowledge needs to be overwritten, as it is no longer valid. This is the case,\nfor example, with applications whose communication pattern is changed due to an update.\nRequirements:\nThe CL requirements presented below are derived from the exemplary CL scenario.\nML models should meet the respective requirements in order to be used in CL scenarios. At the same\ntime, the requirements should be included in an application-oriented evaluation protocol.\nConstant processing time: Constant processing time is a basic requirement for streaming scenarios.\nIn order to incrementally train on an inﬁnite stream of data the computational complexity has\nto be constant for each training step. This is especially true for the addition of new knowledge.\nThe update complexity must not depend on the number of already processed samples or tasks.\nAssuming that each block in the exemplary scenario is considered as a task, no data stream can be\nprocessed without a constant update complexity.\nConstant memory footprint: The memory consumption is also limited by a requirement resulting\nfrom the streaming concept. Memory consumption must be constant, otherwise an application in\na streaming scenario is not feasible. If all data has to be cached for reprocessing, in theory, an\ninﬁnite amount of memory has to be available. The preprocessed and uncompressed ﬂow data,\nfor example, correspond to a volume of about 441.67 GB. This amount was generated within one\nweek of data collection. Even though only a subset of the data is stored, new challenges arise, e.g.,\nwhich samples to save and for how long. Furthermore, the presented application scenario is only a\n“modest” computer network with regard to its scope. In other applications, such as image or video\nprocessing contexts, the data volumes can be much larger. A single HD video stream, for example,\ngenerates larger amounts of data, which is why codecs are used for compression.\nNo foresight into future data: In online real-world scenarios, looking at future data is impossible.\nThus, an adjustment of diﬀerent hyper-parameters based on future data is generally not feasible.\nThis requirement in turn causes limitations, especially for model selection, i.e., the determination\nof a decent DNN architecture. In addition, other crucial hyper-parameters such as the learning\nrate cannot be adjusted in advance.\nLimited access to past data: Random access to past data is also a limitation in terms of storage\nrequirements. It mainly concerns the subsequent adjustment of hyper- or model parameters based\non past data. While a certain number of samples can be cached, this is only possible if the available\nmemory has a constant size. Thus, as soon as new data is available, memorized samples must be\nBenedikt Pfülb\npage 59\n\nConclusion\nExemplary Real-World Scenario\noverwritten or individually replaced. Challenges arise related to the extent and speciﬁcation of the\nretained data. The choice of samples is particularly challenging in streaming scenarios.\nChangeable data distribution: According to the CL paradigm, it is crucial that a change in the\ndata-generating distribution is assumed (e.g., section 2.3.1 or mixed forms). As a result, the ML\nmodel has be able to handle diﬀerent types of drifts and shifts. The model should also be able\nto detect changes on its own and react accordingly. However, measurability of changes is not\nalways easy to implement. One the one hand, changes may be insigniﬁcant (outliers), not directly\ndetectable, or occur slowly, e.g., over weeks or even months. On the other hand, changes in the\ndistributions can occur in very short time intervals. The results of the investigation of a short period\nclearly proves that (see section 5.4.2). In the presented scenario, the special task is to recognize\nthese changes online and to react correctly.\n5.6\nConclusion\nThe prediction of network ﬂows or its meta-data is an adequate application scenario, which can be\nclassiﬁed as continual learning task. The data situation is interesting, because the target values (labels)\nare already speciﬁed during the data collection. Thus, the exemplary scenario is predestined for\nsupervised ML. In addition, the application of ﬂow prediction can be used for many optimizations in\nthe area of computer networks, e.g., optimized routing.\nOne drawback of the data is its uneven distribution, as shown by the data analysis in section 5.3.\nThis is particularly problematic for DNNs. To counteract this, a transformation into a classiﬁcation task\nis indispensable. As a consequence, the ground-truth of the data is lost (section 6.1.1). Nevertheless,\nthe performed experiments in section 5.4 show that a predication is possible with DNNs. Although it\nis diﬃcult to make a concrete statement regarding quality, the ML model seems to be able to infer\nknowledge from the data. Further comparative studies could attempt its measurement or develop an\napplication scenario with comparable results.\nAll in all, requirements for a CL application can be derived from the presented exemplary real-world\nscenario. However, the chosen scenario does not cover all possible requirements that may arise in\npractice. In other scenarios, not all data samples may be labeled at the time of acquisition. This\nadditional criterion could lead to further requirements as known from few-shot learning. Basically, the\nderived real-world requirements should be implemented in a CL investigation protocol.\nAnother ﬁnding is that it is diﬃcult to carry out investigations within real-world scenarios. This is\nespecially due to the emerging constraints. Indeterminable dynamics of the data generating function\nand incomprehensible changing data distributions contribute to the challenges. For these reasons, the\nexemplary real-world scenario is unsuitable for the investigation of ML models in the CL context.\npage 60\nBenedikt Pfülb\n\n6.\nApplication-Oriented Continual Learning\nProtocol\nChapter Contents\n6.1\nUsed Datasets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n6.1.1\nGround-Truth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n6.2\nSequential Learning Tasks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n6.3\nRealistic Application-Oriented Evaluation Protocol\n. . . . . . . . . . . . . . . . .\n66\n6.3.1\nReal-World Continual Learning Requirements . . . . . . . . . . . . . . . .\n67\n6.3.2\nHyper-Parameter Selection . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\n6.3.3\nModel Training and Re-Training\n. . . . . . . . . . . . . . . . . . . . . . .\n69\n6.3.4\nModel Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\n6.4\nDiscussion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n6.5\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\nIn this chapter, an evaluation protocol is presented, which will be used to examine a machine\nlearning (ML) model in the context of continual learning (CL). A consistent evaluation strategy is\nessential to establish comparability between diﬀerent models (Pfülb, Gepperth, et al. 2018; Pfülb and\nGepperth 2019). This includes various real-world requirements (see chapter 5). Taking these into\naccount illustrates the applicability catastrophic forgetting (CF) avoidance models in CL scenarios.\nThe basic problem of related works is that every individual study uses a similar but often non-\ntransparent evaluation scheme. This may allow comparability within the study itself, but prevents\ndirect comparisons to other studies. Not only the metrics are concerned, but also the scenario for\ntraining and evaluation. For this reason, similar models that ﬁt into a speciﬁc scheme are often used.\nFurthermore, the respective evaluation may not be designed for real-world requirements. Since many\nspecialized models are mostly intended as proof-of-concept, they are only tested in ﬁctitious scenarios.\nOracles are used, for example, which support the model with additional information for training and\ntesting. The problem is that this information is available in very few real-world applications. This\nleads to a shift of the problem towards the acquisition and provision of information. Thus, direct\ncomparisons across diﬀerent studies are challenging.\nAnother issue of related studies is that detailed evaluation procedure is not always complete\nor suﬃciently explicit. This may be due to page limitations of the publication. Alternatively, the\nprogramming code can serve as basis to describe the evaluation scheme. However, this requires an\nintensive study of complex code. Moreover, there is no guarantee that the evaluation part of the code is\navailable. To address this very problem, a detailed evaluation scheme is developed and presented in this\nwork. It allows for a more realistic assessment regarding the CF eﬀect and thus the CL characteristics.\nContributions\nThis chapter is supposed to answer research question RQ 1, precisely RQ 1.2 and\nRQ 1.3 (see section 4.1). Accordingly, an evaluation protocol, which measures the CF eﬀect, is developed\nas the main contribution of this chapter. The protocol reﬂects requirements occurring in real-world CL\napplications. The ﬁrst component for the development of an investigation protocol is represented by the\nrequirements (RQ 1.1) which are derived from the real-world scenario presented in chapter 5. Second,\nthe occurrence of the CF eﬀect is illustrated. A measurement of the CF eﬀect serves as an answer to\nRQ 1.2. The third sub-question (RQ 1.3) concerns the evaluation schemes issued in related work (see\nBenedikt Pfülb\npage 61\n\nUsed Datasets\nContinual Learning Protocol\nchapter 3). These schemes are included as an answer to research question RQ 1.3. To conclude, the\ncontribution of this chapter is an evaluation protocol in the form of a ﬂexible framework that can be\nused to evaluate diﬀerent models in the CL context. Thus, other and new CF avoidance models can\neasily be added, evaluated and compared.\nStructure\nThe ﬁrst section of this chapter describes the datasets (mainly image datasets) used as a\nbasis for evaluation (see section 6.1). Image datasets have a high number of input features. However,\nthe question arises from which point onward a dataset is considered high-dimensional. A color image\nwith the resolution 100×100 pixels (=3000 features) is nowadays assumed to be low dimensional in\nthe context of image/video processing. Vijayakumar, D’Souza, et al. (2005), for example, designate 90\nfeatures as “high-dimensional spaces”.\nAt the same time, a high number of dimensions may also represent information via very few features,\ne.g., in one pixel of an image. In order to address this issue, commonly available datasets that are\napplied in other studies are used. Furthermore, the chosen size of the datasets is moderate so that they\ncan be processed by commercially available hardware. Another advantage of image datasets is related\nto a human’s capability to interpret them. Thus, the sample itself can be validated by a human. This\nmethod is especially interesting for models that generate data samples.\nThe second part of this chapter describes the underlying scheme for deriving knowledge from the\ndata. Datasets are divided into diﬀerent chunks and provided to the model step by step. These\npredeﬁned tasks are referred to as Sequential Learning Task (SLT). They describe, among other things,\nthe level of diﬃculty of a SLT. In section 6.2, the SLTs are described in detail, and the used SLTs are\nsummarized.\nThe third and last part of the present chapter refers to the training and evaluation scheme\n(see section 6.3). The scheme focuses on the aspect of CL while meeting the application-oriented\nrequirements and constraints. The latter include the selection of hyper-parameters, (re-)training of\nmodels and the measurement of quality.\n6.1\nUsed Datasets\nIn order to measure the impact of catastrophic forgetting during continual learning scenarios, publicly\navailable datasets are used. Thus, all results can be reproduced and other models can be evaluated.\nEach dataset X consists of a collection of n individual samples. Each sample xn, n ∈{1, . . . , N} is\ndescribed by an m-dimensional vector. Multidimensional data, such as images, are transformed into\na one-dimensional form by a ﬂattening operation. A color image with a width of 100 pixels and a\nheight of 100 pixels (and 3 color channels) is, for example, transformed into a vector x with m = 30000\nentries.\nAll of the used datasets belong to the category of classiﬁcation problems, with only one single\ncorrect class assigned to each sample (section 2.2). For each sample, the target value is an element of\n{0, . . . , C −1}, while C is the number of diﬀerent classes. The target value yn, or label, is used in the\none-hot format (see equation 2.1). If the original dataset is not split into training and test data, it\nwill be split randomly into 90% training and 10% test data. By shuﬄing the data, an approximately\neven distribution within the classes is ensured for each training batch. Normalization is yet another\npre-processing step that is performed on the data (not labels). Since image data and their pixels\nare often represented by 8 bits, min-max normalization is used. Input values are converted into the\nrange [−1, +1] (or [0, +1]). This range of values is beneﬁcial because the highest possible number\nof ﬁne-granular numbers can be represented. This is due to the representation of ﬂoating point\nnumbers (Bush 2014).\nMNIST\nThe Modiﬁed National Institute of Standards and Technology (MNIST) dataset is probably\nthe most well-known dataset (see LeCun, Bottou, et al. 1998). It is often used for teaching machine\nlearning, as it is integrated into common software frameworks. It consists of grayscale images with\ndigits from 0 to 9 in a resolution of 28 by 28 pixels. The standard dataset consists of 60 000 training\nand 10 000 test images. MNIST is a modiﬁed version of the NIST dataset (Grother 1995).\nIn the present work, two sub-datasets are merged. They comprise the handwritten digits of\nemployees of an American company and high school students. Details of the MNIST dataset are\npresented in table 6.1.\npage 62\nBenedikt Pfülb\n\nContinual Learning Protocol\nUsed Datasets\nTable 6.1: Detailed information of the MNIST dataset.\nresolution training\ntest\ntraining\ntest\nrandom samples (from classes)\nsamples samples balance balance 0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n28×28×1\n60 000\n10 000\n2.1\n2.4\nEven at the very ﬁrst examination of the dataset, MNIST does not seem to be a challenge. The lowest\nerror rate reported by LeCun, Bottou, et al. (1998) is 0.8% when using Support Vector Machine (SVM).\nSeveral attempts have been made to develop improved models or to ﬁnd model conﬁgurations with a\nlower error rate. Currently, the classiﬁcation quality is in the range of the last half percent to 100%.\nAs a result, Chollet (2017) claims that MNIST should be omitted and more sophisticated dataset\nshould be used for research:\n“Many good ideas will not work well on MNIST (e.g. batch norm). Inversely many bad\nideas may work on MNIST and no transfer to real CV.” Chollet 2017\nLooking at the respective publications shows that such statements do not necessarily lead to a lower\nuse of the MNIST dataset. Fortunately, more and more datasets are created and used as a point of\nreference.\nEMNIST\nCohen, Afshar, et al. (2017) present the Extended MNIST (EMNIST) dataset. Above all,\nthis dataset contains additional classes. At the same time, the number of available samples is increased.\nNevertheless, the format of the grayscale images is maintained (see table 6.2). This is due to the fact\nthat MNIST is included and existing models can be evaluated without further adjustments.\nThere are several sub-datasets, which consist of a maximum of 62 classes with 814 255 samples.\nUnfortunately, unbalanced classes are included. For this reason and for the sake of comparability to\nother datasets, a separate subset of the dataset is constructed. The 10 classes containing the most\nsamples are selected.\nTable 6.2: Detailed information of the EMNIST dataset.\nresolution training\ntest\ntraining\ntest\nrandom samples (from classes)\nsamples samples balance balance 0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n28×28×1\n345 000\n57 900\n2.0\n1.9\nFashionMNIST\nThe FashionMNIST dataset attempts to address the simplicity of MNIST, as\npointed out by Xiao, Rasul, et al. (2017):\n“Moreover, Fashion-MNIST poses a more challenging classiﬁcation task than the simple\nMNIST digits data [...]” Xiao, Rasul, et al. 2017\nThe dataset is provided by the Zalando research group (see Xiao, Rasul, et al. 2017). The structure\nis identical to MNIST and can often be used by replacing the download URL. The resolution (di-\nmensionality) of 28 by 28 pixels and even the size with 60 000 training and 10 000 test samples is the\nsame. This has the advantage that models optimized for MNIST can be applied to FashionMNIST\nwithout any adjustments. Unlike MNIST, the grayscale images show representations of diﬀerent types\nof clothing, with 10 available categories (see table 6.3).\nTable 6.3: Detailed information of the FashionMNIST dataset.\nresolution training\ntest\ntraining\ntest\nrandom examples (from classes)\nsamples samples balance balance\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n28×28×1\n60 000\n10 000\n0\n0\nBenedikt Pfülb\npage 63\n\nUsed Datasets\nContinual Learning Protocol\nSVHN\nThe Street View House Numbers (SVHN) dataset (see Netzer and Wang 2011) consists of\ncolor images of house numbers extracted from Google Street View records. In this work, the cropped\nand centered variant is used with a resolution of 32×32 pixels. As depicted in the samples as part of\ntable 6.4, images can be aﬀected by interfering factors.\nTable 6.4: Detailed information of the SVHN dataset.\nresolution training\ntest\ntraining\ntest\nrandom examples (from classes)\nsamples samples balance balance\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n32×32×3\n73 200\n26 000\n12.5\n13.4\nDevanagari\nThe Devanagari Handwritten Character Dataset (DHCD) consists of Indian characters.\n46 elements of diﬀerent character groups are included. Each contains 2 000 samples. The individual\ncharacters are extracted from handwritten documents and labeled by hand. As shown by Acharya,\nPant, et al. (2016), each sample has the same resolution as samples from the MNIST dataset, but a\nmargin of 2 pixels is added on each side. An initial test accuracy of 98.47% is reported.\nIn order to proportionally adjust the dataset compared to the other ones, 10 classes are randomly\nselected. The data is split into 90% training and 10% test samples (see table 6.5).\nTable 6.5: Detailed information of the Devanagari dataset.\nresolution training\ntest\ntraining training\nrandom examples (from classes)\nsamples samples balance balance\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n32×32×1\n18 000\n2 000\n0.1\n1.6\nCIFAR-10\nThe work of Vinet and Zhedanov 2011 proposes the Canadian Institute for Advanced\nResearch (CIFAR) dataset. There are two diﬀerent variants available, CIFAR-10 and CIFAR-100.\nEach number represents the contained number of classes. In the present work, CIFAR-10 is used. This\nvariant consists of 60 000 samples, which are evenly distributed over the 10 classes. The color images\ninclude airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships and trucks, which are all\nconsidered classes (see table 6.6).\nTable 6.6: Detailed information of the CIFAR-10 dataset.\nresolution training\ntest\ntraining\ntest\nrandom examples (from classes)\nsamples samples balance balance\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n32×32×3\n50 000\n10 000\n0\n0\nFruits 360\nThe Fruits 360 dataset consists of color images representing a variety of fruits. Fruits\nwere photographed from diﬀerent angles for the construction of the dataset. 55 244 samples were taken\nand classiﬁed into 81 diﬀerent types of fruits. The images are available with a resolution of 100 by 100\npixels. The background is replaced by white color by using an automatism (see Mureşan and Oltean\n2018). The 10 classes with the best representations are selected for this work (see table 6.7).\nTable 6.7: Detailed information of the Fruits 360 dataset.\nresolution training\ntest\ntraining\ntest\nrandom examples (from classes)\nsamples samples balance balance\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n32×32×3\n7 700\n2 600\n3.2\n3.1\npage 64\nBenedikt Pfülb\n\nContinual Learning Protocol\nUsed Datasets\nMADBase\nSherif Abdelazeem 2010 provide the Modiﬁed ADBase (MADBase) dataset. ADBase\nconsists of 70 000 samples of handwritten Arabic digits. 700 people from the ﬁeld of higher education\nand state enterprises supported the construction of the dataset. The “Modiﬁed” version (MADBase) is\nan adaptation of the MNIST dataset. In particular, the format is adjusted to a uniform resolution of\n28 by 28 pixels (see table 6.8).\nTable 6.8: Detailed information of the MADBase dataset.\nresolution training\ntest\ntraining\ntest\nrandom examples (from classes)\nsamples samples balance balance\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n28×28×1\n60 000\n10 000\n0\n0\nNotMNIST\nEven though NotMNIST is based on the MNIST dimensions, it uses publicly available\nfonts instead of handwriting. In addition, letters from A to J are depicted instead of digits (see\ntable 6.9). The diﬀerent fonts should increase the level of diﬃculty for ML models, see for example\ndiﬀerent Fs:\n,\nor\n(see Yaroslav Bulatov 2011). “Judging by the examples, one would expect this\nto be a harder task than MNIST. This seems to be the case [...]” Yaroslav Bulatov 2011. Similar to\nmany variants of MNIST, the NotMNIST dataset was created for the reason that MNIST seems to be\ntoo simple.\nTable 6.9: Detailed information of the NotMNIST dataset.\nresolution training\ntest\ntraining\ntest\nrandom examples (from classes)\nsamples samples balance balance\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n28×28×1\n529 100\n18 700\n≈0\n≈0\nISOLET\nISOLET is the only non-image dataset used in this work. Samples represent spoken letters\nrecorded from 150 subjects (see Cole and Fanty 1990). It contains 7 797 samples. Each is encoded\nand consists of 617 features (ﬂoat values). In order to establish comparability with other datasets, 10\nclasses are extracted. For its visualization, random samples of each class are plotted as illustrated in\ntable 6.10.\nTable 6.10: Detailed information of the ISOLET dataset.\nresolution training\ntest\ntraining\ntest\nrandom examples (from classes)\nsamples samples balance balance\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n617×1\n2 300\n600\n0.3\n0\n6.1.1\nGround-Truth\nThe selection of datasets is subject to the ground-truth. In particular, it refers to class membership,\nwhich is crucial in the context of supervised learning. This applies to datasets whose classes and\nboundaries should be clearly deﬁned. The ground-truth of a sample is given, if the class mapping\nis inherently correct, e.g, a subject should speciﬁcally draw the digit one for the MNIST dataset.\nNevertheless, errors can occur related to the quality of samples to the assignment of labels. The selection\nof MNIST (test) samples in ﬁgure 6.1 is intended to show that there is a margin of interpretation, even\nfor humans. However, this does not change the ground-truth of the data itself. Another frequently\ncited example is the classiﬁcation of data in the area of spam detection. There is a clear separation\nbetween what is spam and what is not. The same is true for spam emails, which may be interpreted as\nvalid emails. Likely, the ground-truth of the data remains unchanged.\nHowever, the ground-truth of the data can change, if a regression problem is turned into a classiﬁcation\nBenedikt Pfülb\npage 65\n\nSequential Learning Tasks\nContinual Learning Protocol\nImage\nLabel\n8\n9\n9\n7\n6\n7\n2\n7\n2\n7\n0\n3\nFigure 6.1: Suspicious digits of the MNIST dataset.\nproblem. For this purpose, the scalar value representing the label is assigned to a concrete class.\nAlthough the deﬁnition of class boundaries is ﬁxed, their basic ground-truth may get lost. This can be\ndue to the deﬁnition of class boundaries.\n6.2\nSequential Learning Tasks\nIn this work, the Sequential Learning Tasks (SLTs) represent diﬀerent CL scenarios. The datasets\npresented in section 6.1 are divided into diﬀerent sub-datasets. The class labels within the datasets are\nused for this splitting. Since all datasets provide the same number of classes (10), any division can be\napplied to any dataset. The identiﬁer of a SLT starts with a D followed by the number of sub-tasks,\nor how many classes are contained in each sub-task. Therefore, the SLT D10 consists of one single task\nwhich contains 10 classes. Since these ten classes represent all available classes, D10 is referred to as\nthe baseline. The experimental results of the baseline task serve as a comparative reference value for\nall other SLTs.\nA baseline experiment corresponds to a joint training with all available data. Another example with\nmultiple sub-tasks is D3-3-3-1. It consists of 3 divisions with three and 1 sub-task with one class. Each\nsub-task within a SLT is enumerated, i.e., T1, T2, . . ., Tx and corresponding classes are assigned. Thus,\neach sub-task is deﬁned by one or more classes. All deﬁned SLTs, respectively their assigned classes\nwithin the sub-task, do not overlap. They are thus disjunct, except for permuted SLTs. Permutation\ndescribes the shuﬄing of the samples’ features according to a random, but ﬁxed rule. In the deﬁnition\nof the SLT D10-p10, the “p” indicates the permutation so that all samples are permuted in the second\ntask. Thus, a second dataset is created, which contains the same information in a diﬀerent order.\nThe deﬁned SLTs (see table 6.11) are used to measure the CF eﬀect. Diﬀerent variations of SLTs\nare deﬁned to test the eﬀectiveness of the procedure. The diﬃculty of the tasks is represented by how\nmany classes are added per task. Accordingly, the most diﬃcult tasks could be the addition of one\nsingle class after another. In order to exclude the inﬂuence of the class order, variations are deﬁned.\nAs described in section 6.1, a training set T train\nx\n, as well as a test set T test\nx\nis available for each sub-task\nTx.\nThe permutation is based on a deﬁned substitution operation. For this purpose, a seeded random\npermutation matrix is generated. This permutation matrix is applied to each sample. Thus, the\norder of the input features (e.g., pixel values) is swapped according to a deﬁned pattern.\nThe\npermutation applied to selected samples is illustrated in ﬁgure 6.2. A permuted dataset contains the\nsame information and classes as the original dataset. Thus, the number of data points, classes and the\ndegree of dimensionality are the same. Merely the order of the presented information/features changes.\nClass\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nInput\nPermuted\nFigure 6.2: Exemplary permuted digits of the MNIST dataset.\n6.3\nRealistic Application-Oriented Evaluation Protocol\nIn this section, the development of an application-oriented CL evaluation protocol is presented. Above\nall, deep learning models are going to be investigated with this protocol, since they are aﬀected\npage 66\nBenedikt Pfülb\n\nContinual Learning Protocol\nRealistic Application-Oriented Evaluation Protocol\nTable 6.11: Deﬁnition of SLTs and the class divisions of their sub-tasks.\nSLT\nSub-tasks\nD10 (baseline)\nT1(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\nD10-p10 (permuted) T1(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\nT2(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\nD9-1a\nT1(0, 1, 2, 3, 4, 5, 6, 7, 8)\nT2(9)\nD9-1b\nT1(0, 1, 2, 4, 5, 6, 7, 8, 9)\nT2(3)\nD9-1c\nT1(0, 2, 3, 4, 5, 6, 7, 8, 9)\nT2(1)\nD5-5a\nT1(0, 1, 2, 3, 4)\nT2(5, 6, 7, 8, 9)\nD5-5b\nT1(0, 2, 4, 6, 8)\nT2(1, 3, 5, 7, 9)\nD5-5c\nT1(0, 2, 5, 6, 7)\nT2(1, 3, 4, 8, 9)\nD5-5d\nT1(3, 4, 6, 8, 9)\nT2(0, 1, 2, 5, 7)\nD5-5e\nT1(0, 1, 3, 4, 5)\nT2(2, 6, 7, 8, 9)\nD5-5f\nT1(0, 3, 4, 8, 9)\nT2(1, 2, 5, 6, 7)\nD5-5g\nT1(0, 5, 6, 7, 8)\nT2(1, 2, 3, 4, 9)\nD5-5h\nT1(0, 2, 3, 6, 8)\nT2(1, 4, 5, 7, 9)\nD5-5i\nT1(0, 1, 2, 6, 7)\nT2(3, 4, 5, 8, 9)\nD3-3-3-1\nT1(0, 1, 2)\nT2(3, 4, 5)\nT3(6, 7, 8)\nT4(9)\nD2-2-2-2-2a\nT1(0, 1)\nT2(2, 3)\nT3(4, 5)\nT4(6, 7)\nT5(8, 9)\nD2-2-2-2-2b\nT1(1, 7)\nT2(0, 2)\nT3(6, 8)\nT4(4, 5)\nT5(3, 9)\nD1-1-1-1-1-1-1-1-1-1a\nT1(0)\nT2(1)\nT3(2)\nT4(3)\nT5(4)\nT6(5)\nT7(6)\nT8(7)\nT9(8)\nT10(9)\nD1-1-1-1-1-1-1-1-1-1b\nT1(7)\nT2(1)\nT3(2)\nT4(0)\nT5(6)\nT6(8)\nT7(4)\nT8(5)\nT9(9)\nT10(3)\nby the CF eﬀect. This eﬀect leads to the (catastrophic) forgetting of all existing knowledge, as\nsoon as an attempt is made to add new knowledge to a model. The special feature of application-\noriented evaluation protocols is the implementation of real-world requirements. The newly developed\ninvestigation protocol should be used to examine CF-avoidance models in particular.\nThe ﬁrst step towards the evaluation protocol is to summarize the requirements as prerequisite\nfor a realistic CL scenario. Second, the selection of hyper-parameter is depicted. The resulting\nhyper-parameters are used for the third step, namely the training and re-training of ML models. As a\nlast step, the metric used to compare a model is outlined.\n6.3.1\nReal-World Continual Learning Requirements\nAn application-oriented evaluation protocol is designed to implement real-world requirements. The\npresented real-world scenario is used for the collection of these requirements (see chapter 5). The\nresulting requirements are brieﬂy summarized below and implemented in the evaluation protocol.\nIncremental training\nA basic requirement all models have to satisfy refers to the ability of being\ntrained incrementally (see section 2.3.2). This means that the ML models are able to add new knowledge\nstep by step. As the used datasets consist of discriminating classiﬁcation tasks, models need to be\ntrainable in a supervised manner. However, the number of classes to be distinguished is unlimited, i.e.,\nnot only binary classiﬁcation tasks can be realized.\nConstant memory consumption\nAnother requirement especially important for real-world scenarios\nconcerns the constant consumption of memory. In general, it can be assumed that a certain amount\nof memory can be reserved by the model. However, memory consumption must not depend on the\ncharacteristics of the concrete CL scenario, e.g., the number of samples or sub-tasks. In a streaming\nscenario, the theoretically resulting memory requirements are inﬁnite.\nNo past data\nThe required constant memory consumption already indicates that no data is available\nfor training from the past or previous sub-tasks. This is especially important in streaming scenarios.\nIn theory, an inﬁnite number of samples or sub-tasks would have to be stored. Accordingly, an inﬁnite\namount of memory would be required. Memory requirements are, however, very limited, especially in\nembedded systems.\nBenedikt Pfülb\npage 67\n\nRealistic Application-Oriented Evaluation Protocol\nContinual Learning Protocol\nNo future data\nAnother real-world requirement concerns the use of future data. For the purpose of\nresearch and evaluation, sub-tasks with a temporal oﬀset are introduced. In the context of realistic\napplications, however, access to future data is impossible. Only the data of the current sub-task and\nits predecessors are available, whereas the latter may be excluded due to memory limitations. This\neliminates two possible training and evaluation procedures: First, at the initial training all data from\nthe past and future are available. The access of future data would correspond to a joint training\ndataset for multiple CL tasks. Second, an adjustment of the current model state or hyper-parameters,\nbased on future data, is impossible. These limitations may seem trivial at a ﬁrst glance, but the\ncorresponding information is not always implemented or explicitly presented in experimental setups\n(e.g., Kirkpatrick, Pascanu, et al. 2017; Goodfellow, Mirza, et al. 2013; Srivastava, Masci, et al. 2013).\nConstant update complexity\nAnother requirement is the training/update complexity regarding\nthe computational runtime. This is particularly true for streaming scenarios. If additional computing\ncapacity is required for each new sub-task or sample, the update method is not suitable for streaming\nscenarios. This requirement prohibits the use of elaborate pruning or aggregation mechanisms that\nhave to be performed after each sub-task. Therefore, the runtime complexity always has to be constant\n(O(1)) and must not depend on the number or complexity of the sub-tasks. This requirement also\nexcludes replay-based models, as long as the number of generated samples depends on the number of\ntasks (e.g., see Shin, Lee, et al. 2017).\nNo oracles\nThe use of oracles during training and testing would also aﬀect the results. An oracle\nprovides additional information. An oracle, for instance, indicates a sample’s origin with regard to\nthe sub-task. However, the use of oracles is prohibited in the context of the evaluation protocol for\ntwo reasons. Firstly, the chances of an oracle’s existence strongly depend on the scenario. Secondly,\nnot all ML models require information provided by oracles. One discrepancy in this regard concerns\nthe trigger for new sub-tasks (adding new knowledge). This is referred to as task boundary. It is\nparticularly easy to identify a possible boundary by tracing the used labels.\nThis information is only decisive for SLTs that have clear task boundaries at all. A changing data\ndistribution is often unknown or ﬂuid (see section 2.3.1), which is the case in the present real-world\nscenario. Therefore, a model should be able to recognize a task boundary or it has to be able to handle\nthe respective changes.\nContinual learning objective\nIn addition to the presented requirements, CL objectives are deﬁned.\nThe basic goal of a CL scenario is to accumulate knowledge. However, it has to and should be possible\nto overwrite incorrect knowledge in real applications. At a deﬁned point in time (e.g., at the end of\nthe training scenario), the maximum amount of knowledge should be available. In general, a loss of\nknowledge is assumed to occur over time. Nevertheless, the loss should take place gradually and not\nsuddenly (i.e., catastrophically).\n6.3.2\nHyper-Parameter Selection\nThe performance of ML models usually depends on hyper-parameters. The choice of the model\narchitecture is one of the hyper-parameters (consider the XOR problem, see Brutzkus and Globerson\n2019). Determining a model’s architecture is referred to as model selection. In general, more artiﬁcial\nneurons or layers should be better, as more complex functions can be approximated. At the same time,\noverﬁtting may occur, which is a disadvantage (see section 2.2.2.5).\nAs many parameters are problem-dependent, they need to be adjusted, e.g., the learning rate ϵ.\nUsually, it is diﬃcult to infer the hyper-parameters from the problem. Instead, suitable parameters are\ndetected via trial-and-error. Accordingly, various parameter values are tested, whereas the best result\nconﬁrms the selection. Since there are often many diﬀerent hyper-parameters available, a grid-search\nneeds to be performed (see section 4.3). This means that each parameter variation is combined with\neach other. The experiments are conducted with the resulting combinations and evaluated as a next\nstep. Depending on the number of hyper-parameters and values, this procedure leads to a large\nnumber of experiments. This can result in a very time and computationally intensive hyper-parameter\noptimization.\nA challenge related to CL scenarios is the time slot and selection of hyper-parameters. In order to\npage 68\nBenedikt Pfülb\n\nContinual Learning Protocol\nRealistic Application-Oriented Evaluation Protocol\nmake a realistic assumption for CL scenarios, it is important that only the data of the ﬁrst task is\navailable for hyper-parameter selection. In other words, it is impossible to base the hyper-parameter\nselection on data from future tasks (or all data). This restriction has a signiﬁcant inﬂuence on the\nchosen architecture and other hyper-parameters. In addition to the grid-search results, parameters\nsuggested by other authors can be used, as long as they have been evaluated.\nModels may be inﬂuenced by other parameters. These include the used batch size B and particularly\nthe number of training iterations. A larger batch size comes with two advantages. The ﬁrst one is\nthat more representative (averaged) gradients can be determined. Compared to a batch size of 1, the\nparameters (weights) of the model are changed speciﬁcally for a single sample. The second advantage\nis that the determination of gradients for larger batches is computationally more eﬃcient by means of\nML frameworks. The limiting factor of the batch size is usually the available memory size. Despite the\nfact that there is usually more memory available, the batch size is set to 100 for all experiments. This\nchoice is due to the great number of related works using the same value (see chapter 3).\nThe number of training iterations as a further parameter is crucial for the ﬁnal performance of\nan ML model. In general, fewer training iterations can lead to a weak performance of the model.\nSince a stochastic optimization method is used for training (Stochastic Gradient Descent (SGD), see\nsection 2.2.2.2), the crucial factors comprise the learning rate ϵ, the derived gradients and the number\nof training iterations. In theory, one training step with a “correctly” initialized model, a “perfect”\nlearning rate ϵ and a set/batch of “representative” data already leads to an optimal model. However,\nthis is only possible in theory for very simple problems. In order to reduce the correlation between\niterations and learning rate, a smaller learning rate has to be chosen, which requires an increased\nnumber of training iterations. If the learning rate is too small (ϵ →0), it takes too many training\niterations (→∞) for the model to converge. Since the two parameters are partially interdependent,\nthe learning rate only is varied whereas the number of training iterations remains ﬁxed.\nDatasets do not always consist of the same number of training samples, e.g., see section 6.1.\nMoreover, the number of training iterations required for the obtaining of an acceptable ML model\nconﬁguration is unknown. Thus, specifying a particular number of training iterations is problem-\ndependent. A comparable evaluation can be assured by specifying how often a complete dataset is\nprocessed. A complete run is referred to as an epoch. The training iterations required to process an\nepoch therefore depend on the batch size and the number of samples within the dataset. Regardless,\nthe speciﬁed number of training iterations can always be a disadvantage for a model. The disadvantages\nresult from either too many or too few iterations. At the same time, the number of iterations may also\nbe related to other hyper-parameters or the problem itself.\nEarly-stopping is a known method to tackle the problem of stopping the training process at a “good”\npoint. However, the implementation of early-stopping is challenging under real-world conditions due to\nthe restriction of data access. Furthermore, it is necessary to periodically save the current progress of\nthe model, which requires a lot of memory. The reserved memory for checkpoints must be allocated for\na certain time before it can be released. Furthermore, it is impossible to exclude the possibility that a\nbetter parameter conﬁguration can be obtained at a later training time. In general, the early-stopping\napproach contradicts application-oriented requirements (see section 6.3.1). In this work, the use of\nfuture or past test data to determine the best stopping point is prohibited.\nIn order to ﬁnd a consensus regarding the number of training epochs, the following requirement\nshould be fulﬁlled by the ML models. The model is not supposed to perform worse even if the\ntraining is “too long”. This also applies to the overﬁtting eﬀect (see section 2.2.2.5). Thus, based\non empirical results, a ﬁxed number of training epochs (Etrain =10) is set for all datasets. All other\nhyper-parameters, such as number of hidden layers L, their size S and other model-speciﬁc parameters\nare adapted by a grid-search. The varied parameters are summarized in the set P. Concrete values for\nthe parameter optimization are given in chapter 7.\n6.3.3\nModel Training and Re-Training\nThe application-oriented evaluation protocol starts with a grid-search for hyper-parameters. For this\npurpose, an ML model m is initialized and trained with a given parameter conﬁguration ⃗p ∈P. In this\nphase, the training data from the ﬁrst task (T train\n1\n∈T1) of the corresponding SLTs is used. The test\ndata T test\n1\ncan be used to evaluate the model at any time during training. The best conﬁguration can\nbe identiﬁed by evaluating the results of experiments with diﬀerent parameter conﬁgurations. However,\nthe number of measurement points and the scope of the test, e.g., the use of the entire test dataset,\nBenedikt Pfülb\npage 69\n\nRealistic Application-Oriented Evaluation Protocol\nContinual Learning Protocol\nrequires a certain eﬀort.\nThe detailed procedure of all training and test steps is presented in algorithm 6.1. Line 1 represents\nthe grid-search, which can be performed in parallel, since all models and parameters are independent\nfor T1. The training loop is shown in line 2, whereas training iterations are limited. The limit is\ncalculated by the number of training samples #(T train\n1\n), divided by the batch size and multiplied by\nthe number of epochs (Etrain = 10). Basically, two operations can be distinguished: Training and\ntesting. A training step is represented by train(m⃗p, T train\n1\n) in line 3.\nDue to the computational intensity of performance measurement, testing after each training step is\ntoo time-consuming and computationally expensive. Therefore, measurement points are set evenly\ndistributed over the full dataset. Depending on the acquired number of measurements, the location of\nmeasuring points is calculated in advance (see line 4). The measurement step (test(m⃗p, T test\n1\n)) is more\ncomplex, as the complete test set needs to be predicted and compared with the target values/labels\n(see line 5). As long as the test data is representative, the determination of a realistic performance\nvalue is assured. The resulting test value (qm⃗p,t) is stored as a quality measure for the respective model\nm, the corresponding parameter conﬁguration ⃗p and the current state at iteration t of the model m.\nAfter the computationally intensive model selection phase (loop in line 1), the collected measurements\nare evaluated. For this purpose, the qualitatively best parameter setup ⃗p∗for a given model m is\nspeciﬁed, based on the maximum achieved value p⃗p,t (shown in line 6). Succeeding this optimization\nstep, the model is (re-)trained only by means of the following tasks (T2, ..., Tx, deﬁned by the SLT).\nThis full evaluation process can be exempliﬁed by a scenario in which a product is manufactured\nand has to acquire additional new knowledge after its delivery. As a result, a hard task boundary is\nimposed between production and application.\nThe following protocol procedure simulates the application. Moreover, new knowledge needs to be\nadded to the model from the following tasks T2 to Tx (see line 7). By deﬁnition, it is still possible to\noptimize certain model parameters of the protocol based on ( ⃗p+). However, the additional optimization\nis limited to a single parameter, namely the learning rate ϵ (see line line 8). The respective limitations\nare discussed in section 6.4.\nThe next step in line 9 results in a copy (checkpoint) of the best model. Subsequently, the pre-\nparameterized model is re-trained (lines 10 and 11) with the training data of the current task (T train\nc\n).\nSimilarly, the quality of the model is measured for the current task (T test\nc\n) at several points during\nthe training process. In addition, the performance on the baseline dataset is measured (see line 13).\nThe baseline dataset D10 contains all test samples (T10) and is therefore considered as a reference\nvalue. However, this data is used only for evaluation purposes. It is excluded from training or other\noptimization steps.\nThe ﬁnal step after each simpliﬁed parameter optimization is the selection of the best model.\nTwo reference points can be used to identify the best model or model conﬁguration. The maximum\nmeasured performance value is one of them. Alternatively, the last measured performance value (at\nthe end of re-training) can constitute a more realistic reference point. The chosen value is subject of a\nlater discussion (see section 6.4).\n6.3.4\nModel Evaluation\nIn this section, the precise evaluation procedure is presented. The evaluation of the experimental\nresults is described in the last part of the protocol (see algorithm 6.1). The algorithms describe how\na test step is executed and how the collected values are evaluated. A second prescient evaluation\nprocedure illustrates the inﬂuence of the strategy on the results.\nThe smallest part of the evaluation step is the test function (test(m, T test\nx\n)). This test function is\nused multiple times in algorithm 6.1, i.e., in lines 5 and 13). The test function only uses test data\nto perform a prediction based on the current state of the model m. Side eﬀects from testing must\nnot inﬂuence the training process or other properties of the model. As speciﬁed by the requirements\n(section 6.3.1), past and future test data may only be accessed for evaluation purposes.\nA complete model test consists of many small test steps, since a batch size B is set for testing.\nThe batch size has no inﬂuence on the test results. Theoretically, it can be as large as the number\nof samples in the test dataset. For reasons of simplicity, the batch size is set to the same size as the\ntraining batch size (B=Btrain =Btest =100). Thus, depending on the size of the test dataset, multiple\ntest steps have to be executed for a complete epoch (Etest =1). This procedure is comparable to the\npage 70\nBenedikt Pfülb\n\nContinual Learning Protocol\nRealistic Application-Oriented Evaluation Protocol\nAlgorithm 6.1: The realistic application-oriented evaluation protocol.\nData: model m, SLT with sub-tasks T1, T2, ...Tx, hyper-parameter value set P\nResult: model with best continual learning quality for parameter conﬁguration qm⃗p∗,t,⃗p+\n1\nforall parameter conﬁguration ⃗p ∈P do\n// determine accuracy for all hyper-parameters when\ntraining on T1\n2\nfor t ←0 to #(T train\n1\n)\nB\n· Etrain do\n3\ntrain(m⃗p, T train\n1\n)\n4\nif t ∈measurement points then\n5\nqm⃗p,t ←test(m⃗p, T test\n1\n)\n6\nm⃗p∗←model m⃗p with maximum qm⃗p,t\n// ﬁnd best model with max. accuracy on T1\n7\nforall Tc ∈(T2, ..., Tx) do\n// subsequent tasks\n8\nforall parameter conﬁguration ⃗p+ ⊂⃗p∗do\n// parameters to be optimized subsequently\n9\nm⃗p∗,⃗p+ ←m⃗p∗\n10\nfor t ←0 to #(T train\nc\n)\nB\n· Etrain do\n11\ntrain(m⃗p∗,⃗p+, T train\nc\n)\n12\nif t ∈measurement points then\n13\nqm⃗p∗,t,⃗p+ ←test(m⃗p∗,⃗p+, T test\nc\n) AND\nqm⃗p∗,t,⃗p+,D10 ←test(m⃗p∗,⃗p+, T test\n1\nof D10)\n14\nm⃗p∗←m⃗p∗,⃗p+ with best qm⃗p∗,t,⃗p+ // ﬁnd best model with max. or max. last accuracy on Tc\nindividual training steps.\nMany other studies in the literature use a wide variety of metrics, and choosing the right metric\nis critical. The decisive criterion is the underlying data and the problem. The following questions\nhave to be considered:\nIs it a regression or a classiﬁcation problem? Is it a binary, single class or a\nmulti-class classiﬁcation problem? What is the goal of the prediction? Is the data evenly distributed\nacross the classes?\nFor the present evaluation protocol, only classiﬁcation problems are examined,\nwhich manifest as single class problems. This means that exactly one label is applicable to each sample\n(see section 2.2). False predictions, however, do not distinguish between the degree of falseness.\nThe goal of the test function is to determine whenever a prediction is correct or incorrect. In other\nscenarios, this is diﬀerent, for example, with regard to the detection of very rare events. Accordingly,\nother metrics such as precision, recall, F1 score, cross entropy are used (see Handelman, Kok, et al.\n2019). Since the datasets of this work have an approximately even class distribution (see section 6.1),\nthe accuracy is used. Thus, other metrics would yield the same results. At the same time, determining\nthe accuracy is more eﬃcient. It simply requires the ratio of correctly and incorrectly classiﬁed samples.\nAs a ﬁrst step, the test function uses the model to perform a predication of the test data T test\nx\n.\nEach single test sample xn is provided with a label yn (one-hot format). Once the predicted class\nlabel matches the true label, it is considered as a correct prediction ˆyn = yn. Otherwise, (ˆyn ̸= yn)\nthe predication is wrong.\naccuracy = #(correct classiﬁed)\n#(tested samples)\n(6.1)\nEquation 6.1: Deﬁnition of the accuracy metric.\nThe procedure described above (multiple use of the test function) speciﬁes how a single measurement\npoint is created. The distribution of the measurement points is even over each training task. Depending\non the number of training samples within the training dataset, the position of measurement points\nis determined. Their placement can be decisive, if there are many training iterations between the\nmeasurement points, and a possibly good model parameter conﬁguration can be skipped. From a\ncomputational point of view, however, it is very time-consuming to perform a full test after each\ntraining step. Therefore, an ML model should be robust in diﬀerent training stages to ensure the\nBenedikt Pfülb\npage 71\n\nRealistic Application-Oriented Evaluation Protocol\nContinual Learning Protocol\nindependence of test point placement. It is expected that the model converges to a suitable local\nminimum and stays there after several training iterations (see section 2.2.2.2). This should apply to\nfurther training steps. The same is expected in the context of overﬁtting (see section 2.2.2.5).\nThe advantage of a supervised learning scenario is that samples from the dataset can help determine\nthe quality of the model. This is crucial for the selection of the hyper-parameters at the beginning\nof training. Test data constitute the basis for the evaluation of a model’s performance, which allows\nfor the determination of hyper-parameters. Experiments are performed with the diﬀerent parameter\nconﬁgurations, and the corresponding results are evaluated.\nOne or more measurements can be used for the evaluation and thus for the selection of parame-\nters. First, the maximum measured accuracy can be decisive for the most likely optimal parameter\nconﬁguration. After the completion of the ﬁrst task T1, the maximum measured accuracy is identiﬁed\n(max(qm⃗p,t)). This value is assigned to a speciﬁc parameter conﬁguration ⃗p ∈P. Thus, the best\npossible parameter conﬁguration is determined for a speciﬁc model m, which has been measured during\nthe initial training phase. The disadvantage is that only one model parameter conﬁguration may\nlead to this result, which cannot be considered as representative. Second, the average accuracy over\nall measured values is a possible procedure, which is more representative than one maximum value.\nTherefore, the estimated parameter conﬁguration is more representative in terms of a constant quality\nof the model. The third and last measured value (or average over some of the last measuring points)\nis represented by the ﬁnal quality of the ML model. It is a realistic quality measure with respect to\napplication-oriented scenarios.\nThe determination of the best possible model m⃗p∗or parameter conﬁguration can be realized by\nmeans of more complex procedures. However, this only applies to the ﬁrst task T1 and models whose\ninitialization takes place within a development environment. The intended application may also have\nan inﬂuence on the used selection criterion. It is assumed that, for example, a model is initialized by a\nmanufacturer. Thus, the ﬁrst selection strategy can be used and the best possible accuracy results are\nexpected. In general, a hyper-parameter adjustment is impossible in certain application scenarios.\nThe initial grid-search is followed by the presentation of the second task T2. During re-training, the\naccuracy of the model is measured by using the test data of the second task (T test\n2\n). The related results\ncan be used to re-evaluate various parameters, as performed for T1. The possibility of a re-optimization\nof parameters, however, strongly depends on the application scenario, e.g., an adjustment of the used\nlearning rate. Re-optimization can be impossible due to various limitations, e.g., for embedded systems\nwith few resources. Therefore, the most restrictive evaluation approach is that no further optimization\ncan be performed. This very strong limitation has been relaxed for the present protocol, even though\nits implementation is straightforward. Thus, the ML models are granted a simple optimization that\nonly aﬀects the learning rate.\nIn addition to the evaluation of the active sub-task, the overall baseline accuracy is measured.\nThe baseline SLT D10 consists of only one task (T1), which includes all test samples of the entire\ndataset. However, neither the baseline nor the test data have to be used for any optimization steps.\nThe re-training step is repeated until all sub-tasks of an SLT are processed (see section 6.2).\nFor the interpretation of the resulting values, the accuracy of the baseline task D10 is crucial. It\nrepresents how much knowledge is retained after all training tasks have been completed. An accuracy\nof 100 % is the optimum. Initially, it is independent of the baseline. However, this value is supposed\nto be unrealistic if it is not reached by the baseline experiment. Therefore, the expected values are\nlimited to a maximum of the baseline results.\nAfter the completion of the entire training process, the question arises which criterion is used to\nidentify the “best” experiment. Another question is whether the last or the highest measured accuracy\nvalue is decisive (or another value). The following example illustrates the basic selection problem. A\nnon-uniform distributed SLT is considered as a starting point, e.g., D9-1. First, the knowledge from 9\nclasses has to be derived by the model. Thus, the expected accuracy for a 10-class problem is 90 % at\nmost for the ﬁrst task T1. Second, the knowledge of a single class needs to be added. Thus, a maximum\ngain in accuracy of 10 % can be expected from T2. Assume that the model is still partly subject to the\nCF eﬀect and that only little knowledge of T1 can be preserved. Consequently, a very low accuracy is\nachieved towards the end of the training process, e.g., 20 %. According to the maximum principle, the\nbest experiment with an accuracy of 90 % is selected. The experiment competes with models that may\nstill have 89 % accuracy at the end of the training process. For uniformly distributed SLTs (e.g., D5-5),\nthis information can be a critical factor, although it does not serve as a selection criterion. Finally,\nthe application-oriented test protocol speciﬁes that from the second task onwards, the last measured\npage 72\nBenedikt Pfülb\n\nContinual Learning Protocol\nDiscussion\naccuracy value is going to be used.\n6.4\nDiscussion\nIn this section, the relation between the evaluation protocol and real CL scenarios is discussed. Open\naspects concern the re-optimization of parameters after each task and the determination of the best\nparameter conﬁguration. Furthermore, the number of training iterations and task boundaries are\naddressed. Finally, a comparison with a prescient evaluation protocol is conducted.\nFirst Hyper-parameter Optimization\nThe ﬁrst point up to discussion concerns the hyper-\nparameter optimization. The initial grid-search is quite realistic and can be considered a standard\nprocedure. Problem-speciﬁc parameters can be determined with this procedure. Without it, an\nestimation of the resulting model quality is very diﬃcult. The same is true for non-continual learning\nscenarios. If no prior knowledge about a scenario is available, it is referred to as zero-domain-knowledge\nscenario. However, such scenarios are rare in the domain of real-world applications. This way, an\ninitial problem-dependent hyper-parameter optimization is justiﬁed.\nSubsequent Hyper-parameter Optimization\nThe subsequent parameter optimization is ques-\ntionable in the context of real-world CL applications. A second extensive optimization is problematic\nfor products that have to operate autonomously after their assembly/delivery. Lightweight devices\nwith very limited resources cannot perform hyper-parameter optimization. Due to memory limitations,\ncheckpoints of an ML model cannot be stored. However, these are required for optimization in some\ncases. The same applies to reference data. After a comprehensive initial optimization phase, an ML\nmodel should be able to add knowledge without much eﬀort. Moreover, an optimization should not\ninvolve the use of external services, such as data centers. This may be prohibited by either limited\nInternet access or data privacy issues.\nIn certain CL scenarios, an ML model cannot be adapted to new knowledge by a grid-search. A\nsecond grid-search is therefore only possible under speciﬁc circumstances. Nevertheless, the protocol\nallows for a second optimization. The implementation of a further optimization is justiﬁed by the\nfollowing arguments. In general, a very costly and complex ﬁne-tuning process is conducted for a\ngiven problem. Fine-tuning involves a ﬁne-grained adjustment of multiple hyper-parameters and the\ntraining process. The goal is to ﬁnd the best possible model conﬁguration. Since ﬁne-tuning cannot\nbe performed speciﬁcally for all datasets and each ML model, further optimizations are performed\nas a compensation. In order not to extensively distort the evaluation results, only one parameter is\noptimized. The learning rate as the chosen parameter has a fundamental inﬂuence on all models and\nproblems. As all models are aﬀected by this parameter, a fair comparison can be conducted.\nMeasurement Points\nAnother aspect that requires a discussion concerns the number and positioning\nof measurement points. ML models can strongly depend on the number of training iterations. This is\nespecially true for the performance measurement of CL tasks. If a model is re-trained for too long,\nit cannot retain the previously derived knowledge anymore. The estimation of an optimal model\nparameter conﬁguration may depend on the positioning of the measurement points. In theory, strongly\ndependent models need to be tested after each training iteration. However, these exhaustive tests are\ncomputationally expensive.\nThe measuring points are associated with checkpoints. Checkpoints represent model parameter\nconﬁgurations with regard to a performance state for a given training iteration. However, creating\na checkpoint for every measuring point would require an enormous amount of memory. Moreover,\ncheckpoints are required to restore the best measured condition. This procedure contradicts the\napplication-oriented requirements, especially after the ﬁrst sub-task. To avoid favoring vulnerable\nmodels, a uniform placement of measurement points is used for all problems and models.\nPerformance Criterion\nThe previously addressed problems are related to the determination of\nthe best model parameter conﬁguration. The general question is which measurement value should\nbe used. Even though section 6.3.4 describes conventional methods (e.g., maximum, last, average),\nother measurement values are possible. Many methods are conceivable, although the evaluation\nBenedikt Pfülb\npage 73\n\nDiscussion\nContinual Learning Protocol\nprotocol assumes that more resources are available for the initial optimization phase. In order to\nensure comparability, a uniform and realistic procedure is crucial. Thus, a selection according to the\nmaximum performance of a model is realistic – at least with regard to the initial optimization.\nFor further re-training steps, the procedure needs to be adapted to the CL scenario. In general,\nthe accuracy of the model can be tracked over time, as long as test data is available. However, due\nto resource constraints, the duration/capacity to store past samples is limited. At the same time, an\noptimization must be possible with the available resources (data, memory, computing power). If this is\nnot the case, an optimization is not applicable at all. This applies to certain real-world scenarios.\nDue to the previously discussed circumstances, the last measuring point is used as the selection\ncriterion for further sub-tasks. This approach is the most restrictive one, even though early-stopping\nmechanism would weaken it. Since early-stopping involves additional problem-dependent parameters\nbased on past test data, this mechanism is not applied. If a corresponding mechanism is already\nimplemented by the ML model, the presented requirements must not be violated. The same applies to\nmethods addressing the overﬁtting eﬀect.\nTask Boundaries\nAnother issue related to real-world applications concerns hard class boundaries.\nThe presented SLTs only describe abruptly changing data distributions. At the same time, classes\nwithin the sub-tasks are always disjoint. This condition is atypical as shown in the exemplary real-world\nscenario (see chapter 5). There are no hard borders and the distribution seems to change arbitrarily\nwithout any chance to inﬂuence this development. Moreover, it is not recognizable whether a concept\nor context drift/shift has occured (see section 2.3.1). This aspect causes problems with regard to\nthe evaluation of CF avoidance models within real-world CL scenarios. Since an evaluation in these\nscenarios is very diﬃcult to interpret, SLTs should simulate a CL task. Thus, the protocol only\ndescribes hard task boundaries in the beginning.\nThe use of task boundaries is due to two reasons. The ﬁrst one is related to the investigated models\nand their characteristics. In general, task boundaries are expected by many ML models and serve\nas a trigger for the various CF avoidance mechanisms (e.g., merging strategies). The CF avoidance\nmechanisms cannot be initiated without the speciﬁcation of boundaries. Unfortunately, the discovery\nof boundaries is usually not part of the models and has to be realized by additional methods. Generally\nspeaking, a new task can be identiﬁed easily based on the labels. This does not necessarily apply to\nreal-world applications. Therefore, only the deﬁned SLTs are provided for this protocol.\nThe second reason is the deﬁnition of a diﬀerent drift scenario. Synthetic (virtual) concept/context\ndrifts/shifts are easy to implement. Moreover, the learning objective would be the same for such\nscenarios. However, many procedures/models would not work without task boundaries. Therefore,\nthe investigation of diﬀerent types of changing data distributions (e.g., see section 2.3.1) is subject to\nfuture research.\nDatasets\nAnother point concerns the datasets used for the evaluation. In section 6.1, various image\ndatasets are presented. The various advantages of image datasets are discussed in the following. Image\ndatasets have many features (high-dimensional). Moreover, the label and sample can be validated by\nvisual inspection. These visual or similar datasets can be found in many studies, even though they are\nhard to assess with regard to their diﬃculty. Only reference values of prediction accuracies can be\nused for this estimation of diﬃculty. However, prediction results are not based on simple standard\nmodels (regarding the architecture) but on highly optimized ones, e.g., AlexNet Krizhevsky, Sutskever,\net al. 2017. Convolutional Neural Networks (CNNs) are particularly interesting in image processing\nmodels, as they are biologically inspired and mimic the process of vision.\nDespite “simple” model architectures, the diﬃculty of individual datasets can be considered in\nrelation to the baseline experiments. Conducted experiments for the SLT D10 (including all classes\nin one task T1) are assumed to be the baseline, which provides an approximate upper bound with\nrespect to a model’s quality. Accordingly, the diﬀerence between the baseline and CL tasks indicates if\na certain model successfully avoids the CF eﬀect. Implications and statements are strengthened by the\ninvestigation of several diﬀerent datasets (see section 6.1).\nMore Realistic Evaluation Protocol\nA fundamental question is whether the protocol allows\nfor a more realistic evaluation of CF avoidance models. To answer this question, the protocol is\ncompared to the prescient protocol, which is summarized in algorithm 6.2. Algorithm 6.2 presents the\npage 74\nBenedikt Pfülb\n\nContinual Learning Protocol\nConclusion\ncompact version of the prescient protocol. In the ﬁrst training section, an ML model with diﬀerent\nparameter constellations is trained (i.e, hyper-parameter optimization). Training starts with the ﬁrst\ntask T1 (see line 1), which is similar to algorithm 6.1. For clarity, the number of training iterations\nwill be abbreviated by tmax = #(T train\n1\n)\nB\n· Etrain. SLTs consisting of only two tasks (T1 and T2) are\nused. Subsequently, a model is re-trained, which is based on a reduced parameter optimization (⃗p+)\nas displayed in line 2. Lines 3 and 4 represents the training loop where the re-training of the model\nm⃗p,⃗p+ is performed with training data from T2.\nBy applying this minor strategic change, a decisive diﬀerence compared to the realistic protocol\n(algorithm 6.1) is expected. Based on the ﬁrst parameter optimization, not only the best model is\nre-trained anymore. Instead, all m⃗p conﬁgurations are re-trained with T train\n2\n. Nevertheless, the reduced\nparameter optimization is performed, represented by the ⃗p+. As a last step, the experiment with\nthe maximum accuracy at any given time is selected from the comprehensive collection of parameter\nconﬁgurations.\nAlgorithm 6.2: The prescient evaluation protocol.\nData: model m, SLT with sub-tasks T1, T2, hyper-parameter value set P\nResult: best model m⃗p∗\n1 initial training of m⃗p on T train\n1\nfor tmax iterations\n2 forall parameter conﬁguration ⃗p+ ⊂⃗p∗do\n// parameters to be optimized subsequently\n3\nfor t ←0 to tmax iterations do\n// re-training of m⃗p on T2\n4\ntrain(m⃗p,⃗p+, T train\n2\n)\n5\nqm⃗p,⃗p+,t ←test(T test\n1\n∪T test\n2\n, t)\n6 m⃗p∗←m⃗p,⃗p+ with best qm⃗p,t,⃗p+\n// ﬁnd best model with max. accuracy on T1 ∪T2\nBy applying the prescient protocol of algorithm 6.2, the results are supposed to be comparable to related\nstudies. Results obtained by the application of the prescient protocol should at least prove whether\nthe code base is valid. Signiﬁcantly worse results are expected from the developed realistic evaluation\nscenario. This is particularly due to the application-oriented requirements and their implementation.\nAs long as the conditions are comparable, models with similar results for CL tasks (SLTs) and baseline\nexperiments can be used for applications in CL scenarios.\nMoreover, it is expected that the model performance applies to all SLTs and all datasets. Considering\nthe implementation of other requirements into the protocol, this assumption is no longer valid. Diﬀerent\ntypes of changing data distributions are not implemented as part of the protocol. Furthermore, the\nevaluation scheme can be modiﬁed for, e.g., few-shot learning or semi-supervised approaches. Depending\non the adaptation, the chosen metric as well as the determination of the “best” parameter conﬁguration\nhave to be modiﬁed. The same applies to the used datasets.\n6.5\nConclusion\nThe presented realistic evaluation protocol is intended to measure the CL performance of CF avoidance\nmodels under application-oriented constraints. The protocol takes several real-world requirements into\naccount (see section 6.3.1). They include, for instance, the restriction of access to past or future data,\nwhich are not available under realistic conditions. As expected, real-world requirements aﬀect the CL\nperformance of CF avoidance models. This contradicts the statements of previous investigations by\nother authors.\nBased on the evaluation protocol, more precise statements regarding the avoidance of the CF eﬀect\ncan be expected. First of all, an ML model has to meet the collected real-world requirements. The\nsubsequent evaluation of the protocol is supposed to reveal to what extent CF can be reduced by\nmeans of the respective avoidance models. The investigated models should achieve similar results\nto those in a non-continual learning scenario (referred to as baseline experiments). In this context,\na model must achieve acceptable results on multiple problems (datasets, see section 6.1) and for\nvarious diﬃculties (Sequential Learning Tasks (SLTs), see section 6.2) . If one or more ML models\ncan demonstrate an acceptable CL performance for this protocol, additional and stricter requirements\nBenedikt Pfülb\npage 75\n\nConclusion\nContinual Learning Protocol\nshould be implemented.\npage 76\nBenedikt Pfülb\n\n7.\nApplication-oriented Evaluation of Proposed\nContinual Learning Methods\nChapter Contents\n7.1\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n7.1.1\nModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n7.1.2\nHyper-Parameter Selection . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n7.1.3\nReproduction of Previous Results by the Prescient Evaluation Protocol . .\n83\n7.1.4\nRealistic Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n7.2\nDiscussion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n90\n7.3\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\nIn this chapter, the investigation of diﬀerent catastrophic forgetting (CF) avoidance models is presented.\nThe application-oriented investigation protocol described in chapter 6 constitutes the basis for the\nevaluation. Models from chapter 3 meeting the protocol’s requirements are examined. The evaluation\nprotocol implements various requirements that originate from real-world application scenarios in the\ncontext of continual learning (CL).\nContributions\nThe goal of this chapter is to investigate diﬀerent CF avoidance models in order\nto determine their CL performance. Accordingly, the protocol proposed in chapter 6 is applied. Two\nresearch questions are answered within this chapter. The ﬁrst answer responds to which CF avoidance\nmodels exist (RQ 2.1). The second one addresses which model can be harmonized with the real-world\nrequirements from the investigation protocol (RQ 2.2). This will ultimately answer research question\nRQ 2 and to what extent the models help reduce the CF eﬀect in CL applications.\nFirst, the models were examined according to a prescient version of the investigation protocol\nthat does not meet the real-world requirements. The results are comparable to those from related\nwork, so that the implementation can be considered valid. However, the results according to the\nreal-world protocol described in chapter 6 show a diﬀerent outcome. Under more realistic conditions,\nthe investigated models and techniques show that the CF eﬀect can only be suppressed for certain tasks.\nIn a direct comparison, Elastic Weight Consolidation (EWC) has the best CL properties. Likewise, the\nevaluation of a model that does not meet the requirements does not exhibit a perfect CL performance.\nThe ﬁnding of this chapter is that none of the investigated CF avoidance models can eﬀectively mitigate\nthe CF eﬀect under real-world conditions.\nStructure\nFirst of all, the experimental setup for the applied investigation procedure is introduced.\nIn this context, a brief summary of chapter 6 is provided. In addition, the model-speciﬁc parameters\nand the associated grid-search are presented. Subsequently, the experimental results of the grid-search\nare summarized. The initial experiments are evaluated according to the prescient protocol. This\nevaluation is supposed to show the validity of the implementation. The main focus of this chapter\nare the conducted experiments based on the realistic protocol. Finally, the results are discussed and\nimplications for this work are presented.\nBenedikt Pfülb\npage 77\n\nExperimental Setup\nEvaluation of Continual Learning Methods\n7.1\nExperimental Setup\nThe experimental setup describes the procedure of the investigation. Experiments are performed by\nusing the prescient (see section 6.4) and the realistic protocol (see chapter 6). The goal is to derive a\nstatement concerning the occurrence of the CF eﬀect based on the experiments. At the same time, the\nmachine learning (ML) models can be veriﬁed for their CL capabilities in a realistic scenario. The\ngeneral process is summarized in ﬁgure 7.1. The process describes the step by step presentation of\nlearning tasks, so-called Sequential Learning Tasks (SLTs) (see ﬁgure 7.1a), to an ML model (see\nﬁgure 7.1b).\nAs part of this process, diﬀerent requirements from real-world scenarios have to be respected.\nDuring a training task, only data of the current sub-task is available (sub-task T1 in blue or T2 in\ngreen). For evaluation purposes, the baseline test data is used during the entire training process. The\nbaseline dataset (D10) consists of all used classes (highlighted in red). In addition to the scheme shown\nin ﬁgure 7.1a, other constraints limit the selection of hyper-parameters during the process. One of\nthe constraints is that no model selection (architectural or parameter decisions) may be performed\nretroactively. The model selection needs to be completed after the ﬁrst sub-task T1.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a) Example SLT D5-5.\n(b) Training and testing scheme.\nFigure 7.1: Parts of the evaluation protocol.\nBefore the application-oriented evaluation protocol is applied, it has to be ensured that comparable\nresults can be achieved with the code base. For this purpose, the prescient protocol described in\nsection 6.4 is utilized (see algorithm 6.2). After reproducing the results from related work, the code\ncan be reused for the realistic evaluation protocol. Initially, a selection of speciﬁc SLTs is processed\n(see section 6.2: D10, D5-5, D9-1 and D10-p10). This list includes only SLTs, which consist of exactly\ntwo sub-tasks. As fewer sub-tasks are considered to be easier, it should at least be possible to achieve\nsatisfactory results for this kind of SLTs.\nIn order to eliminate the diﬃculty of diﬀerent problems, multiple datasets are utilized (see section 6.1,\nonly MNIST, FashionMNIST, NotMNIST, EMNIST, Devanagari, MADBase, CIFAR-10, SVHN and\nFruits 360). If all investigated parameter conﬁgurations related to a model show clear positive results\ntowards the prevention of the CF eﬀect, additional investigations are performed. As the exploration of\nmany diﬀerent hyper-parameters is very computationally expensive, more models can be examined. At\nthe same time, the hyper-parameter selection ensures that a real mitigation of the CF eﬀect is not\nachieved “randomly”.\nIt is expected that all investigated models will provide comparable results to the basic tests according\nto the prescient protocol The realistic investigation protocol is applied to examine the occurrence of\nthe CF eﬀect under real-world conditions. Diﬀerent variants of knowledge loss can be distinguished,\nwhich are outlined in ﬁgure 7.2. Without the application of a CF avoidance technique, the typical CF\nbehavior can be observed, as experimentally shown in ﬁgure 7.2a. According to Goodfellow, Mirza, et al.\n(2013), the application of dropout (Hinton, Srivastava, et al. 2012b) should reduce the eﬀect. Other\npromising mechanisms include the frequently cited EWC. The decisive factor is to what extent the\neﬀect is mitigated. A linear loss, as shown in ﬁgure 7.2b, would already be a signiﬁcant improvement.\nHowever, new challenges arise with linear forgetting. One of them is related to the identiﬁcation\nof the best model conﬁguration, and whether the training process has to be stopped. Early-stopping\nis closely related to other constraints. One of them concerns the access to reference data aiming at\nthe determination of an optimal model conﬁguration. In the ideal case, the CF eﬀect is completely\neliminated so that the model can be considered suitable for real-world CL scenarios (see ﬁgure 7.2c).\nIf one or more models show the absence of the CF eﬀect, the evaluation would be performed under\nmore restrictive conditions and consider more complex SLTs and datasets. In this context, certain\noptimizations could be omitted (see section 6.4), such as adjusting the learning rate for follow-up tasks.\npage 78\nBenedikt Pfülb\n\nEvaluation of Continual Learning Methods\nExperimental Setup\nThus, further vulnerabilities of the CF avoidance models can be identiﬁed.\naccuracy (   )\n0\n50\n100\ntask\n(a) Occurrence of the CF eﬀect.\n100\naccuracy (   )\n0\n50\ntask\n(b) Mitigated/linear CF eﬀect.\n100\naccuracy (   )\n0\n50\ntask\n(c) Prevented CF eﬀect.\nFigure 7.2: Diﬀerent proﬁles of the CF eﬀect.\n7.1.1\nModels\nThe models selected for evaluation are brieﬂy introduced in section 3.2. The selection is based on\nthe criterion whether a model can be aligned with the requirements of the evaluation protocol (see\nchapter 6). In addition, the author’s code base has to be available in order to eliminate implementation\ninterpretations. This is crucial, although even a faulty re-implementation cannot guarantee comparable\nperformance. Thus, only models with a reference implementation can be examined. Details about the\ninvestigated models are presented in the following.\nFully-Connected (FC)\nThe simplest model is the one referred to as Fully-Connected (FC). It\ncorresponds to a standard feed-forward Deep Neural Network (DNN), which is composed of several\nlayers consisting of a ﬁxed number of artiﬁcial neurons (see section 2.2.1). The output function is\nconstituted by the Rectiﬁer Linear Unit (ReLU) function, which is used for all layers. However, the\nlast layer is terminated by a softmax readout. The DNN is optimized by minimizing the cross-entropy\n(see section 2.2.2.1). The number of hidden layers L and the number of artiﬁcial neurons S are\nhyper-parameters.\nThis simple variant does not comprise any mechanism to counteract the CF eﬀect. As a consequence,\nit serves as a reference model. Moreover, it constitutes the basis for a modiﬁcation (application of\ndropout), which is also investigated. In general, the label fully connected (FC) refers to the type\nof connection between the single layers (also referred to as dense layer). It is characterized by the\nfact that each neuron of the preceding layer is connected to each neuron of the subsequent layer (see\nﬁgure 2.5).\nConvolutional Neural Network (CNN)\nCNNs were a decisive development step for image\nprocessing. The idea originates from biology and mimics operations of the visual cortex in living\norganisms. Early developments are attributed to LeCun, Haﬀner, et al. (1999). Since then, CNNs have\nbecome the standard mechanism in image recognition. A comprehensive description is available in\nBehnke (2003).\nCNNs owe their performance to the operations convolution and pooling. Convolutional layers\ntransform the input signal based on ﬁlters that correspond to the receptive ﬁelds. Each ﬁlter consist of\none or multiple kernels. Kernels are deﬁned by matrices that apply the dot-product, which realizes\nvarious image processing operations. These include, for example, the highlighting of edges or blurring.\nThe result of each ﬁlter is a so-called feature map, whose activations are passed on to the next layer.\nThe height and width of the receptive ﬁelds are hyper-parameters that have to be speciﬁed.\nThe same applies to the kernel and step size, which speciﬁes how the ﬁlter is shifted over the\ninput signal. Filters have to be shifted over the edges of the input data (e.g., images) which results in\nan undeﬁned operation. Various padding techniques are used to deﬁne this area, e.g., set values to\nzero or the same value as the border pixels. In addition, pooling layers are introduced which reduce\nthe dimensions. This simple mechanism combines several pixel values (which is also deﬁned as an\nimage area) by using various options. Diﬀerent types of pooling operations can be applied: maximum,\naverage or sum, etc. Max-Pooling, for example, aggregates multiple values from an image area, e.g.,\nBenedikt Pfülb\npage 79\n\nExperimental Setup\nEvaluation of Continual Learning Methods\n2×2 pixels, on the basis of the maximum value.\nThe structure of a typical CNN consists of one or multiple convolutional ﬁlters and pooling layers\napplied alternately. Each one can be parameterized diﬀerently. A CNN is usually terminated by\na linear classiﬁcation layer consisting of artiﬁcial neurons. Weights and biases constitute the free\nparameters of the linear layer. The same is true for the kernels of the convolutional layers. In order to\nreduce the number of parameters, several ﬁlters share the same weights. Accordingly, layers closer\nto the input signal are responsible for local structural properties, such as the enhancement of edges.\nDeeper layers are responsible for details, e.g., textures. As a model’s architecture is usually deﬁned by\nhyper-parameters, complex but proven structures are deployed.\nThe architecture applied in this work consists of two convolutional layers. Each of them contains 32\nand 64 ﬁlters of the size 5×5. The output function is ReLU. Max-pooling with a size of 2×2 pixels is\nintroduced for the reduction of dimensions. The ﬁnal linear classiﬁcation layer consists of 1024 neurons.\nThe cross-entropy loss function is minimized for training.\nAlthough CNNs provide an enormous performance boost for image processing/classiﬁcation, they\nare aﬀected by CF. Similar to FC models, standard CNNs do not have any mechanism to reduce\nor control the CF eﬀect. Therefore, standard CNNs serve as reference models for the application of\nmechanisms to avoid the CF eﬀect.\nElastic Weight Consolidation (EWC)\nEWC as introduced by Kirkpatrick, Pascanu, et al. (2017)\nis probably one of the most well-known and most cited CF avoidance models. The model is classiﬁed as\nregularization approach (see section 2.3.5). Changing important parameters for past tasks is punished\nby adding a penalty term to the loss function (see equation 7.1). With the loss function proposed\nin equation 7.1, regular DNN architectures can be trained. The ﬁrst part of equation 7.1 (LB(θ))\ncorresponds to the loss of the current task B. As the determination of the posterior probability\ncannot be resolved, the Laplace approximation is introduced. A Gaussian distribution is utilized\nto determine the posterior for the individual parameters. The mean corresponds to the parameter\nθ∗\nA. The covariance is represented by the diagonal precision matrix, which is the diagonal of the\nFischer information matrix F. The matrix can be determined by means of the ﬁrst-order derivative.\nIt corresponds to the second derivative of the loss near a minimum. At the same time, the matrix\nis positive semi-deﬁnite. For each parameter i, the factor λ deﬁnes the importance of the previous\ntasks. In this work, the TensorFlow implementation proposed by Kirkpatrick, Pascanu, et al. (2017) is\nintegrated into the evaluation framework.\nL(θ) = LB(θ) +\nX\ni\nλ\n2 Fi(θi −θ∗\nA,i)2\n(7.1)\nEquation 7.1: EWC loss function (taken from Kirkpatrick, Pascanu, et al. 2017).\nLocal Winner Takes All (LWTA)\nSrivastava, Masci, et al. (2013) introduce LWTA, which is\nclassiﬁed as parameter isolation approach. The idea is derived from neuroscience and it is intended to\nmimic the competitive processes between neurons. This competitive process is realized by the LWTA\noutput function (referred to as transfer function), which enables or disables a neuron’s activation (or\nsets it to 0). This mechanism is implemented by introducing blocks, which in turn consist of artiﬁcial\nneurons. Thus, each block comprises n neurons. Due to varying activations of neurons, sub-graphs are\ncreated. This state is illustrated in ﬁgure 7.3. Figure 7.3 represents blocks containing two neurons,\nwhereas only one of them is active at a time (green). The activations of the gray neurons, however, are\nnot passed on.\nThe “competition/interaction function” speciﬁes how the neurons are de-/activated. In the work of\nSrivastava, Masci, et al. (2013), the “hard winner-take-all” function is speciﬁed. As shown in equation 7.2\ni serves as index for the block and j for the neurons within the block. Accordingly, the parameters\nof an active neuron are adjusted or the activation signal is transmitted during the training process.\nLWTA can be considered as an extension of FC models allowing for the same architectures.\npage 80\nBenedikt Pfülb\n\nEvaluation of Continual Learning Methods\nExperimental Setup\nFigure 7.3: Exemplary LWTA block network with n=2 (compare Srivastava, Masci, et al. 2013).\nyj\ni =\n(\nhj\ni\nif hj\ni ≥hk\ni ,\n∀k ∈{1, ..., n}\n0\notherwise\nwhere hi = f(w⊤\nijx)\n(7.2)\nEquation 7.2: LWTA competition/interaction function described by Srivastava, Masci, et al. 2013.\nDropout\nHinton, Srivastava, et al. (2012b) suggest dropout as a solution to the “overﬁtting” problem\nin DNNs (see section 2.2.2.5). Dropout disables a neuron’s activation (sets it to 0) based on a random\nfactor. It is implemented as an output function (transfer function), which can be applied for each\nartiﬁcial neuron. For reasons of simplicity, dropout is often implemented as layer which does not\ncontain any adjustable parameters. However, diﬀerent dropout probabilities can be deﬁned for each\nlayer. One probability is usually deﬁned for the input layer and another one for all other hidden layers.\nAs proposed by Goodfellow, Mirza, et al. (2013), dropout should mitigate the CF problem. Dropout\nis applied to FC-DNNs and CNNs, resulting in Dropout (D)-FC and D-CNN models. Generally\nspeaking, dropout can be applied to all DNNs. EWC already contains the application of dropout and\nLWTA is an output function itself. For this reason, dropout is not integrated into these two models.\nIncremental Moment Matching (IMM)\nIMM by Lee, Kim, et al. (2017) aims at “resolving” the\nCF problem. It is similar to EWC and can be classiﬁed as an approach based on regularization. IMM\ndescribes the approximation of the posterior distribution of DNN parameters by means of a Gaussian\ndistribution. Diﬀerent approximation variants are proposed by Lee, Kim, et al. (2017): Mean-IMM and\nmode-IMM. “Mean-IMM averages the parameters of two networks in each layer, using mixing ratios\nαk with PK\nk αk =1” (Lee, Kim, et al. 2017). The loss function (stated in equation 7.3) is minimized\nbased on the KL-divergence (a fundamental of Bayesian Neural Networks (BNNs)). The mean-IMM\nvariant ignores the variances Σ∗\n1:K.\nµ∗\n1:K, Σ∗\n1:K = arg min\nµ∗\n1:K,Σ∗\n1:K\nK\nX\nk\nαkKL(qk||q1:K)\nµ∗\n1:K =\nK\nX\nk\nαkµk\nΣ∗\n1:K =\nK\nX\nk\nαk\n\u0000Σk + (µk −µ∗\n1:K)(µk −µ∗\n1:K)⊤\u0001\n(7.3)\nEquation 7.3: IMM loss function proposed by Lee, Kim, et al. (2017).\nIn contrast to the mean-IMM variant, the mode-IMM variant uses the covariance information. Similar\nto EWC, an approximation is utilized for the determination based on the Fisher information matrix F.\nBenedikt Pfülb\npage 81\n\nExperimental Setup\nEvaluation of Continual Learning Methods\nWithout the approximation the determination of the posterior distribution would be impractical. The\ndetermination is represented in equation 7.4, where the inverse F is used.\nFk = E\nh ∂\n∂µk\nln p(˜y|x, µk) ·\n∂\n∂µk\nln p(˜y|x, µk)⊤i\n(7.4)\nEquation 7.4: Fisher information matrix for IMM speciﬁed by Lee, Kim, et al. 2017.\nIn addition to these two moment matching methods, various transfer techniques are presented by Lee,\nKim, et al. (2017): Weight-transfer, L2-transfer, Drop-transfer and combinations. As stated by Lee,\nKim, et al. (2017), only the weight-transfer technique is critical for the CL performance. It is thus\napplied as default in this work.\nWeight-transfer deﬁnes the transitioning of a model’s state from one task µk−1 to another µk.\nThe transition of two solutions (model parameter conﬁgurations) is an interpolated point in between.\nThe merging parameter α is varied for the evaluation of an IMM model. Although the IMM model\ncontradicts the presented requirements from chapter 6, it is considered a state-of-the-art approach for\nCL applications. The approach, however, violates one of the deﬁned real-world scenario constraints:\nPast data are required to adjust the merging parameter α and to determine an acceptable solution.\n7.1.2\nHyper-Parameter Selection\nHyper-parameters’ eﬀects are crucial for ML model performance related to a speciﬁc problem. Therefore,\na grid-search is performed to reduce the inﬂuence of problem-dependent parameters. Usually, various\nparameter combinations are explored and the best model (conﬁguration) is selected based on an\nappropriate metric. The larger the range of parameter values and the smaller the steps, the better\nresults can be expected. At the same time, the number of performed experiments increases with\nmore hyper-parameters and varying values. A signiﬁcant amount of computing resources is required\nto conduct a comprehensive investigation. For the implementation of this study, the environment\npresented in section 4.3 is utilized.\nNot all conceivable parameter constellations can be examined, as an almost inﬁnite number of\nthem exists. The architecture of the DNNs, for example, belongs to these parameter constellations.\nHighly specialized network architectures are frequently used. They usually have been optimized for a\nsingle problem. The goal of this study is neither to design the best possible network architecture nor\nto determine the optimal parameters for a particular problem. The objective is rather to conduct an\nappropriate review of the CL performance of various CF avoidance models under real-world conditions.\nThe hyper-parameter optimization is primarily executed to emphasize the validity of the results. This\nis how the independence of the chosen parameters is secured.\nIn the present work, only deep learning models based on DNNs are examined. The DNN architecture\nis one of the hyper-parameters, which has to be adapted to the corresponding problem. The architecture\nis composed of the number of layers L and the artiﬁcial neurons S in each layer. The variations of these\ntwo parameters are selected based on related work. L∈{2, 3} and S ∈{200, 400, 800} are investigated\nas possible combinations.\nThe DNNs applied in this work consist of an input layer (Input), the optional application of\nDropout (D), which is followed by the ﬁrst hidden layer of Fully-Connected (FC) neurons. Again,\nlayers are terminated with a ReLU output function. The last layer is terminated with a softmax\n(SM) (see section 2.2.1). For a three-layer architecture (L = 3), the structure is deﬁned as follows:\nInput-FC1-D-ReLU-FC2-D-ReLU-FC3-SM.\nCNNs are excluded from the architectural choice. Varying the number of possible model parameters\nwould exceed the resulting number of experiments. Therefore, a predeﬁned CNN architecture is\nselected which achieves reasonable results (proposed by Cireşan, Meier, et al. (2011)). In addition\nto the standard Fully-Connected (FC) layers, convolutional (C) and max-pooling (MP) layers are\nintroduced. The structure is deﬁned as follows (including the application of dropout (D)): Input-C1-\nMP-D-ReLU-C2-MP-D-ReLU-FC3-SM. Further hyper-parameters are set for two convolutional (C)\nlayers L=2 with 32 respectively 64 ﬁlters of the size 5×5 and a max-pooling layer of the size 2×2.\nThe last layer consists of 1024 artiﬁcial neurons.\npage 82\nBenedikt Pfülb\n\nEvaluation of Continual Learning Methods\nExperimental Setup\nIn addition to the model architecture, the learning rate ϵ is a crucial parameter. However, the\nlearning rate is independently determined for diﬀerent sub-tasks. For T1 a selection is made from\nϵT1 ∈{0.01, 0.001}. For all further re-training steps, ϵTc ∈{0.001, 0.0001, 0.00001} can be selected, where\nc>1. The standard Momentum Optimizer is applied as an optimizer with the parameter µ=0.99 (see\nSutskever, Martens, et al. 2013). The batch size is B=100 (for training and testing), and the number\nof training epochs is set to E =10 for all investigated models.\nIn addition to the general hyper-parameters, model-speciﬁc parameters have to be deﬁned. In order\nto specify various parameters suggestions from related works are applied. For the LWTA model, the\nnumber of the LWTA blocks is set to 2 for all experiments (see Srivastava, Masci, et al. 2013). This\nis due to the initial investigation of SLTs with only two tasks. For dropout the probability is set to\n0.5 for hidden layers and 0.2 for the input layer (see Goodfellow, Mirza, et al. 2013). To be precise, a\nprobability of 0.5 is applied for CNNs in case of both, hidden and input layers. Kirkpatrick, Pascanu,\net al. (2016) outline the importance factor λ for EWC which is adjusted by a grid-search. At the same\ntime, the value λ=1/ϵ is speciﬁed in the publicly available code base. The approach to deﬁne λ is used\nfor EWC experiments, as the learning rate is varied anyway. For IMM, a vanilla Stochastic Gradient\nDescent (SGD) optimizer is utilized. In order to reﬁne the posterior optimization value α the range\nbetween 0 and 1 with a step size of 0.01 is chosen.\n7.1.3\nReproduction of Previous Results by the Prescient Evaluation Proto-\ncol\nFirst of all, the results from related work will be reproduced in order to demonstrate the validity of the\ncode base. Accordingly, the prescient evaluation protocol is utilized (see algorithm 6.2). The basis for\na statement regarding the validity is a subsequent evaluation and selection of a step-wise parameter\noptimization. The main characteristic of the prescient protocol is that the best parameter conﬁguration\nis determined in retrospective. However, the prescient evaluation protocol contradicts some real-world\nrequirements. Therefore, the statements on obtained results cannot necessarily be transferred to all\napplication-oriented CL scenarios. For reasons of simplicity, only one dataset is examined by means of\nthe prescient protocol (MNIST, see section 6.1).\nThe results of the experiments with the prescient protocol are illustrated in table 7.1. Note that\nthe maximum accuracy is provided depending on the corresponding SLT. Thus, a maximum of 50 %\naccuracy can be obtained for the investigation of the ﬁrst sub-task T1 for a D5-5 SLT. The same applies\nfor the second sub-task T2 of D5-5. The case is diﬀerent for a D9-1 SLT, where the ratio is 90 % for T1\nand 10 % for T2. Therefore, the achievable accuracy after T1 is 90 %. The rightmost column shows the\ntest results for the permuted dataset (see deﬁnition of D10-p10 section 6.2). The test results indicate\nthat the permuted task is slightly aﬀected by the CF eﬀect. In fact, this is true for all investigated ML\nmodels. To conclude, even simple ML models such as Fully-Connected models demonstrate acceptable\nresults.\nAt a ﬁrst glance, the results for D9-1 SLTs suggest that the CF eﬀect can be controlled. The\ncorrespondance of the resulting values (about 90 %) to the maximum measured accuracy reveals that\nthese results are obtained during the ﬁrst task T1. The real CL performance is evident in the results of\nthe D5-5 SLTs. An accuracy between 50 and 60 % means that only little knowledge is left from the ﬁrst\ntask T1 or derived from T2. The application of Dropout (D) does not seem to prevent the CF eﬀect.\nThe results for LWTA and EWC reveal a diﬀerent situation. LWTA is signiﬁcantly better on average\nthan all models that do not address the CF eﬀect. However, EWC achieves by far the best results\nfor the prescient evaluation protocol. The superior performance of EWC for all investigated SLTs is\nevident. Although all tasks are performed on the same dataset, the results show a certain variance.\nThis is due to the inﬂuence of the random initialization of the models, but also to diﬀerent diﬃculties\nof the SLTs. Thus, the assignment of classes to individual tasks may be another inﬂuencing factor.\n7.1.4\nRealistic Evaluation Results\nIn this section, the results of the experiments with the realistic protocol are presented. This investigation\nprotocol implements various requirements that arise in application-oriented CL scenarios. According\nto the protocol (represented in chapter 6), it is impossible to access future or past data after a sub-task\nhas been processed. Therefore, a model has to be selected for the ﬁrst sub-task T1. After that step, a\nmodel’s architecture is ﬁxed and can no longer be changed for the following tasks.\nBenedikt Pfülb\npage 83\n\nExperimental Setup\nEvaluation of Continual Learning Methods\nTable 7.1: Results of the prescient evaluation protocol (accuracy in %).\nmodel\nSLT\nD5-5\nD9-1\nD10-p10\nD5-5a D5-5b D5-5c D5-5d D5-5e D5-5f D5-5g D5-5h D9-1a D9-1a D9-1c\nFC\n69\n63\n58\n65\n61\n58\n61\n69\n87\n87\n86\n97\nD-FC\n58\n60\n61\n66\n61\n54\n63\n64\n87\n87\n85\n96\nCNN\n51\n50\n50\n50\n50\n50\n51\n49\n89\n89\n87\n95\nD-CNN\n51\n50\n50\n50\n50\n50\n50\n49\n81\n84\n87\n96\nLWTA\n66\n68\n64\n73\n71\n62\n68\n71\n88\n91\n91\n97\nEWC\n92\n92\n91\n93\n94\n94\n89\n93\n100\n100\n100\n100\nThe choice of the best possible parameter conﬁguration can be determined on the basis of two\ncriteria. First, the best experiment is identiﬁed by the maximum measured performance criterion. The\nbest state of the model parameters has to be restored retroactively. However, the best criterion is\nusually very diﬃcult to implement in applications consisting of multiple sub-tasks. Therefore, the\nbest criterion is only applicable during the ﬁrst optimization phase. For subsequent CL tasks, this\npossibility is no longer valid due to memory and computational capacities. If the duration of the\ntraining is crucial, the model has to include a procedure for early-stopping. Again, the implementation\nof the latter has to be possible without accessing data of past sub-tasks.\nIn order to conduct experiments, the code base of the investigated models is integrated into the\ndeveloped evaluation framework. As part of this process, the code is adapted or extended by several\ninterfaces. This includes, for example, the basic training- or test-step function. Based on these\nfunctions, a batch of data is presented to a model for training or testing. Parameters and sequences\nof function calls are performed by the framework, e.g., selecting a batch of data from a dataset for a\ngiven task. In order to obtain representative results, the diﬀerent datasets are used for the evaluation\npurposes: MNIST, FashionMNIST, Fruits 360, MADBase, NotMNIST, Devanagari, CIFAR-10 and\nSVHN (see section 6.1). These datasets are divided into diﬀerent types of CL tasks, which are referred\nto as SLTs. For the ﬁrst investigation, a limited selection of SLTs is made, consisting of D5-5 and D9-1\ntasks and the baseline task D10: D5-5a, D5-5b , D5-5c, D5-5d, D5-5e, D5-5f, D5-5g, D5-5h, D9-1a, D9-1a,\nD9-1c and D10-p10. Each of the presented SLTs consists of two CL sub-tasks (T1 and T2). Two learning\nrates are being varied for the ﬁrst sub-task, and three for the second sub-task. 7 models are examined:\nFC, D-FC, CNN, D-CNN, LWTA, EWC and IMM. Two- and three-layer DNNs architectures are\ninvestigated with three diﬀerent sizes to choose from in each layer. Approximately 100 000 parameter\ncombinations are derived, which are performed and evaluated in the experimental setup.\nIn table 7.2, results are presented in an aggregated format which is due to the high number of\nmeasured values. The aggregated accuracy values (in %) are given, whereas the ﬁrst value is the\nbest and the second is the last measured value. The achieved values are color coded according to the\nscale given below. For the aggregation, the best experiment for each SLT (m⃗p∗see algorithm 6.1) is\nselected. The minimum measured accuracies from each SLT category indicate the ﬁnal result. Thus, the\nstatement should be valid with respect to the applicability of all SLT-variants (or diﬃculty levels). The\ncolor scheme applied to table 7.2 refers to the best result, even if it contradicts the application-oriented\nrequirements. Dark or black background indicates a value below the limit of 50 % for D5-5 (and\npermuted task) and respectively 90 % for D9-1 SLTs. A bright or white background illustrates good\nCF avoidance capabilities and thus acceptable CL performances. It becomes obvious that most of the\nresults are below the expected accuracy limit, despite the consideration of the best criterion.\nThe last criterion, second accuracy value in table 7.2, shows equivalent or signiﬁcantly worse\nperformance values. These values correspond to the last measured accuracy value after completion\nof a complete SLT training.\nIt is evident that the permuted SLTs are characterized by a very\nhigh performance. Therefore, permuted tasks seem to be inappropriate for the evaluation of the\nCF eﬀect. Moreover, all of the investigated models provide moderate results for all other tasks.\nIn order to present the experimental results in greater detail, the trends for some individual models\nare depicted with fewer aggregations. Each of the following illustrations summarizes all measurement\npoints of a particular SLT type. The beneﬁt is that the CL performance trends of a model can be\nvisually inspected for multiple datasets at the same time.\npage 84\nBenedikt Pfülb\n\nEvaluation of Continual Learning Methods\nExperimental Setup\nTable 7.2: Overview of the results according to the realistic evaluation protocol (accuracy in %).\nDataset\nSLT\nModel\nFC\nD-FC\nCNN\nD-CNN\nLWTA\nEWC\nCIFAR-10\nD5-5\n30/28\n26/23\n31/10\n30/.18\n31/30\n32/20\nD9-1\n45/10\n37/10\n45/10\n48/10\n45/10\n36/08\nD10-p10\n54/52\n44/43\n52/50\n56/55\n54/51\n57/46\nDevanagari\nD5-5\n49/42\n46/26\n49/45\n49/11\n11/10\n40/23\nD9-1\n86/10\n84/09\n88/10\n89/09\n86/09\n88/09\nD10-p10\n98/98\n98/98\n95/95\n100/100\n97/96\n100/96\nEMNIST\nD5-5\n50/48\n50/48\n50/48\n50/48\n50/48\n36/08\nD9-1\n88/09\n88/09\n89/09\n89/09\n88/09\n92/51\nD10-p10\n99/99\n99/99\n100/100\n100/100\n99/99\n100/98\nFashion-\nMNIST\nD5-5\n46/45\n46/44\n47/45\n46/46\n46/46\n55/47\nD9-1\n78/10\n77/10\n81/10\n81/10\n78/10\n85/50\nD10-p10\n90/88\n88/87\n92/92\n92/92\n90/89\n95/95\nFruits 360\nD5-5\n32/14\n46/13\n14/09\n14/09\n28/11\n34/03\nD9-1\n34/09\n38/21\n14/09\n23/09\n38/09\n55/13\nD10-p10\n100/97\n100/99\n90/88\n97/96\n98/12\n98/90\nMADBase\nD5-5\n49/49\n49/49\n49/49\n49/10\n50/49\n40/26\nD9-1\n89/10\n91/10\n89/10\n90/10\n94/10\n99/70\nD10-p10\n99/99\n99/99\n99/99\n99/99\n99/98\n100/99\nMNIST\nD5-5\n49/48\n49/47\n48/11\n48/15\n10/09\n50/31\nD9-1\n88/10\n88/10\n88/10\n88/10\n87/10\n99/71\nD10-p10\n99/99\n98/98\n99/99\n99/99\n98/98\n100/98\nNotMNIST\nD5-5\n49/49\n49/49\n49/49\n50/49\n50/49\n57/50\nD9-1\n87/10\n86/10\n88/10\n88/10\n87/10\n88/31\nD10-p10\n97/97\n97/97\n98/98\n98/98\n97/97\n99/94\nSVHN\nD5-5\n30/22\n28/08\n20/08\n20/08\n40/20\n28/16\nD9-1\n60/07\n35/07\n67/07\n58/07\n61/07\n26/10\nD10-p10\n81/80\n50/50\n20/20\n84/84\n82/79\n39/29\nD10-p10, D5-5:\n100%\n50%\n75%\nD9-1:\n100%\n90%\n95%\nDropout Experiments\nFigure 7.4 represents the trends of the CL performance for the application\nof dropout to a FC model. This ﬁgure illustrates the investigation of D5-5 SLTs. The blue face on\nthe left-hand side depicts the accuracy for the ﬁrst task T1 (epoch 0-10). On the right-hand side,\nthe accuracy for task T2 is shown as a green face (epoch 10-20). The red face corresponds to the\naccuracy values on the joined test dataset (T1 ∪T2). In addition, the maximum accuracy of the baseline\nexperiment is depicted as a white bar on the right-hand side. The baseline serves as a reference value\nand shows the maximum achievable accuracy by a joint training (D10). The order of the presented\ndatasets is based on the baseline accuracy.\nFigure 7.4 shows that on average half of the red area/face is below the green face for the D-FC\nmodel. Based on the visual representation of the realistic evaluation protocol, it is claimed that dropout\ndoes not mitigate the CF eﬀect. As expected, ML models not addressing the CF eﬀect show poor\nresults in these CL scenarios. The same is true for the application of dropout to CNNs. The CNN\ntrends are comparable to the presented FC models – the CF eﬀect is also not mitigated.\nPermuted Experiments\nFigure 7.5 shows the results of D10-p10 (permutation) experiments with\nthe Fully-Connected (FC) model. FCs models represent standard DNNs in which the CF eﬀect occurs.\nIt is obvious that no green area/face (measurement values from T2) is recognizable, as it is continuously\noverlaid by the combined test dataset T1∪T2. At the same time, the CL performance almost corresponds\nto the maximum measured value of the baseline experiment (white bar). Accordingly, the CF eﬀect\ncannot be directly demonstrated for permuted SLTs. The same eﬀect for permuted tasks in recognized\nin the work of Lee, Kim, et al. (2017) (referred to as “Shuﬄed MNIST”). Based on the obtained\nresults, no more permuted tasks will be considered for future CF studies in this work. Likewise, other\nBenedikt Pfülb\npage 85\n\nExperimental Setup\nEvaluation of Continual Learning Methods\nFigure 7.4: Best D-FC experiments for SLT D5-5.\nrelated works advocate against permuted tasks. Farquhar and Gal (2018) describe permuted tasks as\nunrealistic and as a simpliﬁcation of CL tasks.\nFigure 7.5: Best FC experiments for SLT D10-p10.\nEWC Experiments\nBesides LWTA, Elastic Weight Consolidation (EWC) is a model which addresses\nthe CF eﬀect. Figure 7.6 illustrates the trend of EWC experiments for D5-5 task. Again, the blue face\nrepresents the accuracy of the ﬁrst sub-task T1, and the green one represents the second sub-task T2.\nHowever, the accuracy of the combined test data (red) is half as big of the second sub-task. Thus,\nEWC does not seem to completely mitigate the CF eﬀect equally for all investigated SLT variants.\nThe LWTA model, which is not depicted here, achieves equal or even worse CL performances. As\nFigure 7.6: Best EWC experiments for SLT D5-5.\ndepicted in table 7.1 (prescient protocol), EWC shows the best CL performance. But according to\ntable 7.2, EWC seems to work only for simpler tasks such as the D9-1 SLTs. In order to illustrate why\nthe aggregated results show unfavorable CL performance trends, individual EWC experiments are\npresented in greater detail. The three visualized experiments in ﬁgure 7.7 represent an inconsistent\ntrend. The ﬁgures have to be interpreted the same way as the previously presented ones. Again, the\nblue line represents the accuracy on the test data of T1. The second sub-task T2 (green line) starts at\nthe 10th training epoch of the respective dataset. Red corresponds to the achieved CL performance on\nthe joint test dataset. The dashed line represents the maximum of the baseline experiment.\nEWC achieves acceptable results on the MNIST dataset in ﬁgure 7.7a, even though a linear\nforgetting eﬀect can be detected. In the case of linear forgetting, the problem is to stop the training\nprocess at a training iteration with the best CL performance. According to the realistic evaluation\nprotocol, no data from the previous task can be used for early-stopping.\npage 86\nBenedikt Pfülb\n\nEvaluation of Continual Learning Methods\nExperimental Setup\nA comparable but steeper forgetting is visible in ﬁgure 7.7b, which illustrates the EWC training\non the EMNIST dataset. The trend shows that knowledge from T1 is lost faster for the same D9-1\nSLT. It is more challenging to ﬁnd an appropriate point for the determination of the re-training\nprocess. Figure 7.7c shows an even more drastic trend on the Devanagari dataset. The manifestation\nof the CF eﬀect follows this pattern. The various forms of forgetting in ﬁgure 7.7b are due to the\ndiﬀerent problems/datasets. Even though each dataset is processed for the same number of epochs,\nthe number of training iterations depends on the size of the dataset. Thus, the behavior of EWC\ncannot be determined beforehand as it is problem-dependent. Accordingly, the training process of\nEWC would have to be veriﬁed in a more ﬁne-grained way, e.g., by adding more measurement points.\nAt the same time, data from past tasks need to be accessed in order to stop the training process on a\ncertain point. Especially the last requirement is very challenging, if not impossible to implement in\nreal-world scenarios.\n(a) D9-1b MNIST\n(b) D9-1b EMNIST\n(c) D9-1b Devanagari\nFigure 7.7: EWC experiments for diﬀerent datasets on D9-1 SLTs.\nIn contrast to the previously illustrated D9-1 tasks, D5-5 SLTs are more challenging for EWC. It seems\nthat with EWC it is more diﬃcult to add more knowledge from the second sub-task T2. This challenge\nbecomes visible in the trend reﬂected in ﬁgure 7.8. In all three presented experiments (ﬁgures 7.8a\nto 7.8c), the ﬁnal result does not meet the expected CL performance. Either the CF-eﬀect occurs in\nits pure form, or forgetting starts too early in relation to the increase of knowledge. Therefore, EWC\nonly seems eﬀective under certain circumstances.\nThe behavior of EWC may again be due to the size of the datasets and the associated number of\ntraining iterations. It seems that the mechanism cannot protect the task-relevant weights long enough.\nCounteracting this deﬁcit is a challenge in application-oriented scenarios.\n(a) D5-5d MNIST\n(b) D5-5d EMNIST\n(c) D5-5d Devanagari\nFigure 7.8: EWC experiments for diﬀerent datasets on D5-5 SLTs.\nCL Performance Metric Ωall\nSo far, the accuracy (CL metric) of the joint training dataset has\nbeen measured. In order to increase the signiﬁcance of the measured accuracy values, the metric Ωall\nproposed by Kemker, McClure, et al. (2017) is applied. Ωall describes the ratio of the measured value\nto the baseline performance. Thus, conclusions should be drawn regardless of the diﬃculty of the\ndataset.\nThe best results from table 7.2 are combined with the baseline results. In table 7.3, both the\nbaseline performances and the Ωall values are given. The coloring is similar to table 7.2 (brighter is\nBenedikt Pfülb\npage 87\n\nExperimental Setup\nEvaluation of Continual Learning Methods\nbetter) with emphasis on the Ωall value. A pattern comparable to the previous one in table 7.2 is\nvisible. Unfortunately, the CNN experiments (or the version with dropout Dropout (D)) show some\nanomalies related to the SVHN dataset. The reason is the poor baseline performance, which could be\ndue to the simple architecture or possibly the diﬃculty of the dataset. Results with a too low baseline\nperformance must therefore be ignored. A similar trend is apparent for the plain accuracy metric,\nwhereas the statement regarding the CF eﬀect remains. This is due to the fact that the baseline\nexperiments generally provide very good results.\nTable 7.3: Overview of the results according to the realistic evaluation protocol (metric: Ωall).\nDataset\nSLT\nModel\nFC\nD-FC\nCNN\nD-CNN\nLWTA\nEWC\nCIFAR-10\nD5-5\n50/59\n41/63\n49/64\n48/63\n52/60\n35/91\nD9-1\n51/88\n42/89\n51/90\n52/94\n51/88\n44/82\nD10-p10\n53/102\n43/101\n54/97\n50/112\n52/103\n51/112\nDevanagari\nD5-5\n95/51\n90/51\n98/50\n99/50\n91/12\n88/45\nD9-1\n97/89\n95/88\n99/88\n99/89\n96/89\n99/89\nD10-p10\n97/101\n97/100\n12/811\n99/100\n96/101\n100/100\nEMNIST\nD5-5\n99/51\n99/51\n99/50\n100/50\n99/51\n94/38\nD9-1\n99/89\n99/89\n10/89\n100/89\n99/89\n100/92\nD10-p10\n99/100\n99/100\n10/100\n100/100\n99/100\n100/100\nFashion-\nMNIST\nD5-5\n87/53\n87/52\n90/52\n91/51\n88/53\n93/59\nD9-1\n88/88\n87/88\n91/89\n91/89\n88/88\n95/89\nD10-p10\n89/100\n88/100\n91/101\n92/100\n89/101\n95/100\nFruits\nD5-5\n51/62\n96/48\n78/17\n88/16\n100/28\n63/54\nD9-1\n52/66\n100/38\n79/18\n99/23\n99/39\n91/60\nD10-p10\n100/100\n100/100\n75/119\n80/120\n99/99\n78/126\nMADBase\nD5-5\n99/50\n98/50\n99/50\n99/50\n98/50\n97/41\nD9-1\n99/90\n99/92\n99/90\n99/90\n98/95\n100/99\nD10-p10\n99/100\n99/100\n99/100\n99/100\n98/100\n100/100\nMNIST\nD5-5\n98/51\n97/51\n99/49\n99/49\n95/10\n94/53\nD9-1\n98/90\n98/90\n99/88\n99/88\n98/88\n100/99\nD10-p10\n99/100\n98/100\n99/10\n99/100\n98/100\n100/100\nNotMNIST\nD5-5\n96/51\n96/51\n97/51\n97/51\n96/51\n99/58\nD9-1\n97/90\n96/90\n98/90\n98/90\n97/90\n100/88\nD10-p10\n97/100\n97/100\n98/100\n98/100\n97/100\n99/10\nSVHN\nD5-5\n74/40\n37/76\n20/99\n20/100\n77/52\n30/93\nD9-1\n75/79\n41/86\n20/331\n20/289\n77/79\n30/87\nD10-p10\n80/102\n52/98\n20/100\n20/425\n80/102\n44/89\n100%\n90%\n95%\nIMM Experiments\nAnother model investigated in this work is Incremental Moment Matching\n(IMM) (see section 7.1.1). The IMM model is proposed by Lee, Kim, et al. (2017) and diﬀers from the\nother models in two aspects. Firstly, it supposedly provides a better CL performance with respect\nto the CF eﬀect. Secondly, it cannot be reconciled with the real-world requirements, which causes a\nproblem. As a consequence, the results for IMM are listed separately and not in direct comparison\nwith the models mentioned above.\nThe IMM model oﬀers a variety of diﬀerent techniques and additions that aﬀect the CL performance.\nThese techniques include mean-IMM and mode-IMM. The mean-IMM averages the DNN parameters\nlayer by layer according to a given ratio α. This is realized by the KL-divergence in the objective\nfunction. On the contrary, Mode-IMM adds the covariance information.\nThree transfer techniques are proposed by Lee, Kim, et al. (2017): Weight-transfer, L2-transfer\nand drop-transfer. For this study, the weight-transfer technique is used due to the recommendation in\nthe study. “[...] the use of weight-transfer was critical to the continual learning performance. For this\nreason, the experiments [...] use weight-transfer technique as default” (Lee, Kim, et al. 2017). IMM\npage 88\nBenedikt Pfülb\n\nEvaluation of Continual Learning Methods\nExperimental Setup\nis convincing for more diﬃcult SLTs such as D5-5 tasks. This conclusion can be drawn based on the\nillustrated results in table 7.4.The experiments for the permuted tasks are omitted, as stated earlier in\nthis section.\nThe results of the performed IMM experiments are presented in an aggregated form in table 7.4.\nFirst of all, the best experiment for a SLT (q⃗p∗) and dataset is determined. The best experiment\nconstitutes the foundation for the aggregation of the diﬀerent SLTs. The aggregation is then conducted\nbased on the minimum measured CL performance for a group of SLTs. For D5-5 tasks more than 50 %\naccuracy is desirable in order to evaluate the mitigation of the CF eﬀect. The same applies to D9-1\ntasks where more than 90 % need to be achieved.\nThe obtained results show that in some cases knowledge can be preserved. However, the accuracy\nfor the MNIST dataset does not correspond to the baseline experiment (> 98%). At the same time,\nthe demonstrated results proposed by Lee, Kim, et al. (2017) reveal a better CL performance. The\ninferior performance results in this work may be due to the model selection procedure according to the\nrealistic protocol. In addition, the lack of an extensive ﬁne-tuning process also aﬀects the ﬁnal result.\nTable 7.4: Summary of SLTs for IMM.\nMode\nDataset\nSLT\nCIFAR-10\nDevanagari\nFashion-\nMNIST\nMADBase\nMNIST\nSVHN\nD5-5\nD9-1\nD5-5\nD9-1\nD5-5\nD9-1\nD5-5\nD9-1\nD5-5\nD9-1\nD5-5\nD9-1\nmode\n31\n43\n73\n85\n70\n78\n91\n91\n84\n87\n56\n60\nmean\n30\n43\n67\n85\n62\n78\n82\n92\n82\n88\n50\n59\nThe problem of IMM is the subsequent adjustment of the parameter α, which is used to merge\ntwo model parameter conﬁgurations. The merging process with respect to the resulting accuracy is\nillustrated in ﬁgure 7.9. On the left-hand side, the accuracy trend of training a DNN for the ﬁrst task\nis shown. The blue line represents the test accuracy on T test\n1\nand the green line on T test\n2\nfor 10 epochs\neach. Additionally, the maximum of the baseline experiment with IMM is provided (dashed line). The\nright section of ﬁgure 7.9 represents the α tuning process. The two DNNs (one of the ﬁrst and one\nof the second task) are “merged” in diﬀerent proportions based on the α parameter. The test range\nfor the α values lies between 0 and 1 with a step size of 0.01 (resulting in 100 tested α values). The\ntwo lines on the right-hand side illustrate the test accuracy on the combined test dataset T1 ∪T2. As\ndepicted in ﬁgure 7.9, the mode variant (orange) provides slightly better results than the mean variant\n(red). As expected, the best α value is not exactly 0.5 for a D5-5 SLT. This is clearly illustrated by the\ntwo values for mean α=0.60 and mode α=0.39. Accordingly, an optimal α factor cannot necessarily\nbe calculated with the ratio of the number of classes within the sub-tasks.\nFigure 7.9: Best IMM experiment on SLT D5-5f for the Devanagari dataset.\nAs indicated before, the deﬁnition of the merging parameter α is not trivial for IMM. This is evident\nin ﬁgure 7.10, which can be interpreted the same way as ﬁgure 7.9. The left part of ﬁgure 7.10 shows\nthe independent training process of two DNNs for T1 and T2. The white bar in the middle indicates\nthe maximum of the baseline experiment. On the right-hand side, the obtained test accuracies of the\nvarying α parameter are illustrated. The trends for diﬀerent datasets of the mode-IMM and mean-IMM\nindicate that α has to be determined by cross-validation.\nThe tuning process requires data from the current as well as from the previous sub-tasks (as\nstated by Lee, Kim, et al. (2017)). However, the tuning contradicts the CL requirements for real-world\nBenedikt Pfülb\npage 89\n\nDiscussion\nEvaluation of Continual Learning Methods\nFigure 7.10: Best IMM experiments for SLT D5-5b for diﬀerent datasets.\napplications. A further disadvantage of this optimization strategy is that the very computationally\ncostly Fischer Information Matrix (FIM) has to be calculated repeatedly for each α value. Due to the\nincreased cost, the optimization was not performed for all datasets. In order to determine an adequate\nα value, several hours (> 4 h for 100 diﬀerent α values) and a large amount of memory would be\nrequired (> 8 GB of RAM). The memory issue is complicated by the use of more complex DNNs, since\nthe determination of the FIM depends on the DNN’s architecture. On the one hand, the eﬀort of FIM\ndetermination depends on the implementation. By approximating the FIM, the computational eﬀort\ncan be reduced (an approximation is proposed by Gepperth and Wiech (2019)). On the other hand,\nthe use of past data still contradicts the real-world requirements. This violation limits the applicability\nof IMM for various CL scenarios.\nFigure 7.11 represents the IMM results of a D9-1 SLT similarly to the previous ﬁgures. Comparable\nﬁndings can be derived from this kind of CL tasks, although the ratio of the number of classes from\nT1 to T2 is 9:1, α=0.1 is not always the best merging value. Moreover, mode-IMM is not necessarily\nsuperior than mean-IMM (see ﬁgures 7.10 and 7.11). In order to select the best possible mode for a\nparticular problem, both options need to be evaluated. In general, this exceeds the available capacities\nfor training and evaluation.\nFigure 7.11: Best IMM experiments for SLT D9-1b for diﬀerent datasets.\n7.2\nDiscussion\nThis discussion focuses on the derived ﬁndings, as well as the impact of the real-world evaluation\nprotocol. In particular, the ﬁndings that contradict related work are outlined. Resolving the CF\nproblem constitutes the main subject of the discussion along with the respective conditions.\nTraining Iterations and Measuring Points\nThe inﬂuence of the number of training iterations\non CL performance is the ﬁrst aspect that is discussed. Both the number and positioning of the\nmeasuring points are crucial for the CL performance. Additionally, the mechanism of early-stopping\nis related to these parameters. In real-world CL scenarios, it is often challenging to determine the\noptimal parameters with regard to stopping the (re-)training process. This may be due to two factors.\nIn order to determine an accurate model parameter conﬁguration, test data from previous tasks\nneed to be stored and used. An appropriate model conﬁguration can be determined with past test\ndata by investigating the CL performance. Regular measurements need to be executed in order to stop\nthe re-training process before too much previous knowledge is lost. At the same time, the stored test\npage 90\nBenedikt Pfülb\n\nEvaluation of Continual Learning Methods\nDiscussion\nsamples could be used for the re-training process, which is equivalent to a joint training. This method\nmay be possible in some scenarios, but it contradicts the idea of CL with respect to a potentially\ninﬁnite number of tasks. Consequently, it is impossible to store test samples for each task while\nrespecting the memory constraints. Furthermore, diﬀerent states of model parameter conﬁgurations\n(referred to as checkpoints) need to be stored to return to an optimal model parameter conﬁguration.\nThis, in turn, can be computationally and memory intensive depending on the underlying model.\nDetermining an acceptable stopping point during training is challenging. In most cases, the quality\nmeasurement of a model is based on test data. Assuming that test data is accessible, multiple questions\narise:\nHow much test data are representative? How often the model needs to be tested?\nIf a\ntest step is carried out after each training iteration, it is possible to identify an acceptable model\nparameter conﬁguration. The disadvantage of this method is that it slows down training, especially for\nrepresentative or larger test datasets. If a model is very sensitive in this respect, a tremendous amount\nof computational eﬀort has to be expended. Depending on the selected CL scenario, the present\nmethod is impossible due to few available resources. Therefore, models must be able to determine a\ngood model parameter conﬁguration without much eﬀort, or – even better – they should be robust to\nthis eﬀect. Ensuring that extended (re-)training does not reduce the quality of a model is a criterion\nthat should be investigated in further studies.\nEvaluation Selection Criterion\nA major challenge is the choice of a measure when deciding on the\nbest parameter conﬁguration. The used metric is related to this aspect. In this context, the real-world\nrequirements for the models presented earlier become increasingly important. The best and last criteria\nused in this protocol diﬀer signiﬁcantly. Depending on the CL scenario, it is decisive to what extent\nan evaluation based on the best measured value makes sense at all. Just because a model has (or can)\nachieve maximum performance at a random training/test point, it does not necessarily represent the\noptimum conﬁguration for future tasks. This is evident in the precient evaluation protocol. Ultimately,\nthe last measured value should be decisive. Otherwise, there is no guarantee with regard to quality.\nThe best model parameter conﬁguration is related to early-stopping, the guaranteed quality\nprotection in case of a too long training or the provision of return points (checkpoints) in case of a\ndetected accuracy loss. The choice of a metric is relevant in this context. In the present work, the\nsimple accuracy (see equation 6.1) is used in most cases. However, this metric is only applicable if\nthe dataset is appropriate and thus almost balanced. If a dataset is unbalenced, or if the objective\nof the model is diﬀerent (e.g., safety in the classiﬁcation of cancer), another metric has to be used.\nThe results of the baseline experiment are related to the problem scenario. The metric presented by\nKemker, McClure, et al. (2017) includes the baseline results and thus eliminates the diﬃculty of the\ncomparison of diﬀerent datasets.\nDatasets\nThe next aspect that is discussed concerns the used datasets (see section 6.1). In general,\nthe utilized datasets are well established in the ML community and the respective studies. Even\nthough a dataset like MNIST is considered to be outdated and overly simplistic, several conclusions\ncan be derived. As MNIST is very simple, it is expected to achieve high CL performances. The CL\nperformance is therefore expected to be located in a similar range when compared to the baseline\n(which is not a CL task). The case is diﬀerent for more complex datasets, such as SVHN or CIFAR-10.\nFor more complex datasets, the baseline is signiﬁcantly lower, which is not clearly interpretable in the\ncontext of CL.\nMoreover, it is still diﬃcult to assess the used datasets in terms of complexity, size or number of\nfeatures. Signiﬁcantly larger datasets with more features are easy to ﬁnd. However, more complex\ndatasets can only be investigated with further adjustments regarding the hyper-parameters and the\nused model architecture. In order to draw adequate conclusions in the context of the CL paradigm, an\ninvestigation of these datasets would require a more signiﬁcant investment of time and resources.\nHowever, it is essential to take into account that the investigated image datasets are in contrast to\na real-world dataset as shown in chapter 5. Real-world constraints often are manifested by their very\ncomplex inﬂuence on the data, which raises new challenges for CL models. This complication aﬀects\nthe desired objective within a CL scenario, e.g., obsolete knowledge needs to be speciﬁcally forgotten.\nNevertheless, the applied datasets allow for conclusions regarding the absence of the CF eﬀect.\nBenedikt Pfülb\npage 91\n\nDiscussion\nEvaluation of Continual Learning Methods\nSequential Learning Tasks (SLTs)\nDue to the close relation to the applied datasets, the respective\nSLTs are discussed in the following. An open question is how representative SLTs are compared to real-\nworld CL problems. In fact, CL tasks with hard task boundaries, clearly disjoint classes and uniformly\ndistributed data can be considered unusual for real-world scenarios. The application of the realistic\nevaluation protocol including simple SLTs results in limited CL performance for the examined models.\nExcellent results would, however, reinforce the need for further research into even more realistic\ninvestigation protocols. More realistic evaluation protocols include, for example, the recognition\nand reaction to the addition of new knowledge/classes in diﬀerent changing data distributions (see\nsection 2.3.1). Furthermore, it is important to investigate more complex task constellations, such as\nfew-shot learning, regression problems, or selective forgetting scenarios. Assuming that an ML model\ncan satisfy all requirements for arbitrary CL scenarios is still utopian. Moreover, the circumstances of\na well-working procedure requires further speciﬁcation.\nHyper-Parameter Optimization\nHyper-parameters can be a crucial factor for ML problems. At\nthe same time, certain parameters can be problem-dependent. These kinds of parameters must be\ndetermined by a trial-and-error procedure. Adjusting many diﬀerent values for several hyper-parameters\nleads to a large number of possible combinations. As a result, an extremely large number of experiments\nhas to be performed and evaluated. In order to minimize this scope and cost, the conducted grid-search\nconstitutes a compromise for the examination of multiple datasets. In general, more values could be\nexplored for each problem to achieve better results. A more extensive investigation could include larger\nand more complex model architectures.\nDespite the attempt to conduct a comprehensive study, it can be assumed that the trend will be\nsimilar for a large-scale investigation. It is assumed that better results can be expected, while the\ngeneral conclusions remain the same for larger studies. The basic problem of parameter optimization is\nthe choice of the problem-related parameter values. This concerns, for example, the model architecture\nwhere the hypothesis that more is always better prevails. Unfortunately, this does not apply to other\nhyper-parameters. Therefore, the question arises whether the selected values of a particular parameter\nare appropriate or not. CL speciﬁc models should have as few problem-dependent parameters as\npossible requiring adjustments. The same applies to the strategy of selecting parameter values, e.g.,\nthe more the better is a good option. This should be considered as an important property of a CF\navoidance model favorable for realistic CL scenarios.\nCF Avoidance Models\nThe choice of investigated CF avoidance models is the last aspect that is\ndiscussed. The main focus is on the selection of the examined models. In particular, the question\narises why no further or other models were examined. The selection is due to two reasons. On the one\nhand, the required computational eﬀort per model increases. On the other hand, many approaches\nand optimization methods inherently contradict the application-oriented requirements. The latter is\nthe case, for example, in the examination of the IMM model.\nFurthermore, the availability of a code base is fundamental for the conduction of experiments.\nWithout a publicly available code base, researchers have to develop their own implementation based\non the publications. This causes several disadvantages. Due to a lack of details or omitted critical\noptimization steps, conclusions regarding the CL performance can only be vague. One model which\nprovides very good CL results is Hard Attention to the Task (HAT) proposed by Serra, Suris, et al.\n(2018). The code of HAT is publicly available. As the name of the model indicates, it refers to a\ntask. What is not necessarily clear, however, is that for each sample the task information has to be\ngiven. This means that for both, training and evaluation purposes, the ID of the original task must be\nspeciﬁed for a sample. This corresponds to the use of a training- and test time oracle.\nSuppose that HAT is investigated with an SLT consisting of 10 individual sub-tasks. Applying this\nto the datasets used in this work would mean that each CL task represents a single class. Accordingly,\nthe task ID for each sample would automatically be provided while representing the corresponding\nlabel. This would already correspond to a perfect solution. A code snippet specifying the signature\nof the evaluation function is given in listing 7.1. The eval function (as well as the training function)\nspeciﬁes the parameter t which expect the task ID of a sample. Thus, a perfect oracle would be\navailable for the kind of CL tasks. However, the application of oracles contradicts the requirements\nof many real-world scenarios. This limitation causes the very diﬃcult application of the approach in\ndiﬀerent CL scenarios so that it is omitted in this study. The general problem is that these severe\nconstraints only become obvious after an intensive study of the code base.\npage 92\nBenedikt Pfülb\n\nEvaluation of Continual Learning Methods\nConclusion\n160\ndef\neval(self ,t,x,y):\nListing 7.1: Code snippet from Serra, Suris, et al. 2018 ﬁle: src/approaches/hat.py\nOmitting the investigation of CF avoidance models is due to yet another requirement. As described\nearlier, storing (hold-out) samples for training or testing contradicts memory constraints. Certain\nmodels require no past data in order to reduce the CF eﬀect. Other ML models use the samples only\nto determine the optimal model parameter conﬁguration, as it is the case for IMM. It is, however, a\nproblem to determine how many and which samples need to be stored and for how long. Diﬀerent\napproaches, e.g., Gradient Episodic Memory (GEM), address this very problem. A reduced number of\nsamples is stored and reused for a joint re-training. In fact, replay methods are a common approach to\ncircumvent the CF eﬀect. Nevertheless, the amount of available replay memory is limited due to the\nrequirements. If the CL scenario consists of an inﬁnite number of tasks, this workaround is limited.\n7.3\nConclusion\nThe following insights are revealed by investigating the real-world evaluations protocol. First of all,\nnone of the investigated CF avoidance models can mitigate the CF eﬀect to an acceptable extent under\nmore real-world conditions. Several standard image datasets were used for the study. They consist of\n10 classes that were divided into diﬀerent Sequential Learning Tasks (SLTs). Each task of an SLTs\nconsists of disjoint classes from the dataset. The protocol applied in this study diﬀers from others\nwith regard to its strong focus on application-oriented requirements. Due to the requirements it is\nimpossible, for example, to retroactively set parameters after the completion of the last CL task, e.g.,\nthe model architecture. Likewise, the access to future data is prohibited. The implementation of these\nrequirements by the investigation protocol leads to diﬀerent results than those presented in related\nwork (see chapter 3).\nConsidering the individual experiments of the CF avoidance models leads to the conclusion that\nit is possible to successfully mitigate the CF eﬀect with EWC (under certain conditions). This is\ntrue in case the knowledge from one additional class has to be added. Unfortunately, this statement\ndoes not apply to all investigated problems. It might be due to the fact that EWC is sensitive to the\nnumber of training iterations or strongly depends on parameters, e.g., the importance parameter λ.\nFor more complex tasks, e.g., with ﬁve new classes, the procedure seems to have more diﬃculties with\nregard to the protection of existing knowledge. The IMM model indicates a better performance, but it\ncontradicts the requirements of real-world scenarios where data from the past task are needed. It even\nbecomes more diﬃcult as soon as a large number of tasks occur one after the other. For each task,\nmore and more memory needs to be allocated in order to be able to represent the full data distribution.\nIn terms of the application-oriented requirements, the results imply that none of the investigated\nmodels can suppress the CF eﬀect to a satisfactory extent. Thus, they are not applicable under\nthe deﬁned requirements for CL scenarios, although the evaluation protocol does not even map\nall conceivable requirements of CL scenarios (e.g., variants of changing data distributions). As a\nconsequence, the question arises whether it is adequate to refer to the CF-eﬀect as “controlled” or\n“solved” in the context of diﬀerent CL scenarios. This question can only be answered accurately if the\ncircumstances of a model’s examination are revealed and published. To conclude, it is crucial to fully\nrepresent the circumstances in detail.\nBenedikt Pfülb\npage 93\n\nConclusion\nEvaluation of Continual Learning Methods\npage 94\nBenedikt Pfülb\n\n8.\nNovel Deep Learning Model: Deep Convo-\nlutional Gaussian Mixture Models\nChapter Contents\n8.1\nFundamentals of Gaussian Mixture Models . . . . . . . . . . . . . . . . . . . . . .\n96\n8.1.1\nThe Gaussian Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n8.1.2\nMultivariate Gaussian Distribution . . . . . . . . . . . . . . . . . . . . . .\n97\n8.1.3\nTypes of Covariance Matrices . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n8.1.4\nGaussian Mixture Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n8.2\nRelated Work\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n8.3\nSGD for GMM Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n8.3.1\nGMM Constraint Enforcement\n. . . . . . . . . . . . . . . . . . . . . . . .\n105\n8.3.2\nMax-Component Approximation\n. . . . . . . . . . . . . . . . . . . . . . .\n106\n8.3.3\nAnnealing Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n106\n8.3.4\nTraining Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n108\n8.4\nDeep Convolutional Gaussian Mixture Models\n. . . . . . . . . . . . . . . . . . . . 109\n8.4.1\nTypes of Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n110\n8.4.2\nDCGMM Functionalities . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n115\n8.5\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n8.5.1\nSGD Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n119\n8.5.2\nDCGMM Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n122\n8.6\nDiscussion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n8.7\nConclusion and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\nIn this chapter, a novel deep learning approach referred to as Deep Convolutional Gaussian Mixture\nModels (DCGMMs) is presented. The contents of this chapter are published in Gepperth and Pfülb\n(2021) and Gepperth and Pfülb (2020). The machine learning (ML) model proposed in this work is\nbased on Gaussian Mixture Models (GMMs), which are well-known and investigated. As GMMs are not\ndeep, using them as universal function approximators becomes increasingly challenging. Furthermore,\nconventional training methods such as the Expectation-Maximization (EM) algorithm are unsuitable\nfor continual learning (CL) scenarios. EM requires a data-driven initialization of the underlining\nGMMs, and its use in streaming scenarios is impossible without extensions. However, some properties\nof GMMs make them attractive for CL scenarios. GMMs are less (or at least diﬀerently) aﬀected\nby the catastrophic forgetting (CF) eﬀect. In contrast to Deep Neural Networks (DNNs), GMMs\nare unsupervised ML models (see section 2.2). In order to use these favorable properties for CL\nscenarios, the challenges mentioned above have to be resolved. Thus, this chapter presents the resulting\nadaptations and evaluates the validity of the novel approach. The CL properties of the DCGMMs are\noutlined in detail in the next chapter.\nContributions\nThe contribution of this chapter is a novel deep learning approach based on GMMs,\nwhich is referred to as DCGMM in the present work. Accordingly, an Stochastic Gradient Descent\n(SGD)-based GMMs training method is presented, which supports the use in streaming scenarios.\nAdditionally, the transformation into a deep learning method by means of a stacking of GMMs is\nBenedikt Pfülb\npage 95\n\nFundamentals of Gaussian Mixture Models\nNovel Deep Learning Model: DCGMM\nproposed. The evaluation of DCGMM results in similar or better performances compared to known\nstandard methods. Moreover, the model oﬀers various functionalities that can be used as a basis for\nfurther research.\nIn particular, this chapter addresses research question RQ 3 and describes how a novel deep learning\nmodel can look like that is not directly subject to the CF eﬀect. Thus, this chapter is not set in the\ncontext of the CL paradigm, but rather serves as a basis for further investigations.\nStructure\nFirst of all, GMMs are introduced along with the associated fundamentals (see section 8.1).\nIn the following section, the related work in the context of GMMs is presented and discussed. In the\nthird section, a novel method for GMM training by SGD is presented (see section 8.3). Moreover, an\nannealing scheme is introduced, which enables the use in streaming scenarios. Section 8.4 outlines\nhow the non-deep unsupervised learning procedure can be transformed into a deep learning model. In\norder to prove the validity of the new approach, a comparison to other training methods is performed\nin chapter 9. The chapter concludes with a discussion related to the new model.\n8.1\nFundamentals of Gaussian Mixture Models\nGaussian Mixture Models (GMMs) are probabilistic models based on multiple Gaussian distributions.\nIn the following, the basic elements of GMMs are introduced. The fundamentals include an overview of\nthe Gaussian distribution and the multivariate Gaussian distribution. In addition, the EM algorithm\nis presented as the conventional training method for GMMs.\n8.1.1\nThe Gaussian Distribution\nOne of the most well-known distributions is the Gaussian distribution (referred to as univariate\n(one-dimensional) normal distribution according to Bishop (1995)). Equation 8.1 shows the probability\ndensity function (PDF) of the Gaussian distribution. For any real number x, it follows that N(x|µ, σ)>0.\nThe value range of the function is deﬁned by f : R →R. The ﬁrst part of equation 8.1 normalizes the\nresulting values and, thus, represents probabilities. The probability for a sample x depends on the\nmean µ and the standard deviation σ.\nN(x|µ, σ) :=\n1\n√\n2πσ2 exp\n\u0010\n−\n1\n2σ2 (x −µ)2\u0011\n(8.1)\nEquation 8.1: Probability density function of the Gaussian distribution.\nExemplary PDFs for diﬀerent parameters of the Gaussian distribution are depicted in ﬁgure 8.1. The\ngreen and blue curves have the same mean µ=0. The variance σ aﬀects the maximum and deﬁnes the\nspread. The mean determines the shift on the x axis, as represented by the red line (µ=3 and σ=2).\nBy using a PDF, the probability of a sample can be determined, e.g., for x=4. If inserted into the\nequation 8.1, the probability of x=4 for N(µ=3, σ=2) is ≈0.2196.\n−4\n−2\n0\n2\n4\nx\n0.0\n0.2\n0.4\nN(x|µ, σ)\nN(0, 1)\nN(0, 2)\nN(3, 2)\nFigure 8.1: Plots of exemplary Gaussian distributions.\npage 96\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nFundamentals of Gaussian Mixture Models\nThe parameters of a Gaussian distribution µ and σ can be estimated from data. The expectation\nvalue E and its variance can be used to calculate the parameters of the PDF. The mean µ can be\nestimated by µ = E(X) = 1\nn\nPn\ni=1 xi. The variance is dependent on the mean and is estimated by\nσ=Var(X)=E[(X −µ)2].\nProbability and Likelihood\nThe diﬀerence between probability and likelihood has to be deﬁned\nfor this work. Probabilities generally describe the occurrence of a certain result or event. In contrast to\nthat, the term likelihood is used if a hypothesis is included. A hypothesis can be considered as a model’s\nparameter θ. Thus, with the help of the parameters, the determination of an event’s probability can\nbe inﬂuenced. In the context of parameterized models, probability is referred to as likelihood (Bishop\n1995). The present work follows the same convention.\nLog-Likelihood\nThe log-likelihood is obtained as the natural logarithm of the likelihood L(x)=\nlog p(x|µ, σ). Resulting from the application of the logarithm, the following statement is valid: Since\nthe logarithm is strictly monotonically increasing, a minimum of the log-likelihood function is a\nminimum of the likelihood function. The same is true for the maximum.\nApplying the logarithm to the likelihood function has two advantages. First, the logarithm makes\nthe subsequent determination of multiple log-likelihood values additive. Second, it becomes easier\nto determine the (partial-) derivative as stated, for example, in equation 8.2. Thus, gradient-based\noptimization is computationally simpler, and thus more eﬃcient to implement. Taking into account\nthe precision of ﬂoating points, e.g., 32-bit ﬂoats, the numerical underﬂows are bypassed.\nL(x) = log p(x|µ, σ) = −1\n2 log(2π) −log(σ) −(x −µ)2\n2σ2\n∂L(x)\n∂µ\n=(x −µ)2\nσ2\n∂L(x)\n∂σ\n= −1\nσ + (x −µ)2\nσ3\n(8.2)\nEquation 8.2: Partial derivative of the log-likelihood function for a normal distribution.\nIn the context of the likelihood function, the maximum likelihood is usually of interest. The maximum\nlikelihood deﬁnes the highest outcome of the likelihood function p(x|θ). Accordingly, the parameters θ\nof a model need to be adjusted. The parameters for a Gaussian distribution are θ={µ, σ}, as detailed\nin equation 8.3. The likelihood function is used for optimization by applying the negative logarithm of\nthe likelihood function as a loss function L (Bishop 1995).\nL(x|θ) = −1\n2 log(2π) −log(σ) −(x −µ)2\n2σ2\n(8.3)\nEquation 8.3: Log-likelihood function for a normal distribution.\n8.1.2\nMultivariate Gaussian Distribution\nLikewise, multidimensional data x can be modeled by multivariate Gaussian distributions as shown in\nequation 8.4. The means are represented by a D-dimensional vector µ. The variances are represented\nby a D×D positive-deﬁnite covariance matrix Σ. |Σ| corresponds to the determinant of Σ and Σ−1 is\nthe inverse. Due to the positive-deﬁniteness of the matrix, its inverse exists.\nAn example of a multivariate Gaussian distribution is indicated in ﬁgure 8.2. For a two-dimensional\nspace, the mean x is a 2D vector, whereas the covariance Σ is represented by a 2×2 matrix. The\nprobability for samples is higher in the center than on the edges of the ellipses (see the color bar in the\nBenedikt Pfülb\npage 97\n\nFundamentals of Gaussian Mixture Models\nNovel Deep Learning Model: DCGMM\nN(x|µ, Σ) =\n1\n(2π)\nD\n2\n1\np\n|Σ|\nexp\nn\n−1\n2(x −µ)⊤Σ−1(x −µ)\no\n(8.4)\nEquation 8.4: Multivariate Gaussian distribution for a vector x.\nlower right corner). The rotation indicated by the coordinate system is explained by the covariance\nmatrix Σ. The Gaussian curves at the bottom and right-hand side represent the marginal distributions\non the axes (independent consideration of individual variables x and y) (Bishop 1995).\nFigure 8.2: Plot of exemplary 2D multivariate Gaussian distributions.\n8.1.3\nTypes of Covariance Matrices\nDiﬀerent types of covariance matrices can be speciﬁed for multivariate Gaussian distributions (see\nﬁgure 8.3). The usual case is represented by a full covariance matrix Σfull. Σfull is a symmetric,\npositive-deﬁnite matrix containing D2 independent parameters, where D represents the dimensionality\nof the data. As a result, the number of entries of the covariance matrix quadratically depends on the\ndimensionality of the data. In ﬁgure 8.3a, a Gaussian distribution with a full covariance matrix is\ndepicted. The use of a full covariance matrix leads to the independent expansion of the distribution to\nthe axes of the coordinate system. The main diagonal represents the spread while the oﬀ-diagonal\ndescribes the rotation of the distribution.\nHowever, the use of the full covariance matrix may be impractical in application-oriented contexts,\nespecially with regard to high-dimensional data. Simplifying the covariance matrix reduces the number\nof parameters, which in turn reduces computational complexity. However, a simpliﬁcation of the\ncovariance matrix leads to a limited degree of freedom with regard to the distribution. Thus, not all\ncorrelations from the data can be captured. Diﬀerent degrees of freedom are outlined in ﬁgure 8.3.\nIn addition, the coordinate systems illustrate the degrees of freedom. A ﬁrst simpliﬁcation step is\nconstituted by the diagonal covariance matrix Σdiag =diag(σ2\ni )=Σii. This reduces the number of free\nparameters in the covariance matrix to D parameters. In ﬁgure 8.3b, only the diagonal parameter\nvalues of the covariance matrix are utilized. As a consequence, only the direction of the axes can be\nvaried (axis-aligned ellipsoid).\nThe most simpliﬁed variant is the isotropic covariance matrix Σiso = σ2I. Solely concentric\ndistributions can be modeled by the isotropic covariance type (see ﬁgure 8.3c). Thus, the height and\nwidth in the two-dimensional case is always the same. The main diagonal is represented by one single\nvalue x (Σii =x). The advantage of the isotropic covariance is that the number of parameters of Σiso\nis 1 and, thus, lower than for the diagonal type.\npage 98\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nFundamentals of Gaussian Mixture Models\n(a) Full covariance matrix\n(b) Diagonal covariance matrix\n(c) Isotropic covariance matrix\nFigure 8.3: Three types of covariance matrix.\n8.1.4\nGaussian Mixture Models\nGMMs are probabilistic models based on a ﬁxed number of mixed multivariate Gaussian distributions.\nThey can be used as clustering methods, among others, which is why they can be categorized as\nunsupervised learning methods. By means of a weighting of K Gaussian component densities, it is\nattempted to explain the data X={x1, . . . , xn}. Accordingly, the πk are the weights in the mixture\nof Gaussian components, where k∈{1, ..., K}, and PK\nk=1 πk =1. As a short notation, N(x|µk, Σk) is\nequivalent to Nk(x). The individual likelihoods are added up, as shown in equation 8.5.\np(xn) =\nK\nX\nk=1\nπkNk(xn)\n(8.5)\nEquation 8.5: Likelihood estimation by multiple Gaussian components.\nEach sample is assumed to have been created by a single component density, which is expressed by\nthe unknown latent variable. Thus, the unobservable latent variable zn ∈{1, . . . , K} is re-introduced.\nUnder this assumption, the complete-data likelihood of a single sample is obtained, as represented in\nequation 8.6.\nL(xn, zn) = πznNzn(x)\n(8.6)\nEquation 8.6: The complete-data likelihood.\nThe non-observability of the latent variable implies that it could be marginalized out of the complete-\ndata log-likelihood (in equation 8.6). This assumption makes the expression suitable for optimization,\nand it results in the incomplete-data likelihood displayed in equation 8.7. As a consequence, all\nparameters depend on observable variables.\np(xn) =\nK\nX\nk=1\np(xn, zn = k)\n(8.7)\nEquation 8.7: The incomplete-data likelihood.\nIf all n samples of the dataset X are considered, equation 8.8 represents the total incomplete-data\nlikelihood.\nAs a last step, substituting from equation 8.8 the part in equation 8.5, and setting the result into\nthe log-domain (see section 8.1.1), the total incomplete-data log-likelihood is obtained.The resulting\nBenedikt Pfülb\npage 99\n\nFundamentals of Gaussian Mixture Models\nNovel Deep Learning Model: DCGMM\np(X) =\nY\nn\np(xn) =\nY\nn\nX\nk\np(xn, zn = k)\n(8.8)\nEquation 8.8: The total incomplete-data likelihood.\nfunction is suitable as a loss function L, as it is free of non-observable parameters.\nL = log p(X) =\nX\nn\nlog\nX\nk\nπkNk(xn).\n(8.9)\nEquation 8.9: The total incomplete-data log-likelihood.\nAssuming that not all data samples of a dataset X are available, the notation in equation 8.10 is\nintroduced in order to express the likelihood for only a certain amount of data. This is relevant, for\nexample, in the context of batch-wise processing or streaming scenarios. For reasons of numerical\nstability, the sum is replaced by the expectation value (E).\nL = En\n\"\nlog\nX\nk\nπkNk(xn)\n#\n.\n(8.10)\nEquation 8.10: The log-likelihood loss function.\n8.1.4.1\nResponsibilities\nSince GMMs can be categorized as clustering methods, a soft membership of a sample to a data cluster\nor component can be determined. In other words, the (pseudo-)posterior probability indicates that a\nsample was generated from a particular component k. This is diﬀerent compared to other clustering\nmethods like k-means, whereas k-means is based on hard assignments. The main diﬀerence, however,\nis that GMMs indicate a soft membership of each cluster center. These memberships are referred to as\nresponsibility (denoted as γ), which is determined by using the equation displayed in equation 8.11.\nγk(xn) =\nπkN(xn|µk, Σk)\nPK\nj=1 πjN(xn|µj, Σj)\n(8.11)\nEquation 8.11: Determination of soft cluster membership (responsibilities γ).\n8.1.4.2\nExpectation-Maximization Algorithm\nThe Expectation-Maximization (EM) algorithm published by Dempster, Laird, et al. (1977) is used\nto adjust the parameters of a statistical model, e.g., a GMM. EM is a batch-type algorithm that is\nbased on two steps. Firstly, the Expectation (E) step determines the “expectation” value, i.e., the\nresponsibilities γ(x) (see section 8.1.4.1). The E step assigns probabilities to each sample x ∈X.\nSecondly, the Maximization (M) step changes the parameters of the underlying model in order to\nachieve a higher expectation value. The M step is based on the previously determined expectation\nvalues.\nThe two EM steps are iterated until a certain convergence criterion is fulﬁlled, e.g., a minimal\nchange of the expectation value. The process usually adjusts the parameters very strongly at the\npage 100\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nFundamentals of Gaussian Mixture Models\nbeginning of the optimization, whereas the adjustment of the parameters decreases. Before the ﬁrst E\nstep can be executed, an initialization of the model has to be performed. The deﬁnition of reasonable\ninitial parameters for the means (and covariances) is crucial. k-means is often used for the initialization\nof GMM components in many implementations (note that k-means also has to be initialized). The full\nprocedure is summarized in algorithm 8.1.\nAlgorithm 8.1: Procedure of the EM algorithm.\nData: training data: X, model parameter: θ\nResult: trained mixture model\n1\nInitialize model parameter θ.\n2\nwhile convergence criterion is not met do\n3\nExpectation (E) step: Determination of the preliminary expectation value based on θ.\n4\nMaximization (M) step: Fit the parameters using the maximum likelihood function.\nThe EM steps are comparable to the optimization steps used by the well-known k-means algorithm. For\nk-means, the number of cluster centers needs to be speciﬁed and initialized. As a next step, the centers\nare iteratively moved based on the corresponding “closest” and thus related data points. Finally, the\ndata points are reassigned to the nearest cluster centers based on the distance metric (e.g., Euclidean\ndistance). These steps are repeated until a certain number of iterations is reached or the allocation of\nall data points is constant at one step.\nA similar algorithm is deﬁned for training GMMs. First of all, the number of Gaussian components\nneeds to be speciﬁed and initialized. Subsequently, the cluster centers are gradually adjusted to\nthe data. The basic diﬀerence compared to k-means is that the assignment to a cluster center or\nrespectively component is not hard, but gradual. The gradual assignment is illustrated in ﬁgure 8.4\nby color interpolations of the samples. Gaussian components are represented by the red cross\nand\nthe blue circle\n. The ellipsoids represent the covariance of the individual components. The color\ngradient of each sample represents the proportional assignment to each of the two components. Based\non the sample’s position, both, a component’s position and its variance are adjusted step by step (see\nﬁgures 8.4a to 8.4c).\n(a) Initialization\n(b) After some iterations\n(c) Converged clustering\nFigure 8.4: Illustration of the EM algorithm.\nFor the comparison of k-means and GMM, the operation of the k-means algorithm is outlined in\nﬁgure 8.5. Again, the red cross and the blue circle are supposed to represent cluster centers. A direct\ncomparison shows that a sample is always assigned to exactly one center (no gradual assignment,\nﬁgure 8.5 vs. ﬁgure 8.4). The distance (often Euclidian distance) is used as a basis for the assignment,\nwhich is illustrated by the lines and colors. Similarly, a corresponding converged result is obtained\nafter several optimization iterations (see ﬁgures 8.5a to 8.5c).\nMaximum-Likelihood Estimation\nThe maximum-likelihood estimation (MLE) describes the\nparameter adjustment of a probabilistic model (see Bishop 1995). In case of multiple multivariate\nGaussian distributions, e.g., for a GMMs, the process is almost the same as for single Gaussian\ndistributions. The basis for the MLE is the assumption of a convex loss function L, which is optimized\nby setting the derivative to 0. By calculating the partial derivative for a single component according\nBenedikt Pfülb\npage 101\n\nFundamentals of Gaussian Mixture Models\nNovel Deep Learning Model: DCGMM\n(a) Initialization\n(b) After some iterations\n(c) Converged clustering\nFigure 8.5: Illustration of the k-means algorithm.\nto\n∂\n∂µ, the result is µnew = 1\nN\nPN\nn=1 xn (which corresponds to the average). The derivative for\n∂\n∂Σ is\nΣnew = 1\nN\nPN\nn=1(xn −µnew)(xn −µnew)⊤, with dependence on µnew. Although the expected value\nis E[µnew]=µ, this is not true for E[Σnew]̸=Σ. Therefore, the optimization step has to be repeated\niteratively until a model convergence is detected.\nA further adjustment has to be performed for GMMs with K multivariate Gaussian distributions.\nIn addition to the k components, the mixing coeﬃcients πk have to be adapted. The procedure follows\nthe same principle of zeroing the derivative. Additionally, weighting is applied which is represented\nby the responsibilities γk of each sample (see section 8.1.4.1). Accordingly, the mean is initially\ndetermined by multiplying γk(x), which is represented in equation equation 8.12. Due to the available\nK components, a weighted mean is calculated depending on the responsibility γ(xn) of a sample (see\nequation 8.12), whereby Nk = PN\nn=1 γ(xn).\nµk = 1\nNk\nN\nX\nn=1\nγk(xn)xn\n(8.12)\nEquation 8.12: Determiniation of the weighted mean for GMMs.\nThe same weighting procedure is applied to the determination of the variances. Again, the responsibilites\nare used as weighting factors (see equation 8.13).\nΣk = 1\nNk\nN\nX\nn=1\nγk(xn)(xn −µ)(xn −µ)⊤\n(8.13)\nEquation 8.13: Determiniation of the weighted variances for GMMs.\nAs a last adaption step, the mixing coeﬃcients πk have to be adjusted, where PK\nk=1 πk =1 must be\nrespected. The normalization is realized by adding a Lagrange multiplier (Bertsekas 1996), which\nresults in equation 8.14, where Nk =PN\nn=1 γk(xn).\nπk = Nk\nN\n(8.14)\nEquation 8.14: Determiniation of the mixing coeﬃcients for GMMs.\nProperties of EM\nThe EM algorithm has some inherent advantages and disadvantages which are\nbrieﬂy discussed in the following. A decisive advantage is that the EM algorithm does not contain any\npage 102\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nRelated Work\nhyper-parameters that have to be adapted to the problem in a complex manner, e.g., by grid-search.\nThis advantage constitutes a direct contrast to stochastic methods, e.g., SGD, where at least a learning\nrate ϵ needs to be speciﬁed. A disadvantage, however, is related to the consideration of EM as batch\nlearning algorithm which does not allow for incremental training (see section 2.3.2). Consequently, the\nentire dataset has to be accessible, which makes EM unsuitable for very large datasets or streaming\nscenarios.\nAnother problem is related to an assumption of the EM algorithm. Namely, that the latent variable\n(un-observable) can be removed by marginalization. However, this is only the case if the loss function\nis truly convex, which in turn is only true if the samples are drawn from exactly one component at a\ntime. The Jensen’s inequality (Cvetkovski 2012) solely holds for the assumption of marginalization (as\na lower bound), while ensuring that each E-step truly guarantees an improvement. Nevertheless, the\nEM method works for many types of problems. Regardless of the EM training process, a certain set of\ncomponents based on the problem needs to be available.\n8.2\nRelated Work\nThis section presents related work in the context of the novel approach, and particularly with regard\nto GMMs. Accordingly, the main focus is on the design of a training procedure for GMMs that can be\nused in the context of CL. Another main aspect is related to the transformation of the hierarchical\nmodels into a “deep” model design.\nIncremental GMM Training\nThe ﬁrst subject of the related work addresses the challenges raised\nby the standard EM algorithm (Dempster, Laird, et al. 1977). The prerequisite for the application of\nthe EM is constituted by the complete dataset that is needed for the E-step. However, the provision of\nthe complete dataset is impractical for today’s size of datasets. Early attempts have tried to enable\nincremental learning (Titterington 1984). However, many processes still present challenges: “Numerical\nintegration is often necessary and the fact that we are dealing with incomplete data will add to the\ncomplications.” (Titterington 1984).\nA frequently used variant is proposed by Cappé and Moulines (2009). The stochastic EM (sEM)\nalgorithm is presented as an online variant of EM. sEM is able to replace the M-step by a stochastic\napproximation. An advantage compared to Titterington’s (1984) proposal is that the parameter\nconstraints (see section 8.1.4) are automatically fulﬁlled. The work of Chen, Zhu, et al. (2018) (variance\nreduced stochastic EM (sEM-vr)) illustrates that these algorithms are still the focus of current research.\nAnother approach is based on the so-called “core-sets” as proposed by Feldman, Faulkner, et al. (2011).\nA sub-dataset of the entire dataset, which is presented as a data stream, is derived as a representative\nfor the entire dataset. The sub-dataset is referred to as core-set and used for training. In the context of\nstreaming scenarios, the core-set is extracted from each new data block and merged with the previous\ncore-set.\nAnother online variant is proposed by Vijayakumar, D’Souza, et al. (2005) with Locally Weighted\nProjection Regression (LWPR). Their model can be considered as a GMM, which can be updated\nonline based on gradients.\nNumber of GMM Components\nAnother problem is the deﬁnition of the number of Gaussian\ncomponents K, which cannot be adjusted in retrospect. Vlassis and Likas (2002) propose a greedy EM\nvariant in order to overcome the problem of deﬁning the number of required GMM components . Unlike\nthe data-driven method by Ghahramani and Hinton (1997) (Mixtures of Factor Analyzer (MFA)) or\nTipping and Bishop (1999) (Principal Component Analysis (PCA)), Vlassis and Likas’s (2002) approach\ndoes not rely on data for pre-deﬁning the number of the necessary components. On the contrary, the\ngreedy variant (based on Li and Barron’ (2000) theory) successively adds further components. The\nwork of Engel and Heinen (2010) follows a similar approach. A new component is added each time the\nminimum likelihood criterion is met by a new sample.\nPinto and Engel (2015) introduce the Incremental Gaussian Mixture Network (IGMN), which focuses\non reducing the time complexity from O(NKD3) to O(NKD2). The precision matrix (Bernardo and\nSmith 2008; Bishop 1995) is used instead of the covariance matrix in order to achieve computational\neﬃciency.\nThe advantage of the proposed method is that the calculation of the inverse of the\ndeterminant (O(D3)) is replaced by an operation with a time complexity of O(D2). Furthermore, the\nBenedikt Pfülb\npage 103\n\nSGD for GMM Training\nNovel Deep Learning Model: DCGMM\nIGMN model allows for the dynamic addition of new components. Merging statistically equivalent\nGaussian components is another approach presented by Song and Wang (2005). Their approach is\nintended to enable online data stream clustering. Taking into account the equivalence criterion (W\nstatic for covariances and Hotelling’s T 2 for means), access to past data is veriﬁed and granted if\nnecessary.\nThe approach of Kristan, Skočaj, et al. (2008) comprises the addition and removal of Gaussian\ncomponents (unlearning). In order to add GMM components, the second derivative of a batch of\nsamples is calculated and the “asymptotic mean integrated squared error” is determined. The proposed\nmechanism is used in an iterative merge process until convergence behavior is established. The validity\nof their method has been proven for one-dimensional distributions (Kristan, Skočaj, et al. 2008).\nGMM Initialization\nAnother fundamental problem concerns all variants based on the EM algorithm,\nwhich heavily depend on the initial state of the Gaussian components. The initialization problem is\nreviewed, for example, by Baudry and Celeux (2015). Diﬀerent variations of the EM algorithm (among\nothers sEM) are investigated using diﬀerent initialization mechanisms. Baudry and Celeux (2015)\nidentify the fundamental problem: “[...] the initialization issue remains and can be a most inﬂuencial\nfactor.”\nSGD Training Methods\nHosseini and Sra (2015) describe an alternative method for training\nGMMs based on Riemannian manifold optimization technique. In the latest work of Hosseini and\nSra (2020), a SGD based solution is presented. Their approach introduces several hyper-parameters.\nBesides, it circumvents the initialization problem by a variant of k-means (k-means++).\nAnnealing\nA similar approach is presented by Verbeek, Vlassis, et al. (2005) who use the EM-\nalgorithm in order to allow for a self-organization of the components for mixture models. A behavior\nsimilar to standard Self-Organizing Maps (SOMs) (Kohonen 1982) is achieved by adding a penalty\nterm.\nLog-Likelihood Approximations\nSeveral diﬀerent suggestions have been made for the approx-\nimation of the log-likelihood. Four approximations are presented by Pinheiro and Bates (1995):\nLinear Mixed-Eﬀects-, Laplacian-, Importance Sampling-, and the Gaussian quadrature approximation.\nOrmoneit and Tresp (1998) propose the maximum penalized likelihood and a Bayesian approach, which\nboth provide better results than the maximum likelihood. It aims at the avoidance of the determination\nproblems in the high-dimensional data spaces, such as singularities or local maxima. A variant which\nis particularly more computational eﬃcient is presented in the work of Dognin, Goel, et al. (2009). A\nsimilar approach is described by Van Den Oord and Schrauwen (2014). It is characterized by a “hard”\nassignment.\nHigh-Dimensional Data\nTraining with high-dimensional data is a common and widely researched\nproblem. Ge, Huang, et al. (2015) tackle the problem by using several moments (3rd, 4th and 6th\norder) in order to characterize the Gaussian components of a GMMs. However, their approach has\na disadvantage, as it is diﬃcult to implement in streaming scenarios. The work of Richardson and\nWeiss 2018 shows that GMMs can be used in high-dimensional domains, such as the generation of\nimages. Goodfellow, Pouget-Abadie, et al. (2014) compare them with a current research topic/model,\nnamely Generative Adverserial Networks (GANs). The problem of determining the inverse of large\nmatrices is realized by using MFA (Richardson and Weiss 2018). Another fundamental problem of\nhigh-dimensional data is circumvented by the so-called “logsumexp” trick (Nielsen and Sun 2016).\n8.3\nStochastic Gradient Descent for Training Gaussian Mixture\nModels\nStochastic Gradient Descent (SGD) is an eﬃcient method to adjust parameters within a model (see\nsection 2.2.2.2). Models can be trained incrementally based on the SGD (see section 2.3.2 for a\ndeﬁnition), which is a requirement for many CL scenarios. In addition, gradient descent is used as the\nbasis for almost all deep learning approaches. Incremental learning, however, only describes the step\npage 104\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nSGD for GMM Training\nby step extraction of knowledge from data – not the preservation of existing knowledge. The ability to\nlearn in an incremental manner does not ensure that a model is automatically suitable for CL, which\nis due to the CF eﬀect (see section 2.3.4).\nThis section shows how GMMs can be eﬃciently trained by a SGD based method. In order to\nenable SGD training, the GMM requirements need to be satisﬁed. These include, for example, the\nnormalization of the weights π or the positive-deﬁniteness of the covariance matrix Σ. How these\nconstraints are fulﬁlled is described in section 8.3.1. Due to numerical issues, it is challenging to\nuse the standard log-likelihood as a loss function for the model parameter optimization. A possible\nsolution is presented in section 8.3.2. Another crucial and already mentioned problem is related to\nthe initialization of a GMMs, which is often avoided in related work. In general, the complete dataset\nand another clustering algorithm (e.g., k-means) are used and initialized before the GMM training\nstarts. The problem is that a pre-initialization by means of the complete dataset is impossible in\nstreaming scenarios. In order to tackle the initialization problem, an annealing scheme is introduced in\nsection 8.3.3. The basic idea is related to the training of SOMs which is adapted to the domain of\nGMM training. This section concludes with a summary of the complete training process of GMMs by\nmeans of SGD (see section 8.3.4).\n8.3.1\nGMM Constraint Enforcement\nIn order to train GMMs via SGD, multiple requirements regarding the parameters need to be fulﬁlled.\nUsually, the EM algorithm fulﬁlls these requirements. Meeting the GMM requirements involves the\nfollowing two adjustments for each SGD based training step.\n8.3.1.1\nWeights π\nThe ﬁrst parameter that has to be adjusted concerns the weights π of the K GMM components. The\nweights describe the prior probability (up to this training point) for the sample x being drawn from the\nkth component. The following procedure is adapted from Hosseini and Sra (2015) and is comparable\nto the application of the Softmax function (see section 2.2.2.1). A corresponding weight πk is assigned\nto each component k. In order to normalize the weights, i.e., P\nk=1 πk = 1, the free parameter ξ is\nintroduced. Equation 8.15 illustrates that the weights can be normalized.\nπk =\nexp(ξk)\nP\nj exp(ξj).\n(8.15)\nEquation 8.15: Normalization of GMM component weights π.\n8.3.1.2\nCovariance Matrix\nFor computational reasons, the covariance matrix is not used directly. Instead, its inverse matrix\n– the precision matrix P = Σ−1 – is applied (see Bernardo and Smith 2008; Bishop 1995). The\ndiagonal covariance (Σdiag) or the diagonal precision matrix respectively is introduced as yet another\nsimpliﬁcation (see section 8.1.3). This is required due to the particularly high dimensionalities of the\ndata. Determining the full covariance matrix is too memory intensive and therefore omitted.\nThe diagonal matrices Dk (for each component k) are re-parameterized as Pk = D2\nk. By using\na Cholesky decomposition/factorization (Mayers, Golub, et al. 1986), a symmetric positive-deﬁnite\nmatrix can be decomposed into the product of a triangular matrix and its transpose. As a result,\nthe used precisions are guaranteed to be positive-deﬁnite. For the diagonal case detΣk =detP −1\nk\n=\n\u0000det(D2\nk)\n\u0001−1 =\n\u0000tr(Dk)\n\u0001−2 can be calculated. Due to the fact that the decomposition is still complex,\nit is only performed once. The elements below the diagonal need to be removed after each gradient\ndescent step.\nBenedikt Pfülb\npage 105\n\nSGD for GMM Training\nNovel Deep Learning Model: DCGMM\n8.3.2\nMax-Component Approximation\nThe minimization of the loss function L is performed by means of SGD. A problem is that underﬂows\noften occur during the determination of the loss. High data dimensionalities are responsible for the\nunderﬂows. The problem causes divisions by zero, which leads to Not a Number (NaN) values in\ncomputer-aided applications. Instead of using the vanilla log-likelihood as a loss function, a lower\nbounded version is introduced, which is referred to as max-component approximation. The approxi-\nmation is denoted as ˆL and illustrated in section 8.3.2. k∗is deﬁned as arg maxk πkNk(xn, µk, Σk)\nand represents the loss value based on the best matching unit (BMU). A BMU is the component that\nprovides the highest responsibility for a given sample.\nL = En\n\"\nlog\nX\nk\nπkNk(xn)\n#\n,\nˆL = En\nh\nlog\n\u0000πk∗Nk∗(xn)\n\u0001i\nwhere\nL ≥ˆL\n(8.16)\nEquation 8.16: Max-component approximation ˆL.\nThe max-component approximation is not tight, but constitutes a valid approximation for high data\ndimensionalities (see Gepperth and Pfülb 2021 for a proof). Usually, the so-called logsumexp trick (see\nMcElreath 2018) is applied to handle numerical instabilities caused by exponentials. However, the\napplication of the logsumexp only mitigates the problem but does not eliminate it. Underﬂows occur\nwhen the distances increase, which is usually the case in the context of large data dimensionalities. If\n32-bit ﬂoats are used, for example, it is attempted to ﬁnd a component probability of Nk =e−101 with\nthe highest probability Nk∗= e3. This results in\nNk\nNk∗= e−104. The value exceeds the representable\nrange of a 32-bit ﬂoating point unit and leads to an underﬂow, respectively NaN. The loss function ˆL\npresented in section 8.3.2 has the advantage of avoiding these numerical problems.\nAnother problem related to the standard log-likelihood L is known as undesirable local optima.\nIn particular, degenerate solutions arise, which are omitted by the max-component approximation.\nHowever, degenerate solutions are characterized by all parameters (weights, centroids and covariance\nmatrices) taking almost the same values. For this reason ∀k holds that πk ≈\n1\nK , µk = E[X] and\nΣk = Cov(X), which is visualized in ﬁgure 8.6a. The component weights πk are indicated as part\nof the ﬁgure. The mentioned parameter conﬁguration causes vanishing gradients. Even though the\ndegenerate solution pattern is suppressed by the max-component approximation, a new problem arises,\nnamely a single-component solution or sparse-component solution. In both of these cases, only one or\na subset of GMM components is selected over and over again. This means that other components can\nnever become the BMU and, thus, never get adapted.\nAn exemplary sparse-component solution is illustrated in ﬁgure 8.6b. It is noted that mainly one\ncomponent is selected (with the highest π), which looks similar to components of the degenerate\nsolution. After the initialization, a ﬁrst random component is updated so that for all following steps\nonly the same one is selected as BMU. The degenerate or sparse-component solution problem is\naddressed by the annealing scheme introduced in the following section (section 8.3.3).\n8.3.3\nAnnealing Procedure\nA fundamental problem of training GMMs without data-driven initialization is related to undesirable\nlocal optima. This problem occurs whenever the parameters of the model are updated by SGD for the\nstandard log-likelihood L, as well as for the max-component approximation ˆL presented in section 8.3.2.\nSo-called sparse-component solutions occur almost every time (see ﬁgure 8.6b). They are characterized\nby only one or a few dominant components. These GMM components describe the complete or\na very large subset of the data: πki ≫0, µki = E[Xki ⊂X], Σki = Cov(Xki ⊂X). The remaining\ncomponents never become BMU, which is expressed by πk ≈0. These not-adjusted components\n(means µk, covariance matrix Σ respectively the precision matrices P ) remain in their initial state.\npage 106\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nSGD for GMM Training\n(a) A degenerated solution\n(b) A sparse component solution\nFigure 8.6: Degenarated and sparse-component solutions for the MNIST dataset.\nEquation 8.17 outlines that the gradients consequently vanish for δkk∗. Only the parameters of the\nBMU components, i.e., k∗, are adjusted.\n∂ˆL\n∂µk\n= En [Pk (xn −µk) δkk∗]\n∂ˆL\n∂Pk\n= En\n\u0002\u0000(Pk)−1 −(xn −µk)(xn −µk)⊤\u0001\nδkk∗\u0003\n∂ˆL\n∂πk\n= π−1\nk En [δkk∗] .\n(8.17)\nEquation 8.17: Gradients for the max-component approximation ˆL loss function.\nThe subsequently proposed annealing approach provides a mechanism to circumvent the vanishing\ngradient problem and to avoid the undesirable local optima. The fundamental idea is to punish the\nemerging characteristic response pattern. Accordingly, a new hyper-parameter σ is introduced, which is\nconstantly being reduced until it disappears. The new loss function is represented by ˆLσ and denoted\nas smoothed max-component log-likelihood. As shown in equation 8.18, a term is added which depends\non the σ parameter. The term indicates that not only the BMU is updated by SGD, but also the\nneighboring components. Thereby, the σ parameter deﬁnes the intensity’s inﬂuence on the neighboring\ncomponents. The smoothed max-component approximation deﬁnes an eﬀective upper bound for ˆLσ\n(see Gepperth and Pfülb 2021 for a proof).\nˆLσ = Enmaxk\n\" X\nj\ngkj(σ) log\n\u0010\nπjNj(xn)\n\u0011#\n= En\nX\nj\ngk∗j(σ) log\n\u0010\nπjNj(xn)\n\u0011\n.\n(8.18)\nEquation 8.18: Smoothed max-component approximation ˆLσ.\nIn order to deﬁne a factor for inﬂuencing the neighboring components, various structures can be\nimplemented. One possibility is to arrange the K GMM component in a grid of the size\n√\nK×\n√\nK. If\na component k∗is chosen, it inﬂuences all neighboring components. The inﬂuence value is described\nby an uni-modal Gaussian proﬁle of spatial variance ∼σ2. In order not to favor/disadvantage the\ncomponents on the edges, the grid is assumed to be periodical (or spherical). The visualization in\nﬁgure 8.7 illustrates the type of inﬂuence over time. Figure 8.7 is up-scaled for visualization. For\nBenedikt Pfülb\npage 107\n\nSGD for GMM Training\nNovel Deep Learning Model: DCGMM\nK =25, each grid area would consist of 5×5 pixels.\nDiﬀerent stages of the smoothing ﬁlters are shown in ﬁgure 8.7. On the left-hand side, an initial\nhigh value for σ is applied. In each ﬁeld, the inﬂuence of neighboring components is indicated, whereas\ndarker faces represent a higher inﬂuence on the closest component. If the most upper left component\nis selected as BMU, for example, the periodic eﬀect is clearly visible in the upper right, lower left and\nright corner. During the training process, σ is reduced (see middle) until ﬁnally only k∗is updated\n(see right). A one dimensional grid is another possible structure, as it requires the same computational\neﬀort as a 2D grid. A side eﬀect of the applied annealing scheme is that the GMM components exhibit\ntopological sorting. Its center depends on the choice of the ﬁrst BMU and the respective sample.\nFigure 8.7: Visualization of diﬀerent Gaussian smoothing ﬁlters.\nAdjusting the introduced annealing parameter σ raises a new challenge. At the beginning of the\ntraining process, the value is set to σ(t)=σ0. For the reduction, σ←0.9σ is calculated. σ is therefore\nreduced asymptotically until the limit σ = σ∞is reached. This reduction mechanism guarantees a\nsmooth transition from ˆLσ (equation 8.18) to ˆL (section 8.3.2). An applicable default value for the\nlower and upper σ-threshold is speciﬁed in section 8.3.4.\nDue to the fact that the inﬂuence of σ represents an upper bound with respect to the quality criterion,\nσ is reduced whenever the loss does not improve any further. In order to determine the stationarity of\nthe loss, the sliding smoothed average of the loss value is traced: ℓ(t)=(1 −α)ℓ(t −1) + α ˆLσ(t) . α is\nutilized to deﬁne the time component by validating the increase of ˆL every 1\nα iteration. As soon as\n∆<δ is reached (see equation 8.19), σ is reduced as described.\n∆=\nℓ(t) −ℓ(t −α−1)\nℓ(t −α−1) −ˆLσ(t = 0)\n(8.19)\nEquation 8.19: Deﬁnition of the σ reduction criterion ∆.\n8.3.4\nTraining Procedure\nIn this section, all of the presented requirements, methods and adaptions used for the SGD based\ntraining of GMMs are combined. Accordingly, the universal initial values for parameters are presented.\nIn addition, the complete SGD training process is summarized in algorithm 8.2.\nFirst of all, SGD based training requires the deﬁnition of diﬀerent parameters. These deﬁnitions\nare stated in line 1 of algorithm 8.2. The learning rate ϵ is always problem-dependent. A relatively\nsmall learning rate often results in longer convergence times. A too high learning rate usually inhibits\nreaching an acceptable local optimum. The same applies to the SGD based GMM training. The time\nparameter α (reduction criterion of σ) can be set to the same value as the learning rate (α=ϵ). The\nrelated parameter δ (limit for the detection of stationarity) is set to 0.05, which is a good choice for all\ninvestigated datasets. The diagonal matrix for the precision is set to DmaxI, where Dmax =20 works\nfor all experiments.\nGMM component means µ are initialized via small random numbers ([−µi, +µi]). Component\nweights π are initialized to\n1\nK . Regarding the number of components K, the bottom line is “more is\npage 108\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nDeep Convolutional Gaussian Mixture Models\nalways better” (depending on memory and computational limitations). The smoothed max-component\nlog-likelihood ˆLσ from section 8.3.2 is used for optimization. Initial σ values are chosen large enough\nfor all components to be aﬀected by the SGD update step. A default value is\n√\nK, which oﬀers a\ngood initialization. σ∞can be deﬁned so that the annealing process completely vanishes or becomes\nneglectable.\nThe training loop is represented in line 2. Line 3 deﬁnes the creation or update of the annealing\nmask, depending on the current σ. Subsequently, parameters are updated based on the determined\ngradients (line 4). The following two steps describe the enforcement of the GMM constraints (see\nsection 8.3.1). Firstly, the precision matrices are clipped in line 5 in a way that the value range of the\ndiagonal entries remains between 0 and D2\nmax. The clipping prevents entries from growing beyond the\nrepresentable range of values, which is important for data points that always remain the same (e.g.,\nblack border pixels). Secondly, line 6 represents the normalization step of the component weights.\nFinally, the current loss needs to be checked for stationarity. Therefore, the current sliding average\nof the loss is updated (line 7). If a stationarity of the loss is detected (lines 8 and 9), the σ is reduced\n(line 10). During the (step by step) training process, a smooth transition towards the max-component\napproximation ˆL is achieved.\nAlgorithm 8.2: Steps of SGD-GMM training.\nData: initializer values: µi, K, ϵ, σ0/σ∞, δ and data X\nResult: trained GMM model\n1\nµ ←U(−µi, +µi), π ←1/K, P ←IDmax, σ ←σ0\n// initialize parameters\n2\nforall t < T do\n// training loop\n3\ng(t) ←create_annealing_mask(σ,t)\n// see section 8.3.3\n4\nµ(t) ←ϵ ∂ˆ\nLσ\n∂µ +µ(t-1), P (t) ←ϵ ∂ˆ\nLσ\n∂P +P (t-1), π(t) ←ϵ ∂ˆ\nLσ\n∂π +π(t-1)\n5\nP (t) ←precisions_clipping(P , Dmax)\n//see section 8.3.1\n6\nπ(t) ←normalization(π(t))\n//see equation 8.15\n7\nℓ(t) ←(1−α)ℓ(t−1)+α ˆLσ(x(t))\n// sliding likelihood\n8\nif annealing update iteration then\n9\nif ∆< δ then\n// ∆see equation 8.19\n10\nσ(t) ←0.9σ(t−1)\n8.4\nDeep Convolutional Gaussian Mixture Models\nThe training procedure described in section section 8.3 summarizes a way to eﬃciently train GMMs by\nSGD. Vanilla GMMs are still considered unsuitable for supervised ML scenarios. The proposed GMM\ntraining method cannot be used for CL tasks as described in chapter 6. Furthermore, GMMs and their\nability to approximate complex functions strongly depend on the number of Gaussian components K.\nAn adequate approximation of functions is very memory and computation intensive for complex data\ndistributions. DNNs do not have this problem, since they can approximate complex functions due to\ntheir “depth” (expressed by the universal approximation theorem).\nHow GMMs can be stacked like layers of artiﬁcial neurons is described in this section. The stacking\nshould allow for the approximation of more complex functions with less computational resources (GMM\ncomponents). An additional mechanism from the area of image processing is introduced, namely\nfolding. Folding is carried out in a manner similar to Convolutional Neural Networks (CNNs). The\ncombination of stacking GMMs and folding operations is referred to as Deep Convolutional Gaussian\nMixture Model (DCGMM). Thus, DCGMMs consist of several diﬀerent layers which transform it into\na deep learning model. Diﬀerent types of layers are introduced, among others, a folding layer and\na classiﬁcation layer. The latter enables the classiﬁcation of samples, which allows DCGMMs to be\nclassiﬁed as supervised learning model.\nDCGMMs oﬀer a wide range of functionalities, such as classiﬁcation, density estimation and\nsampling.\nIn the following, these functionalities are described and illustrated.\nSampling is an\ninteresting research area, which includes the manipulation of images and videos. At the same time, the\nBenedikt Pfülb\npage 109\n\nDeep Convolutional Gaussian Mixture Models\nNovel Deep Learning Model: DCGMM\nability to generate samples is an integral part of pseudo-rehearsal approaches in the ﬁeld of CL. GMMs\ncan generate samples themselves (independent of their training type), but with some restrictions.\nIn the beginning of the following section, diﬀerent types of layers that can be stacked into a deep\nmodel are introduced (see section 8.4.1). Section 8.4.2 presents the architecture-level functionalities of\nDCGMMs. The fundamental functionalities (inference and sampling) are described by a step-by-step\nvisualization.\n8.4.1\nTypes of Layers\nDCGMMs are hierarchical models consisting of diﬀerent types of layers. The diﬀerent layer types,\nas well as their input and output formats are introduced in the following. Diﬀerent functionalities\nof each layer are provided for each direction or data ﬂow mode, i.e., estimation and sampling. The\nposition of a layer is indicated by L. An output- or input tensor of a layer is denoted according to\nthe commonly used NHWC format. The NHWC format is designed for image data and generally\nused in conjunction with CNNs. N speciﬁes the number of processed samples per step, which usually\ncorresponds to the batch size B. W and H deﬁne the width and height of the images, C the number\nof channels. The following applies to samples that are images: C =1 corresponds to a grayscale image,\nwhile C =3 represents colored (RGB) pictures.\nEach output tensor of a layer is represented by O(L) ∈R4. The forward direction is referred to as\nestimation. For an estimation, each layer with the index L converts the output tensor of the previous\nlayer O(L−1) into its own output O(L). The reverse direction is known as sampling and the input\ntensor is presented as I(L) ∈R4. I(L+1) is obtained from a certain layer L and transformed into I(L).\nDepending on the type of layer, the internal parameters are represented by θ(L).\n8.4.1.1\nFolding Layer\nForward/Estimation\nThe forward step of the folding layer performs a similar transformation as\nthe convolutional layers of CNNs. The input signal (usually an image) is decomposed by a folding\nlayer into several sub-signals. The exact operation is determined by several parameters. In order to\nextract diﬀerent parts/patches of an image, a ﬁlter size f (L)\nX , f (L)\nY\nand its shift (stride) ∆(L)\nX , ∆(L)\nY\nare deﬁned. Thus, each extracted patch is dumped into a separate channel (C) in the folding layer’s\noutput. The output signal O(L−1)\nNHW C of the previous layer is transformed to O(L)\nNH′W ′C′ by the operation\ndeﬁned in equation 8.20.\nH(L) = 1 + H(L−1) −f (L)\nY\n∆(L)\nY\n,\nW (L) = 1 + W (L−1) −f (L)\nX\n∆(L)\nX\nand\nC(L) = C(L−1)f (L)\nX f (L)\nY\n(8.20)\nEquation 8.20: Determination of the output dimensions of a folding layer.\nFigure 8.8 illustrates the forward step for the folding layer based on an example (reading direction:\nleft to right). For the sake of simplicity, a batch size B of 1 is assumed, so that N can be omitted. The\nsimpliﬁed image dataset consists of two samples with a resolution of 5×5 pixel (grayscale). Images\nhave not been rasterized. Solely the grid is supposed to indicate the resolution as depicted in ﬁgure 8.8.\nThe shape of the previous layer output is: 1, 5, 5, 1. Parameters of the folding layer are: The ﬁlter size\nfY =fX =3 and the stride ∆Y =∆X =2. Thus, a ﬁlter of the size 3×3 with the size 2 in both, x and y\ndirection is pushed over the input signal. The application of the ﬁlter extracts multiple patches which\nare dumped into the output’s one-dimensional channels. The extraction is represented in ﬁgure 8.8\nby diﬀerent colors. The equation equation 8.20 allows the calculation of the size of the output tensor\nO(L)\nNW HC =1, 2, 2, 9. A folding layer does not have any adjustable layer parameters θ(L), as merely a\ntransformation of the signal is performed.\npage 110\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nDeep Convolutional Gaussian Mixture Models\nFigure 8.8: Exemplary visualization of the forward operation of a folding layer.\nBackward/Sampling\nFor generating samples, the folding layer has to invert the splitting process\nby merging the individual patches. In the simplest case, the ﬁlter size equals the stride. In this special\ncase, merging is realized by assembling the individual patches. Merging gets more complicated when\nthe patches overlap, as shown in ﬁgure 8.9 (reading direction: right to left). The individual patches\nprovided as channel vectors (color coded in ﬁgure 8.9) from I(L+1) need to be transformed back, based\non the ﬁlter size fY =fX. Various strategies can be applied for re-assembling the patches. One strategy,\nfor example, would be to average the overlapping pixels. The merging procedure is subject to losses\nas illustrated on the left-hand side of ﬁgure 8.9 Therefore, a mechanism for improving the quality is\npresented later in section 8.4.2.5.\nFigure 8.9: Exemplary visualization of the sampling operation of a folding layer.\n8.4.1.2\nPooling Layer\nForward/Estimation\nThe introduced pooling layer performs the same operations as the pooling\nlayer in CNNs (see Zeiler and Fergus 2014). A standard max-pooling layer has the kernel size k(L)\nY\nand k(L)\nX\nalong with the stride parameters ∆(L)\nX\nand ∆(L)\nY . Depending on the deﬁned parameters, the\nforward operation implements a aggregation/compression of several values that is subject to losses.\nDiﬀerent methods can be applied for the reduction operation, e.g., maximum or average.\nFigure 8.10 shows an example of the aggregation operation for max- and average-pooling. A 4×4\nmatrix is given on the left-hand side. Furthermore, a pooling layer with k(L)\nY\n=k(L)\nX =2 and a stride of\n∆(L)\nX =∆(L)\nY\n=2 is utilized. The resulting kernels are illustrated by diﬀerent colors and the respective\naggregated values on the right-hand side. Overlaps of kernels may occur, depending on the striding\nparameter. Especially the edges of the matrices require special treatment, e.g, padding with zeros.\nBackward/Sampling\nTo generate samples, the pooling operation needs to be reversed. The pooling\nparameters include the kernel and the stride (k(L)\nY , k(L)\nX\nand ∆(L)\nX , ∆(L)\nY ). A simple reverse operation\nfor unpooling is illustrated in ﬁgure 8.11. One and the same input value is used several times depending\non the pooling parameters. Again, other reverse operations could be realized. One example that\nrequires additional internal parameters is proposed by David and Netanyahu (2016). In this case, the\nBenedikt Pfülb\npage 111\n\nDeep Convolutional Gaussian Mixture Models\nNovel Deep Learning Model: DCGMM\nFigure 8.10: Exemplary visualization of the pooling operation.\nposition of the highest value is stored and reused. In general, the information loss of the forward\noperation cannot be fully recovered. Besides the unpooling operations, other methods related to image\nprocessing can be utilized, e.g., the nearest-neighbor up-sampling (known as scaling).\nFigure 8.11: Exemplary visualization of the unpooling operation.\n8.4.1.3\nGMM Layer\nForward/Estimation\nA single GMM layer represents an independent GMM consisting of K compo-\nnents, as stated in section 8.1. The trainable parameters include the weights πk, the means/centroids\nµk and the covariances Σk where k∈K. A GMM layer is referred to as “convolutional”, if each channel\n(H ×W) of the previous output O(L−1)\nNHW C is mapped using the same parameters θ(L). In ﬁgure 8.8,\nthe output of a folding layer is illustrated. The colored channel vectors (O(L−1)\nNW H:) correspond to the\ninput of the convolutional GMM layer. Each component k is thus able to learn from a patch of the\npredecessor layer and determines their individual aﬃliation to each component k. Alternatively, a\nnon-convolutional GMM layer consists of separate parameters for each input channel, which requires\nmore resources. However, non-convolutional GMM layers cause the number of parameters to be\ndependent on W (L−1)×H(L−1). They would thus strongly depend on the convolution parameters of\nthe predecessor layer.\nThe output of a convolutional GMM layer comprises the responsibilities for each input patch. The\ndetermination of responsibilities is outlined in equation 8.21. In addition, the loss value as a separate\noutput of a GMM layer can be valuable.\npNHW k\n\u0000O(L−1)\u0001\n= Nk\n\u0000O(L−1)\nNHW :|µk, Σk\n\u0001\nO(L)\nNHW K ≡\npNHW k\nPW (L−1)×H(L−1)\ni=1\npNHW i\n(8.21)\nEquation 8.21: Determination of the convolutional responsibilities of a GMM layer.\nThe smoothed max-component approximation from section 8.3.3 is applied for training a GMM layer.\nThe training is executed by the presented SGD-based approach (see section 8.3.4). Equation 8.22\ndescribes the convolutional loss function L, which takes each individual patch into account.\nFigure 8.12 visualizes an exemplary GMM layer (reading direction: left to right). The output signal\nof the predecessor layer OL−1 corresponds to the folding operation in ﬁgure 8.8 (see section 8.4.1.1).\nEach color-coded channel vector represents a patch from the original input signal, e.g., an image. Due\nto the context of the visualization, it is assumed that the operation is performed for a single sample\npage 112\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nDeep Convolutional Gaussian Mixture Models\nL(L)\nHW =\nX\nn\nlog\nX\nk\nπkpNHW K(O(L−1))\nL(L) =\nP\nHW L(L)\nHW\nH(L−1)W (L−1)\n(8.22)\nEquation 8.22: Determination of the convolutional loss L of a GMM layer.\n(N =1). In ﬁgure 8.12, the GMM layer consists of K =3 ×3=9 components.\nThe GMM parameters θ, as shown in ﬁgure 8.12, include the means µ (top) and the covariances\nΣ (bottom). The weights πk belong to the free parameters, which are omitted here for the sake of\nsimplicity. Each parameter (µ, Σ) is arranged in a two-dimensional grid 3×3 (bold face), as described\nin section 8.3.3. The individual means and variances are transformed into 3×3 image in order to ease\ntheir illustration and interpretation.\nBased on the (already initialized) internal parameters and the output of the previous layer O(L−1)\nNHW C,\nthe corresponding responsibilities γ (see equation 8.22) are determined. The responsibilities are passed\non as the output O(L) of a GMM layer. The output format corresponds to the HW of the previous layer\nand the number of components K. Thus, the output contains the responsibility of each component k\nfor each patch. The complete inference process for a multi-layer DCGMM is described in section 8.4.2.2.\nFigure 8.12: Exemplary visualization of the forward operation of a GMM layer.\nBackward/Sampling\nThe backward operation of a GMM can be used to generate samples from\nthe derived data distribution. Initially, the input I(L+1) of the successor layer is used for component\nselection. The sampling process is presented in ﬁgure 8.13 (reading direction: right to left). Each\nchannel vector of I(L+1) deﬁnes the component out of which a sample should be drawn. For a non-folding\noperation, a GMM component represents complete samples. In the unsupervised case, a multinomial\ndistribution can be used for the component selection. A combination of the multinomial distribution\nand the weights π can be considered a selection criterion.\nBenedikt Pfülb\npage 113\n\nDeep Convolutional Gaussian Mixture Models\nNovel Deep Learning Model: DCGMM\nFor the case depicted in ﬁgure 8.13, the input signal I(L+1)\nNHW C deﬁnes for which patch a component\nk should be chosen. The components that are supposed to be selected are highlighted in green. The\ntransformed selection is shown below in order to match the arrangement of the representation of the\ninternal parameters θ (µ and Σ). Likewise, the selection of the corresponding GMM components is\noutlined in green. For each HW position in the input signal, a sample is generated for each patch\nxi by drawing samples from the diﬀerent multinomial distributions (xi ∼N(µi, Σi)). The generated\npatches are displayed as transformed image for visualization purposes. In the presented case, the\ninput results in the generation of an image constructed from 4 patches. The patches are passed in the\nNHWC format as input to the next layer (e.g., (un-)folding layer shown in ﬁgure 8.8).\nVarious procedures can be executed for the selection of components. In the scenario presented\nin ﬁgure 8.13, the channels of the input signal precisely deﬁne which component has to be selected.\nHowever, a speciﬁc selection criterion is not always obvious, as the input signal consists of real numbers\nand, in the best case, represents a probability distribution.\nFigure 8.13: Exemplary visualization of the sampling operation of a GMM layer.\nThe input signal I(L+1) is primarily used to select the components for generating samples. If the input\nsignal represents probabilities or is converted accordingly, it serves as selection criterion. Samples\ncan be generated from all components by using probabilities. In order to limit the selection signal, a\nmethod referred to as top-S-sampling is introduced. The top-S-sampling method describes the use\nof the S highest activations as selection criterion. This method prevents random sampling from less\nlikely components.\n8.4.1.4\nClassiﬁcation Layer\nForward/Estimation\nA classiﬁcation layer is deﬁned by a conventional linear layer that allows a\nlabel’s assignment to an input signal. DCGMMs can be categorized as supervised model due to their\nability to classify samples. For the training of the classiﬁcation layer, the labels of the samples need to\nbe given. The internal parameters θ comprise the weights W and the bias b. Therefore, linear layers\nconsist of standard artiﬁcial neurons as used in DNNs (see section 2.2.1.2).\nThe linear layer is trained by optimizing the cross-entropy loss by SGD (see section 2.2.2.1). The\nlogits (output of L−1, e.g., responsibilities of a GMM layer) are ﬂattened (O(L−1) ∈RN×HW C) and\npage 114\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nDeep Convolutional Gaussian Mixture Models\ntransformed by an aﬃne transformation (O(L) = ˆO\n(L−1)W (L)+b(L)). As a result, each input sample\ncan be assigned a class label based on the output of the previous layer. The dimension and number of\nvariables of the classiﬁcation layer depends on the number of input parameter.\nIn ﬁgure 8.14, a binary classiﬁcation problem is depicted, i.e., zero (blue) and one (green) (reading\ndirection: left to right). In general, the linear layer can be used for multi-class classiﬁcation. The\nprevious layer O(L−1) is assumed to be the output of a GMM layer (responsibilities γ) consisting of\nK =4 components. Thus, the probability that a sample has been created from a certain component is\nthe expected information value of the previous output. The linear layer was trained according to a\ncomponent’s responsibility for a speciﬁc class. This assumption can only be satisﬁed if the model is\nconverged. Based on the readout layer and the one-hot encoding (see section 2.2), an appropriate class\nlabel can be assigned to a sample.\nFigure 8.14: Exemplary visualization of the forward operation of a classiﬁcation layer.\nBackward/Sampling\nThe backward operation of the classiﬁcation layer inverts the labeling process.\nThus, it is possible to create samples by specifying a certain class, which is referred to as conditional\nsampling. For the generation of a sampling signal, the classiﬁcation is reversed by I(L−1) :=W (L)⊤I(L)−\nb(L). Given is a desired class as input that is encoded as a one-hot vector. Subsequently, it is converted\nas an input signal I(L−1) for the predecessor layer.\nThe sampling process is illustrated in ﬁgure 8.15 (reading direction: right to left). For visualization\npurposes, it is assumed that the predecessor layer (L−1) is a GMM layer, which passes on responsibilities\nduring training. The reverse input signal (I(L)) provides the basis for sampling by selecting a GMM\ncomponent for a speciﬁc associated class. In ﬁgure 8.15, a sample from the class zero should be created.\nThe linear layer converts the one-hot encoded class into an input signal for the layer above. A complete\nexample for the sampling process of a full DCGMM is presented in section 8.4.2.4.\nFigure 8.15: Structural visualization of the linear layer for sampling from a DCGMM.\n8.4.2\nDCGMM Functionalities\nDCGMMs oﬀer a variety of functions which are presented in this section. At ﬁrst, a strategy is\nproposed for training DCGMMs end-to-end. Subsequently, the inference process for classiﬁcation is\ndescribed in more detail. Another function is the ability to detect outliers, which is related to density\nestimation. Finally, the process of generating samples is outlined in greater detail. The respective\ndetails refer to conditional sampling and a strategy to improve the quality of the samples (image\nsharpening). Especially the conditional sampling and outlier detection oﬀer many applications in the\narea of CL, e.g., pseudo-rehearsal mechanism or the detection of tasks boundaries.\nBenedikt Pfülb\npage 115\n\nDeep Convolutional Gaussian Mixture Models\nNovel Deep Learning Model: DCGMM\n8.4.2.1\nEnd-to-End Training\nDCGMMs can be trained by SGD, even if they are not convolutional and do not have deep structures.\nHowever, not a single objective function is used for the training process, as it is the case for DNNs or\nCNNs. Instead, a individual loss function L(L) is used to optimize the parameters θ(L) of each GMM\nlayer L. Note that the process is not layer-by-layer but end-to-end. End-to-end means that one batch\nof samples is used to optimize parameters in all layers. Layer-by-layer corresponds to a layer-wise\ntraining, where always one layer after another is trained until convergence is achieved. Plain SGD is\nused for optimization, since advanced SGD strategies lead to problems (e.g., RMSProp (Tieleman and\nHinton 2012) or Adam (Kingma and Ba 2015)). The only peculiarity of the GMM training is that for\nsome training iterations the centroids are adjusted ﬁrst, before covariances are adapted. The divided\ntraining procedure accelerates the convergence behavior.\n8.4.2.2\nInference\nIn the following, the inference process is illustrated by using a simple DCGMM model consisting of\nthree layers (see ﬁgure 8.16, reading direction: left to right). The exemplary DCGMM architecture\nconnects the basic layers shown in section 8.4.1, excluding the pooling layer. First of all, a folding layer\nwith the ﬁlter fY =fX =5 and stride ∆Y =∆X =1 is deﬁned. Thus, the input signal (grayscale image)\nwith 5×5 pixels is dumped into the 1×1 ×25 dimensional channel vector. The vector is represented by\nthe long bar below the folding layer (see section 8.4.1).\nThe GMM layer consists of K = 9 components, whereas only the means are presented for the\nsake of simplicity. The model is already converged, which becomes evident by means of the drafted\nprototypes/centroids. In section 8.4.1, an input signal in form of an image representing the number 0\nis applied. The initial folding layer only conducts a transformation into a one-dimensional vector. The\nvector activates the components of the GMM layer diﬀerently, whereas one component shows a very\nhigh activation (marked in green). The activations (correspond to the responsibilities) are forwarded\nto the classiﬁcation layer. For high data dimensionalities, there is always one responsibility that is\n≈1.0 for an appropriately converged model. This conclusion is empirically veriﬁed in section 8.5.1.3.\nIn addition, the transformed responsibilities are depicted in section 8.4.1. The last linear layer has\n“learned” from the labeled data and the respective responsibilities from the GMM layer that class\nzero is assigned for the active (central) component. Thus, the associated class can be assigned to the\nsample.\nFigure 8.16: Exemplary DCGMM instance for visualizing the inference process.\n8.4.2.3\nOutlier Detection\nOutlier detection is a frequently used function of unsupervised learning/clustering methods, such as\nGMMs. It is utilized, for example, to identify samples that are not part of the training data, e.g.,\ncaused by measurement errors. Another example is the detection of anomalies in various areas such as\npage 116\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nDeep Convolutional Gaussian Mixture Models\nattacks on IT systems. The detection of outliers is more conﬁdent when a model is in a stable phase,\nfor example, after training. However, the deﬁnition of a sample as inlier or outlier is problem-dependent\nand therefore a hyper-parameter. For detection, the log-likelihood L(x) of a sample can be compared\nto the long-term average log-likelihood En(L) of the model. In addition, the variance Varn(L) of a\nbatch can serve as an indicator for detecting outliers.\nAn outlier (or inlier) can be determined as speciﬁed in equation 8.24 by each GMM layer L at\nany position of a DCGMM. The inﬂuence of the variance on the threshold can be deﬁned by the\nhyper-parameter C (see equation 8.24), For the convolutional part, an outlier can also be determined\nfor individual patches (HW) (see section 8.4.1.3). Considering the parameter conﬁguration from\nsection 8.4.2.2, where no convolution takes place in the ﬁrst folding layer, it is speciﬁed that H =W =1.\nThus, the proposed conﬁguration deﬁnes that the entire input sample is speciﬁed as inlier or outlier.\nO(L)\nNHW : ≡En\n\"\nL(L)\nNHW : −C\nq\nVarn\n\u0000L(L)\nNHW :\n\u0001\n#\n.\n(8.23)\nEquation 8.23: Determination of the outlier detection threshold.\nThe density signal O(L)\nNHW : can be used in equation 8.24 to determine whether a sample is an in- or\noutlier. Since C is problem-speciﬁc, C can only be determined by cross-validation. The larger C, the less\nrestrictive the outlier detection method. The stated outlier detection procedure is easy to implement,\nalthough more intelligent deﬁnitions of the threshold can be realized.\nfoutlier(x) =\n(\noutlier\nif L(L)\nHW < O(L)\nHW\ninlier\nif L(L)\nHW ≥O(L)\nHW\n(8.24)\nEquation 8.24: Determination of outliers and inliers.\n8.4.2.4\nSampling\nSampling is an interesting functionality that can be used in a wide range of applications, e.g., for CL\napproaches based on pseudo-rehearsal mechanisms. The sampling process for a model consisting of\nthree layers is illustrated in ﬁgure 8.17 (reading direction: right to left). Samples can be created in a\nconditional manner by adding a linear classiﬁcation layer on top. The classiﬁcation layer allows to\ngenerate an input signal for the predecessor (GMM) layer in order to speciﬁcally generate a sample\nfrom a certain component. Thus, samples can be generated selectively from a speciﬁc class.\nIn order to generate a sample from the class zero, the input class coded as one-hot vector needs\nto be inverted (marked green on the right-hand side). The resulting signal serves as a selector for a\ncorresponding component k (illustrated in a reshaped form and highlighted in green). Each GMM\ncomponent k is represented in ﬁgure 8.18 in a reshaped manner in order to ease interpretation. The\nselection strategy allows to create a sample from a certain component (center) by sampling from\na multivariate normal distribution (xk ∼N(µk, Σk)). The generated sample is also reshaped in\nﬁgure 8.18. Finally, the sample generated by the GMM layer needs to be reshaped into the original\ninput format (image). The reshaping process is the inverse transformation of the folding layer (left-hand\nside in ﬁgure 8.17).\nThe inversion of the linear layer is adapted for DCGMMs which makes it suitable for the selection\nof a GMM component. The applied softmax function is shift-invariant which allows for the inversion\nbased on a constant s. For the inverse of the weight matrix W , it is assumed that the columns are\northogonal, which is an approximation. The signal needs to be deﬁned within the value range [0, 1]\nand have a unit sum (like the responsibilities). Therefore, the signal represents the selection of a\ncertain class. Thus, the sampling control signal corresponds to the expected posterior probabilities of a\nGMM layer for a given class. At the same time, the weights π are ignored for generating the samples.\nBenedikt Pfülb\npage 117\n\nDeep Convolutional Gaussian Mixture Models\nNovel Deep Learning Model: DCGMM\nFigure 8.17: Exemplary DCGMM model for visualizing the sampling process.\nFigure 8.18: Exemplary GMM component visualization.\nNevertheless, the component weights π can be used in addition to the given selection criterion.\nI = s(W I + b) ⇒i ≈W ⊤\u0000s−1(I) + k −b\n\u0001\n(8.25)\nEquation 8.25: Inversion of the linear layer for conditional sampling.\n8.4.2.5\nSharpening\nWhen generating samples with DCGMMs, information may get lost by using folding or pooling layers.\nThe information loss is either due to the compression of the pooling operation or the overlapping\npatches during folding. Sampling, as shown in section 8.4.2.4, usually starts with the highest layer,\ne.g., in the conditional case with the linear classiﬁcation layer. The generated signal for sampling is\nforwarded end-to-end until the ﬁrst layer is reached. The quality of the generated samples suﬀers from\nthe non-invertable mapping, e.g., the composition of overlapping patches. In order to counteract the\nquality loss, the next higher GMM layer can adapt the resulting signal. The input signal is propagated\nforward (in the reverse direction). Accordingly, g gradient ascent steps with a step size of ϵS are\nperformed. Due to the application of the resulting gradients, the signal is improved as stated in\npage 118\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nEvaluation\nequation 8.26. As a result, the correlations mapped by the corresponding GMM layer L can be partially\nrestored. To conclude, all GMM layers in a DCGMM architecture can be useful for the sharpening\nprocess, whereas the highest GMM layer generally contributes the most to the improvements. After\nsharpening, the adjusted signal I(L−1) is passed to the layer above L−2.\nI(L−1) →I(L−1)+ ∂L(L)\n∂I(L−1) ϵS\n(8.26)\nEquation 8.26: Gradient ascend step to compensate quality losses (image sharpening).\n8.5\nEvaluation\nIn order to validate the presented SGD-based training approach for GMMs, it is compared to a variant\nof the standard EM training algorithm. sEM is a modiﬁed method for training GMMs in an incremental\nmanner introduced by Cappé and Moulines (2009). The goal is not to outperform sEM, but to obtain\nequivalent results that demonstrate the validity of the SGD-based GMM training approach. Various\nexperiments are conducted which allow a direct comparison of the clustering abilities as well as the\nﬁnal log-likelihood values. Thus, the inﬂuence of the annealing procedure on the overall result is also\ninvestigated. An evaluation of results always needs to take the initialization process into account.\nFor sEM, a conventional data-driven initialization is performed by using k-means, whereas the novel\napproach is applied without this initialization. Considering that a data-driven initialization requires the\ncomplete dataset, the stochastic variant of EM seems redundant. Even if the initialization is beneﬁcial\nfor sEM, the comparison to the SGD approach is valid. The results of the SGD and sEM training are\nsummarized in section 8.5.1. Furthermore, the experimental results for the deep convolutional variant\n(DCGMM) are presented in section 8.5.2. The diﬀerent functionalities of DCGMM are investigated by\nconducting experiments with various parameter conﬁgurations of DCGMMs.\nThe following conditions apply to all presented results, as each experiment is repeated ten times\nwith the same parameter conﬁguration. The resulting values are averaged as meta-results (including\nthe variance). Thus, the dependency of results on the initial state (random initialization) should be\nneglectable. In both variants, SGD and sEM, diagonal precision matrices are used. Otherwise, the\npresented datasets in section 6.1 would exceed the memory and computing resources. This is due to\nthe relatively “high” data dimensionalities of the samples. Moreover, all samples within each dataset\nare normalized to the value range [0, 1] by a min-max normalization (see section 6.1).\n8.5.1\nSGD Experiments\nFor the validation of the SGD-based approaches, ﬂat DCGMMs as simple GMMs variant are examined\nﬁrst in order to allow a comparison. In the following, the parameter conﬁgurations used for both\ntraining methods are speciﬁed. The number of GMM components is set to K =64 and a batch size of\nB=1 is applied.\nHowever, diﬀerent procedures are used for the initialization of SGD training. Precisely, the cluster\ncenters are initialized by µ=U(−µi, +µi), whereas µi =0.1 (if not stated otherwise). Dmax is set to 20\nfor the initialization of the precision matrices P .\nIn case of sEM training, the k-means clustering algorithm initializes the GMM components. In\ncontrast to sEM, the SGD approach does not require a data-driven initialization, e.g., via k-means.\nHowever, sEM would yield even poorer results without a data-driven initialization (as shown later).\nBesides the ﬁxed parameters, hyper-parameters were tuned by a grid-search. The following hyper-\nparameters were explored with regard to sEM: α∈{0.01, 0.5}, α0 ∈{0.05, 0.1}, ρmin ∈{0.001, 0.0001}.\n8.5.1.1\nClustering Performance\nIn order to compare the clustering performance of sEM and SGD training, two clustering metrics are\napplied: The Davies-Bouldin score proposed by Davies and Bouldin (1979), as well as the Dunn index\nBenedikt Pfülb\npage 119\n\nEvaluation\nNovel Deep Learning Model: DCGMM\nsuggested by Dunn (1973). In general, clustering metrics are based on the results of cluster center\nassignment. In each case, the best results from the grid search experiments are compared. In case of\nthe Davies-Bouldin score, less is better – whereas more is better with regard to the Dunn index. For\nreasons of performance (regarding the computation of the metrics), the ﬁrst 1000 samples of the test\ndataset were utilized for each measurement. The results for both metrics and both training methods\n(SGD and sEM) are presented in table 8.1.\nTable 8.1 shows a balanced distribution of the best values (marked in bold if better than half a\nstandard deviation). Even though the SGD approach shows slightly better results, it can be noted that\nboth methods are comparable with regard to their clustering performance. The SGD-based approach\nis therefore considered valid in terms of clustering performance.\nTable 8.1: Clustering performance comparison of SGD and sEM training.\nDataset\nMetric\nAlgo.\nDavies-Bouldin score\nDunn index\nSGD\nsEM\nSGD\nsEM\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nMNIST\n2.50\n0.04\n2.47\n0.04\n0.18\n0.02\n0.16\n0.02\nFashionMNIST\n2.06\n0.05\n2.20\n0.04\n0.20\n0.03\n0.19\n0.02\nNotMNIST\n2.30\n0.03\n2.12\n0.03\n0.15\n0.03\n0.14\n0.04\nDevanagari\n2.60\n0.04\n2.64\n0.02\n0.33\n0.01\n0.27\n0.04\nSVHN\n2.34\n0.04\n2.41\n0.03\n0.15\n0.02\n0.15\n0.02\n8.5.1.2\nRobustness of Initialization\nAnother investigation focuses on the robustness with respect to initialization of GMMs. In general,\nEM-based methods, i.e., sEM, are sensitive to initialization (see Baudry and Celeux (2015)). In order\nto demonstrate the robustness and the advantage of the annealing procedure, the eﬀects of diﬀerent\ninitial states are investigated.\nAfter initialization, GMMs are trained on 9 classes (1 to 9) for 3 epochs. For the centroid’s (µ)\ninitialization, three strategies are investigated: Random (µi ∈{0.1, 0.3, 0.5}), non-random (training\none epoch on samples of class 0) and without annealing. The ﬁnal log-likelihood values serve as a\nperformance criterion. The initialization of the precisions remains the same, as described above. If the\nchosen initial precision values are too small, undesirable solutions will result as stated in ﬁgure 8.6.\nThe outcomes of the experiments are presented in table 8.2.\nThe ﬁnal log-likelihood values show that the training with the annealing scheme is very robust with\nrespect to the initialization. This is especially true for random initialization. Small initialization values\nµi are considered as advantageous. Likewise, comparable results can be obtained for a non-random\ninitialization. Compared to the switched-oﬀannealing process, the added value becomes clearer. The\nswitched oﬀannealing mechanism mainly leads to sparse-component solutions, like for randomly\ninitialized sEM trained GMMs.\nTable 8.2: Diﬀerent random and non-random centroid initializations on SGD training.\nDataset\nInitialization\nrandom\nnon-random\nno annealing\nµi =0.1\nµi =0.3\nµi =0.5\ninit class 0\nµi = 0.1\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nMNIST\n205.47\n1.08\n205.46\n0.77\n205.68\n0.78\n205.37\n0.68\n124.1\nFashionMNIST\n231.22\n1.53\n231.58\n2.84\n231.00\n1.11\n229.59\n0.59\n183.0\nNotMNIST\n−48.41\n1.77\n−48.59\n1.56\n−48.32\n1.13\n−49.37\n2.32\n-203.8\nDevanagari\n−15.95\n1.59\n−15.76\n1.34\n−17.01\n1.11\n−22.07\n4.59\n-263.4\nFruits 360\n12 095.80 98.02 12 000.70 127.00 12 036.25 122.06 10 912.79 1 727.61\n331.2\nSVHN\n1 328.06\n0.94 1 327.99\n1.59 1 328.40\n1.17\n1 327.80\n0.94\n863.2\nISOLET\n354.34\n0.04\n354.36\n0.04\n354.36\n0.04\n354.20\n0.05\n201.5\npage 120\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nEvaluation\n8.5.1.3\nDensity Estimation\nAnother functionality of GMMs is the estimation of a sample’s density. Again, results of the sEM and\nSGD based training method are compared. The maximum achieved log-likelihood at the end of the\ntraining process serves as a reference point. Again, diﬀerent datasets from section 6.1 are used for the\nexperiments. For GMM training, each method can iterate 3 training epochs on each dataset. This\nshould be a suﬃcient training time for each algorithm to converge using a batch size of B=1.\nIn ﬁgure 8.19, the resulting centroids/prototypes (or means µ) from one of the conducted SGD\nexperiments are visualized. For most datasets, the prototypes are clearly visible, e.g., handwritten\ndigits in MNIST. However, for the SVHN dataset in ﬁgure 8.19b, the number of available prototypes\n(K =64) is not suﬃcient. Similar prototypes overlap, resulting in poor log-likelihood values as visible\nin table 8.3. As shown in table 8.3, both approaches produce similar results, except that sEM cannot\nhandle the dimensionality of SVHN (3 072) and Fruits 360 (30 000). Another diﬀerence is that the\napplied annealing scheme of the SGD approach leads to a topological sorting of the GMM components\n(similar elements are closer together, see ﬁgure 8.19).\n(a) MNIST\n(b) SVHN\n(c) FashionMNIST\n(d) Devanagari\n(e) NotMNIST\n(f) Fruits 360\nFigure 8.19: Exemplary results of centroids learned by SGD for diﬀerent datasets.\nThe comparison of the ﬁnal log-likelihood values in table 8.3 indicates the superiority of the SGD-based\napproach over sEM. All of the compared values that show superiority by more than half of a standard\ndeviation are highlighted in bold. The focus is not to challenge the sEM or other GMM training\nalgorithms, but to achieve equal values. In addition, the average responsibilities of the BMUs for\nthe test dataset (for SGD) are presented in table 8.3. These results show that for the average of the\ntested samples a single BMU (GMM component) is responsible – as expected for high-dimensional\ndata. Despite a k-means initialization that requires the complete dataset and a lot of computation\ntime, the SGD method seems superior. The superiority of the SGD approach is especially true for the\ndataset with the highest dimensionalities (SVHN: 3 072 and Fruits 360: 30 000 dimensions). With the\nexception of SVHN, the number of components is insuﬃcient for both training methods. Insuﬃcient\nGMM components are not a fundamental problem of the training method, but of the complexity of\nthe dataset in relation to the available number of components K.\nIn order to illustrate the problem of the sEM’s poor training results, the derived centroids for the\nSVHN and Fruits 360 dataset are depicted in ﬁgure 8.20. It becomes obvious that a sparse-component\nsolution has emerged for both datasets, as many components have low brightness values (on all RGB\nchannels). The visualized solution explains the lower log-likelihood results in table 8.3 for these two\ndatasets. Despite the k-means initialization with already meaningful prototypes being derived from\nthe dataset, sEM training can lead to a sparse-component solutions.\nBenedikt Pfülb\npage 121\n\nEvaluation\nNovel Deep Learning Model: DCGMM\nTable 8.3: Final log-likelihood comparison of SGD and sEM training.\nDatasetAlgo.\nSGD\nsEM\n∅max pk∗\nmean\nstd\nmean\nstd\nMNIST\n0.992 674\n216.6\n0.31\n216.8\n1.38\nFashionMNIST 0.997 609\n234.5\n2.28\n222.9\n6.03\nNotMNIST\n0.998 713\n−34.7\n1.16\n−40.0\n8.90\nDevanagari\n0.999 253\n−14.6\n1.09\n−13.4\n6.16\nFruits 360\n0.999 746 11754.3 75.63 5 483.0 1 201.60\nSVHN\n0.998 148\n1329.8\n0.80 1 176.0\n16.91\nISOLET\n0.994 069\n354.2\n0.01\n354.5\n0.37\nFigure 8.20: Visualization of sparse-component solutions for sEM for Fruits 360 and SVHN.\n8.5.2\nDCGMM Experiments\nIn this section, more complex DCGMMs architectures consisting of multiple layers are investigated.\nAs presented in section 8.4.1, diﬀerent types of layers are connected in diﬀerent combinations and with\nvarious parameter conﬁgurations. An initial investigation is performed with DCGMMs consisting of\ntwo and three GMM layers. Diﬀerent structures of the DCGMM architectures are deﬁned in table 8.4.\nIn the following, the individual layers and their parameter conﬁguration are deﬁned in table 8.4.\nFolding layers (F) are conﬁgured by the parameters fY , fX (patch size) and ∆Y , ∆X (stride). GMM\nlayers (G) are speciﬁed by the number of Gaussian components K. (Max-)Pooling layers (P) are\ndeﬁned by the kernel and the striding size: kY , kX, ∆Y , ∆X. Classiﬁcation layers (C) are omitted,\nbecause the application and functionality is investigated in the next chapter. The number of training\niterations is set to E =25 (epochs) for all experiments, unless stated otherwise. Likewise, sharpening is\nperformed for g=1000 iterations with the sharpening rate ϵS =0.1.\nFor reasons of layer parameterization, dataset dependent conﬁguration values are given in table 8.4.\nIn order to omit the adjustment of the folding operation to diﬀerent datasets, only datasets with\nthe same structure are investigated. However, an investigation of all DCGMM architectures for all\npresented datasets (see section 6.1) would exceed the resources. For this reason, two structurally\nsimilar datasets are investigated (MNIST and FashionMNIST).\nTable 8.4: Conﬁgurations of diﬀerent DCGMM architectures.\nlayer ID\n1L\n2L-a\n2L-b\n2L-c\n2L-d\n2L-e\n3L-a\n3L-b\n1\nF(28,28,1,1) F(20,20,8,8) F(7,7,7,7) F(8,8,2,2)\nF(28,28,1,1) F(4,4,2,2)\nF(3,3,1,1) F(28,28,1,1)\n2\nG(25)\nG(25)\nG(25)\nG(25)\nG(25)\nG(25)\nG(25)\nG(25)\n3\nF(2,2,1,1)\nF(4,4,1,1) F(11,11,1,1) F(1,1,1,1)\nF(13,13,1,1) P(2,2)\nF(1,1,1,1)\n4\nG(36)\nG(36)\nG(36)\nG(36)\nG(36)\nF(4,4,1,1) G(25)\n5\nG(25)\nF(1,1,1,1)\n6\nP(2,2)\nG(25)\n7\nF(6,6,1,1)\n8\nG(49)\ncomment\nvanilla\n1 conv.\n1 conv.\n1 conv.\nno\n1 conv.\n2 conv.\nno\nGMM\nlayer\nlayer\nlayer\nconvolutions\nlayer\nlayers\nconvolutions\npage 122\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nEvaluation\n8.5.2.1\nInterpretability of DCGMMs\nBefore presenting the experimental results, the interpretation of a two-layer DCGMMs is introduced.\nIn order to support the interpretation, the individual GMM layers (2 and 4) of the 2L-a architecture\n(stated in table 8.4) are visualized. The model is trained with samples from the MNIST dataset (classes\n0 to 4). The ﬁrst folding layer ensures that 4 overlapping patches (two in x- and two in y-direction)\nof the size 20×20 are extracted from each input image (∆Y =∆X =8). The left part of ﬁgure 8.21\nrepresents the derived prototypes (each 20×20 dimensions) of the ﬁrst GMM layer. On the very right,\nthe prototypes of the second GMM layer are depicted. In the center (outlined in red), a selected\nprototype of the second GMM layer is shown in an enlarged form. Accordingly, the dimensionality of\nthe selected prototype is based on the number of patches (2×2) and the number of prototypes of the\nprevious GMM layer (5×5). The dimensionality of the last GMM layer is not aﬀected by the previous\nfolding layer (layer 3). Thus, a prototype of the second GMM layer describes which prototypes of the\nprevious layer have a high activity (white corresponds to a high responsibility, black to low).\nIn case of inference, it is assumed that an input signal with a displayed 0 is applied. In the ﬁrst\nGMM layer (see ﬁgure 8.21), the input results in an activation of the 4 highlighted patches (indicated\nby color). The most active ones are shown, again, on the right-hand side of the prototypes. Moreover,\ntheir positions on the grid are indicated by their coordinates. The activation combinations remain\nunchanged and are passed on to the second GMM layer. Therefore, the 0 input generates an activation\npattern as shown in the red frame. Black indicates a low activation and white indicates a high\nactivation. The diﬀerent colored grids (gree, blue, pink and cyan) represent the positions of the input\npatches, whereas the initial folding is responsible for the patches. Each grid/quadrant can represent\nall component positions of the previous GMM layer. The second GMM layer thus learns diﬀerent\nactivation patterns from the ﬁrst GMM layer. The activation of the last layer’s component therefore\ncorresponds to the pattern of the input. The ﬁnal activation can be further processed, e.g. to perform\nclassiﬁcation.\nIn the reverse direction, i.e., for the generation of samples, the selection of the associated GMM\ncomponents is reversed. Again, it is assumed that the red GMM component on the right-hand side is\nselected for sampling. Accordingly, the enlarged activation pattern is generated ﬁrst and then passed\non to the preceding layer. The activation pattern is used to select the corresponding components of\nthe ﬁrst GMM layer (gree, blue, pink and cyan on the left-hand side). The four patches in ﬁgure 8.21\ncorrespond to the four corners of the sample that has to be generated. Finally, these patches need to\nbe combined into one image and, if necessary, optimized by the sharpening process.\nFigure 8.21: Interpretation of GMM components from the DCGMM instance (2L-a).\n8.5.2.2\nClustering\nSimilarly to the experiments in section 8.5.1.1, DCGMMs are examined with respect to their clustering\nperformance. Once again, the metrics Dunn index Dunn 1973 (more is better) and the Davies-Bouldin\nscore Davies and Bouldin 1979 (less is better) are applied. Accordingly, samples and the assigned\ncluster IDs are inspected. Results of the examined architectures (see table 8.4) are summarized in\ntable 8.5. For each of the two examined datasets, the best results for all architectures are highlighted\nin bold face. Mainly non-convolutional, but deeper DCGMMs imply a better clustering performance.\nAs a consequence, non-convolutional but deep DCGMM architectures should be preferred, if a high\nclustering capacity is the goal.\nBenedikt Pfülb\npage 123\n\nEvaluation\nNovel Deep Learning Model: DCGMM\nTable 8.5: Clustering performance comparison of diﬀerent DCGMM architectures.\nDataset\nMetric Architecture 1L 2L-a 2L-b 2L-c 2L-d 2L-e 3L-a 3L-b\nMNIST\nDunn index\n0.14 0.14 0.13 0.12 0.19 0.15 0.15 0.15\nDavies-Bouldin score 2.59 2.73 3.06 2.62 2.57 2.55 2.65 2.53\nFashionMNIST\nDunn index\n0.14 0.15 0.16 0.15 0.11 0.11 0.09 0.13\nDavies-Bouldin score 2.37 2.77 2.62 2.70 2.40 2.92 3.20 2.35\n8.5.2.3\nOutlier Detection\nOutlier detection is one of the frequently used functions of clustering methods. In order to determine\nthe capabilities of DCGMMs, the following experiments are performed. Firstly, the diﬀerent DCGMM\narchitectures from table 8.4 are trained on the ﬁrst ﬁve classes (0 to 4). Secondly, test samples of the\nclasses 0 to 4 are tested for inliers and samples from the classes 5 to 9 for outliers. For outlier/inlier\ndetection, the log-likelihood (log-probability) value of the last/highest layer is used as described in\nsection 8.4.2.3. In order to eliminate the choice of the parameter C of equation 8.23, C is varied for\nthe range [−2, 2]. Thus, a ROC-like curve (receiver operating characteristic curve) is obtained, which\nrepresents the ratio independently of this hyper-parameter.\nThe curves in ﬁgure 8.22 represent the diﬀerent architectures stated in table 8.4. As illustrated,\nconvolutional architectures provide signiﬁcantly better outlier detection performance. This performance\ncontrasts non-convolutional architectures, such as 2L-d and 3L-b. In the context of outlier detection,\nthe situation is diﬀerent as stated with regard to the clustering capacity in section 8.5.2.2.\n0\n25\n50\n75\n100\ninliers kept (%)\n0\n25\n50\n75\n100\noutliers kept (%)\n1L\n2L-a\n2L-b\n2L-c\n2L-d\n2L-e\n3L-a\n3L-b\n(a) MNIST\n0\n25\n50\n75\n100\ninliers kept (%)\n0\n25\n50\n75\n100\noutliers kept (%)\n1L\n2L-a\n2L-b\n2L-c\n2L-d\n2L-e\n3L-a\n3L-b\n(b) FashionMNIST\nFigure 8.22: Outlier detection capabilities of diﬀerent DCGMMs architectures.\n8.5.2.4\nSampling and Sharpening\nIn this section, the DCGMM architecture’s impact on sample generation is presented. The ﬁrst 5\nclasses (0 to 4) of the MNIST and FashionMNIST dataset (see section 6.1) are used for this purpose.\nAgain, the dataset selection is limited due to the speciﬁc parameters of the architectures and an eased\ncomparison. Since the diﬀerence in quality is visible at a glance, no explicit metric is used for the\nevaluation. Furthermore, the inﬂuence of the top-S-sampling mechanism is evaluated. Finally, the\nsharpening method is applied on the generated samples.\nSampling from Convolutional Architectures\nDCGMMs allow the splitting of input data into\npartials (patches) by the application of folding layers. In the following, the inﬂuence of the folding\noperation on image datasets (MNIST and FashionMNIST) is presented. Therefore, diﬀerent archi-\ntectures (see table 8.4) are used to generate sample images. The single-layer model (1L), as well as\nthe non-convolutional two-layer architecture 2L-d, are used as a reference (baseline). The diﬀerence\nbetween larger and smaller patches is illustrated by the 2L-c variant (larger) and the 2L-e architecture\n(smaller patches).\nThe exemplary generated samples depicted in ﬁgures 8.23 and 8.24 illustrate that non-convolutional\npage 124\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nEvaluation\narchitectures frequently create duplicate images. Some of the duplicates are marked with red letters\nwhich are considered to be identical (see ﬁgures 8.23a and 8.23b). In contrast, identical samples are\nnot generated with the convolutional variants (see ﬁgures 8.23c and 8.23d). The same applies for the\ngenerated samples of the FashionMNIST dataset in ﬁgure 8.24. For both image datasets, it is visible\nthat the smaller the patches, the more diverse and sharper the generated images.\n(a) 1L (vanilla GMM)\n(b) 2L-d (non-convolutional)\n(c) 2L-c (convolutional)\n(d) 2L-e (convolutional)\nFigure 8.23: The impact of convolution on sampling for MNIST.\n(a) 1L (vanilla GMM)\n(b) 2L-d (non-convolutional)\n(c) 2L-c (convolutional)\n(d) 2L-e (convolutional)\nFigure 8.24: The impact of convolution on sampling for FashionMNIST.\nControlling Diversity by Top-S-Sampling\nThe top-S-sampling mechanism (described in sec-\ntion 8.4.1.3) constrains the selection of components used for sampling. In order to illustrate the\neﬀect of top-S-sampling, the 2L-c DCGMM architecture is used to generate samples with diﬀerent\nconﬁgurations of S (see ﬁgures 8.25 and 8.26). As highlighted in ﬁgure 8.25, a smaller S leads to\na reduced diversity of the generated samples. Vice versa, a larger S causes more diverse samples.\nHowever, the generated samples can degenerate if one ore multiple inappropriate components are chosen.\nAccordingly, this observation correlates with the increasing number of generated samples showing\ndefects. Therefore, the choice of an optimal S is problem-dependent, as illustrated in ﬁgure 8.26.\n(a) S =2\n(b) S =5\n(c) S =10\nFigure 8.25: The impact of diﬀerent values of S for top-S-sampling for MNIST.\nBenedikt Pfülb\npage 125\n\nDiscussion\nNovel Deep Learning Model: DCGMM\n(a) S =2\n(b) S =5\n(c) S =10\nFigure 8.26: The impact of diﬀerent values of S for top-S-sampling for FashionMNIST.\nImage Sharpening\nAnother functionality oﬀered by DCGMMs is a sharpening method for samples.\nSharpening is realized by gradient ascent as explained in section 8.4.2.5. In order to demonstrate\nthe eﬀect of sharpening, samples of the MNIST and FashionMNIST dataset are generated by a 2L-c\nDCGMM instance. Figures 8.27 and 8.28 illustrate samples before (left) and after the sharpening\nprocess (middle). The diﬀerences are highlighted on the right-hand side, as they are hard to recognize.\nThe adjustments of the sharpening method are highlighted by visualizing the diﬀerences before and\nafter the optimization.\nIn principle, sharpening does not change the shape of a sample, but improves its quality. Thus, hard\nedges resulting from the inversion of the folding process can be mitigated. The eﬀect becomes more\nobvious in ﬁgure 8.28. The transitions of the individual patches become smoother. Nevertheless, the\nsharpening process depends on two parameters: The sharpening rate ϵS and the number of performed\nsharpening iterations g.\nBesides improving the quality of generated samples, sharpening can be transferred for other\nfunctionalities. The sharpening function of DCGMMs can be used to reduce the noise in images. The\nsame applies for defects, which can be identiﬁed by outlier detection and repaired by sharpening the\ndefect part (e.g., Xie, Xu, et al. 2012). Moreover, inpainting is another functionality that may be\nimplemented by means of the sharpening approach. Inpainting is also used to manipulate speciﬁc areas\nof images (or other types of data, e.g. audio or videos) based on previously learned data distributions\n(e.g., Yenamandra, Khurana, et al. 2021; Kim, Woo, et al. 2019).\n(a) Before sharpening\n(b) After sharpening\n(c) Diﬀerences\nFigure 8.27: Impact of sharpening on generated samples for MNIST.\n8.6\nDiscussion\nVarious aspects of the proposed SGD-based training approach for GMMs are discussed below. The\nfollowing aspects are subject to the discussion: The novelty and the validity of DCGMMs, new emerging\nhyper-parameters, a justiﬁcation of GMMs and the annealing scheme.\npage 126\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nDiscussion\n(a) Before sharpening\n(b) After sharpening\n(c) Sharpening diﬀerences\nFigure 8.28: Impact of sharpening on generated samples for FashionMNIST.\nNovelty\nIn general, both GMMs and SGD are well established and explored. Another GMM training\nprocedure using SGD is presented by Hosseini and Sra (2015). Recent research (Hosseini and Sra\n2020) implies the issue’s relevance. Hosseini and Sra’s (2015) method is similar to the presented SGD\ntraining scheme, although the related approach introduces multiple diﬀerent hyper-parameters (ρ, κ,\nα, β and Λ).\nAs described in related work, the main diﬀerence is that the initialization problem is circumvented\nby using a data-driven method (e.g., k-means). k-means itself requires an initialization and is a batch\ntype algorithm (is not applicable for streaming data).\nOther research (Verbeek, Vlassis, et al. 2005) establishes a connection between mixture models\nand an annealing scheme that is related to SOMs. The goal of Verbeek, Vlassis, et al. (2005) is to use\na mixture model as kind of a SOM. This validity of the relationship between SOMs and GMMs is\nproven by Gepperth and Pfülb (2020).\nVan Den Oord and Schrauwen (2014) present Deep GMMs which are trained with a special variant\nof (hard) EM. The interpretation is based on paths through the network, whereas no folding or\nclassiﬁcation is conducted. Another deep version of GMMs is presented by Viroli and McLachlan\n(2019). They combine their deep variant with a stochastic version of the EM algorithm for training.\nThe conceptual diﬀerence between the presented deep models and Viroli and McLachlan’s (2019)\nproposal is that they are considered to have a one-to-one correspondence to a ﬂat model. Since the\ncomponent weights are not normalized in each layer, the layers are not independently optimized.\nInstead, they are intrinsically connected by paths. The decomposition of these layer correlations by\nthe SGD approach proposed in this work means that the layers can be optimized independently, which\nhas a signiﬁcant impact on scalability. Another advantage of the separation is that additional layers\ncan be introduced, e.g., folding, pooling and classiﬁcation. Diﬀerent techniques and methods from\nrelated work have inspired the novel approach introduced as part of this work.\nValidity\nIn the following, the validity of the SGD based training approach for GMM is subject to\ndiscussion. The validity of the novel training approach was conﬁrmed by comparison with a well\nestablished training method. The results of the experimental investigation indicate that the SGD based\nGMM training is at least equivalent to an online variant of EM, i.e., the stochastic EM (sEM) Cappé\nand Moulines 2009. Nevertheless, the direct comparison is unfair, as the conventional sEM is favored. In\nthis context, the centroids for an sEM trained GMM need to be initialized. A meaningful initialization\nis only possible with other data-driven mechanisms, e.g., by the application of k-means. This mechanism\nis used in many related works. Without it, no acceptable results can be achieved by various EM\nbased training methods. A direct comparison of the stochastic EM variant (sEM) and the SGD-based\ntraining method shows that equivalent results are achieved by using SGD. Accordingly, the SGD\napproach can be considered a valid procedure to train GMMs. Despite the disadvantage of a lacking\ninitialization, the SGD approach achieves better results for high-dimensional data.\nDrawing conclusions with regard to the validity of the DCGMM is challenging. A direct comparison\nwith a similar method is not conducted. This is due to the fact that no comparable training procedure\nor model exists. For this reason, a comprehensive evaluation and ﬁndings on the validity of DCGMMs\nis considered as an objective of future research.\nBenedikt Pfülb\npage 127\n\nDiscussion\nNovel Deep Learning Model: DCGMM\nHyper-Parameters\nDiﬀerent hyper-parameters emerge in the context of SGD-based training. The\nsame applies to the novel DCGMMs. Besides the learning rate ϵ, the newly introduced parameters\ninclude α as reduction criterion and δ as limit to detect the loss’ stationarity. Furthermore, initialization\nparameters need to be deﬁned: The number of Gaussian components K, µ, pi and Σ as initialization\nfor each GMM. The number of new hyper-parameters seems to be problematic at ﬁrst. However, these\nparameters can usually be chosen or applied intuitively to all examined datasets. Section 8.3.4 provides\na recommendation how to choose these hyper-parameters. The same hyper-parameters provide good\nresults for diﬀerent problems/datasets from section 6.1. This contrasts conventional training methods,\ne.g., sEM, where larger ranges of parameters need to be explored. Thus, the SGD-based training\nprocess does not seem to depend a lot on hyper-parameters and its deﬁnition is easier.\nAt the same time, some parameters can be selected interdependently, which reduces the number\nof new parameters. In fact, the learning rate ϵ has to be adjusted depending on the problem, as it\nis the case for all stochastic procedures. The enormous advantage of EM is therefore lost due to the\ntransfer to an online variant. For the online variant sEM, the parameters need to be adjusted in a\nsimilar way as the learning rate, namely in a problem-speciﬁc manner. Even though the number of\nGMM components K remains the more the better, K is always resource-dependent.\nFurthermore, certain hyper-parameters of the SGD training approach are easier to interpret than\nthose of sEM. δ, for example, deﬁnes the stationarity threshold and thus inﬂuences the temporal\nconvergence behavior. Hyper-parameters of other ML methods are not as intuitively to select or adjust,\ne.g., for sEM: ρ0, ρ∞and α0.\nJustiﬁcation of GMMs\nAlthough images do not necessarily follow a Gaussian distribution, their\nrepresentation can be approximated by GMMs. Unlike the EM algorithm, the SGD based method\nmakes no hard assumptions regarding the unobservable latent variable. Thus, SGD training does not\nassume that a sample is drawn from exactly one component, even though this approximation holds\n(see max pk∗responsibilities in table 8.3 of section 8.5.1.3).\nThe universal approximation theorem applies to DNNs with multiple hidden layers. In fact, the same\napplies to GMMs. For GMMs, however, it depends on the number of available Gaussian components\nK. “A Gaussian mixture model is a universal approximator [...] with enough components.” (Good-\nfellow, Bengio, et al. 2016). Despite the universal approximation theorem (see Pinkus 1999; Hornik,\nStinchcombe, et al. 1989), the question of how many components or respectively layers are needed\nto represent the data distribution remains unanswered. This is related to the available memory and\ncomputational resources.\nThe limitiation of adding more GMM components is softened considerably by stacking multiple\nGMMs with less components, which is similar to layers in a DNN. Thus, more complex functions can\nbe approximated by DCGMMs with fewer memory and computational resources. Accordingly, the\nintroduction of folding and pooling layer(s) should improve the performance quality for high-dimensional\ndata like images (as the operations in CNNs).\nAnnealing Scheme\nThe eﬀect of the introduced annealing scheme is another aspect to be discussed.\nThe basic idea of annealing is often applied to the training of SOMs. In the context of GMMs, the\napplication of the proposed annealing strategy leads to several advantages. Annealing prevents the\nemergence of degenerate solutions (see ﬁgure 8.6a for an impression). Unlike other training methods,\ntrained GMMs with the annealing scheme are not aﬀected by the well-known “mode collapse”, which\nis particularly known for GANs (Richardson and Weiss 2018). Mode collapse describes the problem\nthat the generator of a GAN draws the same sample over and over again, which is due to the lacking\ncapability of the discriminator to distinguish generated and real samples.\nThe main advantage of annealing is that it eliminates the need for a data-driven initialization, which\nis particularly beneﬁcial for streaming scenarios. The initialization problem is often circumvented by\nthe application of other clustering algorithms requiring the complete dataset, as well as an initialization,\ne.g., k-means. Disadvantages of this data-driven initialization technique comprise the introduction of\nadditional hyper-parameters and the dependence on an initialization. The introduced hyper-parameters\nfor the presented annealing scheme can be automatically selected and adjusted (which is at least true\nfor all examined datasets). Based on the traced stationarity of the log-likelihood, the parameters are\nautomatically adjusted. Thus, the adaption of suitable hyper-parameters is of minor importance.\nOther advantages could be mentioned and discussed. The application of the scheme, for example,\npage 128\nBenedikt Pfülb\n\nNovel Deep Learning Model: DCGMM\nConclusion and Future Work\nis computationally marginal and therefore does not aﬀect the training process. At the same time,\nannealing ensures that a topological sorting of the components is performed. Even though the sorting\nis not used in this work, further functionalities could be implemented by using it. Further research\ncould be conducted related to neighborhood components, or other annealing patterns could be applied.\n8.7\nConclusion and Future Work\nIn this chapter, a numerically stable SGD-based training method for GMMs was presented. An\nannealing strategy is introduced as a special feature, which ensures that no data-driven initialization\nis needed. In general, a data-driven initialization constitutes a huge disadvantage, which is why it is\nomitted in many related works. At the same time, annealing prevents the emergence of degenerate\nsolutions. This eﬀect can be compared to the well-known “mode collapse” in the context of GANs (see\nRichardson and Weiss 2018).\nBased on the SGD training of GMMs, a novel ML model is proposed, which is referred to as DCGMM.\nDCGMMs consist of multiple GMM layers that are trained by SGD. Similarly to DNNs, DCGMMs\nrepresent deep GMMs that can be considered as universal function approximators. Furthermore,\nDCGMMs allow the combination of GMM layers with other types of layers, such as classiﬁcation,\nfolding and pooling layers (known from CNNs).\nOther layer types may be implemented, which\nrepresents a great potential in terms of research and further development. Moreover, DCGMMs\noﬀer various functionalities, such as density estimation, sampling, outlier detection, sharpening or\nclassiﬁcation. Accordingly, the novel DCGMM oﬀers a great research potential for diﬀerent application\nscenarios or functionalities such as inpainting. The classiﬁcation of samples is addressed in the next\nchapter (see chapter 9).\nSince DCGMMs can be considered as a novel ML model, a number of new areas of research arise.\nThe properties associated with the data can be examined. This aspect includes, for example, how the\nbehavior aﬀects diﬀerent data distributions within the data. Based on the density estimation, the\nlearning rate or the annealing parameter σ can be adjusted similar to a weighting mechanism for each\nindividual sample. The advantage would be that the model itself determines the required weighting\nfactor in data stream samples. Thus, outlier and inlier can be in-/excluded in the training process.\nSimilarly, the obtained log-likelihood values can be set in relation to the sliding log-likelihood. The\nresulting factor can be used in further application scenarios, e.g., CL tasks. Again, a higher or lower\nweighting of potentially known or unknown samples (outliers) can be utilized. The presented concept\nfor controlling the annealing parameter σ is only a ﬁrst proposal. It describes the adjustment of σ\nbased on the stationarity of the log-likelihood, although more intelligent methods may be deployed.\nThis can be particularly important for the adjustment of individual training samples.\nAnother optimization with regard to the diagonalized covariance matrices or precision matrices\nmay be of interest for future research. The idea is transferred from MFAs models, which can be\nimplemented as layers (Tang, Salakhutdinov, et al. 2012). Another open issue concerns the holistic\nmapping of multiple individual layer loss functions into a single one. Usually, parameters of deep\nlearning models are optimized according to one loss function, as it is the case for DNNs or CNNs.\nFuture research could also focus on the use of advanced optimizers such as Adam (Kingma and Ba\n2015). Attempts to perform the respective optimizations did not lead to convergence.\nMoreover, applying approaches for dynamic addition and removal of GMM components seems to be\nan interesting possibility (e.g., Song and Wang 2005; Kristan, Skočaj, et al. 2008). The integration of\nthe weights π into the sampling process is yet another open aspect. Even if the implementation allows\nthe weight’s inclusion, its eﬀect remains unresolved. The deployment in other application-oriented\nscenarios raises new challenges. This may include applications that process related sequential data,\nsuch as speech, text or video streams.\nBasically, the SGD-based training method of GMM and its transformation into layers (DCGMM) are\nconsidered as an independent, novel ﬁnding of this work. Accordingly, the results of this chapter are not\nnecessarily related to the CL context. However, the properties and functionalities of GMMs/DCGMMs\nconstitute the basis for further considerations and the application within CL scenarios. Therefore, the\ninvestigation of DCGMMs in the context of CL is the main focus of the next chapter.\nBenedikt Pfülb\npage 129\n\nConclusion and Future Work\nNovel Deep Learning Model: DCGMM\npage 130\nBenedikt Pfülb\n\n9.\nContinual Learning with DCGMMs\nChapter Contents\n9.1\nGaussian Mixture Replay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n9.1.1\nPseudo-Rehearsal Procedure . . . . . . . . . . . . . . . . . . . . . . . . . .\n133\n9.2\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n9.2.1\nExperimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n134\n9.2.2\nTask Boundary Detection Experiments . . . . . . . . . . . . . . . . . . . .\n139\n9.2.3\nContinual Learning Experiments\n. . . . . . . . . . . . . . . . . . . . . . .\n140\n9.3\nDiscussion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n9.4\nConclusion and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\nThis chapter introduces a continual learning (CL) learning model that is referred to as Gaussian\nMixture Replay (GMR) and based on Deep Convolutional Gaussian Mixture Models (DCGMMs)\n(see chapter 8). The contents of this chapter have been already published in Pfülb and Gepperth\n2021; Pfülb, Gepperth, et al. 2021. DCGMMs were evaluated with regard to their validity and other\nfunctionalities, reviewing the clustering capabilities and ability to generate samples in the previous\nchapter. This chapter speciﬁcally examines the CL performance of the GMR approach in conjunction\nwith the classiﬁcation capabilities.\nGMR is classiﬁed as a pseudo-rehearsal method (see section 2.3.5), which is based on the sampling\nfunction of DCGMMs. For the investigation of CL capabilities, the investigation protocol presented\nin chapter 6 is applied. The protocol is characterized by the implementation of various real-world\nrequirements. These requirements include, for example, prohibiting the use of future data. Another\napplication-oriented requirement is that past data cannot be accessed due to memory limitations.\nEspecially in streaming scenarios, the availability of an inﬁnite amount of memory to store all data\nsamples is unrealistic. Furthermore, the evaluation protocol deﬁnes CL tasks containing diﬀerent\nclassiﬁcation sub-tasks. These tasks are referred to as Sequential Learning Tasks (SLTs) and they\nrepresent CL problems with hard but known task boundaries. The hard task boundaries, however,\ndo not apply to all variants of CL scenarios. They are considered easier than gradual or unknown\ntransitions. In particular, the indicated boundaries constitute a basic prerequisite for many CL models,\ne.g., Deep Neural Networks (DNNs) based models, which are purely discriminative methods. This is\ndue to the fact that they cannot detect outliers without additional eﬀort.\nContributions\nThe contribution of this chapter is a CL approach, referred to as Gaussian Mixture\nReplay (GMR), which is based on the novel DCGMM model. GMR implements a pseudo-rehearsal\n(replay) method, which is particularly suitable for CL scenarios. Pseudo-rehearsal methods ensure that\nsamples from the past do not need to be stored. Therefore, samples can be generated on demand and\nused for replay.\nA comparison of GMR and other replay-based models shows that similar CL performance can be\nreached. At the same time, other functionalities of the DCGMMs can be used to identify important\nproperties of CL tasks. The detection of task boundaries is one important characteristic of CL tasks,\nwhich contrasts with other investigated models. This ability arises from the underlying Gaussian\nMixture Models (GMMs), which can be used as an unsupervised clustering method. Based on the\npossible density estimation of one or multiple samples, class boundaries can be detected.\nBenedikt Pfülb\npage 131\n\nGaussian Mixture Replay\nContinual Learning with DCGMMs\nCompared to the well-known deep methods, e.g., DNNs, GMMs are not directly aﬀected by the\ncatastrophic forgetting (CF) eﬀect. First results indicate that GMR exhibits comparable or better CL\nproperties, except for the ﬁnal classiﬁcation performance which is not yet comparable. Therefore, the\naim of the GMR model is to demonstrate the fundamental CL properties of DCGMMs. Based on this\nﬁrst CL proposal, further optimizations and adaptations can be explored.\nStructure\nInitially, the pseudo-rehearsal procedure Gaussian Mixture Replay (GMR) is presented\nin section 9.1. In section 9.2, the GMR procedure is evaluated. Then the experimental setup is\nintroduced (see section 9.2.1). The setup describes the evaluation process and the compared CL\nmodels. Subsequently, an investigation with regard to the detection of task boundaries by the GMR\nmodel is summarized in section 9.2.2. Finally, a comparison with other CL models is conducted with\nregard to the CL performance. Even if replay-based methods were previously excluded by real-world\nrequirements, Generative Replay (GR) (Shin, Lee, et al. 2017) is used for comparison. Several aspects\nof the procedure are discussed in section 9.3. These procedures include a strategy which addresses\nthe limitation of applying pseudo-rehearsal approaches in CL scenarios. The chapter is concluded in\nsection 9.4, where open issues for further research are outlined.\n9.1\nGaussian Mixture Replay\nThe Gaussian Mixture Replay (GMR) procedure is based on the DCGMMs presented in chapter 8.\nLikewise, for training GMR the proposed Stochastic Gradient Descent (SGD) training method is used\n(see section 8.3). In this context, the annealing scheme (see section 8.3.3) is applied as well. The\napplication of annealing has two advantages. Firstly, it solves the problem of degenerate solutions\nwhile being numerically stable. Secondly, it allows for the training of GMMs without a data-driven\ninitialization.\nThus, a systematic comparison with other deep learning methods, e.g., based on\nDNNs, is possible. Another advantage of DCGMMs is the architectural arrangement of diﬀerent types\nof layers. Stacking multiple GMM layers allows for the approximation of more complex functions.\nOtherwise, a single GMM is limited by the number of GMM components in terms of memory and\ncomputational eﬀort. In addition, the use of folding layers allows to achieve better results for the\nrespective (image) datasets. The introduction of a classiﬁcation layer enables a supervised inference\nand thus the generation of class speciﬁc samples (conditional sample generation).\nThe ﬁrst examination of the proposed GMR model is based on a simple DCGMM architecture\nconsisting of three layers. More complex and deeper convolutional variants can be realized, which tend\nto achieve better CL performances. The ﬁrst layer is a folding layer, which transforms the input data\ninto a one-dimensional vector (see section 8.4.1.1). Accordingly, no convolution or folding is performed\nas it is the case in Convolutional Neural Networks (CNNs). The second layer is a GMM layer, which\ndeﬁnes multiple Gaussian components K parametrized by the prototypes µk and variances Σk (see\nsection 8.4.1.3). The exact training, inference and sampling procedure is presented in sections 8.3.4,\n8.4.2.2 and 8.4.2.4.\nThe output of a GMM layer corresponds to the posterior distribution of a sample, referred to\nas responsibilities γk. The responsibilities are subsequently converted into class memberships by a\nclassiﬁcation layer (third and last layer). Thus, the GMR model combines both a Learner and a\nGenerator in one and the same ML model. Functionalities such as sample generation, outlier detection\nand classiﬁcation are combined in a single entity. The basic structure is illustrated in ﬁgure 9.1. It is\npossible to show that GMR has an adequate CL performance with this simple architectural structure.\nFigure 9.1: Meta-structure of the GMR model.\nAs stated in chapter 8, a DCGMM instance can be trained incrementally by SGD. If the training is\nperformed in terms of the CL paradigm, the GMM layers are not or diﬀerently aﬀected by the CF\neﬀect. The situation is diﬀerent for the classiﬁcation layer (linear classiﬁer layer), which is aﬀected\nby the CF eﬀect. Due to the CF aﬀected linear layer at the end of the DCGMMs, the problem shifts.\npage 132\nBenedikt Pfülb\n\nContinual Learning with DCGMMs\nGaussian Mixture Replay\nHowever, the architecture ensures that the storage of knowledge and the associated classiﬁcation is\nseparated.\nAssumed is a GMM layer consisting of only 2 Gaussian components which is trained class by class.\nAs soon as the training of the second class begins, one of the two components (the best matching\nunit (BMU)) is gradually adapted to the second class. The adaption can be described as gradual, as\nprecisely one BMU (component) is adjusted for the second data distribution. In ﬁgure 9.2, diﬀerent\nstates of training are visualized for K =4 Gaussian components. The training is conducted for E =50\ntraining epochs, which corresponds to a very long training process for a two-class problem. Over\na long period of time, one prototype after another is gradually adjusted (see intermediate states of\ncomponents in ﬁgure 9.2). Accordingly, forgetting does not occur catastrophically in a GMM layer.\nFigure 9.2: Visualization of an incrementally trained DCGMM for E =50.\nThe problem of catastrophic forgetting still occurs in the linear classiﬁcation layer. However, the last\nclassiﬁcation layer is solely responsible for assigning a class label to an input signal. As a result, the\nlinear layer only stores the knowledge about the assignment of a label to a corresponding active GMM\ncomponent. The GMR procedure primarily addresses the catastrophic forgetting eﬀect in the last\nlayer. For this purpose, a corresponding activation of the GMM layer with an associated class label\nneeds to be generated.\n9.1.1\nPseudo-Rehearsal Procedure\nIn order to prevent the forgetting in the last layer, the pseudo-rehearsal mechanism GMR is introduced.\nTo be precise, samples are generated from previously learned data distributions and merged with\nthe current data distribution for a joint training. The conditional sampling strategy illustrated in\nsection 8.4.2.4 is used to generate proportionally even samples from all classes. Interestingly, the term\n“proportionally” is a key issue for streaming- and CL scenarios which will be discussed later.\nThe replay scheme for the CL tasks (speciﬁcally for SLTs from section 6.2) is illustrated in ﬁgure 9.3.\nAt ﬁrst, samples for tasks Tt for t>1 have to be generated. The number of samples #(t) that need to\nbe generated depends on the number of samples for the current task #(T train\nt\n). Again, this primarily\ncontradicts the idea of data streams, as the exact number is not known beforehand. In this context,\nthe generation of a ﬁxed number of samples is a shortcut for the replay approach. The goal is to\ndemonstrate a reasonable CL performance of GMR.\nFigure 9.3: Replay scheme for GMR.\nVarious strategies can be used to address the challenges due to the proportional sampling constraint.\nFirstly, the number of generated samples can be set to a ﬁxed number, assuring that a suﬃcient\nnumber of representative samples is available for re-training. As long as there are enough samples, this\nmethod ensures that the existing knowledge can be preserved. Secondly, the challenge of proportional\nsampling can be tackled by adjusting the importance of a sample. If generated samples are assigned a\nhigher weighting, e.g., by a larger learning rate ϵgenerated, a smaller number of samples needs to be\ngenerated. Thirdly, the merging strategy for generated and new samples can be a decisive factor. The\nfactor deﬁnes the mixing ratio that is applied to specify the ratio in each training batch.\nBenedikt Pfülb\npage 133\n\nEvaluation\nContinual Learning with DCGMMs\nFor a ﬁrst investigation of GMR, the strategy of a ﬁxed number of samples with a proportional\nmerging ratio is implemented. Initially, the respective class labels are used to determine the ratio of\ngenerated and new samples. This can be realized simply by counting the number of learned classes\nand the number of new classes within a batch. This is how a balanced relationship between new and\nold data is achieved. A 50 : 50 merging ratio, for example, is deﬁned for two CL task consisting of two\nclasses each.\n9.2\nEvaluation\nIn order to evaluate GMR, several experiments are conducted. First, the experimental setup is described\nin section 9.2.1. This includes a brief description of the applied evaluation protocol from chapter 6, as\nwell as a presentation of the diﬀerent CL models and their respective hyper-parameters. Subsequently,\nthe ability of GMR’s tasks boundary recognition is demonstrated in a purely unsupervised manner. In\ngeneral, the declaration of task boundaries is often expected by conventional CL approaches, as the\nboundaries cannot be detected by the models themselves. Section 9.2.2 shows that the functionality to\ndetect task boundaries is already implemented by the DCGMM procedure.\nFinally, the CL capabilities of diﬀerent ML models are presented in section 9.2.3. The compared\nmethods include the Elastic Weight Consolidation (EWC) proposed by Kirkpatrick, Pascanu, et al.\n(2017). In order to strengthen the signiﬁcance, a widely known generative approach is examined.\nShin, Lee, et al. (2017) describe the pseudo-replay approach which is referred to as GR. GR can be\nused with diﬀerent technologies for generating samples, e.g., Generative Adverserial Networks (GANs)\n(Goodfellow, Pouget-Abadie, et al. 2014) or Variational Autoencoders (VAEs) (see Kingma and Welling\n2013). Hence, the novel GMR is compared with two state-of-the-art CL models.\n9.2.1\nExperimental Setup\nFor all of the presented experiments, 10 repetitions are performed for one and the same parameter\nconﬁguration. The obtained results are aggregated in a so-called meta-experiment by averaging the\nperformance values (and its variances). This ensures that the inﬂuence of initialization is reduced for\ncertain parameter conﬁgurations.\n9.2.1.1\nApplication-Oriented Evaluation Protocol\nThe application-oriented evaluation protocol from chapter 6 is brieﬂy presented in the following. The\nbasis for the evaluation are SLTs (see section 6.2), which deﬁne a decomposition of datasets (e.g.,\nsection 6.1). Thus, the decomposition deﬁnes sub-tasks with one or multiple classes that are used\nfor training. The subscripts indicate how many sub-tasks a SLT comprises, e.g., D5-5 consists of two\nsub-tasks where each contains 5 disjoint classes. The sub-task sequence is denoted as T1, T2, . . ., Tx.\nThe reference value is the result of the baseline experiment (D10), which consists of one task that\ncontains all 10 classes of a dataset. This non-CL measurement value is considered as a benchmark for\nthe maximum achievable CL performance.\nThe deﬁnition of sub-task boundaries is one speciﬁcation that is hard to reconcile with speciﬁc\nreal-world requirements. In an application-oriented context, the speciﬁcation of sub-task boundaries\nis not necessarily given, nor are the sub-task boundaries hard. The problem is that most of the CL\nmodels assume that this information is available. The permanent examination of the training labels\ncould be used as task boundary, but this only applies for special CL scenarios.\nIn addition to the sub-tasks, the protocol deﬁnes the fulﬁllment of several real-world requirements.\nThis includes, above all, the causal temporal relation, which prohibits the use of future data. Likewise,\nthe use of past data is forbidden due to memory limitations, which is especially true for streaming\nscenarios. A retroactive adjustment or model selection of the architecture is also not possible. Even if\nall these requirements are considered as “self-evident”, investigations in the CL context usually do not\nenforce these requirements.\nIn order to establish a comparable evaluation, classiﬁcation accuracy is applied to measure the CL\ncapabilities. This metric is only applicable if the data distributions of the datasets within each class are\nreasonably uniform. In order to exclude the inﬂuence of the baseline accuracy, only the diﬀerence to\npage 134\nBenedikt Pfülb\n\nContinual Learning with DCGMMs\nEvaluation\nthe maximum of the baseline experiment is presented. In the following, the resulting CL performance\nis referred to as baseline accuracy.\nA special feature of pseudo-rehearsal methods is the generation of samples for a joint training\nprocess. After each sub-task, the model Mt with a parameter conﬁguration at the time t is used to\ngenerate a sample dataset Gt. The generated samples are merged with samples from the current\nsub-task and presented to the model. The process is visualized in ﬁgure 9.4. As depicted, the model\nM is ﬁrstly trained with samples from T train\n1\n. After the initial training phase is completed, the model\nis used to generate the dataset G1 from the learned data distribution. Samples from G1 are mixed in a\nspecial ratio. The mixing ratio is calculated by the proportion of number of classes in T1 and T2 (5:5\nin this example). Thus, re-training is conducted with batches of data consisting of 50 % samples from\nG1 and 50 % of T train\n2\n.\nAfter processing all sub-tasks, the CL performance is measured by determining the accuracy of the\ncombined test dataset (Dtest\n10 ). The ﬁnal CL capacity is deﬁned by the diﬀerence between the ﬁnal\naccuracy achieved for a given SLT and the joint training of the model with Dtrain\n10\n.\nFigure 9.4: Rehearsal approach for the model training (SLT D5-5a on MNIST).\n9.2.1.2\nCL Models and Hyper-Parameters\nDiﬀerent models are used to compare their CL performance. One of the examined models is EWC\n(Kirkpatrick, Pascanu, et al. 2017). EWC has attracted attention as the most promising CF avoidance\nmodel from the study described in chapter 7. In order to make a comparison with another pseudo-\nrehearsal procedure, GR (Shin, Lee, et al. 2017) is investigated. GR can be deployed with diﬀerent\ntypes of generators: GANs (Goodfellow, Pouget-Abadie, et al. 2014) and VAEs (Kingma and Welling\n2013). Even though this procedure has some disadvantages which are not easy to reconcile with\nreal-world requirements (discussed in section 9.3), this procedure serves as state-of-the-art reference\nmodel.\nFirst of all, the diﬀerent approaches (EWC, GR and GMR) are brieﬂy introduced and their\nhyper-parameters are outlined. In order to conduct a fair comparison, several hyper-parameters are\noptimized for each model by using a grid-search. Due to the complex parameter conﬁguration for the\nGR approach, only three datasets with the same dimensions are used for the present study.\nElastic Weight Consolidation (EWC)\nOne of the most cited models for CL is EWC, as presented\nby Kirkpatrick, Pascanu, et al. (2017). It is a typical regularization approach that is based on DNNs.\nEWC stores the parameters distribution of the model after completing a task ⃗\nθTt. By calculating\nthe Fischer Information Matrix (FIM) ⃗F Tt, the “importance” of each parameter for a respective data\ndistribution is determined. The FIM is subsequently used in the loss function of EWCin order to\npenalize the change of the important parameters for the old tasks and the current sub-task Tc. Likewise,\nin EWC, as well as in conventional DNNs, the cross-entropy loss is minimized by optimizers, such\nas Adam (Kingma and Ba 2015). Hyper-parameters include the regularization constant λ, but also\nthe underlying DNN architecture (number and size of layers). As suggested in the code base, λ is set\nequal to ϵ−1, which eliminates this hyper-parameter. An alternative variant for FIM calculation is\nused for direct comparison, namely the Matrix of SQuares (MaSQ) (see Gepperth and Wiech 2019).\nThis variant does not require as many resources and produces comparable results.\nAs only one hyper-parameter needs to be tuned in addition to EWC’s architecture, the learning rate\nis chosen from ϵ∈{0.001, 0.0001, 0.00001, 0.000001, 0.0000001}. The underlying DNN architecture is\nBenedikt Pfülb\npage 135\n\nEvaluation\nContinual Learning with DCGMMs\nLEW C = LTc(θ) + λ\n2\nc−1\nX\nt=1\nX\ni\nF Tt\ni\n\u0010\nθi −θTt\ni\n\u00112\nEquation 9.1: EWC loss function (see Kirkpatrick, Pascanu, et al. 2017) adapted to the used terminology.\nbased on a three-layer DNN (L = 3) consisting of 800 neurons each (S = 800). This architecture is\npreferred, as acceptable results for the examined datasets are already obtained. The number of training\nepochs is set to E =10 epochs, because more training iterations aﬀect the CL performance of EWC\n(see section 7.1.4). In order to mitigate the eﬀect of training iterations, EWC is evaluated for the\nbest measured accuracy instead of the last value. Nevertheless, this methodology is inﬂuenced by the\nnumber and distribution of measurement points. In general, the CL performance of an ML model\nshould not or, if at all, marginally depend on the number of training iterations. This aspect is related\nto real-world constraints, where the determination of the best possible model parameter conﬁguration\nis usually diﬃcult (early-stopping).\nGenerative Replay (GR)\nThe second model for comparison is the generative approach Generative\nReplay (GR). The approach was implemented for the experiments according to Shin, Lee, et al. (2017)\n(as published by Gulrajani, Ahmed, et al. (2017)). In general, the replay process is the same as for GMR:\nAfter sub-task completion, samples are generated, which are then replayed into the following training\nprocess. GR is based on a generator and a separated solver. Unlike the presented GMR model, the\ngenerator and solver of GR are separate independent entities. Diﬀerent types of generating or solving\nmodels can be used for each part. For the generator, two common and particularly powerful deep\nlearning models are deployed: VAEs (Kingma and Welling 2013) or GANs (Goodfellow, Pouget-Abadie,\net al. 2014) (respectively a variant referred to as Wasserstein GAN proposed by Arjovsky, Chintala, et al.\n(2017)). These two generating components are brieﬂy described below. In addition to the generator, a\nsolver is introduced. Usually, it is a DNN or CNN which is (re-)trained with the generated samples\nand the current data distribution. Thus, the joint training counteracts the CF eﬀect when training the\nsolver.\nGANs\nGoodfellow, Pouget-Abadie, et al. (2014) introduce Generative Adverserial Networks\n(GANs) that comprise a generating g and a discriminating d component. Both components (g and d)\ncan be considered as independent DNNs. The generating part tries to generate samples that fool the\ndiscriminating part d. In turn, the discriminator tries to learn how to distinguish real samples from\ngenerated ones. Generator and discriminator thus work against each other, resulting in a so-called\n“zero-sum game” (formulated in section 9.2.1.2). The counterplay causes the generator to learn how\nto generate realistic samples from the data distribution. These samples are utilized as described in\nsection 9.1.1 for a joint re-training (see Shin, Lee, et al. 2017).\narg min\ng\nmax\nd\nV (d, g) = EX∼pdata log\n\u0000d(x)\n\u0001\n+ Ex∼pmodel log\n\u00001 −d(x)\n\u0001\nEquation 9.2: Formulation of the zero-sum game in GANs (Goodfellow, Bengio, et al. 2016).\nVAEs\nKingma and Welling (2013) propose VAEs that comprise two parts, the encoder and the\ndecoder. The basic idea is to compress an input signal layer-by-layer by reducing the number of\navailable artiﬁcial neurons (encoder). The decoder as a second part tries to restore the original input\nsignal from the latent space (representation of compressed data). The main diﬀerence compared to an\nauto-encoder is an additional layer between the decoder and encoder. This additional layer realizes the\nvalues’ transformation to probabilistic values (e.g., by a multivariate Gaussian section 8.1.2 vector).\nVAEs oﬀer various functions, e.g., de-noising or (conditional) sample generation. The goal of VAEs\nis thus to minimize the reconstruction loss (see section 9.2.1.2, where DKL is the Kullback-Leibler\ndivergence).\nSolver\nAs an addition to the generating component (GAN or VAE), a further DNN is introduced\nby GR. The component that is responsible for classiﬁcation is referred to as solver. Similarly to the\npage 136\nBenedikt Pfülb\n\nContinual Learning with DCGMMs\nEvaluation\nL = Ez∼q(z|x) log\n\u0000pmodel(x|z)\n\u0001\n−DKL\n\u0000q(zx)||pmodel(z)\n\u0001\nEquation 9.3: Standard loss function for VAEs (Goodfellow, Bengio, et al. 2016).\ngenerating part, diﬀerent ML models may realize the solvers.\nHyper-Parameters of GR\nGenerative Replay (GR) introduces multiple entities. For all of\nthem, diﬀerent hyper-parameters need to be speciﬁed, e.g., the architectures. Usually, Keras (ML\nframework) components are used for implementation. In the following, the individual types of layers\nused for building the individual components for GR are presented:\n• Fully-Connected (FC) layer which is often referred to as dense layer.\n• LeakyReLU is the attached output function (compare ReLU section 2.2.1.2). See section 9.2.1.2\nand ﬁgure 9.5 for a deﬁnition. The exact value (here 0.2) slightly varies in many implementations.\nfϕ(x) =\n(\nx\nif x > 0,\n0.2x\notherwise\nEquation 9.4: Leaky ReLU output function.\nFigure 9.5: Plot of Leaky ReLU output function.\n• Batch Normalization (BatchNorm) layers try to normalize the input by a transformation (Ioﬀe and\nSzegedy 2015). BatchNorm layers ensure that the neural network converges faster and therefore\nless training iterations are required.\n• 2D convolution (Conv2D) layers represent the standard convolutional operation used by CNNs\n(LeCun, Haﬀner, et al. 1999) with a given ﬁlter and kernel size. Such layers are very advantageous\nin the ﬁeld of image processing. The reverse direction is achieved with the transposed variant\n(Conv2DTrans).\n• Dropout (Hinton, Srivastava, et al. 2012b) layers assure that by a deﬁned chance certain activations\nare not passed to the next layer (in the present work, the dropout rate is set to 0.2 and 0.3\nrespectively).\n• Reshape and Flatten layers are responsible for transforming the input and output respectively.\nThe architecture of the deployed GAN is presented in table 9.1. The architecture described in table 9.2\nis used for GR experiments with VAE. For both generating methods, the architecture of the solver is\nspeciﬁed. In addition to the architectural structure, the number of free model parameters is composed\nby the diﬀerent layer types.\nTable 9.1: Parameter conﬁguration for Solver and GAN (generator and discriminator).\nSolver\nGenerator\nDiscriminator\nType\nShape Parameters\nType\nShape\nParameters\nType\nShape\nParameters\nReshape 28,28,1\n-\nFlatten\n80\n-\nReshape\n28,28,1\n-\nFlatten\n784\n-\nFC\n12544\n1 016 064\nConv2D\n14,14,64\n1 664\nFC\n400\n314 000\nBatchNorm\n12544\n50 176\nLeakyReLU 14,14,64\n-\nReLU\n400\n-\nLeakyReLU\n12544\n-\nDropout\n14,14,64\n-\nFC\n400\n160 400\nReshape\n7,7,256\n-\nConv2D\n7,7,128\n204 928\nReLU\n400\n-\nConv2DTrans 7,7,128\n819 328\nLeakyReLU 7,7,128\n-\nFC\n400\n160 400\nBatchNorm\n7,7,128\n512\nDropout\n7,7,128\n-\nReLU\n400\n-\nLeakyReLU\n7,7,128\n-\nConv2D\n7,7,256\n819 456\nFC\n10\n4 010\nConv2DTrans 14,14,64\n204 864\nLeakyReLU 7,7,256\n-\nSoftmax\n10\n-\nBatch\n14,14,64\n256\nDropout\n7,7,256\n-\nLeakyReLU\n14,14,64\n-\nFlatten\n12544\n-\nConv2DTrans 28,28,1\n1 601\nFC\n1\n12 545\nP\n638 810\nP\n2 092 801\nP\n1 038 593\nBenedikt Pfülb\npage 137\n\nEvaluation\nContinual Learning with DCGMMs\nTable 9.2: Parameter conﬁguration for Solver and VAE (encoder and decoder).\nSolver\nEncoder\nDecoder\nType\nShape Parameters\nType\nShape\nParameters\nType\nShape\nParameters\nReshape 28,28,1\n-\nReshape 28,28,1\n-\nFlatten\n20\n-\nFlatten\n784\n-\nConv2D 14,14,64\n1 088\nFC\n100\n2 100\nFC\n400\n314 000\nConv2D 7,7,128\n131 200\nFC\n6272\n633 472\nReLU\n400\n-\nFlatten\n6272\n-\nReshape\n7,7,128\n-\nFC\n400\n160 400\nFC\n100\n627 300\nConv2DTrans 14,14,64\n131 136\nReLU\n400\n-\nFC\n20\n2 020\nConv2DTrans 28,28,1\n1 025\nFC\n400\n160 400\nFC\n40\n840\nReLU\n400\n-\nFC\n10\n4 010\nSoftmax\n10\n-\nP\n638 810\nP\n762 448\nP\n767 733\nGR Training Process\nIn contrast to the deﬁnition of a constant batch size of B = 100, 2·B\nis used for re-training (t > 1). Thus, merging the generated and actual training data is realized by\ncombining generated samples and samples from the current task. For the sake of simplicity, the same\namount of data is generated as contained in the new task.\nFor the GR approach, the generator needs to be trained ﬁrst. Therefore, E =50 epochs of training\niterations are conducted on the training data. The learning rate for the generator is tuned by a\ngrid-search in each case (VAE and GAN), varying ϵG ∈{0.001, 0.0001}. After samples are generated\nfor the past task, the solver is re-trained with the merged training data for E =25 epochs. The Adam\noptimizer (Kingma and Ba 2015) is used to train the solver with a deﬁned learning rate of ϵS =0.001.\nAfter completion of a CL task, all components (generator and solver) are re-initialized, which means\nthat they are re-trained from scratch with the merged (joint) dataset.\nGaussian Mixture Replay GMR\nSeveral hyper-parameters need to be adjusted for the GMR\ntraining process. The underlying DCGMM architecture is ﬁxed and based on three layers (as for\nthe other models). The ﬁrst layer is a folding layer which is only used to ﬂatten (or reshape) the\ninput signal (transform 2D images into a one dimensional vector). Since no folding is performed, no\nparameters are adjusted by a grid-search. The second layer is a GMM layer. The third and last layer\nis the linear classiﬁcation layer, which allows the assignment of class labels in a supervised manner. A\ncomparable model architecture is depicted in ﬁgure 8.16. It is also comparable to the L1 architecture\ndescribed in table 8.4. Thus, the DCGMM used for GMR is neither convolutional nor deep. Despite\nthe simple DCGMM architecture, limited classiﬁcation accuracies and CL capacities can be expected.\nOne of the varied hyper-parameters is the number of Gaussian components K ∈{6×6, 8×8, 10×10}.\nThis adaption is used to verify the statement “more is always better” for the number of Gaussian\ncomponents. Moreover, a special reset method for the annealing scheme (see section 8.3.3) is introduced.\nThe reset method describes the reset of the annealing parameter σ after the completion of a task.\nIt is noted that the σ parameter deﬁnes the intensity of neighboring updates around the BMU. By\nmeans of the reset method, σ is set proportionally to the start value σ0 of the training. This procedure\nsimulates a training from scratch for the underlying GMM components. The following values are\ninvestigated as reset factor by the grid-search: σreset ={−1., 0., 0.25, 0.5, 0.75, 1.}, whereas −1 disables\nthe reset. Based on the grid-search, diﬀerent loss functions for the last classiﬁcation layer are evaluated\n(Cross-Entropy (CE) loss and Mean Squared Error (MSE), see section 2.2.2.1).\nBoth, the GMM and the classiﬁcation layer are trained with the same learning rate ϵG =ϵC = 0.01.\nThe classiﬁcation layer is trained by the application of two diﬀerent loss functions, MSE and CE loss.\nIn order to verify that GMR is insensitive to the conducted training iterations, a high number of\ntraining epochs is deﬁned. The ﬁrst sub-task T1 starts with E = 50 training epochs. Depending on\nan SLT’s number of sub-tasks, the number of training epochs is doubled for each task. This choice is\nmade in order to exploit a possible linear forgetting eﬀect. All other parameters are set as deﬁned\nin section 8.3.4, and therefore not adapted to a special problem. The training and test data are\nnormalized to the value range of [0, 1].\npage 138\nBenedikt Pfülb\n\nContinual Learning with DCGMMs\nEvaluation\n9.2.2\nTask Boundary Detection Experiments\nThe ﬁrst experiment exclusively refers to the GMR model. The goal is to determine to what extent the\nmodel is suitable for the recognition of sub-task boundaries. In order to detect sub-task boundaries,\nit is an important functionality for CL scenarios, which is oﬀered explicitly by only few ML models\n(e.g.,DNNs). In general, it is assumed that task boundaries are known and implicitly given. The outlier\ndetection method described in section 8.4.2.3 is applied for the detection of task boundaries.\nTo demonstrate the detection capabilities, the GMR model is trained step by step with an SLT\nconsisting of one class at a time (D1-1-1-1-1-1-1-1a, T1, . . ., T10, for the MNIST and FashionMNIST\ndataset). The log-likelihood value of each batch is traced, so that a new task can be recognized. If a\nloss value of a data batch is below 80 % of the sliding log-likelihood, a task boundary is detected. The\nsliding log-likelihood is traced anyway in order to implement the annealing scheme (see section 8.3.3).\nA diﬀerence with regard to the batch- and sliding log-likelihood needs to be detected within 10\ntraining iterations. After a boundary detection, the model performs 500 training iterations on the\nnew sub-task, before the detection mechanism is re-activated. Again, for ensuring the stability of\nthe process, long training periods with E = 200 epochs are speciﬁed for each sub-task. The result\nfor the three datasets is visualized in ﬁgure 9.6. For each run of the experiment (10 repetitions), a\nslightly transparent red bar is drawn whenever a task boundary is detected. As shown in ﬁgure 9.6,\nthe dark/solid bars indicate the detection of 10 consecutive classes in most cases. Only in a few\nexperiments a sub-task boundary was re-detected during the long training process.\nFigure 9.6: GMR task detection capabilities for a single class SLT.\nFigure 9.7 depicts the experimental result of the task detection for a D2-2-2-2-2 SLT. A reduced number\nof training iterations is conducted and only the ﬁrst 3 tasks and are illustrated in ﬁgure 9.7 in order to\nsimplify the visualization. In addition to the detected task boundaries (red bars), the log-likelihood\nvalues are illustrated as blue line. It becomes obvious why the simple heuristic produces acceptable\nresults. For each new sub-task, a signiﬁcant drop in log-likelihood is noticeable.\n(a) MNIST\n(b) FashionMNIST\nFigure 9.7: GMR task detection capabilities for a two class SLT.\nThese two experimental results imply that (hard) sub-task boundaries can be detected with GMMs.\nDepending on the chosen strategy, boundaries can be detected after 10 training iterations on a new\ntask. More complex and intelligent heuristics can be introduced in order to detect the boundaries.\nBenedikt Pfülb\npage 139\n\nEvaluation\nContinual Learning with DCGMMs\n9.2.3\nContinual Learning Experiments\nThe following experiments are based on the speciﬁcations of the CF protocol described in chapter 6.\nA brief version of the protocol is available in section 9.2.1.1. In this section, the presented Gaussian\nMixture Replay (GMR) model (see section 9.1) is investigated with regard to its CL performance. The\nwell-known Elastic Weight Consolidation (EWC) model proposed by Kirkpatrick, Pascanu, et al. (2017)\nis used for comparison. Elastic Weight Consolidation (EWC) is categorized as regularization approach\n(see sections 7.1.1 and 9.2.1.2). Since GMR is considered a pseudo-rehearsal approach, it is compared\nto another replay approach referred to as Generative Replay (GR) (Shin, Lee, et al. 2017) method.\nThe following conclusions are drawn from the experimental results which are supported by a grid-\nsearch with 4 380 performed experiments (see section 9.2.1.2). Ten repetitions of an experiment with\nthe same speciﬁc parameter conﬁgurations are aggregated as meta-results, whereas the measurement\nvalues are averaged. In addition to the average, the standard deviation is given for an improved\nassessment of the results.\nThe last measured performance value is always the measurement criterion, except for EWC. The\nbest (maximum) measured performance value is used for EWC in order to compensate a too long\ntraining time, as EWC is sensitive in this respect. The baseline experiment (SLT D10, see section 6.2)\nwhich combines all CL sub-tasks in one is used as a reference value. Experiments are conducted for\na subset of datasets. Otherwise additional architectures for the diﬀerent models would have to be\nintroduced. This is especially true for the GR approach, as sophisticated architectures for the generator\nare deployed. Thus, only the architectures proposed in related works are investigated.\nThe results of the conducted CL experiments are summarized in table 9.3. The best accuracy\nvalues of the baseline experiment (ﬁrst line), as well as the diﬀerences (diﬀ) to the individual SLTs are\npresented. The greater the diﬀerence, the worse the CL performance of each model. In turn, smaller\ndiﬀvalues indicate a better CL performance. Averaged values are listed for the GR variants with the\ndiﬀerent generator types (VAE and GAN). For the VAE variant, the standard deviation is maximally\n7 % for all SLTs. This variance implies a constant performance over all repetitions for all SLTs. For\nGANs, the standard deviation is at most 35 %. An explanation of the high variance is presented later.\nAs indicated by the results in table 9.3, none of the models provides a perfect CL performance\nfor the investigation protocol. It becomes obvious that EWC shows the greatest CL performance loss.\nThis is due to the sensitivity of the placement of measurement points and the number of training\niterations. In case more test-/measurement-points could be carried out, better results can be expected.\nHowever, this would tremendously increase the investigation eﬀort, which is hard to realize in certain\napplication-oriented scenarios.\nFor the investigation of the GR approach, two variants of generators are applied, where VAE is\nsuperior to GAN. This is due to the sensitivity of the GAN training. Without an extensive adaptation\nof the scenario, and respectively the problem and hyper-parameters, an even worse CL performance can\nbe observed. This is implied by the high standard deviation of 35 % for GR using GANs. VAEs seem\nto be less vulnerable to hyper-parameters, so that acceptable CL performance values are obtained.\nThe new GMR approach shows the lowest level of CL performance loss, but a poor baseline\nperformance (in contrast to the other models). This is due to the very simple DCGMM architecture\n(non-convolutional and non-deep). Initial experiments with non-deep and convolutional DCGMMs\nresult in a classiﬁcation accuracy of up to 97 % for the baseline task.\nRegarding the GMR hyper-parameters, the evaluation of the experimental results conﬁrms that\nmore Gaussian components result in a better performance. As expected, GMR experiments with\nK =100 components consistently demonstrate the best CL performances. This conclusion is similar\nto the reset method, which inﬂuences the σ value after each training task. Experiments without\nresetting the σ value always provide the best CL performances. No reset corresponds to a continual\ntraining process without adjusting σ at task boundaries. In comparison to a complete/full reset of σ\nto σ0, a new topological sorting of the prototypes can be established. At the same time, the ﬁnal CL\nperformance is slightly worse than without a reset. Another ﬁnding is that the CE loss function for\ntraining the classiﬁcation layer always provides the best CL results.\nIn order to provide more insights into the training/evaluation process, individual trends of the\n(meta-)experiments are presented in ﬁgure 9.8. The developments are visualized for diﬀerent datasets,\nwhereas the trends are similar for all investigated datasets.\nThe various colored lines represent\nthe averaged accuracy of the meta-experiments (10 experiment repetitions) for the best parameter\nconﬁguration. Baseline experiments with the SLT D10 (containing all 10 classes) are conducted in\npage 140\nBenedikt Pfülb\n\nContinual Learning with DCGMMs\nEvaluation\nTable 9.3: Results of the CL experiments for GMR, EWC and GR.\nSLT\nmodel\ndataset\nGMR\nEWC\nGR\nMNIST\nFashionMNIST\nDevanagari\nMNIST\nFashionMNIST\nDevanagari\nMNIST\nFashionMNIST\nDevanagari\nacc.\nstd\nacc.\nstd\nacc.\nstd\nacc.\nstd\nacc.\nstd\nacc.\nstd\nacc.\nstd\nacc.\nstd\nacc.\nstd\nD10 baseline\n87.4\n0.5\n73.9\n0.2\n74.1\n0.7\n97.6\n0.2\n87.6\n0.4\n95.6\n0.5\n99.3\n0.1\n99.3\n0.4\n99.1\n0.3\ndiﬀ.\nstd\ndiﬀ.\nstd\ndiﬀ.\nstd\ndiﬀ.\nstd\ndiﬀ.\nstd\ndiﬀ.\nstd\nVAE GAN VAE GAN VAE GAN\nD9-1a\n1.3\n0.5\n2.7\n0.2\n3.2\n0.7\n41.8\n0.2\n9.6\n0.4\n56.6\n0.5\n3.6\n62.2\n33.2\n40.6\n16.0\n88.1\nD9-1b\n3.5\n2.1\n1.5\n0.8\n1.4\n0.8\n50.7\n7.7\n20.1\n2.5\n29.7\n13.3\n3.3\n44.0\n28.2\n40.8\n14.8\n84.7\nD5-5a\n0.6\n1.5\n1.2\n1.5\n6.8\n1.3\n35.3\n6.6\n32.7\n4.2\n46.0\n15.3\n3.0\n17.4\n28.3\n34.1\n8.8\n51.7\nD5-5b\n1.3\n1.9\n1.9\n0.4\n4.7\n1.5\n35.0\n1.8\n36.0\n2.7\n47.1\n0.1\n2.3\n9.0\n28.0\n34.7\n9.3\n48.5\nD2-2-2-2-2a\n9.5\n3.8\n8.5\n0.9\n22.5\n2.7\n72.2\n7.4\n55.6\n4.1\n72.1\n2.7\n23.8\n63.3\n30.7\n54.3\n56.0\n79.0\nD2-2-2-2-2b\n10.4\n5.2\n5.7\n2.3\n14.7\n2.9\n72.6\n3.2\n57.3\n5.0\n73.2\n2.3\n22.9\n75.1\n35.1\n60.2\n49.9\n79.3\nD1-1-1-1-1-1-1-1-1-1a\n23.3\n4.1\n19.2\n1.8\n27.5\n7.5\n74.5\n3.8\n55.2\n5.6\n71.1\n2.3\n50.8\n80.3\n48.6\n69.8\n73.5\n88.6\nD1-1-1-1-1-1-1-1-1-1b\n16.3\n2.1\n19.6\n2.0\n35.6\n2.7\n76.6\n3.4\n56.0\n4.2\n77.2\n2.7\n50.8\n82.3\n49.4\n81.3\n60.9\n79.4\norder to calculate the reference value. The relation between the reference value and the measured CL\nperformance is illustrated in ﬁgure 9.8. It is referred to as baseline accuracy. The standard deviation\nis visualized by means of the face/area. For each task, 10 evenly distributed quality measurements are\nperformed during each task training.\nThe EWC results (red) in all sub-ﬁgures demonstrate that the longer the training lasts, the more\nCL performance is lost. What does not become obvious is that a more ﬁne-grained investigation with\nmore measuring points would be beneﬁcial for EWC. However, there is no implemented mechanism to\nstop the training procedure at the best possible model parameter conﬁguration. The stopping criterion\nshould be realized without violating the requirements of real-world applications, e.g., by using samples\nfrom the past.\nThe GR approach shows two diﬀerent results with respect to the CL performance. The deployment\nof GANs as generator for the GR procedure reveals an inferior CL performance in contrast to VAEs.\nIn general, GANs have the ability to generate samples that are at least as good as VAE. The problem\nis that the chosen model architecture, hyper-parameters or the training process do not seem suﬃciently\noptimized for these datasets and SLTs. In contrast to that, GR with VAEs yields signiﬁcantly better\nCL performance. The obtained VAE results are comparable to the CL performances of GMR (taking\ninto account the lower baseline of GMR). These trends become obvious for all SLTs and datasets\npresented in ﬁgure 9.8.\n(a) SLT D9-1b on Devanagari\n(b) SLT D5-5a on FashionMNIST\n(c) SLT D2-2-2-2-2b on FashionMNIST\n(d) SLT D1-1-1-1-1-1-1-1-1-1a on FashionMNIST\nFigure 9.8: Trends for diﬀerent SLT experiments for GMR, EWC and GR.\nIn order to oﬀer more details of the GMR method, the prototypes µ, variances Σ, as well as the\ngenerated samples are illustrated in ﬁgure 9.9. The MNIST dataset in combination with the D5-5a SLT\nis applied, whereas the SLT deﬁnes a split of the 10 class dataset into two tasks with ﬁve classes each.\nFigures 9.9a and 9.9d (top left and bottom) show the derived prototypes (µ) and variances (Σ) after\ntraining T1 for the classes 0, 1, 2, 3 and 4. The Gaussian components represent the model parameter\nconﬁguration after 50 epochs of training, although convergence has already been achieved much earlier.\nBenedikt Pfülb\npage 141\n\nDiscussion\nContinual Learning with DCGMMs\nThe GMM layer has 10×10 components that are represented as a grid. Again, the topological sorting\nthat gradually passes over the edges (left right and bottom top) becomes visible.\nAfter completing the training of T1, samples G1 are generated as shown in ﬁgure 9.9b. The\nplacement corresponds to the (repeated) sampling vector consisting of all 5 classes (0, 1, 2, 3, 4, 5).\nThe diversity of the individual samples is limited by the chosen architecture and the top-S-sampling\nstrategy. Moreover, the applied variances become visible, which can be interpreted as noise.\nFor the second sub-task, the training data of T train\n2\nis merged with the generated samples G1 (⊗\ncorresponds to a mixing ratio of 1:1). Certain components can be preserved in the long-term by means\nof the replay mechanism. The prototypes in ﬁgure 9.9c and the variances in ﬁgure 9.9e represent the\nstate after another 50 epochs of training. Comparing the prototypes of T1 with T2 indicates that the\npositions of the retained components for T1 have not changed. Only some components have been\n“overwritten” in order to reﬂect/address the data distribution from T2.\nTraining T1(0,1,2,3,4)\nGenerating G1\nTraining T2(5,6,7,8,9)\n(a) Prototypes (µ) after training T1.\n(b) Batch of generated samples (G1).\n(c) Prototypes after training G1⊗T1.\n(d) Variances (Σ) after training T1.\n(e) Variances after training G1⊗T2.\nFigure 9.9: Visualization of GMM µ, Σ and generated samples G1 for SLT D5-5a.\n9.3\nDiscussion\nIn this section, various aspects of the proposed GMR approach are discussed. One aspect is the\nsampling quality, which is depicted in ﬁgure 9.9. Furthermore, it is discussed why the comparative\nperformance values diﬀer from conclusions drawn in related work. Another point up to discussion is\nrelated to the interpretation of the ﬁndings. Moreover, advantages of the GMR are outlined, speciﬁcally\nwith focus on the functionalities. In this context, the reconciliation with the real-world requirements is\ndiscussed, as well as the role of replay approaches for the CL paradigm.\nSampling Quality\nReplaying generated samples is not a novel CL approach, especially with regard to\nmitigating the CF eﬀect in deep learning models. VAEs and GANs oﬀer enormous potential concerning\nthe samples’ quality and variability (e.g., Wang, Liu, et al. 2018; Karras, Aila, et al. 2018). In general,\nthe pseudo-rehearsal process is strongly dependent on the quality and complexity of the generator. If\npage 142\nBenedikt Pfülb\n\nContinual Learning with DCGMMs\nDiscussion\nthe generated samples do not fully represent the problem, an inappropriate data distribution may be\nlearned, which leads to a loss of previous knowledge.\nThe GMR method is based on DCGMMs introduced in chapter 8. DCGMMs represent an SGD-\nbased training technique and layeriﬁcation of GMMs. The generated samples in ﬁgure 9.9b indicate\nthat the quality is not convincing with respect to the original data distribution (MNIST). It is noted\nthat the simplest model architecture is deployed (a non-convolutional folding layer, a GMM and a\nclassiﬁcation layer).\nIn the context of CL, the sampling quality is suﬃcient for obtaining a comparative CL performance.\nThe sampling quality contrasts the other generative models, which can only be re-trained with high-\nquality and representative samples. In relation to the GR model, both solver and generator need to be\nindependently re-trained again and again for each CL sub-task. This procedure is mandatory, since all\ncomponents are subject to the CF eﬀect, and previously learned knowledge is lost by further retraining.\nThe occurrence of the CF eﬀect is diﬀerent for the GMR approach. The basic knowledge is mainly\nstored in an unsupervised manner within the GMM layer. The eﬀect of slow forgetting in GMM layers\nis demonstrated in section 9.1. As a consequence, the linear classiﬁcation layer is primarily aﬀected\nby CF. Therefore, the pseudo-rehearsal mechanism addresses knowledge prevention in the last linear\nclassiﬁcation layer. Thus, by replaying generated samples, the assignment of a class to a Gaussian\ncomponent is refreshed. This eﬀect becomes obvious in the position of the prototypes in ﬁgure 9.9a\nand ﬁgure 9.9c, as well as in the trends of ﬁgure 9.8. For this reason, the quality of the generated\nsamples is less relevant.\nAs the quality of the generated samples is not crucial, other replay mechanisms can be realized.\nAnother approach could comprise the freezing of particularly representative components and the\nassociated class. In this case, the activation pattern only would have to be replayed. Therefore, no\nsamples would need to be generated.\nComparative Performance Values\nAnother point up to discussion concerns the low performance\nvalues of the reference CL model in the context of related work. This may be due to several factors.\nIn general, determining suitable hyper-parameters is time- and resource consuming. Even though a\ncertain number of parameters is varied for the investigation (see section 9.2.1.2), specifying them for\ndiverse model types, CL tasks (SLTs) and problems (datasets) is challenging. This is especially true\nif models or their CL performance highly depend on the choice of hyper-parameters. The deployed\narchitecture is one of the hyper-parameters which is diﬃcult to deﬁne for diﬀerent types of tasks and\nproblems.\nThe applied evaluation strategy is another crucial factor that inﬂuences the resulting CL performance.\nAs mentioned before, EWC seems to be more sensitive to the number of re-training iterations,\nrespectively the position of measurement points (see ﬁgure 9.8). In ﬁgure 9.8, a peak in the CL\nperformance is noted after a task change. If more measurements are performed, a higher maximum\nperformance can be expected. However, this would lead to the disadvantage that the perfect time or\nmodel parameter speciﬁcation needs to be determined, which can be diﬃcult under certain conditions.\nFurthermore, GANs as sample generators cause various challenges. The mutual training process of\ngenerator and discriminator leads to dependencies. If one of both components is not well conﬁgured,\nthe entire training process is aﬀected. Furthermore, GANs are subject to “mode collapse”. In this case,\nthe GAN solely generates identical samples from a single class, which cannot be distinguished from\nreal samples by the discriminator. Thus, the variability disappears and so do the samples from other\nclasses. Alternatively, the discriminator dominates, whereupon the generator learns nothing (caused\nby vanishing gradients). In this context, the relationship between the alternating training of generator\nand discriminator is particularly important. These kinds of problems are addressed by many diﬀerent\nGAN variants, e.g., Wasserstein GAN (Arjovsky, Chintala, et al. 2017), DCGAN (Radford, Metz, et al.\n2016), BEGAN (Li, Xiao, et al. 2018). However, even the adapted variants need further optimization\n(see Gulrajani, Ahmed, et al. 2017).\nAnother problem of GANs is that they are aﬀected by the CF eﬀect as outlined by Thanh-Tung\nand Tran (2018). Therefore, GANs always need to be trained with the joint training dataset by using\nself-generated samples. Furthermore, the determination and adjustment of GAN parameters seems\nto be a fundamental issue since the training process is very sensitive. This problem manifests in the\nexperimental results, where no satisfying parameter conﬁguration was identiﬁed. For the application\nof VAEs, the parameterization seems to be easier with respect to the resulting CL performance.\nBenedikt Pfülb\npage 143\n\nDiscussion\nContinual Learning with DCGMMs\nInterpretation of Results\nThe results of the CL investigation can be interpreted as follows. It is\nnoted that the GMR approach exhibits valuable CL capabilities. Moreover, the model is less complex\nand the stored knowledge is easier to interpret, i.e., prototypes, variances, etc. The examined GMR\narchitecture has a lower number of free parameters and is therefore more memory eﬃcient. The GR\nvariants contain approx. 3 000 000 free parameters on average. In contrast, the used GMR model\ncontains 2Kd+10K +10 = 201010 (for components K = 100, data dimensionality d = 1000 and 10\nclasses).\nThe current disadvantage of GMR is the signiﬁcantly lower baseline performance. The baseline\nperformance of 87.4 % on the MNIST dataset is too weak. In contrast, the DNN used for EWC presents\na baseline performance of 97.5 % which is due to the architecture of the DNN. The EWC, however,\nhas no inﬂuence on the baseline performance. State-of-the-art performance for MNIST is presented\nby the solver network (99.3 %) of the GR approach (without ﬁne tuning). Regardless, the obtained\nresults are in the error range of 10−1 within the world rankings.\nFurther optimizations regarding the architecture and ﬁne-tuning of parameters can lead to better\nCL performances. However, the trends of the diﬀerent models will remain the same. The problem of the\npoor baseline performance of GMR can be addressed by using more complex DCGMM architectures.\nFirst experiments imply that a folding variant of the same DCGMM may achieve a baseline performance\nof up to 97.5 %. This value still does not equal the common 99 % on MNIST. It is nevertheless acceptable\nfor the conduction of further research. Considering that the diﬀerent components of the GR approaches\nuse convolutional layers, the CL performance of the GMR is quite acceptable.\nFunctionalities\nAnother diﬀerence between GMR and the other investigated models is the range of\nproposed functionalities. GMR owes its functionalities to the underlying DCGMM. Discriminative\nmethods such as DNNs are not able to perform density estimation. Thus, it is diﬃcult to distinguish\noutliers from inliers. This function is a signiﬁcant advantage with respect to a reaction to task\nboundaries.\nMost CL models are based on the deﬁnition of task boundaries. In order to resolve CL problems\n(SLTs), all models require and use the task boundary trigger, e.g., to start a merging mechanism\nor start generating samples. Without specifying the boundaries, an additional model needs to be\nintroduced, even though it adds complexity. The same applies to the separation of generator and\nlearner, as it is the case for the GR approach. In contrast to that, GMR combines generator and\nsolver in one entity. Other functions such as sampling, inpainting or variation generation can only be\nperformed with generative methods. Unfortunately, EWC does not belong to this type of methods\n(which is not its goal).\nReconcile with Real-World Requirements\nAnother point that is discussed addresses the fulﬁll-\nment of real-world requirements for CL applications. One requirement for all investigations is that\nneither data from the past nor from the future can be accessed for optimization purposes. This is\nspeciﬁed as part of the investigation protocol (see chapter 6). Both, GMR and GR generate samples\nfrom past data distributions and therefore do not require additional memory for hold-out buﬀers.\nLikewise, EWC does not require samples from the past.\nHowever, another requirement is related to a constant time complexity and the number of previous\nsub-tasks. This requirement can only be fulﬁlled, if the same number of samples is generated for each\nnew sub-task. If the number needs to be proportional to the number of training samples in the new\nsub-task, this requirement is particularly diﬃcult to realize. This particularly applies to streaming\nscenarios, as the number of samples is not known beforehand. As a consequence, a constant number\nof generated replay samples has to be suﬃcient for all replay models, regardless of the number of\nsub-tasks. This constraint was not implemented as part of the present investigation, although GMR\nis hardly aﬀected by the number of generated samples. A replay experiment with a ﬁxed number of\nsamples (5000) per sub-task achieves the same CL performance. This is due to the location where the\nforgetting takes place, namely the classiﬁcation layer.\nSince EWC does not demand any past or future data for suppressing the CF eﬀect, deﬁning a\npoint to stop the training process is crucial. A mechanism or criterion to stop the training process is\nmissing in EWC and would have to be added. At the same time, no samples from the past can be\nused to determine an appropriate point to stop the training process. This, in turn, would contradict\nthe memory constraints for a potentially inﬁnite number of sub-tasks.\npage 144\nBenedikt Pfülb\n\nContinual Learning with DCGMMs\nConclusion and Future Work\nAnother constraint prohibits the access of future data. Future data is sometimes used to adjust\nthe model architecture or hyper-parameters in advance. This methodology is often applied, although\nit is technically unfeasible in real-world applications. The same is true for the architecture of the\ngenerator and solver for the GR approach used in this study. Problem independence with regard to\nthe hyper-parameters seems to be an advantage of the GMR model. The parameter optimization\nalways results in the same parameter conﬁguration for all problems/datasets – at least in this study.\nAccordingly, the deployment of GMR could be realized in zero-knowledge scenarios.\nGenerative-Replay and Real-World Constraints\nAnother issue related to generative-replay\napproaches and CL is of a general nature. The ratio of samples from new to old classes must be\nadhered to, even if a ﬁxed number of samples is generated. This is due to the sensitivity of the DNNs\nin terms of data distribution and the number of classes. If generated samples from 99 classes are mixed\nwith a single new class, the resulting ratio is 99:1. For a batch of data with the size B=100, 99 samples\nare generated and only one sample of the stream can be processed at a time. This constantly increases\nthe training time by each CL task’s number of new classes. For this reason, replay approaches were\nexcluded from the initial investigation in chapter 7.\nThe basic problem is that data needs to be replayed in order to enable joint training. In fact,\nreplay approaches contradict the idea of continual learning. The following method addresses this\nproblem, but solely mitigates the arising problem of constant update complexity. The underlying idea\nis that the model itself determines whether or not forgetting begins. DCGMMs provides the required\nfunctionalities (i.e., density estimation) to detect forgetting. Implementing the detection of forgetting\nand proper reactions is considered a subject for further research, even though it merely shifts the\nproblem. If the forgetting rate increases, more knowledge needs to be retrained and the more complex\nthe training becomes for new tasks. For this reason, the question arises whether a replay approach\nconstitutes the best possible solution for CL tasks.\n9.4\nConclusion and Future Work\nIn this chapter, the Gaussian Mixture Replay (GMR) model is introduced which represents a pseudo-\nrehearsal approach for CL based on DCGMMs (see chapter 8).\nThe method describes a replay\nmechanism that generates samples of the learned data distribution. Generated samples are utilized\nto realize a joint training with past and current data. The main diﬀerence to conventional replay\napproaches, e.g., GR (Shin, Lee, et al. 2017), is that forgetting mostly takes place in the last linear\nclassiﬁcation layer. Therefore, the main part of knowledge is stored in the unsupervised GMM layer.\nGenerated samples are used to mitigate the CF eﬀect by a joint training of the liner layer which is\nresponsible for mapping a label to an active Gaussian component.\nFor the evaluation, diﬀerent datasets are divided into SLTs, which represent various CL tasks with\nhard task boundaries. Moreover, the evaluation protocol presented in chapter 6 implements requirements\nfrom real-world applications. Access to data from future tasks is prohibited for optimization purposes.\nIn order to establish comparability, diﬀerent CL models were subject to the same evaluation protocol.\nThis includes the well-known EWC (Kirkpatrick, Pascanu, et al. 2017), as well as state-of-the-art\npseudo-rehearsal methods such as GR (Shin, Lee, et al. 2017). The latter can operate with diﬀerent\ntypes of generators: VAEs (Kingma and Welling 2013) and GANs (Goodfellow, Pouget-Abadie, et al.\n2014).\nIn summary, the CL investigation results indicate that the GMR model obtains comparable CL\nperformance even with the simplest architecture. It is claimed that the GMR model is aﬀected\ndiﬀerently by the CF eﬀect compared to other deep learning models (e.g., DNNs). A limitation,\nhowever, is the lower baseline performance, which is about 10 % worse compared to the other models.\nHowever, this can be compensated with more complex convolutional and deeper architectures. Initial\ninvestigations indicate accuracies of up to 97 %, which is acceptable as a basis for future research.\nThe problem of the lower baseline performance can be addressed by the use of deeper and/or\nconvolutional DCGMMs. A study in the context of the CL paradigm could be a ﬁrst step to verify a\ncausal correlation. A better quality of the generated samples could have a particular positive impact\non the CL performance. Investigating other problem domains is another area for future research. The\nstudy of discrete features could be part of these problem domains. In this context, an adaptation of\nthe sampling process could be beneﬁcial.\nBenedikt Pfülb\npage 145\n\nConclusion and Future Work\nContinual Learning with DCGMMs\nAnother open research questions concerns the gradual transition between the tasks. A more\nintelligent adaptation to diﬀerent types of changing data distributions is important with regard to\nreal-world scenarios. The basis could be provided by monitoring the log-likelihood of streamed samples.\nVarious reactions would be an option, such as the adjustment of the learning rate ϵ or the annealing\nparameter σ. The decision when, which and how many samples need to be generated by the model\nitself is a further optimization of the replay process.\nIn the CL context, other mechanisms can be realized in order to protect prototypes from modiﬁca-\ntions. One approach could be the utilization of BMU counters or component weights π for the decision\nwhether a prototype should be protected or adapted. “Selective forgetting” can be considered as a\nyoung research area for DNNs (see Hayase, Yasutomi, et al. 2020). In this context, existing knowledge\nneeds to be explicitly removed or changed. Assuming that the individual GMM components store the\nderived knowledge, the presented model constitutes an interesting opportunity for selective forgetting.\nVarious mechanisms for adding and removing GMM components (Kristan, Skočaj, et al. 2008) can be\nrealized in order to address the selective forgetting or the CL paradigm.\npage 146\nBenedikt Pfülb\n\n10.\nFindings and General Discussion\nChapter Contents\n10.1 Application-Oriented CL Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . 147\n10.2 Continual Learning Investigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n10.3 Novel Deep Learning Model DCGMM . . . . . . . . . . . . . . . . . . . . . . . . . 150\n10.4 Gaussian Mixture Replay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nThis chapter highlights the relevant conclusions in order to answer the research questions of the present\nwork. This summary includes the ﬁndings and challenges that were obtained in the course of this work.\nLikewise, the research questions are revisited and answered.\nStructure\nThe structure of this chapter corresponds to the sequence of the research questions\npresented in chapter 4. For each section, a brief summary of the corresponding chapter is presented.\nIt is followed by a holistic discussion of the rising challenges, before the research questions RQs are\nanswered.\nFirst of all, the presented evaluation protocol is discussed in section 10.1. The evaluation protocol\nis applied in a study of diﬀerent deep learning models. Findings of the investigation with regard to\nthe continual learning (CL) performance are discussed in section 10.2. An independent part of this\nwork focuses on the development of a novel deep learning model – referred to as Deep Convolutional\nGaussian Mixture Model (DCGMM). DCGMMs and their functionalities are discussed in section 10.3.\nThis chapter is concluded with a discussion of Gaussian Mixture Replay (GMR), which transfers the\npresented model into the CL context. Again, the evaluation protocol is used in section 10.4 in order to\nestablish comparability.\n10.1\nApplication-Oriented CL Evaluation Protocol\nThe evaluation protocol proposed in chapter 6 is intended to detect the occurrence of the catastrophic\nforgetting (CF) eﬀect in deep learning models. The CL performance is related to the CF eﬀect due to\nthe ability to protect past knowledge. In order to develop an application-oriented evaluation protocol,\nvarious real-world requirements are derived from the real-world scenario presented in chapter 5.\nThe protocol describes the training and evaluation process of machine learning (ML) step by step.\nThe protocol implements various application-oriented requirements, as it aims at drawing conclusions\nregarding the applicability of a ML model. The requirements are derived from an exemplary real-world\nscenario, where CL capabilities constitute a basic prerequisite. Unfortunately, it has become obvious\nthat an evaluation within the exemplary scenario is challenging. The dynamics and properties of the\ndata would lead to results that are diﬃcult to interpret.\nThe comparison of the proposed evaluation protocol to others from related works reveals several\naspects. In many cases, the appropriate level of detail is lacking or its speciﬁcation is missing. However,\nthese “small” details can be decisive for the applicability of CL models in real-world application\nscenarios. The proposed protocol distinguishes between two quality measurement criteria: “Best” and\n“last”. The best quality criterion is deﬁned by the best possible model parameter conﬁguration that\ncan be achieved during the training process. Quite often, this criterion can only be implemented for\nthe initial model training, which corresponds to the product development or manufacturing phase. In\nBenedikt Pfülb\npage 147\n\nApplication-Oriented CL Evaluation Protocol\nFindings and Discussion\nthis phase, suﬃcient computational resources are available for the optimization. The same applies to a\ncomputational optimization (grid-search) regarding hyper-parameters, e.g., a model’s architecture.\nDuring the initialization phase, data from future tasks are not accessible, which corresponds to the\napplication in a real-world scenario. A comparable scenario is the manufacturing of a robot and its\ndelivery, although knowledge needs to be added after the delivery. The constraints and requirements\nof the realistic scenario are implemented as part of the proposed evaluation protocol. Speciﬁcally\nprepared datasets with a high number of features are supposed to allow for the investigation of CL\ncapabilities of various CL avoidance models. In order to derive CL tasks, the datasets are divided into\nmultiple sub-tasks. The sub-tasks consist of diﬀerent disjoint class combinations, where knowledge\nneeds to be extracted sequentially task by task. It is obvious that this type dataset division does not\nrepresent all possible CL tasks, i.e., diﬀerent types of changing data. Nevertheless, ﬁrst conclusions\ncan be drawn with regard to the extraction and preservation of knowledge from the entire dataset.\nAccordingly, the so-called Sequential Learning Tasks (SLTs) represent the easiest type of CL task\nwhich is due to the indicated hard task boundaries.\nThe adaptation of problem-dependent hyper-parameters is crucial for the performance of ML\nmodels. The CL evaluation protocol speciﬁes an initial hyper-parameter optimization for several\nparameters. The standard optimization procedure is based on a training dataset where multiple\nexperiments are performed with diﬀerent combinations of hyper-parameters. After conducting multiple\nexperiments, the best parameter constellation is determined according to the experimental results. In\nthis optimization phase, the model’s architecture is selected.\nFor the following CL sub-tasks, a retroactive optimization with respect to a model’s architecture is\nunrealistic. Some hyper-parameters can be adapted, although this is very computationally intensive.\nThis kind of optimization would be challenging for embedded services due to limited resources, e.g.,\nmemory, processing or power supply. Nevertheless, the protocol allows the subsequent optimization of\none important hyper-parameter, i.e, the learning rate. The main purpose of a second optimization is\nto counteract the lack of an extensive ﬁne-tuning process in the initial phase. Thus, this second and\nsmaller optimization should compensate the lacking ﬁne-tuning.\nReal-World Requirements for CL Scenarios RQ 1.1\nRQ 1.1 addresses the question of real-\nworld requirements for CL scenarios. An exemplary scenario described as part of chapter 5 serves as a\nbasis for deriving CL requirements. The CL scenario describes a challenge from the area of computer\nnetworks which should be tackled by the application of an ML technique. Unknown metadata at the\nbeginning of a network connection is the information that is supposed to be predicted, e.g, the duration\nor number of transferred bytes. The challenge is to train an ML model and enable the prediction in a\ndata stream.\nMetadata of completed communications are provided as a stream of data in order to train an\nML model. For this reason, the corresponding ML model has to fulﬁll diﬀerent scenario-speciﬁc\nrequirements, e.g., incremental training. Deep Neural Networks (DNNs) constitute a suitable method,\nsince they have a constant update complexity and derive complex functions from data. The goal is to\npredict metadata (e.g., bit-rate of a communication) that can be used, for example, in more advanced\napplications such as optimized routing technologies. In the context of the present work, the exemplary\nscenario is discussed from the perspective of the CL paradigm. Accordingly, various requirements for\nthe CL scenario were derived. Considering the dynamics and inﬂuence of experimental trends in all\naspects, e.g., human variations, application updates and many more, continual adaptation is essential\nfor success.\nThe CL evaluation within the real-world scenario seems very challenging. The unknown change of\nthe data distribution leads to untraceable dynamics, causing obstacles related to the speciﬁcation of a\nlearning objective. To conclude, the derivation of various requirements and their implementation by\nthe evaluation protocol seem to be the only option.\nCF Eﬀect in DNNs RQ 1.2\nThe second sub-research question RQ 1.2 addresses the manifestation\nof the CF eﬀect in DNNs. After training a DNN with an excellent performance, additional knowledge\nshould be added. The new knowledge is presented in form of a new/diﬀerent data distribution. The\nproblem of an abrupt knowledge loss arises due to further training steps on the new data distribution.\nThe previously derived knowledge is lost after a few training steps. Thus, the CF prevents/complicates\nthe use of DNNs in CL scenarios.\npage 148\nBenedikt Pfülb\n\nFindings and Discussion\nContinual Learning Investigation\nIt is obvious that this behavior is not new and has been described in many other works. The\neﬀect is proven by training DNNs with a dataset that is divided into two tasks of disjoint classes.\nEach sub-task represents a diﬀerent data distribution. By measuring the prediction accuracy on test\nsamples of each task, the occurrence of the CF eﬀect becomes observable. At the same time, the\nseparation of training and test data ensures a valid conclusion regarding the memorization of training\ndata. Considering the performance measures obtained for the individual sub-tasks, the rapid drop in\nknowledge with regard to the previous sub-tasks becomes clear.\nExisting CF Evaluations Protocols RQ 1.3\nResearch question RQ 1.3 refers to the investigations\ncarried out in related works.\nVarious scientiﬁc studies deﬁne applied evaluation schemes in an\ninconsistent manner. The experimental setups of related work usually describe various CL tasks and\nhyper-parameter conﬁgurations for evaluation purposes. In addition, the parameters leading to the\nbest possible performance are presented. But the determination of the the parameter constellation or\nthe description of a dedicated evaluation procedure is often omitted. This is not necessarily due to\nthe authors’ intentions, but rather to self-evident requirements or page limits. Even if a code base is\navailable, the scheme and evaluation strategy is frequently omitted. The absence may be due to the\nfact that the focus of research is often on a novel CF avoidance model. Tracing the code base of an\nimplemented protocol is usually very time-consuming. These circumstances lead to the challenging\ncomprehensibility of results and classiﬁcation of the ﬁndings.\nApplication-Oriented CL Protocol RQ 1\nRQ 1 is answered by the evaluation protocol presented\nin chapter 6. The protocol implements several real-world requirements resulting from the answers of\nthe sub-research question RQ 1.1. Based on the conclusion of RQ 1.2, the protocol investigates CL\ntasks that allow to measure the occurrence of the CF eﬀect. The answer to RQ 1.3 helps identify the\ndeﬁcits of the existing protocols, which supports the speciﬁcation of a precise evaluation protocol.\nSeveral diﬀerent ML models can be investigated under uniform conditions by means of the developed\nevaluation protocol. These conditions allow for a comparison of the CF avoidance models and a\nclassiﬁcation of the methods with regard to the requirements. Conclusions and ﬁndings drawn by the\nCL investigation protocol always need to be set into the context of the implemented requirements of\nthe protocol. Accordingly, ﬁndings regarding the CF avoidance capabilities of a deep learning model\ncan never be absolute.\n10.2\nContinual Learning Investigation\nThe proposed CL evaluation protocol outlined in chapter 6 is used to examine various CF avoidance\nmodels. Various existing models were examined for the investigation. According to the respective\nauthors, all of them “avoid”, “mitigate” or at least “weaken” the CF eﬀect. The present study comprises\nstandard models such as Fully-Connected (FC)-DNNs and Convolutional Neural Networks (CNNs),\nwhich do not address the CF eﬀect and therefore are considered as reference values (baselines).\nFurthermore, the avoidance eﬀect of dropout (Hinton, Srivastava, et al. 2012b) applied to the two\nstandard models as proposed by Goodfellow, Mirza, et al. (2013) is evaluated. Special attention is\ndedicated to diﬀerent CF avoidance models: Local Winner Takes All (LWTA) (Srivastava, Masci,\net al. 2013), Elastic Weight Consolidation (EWC) (Kirkpatrick, Pascanu, et al. 2016) and Incremental\nMoment Matching (IMM) (Lee, Kim, et al. 2017). These models can be reconciled with the real-world\nrequirements of the investigation protocol, with the exception of IMM. Even though IMM needs data\nfrom a past task, the model was examined as it is considered a state-of-the-art model. The contradiction\nwith the presented real-world CL requirements concerns many other CF avoidance models presented in\nchapter 3, e.g, the introduction of oracles specifying the task ID of a sample as for the Hard Attention\nto the Task (HAT) model (Serra, Suris, et al. (2018)).\nIn order to realize the empirical study, the code bases of the ML models were integrated into the\ndeveloped investigation framework. The framework implements the evaluation protocol and enables\nboth, a uniform execution of experiments and the evaluation itself. The experimental results indicate\nthat none of the investigated models provides a satisfying CL performance for all examined datasets\nand all CL tasks while complying with the application-oriented requirements. The obtained results\ncontrast the conclusions and ﬁndings by the corresponding models’ authors. However, signiﬁcantly\nBenedikt Pfülb\npage 149\n\nNovel Deep Learning Model DCGMM\nFindings and Discussion\nbetter CL performance (similar to the results of related work) can be obtained if the models are\nevaluated according to a less application-oriented protocol.\nNevertheless, some models achieve an acceptable CL performance for a few CL tasks. By applying\nEWC, a “catastrophic” forgetting can sometimes be avoided, whereas a “linear” forgetting process was\nobserved. This particular trend can be determined for CL tasks where less knowledge needs to be\nadded (e.g., single class). Unfortunately, this does not apply to CL tasks where more knowledge is\nadded (e.g., from ﬁve classes). Also, the linear forgetting eﬀect depends on the number of performed\ntraining iterations. Conversely, IMM performs better on tasks with more knowledge value, respectively\nmore classes. It is shown that IMM obtains acceptable results with a subsequent adjustment of the\nhyper-parameter α. In order to tune the parameter that deﬁnes the importance of the individual tasks,\npast data has to be used, even though this is a contradiction to the real-world requirements.\nExisting CF Avoidance Methods RQ 2.1\nRQ 2.1 refers to deep learning models addressing the\nCF problem. This question is answered by a systematic literature review and the resulting list of\nvarious methods (see section 3.2). Various existing models try to circumvent the problem in diﬀerent\nways (see section 2.3.5). A fundamental problem is that an implementation is not provided for all\nCF avoidance models. Furthermore, not all CF avoidance models comply with the deﬁned real-world\nrequirements. This includes, for example, the Hard Attention to the Task (HAT) model proposed by\nSerra, Suris, et al. (2018). In addition to the features of a sample, the task ID must be speciﬁed. For\nCL tasks with one new class each, this would be equivalent to the real sample’s label.\nA great number of new approaches or optimized methods is constantly being published. This trend\nis accompanied by new workshops and categories at high ranked conferences (e.g., ICLR and CVPR)\naddressing CL. Current developments indicate that the CF problem can still not be considered as fully\nsolved in the CL context.\nCL Investigation Framework RQ 2.2\nThe second sub-research question RQ 2.2 relates to the\nevaluation of CL models. This question is answered by providing an investigation framework for the\nexamination of ML models according to the uniform evaluation protocol. The framework requires\ncertain interfaces that need to be met. The manual model integration into the framework needs special\nattention in order to comply with the requirements. If a model stores samples, it is only allowed to\nallocate a limited amount of memory. This condition has to ensured by an inspection of the code or\nmonitoring the allocated memory.\nIn addition, the framework developed as part of this work includes multiple functionalities and\nsatisﬁes diﬀerent prerequisites. The framework also provides diﬀerent datasets and the functionality to\nimplement various types of CL tasks. Furthermore, the framework comprises a mechanism to perform\nlarge-scale parameter optimization with all relevant components.\nCL Investigation RQ 2\nRQ 2 can be answered by interpreting the CL performance results. The\nquestion is to what extent the investigated models can control the CF eﬀect under application-oriented\nconstraints. None of the studied models can completely eliminate or mitigate the forgetting eﬀect for\nall CL tasks and all datasets. Only for certain problems and for some particular tasks the CF eﬀect\ncan be mitigated under the application-oriented conditions. Therefore, conclusions can solely be drawn\nif the exact circumstances of the avoidance or mitigation of the CF eﬀect are provided.\n10.3\nNovel Deep Learning Model DCGMM\nA novel deep learning model referred to as Deep Convolutional Gaussian Mixture Models (DCGMMs)\nis proposed in chapter 8. DCGMMs are based on Gaussian Mixture Models (GMMs), which are\nnot considered as deep learning models. GMMs are usually trained by variants of the Expectation-\nMaximization (EM) algorithm (see Dempster, Laird, et al. 1977), and they belong to the unsupervised\nlearning methods. EM has some disadvantages that complicate its use in a data stream. Even though\nsome adaptations, such as a stochastic version referred to as stochastic EM (sEM) (Cappé and Moulines\n2009), allow an incremental training, the initialization problem of EM and sEM remains.\nThe Stochastic Gradient Descent (SGD) based training method presented in section 8.3 allows\nthe incremental training of GMMs and solves the data-driven initialization problem. In order to\npage 150\nBenedikt Pfülb\n\nFindings and Discussion\nNovel Deep Learning Model DCGMM\ndenote GMMs as “deep” models, a layer arrangement is suggested in section 8.4. In addition, various\nsupplementary layer types are introduced which support the use of GMM layers. On the one hand,\nclassiﬁcation layers transform DCGMMs into a supervised model. On the other hand, folding/convo-\nlutional layers perform a comparable transformation which empowers CNNs in the context of image\nprocessing. The layeriﬁcation realizes many diﬀerent functionalities: density estimation, classiﬁcation,\n(conditional) sampling and outlier detection, just to name a few. In order to verify the validity of\nthe SGD-based training approach, various experiments prove that equivalent results as compared to\nconventional standard training algorithms (EM, sEM respectively) can be achieved. As the focus is\nnot solely on CL applications, the novel model is also suitable for other research areas.\nThe new training procedure is in no way intended to present a superior approach, e.g., compared\nto EM or sEM. Nonetheless, no data-driven initialization is required for training GMMs, which is a\nclear advantage compared to the standard procedures. Even though the training procedure introduces\nmultiple hyper-parameters, the parameters are often intuitively chosen or do not seem to be very\nproblem-dependent. All of these advantages advocate for further exploring this approach.\nThe associated research question RQ 3 consists of two sub-questions, which can be considered\nas alternatives to each other. Either existing approach can be extended, or multiple CF avoidance\ntechniques can be combined. The alternative research question focuses on the development of a novel\ndeep learning model in the context of the CL paradigm. The latter is the case in the present work.\nExisting CF Avoidance Techniques RQ 3.1\nSub-research question RQ 3.1 addresses how CF\navoidance models suppress the forgetting eﬀect. The basic idea is to understand existing CF avoidance\ntechniques in order to combine or extend them. The aim is thus to develop an improved or novel CF\navoidance technique.\nCategorizing existing deep learning methods with regard to their approach towards CF reﬂects\ntheir underlying concept (see section 2.3.5). The corresponding question can therefore be answered by\na literature review (see section 3.2). The best model in this study is the regularization approach EWC\n(see section 7.1.1). EWC is based on a penalty term in the loss function that prevents task-speciﬁc\nmodel parameters from being changed. However, determining the importance of parameters for diﬀerent\ntasks is challenging. Another problem is related to stopping the training process in order to get the\nbest possible model parameter conﬁguration. The determination of the best possible stopping point\nbecomes more challenging in application-oriented scenarios.\nParameter isolation constitutes yet another CF avoidance category – LWTA is one of them. Diﬀerent\nsections of the underlying DNNs are identiﬁed and assigned to a certain sub-task. The basic problem\nis the selection of a section that is responsible for a certain sub-task or sample. If this additional\ninformation is unavailable, the choice becomes challenging. The same is true for the implementation\nof multiple readout layers. Usually, oracles provide guidance for the selection. As oracles are rare in\nreal-world scenarios, they are prohibited by the application-oriented requirements. By specifying a\nsample’s task ID, a CL task consisting of one class per task is already solved.\nAnother approach that suppresses the CF eﬀect is replay. Both, rehearsal and pseudo-rehearsal\nensure that a joint training with a data distribution of past and current sub-tasks can be performed.\nThis way, knowledge can be derived from all combined data distributions. Replay processes simulate a\njoint training with all data samples.\nReplay mechanisms do not directly address the CF eﬀect, but they frequently achieve the best CL\nperformance. Storing samples from old tasks violates application-oriented memory constraints. In\norder to consistently store samples, a potentially inﬁnite amount of memory needs to be provided. As\nthis is impossible in data streams, the number of samples that can be stored is limited. This leads to\nnew challenges, e.g., which samples are most representative and how a potentially inﬁnite number of\ntasks can be processed. As an alternative to storing samples, the sample’s inﬂuence on the model is\nstored (e.g., the gradients). Storage capacity, however, is still limited, so that the same challenge arises\nagain, sooner or later.\nAnother method for the realization of rehearsal is the generation of samples from the derived data\ndistribution. Generative models like Variational Autoencoders (VAEs) (Kingma and Welling 2013) or\nGenerative Adverserial Networks (GANs) (Goodfellow, Pouget-Abadie, et al. 2014) are considered to\nbe excellent generators that can be used for pseudo-rehearsal procedures. Generators can be used to\ncircumvent memory constraints by simply generating samples that represent the past data distribution.\nThus, representative samples can be generated and mixed with samples from new sub-tasks in order to\nBenedikt Pfülb\npage 151\n\nGaussian Mixture Replay\nFindings and Discussion\nprovide a joint training dataset.\nHowever, new challenges arise with regard to the quality of the generated samples. If generated\nsamples do not completely represent the data distribution of a previous sub-task, knowledge may get\nlost. For each re-training step, the previously generated samples need to be used for the derivation of\nthe joint data distribution. Furthermore, it is often diﬃcult to measure the quality of the generated\nsamples. In addition, the complexity of the training process of the generators is another challenge.\nOther open questions concern the required number of generated samples and the update complexity.\nThe fundamental problem of using generative approaches is that it contradicts the idea of continual\nlearning. By simulating a joint training, the CF is not suppressed, but bypassed. Once new knowledge\nis added, old knowledge needs to be relearned. This leads to an increasing training complexity with\neach new task. Thus, it is no longer possible to eﬃciently add further knowledge after a certain amount\nof knowledge has been processed. For this reason, replay approaches were excluded from the CF\navoidance investigation.\nNon-Deep Learning Methods RQ 3.2\nRQ 3.2 relates to ML models that are not inherently\ndeep, but not directly subject to the CF eﬀect. In this context, many diﬀerent ML models/algorithms\nare worth to be considered. Examining various deep learning techniques in the CL context drew\nattention to GMMs. IMM (Lee, Kim, et al. (2017)) models the importance of parameters by Mixture\nof Gaussians (MoG). Research in the area of CL shows that GMMs are investigated and used in CL\nscenarios, e.g., by Kristan, Skočaj, et al. (2008) (or see section 8.2). Therefore, an attempt is made to\naddress the open issues of the standard learning procedure and develop a new GMM training technique\nbased on SGD.\nGMMs are inherently unsupervised learning methods. Without the ability to perform classiﬁcation,\na comparison to discriminative ML models (e.g., DNNs) is impossible. DNNs consist of L numbers of\nhidden layers and S numbers of artiﬁcial neurons. The more layers and artiﬁcial neurons, the more\ncomplex functions can be approximated (universal approximation theorem, see Cybenko 1989). The\nsame applies to GMMs, but their ability to approximate complex functions depends on the number of\nGaussian components K (see Goodfellow, Bengio, et al. 2016). Since the computational cost linearly\ndepends on the number of components K, layeriﬁcation would be beneﬁcial for GMMs. Thus, the\nobjective is to develop a deep learning model based on the SGD-based optimization technique and a\nlayerized version of GMMs.\nNovel Deep Learning Method RQ 3\nA part of the answer to research question RQ 3 is the\ndeveloped novel deep learning approach referred to as Deep Convolutional Gaussian Mixture Model\n(DCGMM). The novel procedure is based on a SGD training process for (deep) GMMs. The annealing\nscheme is very useful (outstanding), as it allows GMM training without a data-driven initialization.\nSimilarly, DNNs can be trained without data-driven initialization.\nMoreover, layering GMMs supports their combination with conventional types of layers known from\nother deep learning models. Therefore, DCGMMs can be denoted as deep learning models. By adding\nvarious operations and layers, DCGMMs can be used to discriminate (classify) or to (conditionally)\ngenerate samples. In order to answer the research question RQ 3 completely, the CL properties of\nDCGMMs need to be evaluated. A CL investigation of DCGMMs is part of an extension and referred\nto as Gaussian Mixture Replay (GMR). GMR is a replay mechanism for DCGMMs (see section 10.4).\n10.4\nGaussian Mixture Replay\nGMR is a pseudo-rehearsal approach for CL based on DCGMMs (see chapter 8). The replay method\nmakes use of the conditional sampling functionality of DCGMMs in order to generate samples that\nare replayed for training new sub-tasks. The basic diﬀerence to conventional deep learning models\n(like DNNs) is that the knowledge is mainly stored inside the GMM layers. GMMs belong to the\nunsupervised learning methods. DCGMMs obtain their classiﬁcation ability by an additional standard\nlinear classiﬁcation layer. The linear layer is responsible for mapping an activation of one or multiple\nGMM components to a single class. Therefore, the replay mechanism is mainly used to protect\nknowledge in the last layer.\nIn order to evaluate the GMR approach, the evaluation protocol from chapter 6 is applied. Further-\nmore, EWC (Kirkpatrick, Pascanu, et al. 2016) and Generative Replay (GR) (Shin, Lee, et al. 2017) as\npage 152\nBenedikt Pfülb\n\nFindings and Discussion\nGaussian Mixture Replay\nanother replay approach is investigated and compared to GMR. The obtained results indicate that\nthe CL performance of EWC is very dependent on the number of performed training iterations and\nmeasurement points. Thus, EWC achieved the lowest CL performance according to the evaluation\nprotocol.\nGR was deployed and evaluated with two diﬀerent types of generators: VAEs (Kingma and Welling\n2013) and GANs (Goodfellow, Pouget-Abadie, et al. 2014). A CNN served as a solver for the GR\napproach. The investigation indicates that the CL performance of GR strongly depends on the quality\nof the generator. Despite the large number of introduced hyper-parameters and the deﬁnition of the\ntraining scheme, GAN training seems to be unstable in the conducted experiments, unlike VAEs.\nTherefore, the CL performance of GR including VAEs is considered as representative. A satisfactory CL\nperformance of GR can be recognized, especially for CL tasks consisting of two sub-tasks. Nevertheless,\nthe more CL sub-tasks are included, the lower the resulting CL performances.\nThe solver (CNN) of GR, as well as the DNN used for EWC outperforms the baseline performance\nof GMR. The inferior performance of GMR can be attributed to the applied DCGMM architecture –\na single GMM layer (K =100) and a classiﬁcation layer. Similar classiﬁcation rates to those of simple\nDNNs (as used for EWC) can be obtained with a convolutional DCGMM. Considering the baseline\naccuracy for the MNIST dataset (about 97 %) implies that the obtained performance is not equivalent\nto the state-of-the-art for this dataset. Nevertheless, the goal is to provide a CL performance that is\nacceptable for an application-oriented context. This is exactly the case for the GMR approach. A CL\nperformance comparable to other generative replay methods is achieved with respect to the baseline\naccuracy.\nReconcile GMR with Real-World Requirements\nThe main challenge is to reconcile a pseudo-\nrehearsal mechanism with application-oriented real-world requirements. First of all, the fundamental\nproblem of replay approaches is introduced. Basically, the challenge with replay approaches is that\nthey violate the requirement of constant update complexity. This is due to the generation of “past”\nsamples. It may not be possible to create enough samples for a representative data distribution due to\na ﬁxed number of generated samples. Thus, generating 20 samples would be suﬃcient, as long as 20\nclasses need to be distinguished. The number of generated samples can be increased step-wise. Even\nthough this strategy is problem-dependent, it is the only possibility for streaming scenarios. A second\nand more diﬃcult challenge is related to the joint training dataset. In this context, the particular\ndependence on strongly varying data distributions within the classes is especially problematic for\nDNNs. The more knowledge/classes have been learned in past tasks, the more samples need to be\ngenerated relative to the new knowledge/classes. A weighting mechanism can be considered as a ﬁrst\nattempt to address this problem, but it only works for a particular number of knowledge/classes.\nAll replay processes are aﬀected by this uneven data distribution problem, including the presented\nGMR. GMR, respectively DCGMMs, oﬀer several possibilities to address this problem. Estimating\nand monitoring the densities of training samples or generated samples can be accomplished by the\nmodel itself. By means of these mechanisms, forgetting can be detected and counteracted, e.g., by\nweighting or generating samples for a joint training. In this context, best matching unit (BMU) counter\nor component weights π can become involved in order to protect or release GMM components for\nfurther training. Therefore, the GMR model can be considered as a ﬁrst CL approach incorporating\nDCGMMs.\nCL with GMRs RQ 3\nThe replay procedure GMR is the second part of the answer to research\nquestion RQ 3. Accordingly, the presented GMR is another attempt to address the CF eﬀect for\ndeep learning models. The proposed DCGMM is used as basis for the replay mechanism. A ﬁrst\ninvestigation indicates that GMR provides various properties suitable for CL scenarios. The CF eﬀect\nis addressed by the proportional shifting of knowledge into unsupervised GMMs. A layered version\nof GMMs is diﬀerently aﬀected by forgetting compared to DNNs. Thus, the novel approach – the\nDCGMM – and the replay method GMR answer research question RQ 3.\nComparability with Other CL Methods RQ 4\nResearch question RQ 4 relates to the CL\nperformance of GMR and other CL models, especially deep learning models addressing the CF eﬀect.\nIn general, the investigated DCGMM architecture achieves a lower baseline performance in non-CL\nscenarios than other deep learning models (e.g., CNNs). A competitive value of >99 % accuracy on\nBenedikt Pfülb\npage 153\n\nGaussian Mixture Replay\nFindings and Discussion\nthe MNIST dataset compared to the initial 87 % is a too weak result. Nevertheless, classiﬁcation\naccuracies of about 97 % are achieved with a convolutional variant of the same DCGMM architecture.\nThe investigation of the GMR’s CL performance indicates similar results when compared to other\nreplay approaches, such as GR (Shin, Lee, et al. 2017). However, considering the baseline performance,\nthe GMR approach clearly illustrates the functionalities and capabilities required in diﬀerent CL\nscenarios. The superiority of a simpler parameterization is another important advantage. The CL\nresults indicate that a simple replay mechanism (GMR) based on DCGMMs leads to an acceptable\nCL performance. Thus, GMR and respectively DCGMMs serve as a basis for further investigations in\nmany other research areas, even beyond CL.\npage 154\nBenedikt Pfülb\n\n11.\nConclusion and Outlook\nChapter Contents\n11.1 Summary and Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n11.2 Prospects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\nThis chapter summarizes the conclusions and prospects of the present work. The work is considered on\nthe whole for the purpose of outlining the lessons learned. Furthermore, a collection of contributions is\npresented. Finally, newly emerging issues are addressed, which imply a need for further research.\n11.1\nSummary and Conclusion\nThe ability to continuously accumulate knowledge has been a decisive factor for the evolution of the\nhuman species. The same applies to the development of weak and strong artiﬁcial intelligences (AIs)\nand many application-oriented scenarios. A branch of AI research is concerned with a deep learning\napproach referred to as Deep Neural Networks (DNNs). Biologically inspired machine learning (ML)\nmodels have enormous potential and oﬀer many advantages compared to traditional ML models.\nNevertheless, DNNs suﬀer from an eﬀect referred to as catastrophic forgetting (CF). The CF eﬀect\nmanifests in an abrupt forgetting of existing knowledge, as soon as new knowledge is added. In order\nto overcome the problem, the complete training process usually has to be repeated with old and new\ndata samples. A disadvantage of this procedure is that, on the one hand, old training data must be\nkept available. On the other hand, the energy- and computationally-intensive process has to be entirely\nrepeated.\nThe CF problem has been investigated for quite some time. As a result of this research, various\nCF avoidance models and methods have been published. In general, other CF avoidance approaches\nensure that the eﬀect is mitigated or that the problem is completely solved. Accordingly, this is\nexperimentally proven in the related work, where each new model achieves even better continual\nlearning (CL) performances. As various types of CF avoidance techniques are frequently compared\nwith each other, a superiority of each novel model is demonstrated.\nHowever, the comparison of deep learning models addressing the CF eﬀect is not universally speciﬁed.\nAs a ﬁrst step of this work, a detailed and generally valid evaluation protocol with respect to CL\nscenarios is introduced. The presented evaluation protocol provides a general scheme for investigating\nML models, especially deep learning models, i.e., DNNs. As a special feature of the protocol, it respects\ndiﬀerent requirements of application oriented real-world scenarios. In order to develop a valid protocol,\nrequirements were derived from an exemplary CL scenario. The result of this ﬁrst step is an evaluation\nframework that allows the investigation of ML models in CL scenarios.\nThe second step of this work describes the investigation of diﬀerent CF avoidance models in order\nto quantify the obtained CL performance. The investigation required the code-wise embedding of the\nmodels into the evaluation framework. The result of the evaluation indicates that none of the examined\nML models can achieve an acceptable CL performance. This conclusion contrasts the ﬁndings of other\nauthors. The weak CL performance can be explained by means of the application-oriented requirements\nthat are implemented by the investigation protocol.\nNevertheless, individual CL tasks obtain satisfactory CL performances. The comprehensive investi-\ngation mainly implies that CF avoidance models in general can only “control”, “mitigate” or “suppress”\nBenedikt Pfülb\npage 155\n\nProspects\nConclusion and Outlook\nthe CF eﬀect under certain circumstances and under a given evaluation protocol. General assertions,\nsuch as “Here, we propose incremental moment matching (IMM) to resolve the catastrophic forgetting\nproblem.” (Lee, Kim, et al. 2017), can thus be regarded as inappropriate. A further point of criticism\nconcerns the frequent lack or only partial indication of the investigation/evaluation scheme used in\nother research. Usually, the code of the CF avoidance model is given, whereby the implementation of\nthe evaluation is omitted. Therefore, many published research results are not clearly reproducible.\nThe third part of this work focused on the development of a novel deep learning model, which\nhelps to make the forgetting eﬀect more controllable. The novel model is based on Gaussian Mixture\nModels (GMMs). The novelty of this work’s approach is related to the ability to train GMMs by\nusing Stochastic Gradient Descent (SGD), solve the data-driven initialization problem and realize\na “deep” arrangement in layers. The novel model is referred to as Deep Convolutional Gaussian\nMixture Model (DCGMM). The main diﬀerence compared to standard DNNs is that the major\npart of knowledge is stored in GMM layers and only the assignment of a class label is performed\nby a conventional linear layer. An advantage is that DCGMMs inherently support many important\nfunctionalities, e.g., density estimation or the ability to generate samples.\nAs the last step of this work, the newly developed deep learning model is transferred into the\nCL context. For this purpose, the replay approach to avoid CF is combined with DCGMMs, which\nis referred to as Gaussian Mixture Replay (GMR). GMR implements a pseudo-rehearsal procedure\nusing the functionality of DCGMMs in order to generate samples and realize a joint training. Similar\nCL capabilities can be achieved when comparing GMR with other replay processes. Thus, GMR\noﬀers basic functionalities that can help perform further research on the fulﬁllment of remaining\napplication-oriented CL requirements.\nIn sum, the main contributions of this work include the following aspects:\n• A detailed description of the investigation protocol for CL scenarios, which includes application-\noriented requirement extracted from an exemplary real-world scenario, is provided.\n• The results of the investigation of several CF avoidance models indicate that the CF eﬀect can only\nbe considered as “solved” under certain conditions that are challenging to reconcile with application\noriented CL requirements.\n• A novel deep learning model is introduced referred to as Deep Convolutional Gaussian Mixture\nModel (DCGMM). DCGMMs are based on GMMs that are arranged in layers and trained by SGD\nwithout any data-driven initialization.\n• Gaussian Mixture Replay (GMR) is proposed as a CL approach, which is based on DCGMMs.\nGMR is classiﬁed as a (pseudo-rehearsal) replay approach with an acceptable CL performance in\napplication-oriented scenarios.\n11.2\nProspects\nThe ﬁndings of this work can serve as a basis for future research. Examining the CL investigation\nprotocol oﬀers a ﬁrst starting point for enhancements. Possible reﬁnements include the possibility\nof describing and implementing more complex CL tasks. The investigation of further ML models is\nanother open issue. In this context, the assessment of possible application scenarios for diﬀerent models\nseems interesting. Another fundamental need for further research concerns the novel deep learning\nmodel DCGMM and its application in various scenarios. Furthermore, the transfer of DCGMMs to\nother contexts such as reinforcement learning or the processing of sequences can constitute the focus\nof future research.\nIn the following, some open issues and promising ideas for further research are presented. They are\ndedicated to the three major ﬁndings of this work:\n• Investigation protocol and study:\n– Adding further real-world requirements related to a changing data distribution.\n– The addition of other datasets, including (more non-visual) problems with higher dimensionalities.\n– Further development of the evaluation framework towards an independence of programming\nlanguages and ML frameworks.\n– Implementation of a requirements catalog for a transparent comparison of diﬀerent ML models.\n– Integration of an automated runtime and memory analysis.\n– Comparison of other (novel) CF avoidance models based on the uniform investigation protocol.\npage 156\nBenedikt Pfülb\n\nConclusion and Outlook\nProspects\n• Deep Convolutional Gaussian Mixture Models (DCGMMs):\n– Optimization of the DCGMM model with regard to functionalities and implementation, e.g.,\nMixtures of Factor Analyzer (MFA) layer.\n– Investigation of new mechanisms for outlier detection or regularization of the annealing parameter\nσ.\n– Investigating the possibility of targeted/intentional forgetting with regard to deletion or adapta-\ntion of speciﬁc knowledge.\n– Examination of the robustness known from the ﬁeld of adversarial attacks.\n– Applying DCGMMs as a basis for other ML methodologies, e.g., reinforcement or transfer\nlearning, and other areas such as unsegmented or connected tasks (like Recurrent Neural\nNetworks (RNNs)).\n– Adapting the functionalities into new areas such as inpainting or noise suppression.\n– Investigating the suitability for other application areas.\n• CL methodology (GMR):\n– Customization of the GMR replay procedure in order to meet the requirements of constant\nre-training complexity.\n– Using density estimation for the development of a novel method without a replay approach.\n– Further investigation of similar CL scenarios, e.g., with changing data distributions.\nAs a ﬁnal note, the following thought serves as a reminder of continual learning’s importance. Computer\nscience is actually trying to imitate what nature has produced over several millions of years of evolution:\nThe human brain. In just a few decades, machine learning algorithms have become increasingly\npowerful and sometimes even frightening to many people within our society. After all, the brain’s\ncapabilities constitute the “Holy Grail” in the context of continual learning. The ability to continuously\naccumulate, delete, and selectively correct knowledge can still be considered a challenge in the ﬁeld of\nmachine learning. Biologically inspired machine learning models attempt to mimic the brain’s complex\nprocesses, which are often inexplicable, even for neuroscience. Moreover, only highly specialized and\ncustomized algorithms can solve abstract problems. But they fail as soon as a small change of a single\nparameter occurs. Accordingly, the outstanding capabilities of biological (continual) learning have not\nyet been achieved.\n“The question of whether a computer can think is no more interesting than the question of whether a\nsubmarine can swim.” - Edsger W. Dijkstra.\nBenedikt Pfülb\npage 157\n\nProspects\nConclusion and Outlook\npage 158\nBenedikt Pfülb\n\nA.\nListings\nChapter Contents\nList of Figures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\nList of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\nList of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\nList of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\nList of Abbreviatons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169\nList of References\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\nBenedikt Pfülb\npage 159\n\nListings\npage 160\nBenedikt Pfülb\n\nListings\nList of Figures\n1.1\nExample of visual classiﬁcation problems. . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nInitial training of an ML model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.3\nEvaluation of the initially trained ML model. . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.4\nAdding additional knowledge to a pre-trained ML model.\n. . . . . . . . . . . . . . . .\n3\n1.5\nEvaluation of the re-trained ML model.\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.6\nRe-evaluation of the re-trained ML model. . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.7\nJoint training of an ML model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.8\nTesting of joint trained ML model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.1\nA simpliﬁed visualization of a biological neuron.\n. . . . . . . . . . . . . . . . . . . . .\n10\n2.2\nIllustration of the components of a single artiﬁcial neuron. . . . . . . . . . . . . . . . .\n10\n2.3\nPlot of sigmoid output function.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.4\nPlot of rectiﬁer output function.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.5\nLayer structure of an MLP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.6\nPlot of f(x) = x2.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.7\nPlot of f ′(x) = 2x. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.8\nVisualization of gradient descent steps. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n2.9\nExemplary function f(x) and its minima.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n15\n2.10 Visualization of a 2D gradient descent step. . . . . . . . . . . . . . . . . . . . . . . . .\n16\n2.11 Example of generalization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.12 Example of overﬁtting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n2.13 Changes in the data distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2.14 Changes in the empirical distribution.\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2.15 Types of changes over time. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2.16 The life-long learning paradigm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n2.17 Visualization of the CF eﬀect, a linear forgetting and the baseline. . . . . . . . . . . .\n23\n2.18 Visualization of the objective of regularization methods. . . . . . . . . . . . . . . . . .\n24\n2.19 Outline of rehearsal methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.20 Delineation of pseudo-rehearsal methods.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4.1\nResearch structure and interrelations of research questions.\n. . . . . . . . . . . . . . .\n35\n5.1\nOutline of the exemplary real-world problem. . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.2\nUse of ML to enable the classiﬁcation of network ﬂows.\n. . . . . . . . . . . . . . . . .\n41\n5.3\nUse case scenario with congestion on a single shortest path. . . . . . . . . . . . . . . .\n42\n5.4\nPreventing path congestion by throughput prediction.\n. . . . . . . . . . . . . . . . . .\n42\n5.5\nFlow Data Stream Pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n5.6\nInfrastructure model of the campus network in which ﬂow data is collected. . . . . . .\n44\n5.7\nData ﬂow diagram of network ﬂow data. . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n5.8\nHistograms of a block for all ﬂow labels. . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n5.9\nClass distribution for all blocks of the dataset.\n. . . . . . . . . . . . . . . . . . . . . .\n50\n5.10 Transport protocol-based network ﬂow tagging of the t-SNE results.\n. . . . . . . . . .\n50\n5.11 Locality-based network ﬂow tagging of the t-SNE results.\n. . . . . . . . . . . . . . . .\n51\n5.12 Application-based network ﬂow tagging of the t-SNE results.\n. . . . . . . . . . . . . .\n51\n5.13 Concept drift/shift experiment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.14 Training epochs comparison of slowest and fastest GPU. . . . . . . . . . . . . . . . . .\n54\n5.15 Visualization of the optimizer’s side eﬀect. . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n5.16 Trend for eight selected experiments. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n6.1\nSuspicious digits of the MNIST dataset. . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n6.2\nExemplary permuted digits of the MNIST dataset. . . . . . . . . . . . . . . . . . . . .\n66\nBenedikt Pfülb\npage 161\n\nList of Figures\nListings\n7.1\nParts of the evaluation protocol.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n7.2\nDiﬀerent proﬁles of the CF eﬀect. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n7.3\nExemplary LWTA block network with n=2. . . . . . . . . . . . . . . . . . . . . . . . .\n81\n7.4\nBest D-FC experiments for SLT D5-5.\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n7.5\nBest FC experiments for SLT D10-p10. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n7.6\nBest EWC experiments for SLT D5-5.\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n7.7\nEWC experiments for diﬀerent datasets on D9-1 SLTs. . . . . . . . . . . . . . . . . . .\n87\n7.8\nEWC experiments for diﬀerent datasets on D5-5 SLTs. . . . . . . . . . . . . . . . . . .\n87\n7.9\nBest IMM experiment on SLT D5-5f for the Devanagari dataset.\n. . . . . . . . . . . .\n89\n7.10 Best IMM experiments for SLT D5-5b for diﬀerent datasets. . . . . . . . . . . . . . . .\n90\n7.11 Best IMM experiments for SLT D9-1b for diﬀerent datasets. . . . . . . . . . . . . . . .\n90\n8.1\nPlots of exemplary Gaussian distributions. . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n8.2\nPlot of exemplary 2D multivariate Gaussian distributions. . . . . . . . . . . . . . . . .\n98\n8.3\nThree types of covariance matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n8.4\nIllustration of the EM algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n101\n8.5\nIllustration of the k-means algorithm.\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n102\n8.6\nDegenarated and sparse-component solutions for the MNIST dataset. . . . . . . . . . .\n107\n8.7\nVisualization of diﬀerent Gaussian smoothing ﬁlters. . . . . . . . . . . . . . . . . . . .\n108\n8.8\nExemplary visualization of the forward operation of a folding layer. . . . . . . . . . . .\n111\n8.9\nExemplary visualization of the sampling operation of a folding layer. . . . . . . . . . .\n111\n8.10 Exemplary visualization of the pooling operation. . . . . . . . . . . . . . . . . . . . . .\n112\n8.11 Exemplary visualization of the unpooling operation.\n. . . . . . . . . . . . . . . . . . .\n112\n8.12 Exemplary visualization of the forward operation of a GMM layer. . . . . . . . . . . .\n113\n8.13 Exemplary visualization of the sampling operation of a GMM layer.\n. . . . . . . . . .\n114\n8.14 Exemplary visualization of the forward operation of a classiﬁcation layer.\n. . . . . . .\n115\n8.15 Structural visualization of the linear layer for sampling from a DCGMM.\n. . . . . . .\n115\n8.16 Exemplary DCGMM instance for visualizing the inference process. . . . . . . . . . . .\n116\n8.17 Exemplary DCGMM model for visualizing the sampling process.\n. . . . . . . . . . . .\n118\n8.18 Exemplary GMM component visualization.\n. . . . . . . . . . . . . . . . . . . . . . . .\n118\n8.19 Exemplary results of centroids learned by SGD for diﬀerent datasets. . . . . . . . . . .\n121\n8.20 Visualization of sparse-component solutions for sEM for Fruits 360 and SVHN. . . . .\n122\n8.21 Interpretation of GMM components from the DCGMM instance. . . . . . . . . . . . .\n123\n8.22 Outlier detection capabilities of diﬀerent DCGMMs architectures. . . . . . . . . . . . .\n124\n8.23 The impact of convolution on sampling for MNIST. . . . . . . . . . . . . . . . . . . . .\n125\n8.24 The impact of convolution on sampling for FashionMNIST.\n. . . . . . . . . . . . . . .\n125\n8.25 The impact of diﬀerent values of S for top-S-sampling for MNIST. . . . . . . . . . . .\n125\n8.26 The impact of diﬀerent values of S for top-S-sampling for FashionMNIST. . . . . . . .\n126\n8.27 Impact of sharpening on generated samples for MNIST. . . . . . . . . . . . . . . . . .\n126\n8.28 Impact of sharpening on generated samples for FashionMNIST. . . . . . . . . . . . . .\n127\n9.1\nMeta-structure of the GMR model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n132\n9.2\nVisualization of an incrementally trained DCGMM for E =50. . . . . . . . . . . . . . .\n133\n9.3\nReplay scheme for GMR.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n133\n9.4\nRehearsal approach for the model training.\n. . . . . . . . . . . . . . . . . . . . . . . .\n135\n9.5\nPlot of Leaky ReLU output function. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\n9.6\nGMR task detection capabilities for a single class SLT. . . . . . . . . . . . . . . . . . .\n139\n9.7\nGMR task detection capabilities for a two class SLT. . . . . . . . . . . . . . . . . . . .\n139\n9.8\nTrends for diﬀerent SLT experiments for GMR, EWC and GR. . . . . . . . . . . . . .\n141\n9.9\nVisualization of GMM µ, Σ and generated samples G1 for SLT D5-5a. . . . . . . . . .\n142\npage 162\nBenedikt Pfülb\n\nListings\nList of Tables\n4.1\nUsed computational resources.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5.1\nFlow feature normalization and encoding examples. . . . . . . . . . . . . . . . . . . . .\n46\n5.2\nDetails of the ﬂow features in the data stream.\n. . . . . . . . . . . . . . . . . . . . . .\n47\n5.3\nOverview of the varied hyper-parameters for the initial DNN architecture grid-search.\n52\n5.4\nResults of the architectural hyper-parameters experiments.\n. . . . . . . . . . . . . . .\n53\n5.5\nContext-speciﬁc label boundaries for ﬂow streaming experiments. . . . . . . . . . . . .\n54\n5.6\nFeature groups for the ﬂow data experiments. . . . . . . . . . . . . . . . . . . . . . . .\n55\n5.7\nOverview of the ﬂow prediction experiment results. . . . . . . . . . . . . . . . . . . . .\n56\n6.1\nDetailed information of the MNIST dataset. . . . . . . . . . . . . . . . . . . . . . . . .\n63\n6.2\nDetailed information of the EMNIST dataset. . . . . . . . . . . . . . . . . . . . . . . .\n63\n6.3\nDetailed information of the FashionMNIST dataset.\n. . . . . . . . . . . . . . . . . . .\n63\n6.4\nDetailed information of the SVHN dataset.\n. . . . . . . . . . . . . . . . . . . . . . . .\n64\n6.5\nDetailed information of the Devanagari dataset. . . . . . . . . . . . . . . . . . . . . . .\n64\n6.6\nDetailed information of the CIFAR-10 dataset.\n. . . . . . . . . . . . . . . . . . . . . .\n64\n6.7\nDetailed information of the Fruits 360 dataset.\n. . . . . . . . . . . . . . . . . . . . . .\n64\n6.8\nDetailed information of the MADBase dataset.\n. . . . . . . . . . . . . . . . . . . . . .\n65\n6.9\nDetailed information of the NotMNIST dataset. . . . . . . . . . . . . . . . . . . . . . .\n65\n6.10 Detailed information of the ISOLET dataset. . . . . . . . . . . . . . . . . . . . . . . .\n65\n6.11 Deﬁnition of SLTs and the class divisions of their sub-tasks.\n. . . . . . . . . . . . . .\n67\n7.1\nResults of the prescient evaluation protocol. . . . . . . . . . . . . . . . . . . . . . . . .\n84\n7.2\nOverview of the results according to the realistic evaluation protocol. . . . . . . . . . .\n85\n7.3\nOverview of the results according to the realistic evaluation protocol. . . . . . . . . . .\n88\n7.4\nSummary of SLTs for IMM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n89\n8.1\nClustering performance comparison of SGD and sEM training.\n. . . . . . . . . . . . .\n120\n8.2\nDiﬀerent random and non-random centroid initializations on SGD training. . . . . . .\n120\n8.3\nFinal log-likelihood comparison of SGD and sEM training. . . . . . . . . . . . . . . . .\n122\n8.4\nConﬁgurations of diﬀerent DCGMM architectures. . . . . . . . . . . . . . . . . . . . .\n122\n8.5\nClustering performance comparison of diﬀerent DCGMM architectures.\n. . . . . . . .\n124\n9.1\nParameter conﬁguration for Solver and GAN. . . . . . . . . . . . . . . . . . . . . . . .\n137\n9.2\nParameter conﬁguration for Solver and VAE.\n. . . . . . . . . . . . . . . . . . . . . . .\n138\n9.3\nResults of the CL experiments for GMR, EWC and GR. . . . . . . . . . . . . . . . . .\n141\nBenedikt Pfülb\npage 163\n\nList of Tables\nListings\npage 164\nBenedikt Pfülb\n\nListings\nList of Algorithms\n2.1\nSteps of backpropagation algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n6.1\nThe realistic application-oriented evaluation protocol. . . . . . . . . . . . . . . . . . . .\n71\n6.2\nThe prescient evaluation protocol. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n8.1\nProcedure of the EM algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n101\n8.2\nSteps of SGD-GMM training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n109\nBenedikt Pfülb\npage 165\n\nList of Algorithms\nListings\npage 166\nBenedikt Pfülb\n\nListings\nList of Equations\n2.1\nOne-hot encoding for single-class classiﬁcation problems. . . . . . . . . . . . . . . . . .\n9\n2.2\nExample activation function.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.3\nSigmoid output function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.4\nRectiﬁer output function.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n2.5\nMSE loss function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.6\nCE loss function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.7\nSoftmax function and its properties.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n6.1\nDeﬁnition of the accuracy metric. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n7.1\nEWC loss function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n7.2\nLWTA competition/interaction function. . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n7.3\nIMM loss function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n7.4\nFisher information matrix for IMM.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n8.1\nProbability density function of the Gaussian distribution.\n. . . . . . . . . . . . . . . .\n96\n8.2\nPartial derivative of the log-likelihood function for a normal distribution.\n. . . . . . .\n97\n8.3\nLog-likelihood function for a normal distribution. . . . . . . . . . . . . . . . . . . . . .\n97\n8.4\nMultivariate Gaussian distribution for a vector x. . . . . . . . . . . . . . . . . . . . . .\n98\n8.5\nLikelihood estimation by multiple Gaussian components. . . . . . . . . . . . . . . . . .\n99\n8.6\nThe complete-data likelihood. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n8.7\nThe incomplete-data likelihood. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n8.8\nThe total incomplete-data likelihood. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n8.9\nThe total incomplete-data log-likelihood. . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n8.10 The log-likelihood loss function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n8.11 Determination of soft cluster membership. . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n8.12 Determiniation of the weighted mean for GMMs. . . . . . . . . . . . . . . . . . . . . .\n102\n8.13 Determiniation of the weighted variances for GMMs. . . . . . . . . . . . . . . . . . . .\n102\n8.14 Determiniation of the mixing coeﬃcients for GMMs. . . . . . . . . . . . . . . . . . . .\n102\n8.15 Normalization of GMM component weights π. . . . . . . . . . . . . . . . . . . . . . . .\n105\n8.16 Max-component approximation ˆL.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n106\n8.17 Gradients for the max-component approximation ˆL loss function. . . . . . . . . . . . .\n107\n8.18 Smoothed max-component approximation ˆLσ. . . . . . . . . . . . . . . . . . . . . . . .\n107\n8.19 Deﬁnition of the σ reduction criterion ∆.\n. . . . . . . . . . . . . . . . . . . . . . . . .\n108\n8.20 Determination of the output dimensions of a folding layer. . . . . . . . . . . . . . . . .\n110\n8.21 Determination of the convolutional responsibilities of a GMM layer.\n. . . . . . . . . .\n112\n8.22 Determination of the convolutional loss L of a GMM layer.\n. . . . . . . . . . . . . . .\n113\n8.23 Determination of the outlier detection threshold. . . . . . . . . . . . . . . . . . . . . .\n117\n8.24 Determination of outliers and inliers. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n117\n8.25 Inversion of the linear layer for conditional sampling. . . . . . . . . . . . . . . . . . . .\n118\n8.26 Gradient ascend step to compensate quality losses. . . . . . . . . . . . . . . . . . . . .\n119\n9.0\nEWC loss function adapted to the used terminology. . . . . . . . . . . . . . . . . . . .\n136\n9.0\nFormulation of the zero-sum game in GANs. . . . . . . . . . . . . . . . . . . . . . . . .\n136\n9.0\nStandard loss function for VAEs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\n9.0\nLeaky ReLU output function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\nBenedikt Pfülb\npage 167\n\nList of Equations\nListings\npage 168\nBenedikt Pfülb\n\nListings\nList of Abbreviations\nAI\nartiﬁcial intelligence\nANN\nArtiﬁcial Neural Network\nASN\nAutonomous System Number\nBMU\nbest matching unit\nCE\nCross-Entropy\nCF\ncatastrophic forgetting\nCL\ncontinual learning\nCNN\nConvolutional Neural Network\nCSV\nComma-separated values\nDCGMM Deep Convolutional Gaussian Mixture Model\nDNN\nDeep Neural Network\nDNS\nDomain Name System\nD\nDropout\nEM\nExpectation-Maximization\nEWC\nElastic Weight Consolidation\nFC\nFully-Connected\nFIM\nFischer Information Matrix\nGAN\nGenerative Adverserial Network\nGMM\nGaussian Mixture Model\nGMR\nGaussian Mixture Replay\nGR\nGenerative Replay\nIMM\nIncremental Moment Matching\nJSON\nJavaScript Object Notation\nLWTA\nLocal Winner Takes All\nMFA\nMixtures of Factor Analyzer\nMLE\nmaximum-likelihood estimation\nMLP\nMulti Layer Perceptron\nML\nmachine learning\nMSE\nMean Squared Error\nNHWC\nNumber of samples × Height × Width × Channels\nNN\nNeural Network\nNaN\nNot a Number\nPCA\nPrincipal Component Analysis\nPDF\nprobability density function\nQoS\nQuality of Service\nRNN\nRecurrent Neural Network\nReLU\nRectiﬁer Linear Unit\nReST\nRepresentational State Transfer\nSDN\nSoftware Deﬁned Network\nSGD\nStochastic Gradient Descent\nSLT\nSequential Learning Task\nSOM\nSelf-Organizing Map\nSSH\nSecure Shell\nSVM\nSupport Vector Machine\nTCP\nTransmission Control Protocol\nUDP\nUser Datagram Protocol\nVAE\nVariational Autoencoder\nVLAN\nVirtual Local Area Network\nsEM\nstochastic Expectation-Maximization (EM)\nt-SNE\nt-distributed Stochastic Neighbor Embedding\nBenedikt Pfülb\npage 169\n\nList of Abbreviations\nListings\npage 170\nBenedikt Pfülb\n\nListings\nList of References\nAcharya, Shailesh, Ashok Kumar Pant, and Prashnna Kumar Gyawali (2016). “Deep learning based\nlarge scale handwritten Devanagari character recognition”. In: SKIMA 2015 - 9th International\nConference on Software, Knowledge, Information Management and Applications. doi: 10.1109/\nSKIMA.2015.7400041.\nAdel, Tameem, Cuong V. Nguyen, Richard E. Turner, Zoubin Ghahramani, and Adrian Weller (2019).\n“Interpretable Continual Learning”. In: url: https://openreview.net/forum?id=S1g9N2A5FX.\nAgarwal, S, M Kodialam, and T Lakshman (2013). “Traﬃc engineering in software deﬁned networks”.\nIn: IEEE pp 2211.January.\nAlberts, B., D. Bray, J. Lewis, M. Raﬀ, K. Roberts, and J.D. Watson (2002). Molecular Biology of the\nCell. 4th. Garland.\nAljundi, R., P. Chakravarty, and T. Tuytelaars (2017). “Expert Gate: Lifelong Learning with a Network\nof Experts”. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 7120–7129. doi: 10.1109/CVPR.2017.753.\nAljundi, Rahaf, Min Lin, Baptiste Goujaud, and Yoshua Bengio (2019). “Gradient based sample\nselection for online continual learning”. In: Advances in Neural Information Processing Systems\n32.NeurIPS. issn: 10495258. arXiv: 1903.08671.\nArbib, Michael A and James J Bonaiuto (2016). From neuron to cognition via computational neuro-\nscience. MIT Press.\nArjovsky, Martin, Soumith Chintala, and Léon Bottou (2017).\n“Wasserstein GAN”.\nIn:\narXiv:\n1701.07875. url: http://arxiv.org/abs/1701.07875.\nAwduche, Daniel O (1999). “MPLS and Traﬃc Engineering in IP Networks”. In: IEEE Communications\nMagazine.\nAzzouni, Abdelhadi, Raouf Boutaba, and Guy Pujolle (2017).\n“NeuRoute: Predictive Dynamic\nRouting for Software-Deﬁned Networks”. In: 13th International Conference on Network and Service\nManagement.\nBaudry, Jean Patrick and Gilles Celeux (2015). “EM for mixtures: Initialization requires special care”. In:\nStatistics and Computing 25.4, pp. 713–726. issn: 15731375. doi: 10.1007/s11222-015-9561-x.\nBehnke, Sven (2003). Hierarchical neural networks for image interpretation. Vol. 2766, pp. 1–220.\nisbn: 3540407227.\nBenson, Theophilus, Aditya Akella, and David A Maltz (2010). “Network Traﬃc Characteristics of\nData Centers in the Wild”. In: 10th ACM SIGCOMM conference on Internet measurement.\nBernardo, José M. and Adrian F.M. Smith (2008). “Bayesian Theory”. In: Bayesian Theory, pp. 1–595.\ndoi: 10.1002/9780470316870.\nBertsekas, Dimitri P. (1996). Constrained Optimization and Lagrange Multiplier Methods (Optimization\nand Neural Computation Series). 1st ed. Athena Scientiﬁc. isbn: 1886529043.\nBishop, Christopher M. (1995). Neural Networks for Pattern Recognition. USA: Oxford University\nPress, Inc. isbn: 0198538642.\nBoutaba, Raouf, Mohammad A. Salahuddin, Noura Limam, Sara Ayoubi, Nashid Shahriar, Felipe\nEstrada-Solano, and Oscar M. Caicedo (2018). “A Comprehensive Survey on Machine Learning for\nNetworking: Evolution, Applications and Research Opportunities”. In: Journal of Internet Services\nand Applications.\nBrim, Scott W. and Brian E. Carpenter (2002). Middleboxes: Taxonomy and Issues. RFC 3234.\nBrownlee, Jason (2019). Probability for machine learning: Discover how to harness uncertainty with\nPython. Machine Learning Mastery.\nBrutzkus, Alon and Amir Globerson (2019). “Why do larger models generalize better? A theoretical\nperspective via the XOR problem”. In: 36th International Conference on Machine Learning, ICML\n2019 2019-June, pp. 1310–1318. arXiv: 1810.03037.\nBuda, Mateusz, Atsuto Maki, and Maciej A Mazurowski (2018). “A systematic study of the class\nimbalance problem in convolutional neural networks”. In: Neural Networks 106, pp. 249–259.\nBush, Bruce M. (2014). “The Perils of Floating Point”. In: Lahey Computer Systems, Inc., p. 8. url:\nhttp://www.lahey.com/float.htm.\nBenedikt Pfülb\npage 171\n\nList of References\nListings\nBuzzega, Pietro, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara (2020). “Dark\nExperience for General Continual Learning: a Strong, Simple Baseline”. In: Advances in Neural\nInformation Processing Systems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual. url: https://proceedings.neurips.cc/\npaper/2020/hash/b704ea2c39778f07c617f6b7ce480e9e-Abstract.html.\nCappé, Olivier and Eric Moulines (2009). “On-Line Expectation-Maximization Algorithm for Latent\nData Models”. In: Journal of the Royal Statistical Society. Series B (Statistical Methodology) 71.3,\npp. 593–613. issn: 13697412, 14679868. url: http://www.jstor.org/stable/40247590.\nCauchy, Augustin et al. (1847).\n“Méthode générale pour la résolution des systemes d’équations\nsimultanées”. In: Comp. Rend. Sci. Paris 25.1847, pp. 536–538.\nChaudhry, Arslan, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip H S Torr (2018). “Rie-\nmannian Walk for Incremental Learning: Understanding Forgetting and Intransigence Arslan”. In:\n1, pp. 1–22. arXiv: arXiv:1801.10112v3.\nChaudhry, Arslan, Albert Gordo, Puneet K Dokania, Philip Torr, and David Lopez-paz (2021). “Using\nHindsight to Anchor Past Knowledge in Continual Learning”. In: arXiv: arXiv:2002.08165v2.\nChaudhry, Arslan, Ranzato Marc’Aurelio, Marcus Rohrbach, and Mohamed Elhoseiny (2019). “Eﬃcient\nlifelong learning with A-GEM”. In: 7th International Conference on Learning Representations,\nICLR 2019, pp. 1–20. arXiv: 1812.00420.\nChen, Jianfei, Jun Zhu, Yee Whye Teh, and Tong Zhang (2018).\n“Stochastic expectation max-\nimization with variance reduction”.\nIn: Advances in Neural Information Processing Systems\n2018-Decem.NeurIPS, pp. 7967–7977. issn: 10495258.\nChen, Zhiyuan and Bing Liu (2016).\n“Lifelong Machine Learning”.\nIn: Synthesis Lectures on\nArtiﬁcial Intelligence and Machine Learning 10.3, pp. 1–27. issn: 19394616. doi: 10.2200/\nS00737ED1V01Y201610AIM033.\nChollet, François (2017). François Chollet Twitter. url: https://twitter.com/fchollet/status/\n852594987527045120 (visited on 03/25/2021).\nCireşan, Dan C., Ueli Meier, Jonathan Masci, Luca M. Gambardella, and Jürgen Schmidhuber (2011).\n“Flexible, high performance convolutional neural networks for image classiﬁcation”. In: IJCAI\nInternational Joint Conference on Artiﬁcial Intelligence, pp. 1237–1242. issn: 10450823. doi:\n10.5591/978-1-57735-516-8/IJCAI11-210. eprint: arXiv:1011.1669v3.\nCisco (2008). Flexible NetFlow Technology White Paper.\nClaise, B. (2004). Cisco Systems NetFlow Serv. Export V9. RFC 3954.\nCohen, Gregory, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik (2017). “EMNIST: Extending\nMNIST to handwritten letters”. In: Proceedings of the International Joint Conference on Neural\nNetworks 2017-May, pp. 2921–2926. doi: 10.1109/IJCNN.2017.7966217. eprint: 1702.05373.\nCole, Ronald and Mark Fanty (1990). “Spoken letter recognition”. In: pp. 385–390. doi: 10.3115/\n116580.116725.\nCvetkovski, Zdravko (2012). “Convexity, Jensen’s Inequality”. In: Inequalities: Theorems, Techniques\nand Selected Problems. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 69–77. isbn: 978-3-642-\n23792-8. doi: 10.1007/978-3-642-23792-8_7. url: https://doi.org/10.1007/978-3-642-\n23792-8_7.\nCybenko, G. (1989). “Approximation by superpositions of a sigmoidal function”. In: Mathematics of\nControl, Signals, and Systems 2.4, pp. 303–314. issn: 0932-4194. doi: 10.1007/BF02551274. url:\nhttp://link.springer.com/10.1007/BF02551274.\nDavid, Omid E. and Nathan S. Netanyahu (2016). “DeepPainter: Painter Classiﬁcation Using Deep\nConvolutional Autoencoders”. In: pp. 20–28.\nDavies, David L. and Donald W. Bouldin (1979).\n“A Cluster Separation Measure”.\nIn: IEEE\nTransactions on Pattern Analysis and Machine Intelligence PAMI-1.2, pp. 224–227. issn: 01628828.\ndoi: 10.1109/TPAMI.1979.4766909.\nDayan, Peter, Laurence F Abbott, et al. (2003). “Theoretical neuroscience: computational and mathe-\nmatical modeling of neural systems”. In: Journal of Cognitive Neuroscience 15.1, pp. 154–155.\nDelange, M., R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh, and T. Tuyte-\nlaars (2021). “A continual learning survey: Defying forgetting in classiﬁcation tasks”. In: IEEE\nTransactions on Pattern Analysis and Machine Intelligence, pp. 1–1.\nissn: 1939-3539.\ndoi:\n10.1109/TPAMI.2021.3057446.\nDempster, A. P., N. M. Laird, and D. B. Rubin (1977). “ Maximum Likelihood from Incomplete Data\nVia the EM Algorithm ”. In: Journal of the Royal Statistical Society: Series B (Methodological)\n39.1, pp. 1–22. doi: 10.1111/j.2517-6161.1977.tb01600.x.\npage 172\nBenedikt Pfülb\n\nListings\nList of References\nDognin, Pierre L., Vaibhava Goel, John R. Hershey, and Peder A. Olsen (2009). “A fast, accurate\napproximation to log likelihood of Gaussian mixture models”. In: ICASSP, IEEE International\nConference on Acoustics, Speech and Signal Processing - Proceedings 3, pp. 3817–3820. issn:\n15206149. doi: 10.1109/ICASSP.2009.4960459.\nDuch, Włodzisław and Norbert Jankowski (1999). “Survey of neural transfer functions”. In: Neural\nComputing Surveys 2, pp. 163–212.\nDuchi, John, Elad Hazan, and Yoram Singer (2010). “Adaptive subgradient methods for online learning\nand stochastic optimization”. In: COLT 2010 - The 23rd Conference on Learning Theory 12,\npp. 257–269.\nDunn, J. C. (1973). “A fuzzy relative of the ISODATA process and its use in detecting compact\nwell-separated clusters”. In: Journal of Cybernetics 3.3, pp. 32–57. issn: 00220280. doi: 10.1080/\n01969727308546046.\nEngel, Paulo Martins and Milton Roberto Heinen (2010). “Incremental Learning of Multivariate\nGaussian Mixture Models”. In: Advances in Artiﬁcial Intelligence – SBIA 2010. Berlin, Heidelberg:\nSpringer Berlin Heidelberg, pp. 82–91. isbn: 978-3-642-16138-4.\nEykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash,\nTadayoshi Kohno, and Dawn Song (2017). “Robust Physical-World Attacks on Deep Learning\nModels”. In: arXiv: 1707.08945. url: http://arxiv.org/abs/1707.08945.\nFadlullah, Zubair Md, Fengxiao Tang, Bomin Mao, Nei Kato, Osamu Akashi, Takeru Inoue, and\nKimihiro Mizutani (2017a). “State-of-the-Art Deep Learning: Evolving Machine Intelligence Toward\nTomorrow’s Intelligent Network Traﬃc Control Systems”. In: IEEE Communications Surveys and\nTutorials 19.4, pp. 2432–2455. issn: 1553877X. doi: 10.1109/COMST.2017.2707140.\nFadlullah, Zubair Md, Fengxiao Tang, Bomin Mao, Nei Kato, Osamu Akashi, Takeru Inoue, and\nKimihiro Mizutani (2017b).\n“State-of-the-Art Deep Learning: Evolving Machine Intelligence\nToward Tomorrow’s Intelligent Network Traﬃc Control Systems”. In: IEEE Communications\nSurveys & Tutorials 19.4, pp. 2432–2455.\nFarquhar, Sebastian and Yarin Gal (2018). “Towards robust evaluations of continual learning”. In:\narXiv. issn: 23318422. arXiv: 1805.09733.\nFausett, Laurene (1994). Fundamentals of Neural Networks: Architectures, Algorithms, and Applications.\nUSA: Prentice-Hall, Inc. isbn: 0133341860.\nFeldman, Dan, Matthew Faulkner, and Andreas Krause (2011). “Scalable Training of Mixture Models\nvia Coresets”. In: Advances in Neural Information Processing Systems. Ed. by J. Shawe-Taylor,\nR. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger. Vol. 24. Curran Associates, Inc., p. 9. url:\nhttps://proceedings.neurips.cc/paper/2011/file/2b6d65b9a9445c4271ab9076ead5605a-\nPaper.pdf.\nFernando, Chrisantha, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu,\nAlexander Pritzel, and Daan Wierstra (2017). “PathNet: Evolution Channels Gradient Descent in\nSuper Neural Networks”. In: issn: 1701.08734. arXiv: 1701.08734. url: http://arxiv.org/abs/\n1701.08734.\nFlach, Peter (2012). Machine learning: the art and science of algorithms that make sense of data.\nCambridge university press.\nFrench, Robert M. (1997). “Pseudo-recurrent Connectionist Networks: An Approach to the ’Sensitivity-\nStability’ Dilemma”. In: Connection Science 9.4, pp. 353–380. issn: 09540091. doi: 10.1080/\n095400997116595.\nGe, Rong, Qingqing Huang, and Sham M. Kakade (2015). “Learning mixtures of gaussians in high\ndimensions”. In: Proceedings of the Annual ACM Symposium on Theory of Computing 14-17-June-\n2015, pp. 761–770. issn: 07378017. doi: 10.1145/2746539.2746616. arXiv: 1503.00424.\nGepperth, Alexander and Benedikt Pfülb (2020). “A Rigorous Link Between Self-Organizing Maps and\nGaussian Mixture Models”. In: Artiﬁcial Neural Networks and Machine Learning - ICANN 2020 -\n29th International Conference on Artiﬁcial Neural Networks, Bratislava, Slovakia, September 15-18,\n2020, Proceedings, Part II. Vol. 12397. Lecture Notes in Computer Science. Springer, pp. 863–872.\ndoi: 10.1007/978-3-030-61616-8\\_69. url: https://doi.org/10.1007/978-3-030-61616-\n8\\_69.\nGepperth, Alexander and Benedikt Pfülb (2021). “Gradient-based training of Gaussian Mixture\nModels in High-Dimensional Spaces”.\nIn: Neural Processing Letters.\nissn: 1573-773X.\ndoi:\n10.1007/s11063-021-10599-3. eprint: 1912.09379.\nGepperth, Alexander and Florian Wiech (2019). “Simpliﬁed Computation and Interpretation of\nFisher Matrices in Incremental Learning with Deep Neural Networks”.\nIn: Lecture Notes in\nBenedikt Pfülb\npage 173\n\nList of References\nListings\nComputer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in\nBioinformatics) 11728 LNCS, pp. 481–494. issn: 16113349. doi: 10.1007/978-3-030-30484-3_39.\nGhahramani, Zoubin and Geoﬀrey E. Hinton (1997). “The EM Algorithm for Mixtures of Factor\nAnalyzers”. In: Compute, pp. 1–8. issn: 0003-0023. doi: 173068. arXiv: 173068.\nGlorot, Xavier and Yoshua Bengio (2010). “Understanding the diﬃculty of training deep feedforward\nneural networks”. In: Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence\nand Statistics. Ed. by Yee Whye Teh and Mike Titterington. Vol. 9. Proceedings of Machine\nLearning Research.\nChia Laguna Resort, Sardinia, Italy: PMLR,\npp. 249–256.\nurl: http:\n//proceedings.mlr.press/v9/glorot10a.html.\nGlorot, Xavier, Antoine Bordes, and Yoshua Bengio (2011). “Deep Sparse Rectiﬁer Neural Networks”.\nIn: Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics.\nEd. by Geoﬀrey Gordon, David Dunson, and Miroslav Dudík. Vol. 15. Proceedings of Machine\nLearning Research. Fort Lauderdale, FL, USA: PMLR, pp. 315–323. url: http://proceedings.\nmlr.press/v15/glorot11a.html.\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville (2016). Deep Learning. http://www.deeplearn\ningbook.org. MIT Press.\nGoodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio (2014). “Generative Adversarial Nets”. In: Advances in Neural\nInformation Processing Systems 27.\nGoodfellow, Ian J., Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio (2013). “An Empirical\nInvestigation of Catastrophic Forgetting in Gradient-Based Neural Networks”. In: issn: 1751-8113.\ndoi: 10.1088/1751-8113/44/8/085201. eprint: 1312.6211. url: http://arxiv.org/abs/1312.\n6211.\nGoodfellow, Ian J., Jonathon Shlens, and Christian Szegedy (2015). “Explaining and harnessing\nadversarial examples”. In: 3rd International Conference on Learning Representations, ICLR 2015 -\nConference Track Proceedings, pp. 1–11. arXiv: 1412.6572.\nGraf, Hans Peter, Eric Cosatto, Leon Bottou, Igor Durdanovic, and Vladimir Vapnik (2005). “Parallel\nsupport vector machines: The cascade SVM”. In: Advances in Neural Information Processing\nSystems. issn: 10495258.\nGraupe, Daniel (2013). Principles of artiﬁcial neural networks. Vol. 7. World Scientiﬁc.\nGrother, P. (1995). “NIST Special Database 19 Handprinted Forms and Characters Database”. In.\nGulrajani, Ishaan, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville (2017).\n“Improved Training of Wasserstein GANs”. In: arXiv: 1704.00028. url: http://arxiv.org/abs/\n1704.00028.\nHagan, Martin T, Howard B Demuth, and Mark Beale (1997). Neural network design. PWS Publishing\nCo.\nHan, Song, Huizi Mao, Enhao Gong, Shijian Tang, William J. Dally, JeﬀPool, John Tran, Bryan\nCatanzaro, Sharan Narang, Erich Elsen, Peter Vajda, and Manohar Paluri (2017). “DSD: Dense-\nsparse-dense training for deep neural networks”. In: 5th International Conference on Learning\nRepresentations, ICLR 2017 - Conference Track Proceedings. arXiv: 1607.04381.\nHandelman, Guy S., Hong Kuan Kok, Ronil V. Chandra, Amir H. Razavi, Shiwei Huang, Mark Brooks,\nMichael J. Lee, and Hamed Asadi (2019). “Peering Into the Black Box of Artiﬁcial Intelligence:\nEvaluation Metrics of Machine Learning Methods”. In: American Journal of Roentgenology 212.1.\nPMID: 30332290, pp. 38–43. doi: 10.2214/AJR.18.20224. eprint: https://doi.org/10.2214/\nAJR.18.20224. url: https://doi.org/10.2214/AJR.18.20224.\nHardegen, Christoph, Benedikt Pfülb, Sebastian Rieger, and Alexander Gepperth (Dec. 2020). “Pre-\ndicting Network Flow Characteristics Using Deep Learning and Real-World Network Traﬃc”. In:\nIEEE Trans. on Netw. and Serv. Manag. 17.4, 2662–2676. issn: 1932-4537. doi: 10.1109/TNSM.\n2020.3025131. url: https://doi.org/10.1109/TNSM.2020.3025131.\nHardegen, Christoph, Benedikt Pfülb, Sebastian Rieger, Alexander Gepperth, and Sven Reißmann\n(2019). “Flow-based Throughput Prediction using Deep Learning and Real-World Network Traﬃc”.\nIn: Proceedings of the 15th International Conference on Network and Service Management.\nHardegen, Christoph and Sebastian Rieger (2020). “Prediction-based Flow Routing in Programmable\nNetworks with P4”. In: 16th International Conference on Network and Service Management,\nCNSM 2020, 2nd International Workshop on Analytics for Service and Application Management,\nAnServApp 2020 and 1st International Workshop on the Future Evolution of Internet Protocols,\nIPFuture 2020, pp. 3–7. doi: 10.23919/CNSM50824.2020.9269072.\npage 174\nBenedikt Pfülb\n\nListings\nList of References\nHayase, Tomohiro, Suguru Yasutomi, and Takashi Katoh (2020). “Selective Forgetting of Deep Networks\nat a Finer Level than Samples”. In: arXiv: 2012.11849. url: http://arxiv.org/abs/2012.11849.\nHayes, Tyler L., Ronald Kemker, Nathan D. Cahill, and Christopher Kanan (2018). “New Metrics and\nExperimental Paradigms for Continual Learning”. In: 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops (CVPRW), pp. 2112–21123. doi: 10.1109/cvprw.2018.\n00273.\nHaykin, Simon (2009). Neural networks and learning machines, 3/E. Pearson Education India.\nHeaton, Jeﬀ(2015). “Artiﬁcial intelligence for humans, volume 3: Deep learning and neural networks;\nheaton research”. In: Inc.: St. Louis, MO, USA.\nHinton, Geoﬀrey, Nitish Srivastava, and Kevin Swersky (2012a). Lecture 6.5 - RmsProp: Divide the\ngradient by a running average of its recent magnitude.\nHinton, Geoﬀrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov\n(2012b). “Improving neural networks by preventing co-adaptation of feature detectors”. In: pp. 1–18.\nissn: 9781467394673. doi: arXiv:1207.0580. arXiv: 1207.0580. url: http://arxiv.org/abs/\n1207.0580.\nHornik, Kurt, Maxwell Stinchcombe, and Halbert White (1989). “Multilayer feedforward networks\nare universal approximators”.\nIn: Neural Networks 2.5, pp. 359–366.\nissn: 08936080.\ndoi:\n10.1016/0893-6080(89)90020-8.\nHosseini, Reshad and Suvrit Sra (2015). “Matrix Manifold Optimization for Gaussian Mixtures”.\nIn: Advances in Neural Information Processing Systems. Ed. by C. Cortes, N. Lawrence, D. Lee,\nM. Sugiyama, and R. Garnett. Vol. 28. Curran Associates, Inc. url: https://proceedings.\nneurips.cc/paper/2015/file/dbe272bab69f8e13f14b405e038deb64-Paper.pdf.\nHosseini, Reshad and Suvrit Sra (2020). “An alternative to EM for Gaussian mixture models: batch\nand stochastic Riemannian optimization”. In: Mathematical Programming 181.1, pp. 187–223.\nissn: 14364646.\ndoi: 10 . 1007 / s10107 - 019 - 01381 - 4.\neprint: 1706 . 03267.\nurl: https :\n//doi.org/10.1007/s10107-019-01381-4.\nHsu, Yen Chang, Yen Cheng Liu, Anita Ramasamy, and Zsolt Kira (2018). “Re-evaluating continual\nlearning scenarios: A categorization and case for strong baselines”. In: arXiv Nips. issn: 23318422.\narXiv: 1810.12488.\nHuszár, Ferenc (2018). “Note on the quadratic penalties in elastic weight consolidation”. In: Proceedings\nof the National Academy of Sciences of the United States of America 115.11, E2496–E2497. issn:\n10916490. doi: 10.1073/pnas.1717042115. arXiv: arXiv:1712.03847v1.\nIoﬀe, Sergey and Christian Szegedy (2015). “Batch Normalization: Accelerating Deep Network Training\nby Reducing Internal Covariate Shift”. In: Proceedings of the 32nd International Conference on\nMachine Learning. Ed. by Francis Bach and David Blei. Vol. 37. Proceedings of Machine Learning\nResearch. Lille, France: PMLR, pp. 448–456. url: https://proceedings.mlr.press/v37/\nioffe15.html.\nIyengar, Jana, Costin Raiciu, Sebastien Barre, Mark J. Handley, and Alan Ford (2011). Architectural\nGuidelines for Multipath TCP Development. RFC 6182.\nJapkowicz, Nathalie and Shaju Stephen (2002). “The class imbalance problem: A systematic study”.\nIn: Intelligent data analysis 6.5, pp. 429–449.\nJoseph, K. J. and Vineeth N. Balasubramanian (2020). “Meta-Consolidation for Continual Learning”.\nIn: Advances in Neural Information Processing Systems. Ed. by H. Larochelle, M. Ranzato, R.\nHadsell, M. F. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., pp. 14374–14386. url:\nhttps://proceedings.neurips.cc/paper/2020/file/a5585a4d4b12277fee5cad0880611bc6-\nPaper.pdf.\nJurkiewicz, Piotr, Grzegorz Rzym, and Piotr Borylo (2018). “Flow Length and Size Distributions in\nCampus Internet Traﬃc”. In: Computing Research Repository.\nKamra, Nitin, Umang Gupta, and Yan Liu (2017). “Deep Generative Dual Memory Network for\nContinual Learning”. In: CoRR abs/1710.10368. arXiv: 1710.10368. url: http://arxiv.org/\nabs/1710.10368.\nKarahoca, Adem (2012). Advances in data mining knowledge discovery and applications. BoD–Books\non Demand.\nKarras, Tero, Timo Aila, Samuli Laine, and Jaakko Lehtinen (2018). “Progressive growing of GANs\nfor improved quality, stability, and variation”.\nIn: 6th International Conference on Learning\nRepresentations, ICLR 2018 - Conference Track Proceedings, pp. 1–26.\nKelleher, John D (2019). Deep learning. MIT press.\nBenedikt Pfülb\npage 175\n\nList of References\nListings\nKeller, James M, Derong Liu, and David B Fogel (2016). Fundamentals of computational intelligence:\nneural networks, fuzzy systems, and evolutionary computation. John Wiley & Sons.\nKemker, Ronald, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan (2017).\n“Measuring Catastrophic Forgetting in Neural Networks”. In: issn: 1091-6490. doi: 10.1073/pnas.\n1611835114. eprint: 1708.02072. url: http://arxiv.org/abs/1708.02072.\nKim, Dahun, Sanghyun Woo, Joon Young Lee, and In So Kweon (2019). “Deep video inpainting”. In:\nProceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n2019-June, pp. 5785–5794. issn: 10636919. doi: 10.1109/CVPR.2019.00594. eprint: 1905.01639.\nKim, Hyo-Eun, Seungwook Kim, and Jaehwan Lee (2018). “Keep and Learn: Continual Learning\nby Constraining the Latent Space for Knowledge Preservation in Neural Networks”. In: arXiv:\n1805.10784. url: http://arxiv.org/abs/1805.10784.\nKingma, Diederik P. and Jimmy Lei Ba (2015). “Adam: A method for stochastic optimization”. In: 3rd\nInternational Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings,\npp. 1–15. eprint: 1412.6980.\nKingma, Diederik P and Max Welling (2013). “Auto-Encoding Variational Bayes”. In: Ml, pp. 1–14.\narXiv: 1312.6114. url: http://arxiv.org/abs/1312.6114.\nKirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,\nClaudia Clopath, Dharshan Kumaran, and Raia Hadsell (2017). “Overcoming catastrophic forgetting\nin neural networks”. In: Proceedings of the National Academy of Sciences 114.13, pp. 3521–3526.\nissn: 0027-8424. doi: 10.1073/pnas.1611835114. eprint: https://www.pnas.org/content/114/\n13/3521.full.pdf. url: https://www.pnas.org/content/114/13/3521.\nKirkpatrick, James, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,\nClaudia Clopath, Dharshan Kumaran, and Raia Hadsell (2016). “Overcoming catastrophic forgetting\nin neural networks”. In: CoRR abs/1612.00796. arXiv: 1612.00796. url: http://arxiv.org/\nabs/1612.00796.\nKohonen, Teuvo (1982). “Self-organized formation of topologically correct feature maps”. In: Biological\nCybernetics 43.1, pp. 59–69. issn: 03401200. doi: 10.1007/BF00337288.\nKreutz, Diego, Fernando MV Ramos, Paulo Verissimo, Christian Esteve Rothenberg, Siamak Azodol-\nmolky, and Steve Uhlig (2015). “Software-Deﬁned Networking: A Comprehensive Survey”. In:\nProceedings of the IEEE.\nKristan, Matej, Danijel Skočaj, and Ales Leonardis (2008). “Incremental learning with Gaussian\nmixture models”. In: 1, pp. 25–32.\nKrizhevsky, Alex, Ilya Sutskever, and Geoﬀrey E. Hinton (2017). “ImageNet Classiﬁcation with Deep\nConvolutional Neural Networks”. In: Communications of the ACM 60.6, pp. 84–90. issn: 0001-0782.\ndoi: 10.1145/3065386. url: https://dl.acm.org/doi/10.1145/3065386.\nKubat, Miroslav, Stan Matwin, et al. (1997). “Addressing the curse of imbalanced training sets:\none-sided selection”. In: Icml. Vol. 97. 1. Citeseer, p. 179.\nLeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haﬀner (1998). “Gradient-based learning\napplied to document recognition”. In: Proceedings of the IEEE 86.11, pp. 2278–2323. issn: 00189219.\ndoi: 10.1109/5.726791. eprint: 1102.0183.\nLeCun, Yann, Patrick Haﬀner, Léon Bottou, and Yoshua Bengio (1999). “Object Recognition with\nGradient-Based Learning”. In: Shape, Contour and Grouping in Computer Vision. Berlin, Heidelberg:\nSpringer Berlin Heidelberg, pp. 319–345. isbn: 978-3-540-46805-9. doi: 10.1007/3-540-46805-\n6_19. url: https://doi.org/10.1007/3-540-46805-6_19.\nLeCun, Yann A., Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller (2012). “Eﬃcient BackProp”.\nIn: Neural Networks: Tricks of the Trade: Second Edition. Berlin, Heidelberg: Springer Berlin\nHeidelberg, pp. 9–48. isbn: 978-3-642-35289-8. doi: 10.1007/978-3-642-35289-8_3. url:\nhttps://doi.org/10.1007/978-3-642-35289-8_3.\nLee, Sang-Woo, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang (2017). “Overcoming\nCatastrophic Forgetting by Incremental Moment Matching”. In: Nips, pp. 1–16. eprint: 1703.08475.\nurl: http://arxiv.org/abs/1703.08475.\nLesort, Timothée, Massimo Caccia, and Irina Rish (2021).\n“Understanding Continual Learning\nSettings with Data Distribution Drift Analysis”. In: pp. 1–9. arXiv: 2104.01678. url: http:\n//arxiv.org/abs/2104.01678.\npage 176\nBenedikt Pfülb\n\nListings\nList of References\nLesort, Timothée, Hugo Caselles-Dupré, Michael Garcia-Ortiz, Andrei Stoian, and David Filliat (2019).\n“Generative Models from the perspective of Continual Learning”. In: 2019 International Joint\nConference on Neural Networks (IJCNN), pp. 1–8.\nLesort, Timothée, Andrei Stoian, Jean François Goudou, and David Filliat (2019). “Training discrim-\ninative models to evaluate generative ones”. In: Lecture Notes in Computer Science (including\nsubseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics) 11729 LNCS,\npp. 604–619. issn: 16113349. doi: 10.1007/978-3-030-30508-6_48. arXiv: 1806.10840.\nLi, Der-Chiang, Susan C Hu, Liang-Sian Lin, and Chun-Wu Yeh (2017). “Detecting representative\ndata and generating synthetic samples to improve learning accuracy with imbalanced data sets”.\nIn: PloS one 12.8, e0181853.\nLi, Jonathan Q. and Andrew R. Barron (2000). “Mixture density estimation”. In: Advances in Neural\nInformation Processing Systems x, pp. 279–285. issn: 10495258.\nLi, Yanchun, Nanfeng Xiao, and Wanli Ouyang (2018). “Improved boundary equilibrium generative\nadversarial networks”. In: IEEE Access 6.c, pp. 11342–11348. issn: 21693536. doi: 10.1109/\nACCESS.2018.2804278.\nLi, Zhizhong and Derek Hoiem (2018). “Learning without Forgetting”. In: IEEE Transactions on\nPattern Analysis and Machine Intelligence 40.12, pp. 2935–2947. issn: 19393539. doi: 10.1109/\nTPAMI.2017.2773081. arXiv: arXiv:1606.09282v3.\nLopez-Paz, David and Marc’Aurelio Ranzato (2017). “Gradient Episodic Memory for Continual\nLearning”. In: Proceedings of the 31st International Conference on Neural Information Processing\nSystems. NIPS’17. Long Beach, California, USA: Curran Associates Inc., 6470–6479. isbn:\n9781510860964.\nMaaten, Laurens van der (2014). “Accelerating t-SNE using Tree-Based Algorithms”. In: Journal of\nMachine Learning Research.\nMaaten, Laurens van der and Geoﬀrey Hinton (2008). “Visualizing Data using t-SNE”. In: Journal of\nMachine Learning Research.\nMallya, Arun and Svetlana Lazebnik (2018). “PackNet: Adding Multiple Tasks to a Single Network by\nIterative Pruning”. In: Proceedings of the IEEE Computer Society Conference on Computer Vision\nand Pattern Recognition, pp. 7765–7773. issn: 10636919. doi: 10.1109/CVPR.2018.00810. arXiv:\n1711.05769.\nMaxMind (2019).\n“MaxMind: IP Geolocation and Online Fraud Prevention”.\nIn:\nurl: https:\n//dev.maxmind.com/geoip/geoip2/geolite2/.\nMayers, David F., Gene H. Golub, and Charles F. van Loan (1986). Matrix Computations. Vol. 47.\n175, p. 376. isbn: 9781421407944. doi: 10.2307/2008107.\nMcClelland, James L, Bruce L McNaughton, and Randall C O’Reilly (1995). “Why there are comple-\nmentary learning systems in the hippocampus and neocortex: insights from the successes and failures\nof connectionist models of learning and memory.” In: Psychological review 102.3, pp. 419–457. issn:\n0033-295X. doi: 10.1037/0033-295X.102.3.419. url: http://eutils.ncbi.nlm.nih.gov/\nentrez/eutils/elink.fcgi?dbfrom=pubmed&id=7624455&retmode=ref&cmd=prlinkshttp:\n//www.ncbi.nlm.nih.gov/pubmed/7624455.\nMcCloskey, Michael and Neal J. Cohen (1989). “Catastrophic Interference in Connectionist Networks:\nThe Sequential Learning Problem”. In: Psychology of Learning and Motivation - Advances in\nResearch and Theory 24.C, pp. 109–165. issn: 00797421. doi: 10.1016/S0079-7421(08)60536-8.\nMcCulloch, Warren S. and Walter Pitts (1943). “A logical calculus of the ideas immanent in nervous\nactivity”. In: The Bulletin of Mathematical Biophysics 5.4, pp. 115–133. issn: 0007-4985. doi:\n10.1007/BF02478259. url: http://link.springer.com/10.1007/BF02478259.\nMcElreath, Richard (2018). “Statistical rethinking: A bayesian course with examples in R and stan”.\nIn: Statistical Rethinking: A Bayesian Course with Examples in R and Stan, pp. 1–469. doi:\n10.1201/9781315372495.\nMcKeown, Nick, Tom Anderson, Hari Balakrishnan, Guru Parulkar, Larry Peterson, Jennifer Rexford,\nScott Shenker, and Jonathan Turner (2008). “OpenFlow: Enabling Innovation in Campus Networks”.\nIn: ACM SIGCOMM Computer Communication Review.\nMermillod, Martial, Aurélia Bugaiska, and Patrick BONIN (2013). “The stability-plasticity dilemma:\ninvestigating the continuum from catastrophic forgetting to age-limited learning eﬀects”.\nIn:\nFrontiers in Psychology 4, p. 504. issn: 1664-1078. url: https://www.frontiersin.org/\narticle/10.3389/fpsyg.2013.00504.\nMitchell, Tom M. (1997). Machine Learning. New York: McGraw-Hill. isbn: 978-0-07-042807-2.\nBenedikt Pfülb\npage 177\n\nList of References\nListings\nMohammed, A. R., S. A. Mohammed, and S. Shirmohammadi (2019). “Machine Learning and Deep\nLearning Based Traﬃc Classiﬁcation and Prediction in Software Deﬁned Networking”. In: 2019\nIEEE International Symposium on Measurements Networking.\nMori, Tatsuya, Masato Uchida, Ryoichi Kawahara, Jianping Pan, and Shigeki Goto (2004). “Identifying\nElephant Flows Through Periodically Sampled Packets”. In: 4th ACM SIGCOMM Conference on\nInternet Measurement.\nMureşan, Horea and Mihai Oltean (2018). “Fruit recognition from images using deep learning”. In:\nActa Universitatis Sapientiae, Informatica 10.1, pp. 26–42. issn: 2066-7760. doi: 10.2478/ausi-\n2018-0002. eprint: 1712.00580. url: http://arxiv.org/abs/1712.00580.\nNagy, David G and Gergo Orban (2016). “Episodic memory for continual model learning”. In: Nips,\npp. 1–5. arXiv: arXiv:1712.01169v1.\nNajafabadi, Maryam M, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald, and\nEdin Muharemagic (2015). “Deep learning applications and challenges in big data analytics”. In:\nJournal of big data 2.1, pp. 1–21.\nNesterov, Yu (2013). “Gradient methods for minimizing composite functions”. In: Mathematical\nProgramming 140.1, pp. 125–161. issn: 00255610. doi: 10.1007/s10107-012-0629-5.\nNetzer, Yuval and Tao Wang (2011). “Reading digits in natural images with unsupervised feature\nlearning”. In: Neural Information Processing Systems (NIPS), pp. 1–9.\nNguyen, Cuong V., Yingzhen Li, Thang D. Bui, and Richard E. Turner (2018). “Variational continual\nlearning”. In: 6th International Conference on Learning Representations, ICLR 2018 - Conference\nTrack Proceedings Vi, pp. 1–18. arXiv: 1710.10628.\nNguyen, Giang, Stefan Dlugolinsky, Martin Bobák, Viet Tran, Alvaro Lopez Garcia, Ignacio Heredia,\nPeter Malík, and Ladislav Hluch`y (2019). “Machine learning and deep learning frameworks and\nlibraries for large-scale data mining: a survey”. In: Artiﬁcial Intelligence Review 52.1, pp. 77–124.\nNguyen, Thuy TT and Grenville J Armitage (2008). “A Survey of Techniques for Internet Traﬃc\nClassiﬁcation using Machine Learning”. In: IEEE Communications Surveys.\nNielsen, Frank and Ke Sun (2016). “Guaranteed bounds on information-theoretic measures of univariate\nmixtures using piecewise log-sum-exp inequalities”. In: Entropy 18.12. issn: 10994300. doi:\n10.3390/e18120442.\nOrmoneit, Dirk and Volker Tresp (1998). “Averaging, maximum penalized likelihood and Bayesian\nestimation for improving Gaussian mixture probability density estimates”. In: IEEE Transactions\non Neural Networks 9.4, pp. 639–650. issn: 10459227. doi: 10.1109/72.701177.\nParisi, German I., Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter (2019).\n“Continual lifelong learning with neural networks: A review”. In: Neural Networks 113, pp. 54–71.\nissn: 18792782. doi: 10.1016/j.neunet.2019.01.012. arXiv: 1802.07569.\nPfülb, B., A. Gepperth, S. Abdullah, and A. Kilian (2018). “Catastrophic Forgetting: Still a Problem\nfor DNNs”. In: vol. 11139. Lecture Notes in Computer Science. Cham: Springer International\nPublishing, pp. 487–497. isbn: 978-3-030-01417-9. doi: 10.1007/978-3-030-01418-6_48. url:\nhttp://link.springer.com/10.1007/978-3-030-01418-6.\nPfülb, Benedikt and Alexander Gepperth (2019). “A Comprehensive, Application-Oriented Study of\nCatastrophic Forgetting in DNNs”. In: 7th International Conference on Learning Representations,\nICLR 2019 1.c, pp. 1–14. arXiv: 1905.08101.\nPfülb, Benedikt and Alexander Gepperth (2021). “Overcoming Catastrophic Forgetting with Gaussian\nMixture Replay”. In: International Joint Conference on Neural Networks (IJCNN), p. 9. eprint:\n2104.09220.\nPfülb, Benedikt, Alexander Gepperth, and Benedikt Bagus (2021). “Continual Learning with Fully\nProbabilistic Models”. In: 2nd CLVISION CVPR Workshop (Accepted as Findings), p. 10. eprint:\n2104.09240. url: http://arxiv.org/abs/2104.09240.\nPfülb, Benedikt, Christoph Hardegen, Alexander Gepperth, and Sebastian Rieger (2019). “A Study of\nDeep Learning for Network Traﬃc Data Forecasting”. In: Artiﬁcial Neural Networks and Machine\nLearning – ICANN 2019: Text and Time Series. Cham: Springer International Publishing, pp. 497–\n512. isbn: 978-3-030-30490-4.\nPinheiro, Jose C. and Douglas M. Bates (1995). “Approximations to the log-likelihood function in\nthe nonlinear mixed-eﬀects model”. In: Journal of Computational and Graphical Statistics 4.1,\npp. 12–35. issn: 15372715. doi: 10.1080/10618600.1995.10474663.\nPinkus, Allan (1999). “Approximation theory of the MLP model in neural networks”. In: Acta Numerica\n8, pp. 143–195. issn: 14740508. doi: 10.1017/S0962492900002919.\npage 178\nBenedikt Pfülb\n\nListings\nList of References\nPinto, Rafael Coimbra and Paulo Martins Engel (2015). “A Fast Incremental Gaussian Mixture Model”.\nIn: PLoS ONE 10.10. issn: 19326203. doi: 10.1371/journal.pone.0139931. eprint: 1506.04422.\nPoupart, Pascal, Zhitang Chen, Priyank Jaini, Fred Fung, Hengky Susanto, Yanhui Geng, Li Chen,\nKai Chen, and Hao Jin (2016). “Online Flow Size Prediction for Improved Network Routing”. In:\nIEEE 24th International Conference on Network Protocols (ICNP).\nQian, Ning (1999). “On the momentum term in gradient descent learning algorithms”. In: Neural\nNetworks 12.1, pp. 145–151. issn: 08936080. doi: 10.1016/S0893-6080(98)00116-6.\nRadford, Alec, Luke Metz, and Soumith Chintala (2016). “Unsupervised representation learning with\ndeep convolutional generative adversarial networks”. In: 4th International Conference on Learning\nRepresentations, ICLR 2016 - Conference Track Proceedings, pp. 1–16. arXiv: 1511.06434.\nRaj, Rajendra, Mihaela Sabin, John Impagliazzo, David Bowers, Mats Daniels, Felienne Hermans,\nNatalie Kiesler, Amruth N. Kumar, Bonnie MacKellar, Renée McCauley, Syed Waqar Nabi, and\nMichael Oudshoorn (2021). “Professional Competencies in Computing Education: Pedagogies and\nAssessment”. In: Proceedings of the 2021 Working Group Reports on Innovation and Technology in\nComputer Science Education. New York: ACM, 133–161. isbn: 9781450392020. doi: 10.1145/\n3502870.3506570.\nRasmussen, CE. and CKI. Williams (Jan. 2006). Gaussian Processes for Machine Learning. Adaptive\nComputation and Machine Learning. Cambridge, MA, USA: MIT Press, p. 248.\nRastegarfar, Houman, Madeleine Glick, Nicolaas Viljoen, Mingwei Yang, John Wissinger, Lloyd\nLaComb, and Nasser Peyghambarian (2016). “TCP Flow Classiﬁcation and Bandwidth Aggregation\nin Optically Interconnected Data Center Networks”. In: Journal of Optical Communications and\nNetworking 8.\nRatcliﬀ, Roger (1990). “Connectionist Models of Recognition Memory: Constraints Imposed by Learning\nand Forgetting Functions”. In: Psychological Review 97.2, pp. 285–308. issn: 0033295X. doi:\n10.1037/0033-295X.97.2.285.\nRebuﬃ, Sylvestre Alvise, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert (2017).\n“iCaRL: Incremental classiﬁer and representation learning”. In: Proceedings - 30th IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2017 2017-Janua, pp. 5533–5542. doi:\n10.1109/CVPR.2017.587. arXiv: arXiv:1611.07725v2.\nReis, J., M. Rocha, T. K. Phan, D. Griﬃn, F. Le, and M. Rio (2019). “Deep Neural Networks for\nNetwork Routing”. In: 2019 International Joint Conference on Neural Networks.\nRichardson, Eitan and Yair Weiss (2018). “On GANs and GMMs”. In: Advances in Neural Information\nProcessing Systems 2018-Decem.NeurIPS, pp. 5847–5858. issn: 10495258. eprint: 1805.12462.\nRiemer, Matthew, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald\nTesauro (2019). “Learning to learn without forgetting by maximizing transfer and minimizing\ninterference”. In: 7th International Conference on Learning Representations, ICLR 2019, pp. 1–31.\narXiv: 1810.11910.\nRusek, Krzysztof, José Suárez-Varela, Albert Mestres, Pere Barlet-Ros, and Albert Cabellos-Aparicio\n(2019). “Unveiling the Potential of Graph Neural Networks for Network Modeling and Optimization\nin SDN”. In: Proceedings of the 2019 ACM Symposium on SDN Research. ACM.\nRusu, Andrei A., Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray\nKavukcuoglu, Razvan Pascanu, and Raia Hadsell (2016). “Progressive Neural Networks”. In: arXiv:\n1606.04671. url: http://arxiv.org/abs/1606.04671.\nSchlag, Sebastian, Matthias Schmitt, and Christian Schulz (2019). “Faster support vector machines”. In:\nProceedings of the Workshop on Algorithm Engineering and Experiments January.340506, pp. 199–\n210. issn: 21640300. doi: 10.1137/1.9781611975499.16. arXiv: 1808.06394.\nScholkopf, Bernhard and Alexander J. Smola (2001). Learning with Kernels: Support Vector Machines,\nRegularization, Optimization, and Beyond. Cambridge, MA, USA: MIT Press. isbn: 0262194759.\nSchwarz, Jonathan, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye\nTeh, Razvan Pascanu, and Raia Hadsell (2018). “Progress & compress: A scalable framework\nfor continual learning”. In: 35th International Conference on Machine Learning, ICML 2018 10,\npp. 7199–7208. arXiv: 1805.06370.\nSerra, Joan, Dídac Suris, Marius Mirón, and Alexandras Karatzoglou (2018). In: 35th International\nConference on Machine Learning, ICML 2018 10, pp. 7225–7234. arXiv: 1801.01423.\nShadmehr, Reza and Sandro Mussa-Ivaldi (2012). Biological learning and control: how the brain builds\nrepresentations, predicts events, and makes decisions. Mit Press.\nSherif Abdelazeem, Ezzat El-Sherif (2010). The Arabic Handwritten Digits Databases ADBase &\nMADBase. url: http://datacenter.aucegypt.edu/shazeem/ (visited on 06/13/2018).\nBenedikt Pfülb\npage 179\n\nList of References\nListings\nShi, Hongtao, Hongping Li, Dan Zhang, Chaqiu Cheng, and Wei Wu (2017). “Eﬃcient and Robust\nFeature Extraction and Selection for Traﬃc Classiﬁcation”. In: Computer Networks 119.\nShin, Hanul, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim (2017). “Continual learning with deep\ngenerative replay”. In: Advances in Neural Information Processing Systems 2017-Decem.Nips,\npp. 2991–3000. issn: 10495258. eprint: 1705.08690.\nSolinas, M., S. Rousset, R. Cohendet, Y. Bourrier, M. Mainsant, A. Molnos, M. Reyboz, and M.\nMermillod (2021). “Beneﬁcial eﬀect of combined replay for continual learning”. In: ICAART 2021\n- Proceedings of the 13th International Conference on Agents and Artiﬁcial Intelligence 2.Icaart,\npp. 205–217. doi: 10.5220/0010251202050217.\nSong, Mingzhou and Hongbin Wang (2005). “Highly Eﬃcient Incremental Estimation of Gaussian\nMixture Models for Online Data Stream Clustering”.\nIn: Intelligent Computing: Theory and\nApplications III 5803, p. 174. doi: 10.1117/12.601724.\nSrivastava, Rupesh Kumar, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Jürgen\nSchmidhuber (2013). “Compete to Compute”. In: Nips 26, pp. 2310–2318. issn: 10495258. url:\nhttps://proceedings.neurips.cc/paper/2013/file/8f1d43620bc6bb580df6e80b0dc05c48-\nPaper.pdf.\nSutskever, Ilya, James Martens, George Dahl, and Geoﬀrey Hinton (2013). “On the importance of\ninitialization and momentum in deep learning”. In: ICASSP, IEEE International Conference on\nAcoustics, Speech and Signal Processing - Proceedings 2010, pp. 8609–8613. issn: 15206149. doi:\n10.1109/ICASSP.2013.6639346. eprint: arXiv:1301.3605v3.\nTang, Yichuan, Ruslan Salakhutdinov, and Geoﬀrey Hinton (2012). “Deep mixtures of factor analysers”.\nIn: Proceedings of the 29th International Conference on Machine Learning, ICML 2012 1, pp. 505–\n512. eprint: 1206.4635.\nThanh-Tung, Hoang and Truyen Tran (2018). “On Catastrophic Forgetting and Mode Collapse in\nGenerative Adversarial Networks”. In: arXiv: 1807.04015. url: http://arxiv.org/abs/1807.\n04015.\nThrun, Sebastian (1996a). Explanation-Based Neural Network Learning: A Lifelong Learning Approach.\nisbn: 9781461285977.\nThrun, Sebastian (1996b). “Is Learning The n-th Thing Any Easier Than Learning The First?”\nIn: Advances in Neural Information Processing Systems, p. 7.\nissn: 1049-5258.\nurl: http:\n//citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.2898.\nTieleman, T. and G. Hinton (2012). Lecture 6.5 - RmsProp: Divide the gradient by a running average\nof its recent magnitude. COURSERA: Neural Networks for Machine Learning.\nTipping, Michael E. and Christopher M. Bishop (1999). “Mixtures of Probabilistic Principal Com-\nponent Analyzers”. In: Neural Computation 11.2, pp. 443–482. issn: 0899-7667. doi: 10.1162/\n089976699300016728. url: https://direct.mit.edu/neco/article/11/2/443-482/6238.\nTitterington, D. M. (1984). “Recursive Parameter Estimation Using Incomplete Data”. In: Journal of\nthe Royal Statistical Society. Series B (Methodological) 46.2, pp. 257–267. issn: 00359246. url:\nhttp://www.jstor.org/stable/2345509.\nTrappenberg, Thomas (2009). Fundamentals of computational neuroscience. OUP Oxford.\nTsymbal, A (2004). “The Problem of Concept Drift: Deﬁnitions and Related Work”. In: Computer\nScience Department, Trinity College Dublin May.\nValadarsky, Asaf, Michael Schapira, Dafna Shahaf, and Aviv Tamar (2017). “Learning to Route”. In:\nProceedings of the 16th ACM Workshop on Hot Topics in Networks - HotNets-XVI, pp. 185–191. doi:\n10.1145/3152434.3152441. url: http://dl.acm.org/citation.cfm?doid=3152434.3152441.\nVan Den Oord, Aäron and Benjamin Schrauwen (2014). “Factoring variations in natural images with\ndeep Gaussian mixture models”. In: Advances in Neural Information Processing Systems 4.January,\npp. 3518–3526. issn: 10495258.\nVen, Gido M. van de and Andreas S. Tolias (2019). “Three scenarios for continual learning”. In:\npp. 1–18. arXiv: 1904.07734. url: http://arxiv.org/abs/1904.07734.\nVerbeek, J. J., N. Vlassis, and B. J.A. Kröse (2005). “Self-organizing mixture models”. In: Neurocom-\nputing 63.SPEC. ISS. Pp. 99–123. issn: 09252312. doi: 10.1016/j.neucom.2004.04.008.\nVijayakumar, Sethu, Aaron D’Souza, and Stefan Schaal (2005). “Incremental online learning in high\ndimensions”. In: Neural Computation 17.12, pp. 2602–2634. issn: 08997667. doi: 10.1162/\n089976605774320557.\nVinet, Luc and Alexei Zhedanov (2011). “A ’missing’ family of classical orthogonal polynomials”.\nIn: Journal of Physics A: Mathematical and Theoretical 44.8, pp. 1–60. issn: 17518113. doi:\n10.1088/1751-8113/44/8/085201. eprint: 1011.1669.\npage 180\nBenedikt Pfülb\n\nListings\nList of References\nViroli, Cinzia and Geoﬀrey J. McLachlan (2019). “Deep Gaussian mixture models”. In: Statistics\nand Computing 29.1, pp. 43–51. issn: 15731375. doi: 10.1007/s11222-017-9793-z. eprint:\n1711.06929.\nVlassis, Nikos and Aristidis Likas (2002). “A greedy EM algorithm for Gaussian mixture learning”. In:\nNeural Processing Letters 15.1, pp. 77–87. issn: 13704621. doi: 10.1023/A:1013844811137.\nWang, Heng and Zubin Abraham (2015). “Concept drift detection for streaming data”. In: 2015\ninternational joint conference on neural networks (IJCNN). IEEE, pp. 1–9.\nWang, Mowei, Yong Cui, Xin Wang, Shihan Xiao, and Junchen Jiang (2018). “Machine Learning for\nNetworking: Workﬂow, Advances and Opportunities”. In: IEEE Network.\nWang, Shenghui, Stefan Schlobach, and Michel Klein (2010). “What Is Concept Drift and How to\nMeasure It?” In: Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial\nIntelligence and Lecture Notes in Bioinformatics). Vol. 6317 LNAI. March 2014, pp. 241–256.\nisbn: 3642164374. doi: 10.1007/978-3-642-16438-5_17. url: http://link.springer.com/10.\n1007/978-3-642-16438-5_17.\nWang, Ting-Chun, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro\n(2018). “High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs”. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\nWani, M Arif, Farooq Ahmad Bhat, Saduf Afzal, and Asif Iqbal Khan (2020). Advances in deep\nlearning. Springer.\nWidmer, Gerhard and Miroslav Kubát (2004). “Learning in the presence of concept drift and hidden\ncontexts”. In: Machine Learning 23, pp. 69–101.\nXiao, Han, Kashif Rasul, and Roland Vollgraf (2017). “Fashion-MNIST: a Novel Image Dataset\nfor Benchmarking Machine Learning Algorithms”. In: pp. 1–6. eprint: 1708.07747. url: http:\n//arxiv.org/abs/1708.07747.\nXiao, Peng, Wenyu Qu, Heng Qi, Yujie Xu, and Zhiyang Li (2015). “An Eﬃcient Elephant Flow\nDetection with Cost-sensitive in SDN”. In: 1st International Conference on Industrial Networks\nand Intelligent Systems (INISCom).\nXie, Junyuan, Linli Xu, and Enhong Chen (2012). “Image denoising and inpainting with deep neural\nnetworks”. In: Advances in Neural Information Processing Systems 1, pp. 341–349. issn: 10495258.\nYao, H., T. Mai, C. Jiang, L. Kuang, and S. Guo (2019). “AI Routers & Network Mind: A Hybrid\nMachine Learning Paradigm for Packet Routing”. In: IEEE Computational Intelligence Magazine.\nYaroslav Bulatov (2011). Machine Learning, etc notMNIST dataset. url: http://yaroslavvb.\nblogspot.com.br/2011/09/notmnist-dataset.html (visited on 09/03/2018).\nYenamandra, Sriram, Ansh Khurana, Rohit Jena, and Suyash P. Awate (2021). “Learning Image\nInpainting from Incomplete Images using Self-Supervision”. In: pp. 10390–10397. doi: 10.1109/\nICPR48806.2021.9413049.\nZeiler, Matthew D. (2012). “ADADELTA: An Adaptive Learning Rate Method”. In: arXiv: 1212.5701.\nurl: http://arxiv.org/abs/1212.5701.\nZeiler, Matthew D. and Rob Fergus (2014). “Visualizing and understanding convolutional networks”.\nIn: Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence\nand Lecture Notes in Bioinformatics) 8689 LNCS.PART 1, pp. 818–833. issn: 16113349. doi:\n10.1007/978-3-319-10590-1_53. arXiv: 1311.2901.\nZenke, Friedemann, Ben Poole, and Surya Ganguli (2017). “Continual learning through synaptic\nintelligence”. In: 34th International Conference on Machine Learning, ICML 2017 8, pp. 6072–6082.\nissn: 2640-3498. arXiv: 1703.04200.\nZhang, Jiao, Tao Huang, Shuo Wang, and Yun-jie Liu (2019). “Future Internet: Trends and Challenges”.\nIn: Frontiers of Information Technology & Electronic Engineering.\nZhuang, Z., J. Wang, Q. Qi, H. Sun, and J. Liao (2019). “Toward Greater Intelligence in Route\nPlanning: A Graph-Aware Deep Learning Approach”. In: IEEE Systems Journal.\nZintgraf, Luisa M., Taco S. Cohen, Tameem Adel, and Max Welling (2017). “Visualizing Deep Neural\nNetwork Decisions: Prediction Diﬀerence Analysis”. In: 5th International Conference on Learning\nRepresentations, ICLR 2017 - Conference Track Proceedings, pp. 1–12. arXiv: 1702.04595.\nŽliobait˙e, Indr˙e (2010). “Learning under concept drift: an overview”. In: arXiv preprint arXiv:1010.4784.\nBenedikt Pfülb\npage 181\n\nList of References\nListings\npage 182\nBenedikt Pfülb\n\nB.\nStatutory Declaration\nThe submitted doctoral dissertation on the subject “Continual Learning with Deep Learning Methods\nin an Application-Oriented Context” is my own work and to the rules of proper scientiﬁc conduct. I did\nnot seek unauthorized assistance of a third party and I have employed no other sources or means except\nthe ones listed. I clearly marked any direct and indirect quotations derived from the works of others. I\ndid not yet present this doctoral dissertation or parts of it at any other higher education institution\nin Germany or abroad. I hereby conﬁrm the accuracy of the aﬃrmation above. I am aware of the\nsigniﬁcance of this aﬃrmation and the legal ramiﬁcations in case of untrue or incomplete statements. I\naﬃrm in lieu of oath that the statements above are to the best of my knowledge true and complete. I\nagree that for the purpose of assessing plagiarism the dissertation may be electronically forwarded,\nstored and processed.\nPlace, Date\nSignature\nBenedikt Pfülb\npage 183",
    "pdf_filename": "Continual Learning with Deep Learning Methods in an Application-Oriented Context.pdf"
}