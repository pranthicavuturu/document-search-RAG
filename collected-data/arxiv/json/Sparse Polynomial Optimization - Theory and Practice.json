{
    "title": "Sparse Polynomial Optimization - Theory and Practice",
    "context": "",
    "body": "Sparse Polynomial Optimization: Theory\nand Practice\nVictor Magron\nJie Wang\nAugust 26, 2022\narXiv:2208.11158v2  [math.OC]  25 Aug 2022\n\n\nContents\nList of Symbols\n9\n1\nSemideﬁnite programming and sparse matrices\n21\n1.1\nSDP and interior-point methods . . . . . . . . . . . . . . . . .\n21\n1.2\nChordal graphs and sparse matrices . . . . . . . . . . . . . .\n23\n1.3\nNotes and sources . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n2\nPolynomial optimization and the moment-SOS hierarchy\n31\n2.1\nSums of squares and quadratic modules . . . . . . . . . . . .\n33\n2.2\nBorel measures and moment matrices\n. . . . . . . . . . . . .\n34\n2.3\nThe moment-SOS hierarchy . . . . . . . . . . . . . . . . . . .\n36\n2.4\nMinimizer extraction . . . . . . . . . . . . . . . . . . . . . . .\n39\n2.5\nNotes and sources . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nI\nCorrelative sparsity\n45\n3\nThe moment-SOS hierarchy based on correlative sparsity\n47\n3.1\nCorrelative sparsity . . . . . . . . . . . . . . . . . . . . . . . .\n47\n3.2\nA sparse inﬁnite-dimensional LP formulation . . . . . . . . .\n49\n3.3\nThe CS-adpated moment-SOS hierarchy . . . . . . . . . . . .\n52\n3.4\nA variant with SOS of bounded degrees . . . . . . . . . . . .\n55\n3.5\nMinimizer extraction . . . . . . . . . . . . . . . . . . . . . . .\n56\n3.6\nFrom polynomial to rational functions . . . . . . . . . . . . .\n57\n3.7\nNotes and sources . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n4\nApplication in computer arithmetic\n65\n4.1\nPolynomial programs . . . . . . . . . . . . . . . . . . . . . . .\n66\n4.2\nUpper bounds on roundoff errors . . . . . . . . . . . . . . . .\n67\n4.3\nOverview of numerical experiments . . . . . . . . . . . . . .\n70\n4.4\nNotes and sources . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n\n4\nContents\n5\nApplication in deep networks\n75\n5.1\nMultiple layer networks . . . . . . . . . . . . . . . . . . . . .\n75\n5.2\nLipschitz constants . . . . . . . . . . . . . . . . . . . . . . . .\n77\n5.3\nNearly sparse problems\n. . . . . . . . . . . . . . . . . . . . .\n78\n5.4\nOverview of numerical experiments . . . . . . . . . . . . . .\n81\n5.4.1\nLipschitz Constant Estimation\n. . . . . . . . . . . . .\n82\n5.4.2\nRobustness certiﬁcation . . . . . . . . . . . . . . . . .\n83\n5.5\nNotes and sources . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n6\nNoncommutative optimization and quantum information\n91\n6.1\nNoncommutative polynomials\n. . . . . . . . . . . . . . . . .\n91\n6.2\nCorrelative sparsity patterns . . . . . . . . . . . . . . . . . . .\n93\n6.3\nNoncommutative moment and localizing matrices . . . . . .\n94\n6.4\nSparse representations . . . . . . . . . . . . . . . . . . . . . .\n95\n6.5\nSparse GNS construction . . . . . . . . . . . . . . . . . . . . .\n98\n6.6\nEigenvalue optimization . . . . . . . . . . . . . . . . . . . . . 100\n6.6.1\nUnconstrained eigenvalue optimization . . . . . . . . 100\n6.6.2\nConstrained eigenvalue optimization . . . . . . . . . 102\n6.6.3\nExtracting optimizers\n. . . . . . . . . . . . . . . . . . 104\n6.7\nOverview of numerical experiments . . . . . . . . . . . . . . 106\n6.7.1\nAn unconstrained problem . . . . . . . . . . . . . . . 106\n6.7.2\nBell inequalities . . . . . . . . . . . . . . . . . . . . . . 106\n6.8\nNotes and sources . . . . . . . . . . . . . . . . . . . . . . . . . 108\nII\nTerm sparsity\n115\n7\nThe moment-SOS hierarchy based on term sparsity\n117\n7.1\nThe TSSOS hierarchy for unconstrained POPs . . . . . . . . . 118\n7.2\nComparison with SDSOS . . . . . . . . . . . . . . . . . . . . . 122\n7.3\nThe TSSOS hierarchy for constrained POPs . . . . . . . . . . 122\n7.4\nObtaining a possibly smaller monomial basis . . . . . . . . . 126\n7.5\nSign symmetries and a sparse representation theorem . . . . 128\n7.6\nNumerical experiments . . . . . . . . . . . . . . . . . . . . . . 131\n7.6.1\nUnconstrained polynomial optimization\n. . . . . . . 131\n7.6.2\nConstrained polynomial optimization . . . . . . . . . 133\n7.7\nNotes and sources . . . . . . . . . . . . . . . . . . . . . . . . . 135\n8\nExploiting both correlative and term sparsity\n139\n8.1\nThe CS-TSSOS hierarchy . . . . . . . . . . . . . . . . . . . . . 139\n8.2\nGlobal convergence . . . . . . . . . . . . . . . . . . . . . . . . 145\n8.3\nExtracting a solution . . . . . . . . . . . . . . . . . . . . . . . 147\n8.4\nA minimal initial relaxation step\n. . . . . . . . . . . . . . . . 148\n8.5\nNumerical experiments . . . . . . . . . . . . . . . . . . . . . . 148\n8.5.1\nBenchmarks for constrained POPs . . . . . . . . . . . 149\n\nContents\n5\n8.5.2\nThe Max-Cut problem . . . . . . . . . . . . . . . . . . 149\n8.6\nNotes and sources . . . . . . . . . . . . . . . . . . . . . . . . . 151\n9\nApplication in optimal power ﬂow\n155\n9.1\nExtension to complex polynomial optimization . . . . . . . . 155\n9.2\nApplications to optimal power ﬂow\n. . . . . . . . . . . . . . 157\n9.3\nNotes and sources . . . . . . . . . . . . . . . . . . . . . . . . . 160\n10 Exploiting term sparsity in noncommutative optimization\n165\n10.1 Eigenvalue optimization with term sparsity . . . . . . . . . . 165\n10.2 Combining correlative and term sparsity\n. . . . . . . . . . . 168\n10.3 Numerical experiments . . . . . . . . . . . . . . . . . . . . . . 170\n10.4 Polynomial Bell inequalities . . . . . . . . . . . . . . . . . . . 171\n10.5 Notes and sources . . . . . . . . . . . . . . . . . . . . . . . . . 174\n11 Application in stability of control-systems\n177\n11.1 Approximating JSR via SOS relaxations\n. . . . . . . . . . . . 177\n11.2 The SparseJSR Algorithm\n. . . . . . . . . . . . . . . . . . . . 178\n11.3 Numerical Experiments\n. . . . . . . . . . . . . . . . . . . . . 180\n11.3.1 Randomly generated examples . . . . . . . . . . . . . 180\n11.3.2 Examples from control systems . . . . . . . . . . . . . 181\n11.4 Notes and sources . . . . . . . . . . . . . . . . . . . . . . . . . 183\n12 Miscellaneous\n187\n12.1 Nonnegative circuits and binomial squares . . . . . . . . . . 187\n12.2 First-order SDP solvers . . . . . . . . . . . . . . . . . . . . . . 192\n12.3 Notes and sources . . . . . . . . . . . . . . . . . . . . . . . . . 194\nIII\nAppendix: software libraries\n203\nA Programming with MATLAB\n205\nA.1 Sparse moment relaxations with GLOPTIPOLY\n. . . . . . . . 205\nA.2 Sparse SOS relaxations with Yalmip\n. . . . . . . . . . . . . . 207\nB\nProgramming with Julia\n211\nB.1\nTSSOS for polynomial optimization . . . . . . . . . . . . . . . 212\nB.2\nTSSOS for noncommutative optimization . . . . . . . . . . . . 215\nB.3\nTSSOS for dynamical systems\n. . . . . . . . . . . . . . . . . . 217\n\n\nList of Acronyms\nmoment-SOS moment-sums of squares . . . . . . . . . . . . . . . .\n11\nSDP semideﬁnite programming . . . . . . . . . . . . . . . . . . . . .\n12\nSOS sum of squares . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nLP linear programming . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nPSD positive semideﬁnite . . . . . . . . . . . . . . . . . . . . . . . .\n21\nLMI linear matrix inequality . . . . . . . . . . . . . . . . . . . . . . .\n21\nRIP running intersection property\n. . . . . . . . . . . . . . . . . . .\n24\nPOP polynomial optimization problem\n. . . . . . . . . . . . . . . .\n31\nGMP generalized moment problem\n. . . . . . . . . . . . . . . . . .\n32\nCS correlative sparsity . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\ncsp correlative sparsity pattern\n. . . . . . . . . . . . . . . . . . . . .\n47\nCSSOS CS-adpated moment-SOS . . . . . . . . . . . . . . . . . . . .\n53\nQCQP quadratically constrained quadratic program . . . . . . . . .\n85\nnc noncommutative . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\nSOHS sum of Hermitian squares . . . . . . . . . . . . . . . . . . . .\n92\nGNS Gelfand-Naimark-Segal . . . . . . . . . . . . . . . . . . . . . .\n95\nTS term sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n117\ntsp term sparsity pattern . . . . . . . . . . . . . . . . . . . . . . . . .\n117\nTSSOS TS-adpated moment-SOS . . . . . . . . . . . . . . . . . . . .\n120\nCS-TSSOS CS-TS adpated moment-SOS . . . . . . . . . . . . . . . .\n141\n\n8\nContents\nCPOP complex polynomial optimization problem . . . . . . . . . .\n155\nJSR joint spectral radius . . . . . . . . . . . . . . . . . . . . . . . . .\n177\nSONC sum of nonnegative circuits . . . . . . . . . . . . . . . . . . .\n187\nCTP constant trace property . . . . . . . . . . . . . . . . . . . . . . .\n192\n\nList of Symbols\nN\n{0, 1, 2, . . .}\nN∗\n{1, 2, . . .}\nQ\nthe ﬁeld of rational numbers\nR\nthe ﬁeld of real numbers\nC\nthe ﬁeld of complex numbers\n0\nthe zero vector\nx = (x1, . . . , xn)\na tuple of real variables\nsupp( f )\nthe support of the polynomial f\n[m]\n{1, 2, . . . , m}\n| · |\nthe cardinality of a set or 1-norm of a vector\nRn×m\nthe set of n × m real matrices\nSn\nthe set of real symmetric n × n matrices\nS+\nn\nthe set of n × n PSD matrices\n⟨A, B⟩\nthe trace of AB for A, B ∈Sn\nIn\nthe n × n identity matrix\nM ≽0\nM is a PSD matrix\nF, G, H\ngraphs\nG(V, E)\na graph with nodes V and edges E\nV(G) (resp. E(G))\nthe node (resp. edge) set of the graph G\nBG\nthe adjacency matrix of the graph G with unit diagonal\nG ⊆H\nG is a subgraph of H\nG′\na chordal extension of the graph G\nS(G)\nthe set of real symmetric matrices with sparsity pattern G\nΠG\nthe projection from S|V(G)| to the subspace S(G)\ng = {g1, . . . , gm}\na set of polynomials deﬁning the constraints\nR[x]\nthe ring of real n-variate polynomials\nR[x]2d\nthe set of real n-variate polynomials of degree at most 2d\nΣ[x]\nthe set of SOS polynomials\nΣ[x]d\nthe set of SOS polynomials of degree at most 2d\nM(g)\nthe quadratic module generated by g\nM(g)r\nthe r-truncated quadratic module generated by g\nX\na basic semialgebraic set\n\n10\nContents\nµ, ν\nmeasures\nNn\nr\n{α ∈Nn | ∑n\nj=1 αj ≤r}\ndj\nthe ceil of half degree of gj ∈g\nr\nrelaxation order\nrmin\nminimum relaxation order\ny\na moment sequence\nLy\nthe linear functional associated to y\nMr(y)\nthe r-th order moment matrix associated to y\nMr(gy)\nthe r-th order localizing matrix associated to y and g\nδa\nthe Dirac measure centered at a\np\nnumber of variable cliques\ns\nsparse order\nx = (x1, . . . , xn)\na tuple of noncommutating variables\nR⟨x⟩\nthe ring of real nc n-variate polynomials\nWr\nthe vector of nc monomials of degree at most r\nΣ⟨x⟩\nthe set of SOHS polynomials\nDg\nthe nc semialgebraic set associated to g\n\nPreface\nConsider the following list of problems arising from various distinct ﬁelds:\n• Design certiﬁable algorithms for robust geometric perception in the\npresence of a large amount of outliers;\n• Minimizing a sum of rational fractions to estimate the fundamental\nmatrix in epipolar geometry;\n• Computing the maximal roundoff error bound for the output of a\nnumerical program;\n• Certifying the robustness of a deep neural network;\n• Computing the maximum violation level of Bell inequalities;\n• Verifying the stability of a networked system or a control system un-\nder deadline constraints;\n• Approximate stability regions of differential systems, such as reach-\nable sets or positively invariant sets;\n• Finding a maximum cut in a graph;\n• Minimizing the generator fuel cost under alternative current power-\nﬂow constraints.\nAll these important applications related to computer vision, computer\narithmetic, deep learning, entanglement in quantum information, graph\ntheory and energy networks, can be successfully tackled within the frame-\nwork of polynomial optimization, an emerging ﬁeld with growing research\nefforts in the last two decades. One key advantage of these techniques is\ntheir ability to model a wide range of problems using optimization for-\nmulations. Polynomial optimization heavily relies on the moment-sums\nof squares (moment-SOS) approach proposed by Lasserre [Las01], which\nprovides certiﬁcates for positive polynomials. The problem of minimizing\n\n12\nContents\na polynomial over a set of polynomial (in)-equalities is an NP-hard non-\nconvex problem. It turns out that this problem can be cast as an inﬁnite-\ndimensional linear problem over a set of probability measures. Thanks\nto powerful results from real algebraic geometry [Put93], one can convert\nthis linear problem into a nested sequence of ﬁnite-dimensional convex\nproblems. At each step of the associated hierarchy, one needs to solve a\nﬁxed size semideﬁnite program (an optimization program with a linear\ncost and constraints over matrices with nonnegative eigenvalues), which\ncan be in turn solved with efﬁcient numerical tools. On the practical side\nhowever, there is no-free lunch and such optimization methods usually en-\ncompass severe scalability issues. The underlying reason is that for opti-\nmization problems involving polynomials in n variables of degree at most\n2d, the size of the matrices involved at step r ≥d of Lasserre’s hierarchy of\nsemideﬁnite programming (SDP) relaxations is proportional to (n+r\nr ). For-\ntunately, for many applications, including the ones formerly mentioned,\nwe can look at the problem in the eyes and exploit the inherent data structure\narising from the cost and constraints describing the problem, for instance\nsparsity or symmetries.\nThis book presents several research efforts to tackle this scientiﬁc\nchallenge with important computational implications, and provides\nthe development of alternative optimization schemes that scale well\nin terms of computational complexity, at least in some identiﬁed class\nof problems.\nThe presented algorithmic framework in this book mainly exploits the\nsparsity structure of the input data to solve large-scale polynomial opti-\nmization problems. For unconstrained problems involving a few terms, a\nﬁrst remedy consists of reducing the size of the relaxations by discarding\nthe terms which never appear in the support of the sum of squares (SOS)\ndecompositions. This technique, based on a result by Reznick [Rez78],\nconsists of computing the Newton polytope of the input polynomial (the\nconvex hull of the support of this polynomial) and selecting only mono-\nmials with supports lying in half of this polytope.\nWe present sparsity-exploiting hierarchies of relaxations, for either un-\nconstrained or constrained polynomial optimization problems. By con-\ntrast with the dense hierarchies, they provide faster approximation of the\nsolution in practice but also come with the same theoretical convergence\nguarantees. Our framework is not restricted to static polynomial optimiza-\ntion, and we expose hierarchies of approximations for values of interest\narising from the analysis of dynamical systems. We also present various\nextensions to problems involving noncommuting variables, e.g., matrices\n\nContents\n13\nof arbitrary size or quantum physic operators.\nAt this point, we would like to emphasize the existence of alterna-\ntives to the positivity certiﬁcates based on sparse SOS decompositions.\nInstead of computing SOS decompositions with SDP, one can compute\nother positivity certiﬁcates based on linear programming (LP) for Bern-\nstein decompositions or Krivine-Stengle certiﬁcates, geometric/second-\norder cone programming for nonnegative circuits and scaled diagonally\ndominant SOS, relative entropy programming for arithmetic-geometric-\nexponentials. This book also presents an overview of these various alter-\nnative decompositions.\nA second point to emphasize is that the concept of sparsity is inherent\nto many scientiﬁc ﬁelds, and we outline some similarities and differences\nwith the algorithmic framework presented in this book. In the context of\nmachine learning, statistics, or signal processing, exploiting sparsity boils\ndown to select variables or features, usually with ℓ1-norm regularization\n[BT09]. It is commonly employed to make the model or the prediction\nmore interpretable or less expensive to use. In other words, even if the\nunderlying problem does not admit sparse solutions, one still hopes to\nbe able to ﬁnd the best sparse approximation. A similar situation occurs\nin the context of dynamical systems with sparse state constraints and dy-\nnamics, where the set of trajectories is not necessarily sparse. In the context\nof algebraic geometry, people have considered sparse systems of polyno-\nmial equations, where sparse means that the set of terms appearing in each\nequation is ﬁxed. Bernshtein’s theorem [Ber75] is a key ingredient as it pro-\nvides an accurate bound for the expected number of complex roots, based\non the mixed volume of the Newton polytopes of polynomials describing\nthe system. We similarly exploit support information given by Newton\npolytopes for our term-sparsity based hierarchies, presented in Part II.\nThis book is organized as follows:\nChapter 1 recalls some preliminary background on semideﬁnite program-\nming, sparse matrix theory.\nChapter 2 outlines the basic concepts of the moment-SOS hierarchy in\npolynomial optimization.\nPart I\nThe ﬁrst part of the book focuses on the notion of \"correlative spar-\nsity\", occurring when there are few correlations between the variables of\nthe input problem. This research investigation was initially developed by\n[WKKM06] and [Las06].\n\n14\nContents\nChapter 3 is concerned with this ﬁrst sparse variant of the moment-SOS\nhierarchy, based on correlative sparsity.\nChapter 4 explains how to apply the sparse moment-SOS hierarchy to pro-\nvide efﬁciently upper bounds on roundoff errors of ﬂoating-point nonlin-\near programs.\nChapter 5 focuses on robustness certiﬁcation of deep neural networks, in\nparticular via Lipschitz constant estimation.\nChapter 6 describes a very distinct application for optimization of poly-\nnomials in noncommuting variables. We outline promising research per-\nspectives in quantum information theory.\nPart II\nThe second part of the book presents a complementary framework,\nwhere we show how to exploit a distinct notion of sparsity, called \"term\nsparsity\", occurring when there are a small number of terms involved in\nthe input problem by comparison with the fully dense case.\nChapter 7 focuses on this second sparse variant of the moment-SOS hier-\narchy, based on term sparsity.\nChapter 8 explains how to combine correlative and term sparsity.\nChapter 9 extends this term sparsity framework to complex polynomial\noptimization and shows how the resulting scheme can handle optimal\npower ﬂow problems with tens of thousands of variables and constraints.\nChapter 10 extends the framework of exploiting term sparsity to noncom-\nmutative polynomial optimization (namely, eigenvalue optimization).\nChapter 11 is concerned with the application of this term sparsity frame-\nwork to analyze the stability of various control systems, either coming\nfrom the networked systems literature or systems under deadline con-\nstraints.\nChapter 12 presents alternative algorithms to improve the scalability of\npolynomial optimization methods. First, we present algorithms based on\nsums of nonnegative circuit polynomials, recently introduced classes of\nnonnegativity certiﬁcates for sparse polynomials, which are independent\nof well-known methods based on sums of squares. Then, we outline exist-\ning methods to speed-up the computation of the semideﬁnite relaxations.\n\nContents\n15\nAppendix\nAt the end of the book, we describe how to use various solvers avail-\nable either in MATLAB or Julia. This dedicated appendix aims at guiding\npractitioners to solve optimization problems involving sparse polynomi-\nals.\nAppendix A explains how to implement moment-SOS relaxations with\nsoftware packages GLOPTIPOLY and Yalmip.\nAppendix B focuses on our sparsity exploiting algorithms, implemented\nin the TSSOS library available at https://github.com/wangjie212/TSSOS.\n• For the sake of conciseness and clarity of exposition, most proofs are\npostponed to ease the reading. When the proof is either short or sim-\nple, we sometimes include it right after its corresponding statement.\nOtherwise, we refer to this proof in the Notes and sources section at\nthe end of the corresponding chapter.\n• Some of the theorems are framed in the book, in order to emphasize\ntheir speciﬁc importance.\n\n\nBibliography\n[Ber75]\nDavid N Bernshtein.\nThe number of roots of a system of\nequations.\nFunctional Analysis and its applications, 9(3):183–\n185, 1975.\n[BT09]\nAmir Beck and Marc Teboulle.\nA fast iterative shrinkage-\nthresholding algorithm for linear inverse problems.\nSIAM\njournal on imaging sciences, 2(1):183–202, 2009.\n[Las01]\nJean-Bernard Lasserre. Global Optimization with Polynomi-\nals and the Problem of Moments. SIAM Journal on Optimiza-\ntion, 11(3):796–817, 2001.\n[Las06]\nJean B Lasserre. Convergent sdp-relaxations in polynomial\noptimization with sparsity.\nSIAM Journal on Optimization,\n17(3):822–843, 2006.\n[Put93]\nM. Putinar. Positive polynomials on compact semi-algebraic\nsets.\nIndiana University Mathematics Journal, 42(3):969–984,\n1993.\n[Rez78]\nB. Reznick. Extremal PSD forms with few terms. Duke Math-\nematical Journal, 45(2):363–374, 1978.\n[WKKM06] Hayato Waki,\nSunyoung Kim,\nMasakazu Kojima,\nand\nMasakazu Muramatsu.\nSums of squares and semideﬁnite\nprogram relaxations for polynomial optimization problems\nwith structured sparsity.\nSIAM Journal on Optimization,\n17(1):218–242, 2006.\n\n\nPreliminary background\n\n\nChapter 1\nSemideﬁnite programming\nand sparse matrices\nIn this chapter and the next one, we describe the foundations on which\nseveral parts of our work lie. Semideﬁnite programming and sparse ma-\ntrices are described in this chapter while Chapter 2 is dedicated to the\nmoment-SOS hierarchy of SDP relaxations, now widely used to certify\nlower bounds of polynomial optimization problems.\n1.1\nSDP and interior-point methods\nEven though SDP is not our main topic of interest, several encountered\nproblems can be cast as such programs.\nFirst, we introduce some useful notations. We consider the vector space\nSn of real symmetric n × n matrices, which is equipped with the usual\ninner product ⟨A, B⟩= tr(AB) for A, B ∈Sn. Let In be the n × n identity\nmatrix. A matrix M ∈Sn is called positive semideﬁnite (PSD) (resp. positive\ndeﬁnite) if x⊺Mx ≥0 (resp. > 0), for all x ∈Rn. In this case, we write\nM ≽0 and deﬁne a partial order by writing A ≽B (resp. A ≻B) if and\nonly if A −B is positive semideﬁnite (resp. positive deﬁnite). The set of\nn × n PSD matrices is denoted by S+\nn .\nIn semideﬁnite programming, one minimizes a linear objective func-\ntion subject to a linear matrix inequality (LMI). The variable of the prob-\nlem is the vector y ∈Rm and the input data of the problem are the vector\nc ∈Rm and symmetric matrices F0, . . . , Fm ∈Sn. The primal semideﬁnite\n\n22\nChapter 1. Semideﬁnite programming and sparse matrices\nprogram is deﬁned as follows:\npsdp := inf\ny∈Rm\nc⊺y\ns.t.\nF(y) ⪰0\n(1.1)\nwhere\nF(y) := F0 +\nm\n∑\ni=1\nyiFi.\nThe primal problem (1.1) is convex since the linear objective function\nand the linear matrix inequality constraint are both convex. We say that\ny is primal feasible (resp. strictly feasible) if F(y) ⪰0 (resp. F(y) ≻0).\nFurthermore, we associate the following dual problem with the primal\nproblem (1.1):\ndsdp := sup\nG∈Sn\n−⟨F0, G⟩\ns.t.\n⟨Fi, G⟩= ci,\ni ∈[m]\nG ⪰0\n(1.2)\nThe variable of the dual program (1.2) is the real symmetric matrix\nG ∈Sn. We say that G is dual feasible (resp. strictly feasible) if ⟨Fi, G⟩= ci,\ni ∈[m] and G ⪰0 (resp. G ≻0).\nWe will describe brieﬂy the primal-dual interior-point method (used\nfor instance by SDPA [YFN+10], MOSEK [ART03]), that solves the follow-\ning primal-dual optimization problem:\n\n\n\n\n\n\n\ninf\ny∈Rm,G∈Sn\nη(y, G)\ns.t.\n⟨Fi, G⟩= ci,\ni ∈[m]\nF(y) ⪰0, G ⪰0\n(1.3)\nwhere η(y, G) := c⊺y + ⟨F0, G⟩.\nWe notice that the objective function η of the program (1.3) is the dif-\nference between the objective function of the primal program (1.1) and its\ndual version (1.2). We call this function the duality gap. Let us suppose\nthat y is primal feasible and G is dual feasible, then η is nonnegative. In-\ndeed, we have\nη(y, G) =\nm\n∑\ni=1\n⟨Fi, G⟩yi + ⟨F0, G⟩= ⟨F(y), G⟩≥0.\n(1.4)\nThe last inequality comes from the fact that the matrices F(y) and G are\nboth PSD.\n\n1.2. Chordal graphs and sparse matrices\n23\nThen, one can easily prove that the nonnegativity of η implies the fol-\nlowing inequalities:\ndsdp ≤−⟨F0, G⟩≤c⊺y ≤psdp.\n(1.5)\nOur problems that can be cast as SDPs satisfy certain assumptions, so\nthat there exists a (strictly feasible) primal-dual optimal solution (i.e., a\nprimal strictly feasible y solving (1.1) and a dual strictly feasible G solv-\ning (1.2)). Then, all inequalities in (1.5) become equalities and there is no\nduality gap (η(y, G) = 0):\ndsdp = −⟨F0, G⟩= c⊺y = psdp.\n(1.6)\nThus, we will assume that such a primal-dual optimal solution exists\nin the sequel. We also introduce the barrier function\nΦ(y) :=\n\u001a\nlog det(F(y)−1)\nif F(y) ≻0,\n+∞\notherwise.\n(1.7)\nThis barrier function exhibits several nice properties: Φ is strictly con-\nvex, analytic and self-concordant. The unique minimizer yopt of Φ is called\nthe analytic center of the LMI F(y) ⪰0. This self-concordant barrier func-\ntion guarantees that the number of iterations of the interior-point method\nis bounded by a polynomial in the dimension (n and m) and the number\nof accuracy digits of the solution.\n1.2\nChordal graphs and sparse matrices\nWe brieﬂy recall some basic notions from graph theory. An (undirected)\ngraph G(V, E) or simply G consists of a set of nodes V and a set of edges\nE ⊆{{vi, vj} | vi ̸= vj, (vi, vj) ∈V × V}. For a graph G, we use V(G)\nand E(G) to indicate the node set of G and the edge set of G, respectively.\nThe adjacency matrix of a graph G is denoted by BG for which we put ones\non its diagonal. For two graphs G, H, we say that G is a subgraph of H if\nV(G) ⊆V(H) and E(G) ⊆E(H), denoted by G ⊆H. For a graph G(V, E),\na cycle of length k is a set of nodes {v1, v2, . . . , vk} ⊆V with {vk, v1} ∈E\nand {vi, vi+1} ∈E, for i ∈[k −1]. A chord in a cycle {v1, v2, . . . , vk} is\nan edge {vi, vj} that joins two nonconsecutive nodes in the cycle. A clique\nC ⊆V of G is a subset of nodes where {vi, vj} ∈E for any vi, vj ∈C. If a\nclique is not a subset of any other clique, then it is called a maximal clique.\nDeﬁnition 1.1 (chordal graph) A graph is called a chordal graph if all its\ncycles of length at least four have a chord.\nThe notion of chordal graphs plays an important role in sparse matrix the-\nory. In particular, it is known that maximal cliques of a chordal graph can\n\n24\nChapter 1. Semideﬁnite programming and sparse matrices\nbe enumerated efﬁciently in linear time in the number of nodes and edges\nof the graph. See e.g. [Gav72, VA+15a] for the details.\nThe maximal cliques I1, . . . , Ip of a chordal graph (possibly after some\nreordering) satisfy the so-called running intersection property (RIP), i.e., for\nevery k ∈[p −1], it holds\n\nIk+1 ∩\n[\nj≤k\nIj\n\n⊆Ii\nfor some i ≤k.\n(1.8)\nThe RIP actually gives an equivalent characterization of chordal graphs.\nTheorem 1.2 A connected graph is chordal if and only if its maximal cliques\nafter an appropriate ordering satisfy the RIP.\nAny non-chordal graph G(V, E) can always be extended to a chordal\ngraph G′(V, E′) by adding appropriate edges to E, which is called a chordal\nextension of G(V, E). The chordal extension of G is usually not unique.\nWe use the symbol G′ to indicate a speciﬁc chordal extension of G. For\ngraphs G ⊆H, we assume that G′ ⊆H′ always holds for our purpose.\nFor a graph G, among all chordal extensions of G, there is a particular\none G′ which makes every connected component of G to be a clique. Ac-\ncordingly, a matrix with adjacency graph G′ is block diagonal (after an\nappropriate permutation on rows and columns): each block corresponds\nto a connected component of G. We call this chordal extension the maxi-\nmal chordal extension. Besides, we are also interested in smallest chordal\nextensions. By deﬁnition, a smallest chordal extension is a chordal exten-\nsion with the smallest clique number (i.e., the maximal size of maximal\ncliques). However, computing a smallest chordal extension is generally\nNP-complete [ACP87]. Therefore in practice we compute approximately\nsmallest chordal extensions instead with efﬁcient heuristic algorithms; see\n[BK10] for more detailed discussions.\nExample 1.3 Let us consider the graph G(V, E) represented in Figure 1.1, with\nthe set of nodes V = {1, 2, 3, 4, 5, 6} and\nE = {{1, 2}, {1, 3}, {1, 4}, {1, 5}, {1, 6}, {2, 3}, {2, 5}, {3, 6}, {5, 6}}.\nand the corresponding adjacency matrix\nBG =\n\n\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n0\n1\n1\n1\n0\n1\n1\n1\n0\n0\n1\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\n1\n0\n1\n1\n\n\n.\n\n1.2. Chordal graphs and sparse matrices\n25\n6\n4\n5\n1\n2\n3\nFigure 1.1: The graph from Example 1.3.\nOne example of cycle of length 3 is {1, 5, 6} and one example of cycle of length 4\nis {6, 3, 2, 5}. Note that this graph is not chordal since there is no chord in this\nlatter cycle. It is enough to add en edge between the nodes 2 and 6 (or alternatively\nbetween the nodes 3 and 5) to obtain a chordal extension of G.\nLet n ∈N∗. Given a graph G(V, E) with V = [n], a symmetric matrix\nQ with rows and columns indexed by V is said to have sparsity pattern\nG if Qij = Qij = 0 whenever i ̸= j and {i, j} /∈E. Let S(G) be the set of\nreal symmetric matrices with sparsity pattern G. The PSD matrices with\nsparsity pattern G form a convex cone\nS+\n|V| ∩S(G) = {Q ∈S(G) | Q ⪰0}.\n(1.9)\nA matrix in S(G) exhibits a block structure: each block corresponds to\na maximal clique of G. Figure 1.2 depicts an instance of such block struc-\ntures. Note that there might be overlaps between blocks because different\nmaximal cliques may share nodes.\nFigure 1.2: An instance of block structures for matrices in S(G). The blue\narea indicates the positions of possible nonzero entries.\nGiven a maximal clique C of G(V, E), we deﬁne a matrix RC ∈R|C|×|V|\n\n26\nChapter 1. Semideﬁnite programming and sparse matrices\nby\n[RC]ij =\n(\n1,\nif C(i) = j,\n0,\notherwise,\n(1.10)\nwhere C(i) denotes the i-th node in C, sorted with respect to the ordering\ncompatible with V. Note that QC = RCQR⊺\nC ∈S|C| extracts a principal\nsubmatrix QC indexed by the clique C from a symmetry matrix Q, and\nQ = R⊺\nCQCRC inﬂates a |C| × |C| matrix QC into a sparse |V| × |V| matrix\nQ.\nWhen the sparsity pattern graph G is chordal, the cone S+\n|V| ∩S(G) can\nbe decomposed as a sum of simple convex cones, as stated in the following\ntheorem.\nTheorem 1.4 Let G(V, E) be a chordal graph and assume that C1, . . . , Cp\nare the list of maximal cliques of G. Then a matrix Q ∈S+\n|V| ∩S(G) if and\nonly if there exist Qk ∈S+\n|Ck| for k ∈[p] such that Q = ∑\np\nk=1 R⊺\nCkQkRCk.\nGiven a graph G(V, E) with V = [n], let ΠG be the projection from S|V|\nto the subspace S(G), i.e., for Q ∈S|V|,\n[ΠG(Q)]ij =\n(\nQij,\nif i = j or {i, j} ∈E,\n0,\notherwise.\n(1.11)\nWe denote by ΠG(S+\n|V|) the set of matrices in S(G) that have a PSD com-\npletion, i.e.,\nΠG(S+\n|V|) =\nn\nΠG(Q) | Q ∈S+\n|V|\no\n.\n(1.12)\nOne can easily check that the PSD completable cone ΠG(S+\n|V|) and the PSD\ncone S+\n|V| ∩S(G) form a pair of dual cones in S(G). Moreover, for a chordal\ngraph G, the decomposition result for the cone S+\n|V| ∩S(G) in Theorem\n1.4 leads to the following characterization of the PSD completable cone\nΠG(S+\n|V|).\nTheorem 1.5 Let G(V, E) be a chordal graph and assume that C1, . . . , Cp\nare the list of maximal cliques of G. Then a matrix Q ∈ΠG(S+\n|V|) if and\nonly if Qk = RCkQR⊺\nCk ⪰0 for all k ∈[p].\n\n1.3. Notes and sources\n27\nTheorem 1.4 and Theorem 1.5 play an important role in sparse semidef-\ninite programming since they admit us to decompose an SDP with chordal\nsparsity pattern into an SDP of smaller size, which yields signiﬁcant com-\nputational improvement if the sizes of related maximal cliques are small.\n1.3\nNotes and sources\nSDP is relevant to a wide range of applications. The interested reader can\nﬁnd more details on the connection between SDP and combinatorial opti-\nmization in [GLV09], control theory in [BEGFB94], positive semideﬁnite\nmatrix completion in [Lau09]. A survey on semideﬁnite programming\nis available in the paper of Vandenberghe and Boyd [VB96]. We empha-\nsize the fact that SDPs can be solved efﬁciently by software e.g., SeDuMi\n[Stu99], CSDP [Bor99], SDPA [YFN+10], MOSEK [ART03].\nWe refer to [NN94] for more details on barrier functions. Detailed com-\nplexity bounds related to SDP solving with interior-point methods can\nbe found in Section 4.6.3 from [BTN01]. With prescribed accuracy, the\ntime complexity of SDP (in terms of arithmetic operations) is polynomial\nwith resepct to the number of variables with an exponent greater than 3;\nsee [BTN01, Chapter 4] for more details.\nFor more details about sparse matrices and chordal graphs, the reader\nis referred to the survey [VA+15b].\nTheorem 1.4 and Theorem 1.5 are\nstated as Theorem 9.2 and Theorem 10.1 in [VA+15b], respectively, and\nwere derived much earlier in [AHMR88] and [GJSW84], respectively.\nThe equivalence stated in Theorem 1.2 could be read from Theorem 3.4\nor Corollary 1 of [BP93].\n\n\nBibliography\n[ACP87]\nStefan Arnborg, Derek G Corneil, and Andrzej Proskurowski.\nComplexity of ﬁnding embeddings in a k-tree. SIAM Journal\non Algebraic Discrete Methods, 8(2):277–284, 1987.\n[AHMR88] Jim Agler, William Helton, Scott McCullough, and Leiba Rod-\nman. Positive semideﬁnite matrices with a given sparsity pat-\ntern. Linear algebra and its applications, 107:101–149, 1988.\n[ART03]\nErling D Andersen, Cornelis Roos, and Tamas Terlaky.\nOn\nimplementing a primal-dual interior-point method for conic\nquadratic optimization. Mathematical Programming, 95(2):249–\n277, 2003.\n[BEGFB94] Stephen Boyd, Laurent El Ghaoui, Eric Feron, and Venkatara-\nmanan Balakrishnan. Linear matrix inequalities in system and\ncontrol theory. SIAM, 1994.\n[BK10]\nHans L Bodlaender and Arie MCA Koster. Treewidth com-\nputations i. upper bounds.\nInformation and Computation,\n208(3):259–275, 2010.\n[Bor99]\nBrian Borchers.\nCsdp, ac library for semideﬁnite program-\nming. Optimization methods and Software, 11(1-4):613–623, 1999.\n[BP93]\nJean RS Blair and Barry Peyton. An introduction to chordal\ngraphs and clique trees. In Graph theory and sparse matrix com-\nputation, pages 1–29. Springer, 1993.\n[BTN01]\nAharon Ben-Tal and Arkadi Nemirovski. Lectures on modern\nconvex optimization: analysis, algorithms, and engineering applica-\ntions. SIAM, 2001.\n[Gav72]\nF˘anic˘a Gavril. Algorithms for minimum coloring, maximum\nclique, minimum covering by cliques, and maximum inde-\npendent set of a chordal graph. SIAM Journal on Computing,\n1(2):180–187, 1972.\n\n30\nBibliography\n[GJSW84]\nRobert Grone, Charles R Johnson, Eduardo M Sá, and Henry\nWolkowicz. Positive deﬁnite completions of partial hermitian\nmatrices. Linear algebra and its applications, 58:109–124, 1984.\n[GLV09]\nNebojša Gvozdenovi´c, Monique Laurent, and Frank Vallentin.\nBlock-diagonal semideﬁnite programming hierarchies for 0/1\nprogramming. Operations Research Letters, 37(1):27–31, 2009.\n[Lau09]\nMonique Laurent. Matrix completion problems. Encyclopedia\nof Optimization, 3:221–229, 2009.\n[NN94]\nYurii Nesterov and Arkadii Nemirovskii. Interior-point polyno-\nmial algorithms in convex programming. SIAM, 1994.\n[Stu99]\nJos F Sturm. Using sedumi 1.02, a matlab toolbox for optimiza-\ntion over symmetric cones. Optimization methods and software,\n11(1-4):625–653, 1999.\n[VA+15a]\nLieven Vandenberghe, Martin S Andersen, et al.\nChordal\ngraphs and semideﬁnite optimization.\nFoundations and\nTrends® in Optimization, 1(4):241–433, 2015.\n[VA+15b]\nLieven Vandenberghe, Martin S Andersen, et al.\nChordal\ngraphs and semideﬁnite optimization.\nFoundations and\nTrends® in Optimization, 1(4):241–433, 2015.\n[VB96]\nLieven Vandenberghe and Stephen Boyd. Semideﬁnite pro-\ngramming. SIAM review, 38(1):49–95, 1996.\n[YFN+10]\nM. Yamashita, K. Fujisawa, K. Nakata, M. Nakata, M. Fukuda,\nK. Kobayashi, and K. Goto.\nA high-performance software\npackage for semideﬁnite programs : SDPA7. Technical report,\nDept. of Information Sciences, Tokyo Inst. Tech., 2010.\n\nChapter 2\nPolynomial optimization\nand the moment-SOS\nhierarchy\nPolynomial optimization focuses on minimizing or maximizing a polyno-\nmial under a set of polynomial inequality constraints. A polynomial is\nan expression involving addition, subtraction and multiplication of vari-\nables and coefﬁcients. An example of polynomial in two variables x1 and\nx2 with rational coefﬁcients is f (x1, x2) = 1/3 + x2\n1 + 2x1x2 + x2\n2. Semial-\ngebraic sets are deﬁned with conjunctions and disjunctions of polynomial\ninequalities with real coefﬁcients. For instance the two-dimensional unit\ndisk is a semialgebraic set deﬁned as the set of all points (x1, x2) satisfying\nthe (single) inequality 1 −x2\n1 −x2\n2 ≥0.\nIn general, computing the exact solution of a polynomial optimiza-\ntion problem (POP) over a semialgebraic set is an NP-hard problem. In\npractice, one can at least try to compute an approximation of the solution\nby considering a relaxation of the problem instead of the problem itself.\nThe approximated solution may not satisfy all the problem constraints but\nstill gives useful information about the exact solution. Let us illustrate\nthis by considering the minimization of the above polynomial f (x1, x2)\non the unit disk. One can replace this disk by a larger set, for instance\nthe product of intervals [−1, 1] × [−1, 1]. Using basic interval arithmetic,\none easily shows that the range of f belongs to [−4/3, 4/3]. Next, one\ncan replace the monomials x2\n1, x1x2 and x2\n2 by three new variables y1,\ny2 and y3, respectively. One can relax the initial problem by LP, with\na cost of 1/3 + y1 + 2y2 + y3 and one single linear inequality constraint\n1 −y1 −y3 ≥0. By hand-solving or by using an LP solver, one ﬁnds again\na lower bound of −4/3. Even if LP gives more accurate bounds than in-\n\n32\nChapter 2. Polynomial optimization and the moment-SOS hierarchy\nterval arithmetic in general, this does not yield any improvement on this\nexample.\nOne way to obtain more accurate lower bounds is to rely on more so-\nphisticated techniques from the ﬁeld of convex optimization, e.g., SDP.\nIn the seminal paper [Las01] published in 2001, Lasserre introduced a hi-\nerarchy of relaxations allowing to obtain a converging sequence of lower\nbounds for the minimum of a polynomial over a semialgebraic set. Each\nlower bound is computed by SDP.\nThe idea behind Lasserre’s hierarchy is to tackle the inﬁnite-dimensional\ninitial problem by solving several ﬁnite-dimensional primal-dual SDP prob-\nlems. The primal is a moment problem, that is an optimization problem\nwhere variables are the moments of a Borel measure. The ﬁrst moment is\nrelated to means, the second moment is related to variances, etc. Lasserre\nshowed in [Las01] that POP can be cast as a particular instance of the\ngeneralized moment problem (GMP). In a nutshell, the primal moment\nproblem approximates Borel measures. The dual is an SOS problem, where\nthe variables are the coefﬁcients of SOS polynomials (e.g., (1/\n√\n3)2 + (x1 +\nx2)2). It is known that not all positive polynomials can be written with\nSOS decompositions. However, when the set of constraints satisﬁes certain\nassumptions (slightly stronger than compactness) then one can represent\npositive polynomials with weighted SOS decompositions. In a nutshell,\nthe dual SOS problem approximates positive polynomials. The moment-SOS\napproach can be used on the above example with either three moment\nvariables or SOS of degree 2 to obtain a lower bound of 1/3. For this ex-\nample, the exact solution is obtained at the ﬁrst step of the hierarchy. There\nis no need to go further, i.e., to consider primal with moments of greater\norder (e.g. the integrals of x3\n1, x2\n1x2, x4\n1) or dual with SOS polynomials of\ndegree 4 or 6. The reason is that for convex quadratic problems, the ﬁrst\nstep of the hierarchy gives the exact solution!\nIn the sequel, we recall more formally some preliminary background\nmaterial on the mathematical tools related to the moment-SOS hierarchy.\nGiven n, d ∈N, let R[x] (resp. R[x]2d) stand for the vector space of\nreal n-variate polynomials (resp. of degree at most 2d) in the variable\nx = (x1, . . . , xn) ∈Rn. A polynomial f ∈R[x] can be written as f (x) =\n∑α∈A fαxα with A ⊆Nn and fα ∈R, xα = xα1\n1 · · · xαn\nn . The support of f is\ndeﬁned by supp( f ) := {α ∈A | fα ̸= 0}. A basic compact semialgebraic\nset X is a ﬁnite conjunction of polynomial superlevel sets. Namely, given\nm ∈N∗and a set of polynomials g := {g1, . . . , gm} ⊂R[x], one has\nX :={x ∈Rn : g(x) ≥0 for all g ∈g}\n={x ∈Rn : g1(x) ≥0, . . . , gm(x) ≥0}.\n(2.1)\nMany sets can be described as such basic closed semialgebraic sets, and the\nrelated description is not unique. Consider for instance the 2-dimensional\n\n2.1. Sums of squares and quadratic modules\n33\nhypercube X = [0, 1]2. A ﬁrst possible description is given by\nX = {(x1, x2) ∈R2 : x1 −x2\n1 ≥0, x2 −x2\n2 ≥0}\n(2.2)\n= {(x1, x2) ∈R2 : g(x1, x2) ≥0 for all g ∈g},\nwith g = {x1 −x2\n1, x2 −x2\n2}. A second one is given by taking g = {x1, 1 −\nx1, x2, 1 −x2}.\n2.1\nSums of squares and quadratic modules\nLet Σ[x] stand for the cone of SOS polynomials and let Σ[x]d denote the\ncone of SOS polynomials of degree at most 2d, namely Σ[x]d := Σ[x] ∩\nR[x]2d. Note that all SOS polynomials with real coefﬁcients are nonnega-\ntive on Rn. For instance, σ0 = 1\n2(x1 + x2 −1\n2)2 is a square in n = 2 variables\nof degree 2d = 2, and is thus obviously nonnegative on Rn.\nFor the ease of further notation, we set g0(x) := 1, and dj := ⌈deg(gj)/2⌉,\nfor all j = 0, . . . , m. Given a basic compact semialgebraic set X as above\nand an integer r ∈N∗, let M(g) be the quadratic module generated by\ng1, . . . , gm:\nM(g) :=\n(\nm\n∑\nj=0\nσj(x)gj(x) : σj ∈Σ[x], j = 0, . . . , m\n)\n,\nand let M(g)r be the r-truncated quadratic module:\nM(g)r :=\n(\nm\n∑\nj=0\nσj(x)gj(x) : σj ∈Σ[x]r−dj, j = 0, . . . , m\n)\n.\nA ﬁrst important remark is that all polynomials belonging to M(g) are\npositive on X.\nExample 2.1 To illustrate this later point, let us take the polynomial f = x1x2\nin two variables, and the 2-dimensional hypercube X = [0, 1]2 described with the\nbasic closed semialgebraic set given in (2.2), with g = {x1(1 −x1), x2(1 −x2)}.\nLet us consider the following decomposition of f + 1\n8:\nx1x2 + 1\n8 = 1\n2(x1 + x2 −1\n2)2 · 1 + 1\n2 · x1(1 −x1) + 1\n2 · x2(1 −x2).\nThis later decomposition of degree 2 proves that f = x1x2 + 1\n8 lies in the 1-\ntruncated quadratic module M(g)1, with σ0 = 1\n2(x1 + x2 −1\n2)2, σ1 = σ2 = 1\n2,\ng0 = 1, g1 = x1(1 −x1) and g2 = x2(1 −x2). Since σ0, σ1, σ2 are nonnegative\non R2 and g1, g2 are nonnegative on X (by deﬁnition), the above decomposition\n\n34\nChapter 2. Polynomial optimization and the moment-SOS hierarchy\ncertiﬁes that f + 1\n8 ≥0 on the hypercube [0, 1]2. This yields in particular that\n−1\n8 is a lower bound on the minimum of f on the hypercube. Since the minimum\nof f on the hypercube is obviously 0, it is natural to ask if one can compute a lower\nbound greater than −1\n8. The answer is positive: for all arbitrary small ε > 0, there\nexists a decomposition of f + ε in M(g)r, for some positive integer r depending\non ε.\nA second important remark is that M(g)r ⊆M(g)r+1, for all r ∈N∗,\nsince all SOS polynomials of degree 2r can be viewed as SOS polynomials\nof degree 2r + 2.\nTo guarantee the convergence behavior of the relaxations presented in\nthe sequel, we rely on the fact that polynomials which are positive on X lie\nin M(g)r for some r ∈N∗. The existence of such SOS-based representa-\ntions is guaranteed when the following condition holds.\nAssumption 2.2 (Archimedean) There exists N > 0 such that N −∥x∥2\n2 ∈\nM(g).\nA quadratic module M(g) for which Assumption 2.2 holds is said to be\nArchimedean.\nTheorem 2.3 (Putinar’s Positivstellensatz) Assume that the set X is deﬁned\nin (2.1) and the quadratic module M(g) is Archimedean. Then any polynomial\npositive on X belongs to M(g).\nAssumption 2.2 is slightly stronger than compactness. Indeed, compact-\nness of X already ensures that each variable has ﬁnite lower and upper\nbounds. One (easy) way to ensure that Assumption 2.2 holds is to add\na redundant constraint involving a well-chosen N depending on these\nbounds, in the deﬁnition of X.\n2.2\nBorel measures and moment matrices\nGiven a compact set A ⊆Rn, we denote by M (A) the vector space of ﬁ-\nnite signed Borel measures supported on A, namely real-valued functions\nfrom the Borel σ-algebra B(A). The support of a measure µ ∈M (A) is\ndeﬁned as the closure of the set of all points x such that µ(B) ̸= 0 for\nany open neighborhood B of x. We denote by C (A) the Banach space of\ncontinuous functions on A equipped with the sup-norm. Let C (A)′ stand\nfor the topological dual of C (A) (equipped with the sup-norm), i.e., the\nset of continuous linear functionals of C (A). By a Riesz identiﬁcation the-\norem, C (A)′ is isomorphically identiﬁed with M (A) equipped with the\ntotal variation norm denoted by ∥· ∥TV. Let C+(A) (resp. M+(A)) stand\nfor the cone of nonnegative elements of C (A) (resp. M (A)). The topology\nin C+(A) is the strong topology of uniform convergence in contrast with\nthe weak-star topology in M+(A).\n\n2.2. Borel measures and moment matrices\n35\nWith X being a basic compact semialgebraic set, the restriction of the\nLebesgue measure on a subset A ⊆X is λA(dx) := 1A(x)dx, where 1A :\nX →{0, 1} stands for the indicator function of A, namely 1A(x) = 1 if\nx ∈A and 1A(x) = 0 otherwise. A sequence y := (yα)α∈Nn ⊆R is said\nto have a representing measure on X if there exists µ ∈M (X) such that\nyα = R\nxαµ(dx) for all α ∈Nn, where we use the multinomial notation\nxα := xα1\n1 xα2\n2 · · · xαn\nn .\nAssume that µ, ν ∈M+(X) have the same moments y, namely yα =\nR\nX xα dµ = R\nX xα dν, for all α ∈Nn. Let us ﬁx f ∈C (X). Since X is\ncompact, the Stone-Weierstrass theorem implies that the polynomials are\ndense in C (X), so R\nX f dµ = R\nX f dν. Since f was arbitrary, the above\nequality holds for any f ∈C (X), which implies that µ = ν. Therefore, any\nﬁnite Borel measures supported on X is moment determinate.\nThe moments of the Lebesgue measure on A are denoted by\nyA\nα :=\nZ\nxαλA dx ∈R,\nα ∈Nn.\n(2.3)\nThe Lebesgue volume of A is vol A := yA\n0 = R\nλA dx. For all r ∈N, let us\ndeﬁne Nn\nr := {α ∈Nn | ∑n\nj=1 αj ≤r}, whose cardinality is (n+r\nr ). Then a\npolynomial f ∈R[x] is written as follows:\nx 7→f (x) = ∑\nα∈Nn\nfαxα,\nand f is identiﬁed with its vector of coefﬁcients f = ( fα)α∈Nn in the stan-\ndard monomial basis (xα)α∈Nn.\nGiven a real sequence y = (yα)α∈Nn, let us deﬁne the linear functional\nLy : R[x] →R by Ly( f ) := ∑α fαyα, for every polynomial f = ∑α fαxα.\nComing back to the previous 2-dimensional example from Chapter 2.1,\nwith f = x1x2, g1 = x1 −x2\n1 and g2 = x2 −x2\n2, we have Ly( f ) = y11,\nLy(g1) = y10 −y20 and Ly(g2) = y01 −y02.\nThen, we associate to y the so-called moment matrix Mr(y) of order r,\nthat is the real symmetric matrix with rows and columns indexed by Nn\nr\nand the following entrywise deﬁnition:\n[Mr(y)]β,γ := Ly(xβ+γ),\n∀β, γ ∈Nn\nr .\nGiven g ∈R[x], we also associate to y and g the so-called localizing\nmatrix of order r, that is the real symmetric matrix Mr(g y) with rows and\ncolumns indexed by Nn\nr and the following entrywise deﬁnition:\n[Mr(g y)]β,γ := Ly(g(x) xβ+γ),\n∀β, γ ∈Nn\nr .\nLet X be a basic compact semialgebraic set as in (2.1). Then one can check\nthat if y has a representing measure µ ∈M+(X) then Mr(gj y) ⪰0, for all\nj = 0, . . . , m.\n\n36\nChapter 2. Polynomial optimization and the moment-SOS hierarchy\nLet us give a simple example to illustrate the construction of moment\nand localizing matrices.\nExample 2.4 Let us take n = 2 and r = 2. The moment matrix M2(y) is\nindexed by N2\n2 = {(0, 0), (1, 0), (0, 1), (2, 0), (1, 1), (0, 2)} and can be written\nas follows:\nM2(y) =\n\n\n1\n|\ny1,0\ny0,1\n|\ny2,0\ny1,1\ny0,2\n−\n−\n−\n−\n−\n−\n−\ny1,0\n|\ny2,0\ny1,1\n|\ny3,0\ny2,1\ny1,2\ny0,1\n|\ny1,1\ny0,2\n|\ny2,1\ny1,2\ny0,3\n−\n−\n−\n−\n−\n−\n−\ny2,0\n|\ny3,0\ny2,1\n|\ny4,0\ny3,1\ny2,2\ny1,1\n|\ny2,1\ny1,2\n|\ny3,1\ny2,2\ny1,3\ny0,2\n|\ny1,2\ny0,3\n|\ny2,2\ny1,3\ny0,4\n\n\n.\nNext, consider the polynomial g1(x) = x1 −x2\n1 of degree 2. From the ﬁrst-order\nmoment matrix:\nM1(y) =\n\n\n1\n|\ny1,0\ny0,1\n−\n−\n−\ny1,0\n|\ny2,0\ny1,1\ny0,1\n|\ny1,1\ny0,2\n\n,\nwe obtain the following localizing matrix:\nM1(g1y) =\n\n\ny1,0 −y2,0\ny2,0 −y3,0\ny1,1 −y2,1\ny2,0 −y3,0\ny3,0 −y4,0\ny2,1 −y3,1\ny1,1 −y2,1\ny2,1 −y3,1\ny1,2 −y2,2\n\n.\nFor instance, the last entry [M1(g1y)]3,3 is equal to Ly(g1(x) · x2 · x2) = Ly(x1x2\n2 −\nx2\n1x2\n2) = y1,2 −y2,2.\n2.3\nThe moment-SOS hierarchy\nLet us consider the general POP\nP :\nfmin = inf\nx { f (x) : x ∈X},\n(2.4)\nwhere f is a polynomial and X is a basic closed semialgebraic set as in\n(2.1). It happens that this problem can be cast as an LP over probability\nmeasures, namely,\nfmeas :=\ninf\nµ∈M+(X)\n\u001aZ\nX f dµ :\nZ\nX dµ = 1\n\u001b\n.\n(2.5)\n\n2.3. The moment-SOS hierarchy\n37\nTo see that fmeas = fmin holds, let us consider a global minimizer xopt ∈Rn\nof f on X and consider the Dirac measure µ = δxopt supported on this\npoint. Note that this Dirac (probability) measure is feasible for the LP\nstated in (2.5), with value R\nX f dµ = f (xopt) = fmin, which implies that\ninfµ∈M+(X){R\nX f dµ : R\nX dµ = 1} ≤fmin. For the other direction, let us\nconsider a measure µ feasible for LP (2.5). Then, simply observe that since\nf (x) ≥fmin, for all x ∈X, the feasibility of µ implies that R\nX f dµ ≥\nR\nX fmin dµ = fmin\nR\nX dµ = fmin. Since it is true for any feasible solution,\none has infµ∈M+(X){R\nX f dµ : R\nX dµ = 1} ≥fmin. Another way to state\nthis equality is to write\nfmin = sup\nb\n{b : f −b ≥0 on X},\n(2.6)\nwhich is an LP over nonnegative polynomials, and to notice that the dual\nLP of (2.6) is LP (2.5). The equality then follows from the zero duality gap\nin inﬁnite-dimensional LP.\nAfter reformulating P as LP (2.5) over probability measures, one can\nthen build a hierarchy of moment relaxations for the later problem. This\nis done by using the fact that the condition µ ∈M+(X) can be relaxed as\nMr−dj(gj y) ⪰0, for all j = 0, . . . , m, and all r ≥dj = ⌈deg(gj)/2⌉.\nLetting rmin := max {⌈deg( f )/2⌉, d1, . . . , dm}, at step r ≥rmin of the\nhierarchy, one considers the following primal SDP program:\nPr :\nf r :=\ninf\ny\nLy( f )\ns.t.\nMr(y) ⪰0\nMr−dj(gj y) ⪰0,\nj ∈[m]\ny0 = 1\n(2.7)\nBefore considering the corresponding dual SDP program, let us remind\nthat the moment and localizing matrices Mr−dj(gj y) have entries which\nare linear in y. Namely, one has Mr−dj(gj y) = ∑α∈Nn\n2r Cj\nα yα; the matrix\nCj\nα has rows and columns indexed by Nn\nr−dj with (β, γ)-entry equal to\n∑β+γ+δ=α gj,δ. In particular for m = 0, one has g0 = 1 and the matrix\nBα := C0\nα has (β, γ)-entry equal to 1β+γ=α, where 1α=β stands for the func-\ntion which returns 1 if α = β and 0 otherwise. With tj := (n+r−dj\nr−dj ), the dual\nof SDP (2.7) is then the following SDP:\n\n\n\n\n\n\n\n\n\n\n\n\n\nsup\nGj,b\nb\ns.t.\nfα −b1α=0 =\nm\n∑\nj=0\n⟨Cj\nα, Gj⟩,\nα ∈Nn\n2r\nGj ∈S+\ntj ,\nj = 0, . . . , m\n(2.8)\n\n38\nChapter 2. Polynomial optimization and the moment-SOS hierarchy\nWe can rewrite the equality constraints from SDP (2.8) in a more concise\nway, namely as f −b ∈M(g)r. To see this, let us ﬁrst note that a sum of\nsquares σ of degree 2r can be written as v⊺Gv, with\nv := (1, x1, . . . , xn, x2\n1, x1x2, . . . , xr\n1, . . . , xr\nn),\nbeing the vectors of all monomials of degree at most r, and G ⪰0. The\nα-coefﬁcient of σ = v⊺Gv is equal to ⟨Bα, G⟩. Similarly, for any j ∈[m]\nand SOS σj of degree at most 2(r −dj), one can write σj = v⊺\nj Gjvj, with vj\nbeing the vector of all monomials of degree at most r −dj, and Gj ⪰0. One\ncan also check that the α-coefﬁcient of σjgj is equal to ⟨Cj\nα, Gj⟩. Therefore,\nSDP (2.8) is equivalent to the following optimization problem over SOS\npolynomials:\n\n\n\n\n\n\n\n\n\n\n\n\n\nsup\nσj,b\nb\ns.t.\nf −b =\nm\n∑\nj=0\nσjgj\nσj ∈Σ[x]r−dj,\nj = 0, . . . , m\n(2.9)\nor more concisely as\nsup\nb\n{b : f −b ∈M(g)r}.\n(2.10)\nThe dual SDP (2.10) is obtained by replacing the nonnegativity condi-\ntion f −b ≥0 on X of the dual LP (2.6) by the more restrictive condi-\ntion f −b ∈M(g)r. The sequences of SDP programs (2.7) and (2.10) are\ncalled the moment hierarchy and the SOS hierarchy, respectively. In the se-\nquel, we refer to the sequence of primal-dual programs (2.7)–(2.10) as the\nmoment-SOS hierarchy.\nTheorem 2.5 Under\nAssumption\n2.2,\nthe\nhierarchy\nof\nprimal-dual\nmoment-SOS relaxations (2.7)–(2.10) provides nondecreasing sequences of\nlower bounds converging to the global optimum fmin of P (2.4).\nThe above theorem provides the theoretical convergence guarantee of the\nmoment-SOS hierarchy.\nRemark 2.6 Even though we only included inequality constraints in the deﬁni-\ntion of X for the sake of simplicity, equality constraints can be treated in a dedi-\ncated way. For each equality constraint h(x) = 0, with h ∈R[x], one adds the\n\n2.4. Minimizer extraction\n39\nlocalizing constraint Mr−dh(h y) = 0, with dh := ⌈deg(h)/2⌉, in the primal\nmoment program (2.7). Similarly, in the dual SOS program (2.9), one adds a\nterm τh to the sum ∑m\nj=0 σjgj, with τ ∈R[x]2r−2dh.\nIn practice, it is possible to obtain ﬁnite convergence of the hierarchy,\nwhich is the topic of the next section.\n2.4\nMinimizer extraction\nHere we describe sufﬁcient conditions to obtain ﬁnite convergence of the\nmoment-SOS hierarchy and extract the global minimizers of the polyno-\nmial f on X.\nTheorem 2.7 Consider the sequence of primal moment relaxations deﬁned\nin (2.7). If for some r ≥rmin, SDP (2.7) has an optimal solution y which\nsatisﬁes\nrank Mr′(y) = rank Mr′−rmin(y) for some r′ ≤r,\n(2.11)\nthen f r = fmin and the inﬁnite-dimensional LP (2.5) has an optimal solution\nµ ∈M (X)+, which is ﬁnitely supported on t = rank Mr′(y) points of X,\nor equivalently t global minimizers of f on X.\nIf the rank stabilization (also called ﬂatness) condition (2.11) is satisﬁed,\nthen ﬁnite convergence occurs, namely the SDP relaxation (2.7) is exact\nwith optimal value f r = fmin. In addition, one can extract rank Mr′(y)\nglobal minimizers of f on X with the following algorithm.\nAlgorithm 1 Extract\nRequire: The moment matrix Mr′(y) of rank t satisfying the ﬂatness con-\ndition (2.11)\nEnsure: The t points x(i) ∈X, i ∈[t], global minimizers of Problem P (2.4)\n1: Compute the Cholesky factorization CC⊺= Mr′(y)\n2: Reduce C to a column echelon form U\n3: Compute from U the multiplication matrices Ni, i ∈[n]\n4: Compute N := ∑n\ni=1 λiNi with randomly generated coefﬁcients λi\n5: Compute the Schur decomposition N = QTQ⊺\n6: Compute the column vectors {qj}1≤j≤t of Q\n7: Return xi(j) := q⊺\nj Niqj, i ∈[n], j ∈[t]\n\n40\nChapter 2. Polynomial optimization and the moment-SOS hierarchy\nProposition 2.8 The procedure Extract described in Algorithm 1 is sound and\nreturns t global optimizers of Problem P (2.4).\nPROOF Since the ﬂatness condition (2.11) is satisﬁed, y is the moment se-\nquence of a t-atomic Borel measure µ supported on X. Namely, there are t\npoints x(1), . . . , x(t) ∈X such that\nµ =\nt\n∑\nj=1\nκjδx(j),\nκj > 0,\nt\n∑\nj=1\nκj = 1.\nBy construction of the moment matrix Mr′(y), one has\nMr′(y) =\nr\n∑\nj=1\nκjvr′(x(j))v⊺\nr′(x(j)) = VDV⊺,\nwhere the j-th column of V is vr′(x(j)) and D is a t × t diagonal matrix\nwith diagonal (κj)1≤j≤t. One can extract a Cholesky factor C as in Step 1,\nfor instance via singular value decomposition. The following steps of the\nextraction algorithm consist of transforming C into V by suitable column\noperations. The reduction of C to a column echelon form in Step 2 is done\nby Gaussian elimination with column pivoting. By construction of the\nmoment matrix, each row of U is indexed by a monomial xα involved in\nthe vector vr′. Pivot elements in U correspond to monomials xβ(j), j ∈[t] of\nthe basis generating the t solutions. Namely, if w = (xβ(1), xβ(1), . . . , xβ(t))\ndenotes this generating basis, then\nvr′(x(j)) = Uw(x(j)),\nj ∈[t].\nOverall, extracting the global minmizers boils down to solving the above\nsystems of equations. To solve this system, we compute at Step 3 each\nmultiplication matrix Ni, i ∈[n], which contains the coefﬁcients of the\nmonomials xixβ(j), j ∈[t], namely which satisfy\nNiw(x) = xiw(x).\nThe entries of the global minimizers are all eigenvalues of the multipli-\ncation matrices. Since w(x(j)) is an eigenvector common to all multupli-\ncation matrices, one builds the random combination N of Step 4, which en-\nsures with probability 1 that its eigenvalues are all distinct with 1-dimensional\neigenspaces. The Shur decomposition of Step 5 gives the decomposition\nN = QTQ⊺with an orthogonal matrix Q and an upper triangular matrix\nT with eigenvalues of N sorted in increasing order along the diagonal. 2\nExample 2.9 Consider the polynomial optimization problem P (2.4) with f =\n−(x1 −1)2 −(x1 −x2)2 −(x2 −3)2 and X = {x ∈R2 : 1 −(x1 −1)2 ≥\n\n2.5. Notes and sources\n41\n0, 1 −(x1 −x2)2 ≥0, 1 −(x1 −3)2 ≥0}. The ﬁrst SDP relaxation outputs\nf 1 = −3 and rank M1(y) = 3, while the second one outputs f 2 = −2 and\nthe rank stabilizes with rank M1(y) = rank M2(y) = 3. Therefore the ﬂatness\ncondition holds, which implies that fmin = f 2 = −2. The monomial basis is\nv2(x) = (1, x1, x2, x2\n1, x1x2, x2\n2). The column echelon form U of the Cholesky\nfactor of M2(y) is given by\n\n\n1\n0\n1\n0\n0\n1\n−2\n3\n0\n−4\n2\n2\n−6\n0\n5\n\n\n.\nPivot entries correspond to the generating basis w(x) = (1, x1, x2). Therefore\nthe entries of the 3 global minimizers satisfy the following system of polynomial\nequations:\nx2\n1 = −2 + 3x1\nx1x2 = −4 + 2x1 + 2x2\nx2\n2 = −6 + 5x2.\nThe multiplication matrices by x1 and x2 can be extracted from rows in U as\nfollows:\nN1 =\n\n\n0\n1\n0\n−2\n3\n0\n−4\n2\n2\n\n,\nN2 =\n\n\n0\n0\n1\n−4\n2\n2\n−6\n0\n5\n\n.\nAfter selecting a random convex combination of N1 and N2 and computing the\northogonal matrix in the corresponding Schur decomposition, we obtain the 3\nminimizers x(1) = (1, 2), x(2) = (2, 2) and x(3) = (2, 3).\n2.5\nNotes and sources\nThe representation of positive polynomials stated in Theorem 2.3 is the\nwell renowned Putinar’s representation and is proved in [Put93]. Based on\nthis representation, the convergence of the moment-SOS hierarchy, stated\nin Theorem 2.5, has been proved in [Las01].\nThe Riesz identiﬁcation theorem can be found for instance in [LL01].\nWe refer the interested reader to [RF10, Section 21.7] and [Bar02, Chapter\nIV] or [Lue97, Section 5.10] for functional analysis, measure theory and\napplications in convex optimization.\nThe ﬁnite convergence of the moment-SOS hierarchy has been proved\nto hold generically under Assumption 2.2 in [Nie14]. The statement of Al-\ngorithm 1 extracting global minimizers and its correctness proof are avail-\nable in [HL05] (combined with ideas from [LLR08]). The robustness of this\n\n42\nChapter 2. Polynomial optimization and the moment-SOS hierarchy\nalgorithm has been studied in [KPV18]. It was proved that Algorithm 1\nworks under some generic conditions and Assumption 2.2 in [Nie13]. An\ninterpretation of some wrong results, due to numerical inaccuracies, al-\nready observed when solving SDP relaxations for POP on a double preci-\nsion ﬂoating point SDP solver is provided in [LM19].\n\nBibliography\n[Bar02]\nA. Barvinok. A Course in Convexity. Graduate studies in mathe-\nmatics. American Mathematical Society, 2002.\n[HL05]\nD. Henrion and Jean-Bernard Lasserre. Detecting Global Optimal-\nity and Extracting Solutions in GloptiPoly, pages 293–310. Springer\nBerlin Heidelberg, Berlin, Heidelberg, 2005.\n[KPV18] Igor Klep, Janez Povh, and Jurij Volcic. Minimizer extraction in\npolynomial optimization is robust. SIAM Journal on Optimization,\n28(4):3177–3207, 2018.\n[Las01]\nJean-Bernard Lasserre. Global Optimization with Polynomials\nand the Problem of Moments.\nSIAM Journal on Optimization,\n11(3):796–817, 2001.\n[LL01]\nElliott H Lieb and Michael Loss. Analysis, graduate stud. math.,\nvol. 14. In Amer. Math. Soc, 2001.\n[LLR08] Jean Bernard Lasserre, Monique Laurent, and Philipp Rostal-\nski.\nSemideﬁnite characterization and computation of zero-\ndimensional real radical ideals.\nFoundations of Computational\nMathematics, 8(5):607–647, 2008.\n[LM19]\nJean-Bernard Lasserre and Victor Magron. In sdp relaxations,\ninaccurate solvers do robust optimization. SIAM Journal on Op-\ntimization, 29(3):2128–2145, 2019.\n[Lue97]\nD. G. Luenberger. Optimization by Vector Space Methods. John\nWiley & Sons, Inc., New York, NY, USA, 1st edition, 1997.\n[Nie13]\nJiawang Nie. Certifying convergence of lasserre’s hierarchy via\nﬂat truncation. Mathematical Programming, 142(1):485–510, 2013.\n[Nie14]\nJiawang Nie. Optimality conditions and ﬁnite convergence of\nlasserre’s hierarchy.\nMathematical programming, 146(1):97–121,\n2014.\n\n44\nBibliography\n[Put93]\nM. Putinar. Positive polynomials on compact semi-algebraic sets.\nIndiana University Mathematics Journal, 42(3):969–984, 1993.\n[RF10]\nH.L. Royden and P. Fitzpatrick. Real Analysis. Featured Titles for\nReal Analysis Series. Prentice Hall, 2010.\n\nPart I\nCorrelative sparsity\n\n\nChapter 3\nThe moment-SOS hierarchy\nbased on correlative sparsity\nIn this chapter, we describe how to exploit sparsity arising in the data\nof POPs from the perspective of variables, which leads to the notion of\ncorrelative sparsity (CS). We start to explain how to build the correlative\nsparsity pattern (csp) graph in Chapter 3.1. Then, we provide in Chapter\n3.2 an inﬁnite-dimensional LP formulation over probability measures for\nPOPs, based on CS. This LP program is then handled with a CS-adapted\nmoment-SOS hierarchy of SDP relaxations, stated in Chapter 3.3. An al-\nternative approach based on bounded degree SOS is given in Chapter 3.4.\nChapter 3.5 focuses on minimizer extraction. Eventually, we explain in\nChapter 3.6 how to extend this CS exploitation scheme to optimization\nover rational functions.\n3.1\nCorrelative sparsity\nRecall that a general POP is formulized as\nP :\nfmin = inf\nx { f (x) : x ∈X},\n(3.1)\nwhere X = {x ∈Rn : g1(x) ≥0, . . . , gm(x) ≥0}. Roughly speaking,\nthe exploitation of CS in the moment-SOS hierarchy for P consists of two\nsteps:\n(1) decompose the variables x into a set of cliques according to the corre-\nlations between variables emerging in the input polynomial system;\n(2) construct a sparse moment-SOS hierarchy with respect to the former\ndecomposition of variables.\n\n48\nChapter 3. The moment-SOS hierarchy based on correlative sparsity\nLet us proceed with more details. Recall dj := ⌈deg(gj)/2⌉for j ∈[m]\nand rmin := max {⌈deg( f )/2⌉, d1, . . . , dm}. Fix from now on a relaxation\norder r ≥rmin. Let J′ := {j ∈[m] | dj = r} which is possibly nonempty\nonly when r = rmin. We deﬁne the csp graph Gcsp(V, E) associated to POP\n(3.1) whose node set is V = {1, 2, . . . , n} and whose edge set E satisﬁes\n{i, j} ∈E if one of following conditions holds:\n(i) there exists α ∈supp( f ) ∪S\nj∈J′ supp(gj) such that {i, j} ⊆supp(α);\n(ii) there exists k ∈[m] \\ J′ such that {i, j} ⊆S\nα∈supp(gk) supp(α),\nwhere supp(α) := {k ∈[n] | αk ̸= 0} for α = (α1, . . . , αn) ∈Nn. Let\n(Gcsp)′ be a chordal extension of Gcsp1 and {Ik}p\nk=1 be the list of maximal\ncliques of (Gcsp)′ with nk := |Ik| so that the RIP (1.8) holds. Let R[x, Ik]\ndenote the ring of polynomials in the nk variables {xi}i∈Ik for k ∈[p].\nBy construction, one can decompose the objective function f as f = f1 +\n· · · + fp with fk ∈R[x, Ik] for all k ∈[p] (similarly for gj with j ∈J′).\nWe then partition the constraint polynomials gj, j ∈[m] \\ J′ into groups\n{gj | j ∈Jk}, k ∈[p] which satisfy\n(i) J1, . . . , Jp ⊆[m] \\ J′ are pairwise disjoint and Sp\nk=1 Jk = [m] \\ J′;\n(ii) for any k ∈[p] and any j ∈Jk, S\nα∈supp(gj) supp(α) ⊆Ik,\nso that gj ∈R[x, Ik] for all k ∈[p] and j ∈Jk. In addition, suppose that As-\nsumption 2.2 holds. Then all variables involved in POP (3.1) are bounded.\nTo guarantee global convergence of the hierarchy that will be presented\nlater, we need to add some redundant quadratic constraints to the descrip-\ntion of the POP. We summarize all above in the following assumption.\nAssumption 3.1 Consider POP (3.1). The two index sets [n] and [m] are decom-\nposed/partitioned into {I1, . . . , Ip} and {J′, J1, . . . , Jp}, respectively, such that\n(i) The objective function f can be decomposed as f = f1 + · · · + fp with\nfk ∈R[x, Ik] for k ∈[p] and the same goes for the constraint polynomial\ngj with j ∈J′;\n(ii) For all k ∈[p] and j ∈Jk, gj ∈R[x, Ik];\n(iii) The RIP (1.8) holds for I1, . . . , Ip (possibly after some reordering);\n(iv) For all k ∈[p], there exists Nk > 0 such that one of the constraint polyno-\nmials is Nk −∑i∈Ik x2\ni .\n1If Gcsp is already a chordal graph, then we do not need the chordal extension.\n\n3.2. A sparse inﬁnite-dimensional LP formulation\n49\n6\n4\n5\n1\n2\n3\nFigure 3.1: The csp graph for f over X from Example 3.9.\nExample 3.2 Consider an instance of POP (3.1) with f (x) = x2x5 + x3x6 −\nx2x3 −x5x6 + x1(−x1 + x2 + x3 −x4 + x5 + x6) and\nX = {x ∈Rn : g(x) ≥0, for all g ∈g},\nwith g = {(6.36 −x1)(x1 −4), . . . , (6.36 −x6)(x6 −4)}. Here, there are n = 6\nvariables and the number of constraints is m = 6. The related csp graph Gcsp is\ndepicted in Figure 3.1. After adding an edge between nodes 3 and 5, the resulting\ngraph (Gcsp)′ is chordal with maximal cliques I1 = {1, 4}, I2 = {1, 2, 3, 5},\nI3 = {1, 3, 5, 6}. Here p = 3 and one can write f = f1 + f2 + f3 with\nf1 = −x1x4,\nf2 = −x2\n1 + x1x2 + x1x3 −x2x3 + x2x5,\nf3 = −x5x6 + x1x5 + x1x6 + x3x6.\nFor the relaxation order r = rmin = 1, let J′ = [6] and Jk = ∅for k ∈[3];\nfor the relaxation order r ≥2, let J′ = ∅and J1 = {1, 4}, J2 = {2, 3, 5},\nJ3 = {6}. Then Assumption 3.1(i)-(ii) hold. In addition, I1 ∩I2 = {1} ⊆I3,\nand so RIP (1.8), or equivalently Assumption 3.1 (iii), holds. For each i ∈[n],\none has 6.362 −x2\ni ≥0 for all x ∈X, and so one can select N1 = 2 · 6.362,\nN2 = N3 = 4 · 6.362 and add the redundant constraints Nk −∑i∈Ik x2\ni ≥0,\nk ∈[p] in the description of X, so that Assumption 3.1 (iv) holds as well.\n3.2\nA sparse inﬁnite-dimensional LP formulation\nIn this section, we assume J′ = ∅. We now introduce a CS variant of the\ndense inﬁnite-dimensional LP (2.5) formulation over probability measures\nstated in Chapter 2.3. The idea is to deﬁne a new measure for each subset\nIk, k ∈[p], supported on a set Xk described by the constraints which only\ndepend on the variables indexed by Ik, namely,\nXk := {x ∈Rnk : gj(x) ≥0, j ∈Jk}, for k ∈[p].\n\n50\nChapter 3. The moment-SOS hierarchy based on correlative sparsity\nSo X can be equivalently described as\nX = {x ∈Rn : (xi)i∈Ik ∈Xk, k ∈[p]}.\n(3.2)\nSimilarly, for all j, k ∈[p] such that Ij ∩Ik ̸= ∅, deﬁne\nXjk = Xkj := {(xi)i∈Ij∩Ik : (xi)i∈Ij ∈Xj, (xi)i∈Ik ∈Xk}.\nAfterwards, for each k ∈[p] we deﬁne the projection πk : M+(X) →\nM+(Xk) of the space of Borel measures supported on X on the space of\nBorel measures supported on Xk, namely, for all µ ∈M+(X),\nπkµ(B) := µ({x : x ∈X, (xi)i∈Ik ∈B}),\nfor each Borel set B ∈B(Xk). We deﬁne similarly the projections πjk for all\nj, k ∈[p] such that Ij ∩Ik ̸= ∅. For each k ∈[p −1], we also rely on the set\nUk := {j ∈{k + 1, . . . , p} : Ij ∩Ik ̸= ∅}.\nThen the CS variant of (2.5) reads as follows:\nfcs :=\ninf\nµk\np\n∑\nk=1\nZ\nXk\nfk(x) dµk(x)\ns.t.\nπjkµj = πkjµk,\nj ∈Uk, k ∈[p −1]\nZ\nXk\ndµk(x) = 1,\nk ∈[p]\nµk ∈M+(Xk),\nk ∈[p]\n(3.3)\nTo prove fcs = fmin under Assumption 3.1, we rely on the following aux-\niliary lemma, illustrated in Figure 3.2 in the case p = 2. This lemma uses\nthe fact that one can disintegrate a probability measure on a product of\nBorel spaces into a marginal and a so-called stochastic kernel. Given two\nBorel spaces X, Z, a stochastic kernel q(dx|z) on X given Z is deﬁned by\n(1) q(dx|z) ∈M+(X) for all z ∈Z and (2) the function z 7→q(B|z) is\nB(Z)-measurable for all B ∈B(Z).\nLemma 3.3 Let [n] = ∪p\nk=1Ik with nk = |Ik|, Xk ⊆Rnk be given compact sets,\nand let X ⊆Rn be deﬁned as in (3.2). Let µ1 ∈M+(X1), . . . , µp ∈M+(Xp)\nbe measures satisfying the equality constraints of LP (3.3). If RIP (1.8) holds for\nI1, . . . , Ip, then there exists a probability measure µ ∈M+(X) such that\nπkµ = µk,\n(3.4)\nfor all k ∈[p], that is, µk is the marginal of µ on Rnk, i.e., with respect to variables\nindexed by Ik.\n\n3.2. A sparse inﬁnite-dimensional LP formulation\n51\nM+(X)\nM+(X1)\nM+(X2)\nM+(X12)\nπ1\nπ2\nπ21\nπ12\nFigure 3.2: Illustration of Lemma 3.3 in the case p = 2.\nPROOF The proof boils down to constructing µ by induction on p. If p = 1\nand I1 = [n], the conﬁguration corresponds exactly to the dense LP (2.5)\nformulation from Chapter 2.3, and one can simply take µ = µ1. For the\nsake of conciseness, we only provide a proof for the case p = 2. Let I12 :=\nI1 ∩I2 with cardinality n12. If I12 = ∅, then one has X = X1 × X2 and we\ncan simply deﬁne µ as the product measure of µ1 and µ2:\nµ(A × B) := µ1(A) × µ2(B),\nfor all A ∈B(Rn1), B ∈B(Rn2). This product measure µ satisﬁes (3.4).\nNext, let us focus on the hardest case where I12 ̸= ∅. Let πk be the\nnatural projection with respect to Ik\\I12 and let us deﬁne the Borel set\nYk := {πk(x) : x ∈Xk} ∈B(Rnk−n12). It follows that µ1, µ2 can be\nseen as probability measures on the cartesian products Y1 × X12 = X1 and\nX12 × Y2 = X2, respectively. Let ν1 and ν2 be the stochastic kernels of µ1\nand µ2, respectively. Since π12µ1 = π21µ2 =: ν, one can disintegrate µ1\nand µ2 as\nµ1(A × B) =\nZ\nB ν1(A|x)ν(dx),\n∀A ∈B(Y1), B ∈B(Rn12),\nµ2(C × B) =\nZ\nB ν2(C|x)ν(dx),\n∀A ∈B(Y2), B ∈B(Rn12).\nThen, one can deﬁne the measure µ ∈M+(Y1 × Rn12 × Y2) as follows:\nµ(A × B × C) =\nZ\nB ν1(A|x)ν2(C|x)ν(dx),\nfor every Borel rectangle A × B × C ∈B(Y1) × B(X12) × B(Y2). In par-\nticular if A = Y1, one has ν1(A|x) = 1 ν-a.e., and µ(Y1 × B × C) =\nR\nB ν2(C|x)ν(dx) = µ2(B × C), implying that µ2 is the marginal of µ on\n\n52\nChapter 3. The moment-SOS hierarchy based on correlative sparsity\nX12 × Y2 = X2. Similarly, µ1 is the marginal of µ on Y1 × X12 = X1, yield-\ning the desired result.\n2\nNow, we are ready to prove that LP (3.3) is not just a relaxation of the\ndense LP (2.5).\nTheorem 3.4 Consider POP (3.1). If Assumption 3.1 holds, then fcs =\nfmin.\nPROOF The ﬁrst inequality fcs ≤fmin is easy to show: let a be a global\nminimizer of f on X, assumed to exist thanks to the compactness hypoth-\nesis. Let µ = δa be the Dirac measure concentrated on a, and µk := πkµ\nbe its projection on M+(Xk), for each k ∈[p]. Namely, µk is the Dirac\nmeasure concentrated on (ai)i∈Ik ∈Xk, and is in particular a probability\nmeasure supported on Xk. For each pair j, k such that Ij ∩Ik ̸= ∅, the mea-\nsure πjkµj is the Dirac measure concentrated on (ai)i∈Ij∩Ik ∈Xjk, and so\nis πkjµk. Therefore, each measure µk is a feasible solution of (3.3). In ad-\ndition, the objective value of LP (3.3) is equal to ∑\np\nk=1 fk(a) = fmin, which\nproves the ﬁrst inequality.\nTo prove the other inequality fcs ≥fmin, let us ﬁx a feasible solution\n(µk) of LP (3.3). By Lemma 3.3, there exists a probability measure µ ∈\nM+(X) such that πkµ = µk, for each k ∈[p]. Then, one has\np\n∑\nk=1\nZ\nXk\nfk dµk =\np\n∑\nk=1\nZ\nXk\nfk dµ =\nZ\nX\np\n∑\nk=1\nfk dµ =\nZ\nX fdµ ≥fmin.\n2\n3.3\nThe CS-adpated moment-SOS hierarchy\nIn this section, we continue assuming J′ = ∅. For k ∈[p], a moment\nsequence y ⊆R and g ∈R[x, Ik], let Mr(y, Ik) (resp. Mr(g y, Ik)) be the\nmoment (resp. localizing) submatrix obtained from Mr(y) (resp. Mr(g y))\nby retaining only those rows and columns indexed by β ∈Nn\nr of Mr(y)\n(resp. Mr(gy)) with supp(β) ⊆Ik.\nExample 3.5 Consider again Example 3.2. The moment matrix M1(y, I1) is in-\ndexed by the support vectors (0, 0, 0, 0, 0, 0), (1, 0, 0, 0, 0, 0), (0, 0, 0, 1, 0, 0) (cor-\n\n3.3. The CS-adpated moment-SOS hierarchy\n53\nresponding to the monomials 1, x1 and x4, respectively) and reads as follows:\nM1(y) =\n\n\n1\n|\ny1,0,0,0,0,0\ny0,0,0,1,0,0\n−\n−\n−\ny1,0,0,0,0,0\n|\ny2,0,0,0,0,0\ny1,0,0,1,0,0\ny0,0,0,1,0,0\n|\ny1,0,0,1,0,0\ny0,0,0,2,0,0\n\n.\nWith r ≥rmin, the moment hierarchy based on CS for POP (3.3) is\ndeﬁned as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\nyk\n∑\np\nk=1 Lyk( fk)\ns.t.\nMr(yk, Ik) ⪰0,\nk ∈[p]\nMr−dj(gjyk, Ik) ⪰0,\nj ∈Jk, k ∈[p]\nLyk(xα) = Lyj(xα), α ∈Nn\n2r, supp(α) ⊆Ik ∩Ij, j ∈Uk, k ∈[p]\nLyk(1) = 1,\nk ∈[p]\n(3.5)\nNote that SDP (3.5) is equivalent to the following program:\nPr\ncs :\n\n\n\n\n\n\n\n\n\n\n\ninf\ny\nLy( f )\ns.t.\nMr(y, Ik) ⪰0,\nk ∈[p]\nMr−dj(gjy, Ik) ⪰0,\nj ∈Jk, k ∈[p]\ny0 = 1\n(3.6)\nwith optimal value denoted by f r\ncs. Indeed, for any sequence y = (yα)α∈Nn\n2r,\none can deﬁne yk := {yα : α ∈Nn\n2r, supp(α) ⊆Ik}, for all k ∈[p]. One\nobviously has Lyk(1) = 1, and each moment matrix Mr(yk, Ik) is equal\nto Mr(y, Ik) (and similarly for the localizing matrices).\nIn addition, if\nIk ∩Ij ̸= ∅and supp(α) ⊆Ik ∩Ij, then\nLyk(xα) = {yα : supp(α) ⊆Ik ∩Ij} = Lyj(xα).\nLet Σ[x, Ik] ⊆R[x, Ik] be the corresponding cone of SOS polynomials.\nThen the dual of (3.6) is\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsup\nb,σk,j\nb\ns.t.\nf −b = ∑\np\nk=1\n\u0010\nσk,0 + ∑j∈Jk σk,jgj\n\u0011\nσk,0, σk,j ∈Σ[x, Ik],\nj ∈Jk, k ∈[p]\ndeg(σk,0), deg(σk,jgj) ≤2r,\nj ∈Jk, k ∈[p]\n(3.7)\nIn the following, we refer to (3.6)–(3.7) as the CS-adpated moment-SOS\n(CSSOS) hierarchy. To prove that the sequence ( f r\ncs)r≥rmin converges to the\nglobal optimum fmin of the original POP (3.1), we rely on Lemma 3.3.\n\n54\nChapter 3. The moment-SOS hierarchy based on correlative sparsity\nTheorem 3.6 Consider POP (3.1).\nIf Assumption 3.1 holds, then the\nCSSOS hierarchy (3.6)–(3.7) provides a nondecreasing sequence of lower\nbounds ( f r\ncs)r≥rmin converging to fmin.\nRemark 3.7 Despite the convergence guarantee stated in Theorem 3.6, note that\nSDP (3.6) is a relaxation of the dense SDP (2.7) in general, and one can have\nf r\ncs < f r for some relaxation order r. The underlying reason is that the situation\nhere is different from the case of PSD matrix completion (Theorem 1.5). Namely,\nthere is no guarantee that one can obtain a PSD matrix completion Mr(y) from\nthe submatrices Mr(y, Ik), k ∈[p] because of the speciﬁc Hankel structure of\nMr(y). At the end of Appendix B.1, we provide a Julia script showing such\nconservatism behavior.\nAs a corollary of Theorem 3.6, we obtain the following representation\nresult, which is a CS version of Putinar’s Positivstellensatz.\nTheorem 3.8 Let f ∈R[x] be positive on a basic compact semialgebraic set\nX as in (2.1). Let Assumption 3.1 hold. Then,\nf =\np\n∑\nk=1\n \nσk,0 + ∑\nj∈Jk\nσk,jgj\n!\n,\n(3.8)\nfor some polynomials σk,0, σk,j ∈Σ[x, Ik], j ∈Jk, k ∈[p].\nLet us compare the computational cost of the CSSOS hierarchy (3.7)\nwith the dense hierarchy (2.9). For this, we deﬁne τ := maxk∈[p] |Ik| =\nmaxk∈[p] nk, that is, τ is the maximal size of the subsets I1, . . . , Ip.\n(1) The dense SOS formulation (2.9) involves m + 1 SOS polynomials in\nn variables of degree at most 2r, yielding m + 1 SDP matrices of size\nat most (n+r\nr ) and (n+2r\n2r ) equality constraints.\n(2) The CSSOS formulation (3.7) involves p + m SOS polynomials in at\nmost τ variables and of degree at most 2r, yielding p + m SDP ma-\ntrices of size at most (τ+r\nr ) and at most p(τ+2r\n2r ) equality constraints.\nOverall, when n is ﬁxed and r varies, the r-th step of the hierarchy involves\nO (r2n) equality constraints in the dense setting against O (pr2τ) in the\nsparse setting. This allows one to handle POPs involving several hundred\n\n3.4. A variant with SOS of bounded degrees\n55\nvariables if the maximal subset size τ is small (say, τ ≤10). Furthermore,\nas shown in the following example, one can also beneﬁt from the compu-\ntational cost saving when r increases for POPs involving a small number\nof variables (say, n ≤10).\nExample 3.9 Coming back to Example 3.2, let us compare the hierarchy of dense\nrelaxations given in Chapter 2.3 with the CS variant. For r = 1, the dense SDP\nrelaxation (2.9) involves (n+2r\n2r ) = (6+2\n2 ) = 28 equality constraints and pro-\nvides a lower bound of f 1 = 20.755 for fmin. The dense SDP relaxation (2.9)\nwith r = 2 involves (6+4\n4 ) = 210 equality constraints and provides a tighter\nlower bound of f 2 = 20.8608. For r = 2, the sparse SDP relaxation (3.7) in-\nvolves (2+4\n4 ) + 2(4+4\n4 ) = 155 equality constraints and provides the same bound\nf 2\ncs = f 2 = 20.8608. In Appendix A, we provide the MATLAB script allowing\none to obtain these results. The dense SDP relaxation with r = 3 involves 924\nequality constraints against 448 for the sparse variant. This difference becomes\nsigniﬁcant while considering the polynomial time complexity of solving SDP, al-\nready mentioned in Chapter 1.3.\n3.4\nA variant with SOS of bounded degrees\nIn certain situations, alternative representations for positive polynomials\ncan be more interesting as one can bound in advance the degree of the SOS\npolynomials involved.\nTheorem 3.10 Let us ﬁx an r ∈N and let Assumption 3.1 hold with J′ =\n∅. Suppose that, possibly after scaling, 0 ≤gj ≤1 on X for each j ∈[m]. If\nf is positive on X, then\nf =\np\n∑\nk=1\n\nσk +\n∑\nα,β∈N|Jk|\nck,αβ ∏\nj∈Jk\ng\nαj\nj (1 −gj)βj\n\n,\n(3.9)\nfor some ﬁnitely many real scalars ck,αβ ≥0 and SOS polynomials σk ∈\nΣ[x, Ik] with deg(σk) ≤2r.\nThe representation from Theorem 3.10 is called the sparse bounded SOS\n(SBSOS) representation. After replacing the right-hand side in the equality\nconstraint of (3.7) by an SBSOS representation, we obtain the following\n\n56\nChapter 3. The moment-SOS hierarchy based on correlative sparsity\nSBSOS hierarchy, indexed by an integer s ∈N∗:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsup\nb,σk,ck,αβ\nb\ns.t.\nf −b = ∑\np\nk=1\n\u0010\nσk + ∑|α+β|≤s ck,αβ ∏j∈Jk g\nαj\nj (1 −gj)βj\n\u0011\nck,αβ ≥0, |α + β| ≤s,\nk ∈[p], α, β ∈N|Jk|\nσk ∈Σ[x, Ik], deg(σk) ≤2r,\nj ∈Jk, k ∈[p]\n(3.10)\nWhile the degree of each σk is a priori ﬁxed and at most 2r, one can in-\ncrease the degree s of each term ∏j∈Jk g\nαj\nj (1 −gj)βj, which boils down to\nmultiplying the polynomials describing the constraint set X.\nRegarding the computational cost, SDP (3.10) involves p LMI constraints\n(associated to the σk) of maximal size (τ+r\nr ) (recall τ = maxk∈[p] |Ik|), which\ndoes not depend on s. In addition, the number of coefﬁcients ck,αβ is equal\nto (|Jk|+s\ns\n). Therefore, the SBSOS hierarchy offers a computational beneﬁt\nwhen each |Jk| is relatively small.\n3.5\nMinimizer extraction\nAs for the standard dense moment-SOS hierarchy stated in Chapter 2.3,\none can also detect ﬁnite convergence of the CSSOS hierarchy and extract\nglobal minimizers with a dedicated extraction algorithm — the CS variant\nof Algorithm 1.\nTheorem 3.11 Consider POP (3.1). Let Assumption 3.1 (i)–(ii) hold and\nlet us consider the hierarchy of moment relaxations (Pr\ncs)r≥rmin deﬁned in\n(3.6). Let ak := maxj∈Jk{dj} for all k ∈[p]. If for some r ≥rmin, Pr\ncs has\nan optimal solution y which satisﬁes\nrank Mr(y, Ik) = rank Mr−ak(y, Ik) for all k ∈[p],\n(3.11)\nand rank Mr(y, Ij ∩Ik) = 1 for all pairs (j, k) with Ij ∩Ik ̸= ∅, then the\nSDP relaxation (3.6) is exact, i.e., f r\ncs = fmin. In addition, for each k ∈[p],\nlet ∆k := {x(k)} ⊆Rnk be a set of solutions obtained from the extraction\nprocedure Extract, stated in Algorithm 1 and applied to the moment matrix\nMr(y, Ik). Then every x ∈Rn obtained by (xi)i∈Ik = x(k) for some x(k) ∈\n∆k is a global minimizer of POP (3.1).\nNote that Assumption 3.1 (iii)–(iv) are not required in Theorem 3.11,\nas the rank conditions are strong enough to ensure ﬁnite convergence and\n\n3.6. From polynomial to rational functions\n57\nextraction of a subset of global minimizers.\n3.6\nFrom polynomial to rational functions\nHere, we consider the following optimization problem:\nfmin := inf\nx∈X\nt\n∑\ni=1\npi(x)\nqi(x) ,\n(3.12)\nwhere X = {x ∈Rn : g1(x) ≥0, . . . , gm(x) ≥0} is a compact set, all\nnumerators and denominators are polynomials, and all denominators are\npositive on X.\nProblem (3.12) is a fractional programming problem of a rather general\nform. Here, we assume that the degree of each numerator/denomina-\ntor is rather small (≤10), but that the number of terms t can be much\nlarger (10 to 100). For dense data, the number of variables should also\nbe small (≤10). However, this number can be also relatively large (10 to\n100) provided that the problem data exhibits CS, as in Section 3.1. One\nnaive strategy is to reduce all fractions to the same denominator and ob-\ntain a single rational fraction to minimize. However, this approach is not\nadequate since the degree of the common denominator will be potentially\nlarge and even if n is small, one might not be able to solve the ﬁrst-order\nrelaxation of the related moment-SOS hierarchy. A more suitable strategy\nfor solving (3.12) is to cast it as a particular instance of the GMP, namely\nthe following inﬁnite-dimensional LP over measures:\nfmeas :=\ninf\nµi\nt\n∑\ni=1\nZ\nX pi(x) dµi(x)\ns.t.\nZ\nX xαqi(x) dµi(x) =\nZ\nX xαq1(x) dµ1(x), α ∈Nn, i = 2, . . . , t\nZ\nX q1(x) dµ1(x) = 1\nµ1, . . . , µt ∈M+(X)\n(3.13)\nTheorem 3.12 Consider (3.12). Let X be a compact set and assume that\neach qi is positive on X for all i ∈[t]. Then fmeas = fmin.\nPROOF First, we prove that fmin ≥fmeas. Let a be a global minimizer of\nf = ∑t\ni=1\npi\nqi on X, assumed to exist thanks to the compactness hypothe-\n\n58\nChapter 3. The moment-SOS hierarchy based on correlative sparsity\nsis. For each i ∈[t], let µi =\n1\nqi(a)δa be the Dirac measure centered at a\nweighted by\n1\nqi(a). We then have\nZ\nX q1(x) dµ1(x) =\n1\nq1(a)\nZ\nX q1(x)δa dx = 1,\nand for each i < t and all α ∈Nn,\nZ\nX qi(x)xα dµi(x) = aα,\nensuring that the measures (µi)i∈[t] are feasible for (3.13). The associated\noptimal value is\nt\n∑\ni=1\nZ\nX pi(x) dµi(x) =\nt\n∑\ni=1\npi(a)\nqi(a) = f (a) = fmin.\nTo prove the other direction, let (µi)i∈[t] be a feasible solution of (3.13). For\neach i ∈[t], let us deﬁne the measure νi as follows:\nνi(B) :=\nZ\nX∩B qi(x) dµi(x),\nfor each Borel set B in the Borel σ-algebra of Rn. The support of νi is\nX. Since measures on compact sets are moment determinate (see Chapter\n2.2), the moment equality constraints of (3.13) imply that νi = ν, for each\ni ∈{2, . . . , t}. The constraint R\nX q1 dµ1 = 1 implies that ν1 is a probability\nmeasure on X (since its mass is 1). Then one can rewrite the objective value\nas\nt\n∑\ni=1\nZ\nX pi dµi =\nt\n∑\ni=1\nZ\nX\npi\nqi\nqi dµi =\nt\n∑\ni=1\nZ\nX\npi\nqi\ndν1\n=\nZ\nX f dν1 ≥\nZ\nX fmin dν1 = fmin,\nwhere the inequality follows from the fact that f ≥fmin on X.\n2\nThis ﬁrst GMP formulation (3.13) allows one to handle general (possibly\ndense) rational programs. When the numerators and denominators satisfy\na csp similar to the one stated in Chapter 3.1, one can rely on a dedicated\nCS formulation.\nAssumption 3.13 There is a decomposition of [n] = ∪t\ni=1Ii and a partition of\n[m] = ∪t\ni=1Ji such that for each i ∈[t], pi, qi ∈R[x, Ii] (as in Assumption 3.1\n(i) in the case of polynomials), and Assumption 3.1 (ii)–(iv) hold.\n\n3.6. From polynomial to rational functions\n59\nThen, as in Chapter 3.2 for each i ∈[t], with ni = |Ii|, let us deﬁne\nXi := {x ∈Rni : gj(x) ≥0, j ∈Ji},\nso that X can be equivalently described as\nX = {x ∈Rn : (xk)k∈Ii ∈Xi, i ∈[t]}.\nSimilarly, for all pairs i, j ∈[t] such that Ii ∩Ij ̸= ∅, we deﬁne Xij, as well\nas the projection πi : M+(X) →M+(Xi), for each i ∈[t], and πij for all\npairs i, j ∈[t] such that Ii ∩Ij ̸= ∅. Then the CS variant of the inﬁnite-\ndimensional LP (3.13) is given by\nfcs :=\ninf\nµi\nt\n∑\ni=1\nZ\nXi\npi(x) dµi(x)\ns.t.\nπij(qi dµi) = πji(qj dµj),\nj ∈Ui, i ∈[t −1]\nZ\nXi\nqi(x) dµi(x) = 1, µi ∈M+(Xi),\ni ∈[t]\n(3.14)\nTheorem 3.14 Consider (3.12). Let X be a compact set, and assume that qi\nis positive on X for all i ∈[t]. If Assumption 3.13 holds, then fcs = fmin.\nOne can then derive a CS-adpated hierarchy of SDP relaxations for LP\n(3.14):\nPr\ncs :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\nyi\nt\n∑\nk=1\nLyi(pi)\ns.t.\nMr(yi, Ii) ⪰0,\ni ∈[t]\nMr−dj(gjyi, Ii) ⪰0,\nj ∈Ji, i ∈[t]\nLyi(xαqi) = Lyj(xαqj),\n|α| + max {deg(qi), deg(qj)} ≤2r\nand supp(α) ⊆Ii ∩Ij, j ∈Ui, i ∈[t]\nLyi(qi) = 1,\ni ∈[t]\n(3.15)\nwhere |α| := ∑n\ni=1 αi for α ∈Nn.\nExample 3.15 From the Rosenbrock problem\ninf\nx∈Rn\nn−1\n∑\ni=1\n\u0010\n100(xi+1 −x2\ni )2 + (xi −1)2\u0011\n,\n\n60\nChapter 3. The moment-SOS hierarchy based on correlative sparsity\nwe deﬁne the following rational optimization problem\nfmax := sup\nx∈Rn\nn−1\n∑\ni=1\n1\n100(xi+1 −x2\ni )2 + (xi −1)2 .\n(3.16)\nNote that Problem (3.16) has the same critical points as the Rosenbrock problem,\nwhich yields numerical issues for local optimization solvers embedded in opti-\nmization software such as BARON or NEOS. The global optimum fmax = n −1\nof Problem (3.16) is attained at xi = 1, i ∈[n]. Each summand depends on 2\nvariables, so that we can deﬁne Ii = {i, i + 1} for i ∈[n −1]. To bound the\nproblem, we let gi = 16 −x2\ni for i ∈[n]. When calling multiple times local\noptimization solvers with random initial guesses, we obtain local optima far away\nfrom the global optimum. This is in deep contrast with our CS-adpated hierarchy\nof SDP (3.15). After selecting the minimal relaxation order r = 1, we obtain the\nglobal optimum for Problem (3.16) with up to 1000 variables. The typical CPU\ntime ranges from a few seconds to a few minutes on a modern standard laptop.\nThe BARON software (on the NEOS server) can ﬁnd the global optimum in most\ncases when n < 640. For n ≥640, BARON returns wrong values (the ﬁrst\ncorresponding coordinate is equal to −0.995 instead of 1). Overall this rational\nproblem yields numerical instabilities for such local solvers, which can be handled\nwith the CS-adpated SDP relaxations.\n3.7\nNotes and sources\nThe CSSOS hierarchy for POPs was ﬁrst studied in [WKKM06]. Its global\nconvergence was proved in [Las06] soon after by introducing p additional\nquadratic constraints. Lemma 3.3 is proved in [Las06, § 6]. Theorem 3.8\nis stated and proved in [Las06, Corollary 3.9]. An alternative proof is pro-\nvided in [GNS07].\nThe alternative SBSOS representation stated in Theorem 3.10 is pro-\nvided in [WLT18]. We refer to this former paper for more details on prop-\nerties of the related SOS hierarchy.\nTheorem 3.11 is stated in [Las06, Theorem 3.7] and proved in [Las06,\n§ 4.2].\nThe results from Chapter 3.6 are mostly stated in [BHL16]. The proof\nof Theorem 3.14, very similar to the one of Theorem 3.4, can be found in\n[BHL16, § 3.1]. Example 3.15 is provided in [BHL16, § 4.4.2]. The inter-\nested reader can ﬁnd detailed illustrations of the CSSOS hierarchy for ra-\ntional programming in [BHL16, § 4], together with comparison with local\noptimization solvers such as the BARON software [TS05], publicly avail-\nable on the NEOS server [CMM98].\nFor the more advanced problem of set approximation, in particular\nin the context of dynamical systems, exploiting CS is more delicate as\n\n3.7. Notes and sources\n61\nthe set of trajectories of a system with sparse dynamics is not necessarily\nsparse. Recent research efforts have been pursued for volume approxima-\ntion [TWLH21] and region of attraction [TCHL20, SK20].\n\n\nBibliography\n[BHL16]\nFlorian Bugarin, Didier Henrion, and Jean Bernard Lasserre.\nMinimizing the sum of many rational functions. Mathematical\nProgramming Computation, 8(1):83–111, 2016.\n[CMM98]\nJoseph Czyzyk, Michael P Mesnier, and Jorge J Moré. The\nneos server.\nIEEE Computational Science and Engineering,\n5(3):68–75, 1998.\n[GNS07]\nDavid Grimm, Tim Netzer, and Markus Schweighofer. A note\non the representation of positive polynomials with structured\nsparsity. Archiv der Mathematik, 89(5):399–403, 2007.\n[Las06]\nJean B Lasserre. Convergent sdp-relaxations in polynomial\noptimization with sparsity.\nSIAM Journal on Optimization,\n17(3):822–843, 2006.\n[SK20]\nCorbinian Schlosser and Milan Korda. Sparse moment-sum-\nof-squares relaxations for nonlinear dynamical systems with\nguaranteed convergence.\narXiv preprint arXiv:2012.05572,\n2020.\n[TCHL20]\nMatteo Tacchi,\nCarmen Cardozo,\nDidier Henrion,\nand\nJean Bernard Lasserre. Approximating regions of attraction of\na sparse polynomial differential system. IFAC-PapersOnLine,\n53(2):3266–3271, 2020.\n[TS05]\nMohit Tawarmalani and Nikolaos V Sahinidis. A polyhedral\nbranch-and-cut approach to global optimization. Mathemati-\ncal programming, 103(2):225–249, 2005.\n[TWLH21]\nMatteo Tacchi, Tillmann Weisser, Jean Bernard Lasserre, and\nDidier Henrion. Exploiting sparsity for semi-algebraic set vol-\nume computation. Foundations of Computational Mathematics,\npages 1–49, 2021.\n\n64\nBibliography\n[WKKM06] Hayato Waki,\nSunyoung Kim,\nMasakazu Kojima,\nand\nMasakazu Muramatsu.\nSums of squares and semideﬁnite\nprogram relaxations for polynomial optimization problems\nwith structured sparsity.\nSIAM Journal on Optimization,\n17(1):218–242, 2006.\n[WLT18]\nTillmann Weisser, Jean B Lasserre, and Kim-Chuan Toh.\nSparse-bsos: a bounded degree sos hierarchy for large scale\npolynomial optimization with sparsity. Mathematical Program-\nming Computation, 10(1):1–32, 2018.\n\nChapter 4\nApplication in computer\narithmetic\nIn this chapter, we describe an optimization framework to provide upper\nbounds on absolute roundoff errors of ﬂoating-point nonlinear programs,\ninvolving polynomials. The efﬁciency of this framework is based on the\nCSSOS hierarchy which exploits CS of the input polynomials, as described\nin Chapter 3.\nConstructing numerical programs which perform accurate computa-\ntion turns out to be difﬁcult, due to ﬁnite numerical precision of imple-\nmentations such as ﬂoating-point or ﬁxed-point representations. Finite-\nprecision numbers induce roundoff errors, and knowledge of the range\nof these roundoff errors is required to fulﬁll safety criteria of critical pro-\ngrams, as typically arising in modern embedded systems such as aircraft\ncontrollers. Such knowledge can be used in general for developing accu-\nrate numerical software, but is also particularly relevant when consider-\ning migration of algorithms onto hardware (e.g., FPGAs). The advantage\nof architectures based on FPGAs is that they allow more ﬂexible choices\nin number representations, rather than limiting the choice between IEEE\nstandard single or double precision. Indeed, in this case, we beneﬁt from a\nmore ﬂexible number representation while still ensuring guaranteed bounds\non the program output.\nOur method to bound the error is a decision procedure based on a spe-\ncialized variant of the Lasserre hierarchy [Las06], outlined in Chapter 3.\nThe procedure relies on SDP to provide sparse SOS decompositions of\nnonnegative polynomials. Our framework handles polynomial program\nanalysis (involving the operations +, ×, −) as well as extensions to the\nmore general class of semialgebraic and transcendental programs (involv-\ning √, /, min, max, arctan, exp), following the approximation scheme de-\nscribed in [MAGW15]. For the sake of conciseness, we focus in this book\n\n66\nChapter 4. Application in computer arithmetic\non polynomial programs only. The interested reader can ﬁnd more details\nin the related publication [MCD17].\n4.1\nPolynomial programs\nHere we consider a given program that implements a polynomial expres-\nsion f with input variables x taking values in a region X. We assume that\nX is included in a box (i.e., a product of closed intervals) and that X is\nencoded as in (2.1):\nX := { x ∈Rn : g1(x) ≥0, . . . , gl(x) ≥0 },\nfor polynomial functions g1, . . . , gl.\nThe type of numerical constants is denoted by C. In our current imple-\nmentation, the user can choose either 64 bit ﬂoating-point or arbitrary-size\nrational numbers. The inductive type of polynomial expressions f, g1, . . . , gl\nwith coefﬁcients in C is pExprC deﬁned as follows:\ntype pexprC =\nPc of C\n| Px of positive\n| Psub of pexprC ∗pexprC | Pneg of pexprC\n| Padd of pexprC ∗pexprC\n| Pmul of pexprC ∗pexprC\nThe constructor Px takes a positive integer as argument to represent ei-\nther an input or local variable. One obtains rounded expressions using\na recursive procedure round. We adopt the standard practice [Hig02] to\napproximate a real number x with its closest ﬂoating-point representation\nˆx = x(1 + e), with |e| is less than the machine precision ε. In the sequel,\nwe neglect both overﬂow and denormal range values. The operator ˆ· is\ncalled the rounding operator and can be selected among rounding to near-\nest, rounding toward zero (resp. ±∞). In the sequel, we assume round-\ning to nearest. The scientiﬁc notation of a binary (resp. decimal) ﬂoating-\npoint number ˆx is a triple (s, sig, exp) consisting of a sign bit s, a signiﬁcand\nsig ∈[1, 2) (resp. [1, 10)) and an exponent exp, yielding numerical evalua-\ntion (−1)s sig 2exp (resp. (−1)s sig 10exp).\nThe upper bound on the relative ﬂoating-point error is given by ε =\n2−prec, where prec is called the precision, referring to the number of signiﬁ-\ncand bits used. For single precision ﬂoating-point, one has prec = 24. For\ndouble (resp. quadruple) precision, one has prec = 53 (resp. prec = 113).\nLet F be the set of binary ﬂoating-point numbers.\nFor each real-valued operation bopR ∈{+, −, ×}, the result of the cor-\nresponding ﬂoating-point operation bopF ∈{⊕, ⊖, ⊗} satisﬁes the follow-\n\n4.2. Upper bounds on roundoff errors\n67\ning when complying with IEEE 754 standard arithmetic [IEE08] (without\noverﬂow, underﬂow and denormal occurrences):\nbopF( ˆx, ˆy) = bopR( ˆx, ˆy)(1 + e),\n|e| ≤ε = 2−prec.\n(4.1)\nThen, we denote by ˆf (x, e) the rounded expression of f after applying\nthe round procedure, introducing additional error variables e.\n4.2\nUpper bounds on roundoff errors\nThe algorithm roundoff_bound, depicted in Algorithm 2, takes as input x,\nX, f, ˆf, e as well as the set E of bound constraints over e. For a given ma-\nchine ε, one has E := [−ε, ε]m, with m being the number of error variables.\nThis algorithm actually relies on the CSSOS hierarchy from Chapter 3, thus\nroundoff_bound also takes as input a relaxation order r ∈N∗. The algo-\nrithm provides as output an interval enclosure of the error ˆf (x, e) −f (x)\nover K := X × E. From this interval [ f r\nmin, f r\nmax], one can compute | f |r\nmax :=\nmax {−f r\nmin, f r\nmax}, which is a sound upper bound of the maximal absolute\nerror |∆|max := max(x,e)∈K | ˆf (x, e) −f (x) |.\nAlgorithm 2 roundoff_bound\nRequire: input variables x, input constraints X, nonlinear expression f,\nrounded expression ˆf, error variables e, error constraints E, relaxation\norder r\nEnsure: interval enclosure of the error ˆf −f over K := X × E\n1: Deﬁne the absolute error ∆(x, e) := ˆf (x, e) −f (x)\n2: Compute ℓ(x, e) := ∆(x, 0) + ∑m\nj=1\n∂∆(x,e)\n∂ej\n(x, 0) ej\n3: Deﬁne h := ∆−ℓ\n4: [h, h] := ia_bound(h, K)\n▷Compute bounds for h\n5: [ℓr\nmin, ℓr\nmax] := cs_sdp(ℓ, K, r)\n▷Compute bounds for ℓ\n6: return [ℓr\nmin + h, ℓr\nmax + h]\nAfter deﬁning the absolute roundoff error ∆:= ˆf −f (Step 1), one de-\ncomposes ∆as the sum of an expression ℓwhich is afﬁne with resepct to\nthe error variable e and a remainder h. One way to obtain ℓis to compute\nthe vector of partial derivatives of ∆with resepct to e evaluated at (x, 0)\nand ﬁnally to take the inner product of this vector and e (Step 2). Then,\nthe idea is to compute a precise bound of ℓand a coarse bound of h. The\nunderlying reason is that h involves error term products of degree greater\nthan 2 (e.g. e1e2), yielding an interval enclosure of a priori much smaller\nwidth, compared to the interval enclosure of ℓ. One obtains the interval\nenclosure of h using the procedure ia_bound implementing basic interval\n\n68\nChapter 4. Application in computer arithmetic\narithmetic (Step 4) to bound the remainder of the multivariate Taylor ex-\npansion of ∆with resepct to e, expressed as a combination of the second-\nerror derivatives (similar as in [SJRG15]). The algorithm roundoff_bound\nis very similar to the algorithm of FPTaylor [SJRG15], except that SDP\nbased techniques are used instead of the global optimization procedure\nfrom [SJRG15]. Note that overﬂow and denormal are neglected here but\none could handle them, as in [SJRG15], by adding additional error vari-\nables and discarding the related terms using naive interval arithmetic.\nThe bounds of ℓare provided through the cs_sdp procedure, which\nsolves two instances of (3.7), at relaxation order r. We now give more\nexplanation about this procedure. We can map each input variable xi to\nthe integer i, for all i ∈[n], as well as each error variable ej to n + j, for\nall j ∈[m]. Then, deﬁne the sets I1 := [n] ∪{n + 1}, . . . , Im := [n] ∪{n +\nm}. Here, we take advantage of the csp of ℓby using m distinct sets of\ncardinality n + 1 rather than a single one of cardinality n + m, i.e., the\ntotal number of variables. Note that these subsets satisfy (1.8) and one can\nwrite ℓ(x, e) = ∆(x, 0) + ∑m\nj=1\n∂∆(x,e)\n∂ej\n(x, 0) ej. After noticing that ∆(x, 0) =\nˆf (x, 0) −f (x) = 0, one can scale the optimization problems by writing\nℓ(x, e) =\nm\n∑\nj=1\nsj(x)ej = ε\nm\n∑\nj=1\nsj(x)ej\nε ,\n(4.2)\nwith sj(x) := ∂∆(x,e)\n∂ej\n(x, 0), for all j ∈[m]. Replacing e by e/ε leads to com-\nputing an interval enclosure of ℓ/ε over K′ := X × [−1, 1]m. As usual from\nAssumption 2.2, there exists an integer N > 0 such that N −∑n\ni=1 x2\ni ≥0,\nas the input variables satisfy box constraints. Moreover, to fulﬁll Assump-\ntion 3.1, one encodes K′ as follows:\nK′ := {(x, e) ∈Rn+m : g1(x) ≥0, . . . , gl(x) ≥0,\ngl+1(x, e1) ≥0, . . . , gl+m(x, em) ≥0},\nwith gl+j(x, ej) := N + 1 −∑n\ni=1 x2\ni −e2\nj , for all j ∈[m]. The index set of\nvariables involved in gj is [n] for all j ∈[l]. The index set of variables\ninvolved in gl+j is Ij for all j ∈[m].\nThen, one can compute a lower bound of the minimum of ℓ′(x, e) :=\nℓ(x, e)/ε = ∑m\nj=1 sj(x)ej over K′ by solving the following CSSOS problem:\nℓ′r\nmin :=\nsup\nb,σj\nb\ns.t.\nℓ′ −b = σ0 + ∑l+m\nj=1 σjgj\nσ0 ∈∑m\nj=1 Σ[(x, e), Ij]\nσj ∈Σ[(x, e), Jj],\nj ∈[l + m]\ndeg(σjgj) ≤2r,\nj = 0, . . . , l + m\n(4.3)\n\n4.2. Upper bounds on roundoff errors\n69\nA feasible solution of Problem (4.3) ensures the existence of σ1 ∈Σ[(x, e1)],\n. . . , σm ∈Σ[(x, em)] such that σ0 = ∑m\nj=0 σj, allowing the following refor-\nmulation:\nℓ′r\nmin =\nsup\nb,σj,σj\nb\ns.t.\nℓ′ −b = ∑m\nj=1 σj + ∑l+m\nj=1 σjgj\nσj ∈Σ[x],\nj ∈[m]\nσj ∈Σ[(x, ej)], deg(σj) ≤2r,\nj ∈[m]\ndeg(σjgj) ≤2r,\nj ∈[l + m]\n(4.4)\nAn upper bound ℓ′r\nmax can be obtained by replacing sup with inf and ℓ′ −b\nby b −ℓ′ in Problem (4.4). Our optimization procedure cs_sdp computes\nthe lower bound ℓ′r\nmin as well as an upper bound ℓ′r\nmax of ℓ′ over K′, and\nthen returns the interval [ε ℓ′r\nmin, ε ℓ′r\nmax], which is a sound enclosure of the\nvalues of ℓover K.\nWe emphasize two advantages of the decomposition ∆= ℓ+ h and\nmore precisely of the linear dependency of ℓwith resepct to e: scalabil-\nity and robustness to SDP numerical issues. First, no computation is re-\nquired to determine the csp of ℓ, by comparison to the general case. Thus,\nit becomes much easier to handle the optimization of ℓwith the sparse\nSDP (4.4) rather than with the corresponding instance of the dense relax-\nation (Pr), given in (2.7). While the latter involves (n+m+2r\n2r\n) SDP variables,\nthe former involves only m (n+1+2r\n2r\n) SDP variables, ensuring the scalabil-\nity of our framework. In addition, the linear dependency of ℓwith resepct\nto e allows us to scale the error variables and optimize over a set of vari-\nables lying in K′ := X × [−1, 1]m. It ensures that the range of input vari-\nables does not signiﬁcantly differ from the range of error variables. This\ncondition is mandatory in considering SDP relaxations because most SDP\nsolvers (e.g., MOSEK [ART03]) are implemented using double precision\nﬂoating-point. It is impossible to optimize ℓover K (rather than ℓ′ over\nK′) when the maximal value ε of error variables is less than 2−53, due to\nthe fact that SDP solvers would treat each error variable term as 0, and\nconsequently ℓas the zero polynomial. Thus, this decomposition insures\nour framework against numerical issues related to ﬁnite-precision imple-\nmentation of SDP solvers.\nLet us consider the interval [ℓmin, ℓmax], with ℓmin := inf(x,e)∈K ℓ(x, e)\nand ℓmax := sup(x,e)∈K ℓ(x, e). The next lemma states that one can approx-\nimate this interval as closely as desired using the cs_sdp procedure.\nLemma 4.1 (Convergence of the cs_sdp procedure) Let [ℓr\nmin, ℓr\nmax] be the\ninterval enclosure returned by the procedure cs_sdp(ℓ, K, r). Then the sequence\n([ℓr\nmin, ℓr\nmax])r∈N converges to [ℓmin, ℓmax] when r goes to inﬁnity.\nThe proof of Lemma 4.1 is based on the fact that the assumptions of The-\n\n70\nChapter 4. Application in computer arithmetic\norem 3.8 are fulﬁlled for our speciﬁc roundoff error problem. This result\nguarantees asymptotic convergence to the exact enclosure of ℓwhen the re-\nlaxation order r tends to inﬁnity. However, it is more reasonable in practice\nto keep this order as small as possible to obtain tractable SDP relaxations.\nHence, we generically solve each instance of Problem (4.4) at the minimal\nrelaxation order, that is rmin = max {⌈deg ℓ/2⌉, ⌈deg(gj)/2⌉, j = 1, . . . , l +\nm}. Afterwards, we can rely on the COQ computer assistant to obtain for-\nmally certiﬁed upper bounds for the roundoff error; see [MCD17, § 2.3] for\nmore details.\n4.3\nOverview of numerical experiments\nWe present an overview of our method and of the capabilities of related\ntechniques, using an example. Consider a program implementing the fol-\nlowing polynomial expression f:\nf (x) := x2 × x5 + x3 × x6 −x2 × x3 −x5 × x6\n+ x1 × (−x1 + x2 + x3 −x4 + x5 + x6),\nwhere the six-variable vector x := (x1, x2, x3, x4, x5, x6) is the input of the\nprogram. Here the set X of possible input values is a product of closed\nintervals: X = [4.00, 6.36]6. This function f together with the set X ap-\npear in many inequalities arising from the the proof of the Kepler Conjec-\nture [Hal06], yielding challenging global optimization problems.\nThe polynomial expression f is obtained by performing 15 basic op-\nerations (1 negation, 3 subtractions, 6 additions and 5 multiplications).\nWhen executing this program with a set of ﬂoating-point numbers ˆx :=\n( ˆx1, ˆx2, ˆx3, ˆx4, ˆx5, ˆx6) ∈X, one actually computes a ﬂoating-point result ˆf,\nwhere all operations +, −, × are replaced by the respectively associated\nﬂoating-point operations ⊕, ⊖, ⊗. The results of these operations comply\nwith IEEE 754 standard arithmetic [IEE08]. Here, for the sake of clarity,\nwe do not consider real input variables. For instance, (in the absence of\nunderﬂow) one can write ˆx2 ⊗ˆx5 = (x2 × x5)(1 + e1), by introducing an\nerror variable e1 such that −ε ≤e1 ≤ε, where the bound ε is the machine\nprecision (e.g., ε = 2−24 for single precision). One would like to bound\nthe absolute roundoff error |∆(x, e)| := | ˆf (x, e) −f (x)| over all possible\ninput variables x ∈X and error variable e1, . . . , e15 ∈[−ε, ε]. Let us deﬁne\nE := [−ε, ε]15 and K := X × E. Then our bound problem can be cast as\nﬁnding the maximum |∆|max of |∆| over K, yielding the following nonlin-\near optimization problem:\n|∆|max := max\n(x,e)∈K |∆(x, e)|\n= max {−min\n(x,e)∈K ∆(x, e), max\n(x,e)∈K ∆(x, e)}.\n(4.5)\n\n4.3. Overview of numerical experiments\n71\nOne can directly try to solve these two POPs using classical SDP relax-\nations [Las01]. As in [SJRG15], one can also decompose the error term ∆\nas the sum of a term ℓ(x, e), which is afﬁne with resepct to e, and a nonlin-\near term h(x, e) := ∆(x, e) −ℓ(x, e). Then the triangular inequality yields:\n|∆|max ≤max\n(x,e)∈K |ℓ(x, e)| + max\n(x,e)∈K |h(x, e)|.\n(4.6)\nIt follows for this example that ℓ(x, e) = x2x5e1 + x3x6e2 + (x2x5 + x3x6)e3 +\n· · · + f (x)e15 = ∑15\ni=1 si(x)ei, with s1(x) := x2x5, s2(x) := x3x6, . . . , s15(x) :=\nf (x). The Symbolic Taylor Expansions method [SJRG15] consists of using a\nsimple branch and bound algorithm based on interval arithmetic to com-\npute a rigorous interval enclosure of each polynomial si, i ∈[15], over X\nand ﬁnally obtain an upper bound of |ℓ| + |h| over K. In contrast, our\nmethod uses sparse SDP relaxations for polynomial optimization (derived\nfrom [Las06]) to bound |ℓ| and basic interval arithmetic as in [SJRG15]\nto bound |h| (i.e., we use interval arithmetic to bound second-order error\nterms in the multivariate Taylor expansion of ∆with resepct to e).\n• A direct attempt to solve the two polynomial problems occurring in\nEquation (4.5) fails as the SDP solver (in our case SDPA [YFN+10])\nruns out of memory.\n• Using our method implemented in the Real2Float tool1, one obtains\nan upper bound of 760ε for |ℓ| + |h| over K in 0.15 seconds. This\nbound is provided together with a certiﬁcate which can be formally\nchecked inside the COQ proof assistant in 0.20 seconds.\n• After normalizing the polynomial expression and using basic inter-\nval arithmetic, one obtains 8 times more quickly a coarser bound of\n922ε.\n• Symbolic Taylor expansions implemented in FPTaylor [SJRG15] pro-\nvide a more precise bound of 721ε, but the analysis time is 28 times\nslower than with our implementation. Formal veriﬁcation of this\nbound inside the HOL-LIGHT proof assistant takes 27.7 seconds, which\nis 139 times slower than proof checking with Real2Float inside COQ.\nOne can obtain an even more precise bound of 528ε (but 37 times\nslower than with our implementation) by turning on the improved\nrounding model of FPTaylor and limiting the number of branch and\nbound iterations to 10000. The drawback of this bound is that it can-\nnot be formally veriﬁed.\n• Finally, a slightly coarser bound of 762ε is obtained with the ROSA\nreal compiler [DK14], but the analysis is 19 times slower than with\nour implementation and we cannot get formal veriﬁcation of this\nbound.\n1https://forge.ocamlcore.org/projects/nl-certify/\n\n72\nChapter 4. Application in computer arithmetic\n4.4\nNotes and sources\nTo obtain lower bounds on roundoff errors, one can rely on testing ap-\nproaches, such as meta-heuristic search [BdA+12] or under-approximation\ntools (e.g., s3fp [CGRS14]). Here, we are interested in efﬁciently han-\ndling the complementary over-approximation problem, namely to obtain\nprecise upper bounds on the error.\nThis problem boils down to ﬁnd-\ning tight abstractions of linearities or non-linearities while being able to\nbound the resulting approximations in an efﬁcient way. For computer\nprograms consisting of linear operations, automatic error analysis can be\nobtained with well-studied optimization techniques based on SAT/SMT\nsolvers [HGBK12] and afﬁne arithmetic [DGP+09]. However, non-linear\noperations are key to many interesting computational problems arising\nin physics, biology, controller implementations and global optimization.\nTwo promising frameworks have been designed to provide upper bounds\nfor roundoff errors of nonlinear programs. The corresponding algorithms\nrely on Taylor-interval methods [SJRG15], implemented in the FPTaylor\ntool, and on combining SMT with interval arithmetic [DK14], implemented\nin the ROSA real compiler. We refer the interested reader to [MCD17, § 4]\nfor more details on the extensive experimental evaluation that we per-\nformed.\n\nBibliography\n[ART03]\nErling D Andersen, Cornelis Roos, and Tamas Terlaky. On\nimplementing a primal-dual interior-point method for conic\nquadratic optimization. Mathematical Programming, 95(2):249–\n277, 2003.\n[BdA+12]\nMateus Borges, Marcelo d’Amorim, Saswat Anand, David\nBushnell, and Corina S. Pasareanu. Symbolic execution with\ninterval solving and meta-heuristic search. In Proceedings of\nthe 2012 IEEE Fifth International Conference on Software Testing,\nVeriﬁcation and Validation, ICST ’12, pages 111–120, Washing-\nton, DC, USA, 2012. IEEE Computer Society.\n[CGRS14]\nWei-Fan Chiang, Ganesh Gopalakrishnan, Zvonimir Raka-\nmaric, and Alexey Solovyev. Efﬁcient search for inputs caus-\ning high ﬂoating-point errors. In Proceedings of the 19th ACM\nSIGPLAN Symposium on Principles and Practice of Parallel Pro-\ngramming, PPoPP ’14, pages 43–52, New York, NY, USA, 2014.\nACM.\n[DGP+09]\nDavid Delmas, Eric Goubault, Sylvie Putot, Jean Souyris,\nKarim Tekkal, and Franck Védrine.\nTowards an industrial\nuse of ﬂuctuat on safety-critical avionics software. In María\nAlpuente, Byron Cook, and Christophe Joubert, editors, For-\nmal Methods for Industrial Critical Systems, volume 5825 of Lec-\nture Notes in Computer Science, pages 53–69. Springer Berlin\nHeidelberg, 2009.\n[DK14]\nEva Darulova and Viktor Kuncak. Sound Compilation of Re-\nals. In Proceedings of the 41st ACM SIGPLAN-SIGACT Sympo-\nsium on Principles of Programming Languages, POPL ’14, pages\n235–248, New York, NY, USA, 2014. ACM.\n[Hal06]\nThomas C. Hales. Introduction to the ﬂyspeck project. In\nThierry Coquand, Henri Lombardi, and Marie-Françoise Roy,\n\n74\nBibliography\neditors, Mathematics, Algorithms, Proofs, number 05021 in\nDagstuhl Seminar Proceedings, Dagstuhl, Germany, 2006.\n[HGBK12]\nLeopold Haller, Alberto Griggio, Martin Brain, and Daniel\nKroening. Deciding ﬂoating-point logic with systematic ab-\nstraction. In Formal Methods in Computer-Aided Design (FM-\nCAD), pages 131–140, 2012.\n[Hig02]\nN.J. Higham. Accuracy and Stability of Numerical Algorithms:\nSecond Edition. Society for Industrial and Applied Mathemat-\nics, 2002.\n[IEE08]\nIEEE. IEEE Standard for Floating-Point Arithmetic. IEEE Std\n754-2008, pages 1–70, 2008.\n[Las01]\nJean-Bernard Lasserre. Global Optimization with Polynomi-\nals and the Problem of Moments. SIAM Journal on Optimiza-\ntion, 11(3):796–817, 2001.\n[Las06]\nJean B Lasserre. Convergent sdp-relaxations in polynomial\noptimization with sparsity.\nSIAM Journal on Optimization,\n17(3):822–843, 2006.\n[MAGW15] Victor Magron, Xavier Allamigeon, Stéphane Gaubert, and\nBenjamin Werner. Certiﬁcation of real inequalities: templates\nand sums of squares. Mathematical Programming, 151(2):477–\n506, 2015.\n[MCD17]\nVictor Magron, George Constantinides, and Alastair Donald-\nson. Certiﬁed roundoff error bounds using semideﬁnite pro-\ngramming. ACM Trans. Math. Software, 43(4):Art. 34, 31, 2017.\n[SJRG15]\nAlexey Solovyev, Charles Jacobsen, Zvonimir Rakamaric,\nand Ganesh Gopalakrishnan.\nRigorous Estimation of\nFloating-Point Round-off Errors with Symbolic Taylor Expan-\nsions.\nIn Nikolaj Bjorner and Frank de Boer, editors, Pro-\nceedings of the 20th International Symposium on Formal Methods\n(FM), volume 9109 of Lecture Notes in Computer Science, pages\n532–550. Springer, 2015.\n[YFN+10]\nM.\nYamashita,\nK.\nFujisawa,\nK.\nNakata,\nM.\nNakata,\nM. Fukuda, K. Kobayashi, and K. Goto. A high-performance\nsoftware package for semideﬁnite programs : SDPA7. Tech-\nnical report, Dept. of Information Sciences, Tokyo Inst. Tech.,\n2010.\n\nChapter 5\nApplication in deep\nnetworks\nThe Lipschitz constant of a network plays an important role in many ap-\nplications of deep learning, such as robustness certiﬁcation and Wasser-\nstein Generative Adversarial Network. We introduce an SDP hierarchy to\nestimate the global and local Lipschitz constant of a multiple layer deep\nneural network. The novelty is to combine a polynomial lifting for ReLU\nfunction derivatives with a weak generalization of Putinar’s positivity cer-\ntiﬁcate. We empirically demonstrate that our method provides a trade-off\nwith respect to the state-of-the-art LP approach, and in some cases we ob-\ntain better bounds in less time.\n5.1\nMultiple layer networks\nWe focus on multiple layer networks with ReLU activations. Recall that a\nfunction f, deﬁned on a convex set X ⊆Rn, is L-Lipschitz with respect to\nthe norm ∥· ∥if for all x, z ∈X , we have | f (x) −f (z)| ≤L∥x −z∥. The\nLipschitz constant of f with respect to norm ∥· ∥, denoted by L∥·∥\nf , is the\ninﬁmum of all those valid L’s:\nL∥·∥\nf\n:= inf {L : ∀x, z ∈X , | f (x) −f (z)| ≤L∥x −z∥}.\n(5.1)\nWe denote by F the multiple layer neural network, m the number of\nhidden layers, p0, p1, . . . , pm the number of nodes in the input layer and\neach hidden layer. For simplicity, (p0, p1, . . . , pm) will denote the layer\nstructure of network F. Let x0 be the initial input, and x1, . . . , xm be the\nactivation vectors in each hidden layer. Each xi, i ∈[m], is obtained by\n\n76\nChapter 5. Application in deep networks\na weight Ai, a bias bi, and an activation function a, i.e., xi = a(zi) with\nzi = Aixi−1 + bi; see Figure 5.1.\nx0 ∈Rp\nz0 ∈Rp\nz1 ∈Rp1\n. . .\nzm ∈Rpm\nFigure 5.1: Description of a multiple layer neural network.\nWe only consider coordinatewise application of the ReLU activation\nfunction, deﬁned as ReLU(x) = max {0, x} for x ∈R. The ReLU func-\ntion is non-smooth, and we deﬁne its generalized derivative as the set-\nvalued function G(x) such that G(x) = 1 for x > 0, G(x) = 0 for x < 0\nand G(x) = {0, 1} for x = 0.\nThe key reason why neural networks\nwith ReLU activation function can be tackled using polynomial optimiza-\ntion techniques is semialgebraicity of the ReLU function, i.e., it can be ex-\npressed with a system of polynomial (in)equalities. For x, u ∈R, we have\nu = ReLU(x) = max {0, x} if and only if u(u −x) = 0, u ≥x, u ≥0.\nSimilarly, one can exploit the semialgebraicity of its derivative ReLU′; see\nFigure 5.2 and Figure 5.3.\n-1\n-0.5\n0\n0.5\n1\nx\n-1\n-0.5\n0\n0.5\n1\nu\n(a) u = max {x, 0}\n-1\n-0.5\n0\n0.5\n1\nx\n-1\n-0.5\n0\n0.5\n1\nu\n(b) u(u −x) = 0, u ≥x, u ≥0\nFigure 5.2: ReLU (left) and its semialgebraicity (right).\n\n5.2. Lipschitz constants\n77\n-1\n-0.5\n0\n0.5\n1\nx\n-0.5\n0\n0.5\n1\n1.5\nu\n(a) u = 1{x≥0}\n-1\n-0.5\n0\n0.5\n1\nx\n-0.5\n0\n0.5\n1\n1.5\nu\n(b) u(u −1) = 0, (u −1\n2)x ≥0\nFigure 5.3: ReLU′ (left) and its semialgebraicity (right).\nWe assume that the last layer in our neural network is a softmax layer\nwith K entries, that is, the network is a classiﬁer for K labels. For each label\nk ∈{1, . . . , K}, the score of label k is obtained by an afﬁne product with\nthe last activation vector, i.e., c⊺\nk xm for some ck ∈Rpm. The ﬁnal output is\nthe label with the highest score, i.e., z = arg maxk c⊺\nk xm. The product xz of\ntwo vectors x and z is considered as the coordinate-wise product.\n5.2\nLipschitz constants\nSuppose we train a neural network F for K-classiﬁcations and denote by\nAi, bi, ck its parameters. Thus for an input x0 ∈Rp0, the targeted score of\nlabel k can be expressed as Fk(x0) = c⊺\nk xm, where xi = ReLU(Aixi−1 + bi),\nfor i ∈[m]. Let zi = Aixi−1 + bi for i ∈[m]. By applying the chain rule\non the non-smooth function Fk, we obtain a set valued map for Fk at point\nany x0 as GFk(x0) = (∏m\ni=1 A⊺\ni Diag(G(zi)))ck.\nWe ﬁx a targeted label (label 1 for example) and omit the symbol k for\nsimplicity. We deﬁne L∥·∥\nF\nas the supremum of the gradient’s dual norm:\nL∥·∥\nF\n=\nsup\nx0∈Ω, v∈GFk (x0)\n∥v∥∗= sup\nx0∈Ω\n\r\r\r\r\n\u0012 m\n∏\ni=1\nA⊺\ni Diag(G(zi))\n\u0013\nc\n\r\r\r\r\n∗\n,\n(5.2)\nwhere Ωis the convex input space, and ∥· ∥∗is the dual norm of ∥· ∥,\nwhich is deﬁned by ∥x∥∗:= sup∥t∥≤1 |⟨t, x⟩| for all x ∈Rn. In general,\nthe chain rule cannot be applied to composition of non-smooth functions\n[KL18, BP21]. Hence the formulation of GFk and (5.2) may lead to incor-\nrect gradients and bounds on the Lipschitz constant of the networks. The\nfollowing ensures that this is not the case and that the approach is sound.\n\n78\nChapter 5. Application in deep networks\nTheorem 5.1 If Ωis convex, then L∥·∥\nF\nis a Lipschitz constant for Fk on Ω.\nWhen Ω= Rn, L∥·∥\nF\nis the global Lipschitz constant of F with respect\nto norm ∥· ∥. In many cases we are also interested in the local Lipschitz\nconstant of a neural network constrained in a small neighborhood of a\nﬁxed input ¯x0. In this situation the input space Ωis often the ball around\n¯x0 ∈Rn with radius ε: Ω= {x : ∥x −¯x0∥≤ε}. In particular, with the\nL∞-norm (and using l ≤x ≤u ⇔(x −l)(x −u) ≤0), the input space Ω\nis the basic closed semialgebraic set:\nΩ= {x ∈Rn : (x −¯x0 + ε)(x −¯x0 −ε) ≤0}.\n(5.3)\nIn view of Theorem 5.1 and (5.2), the Lipschitz constant estimation problem\n(LCEP) for neural networks with respect to the norm ∥· ∥is the following\nPOP:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsup\nxi,ui,t\nt⊺\n\u0012\n∏m\ni=1 A⊺\ni Diag(ui)\n\u0013\nc\ns.t.\nui(ui −1) = 0, (ui −1/2)(Aixi−1 + bi) ≥0, i ∈[m]\nxi−1(xi−1 −Ai−1xi−2 −bi−1) = 0\nxi−1 ≥0, xi−1 ≥Ai−1xi−2 + bi−1, i = 2, . . . , m\nt2 ≤1, (x0 −¯x0 + ε)(x0 −¯x0 −ε) ≤0\n(5.4)\nIn [LRC20] the authors only use the constraint 0 ≤ui ≤1 on the vari-\nables ui, only capturing the Lipschitz character of the considered activa-\ntion function. We could use the same constraints, and this would allow\nto use activations which do not have semi-algebraic representations such\nas the Exponential Linear Unit (ELU). However, such a relaxation, despite\nvery general, is a lot coarser than the one we propose. Indeed, (5.4) treats\nan exact formulation of the generalized derivative of the ReLU function by\nexploiting its semialgebraic character.\n5.3\nNearly sparse problems\nFor illustration purpose, consider 1-hidden layer networks. Then in (5.4)\nwe can deﬁne natural subsets Ii = {u(i)\n1 , x0}, i ∈[p1] (with resepct to\nconstraints u1(u1 −1) = 0, (u1 −1/2)(A1x0 + b1) ≥0, and (x0 −¯x0 +\nε)(x0 −¯x0 −ε) ≤0); and Jj = {t(j)}, j ∈[p0] (with resepct to constraints\nt2 ≤1). Clearly, Ii, Jj satisfy the RIP condition (1.8) and are subsets with\nsmallest possible size. Recall that x0 ∈Rp0. Hence |Ii| = 1 + p0 and\n\n5.3. Nearly sparse problems\n79\nthe maximum size of the PSD matrices involved in the sparse Lasserre’s\nhierarchy Pr\ncs, given in (3.6), is (1+p0+r\nr\n). Therefore, as in real deep neural\nnetworks p0 can be as large as 1000, the second-order sparse Lasserre’s\nhierarchy P2\ncs, cannot be implemented in practice.\nIn fact (5.4) can be considered as a “nearly sparse” POP, i.e., a sparse\nPOP with some additional “bad\" constraints that violate the sparsity as-\nsumptions. More precisely, suppose that f, gi and subsets Ik satisfy As-\nsumption 3.1. Let g be a polynomial that violates Assumption 3.1 (iii), i.e.,\nRIP (1.8). Then we call the POP\ninf\nx∈Rn{ f (x) : g(x) ≥0, gi(x) ≥0, i ∈[m]},\n(Nearly)\na nearly sparse POP because only one constraint, namely g ≥0, does not\nsatisfy the sparsity pattern corresponding to the RIP (1.8).\nThis single\n“bad\" constraint g ≥0 precludes us from applying the sparse Lasserre\nhierarchy (3.6).\nIn this situation, we propose a heuristic method which can be applied\nto problems with arbitrary many constraints that possibly destroy the spar-\nsity pattern. The key idea of our algorithm is: (i) Keep the “nice\" sparsity\npattern deﬁned without the bad constraints; (ii) Associate only low-order\nlocalizing matrix constraints to the “bad” constraints. In brief, the r-th\norder heuristic hierarchy (HR-r) reads as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\ny\nLy( f )\ns.t.\nM1(y) ⪰0\nMr(y, Ik) ⪰0,\nk ∈[l]\nMr−di(gi y, Ik(i)) ⪰0,\ni ∈[m]\nLy(g) ≥0,\ny0 = 1\n(HR-r)\nWe already have a sparsity pattern with subsets Ik and an additional “bad”\nconstraint g ≥0 (assumed to be quadratic). Then we consider the sparse\nmoment relaxations (3.6) applied to (Nearly) without the bad constraint\ng ≥0 and simply add two constraints: (i) the moment constraint M1(y) ⪰\n0 (with full dense ﬁrst-order moment matrix M1(y)), and (ii) the linear mo-\nment inequality constraint Ly(g) ≥0 (which is the lowest-order localizing\nmatrix constraint M0(g y) ⪰0).\nTo see why the full moment constraint M1(y) ⪰0 is needed, consider\nthe following toy problem:\ninf\nx∈R3{x1x2 + x2x3 : x2\n1 + x2\n2 ≤1, x2\n2 + x2\n3 ≤1}.\n(5.5)\nDeﬁne the subsets I1 = {1, 2}, I2 = {2, 3}. It is easy to check that Assump-\ntion 3.1 holds. Deﬁne\ny = {y000, y100, y010, y001, y200, y110, y101, y020, y011, y002} ∈R10.\n\n80\nChapter 5. Application in deep networks\nFor r = 1, the ﬁrst-order dense moment matrix reads as\nM1(y) =\n\n\ny000\ny100\ny010\ny001\ny100\ny200\ny110\ny101\ny010\ny110\ny020\ny011\ny001\ny101\ny011\ny002\n\n,\nwhereas the sparse moment matrix M1(y, I1) (resp. M1(y, I2)) is the sub-\nmatrix of M1(y) taking red and pink (resp. blue and pink) entries. That is,\nM1(y, I1) and M1(y, I2) are submatrices of M1(y), obtained by restricting\nto rows and columns concerned with subsets I1 and I2 only.\nNow suppose that we need to consider an additional “bad” constraint\n(1 −x1 −x2 −x3)2 = 0. After developing Ly(g), one needs to consider the\nmoment variable y101 corresponding to the monomial x1x3 in the expan-\nsion of g = (1 −x1 −x2 −x3)2, and y101 does not appear in the moment\nmatrices M1(y, I1) and M1(y, I2) because x1 and x3 are not in the same\nsubset. However y101 appears in M1(y), which is of size n + 1.\nNow let us see how this works for problem (5.4). First introduce new\nvariables zi with associated constraints zi −Aixi−1 −bi = 0, so that all\n“bad” constraints are afﬁne. Equivalently, we may and will consider the\nsingle “bad\" constraint g ≥0 with g(z1, . . . , x0, x1, . . .) = −∑i ∥zi −Axi−1 −\nbi∥2 and solve (HR-r). We brieﬂy sketch the rationale behind this refor-\nmulation. Let (yr)r∈N be a sequence of optimal solutions of (HR-r). If\nr →∞, then yr →y (possibly for a subsequence (rk)k∈N), and y corre-\nsponds to the moment sequence of a measure µ, supported on {(x, z) :\ngi(x, z) ≥0, i ∈[p]; R\ng dµ ≥0}. But as −g is a square, R\ng dµ ≥0 im-\nplies g = 0, µ-a.e., and therefore zi = Axi−1 + bi, µ-a.e.. This is why we do\nnot need to consider the higher-order constraints Mr(g y) ⪰0 for r > 0;\nonly M0(g y) ⪰0 (⇔Ly(g) ≥0) sufﬁces. In fact, we impose the stronger\nlinear constraints Ly(g) = 0 and Ly(zi −Axi−1 −bi) = 0 for all i ∈[p].\nFor simplicity, assume that the neural networks have only one single\nhidden layer, i.e., m = 1. Denote by A, b the weight and bias respectively.\nAs in (5.3), we use the fact that l ≤x ≤u is equivalent to (x −l)(x −u) ≤\n0. Then the local Lipschitz constant estimation problem with respect to\nL∞-norm can be written as\n\n\n\n\n\n\n\n\n\n\n\nsup\nx,u,z,t\nt⊺A⊺Diag(u)c\ns.t.\n(z −Ax −b)2 = 0,\nt2 ≤1\n(x −¯x0 + ε)(x −¯x0 −ε) ≤0\nu(u −1) = 0,\n(u −1/2)z ≥0\n(LCEP1)\nDeﬁne the subsets of (LCEP1) to be Ii = {xi, ti}, Jj = {uj, zj} for i ∈\n[p0], j ∈[p1], where p0, p1 are the number of nodes in the input layer\n\n5.4. Overview of numerical experiments\n81\nand the hidden layer, respectively. Then the second-order (r = 2) heuristic\nrelaxation of (LCEP1) is the following SDP:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\ny\nLy(t⊺A⊺Diag(u)c)\ns.t.\ny0 = 1,\nM1(y) ⪰0\nLy(z −Ax −b) = 0,\nLy((z −Ax −b)2) = 0\nM1(−(x(i) −¯x(i)\n0 + ε)(x(i) −¯x(i)\n0 −ε)y, Ii) ⪰0,\ni ∈[p0]\nM1((1 −t2\ni )y, Ii) ⪰0,\nM2(y, Ii) ⪰0,\ni ∈[p0]\nM2(y, Jj) ⪰0,\nM1(uj(uj −1)y, Jj) = 0,\nj ∈[p1]\nM1((uj −1/2)zjy, Jj) ⪰0,\nj ∈[p1]\n(HR-2)\nThe r-th order heuristic relaxation (HR-r) also applies to multiple layer\nneural networks. However, if the neural network has m hidden layers,\nthen the criterion in (5.4) is of degree m + 1. If m ≥2, then the ﬁrst-\norder moment matrix M1(y) is no longer sufﬁcient, as moments of degree\n> 2 are not encoded in M1(y) and some may not be encoded in the mo-\nment matrices M2(y, Ii), if they include variables of different subsets. See\n[CLMPa, Appendix E] for more information to deal with higher-degree\npolynomial objective.\n5.4\nOverview of numerical experiments\nIn this section, we provide results for the global and local Lipschitz con-\nstants of random networks of ﬁxed size (80, 80) and with various sparsities.\nWe also compute bounds of a real trained 1-hidden layer network. For\nall experiments we focus on the L∞-norm, the most interesting case for\nrobustness certiﬁcation. Moreover, we use the Lipschitz constants com-\nputed by various methods to certify robustness of a trained network, and\ncompare the ratio of certiﬁed inputs with different methods. Let us ﬁrst\nprovide an overview of the methods with which we compare our results:\n• SHOR: First-order dense moment relaxation (also called Shor’s re-\nlaxation) applied to (5.4).\n• HR-1/2: ﬁrst/second-order heuristic relaxation applied to (5.4).\n• LP-3/4: LP-based method, called LipOpt, by [LRC20] with degree\n3/4, which corresponds to (3.9) without SOS multipliers.\n• LBS: lower bound obtained by sampling 50000 random points and\nevaluating the dual norm of the gradient.\nThe reason why we list LBS here is because LBS is a valid lower bound on\nthe Lipschitz constant. Therefore all methods should provide a result not\nlower than LBS, a basic necessary condition of consistency.\n\n82\nChapter 5. Application in deep networks\nAs discussed earlier, if we want to estimate the global Lipschitz con-\nstant, we need the input space Ωto be the whole space. In consideration\nof numerical issues, we set Ωto be the ball of radius 10 around the origin.\nFor the local Lipschitz constant, we set by default the radius of the input\nball as ε = 0.1. In both cases, we compute the Lipschitz constant with\nrespect to the ﬁrst label. We use the (Python) code provided by [LRC20]1\nto execute the experiments for LipOpt with the Gurobi solver. For HR-2\nand SHOR, we use the YALMIP toolbox (MATLAB) [Lö04] with MOSEK\nas a backend to calculate the Lipschitz constants for random networks. For\ntrained network, we implement our algorithm in Julia with the MOSEK op-\ntimizer to accelerate the computation. In the following, running time is\nreferred to the time taken by the LP/SDP solver (Gurobi/MOSEK) and “-”\nmeans running out of memory during solving the LP/SDP model.\nIn [LRC20] a certain sparsity structure arising from a neural network\nwas exploited. Consider a neural network F with one single hidden layer,\nand 4 nodes in each layer. The network F is said to have a sparsity ω = 4\nif its weight matrix A is symmetric with diagonal blocks of size at most\n2 × 2:\n\n\n∗\n∗\n0\n0\n∗\n∗\n∗\n0\n0\n∗\n∗\n∗\n0\n0\n∗\n∗\n\n.\n(5.6)\nLarger sparsity values refer to symmetric matrices with band structure\nof a given size. This sparsity structure (5.6) of the networks greatly in-\nﬂuences the number of variables involved in the LP program to solve in\n[LRC20]. This is in deep contrast with our method which does not re-\nquire the weight matrix to be as in (5.6).\nHence when the network is\nfully-connected, our method is more efﬁcient and provides tighter upper\nbounds.\n5.4.1\nLipschitz Constant Estimation\nRandom networks.\nTable 5.1 gives a brief comparison outlook of the re-\nsults obtained by our method and the method in [LRC20]. For (80, 80) net-\nworks, apart from ω = 20, which is not signiﬁcative, HR-2 obtains much\nbetter bounds and is also much more efﬁcient than LP-3. LP-4 provides\ntighter bounds than HR-2 but suffers more computational time, and run\nout of memory when the sparsity increases. For (40, 40, 10) networks, HR-\n1 is a trade-off between LP-3 and LP-4, it provides tighter (resp. looser)\nbounds than LP-3 (resp. LP-4), but takes more (resp. less) computational\ntime.\n1https://openreview.net/forum?id=rJe4_xSFDB.\n\n5.4. Overview of numerical experiments\n83\nTable 5.1: Bounds of global Lipschitz constants (opt) and solver running\ntime (in seconds) of networks of size (80, 80) and (40, 40, 10).\n(80, 80)\n(40, 40, 10)\nω = 20\nω = 40\nω = 60\nω = 80\nω = 20\nω = 40\nω = 60\nω = 80\nHR-2\nopt\n1.45\n2.05\n2.41\n2.68\nHR-1\n0.50\n1.16\n1.82\n2.05\ntime\n3.14\n7.78\n8.61\n9.82\n271.34\n165.68\n174.86\n174.02\nLP-3\nopt\n1.55\n2.86\n3.85\n4.68\nLP-3\n0.56\n1.68\n3.01\n3.57\ntime\n2.44\n10.36\n20.99\n71.49\n3.84\n4.83\n7.91\n6.33\nLP-4\nopt\n1.43\n-\n-\n-\nLP-4\n0.29\n0.85\n-\n-\ntime\n127.99\n-\n-\n-\n321.89\n28034.27\n-\n-\nLBS\nopt\n1.05\n1.56\n1.65\n1.86\nLBS\n0.20\n0.48\n0.61\n0.62\nTable 5.2: Comparison of bounds of global Lipschitz constants and solver\nrunning time on trained network SDP-NN obtained by HR-2, SHOR, LP-3\nand LBS. The network is a fully connected neural network with one hid-\nden layer, with 784 nodes in the input layer and 500 nodes in the hid-\nden layer. The network is for 10-classiﬁcation, and we calculate the upper\nbound with respect to label 2.\nGlobal\nLocal\nHR-2\nSHOR\nLP-3\nLBS\nHR-2\nSHOR\nLP-3\nLBS\nopt\n14.56\n17.85\n-\n9.69\n12.70\n16.07\n-\n8.20\ntime\n12246\n2869\n-\n20596\n4217\n-\nTrained Network\nHere we use the MNIST classiﬁer SDP-NN described\nin [RSL18a]2. The network is of size (784, 500). In Table 5.2, we see that\nthe LP-3 algorithm runs out of memory when applied to the real network\nSDP-NN to compute the global Lipschitz bound. In contrast, SHOR and\nHR-2 still work and moreover, HR-2 provides tighter upper bounds than\nSHOR in both global and local cases. As a trade-off, the running time of\nHR-2 is around 5 times longer than that of SHOR.\n5.4.2\nRobustness certiﬁcation\nMulti-Classiﬁer\nThe above SDP-NN network is a well-trained (784, 500)\nnetwork to classify the digit images from 0 to 9. Denote the parameters of\nthis network by A ∈R500×784, b1 ∈R500, C ∈R10×500, b2 ∈R10. The\nscore of an input x is denoted by yx, i.e., yx = C · ReLU(Ax0 + b1) +\nb2. The label of x, denoted by rx, is the index with the largest score, i.e.,\n2https://worksheets.codalab.org/worksheets/0xa21e794020bb474d8804ec7bc0543f52/\n\n84\nChapter 5. Application in deep networks\nrx = arg max yx. Suppose an input x0 has label r. For ε and x such that\n∥x −x0∥∞≤ε, if for all i ̸= r, yx\ni −yx\nr < 0, then x0 is ε-robust. Alternatively,\ndenote by Lx0,ε\ni,r\nthe local Lipschitz constant of function fi,r(x) = yx\ni −yx\nr\nwith respect to L∞-norm in the ball {x : ∥x −x0∥∞≤ε}. Then the point\nx0 is ε-robust if for all i ̸= r, fi,r(x0) + εLx0,ε\ni,r\n< 0. Since the 28 × 28 MNIST\nimages are ﬂattened and normalized into vectors taking value in [0, 1], we\ncompute the local Lipschitz constant (by HR-2) with respect to x0 = 0 and\nε = 2, the complete value is referred to the following matrix:\nL =\n\n\n∗\n7.94\n7.89\n8.28\n8.64\n8.10\n7.66\n8.04\n7.46\n8.14\n7.94\n∗\n7.74\n7.36\n7.68\n8.81\n8.06\n7.55\n7.36\n8.66\n7.89\n7.74\n∗\n7.63\n8.81\n10.23\n8.18\n8.13\n7.74\n9.08\n8.28\n7.36\n7.63\n∗\n8.52\n7.74\n9.47\n8.01\n7.37\n7.96\n8.64\n7.68\n8.81\n8.52\n∗\n9.44\n7.98\n8.65\n8.49\n7.47\n8.10\n8.81\n10.23\n7.74\n9.44\n∗\n8.26\n9.26\n8.17\n8.55\n7.66\n8.06\n8.18\n9.47\n7.98\n8.26\n∗\n10.18\n8.00\n9.83\n8.04\n7.55\n8.13\n8.01\n8.65\n9.26\n10.18\n∗\n8.28\n7.65\n7.46\n7.36\n7.74\n7.37\n8.49\n8.17\n8.00\n8.28\n∗\n7.87\n8.14\n8.66\n9.08\n7.96\n7.47\n8.55\n9.83\n7.65\n7.87\n∗\n\n\nwhere L = (Lij)i̸=j. Note that if we replace the vector c in (5.4) by −c, the\nproblem is equivalent to the original one. Therefore, the matrix L is sym-\nmetric, and we only need to compute 45 Lipschitz constants (the upper\ntriangle of L).\nFigure 5.4 shows several certiﬁed and non-certiﬁed examples taken\nfrom the MNIST test dataset.\nFigure 5.4: Examples of certiﬁed points (above) and non-certiﬁed points\n(bellow).\nWe take different values of ε from 0.02 to 0.1, and compute the ratio of\ncertiﬁed examples among the 10000 MNIST test data by the Lipschitz con-\nstants we obtain, as shown in Table 5.3. Note that for ε = 0.1, we improve\na little bit by 67% compared to Grad-cert (65%) described in [RSL18a], as\nwe use an exact formulation of the derivative of ReLU function.\n\n5.5. Notes and sources\n85\nTable 5.3: Ratios of certiﬁed test examples for SDP-NN network by HR-2.\nε\n0.02\n0.04\n0.06\n0.08\n0.1\nRatios\n97.24%\n92.84%\n87.10%\n78.34%\n67.63%\n5.5\nNotes and sources\nVarious optimization frameworks have been recently used to certify the\nrobustness of deep neural networks, including SDP [RSL18b, FMP20], LP\n[DSG+18, WK18], mixed integer programming (MIP) [TXT17], outer poly-\ntope approximation [BWC+19, ZWC+18, WZC+18b, WZC+18a], and av-\neraged activation operators [CP20].\nUpper bounds on Lipschitz constants of deep networks can be ob-\ntained by a product of the layer-wise Lipschitz constants [HCC18]. This is\nhowever extremely loose and has many limitations. Note that [VS18] pro-\nposed an improvement via a ﬁner product. Departing from this approach,\n[LRC20] proposed a quadratically constrained quadratic program (QCQP)\nformulation to estimate the Lipschitz constant of neural networks. Shor’s\nrelaxation allows to obtain a valid upper bound. Alternatively, using the\nLP hierarchy, [LRC20] obtains tighter upper bounds. By another SDP-\nbased method, [FRH+19] provides an upper bound of the Lipschitz con-\nstant. However this method is restricted to the L2-norm whereas most ro-\nbustness certiﬁcation problems in deep learning are rather concerned with\nthe L∞-norm. The heuristic approach presented in this chapter can also ap-\nply to nearly sparse polynomial optimization problems, coming from the\ngeneral optimization literature [CLMP22], or for other types of networks,\nsuch as monotone deep equilibrium networks [CLMPb].\nThe proof of Theorem 5.1 is available in [CLMPa, Appendix 1]. The in-\nterested reader can ﬁnd more complete results for global/local Lipschitz\nconstants of both 1-hidden layer and 2-hidden layer networks with vari-\nous sizes and sparsities in [CLMPa, Appendix F and G].\n\n\nBibliography\n[BP21]\nJérôme Bolte and Edouard Pauwels. Conservative set valued\nﬁelds, automatic differentiation, stochastic gradient methods\nand deep learning. Mathematical Programming, 188(1):19–51,\n2021.\n[BWC+19]\nAkhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu,\nand Luca Daniel. Cnn-cert: An efﬁcient framework for cer-\ntifying robustness of convolutional neural networks. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence, vol-\nume 33, pages 3240–3247, 2019.\n[CLMPa]\nT. Chen, J.-B. Lasserre, V. Magron, and E. Pauwels. Semi-\nalgebraic Optimization for Bounding Lipschitz Constants of\nReLU Networks. Proceeding of Advances in Neural Information\nProcessing Systems 33 (NeurIPS 2020).\n[CLMPb]\nT. Chen, J.-B. Lasserre, V. Magron, and E. Pauwels. Semialge-\nbraic Representation of Monotone Deep Equilibrium Models\nand Applications to Certiﬁcation. Proceeding of Advances in\nNeural Information Processing Systems 34 (NeurIPS 2021).\n[CLMP22]\nTong Chen, Jean-Bernard Lasserre, Victor Magron, and\nEdouard Pauwels.\nA sublevel moment-sos hierarchy for\npolynomial optimization. Computational Optimization and Ap-\nplications, 81(1):31–66, 2022.\n[CP20]\nPatrick L Combettes and Jean-Christophe Pesquet. Lipschitz\ncertiﬁcates for layered network structures driven by aver-\naged activation operators.\nSIAM Journal on Mathematics of\nData Science, 2(2):529–557, 2020.\n[DSG+18]\nKrishnamurthy Dvijotham, Robert Stanforth, Sven Gowal,\nTimothy A Mann, and Pushmeet Kohli.\nA dual approach\nto scalable veriﬁcation of deep networks. In UAI, pages 550–\n559, 2018.\n\n88\nBibliography\n[FMP20]\nMahyar Fazlyab, Manfred Morari, and George J Pappas.\nSafety veriﬁcation and robustness analysis of neural net-\nworks via quadratic constraints and semideﬁnite program-\nming. IEEE Transactions on Automatic Control, 2020.\n[FRH+19]\nMahyar Fazlyab, Alexander Robey, Hamed Hassani, Man-\nfred Morari, and George J. Pappas.\nEfﬁcient and accurate\nestimation of lipschitz constants for deep neural networks,\n2019.\n[HCC18]\nTodd Huster, Cho-Yu Jason Chiang, and Ritu Chadha. Limi-\ntations of the lipschitz constant as a defense against adversar-\nial examples. In Joint European Conference on Machine Learning\nand Knowledge Discovery in Databases, pages 16–29. Springer,\n2018.\n[KL18]\nSham M Kakade and Jason D Lee. Provably correct automatic\nsub-differentiation for qualiﬁed programs.\nIn Advances in\nneural information processing systems, pages 7125–7135, 2018.\n[LRC20]\nFabian Latorre, Paul Rolland, and Volkan Cevher. Lipschitz\nconstant estimation of neural networks via sparse polyno-\nmial optimization. In International Conference on Learning Rep-\nresentations, 2020.\n[Lö04]\nJ. Löfberg. Yalmip : A toolbox for modeling and optimization\nin MATLAB. In Proceedings of the CACSD Conference, Taipei,\nTaiwan, 2004.\n[RSL18a]\nAditi Raghunathan, Jacob Steinhardt, and Percy Liang. Cer-\ntiﬁed defenses against adversarial examples. In International\nConference on Learning Representations, 2018.\n[RSL18b]\nAditi Raghunathan, Jacob Steinhardt, and Percy Liang.\nSemideﬁnite relaxations for certifying robustness to adver-\nsarial examples. In Advances in Neural Information Processing\nSystems, pages 10877–10887, 2018.\n[TXT17]\nVincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating ro-\nbustness of neural networks with mixed integer program-\nming. arXiv preprint arXiv:1711.07356, 2017.\n[VS18]\nAladin Virmaux and Kevin Scaman. Lipschitz regularity of\ndeep neural networks: Analysis and efﬁcient estimation. In\nAdvances in Neural Information Processing Systems, pages 3835–\n3844, 2018.\n\nBibliography\n89\n[WK18]\nEric Wong and Zico Kolter. Provable defenses against adver-\nsarial examples via the convex outer adversarial polytope. In\nInternational Conference on Machine Learning, pages 5286–5295.\nPMLR, 2018.\n[WZC+18a] Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui\nHsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon. To-\nwards fast computation of certiﬁed robustness for relu net-\nworks. In International Conference on Machine Learning, pages\n5276–5285. PMLR, 2018.\n[WZC+18b] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong\nSu, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating\nthe robustness of neural networks: An extreme value theory\napproach. arXiv preprint arXiv:1801.10578, 2018.\n[ZWC+18]\nHuan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh,\nand Luca Daniel. Efﬁcient neural network robustness certiﬁ-\ncation with general activation functions. In Advances in neural\ninformation processing systems, pages 4939–4948, 2018.\n\n\nChapter 6\nNoncommutative\noptimization and quantum\ninformation\nIn this chapter, we handle a speciﬁc class of sparse POPs with noncommut-\ning variables, and adapt the concept of CS from Section 3 in this setting. A\nconverging hierarchy of semideﬁnite relaxations for eigenvalue optimiza-\ntion is provided. The Gelfand-Naimark-Segal (GNS) construction is ap-\nplied to extract optimizers if ﬂatness and irreducibility conditions are sat-\nisﬁed. Among the main techniques used are amalgamation results from\noperator algebra. The theoretical results are utilized to compute lower\nbounds on minimal eigenvalue of noncommutative polynomials from the\nliterature, in particular arising from quantum information theory.\n6.1\nNoncommutative polynomials\nWe consider a ﬁnite alphabet x1, . . . , xn (called noncommutating variables)\nand generate all possible words (monomials) of ﬁnite length in these let-\nters.\nThe empty word is denoted by 1.\nThe resulting set of words is\n⟨x⟩, with x = (x1, . . . , xn). We denote by R⟨x⟩the ring of real polyno-\nmials in the noncommutating variables x. An element in R⟨x⟩is called\na noncommutative (nc) polynomial. The support of an nc polynomial f =\n∑w∈⟨x⟩aww is deﬁned by supp( f ) := {w ∈⟨x⟩| aw ̸= 0} and the degree of\nf, denoted by deg( f ), is the length of the longest word in supp( f ). The set\nof nc polynomials of degree at most r is denoted by R⟨x⟩r. Let us denote\nby Wr the vector of all words of degree at most r with resepct to the lex-\nicographic order. Note that Wr serves as a monomial basis of R⟨x⟩r and\n\n92\nChapter 6. Noncommutative optimization and quantum information\nthe length of Wr is equal to σ(n, r) := ∑r\ni=0 ni = nr+1−1\nn−1 . The ring R⟨x⟩is\nequipped with the involution ⋆that ﬁxes R ∪{x1, . . . , xn} point-wise and\nreverses words, so that R⟨x⟩is the ⋆-algebra freely generated by n sym-\nmetric letters x1, . . . , xn. For instance (x1x2 + x2\n2 + 1)⋆= x2x1 + x2\n2 + 1. The\nset of all symmetric elements is deﬁned as Sym R⟨x⟩:= { f ∈R⟨x⟩| f =\nf ⋆}. A simple example of element of Sym R⟨x⟩is x1x2 + x2x1 + x2\n2 + 1.\nAn nc polynomial of the form g⋆g is called an hermitian square. A given\nf ∈Sym R⟨x⟩is a sum of Hermitian squares (SOHS) if there exist nc poly-\nnomials h1, . . . , ht ∈R⟨x⟩such that f = h⋆\n1h1 + · · · + h⋆\nt ht. Let Σ⟨x⟩stand\nfor the set of SOHS. We denote by Σ⟨x⟩r ⊆Σ⟨x⟩the set of SOHS polyno-\nmials of degree at most 2r. We now recall how to check whether a given\nf ∈Sym R⟨x⟩is an SOHS. The existing procedure, known as the Gram\nmatrix method, relies on the following proposition.\nProposition 6.1 Assume that f ∈Sym R⟨x⟩is of degree at most 2d. Then\nf ∈Σ⟨x⟩if and only if there exists G f ⪰0 satisfying\nf = W⋆\nd G f Wd.\n(6.1)\nConversely, given such G f ⪰0 of rank t, one can construct g1, . . . , gt ∈R⟨x⟩\nof degree at most d such that f = ∑t\ni=1 g⋆\ni gi.\nAny symmetric matrix G f (not necessarily positive semideﬁnite) satisfy-\ning (6.1) is called a Gram matrix of f.\nGiven a set of nc polynomials g = {g1, . . . , gm} ⊆Sym R⟨x⟩, the nc\nsemialgebraic set Dg associated to g is deﬁned as follows:\nDg :=\n[\nk∈N∗\n{A = (A1, . . . , An) ∈(Sk)n | gj(A) ⪰0, j ∈[m]}.\n(6.2)\nWhen considering only tuples of k × k symmetric matrices, we use the\nnotation Dk\ng := Dg ∩(Sk)n. The operator semialgebraic set D∞\ng is the set of\nall bounded self-adjoint operators A on a Hilbert space H endowed with\na scalar product ⟨· | ·⟩, making g(A) a positive semideﬁnite operator for\nall g ∈g, i.e., ⟨g(A)v | v⟩≥0, for all v ∈H. We say that an nc polynomial\nf is positive (denoted by f ≻0) on D∞\ng if for all A ∈D∞\ng the operator\nf (A) is positive deﬁnite, i.e., ⟨f (A)v | v⟩> 0, for all nonzero v ∈H. The\nquadratic module M(g), generated by g, is deﬁned by\nM(g) :=\n(\nt\n∑\ni=1\na⋆\ni giai | t ∈N∗, ai ∈R⟨x⟩, gi ∈g ∪{1}\n)\n.\n(6.3)\nGiven r ∈N∗, the truncated quadratic module M(g)r of order r, gener-\nated by g, is\nM(g)r :=\n(\nt\n∑\ni=1\na⋆\ni giai | t ∈N∗, ai ∈R⟨x⟩, gj ∈g ∪{1}, deg(a⋆\ni giai) ≤2r\n)\n.\n(6.4)\n\n6.2. Correlative sparsity patterns\n93\nA quadratic module M is said to be Archimedean if for each a ∈R⟨x⟩, there\nexists N > 0 such that N −a⋆a ∈M. One can show that this is equivalent\nto the existence of an N > 0 such that N −∑n\ni=1 x2\ni ∈M.\nWe also recall the nc analog of Putinar’s Positivstellensatz (Theorem\n2.3) describing nc polynomials positive on D∞\ng with Archimedean M(g).\nTheorem 6.2 (Helton-McCullough) Let { f } ∪g ⊆Sym R⟨x⟩and assume\nthat M(g) is Archimedean. If f (A) ≻0 for all A ∈D∞\ng , then f ∈M(g).\n6.2\nCorrelative sparsity patterns\nWe rely on the same CS framework as in Chapter 3. More concretely, as-\nsuming f = ∑w aww ∈Sym R⟨x⟩and g = {g1, . . . , gm} ⊆Sym R⟨x⟩, we\ndeﬁne the csp graph associated with f and g to be the graph Gcsp with\nnodes V = [n] and with edges E satisfying {i, j} ∈E if one of following\nconditions holds:\n(i) there exists w ∈supp( f ) s.t. xi, xj ∈var(w);\n(ii) there exists k ∈[m] s.t. xi, xj ∈var(gk),\nwhere we use var(g) to denote the set of variables effectively involved in\ng ∈R⟨x⟩. Let (Gcsp)′ be a chordal extension of Gcsp and Ik, k ∈[p] be the\nmaximal cliques of (Gcsp)′ with cardinality being denoted by nk, k ∈[p].\nWe denote by ⟨x(Ik)⟩(resp. R⟨x, Ik⟩) the set of words (resp. nc polynomi-\nals) in the nk variables x(Ik) = {xi : i ∈Ik}. We also deﬁne Sym R⟨x, Ik⟩:=\nSym R⟨x⟩∩R⟨x, Ik⟩. Let Σ⟨x, Ik⟩stand for the set of SOHS in R⟨x, Ik⟩and\nwe denote by Σ⟨x, Ik⟩r the restriction of Σ⟨x, Ik⟩to nc polynomials of de-\ngree at most 2r. In the sequel, we will rely on two speciﬁc assumptions.\nThe ﬁrst one is as follows.\nAssumption 6.3 (Boundedness) Let Dg be as in (6.2). There exists N > 0\nsuch that ∑n\ni=1 x2\ni ⪯N, for all x ∈D∞\ng .\nThen, Assumption 6.3 implies that ∑j∈Ik x2\nj ⪯N, for all k ∈[p]. Thus we\ndeﬁne\ngm+k := N −∑\nj∈Ik\nx2\nj ,\nk ∈[p],\n(6.5)\nand set m′ = m + p in order to describe the same set Dg again as\nDg :=\n[\nk∈N∗\n{A ∈(Sk)n | gj(A) ⪰0, j ∈[m′]},\n(6.6)\nas well as the operator semialgebraic set D∞\ng .\nThe second assumption, which is the strict nc analog of Assumption\n3.1 (i)–(iii), is as follows.\n\n94\nChapter 6. Noncommutative optimization and quantum information\nAssumption 6.4 Let Dg be as in (6.6) and let f ∈Sym R⟨x⟩. The index set\nJ := {1, . . . , m′} is partitioned into p disjoint sets J1, . . . , Jp and the two collec-\ntions {I1, . . . , Ip} and {J1, . . . , Jp} satisfy\n(i) The objective function f can be decomposed as f = f1 + · · · + fp, with\nfk ∈Sym R⟨x, Ik⟩for all k ∈[p];\n(ii) For all k ∈[p] and j ∈Jk, gj ∈Sym R⟨x, Ik⟩;\n(iii) The RIP (1.8) holds for I1, . . . , Ip (possibly after some reordering).\n6.3\nNoncommutative moment and localizing ma-\ntrices\nGiven a sequence y = (yw)w∈W2r ∈Rσ(n,2r) (here we allow r = ∞), let\nus deﬁne the linear functional Ly : R⟨x⟩2r →R by Ly( f ) := ∑w awyw, for\nevery polynomial f = ∑w aww of degree at most 2r. The sequence y is said\nto be unital if y1 = 1 and is said to be symmetric if yw⋆= yw for all w ∈W2r.\nSuppose g ∈Sym R⟨x⟩with deg(g) ≤2r. We further associate to y the\nfollowing two matrices:\n(1) the (noncommutative) moment matrix Mr(y) is the matrix indexed by\nwords u, v ∈Wr, with [Mr(y)]u,v = Ly(u⋆v) = yu⋆v;\n(2) the localizing matrix Mr−⌈deg(g)/2⌉(gy) is the matrix indexed by words\nu, v ∈Wr−⌈deg(g)/2⌉, with [Mr−⌈deg(g)/2⌉(gy)]u,v = Ly(u⋆gv).\nWe recall the following useful facts.\nLemma 6.5 Let g ∈Sym R⟨x⟩with deg(g) ≤2r and let L be the linear func-\ntional associated to a symmetric sequence y := (yw)w∈W2r ∈Rσ(n,2r). Then,\n(1) Ly(h⋆h) ≥0 for all h ∈R⟨x⟩r if and only if the moment matrix Mr(y) ⪰\n0;\n(2) Ly(h⋆gh) ≥0 for all h ∈R⟨x⟩r−⌈deg(g)/2⌉if and only if the localizing\nmatrix Mr−⌈deg(g)/2⌉(gy) ⪰0.\nDeﬁnition 6.6 Let y = (yw)w∈W2r+2δ ∈Rσ(n,2r+2δ) and ˜y = (yw)w∈W2r be\nits truncation. We can write the moment matrix Mr+δ(y) in block form:\nMr+δ(y) =\n\u0014\nMr(˜y)\nB\nB⊺\nC\n\u0015\n.\nWe say that y is δ-ﬂat or that y is a ﬂat extension of ˜y, if Mr+δ(y) is ﬂat over\nMr(˜L), i.e., if rank Mr+δ(y) = rank Mr(˜y).\n\n6.4. Sparse representations\n95\nFor a subset I ⊆[n], let us deﬁne Mr(y, I) to be the moment submatrix\nobtained from Mr(y) after retaining only those rows and columns indexed\nby w ∈⟨x(I)⟩r. For g ∈R⟨x, I⟩with deg(g) ≤2r, we also deﬁne the\nlocalizing submatrix Mr−⌈deg(g)/2⌉(gy, I) in a similar fashion.\n6.4\nSparse representations\nHere, we state our main theoretical result, which is a sparse version of the\nHelton-McCullough Positivstellensatz (Theorem 6.2). For this, we rely on\namalgamation theory for C⋆-algebras.\nGiven a Hilbert space H, we denote by B(H) the set of bounded opera-\ntors on H. A C⋆-algebra is a complex Banach algebra A (thus also a Banach\nspace), endowed with a norm ∥· ∥, and with an involution ⋆satisfying\n∥xx⋆∥= ∥x∥2 for all x ∈A. Equivalently, it is a norm closed subalgebra\nwith involution of B(H) for some Hilbert space H.\nGiven a C⋆-algebra\nA, a state ϕ is deﬁned to be a positive linear functional of unit norm on A,\nand we write often (A, ϕ) when A comes together with the state ϕ. Given\ntwo C⋆-algebras (A1, ϕ1) and (A2, ϕ2), a homomorphism ι : A1 →A2 is\ncalled state-preserving if ϕ2 ◦ι = ϕ1. Given a C⋆-algebra A, a unitary repre-\nsentation of A in H is a ∗-homomorphism π : A →B(H) which is strongly\ncontinuous, i.e., the mapping A →H, g 7→π(g)ξ is continuous for every\nξ ∈H.\nTheorem 6.7 Let (A, ϕ0) and {(Bk, ϕk) : k ∈I} be C⋆-algebras with states,\nand let ιk be a state-preserving embedding of A into Bk, for each k ∈I. Then there\nexists a C⋆-algebra C amalgamating the (Bk, ϕk) over (A, ϕ0). That is, there is\na state ϕ on C, and state-preserving homomorphisms jk : Bk →C, such that\njk ◦ιk = ji ◦ιi, for all k, i ∈I, and such that S\nk∈I jk(Bk) generates C.\nTheorem 6.7 is illustrated in Figure 6.1 in the case I = {1, 2}. We also\nrecall the Gelfand-Naimark-Segal (GNS) construction establishing a corre-\nspondence between ⋆-representations of a C⋆-algebra and positive linear\nfunctionals on it. In our context, the next result restricts to linear function-\nals on R⟨x⟩which are positive on an Archimedean quadratic module.\nTheorem 6.8 Let g ⊆Sym R⟨x⟩be given such that its quadratic module M(g)\nis Archimedean. Let L : R⟨x⟩→R be a nontrivial linear functional with\nL(M(g)) ⊆R≥0. Then there exists a tuple A = (A1, . . . , An) ∈D∞\ng and\na vector v such that L( f ) = ⟨f (A)v, v⟩, for all f ∈R⟨x⟩.\nLet Ik, k ∈[p] and Jk, k ∈[p] be given as in Chapter 6.2. For k ∈[p], let\nus deﬁne\nM(g)k :=\n(\na⋆\n0a0 + ∑\ni∈Jk\na⋆\ni giai | ai ∈R⟨x, Ik⟩, i ∈Jk ∪{0}\n)\n\n96\nChapter 6. Noncommutative optimization and quantum information\n(C, ϕ)\n\u0000B1, ϕ1\n\u0001\n\u0000B2, ϕ2\n\u0001\n\u0000A, ϕ0\n\u0001\nj1\nj2\nι2\nι1\nFigure 6.1: Illustration of Theorem 6.7 in the case I = {1, 2}.\nand\nM(g)cs := M(g)1 + · · · + M(g)p.\n(6.7)\nNext, we state the main foundational result of this section.\nTheorem 6.9 Let { f } ∪g ⊆Sym R⟨x⟩and let Dg be as in (6.6) with the\nadditional quadratic constraints (6.5). Suppose Assumption 6.4 holds. If\nf (A) ≻0 for all A ∈D∞\ng , then f ∈M(g)cs.\nWe provide an example demonstrating that sparsity without an RIP-\ntype condition is not sufﬁcient to deduce sparsity in SOHS decomposi-\ntions.\nExample 6.10 Consider the case of three variables x = (x1, x2, x3) and the poly-\nnomial\nf = (x1 + x2 + x3)2\n= x2\n1 + x2\n2 + x2\n3 + x1x2 + x2x1 + x1x3 + x3x1 + x2x3 + x3x2 ∈Σ⟨x⟩.\nThen f = f1 + f2 + f3, with\nf1 = 1\n2x2\n1 + 1\n2x2\n2 + x1x2 + x2x1 ∈R⟨x1, x2⟩,\nf2 = 1\n2x2\n2 + 1\n2x2\n3 + x2x3 + x3x2 ∈R⟨x2, x3⟩,\nf3 = 1\n2x2\n1 + 1\n2x2\n3 + x1x3 + x3x1 ∈R⟨x1, x3⟩.\n\n6.4. Sparse representations\n97\nHowever, the sets I1 = {1, 2}, I2 = {2, 3} and I3 = {1, 3} do not satisfy the\nRIP condition (1.8) and f ̸∈Σ⟨x⟩cs := Σ⟨x1, x2⟩+ Σ⟨x2, x3⟩+ Σ⟨x1, x3⟩since\nit has a unique Gram matrix by homogeneity.\nNow consider g = {1 −x2\n1, 1 −x2\n2, 1 −x2\n3}. Then Dg is as in (6.6), M(g)cs\nis as in (6.7) and f |D∞\ng ⪰0. However, we claim that f −b ∈M(g)cs if and only\nif b ≤−3. Clearly,\nf + 3 = (x1 + x2)2 + (x1 + x3)2 + (x2 + x3)2\n+ (1 −x2\n1) + (1 −x2\n2) + (1 −x2\n3) ∈M(g)cs.\nSo one has −3 ≤sup {b : f −b ∈M(g)cs}, and the dual of this latter problem\nis given by\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\nyk\n∑3\nk=1 Lyk( fk)\ns.t.\nLyk(1) = 1,\nk = 1, 2, 3\nLyk(h⋆h) ⪰0,\n∀h ∈R⟨x, Ik⟩,\nk = 1, 2, 3\nLyk(h⋆(1 −x2\ni )h) ⪰0,\n∀h ∈R⟨x, Ik⟩, i ∈Ik, k = 1, 2, 3\nLyj|R⟨X(Ij∩Ik)⟩= Lyk|R⟨X(Ij∩Ik)⟩,\nj, k = 1, 2, 3\n(6.8)\nHence, by weak duality, it sufﬁces to show that there exist linear functionals\nLyk : R⟨x, Ik⟩→R satisfying the constraints of problem (6.8) and such that\n∑k Lyk( fk) = −3. Deﬁne\nA =\n\u0014\n0\n1\n1\n0\n\u0015\n,\nB = −A\nand let\nLyk(g) = tr(g(A, B))\nfor g ∈R⟨x, Ik⟩.\nSince Lyk( fk) = −1, the three ﬁrst constraints of problem (6.8) are easily veriﬁed\nand ∑k Lyk( fk) = −3. For the last one, given, say h ∈R⟨x, I1⟩∩R⟨x, I2⟩=\nR⟨x2⟩, we have\nLy1(h) = tr(h(B)),\nLy2(h) = tr(h(A)),\nsince Ly1 (resp. Ly2) is deﬁned on R⟨x1, x2⟩(resp. R⟨x2, x3⟩) and h depends only\non the second (resp. ﬁrst) variable x2 corresponding to B (resp. A).\nBut matrices A and B are orthogonally equivalent as UAU⊺= B for\nU =\n\u0014\n0\n1\n−1\n0\n\u0015\n,\nwhence h(B) = h(UAU⊺) = Uh(A)U⊺and h(A) have the same trace.\n\n98\nChapter 6. Noncommutative optimization and quantum information\n6.5\nSparse GNS construction\nNext, we provide the main theoretical tools to extract solutions of nc op-\ntimization problems with CS. To this end, we ﬁrst present sparse nc ver-\nsions of theorems by Curto and Fialkow. As recalled in Section 2.4 for the\ncommutative case, Curto and Fialkow provided sufﬁcient conditions for\nlinear functionals on the set of degree 2r polynomials to be represented\nby integration with respect to a nonnegative measure. The main sufﬁcient\ncondition to guarantee such a representation is ﬂatness (see Deﬁnition 6.6)\nof the corresponding moment matrix. We recall this result, which relies on\na ﬁnite-dimensional GNS construction.\nTheorem 6.11 Let g ⊆Sym R⟨x⟩and set δ := max {⌈deg(g)/2⌉: g ∈g}.\nFor r ∈N∗, let Ly : R⟨x⟩2r+2δ →R be the linear functional associated to a\nunital sequence y = (yw)w∈W2r+2δ ∈Rσ(n,2r+2δ) satisfying Ly(M(g)r+δ) ⊆\nR≥0. If y is δ-ﬂat, then there exists ˆA ∈Dr\ng for some t ≤σ(n, r) and a unit\nvector v such that\nLy(g) = ⟨g( ˆA)v, v⟩,\n(6.9)\nfor all g ∈Sym R⟨x⟩2r.\nWe now give the sparse version of Theorem 6.11.\nTheorem 6.12 Suppose r ∈N∗.\nLet g ⊆Sym R⟨x⟩2r, and assume\nDg is as in (6.6) with the additional quadratic constraints (6.5). Suppose\nAssumption 6.4(i) holds.\nSet δ := max {⌈deg(g)/2⌉: g ∈g}.\nLet\nLy : R⟨x⟩2r+2δ →R be the linear functional associated to a unital se-\nquence y = (yw)w∈W2r+2δ ∈Rσ(n,2r+2δ) satisfying Ly(M(g)r+δ) ⊆R≥0.\nAssume that the following holds:\n(H1) Mr+δ(y, Ik) and Mr+δ(y, Ik ∩Ij) are δ-ﬂat, for all j, k ∈[p].\nThen, there exist ﬁnite-dimensional Hilbert spaces H(Ik) with dimension\ntk, for all k ∈[p], Hilbert spaces H(Ij ∩Ik) ⊆H(Ij), H(Ik) for all pairs\n(j, k) with Ij ∩Ik ̸= 0, and operators ˆAk, ˆAjk, acting on them, respectively.\nFurther, there are unit vectors vj ∈H(Ij) and vjk ∈H(Ij ∩Ik) such that\nLy( f ) = ⟨f ( ˆAj)vj, vj⟩\nfor all f ∈R⟨x, Ij⟩2r,\nLy(g) = ⟨g( ˆ\nAjk)vjk, vjk⟩\nfor all g ∈R⟨X(Ij ∩Ik)⟩2r.\n(6.10)\nAssuming that for all pairs (j, k) with Ij ∩Ik ̸= ∅, one has\n\n6.5. Sparse GNS construction\n99\n(H2) the matrices ( ˆAjk\ni )i∈Ij∩Ik have no common complex invariant sub-\nspaces,\nthen there exist A ∈Dt\ng, with t := t1 · · · tp, and a unit vector v such that\nLy( f ) = ⟨f (A)v, v⟩,\n(6.11)\nfor all f ∈∑k R⟨x, Ik⟩2r.\nA\nA(I1)\nA(I2)\nA(I1 ∩I2)\nj1\nj2\nι2\nι1\nFigure 6.2: Amalgamation of ﬁnite-dimensional C⋆-algebras.\nExample 6.13 (Non-amalgamation in ﬁnite-dimensional algebras) Given\nI1 and I2, suppose A(I1 ∩I2) is generated by the 2 × 2 diagonal matrix\nA12 =\n\u0014\n1\n2\n\u0015\n,\nand assume A(I1) = A(I2) = M3(R). (Observe that A(I1 ∩I2) is the algebra\nof all diagonal matrices.) For each k ∈{1, 2}, let us deﬁne ιk(A) := A ⊕k,\nfor all A ∈A(I1 ∩I2). We claim that there is no ﬁnite-dimensional C⋆-algebra\nA amalgamating the above Figure 6.2. Indeed, by the Skolem-Noether theorem,\nevery homomorphism Mn(R) →Mm(R) is of the form x 7→P−1(x ⊗Im/n)P\nfor some invertible P; in particular, n divides m. If a desired A existed, then the\nmatrices (A12 ⊕1) ⊗Ik and (A12 ⊕2) ⊗Ik would be similar. But they are not\nas is easily seen from eigenvalue multiplicities.\nAs in the dense case, we can summarize the sparse GNS construction pro-\ncedure described in the proof of Theorem 6.12 into an algorithm, called\nSparseGNS (see [KMP21, Algorithm 4.6]).\n\n100\nChapter 6. Noncommutative optimization and quantum information\n6.6\nEigenvalue optimization\nWe provide SDP relaxations allowing one to under-approximate the small-\nest eigenvalue that a given nc polynomial can attain on a tuple of symmet-\nric matrices from a given semialgebraic set. We ﬁrst recall the celebrated\nHelton-McCullough theorem stating the equivalence between SOHS and\npositive semideﬁnite nc polynomials.\nTheorem 6.14 (Helton-McCullough) Given f ∈Sym R⟨x⟩, f (A) ⪰0, for\nall A ∈(Sk)n, k ∈N∗, if and only if f ∈Σ⟨x⟩.\nIn contrast with the constrained case where we obtain the analog of Puti-\nnar’s Positivstellensatz in Theorem 6.9, there is no sparse analog of Theo-\nrem 6.14, as shown in the following example.\nLemma 6.15 There exist polynomials which are sparse sums of hermitian squares\nbut are not sums of sparse hermitian squares.\nPROOF Let v =\n\u0002\nx1\nx1x2\nx2\nx3\nx3x2\n\u0003\n,\nG f =\n\n\n1\n−1\n−1\n0\nα\n−1\n2\n0\n−α\n0\n−1\n0\n3\n−1\n9\n0\n−α\n−1\n6\n−27\nα\n0\n9\n−27\n142\n\n\n,\nα ∈R,\n(6.12)\nand consider\nf = vG f v⋆\n= x2\n1 −x1x2 −x2x1 + 3x2\n2 −2x1x2x1 + 2x1x2\n2x1\n−x2x3 −x3x2 + 6x2\n3 + 9x2\n2x3 + 9x3x2\n2 −54x3x2x3 + 142x3x2\n2x3.\n(6.13)\nThe polynomial f is clearly sparse with resepct to I1 = {x1, x2} and I2 =\n{x2, x3}. Note that the matrix G f is positive semideﬁnite if and only if\n0.270615 ≲α ≲1.1075, whence f is a sparse polynomial that is an SOHS.\nWe claim that f ̸∈Σ⟨x, I1⟩+ Σ⟨x, I2⟩, i.e., f is not a sum of sparse\nhermitian squares. By the Newton chip method only monomials in v can\nappear in an SOHS decomposition of f. Further, every Gram matrix of f\nin the monomial basis v is of the form (6.12). However, the matrix G f with\nα = 0 is not positive semideﬁnite, and hence f ̸∈Σ⟨x, I1⟩+ Σ⟨x, I2⟩.\n6.6.1\nUnconstrained eigenvalue optimization\nLet Ik stand for the k × k identity matrix. Given f ∈Sym R⟨x⟩of degree\n2d, the smallest eigenvalue of f is obtained by solving the following opti-\nmization problem:\nλmin( f ) := inf {⟨f (A)v, v⟩: A ∈(Sk)n, k ∈N∗, ∥v∥2 = 1}.\n(6.14)\n\n6.6. Eigenvalue optimization\n101\nThe optimal value λmin( f ) of Problem (6.14) is the greatest lower bound\non the eigenvalues of f (A) over all n-tuples A of real symmetric matrices.\nProblem (6.14) can be rewritten as follows:\nλmin( f ) =\nsup\nb\nb\ns.t.\nf (A) −bIk ⪰0,\n∀A ∈(Sk)n, k ∈N∗\n(6.15)\nwhich is in turn equivalent to\nλd( f ) :=\nsup\nb\nb\ns.t.\nf (x) −b ∈Σ⟨x⟩d\n(6.16)\nas a consequence of Theorem 6.14.\nThe dual of SDP (6.16) is\nηd( f ) :=\ninf\ny\nLy( f )\ns.t.\ny1 = 1,\nMd(y) ⪰0\n(6.17)\nOne can compute λmin( f ) by solving a single SDP, either SDP (6.17) or\nSDP (6.16), since there is no duality gap between these two programs, that\nis, one has ηd( f ) = λd( f ) = λmin( f ).\nNow, we address eigenvalue optimization for a given sparse nc poly-\nnomial f = f1 + · · · + fp of degree 2d, with fk ∈Sym R⟨x, Ik⟩2d, for all\nk ∈[p]. For all k ∈[p], let G fk be a Gram matrix associated to fk. The\nsparse variant of SDP (6.17) is\nηd\ncs( f ) :=\ninf\ny\nLy( f )\ns.t.\ny1 = 1,\nMd(y, Ik) ⪰0, k ∈[p]\n(6.18)\nwhose dual is the sparse variant of SDP (6.16):\nλd\ncs( f ) :=\nsup\nb\nb\ns.t.\nf −b ∈Σ⟨x, I1⟩d + · · · + Σ⟨x, Ip⟩d\n(6.19)\nTo prove that there is no duality gap between SDP (6.18) and SDP (6.19),\nwe need the following result.\nProposition 6.16 The set Σ⟨x⟩cs\nd is a closed convex subset of R⟨x, I1⟩2d + · · · +\nR⟨x, Ip⟩2d.\nFrom Proposition 6.16, we obtain the following theorem which does not\nrequire Assumption 6.4.\n\n102\nChapter 6. Noncommutative optimization and quantum information\nTheorem 6.17 Let f ∈Sym R⟨x⟩of degree 2d, with f = f1 + · · · + fp,\nfk ∈Sym R⟨x, Ik⟩2d, for all k ∈[p]. Then, one has λd\ncs( f ) = ηd\ncs( f ), i.e.,\nthere is no duality gap between SDP (6.18) and SDP (6.19).\nRemark 6.18 By contrast with the dense case, it is not enough to compute the\nsolution of SDP (6.18) to obtain the optimal value λmin( f ) of the unconstrained\noptimization problem (6.14). However, one can still compute a certiﬁed lower\nbound λd\ncs( f ) by solving a single SDP, either in the primal form (6.18) or in\nthe dual form (6.19).\nNote that the related computational cost is potentially\nmuch less expensive. Indeed, SDP (6.19) involves PSD matrices of size at most\nmax {σ(nk, d)}p\nk=1 and ∑\np\nk=1 σ(nk, 2d) equality constraints. This is in contrast\nwith the dense version (6.16), which involves PSD matrices of size σ(n, d) and\nσ(n, 2d) equality constraints.\n6.6.2\nConstrained eigenvalue optimization\nHere, we focus on providing lower bounds for the constrained eigenvalue\noptimization of nc polynomials. Given f ∈Sym R⟨x⟩and g = {g1, . . . , gm}\n⊆Sym R⟨x⟩as in (6.2), let us deﬁne λmin( f, g) as follows:\nλmin( f, g) := inf {⟨f (A)v, v⟩: A ∈D∞\ng , ∥v∥= 1},\n(6.20)\nwhich is, as for the unconstrained case, equivalent to\nλmin( f, g) =\nsup\nb\nb\ns.t.\nf (A) −bIk ⪰0,\n∀A ∈D∞\ng .\n(6.21)\nAs usual, let dj := ⌈deg(gj)/2⌉for each j ∈[m], and let\nrmin := max {⌈deg( f )/2⌉, d1, . . . , dm}.\nOne can approximate λmin( f, g) from below via the following hierarchy of\nSDP programs, indexed by r ≥rmin:\nλr( f, g) :=\nsup\nb\nb\ns.t.\nf −b ∈M(g)r\n(6.22)\nThe dual of SDP (6.22) is\nηr( f, g) :=\ninf\ny\nLy( f )\ns.t.\ny1 = 1,\nMr(y) ⪰0\nMr−dj(gjy) ⪰0,\nj ∈[m]\n(6.23)\n\n6.6. Eigenvalue optimization\n103\nUnder additional assumptions, this hierarchy of primal-dual SDP (6.22)-\n(6.23) converges to the optimal value of the constrained eigenvalue prob-\nlem.\nTheorem 6.19 Assume that Dg is as in (6.6) with the additional quadratic con-\nstraints (6.5) and that the quadratic module M(g) is Archimedean. Then the\nfollowing holds for f ∈Sym R⟨x⟩:\nlim\nr→∞ηr( f, g) = lim\nr→∞λr( f, g) = λmin( f, g).\n(6.24)\nThe main ingredient of the proof is the nc analog of Putinar’s Positivstel-\nlensatz, stated in Theorem 6.2.\nLet Dg be as in (6.6) with the additional quadratic constraints (6.5). Let\nM(g)cs be as in (6.7) and let us deﬁne M(g)cs\nr in the same way as the trun-\ncated quadratic module M(g)r in (6.4). Now, let us state the sparse variant\nof the primal-dual hierarchy (6.22)-(6.23) of lower bounds for λmin( f, g).\nFor r ≥rmin, the sparse variant of SDP (6.23) is\nηr\ncs( f, g) :=\ninf\ny\nLy( f )\ns.t.\ny1 = 1,\nMr(y, Ik) ⪰0, k ∈[p]\nMr−dj(gjy, Ik) ⪰0,\nj ∈Jk, k ∈[p]\n(6.25)\nwhose dual is the sparse variant of SDP (6.22):\nλr\ncs( f, g) :=\nsup\nb\nb\ns.t.\nf −b ∈M(g)cs\nr .\n(6.26)\nAn ε-neighborhood of 0 is the set Nε deﬁned for a given ε > 0 by\nNε :=\n[\nk∈N∗\n(\n(A1, . . . , An) ∈(Sk)n : ε2 −\nn\n∑\ni=1\nA2\ni ⪰0\n)\n.\nProposition 6.20 Let { f } ∪g ⊆Sym R⟨x⟩. Assume that Dg contains an ε-\nneighborhood of 0 and that Dg is as in (6.6) with the additional quadratic con-\nstraints (6.5). Then SDP (6.25) admits strictly feasible solutions. As a result,\nthere is no duality gap between SDP (6.25) and its dual (6.26).\nMoreover, we have the following convergence result implied by Theo-\nrem 6.9.\nTheorem 6.21 Let { f } ∪g ⊆Sym R⟨x⟩. Assume that Dg is as in (6.6) with\nthe additional quadratic constraints (6.5). Let Assumption 6.4 hold. Then, one\nhas\nlim\nr→∞ηr\ncs( f, g) = lim\nr→∞λr\ncs( f, g) = λmin( f, g).\n(6.27)\n\n104\nChapter 6. Noncommutative optimization and quantum information\nAs for the unconstrained case, there is no sparse variant of the “perfect”\nPositivstellensatz, for constrained eigenvalue optimization over convex nc\nsemialgebraic sets [BKP16, Chapter 4.4], such as those associated either to\nthe sparse nc ball Bcs := {1 −∑i∈I1 x2\ni , . . . , 1 −∑i∈Ip x2\ni } or the nc polydisc\nD := {1 −x2\n1, . . . , 1 −x2\nn}. Namely, for an nc polynomial f of degree\n2d + 1, computing only SDP (6.18) with optimal value λd+1\ncs ( f, g) when\ng = Bcs or g = D does not sufﬁce to obtain the value of λmin( f, g). This is\nexplained in Example 6.22 below.\nExample 6.22 Let us consider a randomly generated cubic polynomial f = f1 +\nf2 with\nf1 = 4 −x1 + 3x2 −3x3 −3x2\n1 −7x1x2 + 6x1x3 −x2x1 −5x3x1 + 5x3x2\n−5x3\n1 −3x2\n1x3 + 4x1x2x1 −6x1x2x3 + 7x1x3x1 + 2x1x3x2 −x1x2\n3\n−x2x2\n1 + 3x2x1x2 −x2x1x3 −2x3\n2 −5x2\n2x3 −4x2x2\n3 −5x3x2\n1\n+ 7x3x1x2 + 6x3x2x1 −4x3x2x2 −x2\n3x1 −2x2\n3x2 + 7x3\n3,\nf2 = −1 + 6x2 + 5x3 + 3x4 −5x2\n2 + 2x2x3 + 4x2x4 −4x3x2 + x2\n3 −x3x4\n+ x4x2 −x4x3 + 2x2\n4 −7x3\n2 + 4x2x2\n3 + 5x2x3x4 −7x2x4x3 −7x2x2\n4\n+ x3x2\n2 + 6x3x2x3 −6x3x2x4 −3x2\n3x2 −7x2\n3x4 + 6x3x4x2\n−3x3x4x3 −7x3x2\n4 + 3x4x2\n2 −7x4x2x3 −x4x2x4 −5x4x2\n3\n+ 7x4x3x4 + 6x2\n4x2 −4x3\n4,\nand the nc polyball g = Bcs = {1 −x2\n1 −x2\n2 −x2\n3, 1 −x2\n2 −x2\n3 −x2\n4} correspond-\ning to I1 = {1, 2, 3} and I2 = {2, 3, 4}. Then, one has λ2\ncs( f, g) ≃−27.536 <\nλ3\ncs( f, g) ≃−27.467 ≃λ2\nmin( f, g) = λmin( f, g). In Appendix B.2, we provide\na Julia script to compute these bounds.\n6.6.3\nExtracting optimizers\nHere, we explain how to extract a pair of optimizers (A, v) for the eigen-\nvalue optimization problems when the ﬂatness and irreducibility condi-\ntions of Theorem 6.12 hold. We apply the SparseGNS procedure on the\noptimal solution of SDP (6.18) in the unconstrained case or SDP (6.25) in\nthe constrained case.\nProposition 6.23 Given f as in Theorem 6.17, let us assume that SDP (6.18)\n(with d being replaced by d + 1) yields an optimal solution y associated to ηd+1\ncs\n( f ).\nIf the sequence y satisﬁes the ﬂatness (H1) and irreducibility (H2) conditions\nstated in Theorem 6.12, then one has\nλmin( f ) = ηd+1\ncs\n( f ) = Ly( f ).\n\n6.6. Eigenvalue optimization\n105\nAlgorithm 3 SparseEigGNS\nRequire: f ∈Sym R⟨x⟩2d satisfying Assumption 6.4\nEnsure: A and v\n1: Compute ηd+1\ncs\n( f ) by solving SDP (6.18)\n2: if SDP (6.18) is unbounded or its optimum is not attained then\n3:\nStop\n4: end if\n5: Let Md+1(y) be an optimizer of SDP (6.18)\n6: Compute A, v := SparseGNS(Md+1(y))\nWe can extract optimizers for the unconstrained minimal eigenvalue prob-\nlem (6.14) thanks to the following algorithm.\nIn the constrained case, the next result is a direct corollary of Theo-\nrem 6.12.\nCorollary 6.24 Let { f } ∪g ⊆Sym R⟨x⟩, and assume that Dg is as in (6.6)\nwith the additional quadratic constraints (6.5). Suppose Assumptions 6.4(i)-(ii)\nhold. Let y be an optimal solution of SDP (6.25) with optimal value ηr\ncs( f, g) for\nr ≥rmin + δ, such that y satisﬁes the assumptions of Theorem 6.12. Then, there\nexist t ∈N∗, A ∈Dt\ng and a unit vector v such that\nλmin( f, g) = ⟨f (A)v, v⟩= ηr\ncs( f, g).\nExample 6.25 Consider the sparse polynomial f = f1 + f2 from Example 6.22.\nThe moment matrix M3(y) obtained by solving (6.25) with r = 3 satisﬁes the\nﬂatness (H1) and irreducibility (H2) conditions of Theorem 6.12. We can thus\napply the SparseGNS algorithm yielding\nA1 =\n\n\n0.0059\n0.0481\n0.1638\n0.4570\n0.0481\n−0.2583\n0.5629\n−0.2624\n0.1638\n0.5629\n0.3265\n−0.3734\n0.4570\n−0.2624\n−0.3734\n−0.2337\n\n\nA2 =\n\n\n−0.3502\n0.0080\n0.1411\n0.0865\n0.0080\n−0.4053\n0.2404\n−0.1649\n0.1411\n0.2404\n−0.0959\n0.3652\n0.0865\n−0.1649\n0.3652\n0.4117\n\n\nA3 =\n\n\n−0.7669\n−0.0074\n−0.1313\n−0.0805\n−0.0074\n−0.4715\n−0.2238\n0.1535\n−0.1313\n−0.2238\n0.0848\n−0.3400\n−0.0805\n0.1535\n−0.3400\n−0.2126\n\n\nA4 =\n\n\n0.3302\n−0.1839\n0.1811\n−0.0404\n−0.1839\n−0.1069\n0.5114\n−0.0570\n0.1811\n0.5114\n0.1311\n−0.3664\n−0.0404\n−0.0570\n−0.3664\n0.4440\n\n\n\n106\nChapter 6. Noncommutative optimization and quantum information\nwhere\nf (A) =\n\n\n−10.3144\n3.9233\n−5.0836\n−7.7828\n3.9233\n1.8363\n4.5078\n−7.5905\n−5.0836\n4.5078\n−19.5827\n13.9157\n−7.7828\n−7.5905\n13.9157\n8.3381\n\n\nhas minimal eigenvalue −27.4665 with unit eigenvector\nv =\n\u0002\n0.1546\n−0.2507\n0.8840\n−0.3631\n\u0003⊺.\nIn this case all the ranks involved are equal to four. So A2 and A3 are computed\nfrom M3(y, I1 ∩I2), after an appropriate basis change A1 (and the same A2, A3)\nis obtained from M3(y, I1), and ﬁnally A4 is computed from M3(y, I2).\n6.7\nOverview of numerical experiments\nThe aim of this section is to provide experimental comparison between\nthe bounds given by the dense hierarchy and the ones produced by our\nCS variant. The numerical results were obtained with the Julia package\nNCTSSOS employing MOSEK as an SDP solver. The computation was car-\nried out on Intel(R) Core(TM) i9-10900 CPU@2.80GHz with 64G RAM.\n6.7.1\nAn unconstrained problem\nIn Table 6.1, we report results obtained for minimizing the eigenvalue of\nthe nc variant of the chained singular function [CGT88]:\nf = ∑\ni∈J\n\u0000(xi + 10xi+1)2 + 5(xi+2 −xi+3)2\n+ (xi+1 −2xi+2)4 + 10(xi −10xi+3)4\u0001\n,\nwhere J = [n −3] and n is a multiple of 4. We compute lower bounds\non the minimal eigenvalue of f for n = 40, 80, 120, 160, 200, 240. For each\nvalue of n, “mb” stands for maximal sizes of PSD blocks involved either in\nthe sparse relaxation (6.18) or the dense relaxation (6.17). As one can see,\nthe size of the SDP programs is signiﬁcantly reduced after exploiting CS,\nwhich is consistent with Remark 6.18. In addition, the sparse approach\nturns out to be much more efﬁcient and scalable than the dense approach.\n6.7.2\nBell inequalities\nUpper bounds on quantum violations of Bell inequalities can be com-\nputed using eigenvalue maximization of nc polynomials. The classical\n(also most concise) Bell inequality states that v⋆(A1 ⊗B1 + A1 ⊗B2 + A2 ⊗\n\n6.7. Overview of numerical experiments\n107\nTable 6.1: Sparse versus dense approaches for minimizing eigenvalues of\nthe chained singular function. mb: maximal size of PSD blocks, opt: op-\ntimum, time: running time in seconds. “-” indicates an out of memory\nerror.\nn\nsparse\ndense\nmb\nopt\ntime\nmb\nopt\ntime\n40\n13\n0\n0.17\n157\n0\n142\n80\n13\n0\n0.43\n-\n-\n-\n120\n13\n0\n0.65\n-\n-\n-\n160\n13\n0\n0.89\n-\n-\n-\n200\n13\n−0.0014\n1.02\n-\n-\n-\n240\n13\n−0.0016\n1.28\n-\n-\n-\nB1 −A2 ⊗B2)v is at most 2 for all separable states v ∈Ck ⊗Ck and self-\nadjoint Aj, Bj ∈Ck×k with A2\nj = B2\nj = Ik. The so-called Tsirelson’s bound\nimplies that the above quantity is at most 2\n√\n2 when one allows arbi-\ntrary states. This bound on the maximum violation level can be obtained\nby eigenvalue-maximizing a1b1 + a1b2 + a2b1 −a2b2 under the constraints\na2\nj = b2\nj = 1 and aibj = bjai. To show the potential beneﬁts of our approach\nbased on CS, we consider the Bell inequality, called I3322, and compute up-\nper bounds of its maximum violation level. The associated objective func-\ntion is f = a1(b1 + b2 + b3) + a2(b1 + b2 −b3) + a3(b1 −b2) −a1 −2b1 −b2.\nThe set of constraints is a2\nj = aj, b2\nj = bj, and aibj = bjai. Table 6.2 com-\npares the efﬁciency and accuracy of the sparse approach with the dense\none, for different relaxation orders. It can be seen that the sparse approach\nspends much less time while providing almost the same bounds.\nTable 6.2: Sparse versus dense approaches for maximizing the violation\nlevel of the Bell inequality I3322. r denotes the relaxation order.\nr\nsparse\ndense\nmb\nopt\ntime\nmb\nopt\ntime\n2\n28\n0.2509398\n0.01\n13\n0.2590718\n0.01\n3\n88\n0.2508758\n0.22\n25\n0.2512781\n0.02\n4\n244\n0.2508754\n8.40\n41\n0.2509057\n0.02\n5\n628\n0.2508752\n456\n61\n0.2508774\n0.04\n6\n-\n-\n-\n85\n0.2508754\n0.09\n\n108\nChapter 6. Noncommutative optimization and quantum information\n6.8\nNotes and sources\nThe main results presented in this chapter have been published in [KMP21].\nApplications of interest connected with noncommutative optimization arise\nfrom quantum theory and quantum information science [NPA08, PKRR+19]\nas well as control theory [SIG98, dOHMP09]. Further motivation relates to\nthe generalized Lax conjecture [Lax58], where the goal is to obtain computer-\nassisted proofs based on SOHS in Clifford algebras [NT14]. The veriﬁca-\ntion of noncommutative polynomial trace inequalities has also been moti-\nvated by a conjecture formulated by Bessis, Moussa and Villani (BMV) in\n1975 [BMV75], which has been recently proved by Stahl [Sta13] (see also\nthe Lieb and Seiringer reformulation [LS04]). Further efforts focused on\napplications arising from bipartite quantum correlations [GdLL18], and\nmatrix factorization ranks in [GDLL19]. In a related analytic direction,\nthere has been recent progress on multivariate generalizations of the Golden-\nThompson inequality and the Araki-Lieb-Thirring inequality [SBT17, HKT17].\nThere is a plethora of prior research in quantum information theory in-\nvolving reformulating problems as optimization of noncommutative poly-\nnomials. One famous application is to characterize the set of quantum\ncorrelations. Bell inequalities [Bel64] provide a method to investigate en-\ntanglement, which allows two or more parties to be correlated in a non-\nclassical way, and is often studied through the set of bipartite quantum\ncorrelations. Such correlations consist of the conditional probabilities that\ntwo physically separated parties can generate by performing measure-\nments on a shared entangled state. These conditional probabilities sat-\nisfy some inequalities classically, but violate them in the quantum realm\n[CHSH69].\nIn this context, a given noncommutative polynomial in n variables and\nof degree 2d is PSD if and only if it decomposes as a SOHS [Hel02, McC01].\nIn practice, an SOHS decomposition can be computed by solving an SDP\ninvolving PSD matrices of size O(nd), which is even larger than the size of\nthe matrices involved in the commutative case. SOHS decompositions are\nalso used for constrained optimization, either to minimize eigenvalues or\ntraces of noncommutative polynomial objective functions, under noncom-\nmutative polynomial (in)equality constraints. The optimal value of such\nconstrained problems can be approximated, as closely as desired, while\nrelying on the noncommutative analogue of Lasserre’s hierarchy [PNA10,\nCKP12, BCKP13]. The NCSOStools [CKP11, BKP16] library can compute\nsuch approximations for optimization problems involving polynomials in\nnoncommuting variables. By comparison with the commutative case, the\nsize O(nr) of the SDP matrices at a given step r of the noncommutative\nhierarchy becomes intractable even faster.\nA remedy for unconstrained problems is to rely on the adequate non-\ncommutative analogue of the standard Newton polytope method, which\n\n6.8. Notes and sources\n109\nis called the Newton chip method (see e.g., [BKP16, §2.3]) and can be further\nimproved with the augmented Newton chip method (see e.g., [BKP16, §2.4]),\nby removing certain terms which can never appear in an SOHS decompo-\nsition of a given input. As in the commutative case, the Newton polytope\nmethod cannot be applied for constrained problems. When one cannot\ngo from step r to step r + 1 in the hierarchy because of the computational\nburden, one can always consider matrices indexed by all terms of degree\nr plus a ﬁxed percentage of terms of degree r + 1. This is used for in-\nstance to compute tighter upper bounds for maximum violation levels of\nBell inequalities [PV09]. Another trick, implemented in the NCPOL2SDPA\nlibrary [Wit15], consists of exploiting simple equality constraints, such as\n“x2 = 1”, to derive substitution rules for variables involved in the SDP\nrelaxations. Similar substitutions are performed in the commutative case\nby GLOPTIPOLY [HLL09].\nProposition 6.1 can be found, e.g., in [Hel02, §2.2]. The noncommu-\ntative analog of Putinar’s Positivstellensatz is due to Helton and McCul-\nlough [HM04, Theorem 1.2]. Lemma 6.5 is proved in [BKP16, Lemma 1.44].\nTheorem 6.8 is proved in [BKP16, Theorem 1.27]. For more details on\namalgamation theory for C⋆-algebras, see, e.g., [Bla78, Voi85]. Theorem\n6.7 can be found in [Bla78] or [Voi85, Section 5]. This amalgamation the-\nory serves to prove our sparse representation result 6.9, originally stated\nin [KMP21, Theorem 3.3].\nThe notion of ﬂatness was exploited in a noncommutative setting for\nthe ﬁrst time by McCullough [McC01] in his proof of the Helton-McCullough\ntheorem, cf. [McC01, Lemma 2.2]. In the dense case [PNA10] (see also\n[AL12, Chapter 21] and [BKP16, Theorem 1.69]) provides a ﬁrst noncom-\nmutative variant for the eigenvalue problem. See [BCKP13] for a similar\nconstruction for the trace problem. The sparse version of the ﬂat exten-\nsion theorem is stated in [KMP21, Theorem 4.2] and the SparseGNS algo-\nrithm is explicitly given in [KMP21, Algorithm 4.6] for the case of two\nsubsets of variables (the general case is similar). Theorem 6.12 can be seen\nas a noncommutative variant of the result by Lasserre stated in [Las06,\nTheorem 3.7], related to the minimizer extraction in the context of sparse\npolynomial optimization. In the sparse commutative case, Lasserre as-\nsumes ﬂatness of each moment matrix indexed by the canonical basis of\nR[x, Ik]r, for each k ∈[p], which is similar to our ﬂatness condition (H1).\nThe difference is that this technical ﬂatness condition on each Ik adapts\nto the degree of the constraint polynomials in variables in Ik, resulting in\nan adapted parameter δk instead of global δ. We could assume the same\nin Theorem 6.12 but for the sake of simplicity, we assume that these pa-\nrameters are all equal.\nIn addition, Lasserre assumes that each moment\nmatrix indexed by the canonical basis of R[x, Ij ∩Ik)]r is of rank one, for\nall pairs (j, k) with Ij ∩Ik ̸= ∅, which is the commutative analog of our\nirreducibility condition (H2).\n\n110\nChapter 6. Noncommutative optimization and quantum information\nThe absence of duality gap for unconstrained eigenvalue minimization\nis derived in the dense case, e.g., in [BKP16, Theorem 4.1] and relies on\n[MP05, Proposition 3.4], which says that the set of SOHS polynomials is\nclosed in the set of nc polynomials. Proposition 6.16 is a sparse version of\nthis latter result, leading to Theorem 6.17, originally proved in [KMP21,\nTheorem 5.4].\nThe “perfect” Positivstellensatz for constrained eigenvalue optimiza-\ntion over convex nc semialgebraic set (e.g., the nc ball or the nc poly-\ndisc) is stated in [BKP16, §4.4] or [HKM12]. Proposition 6.23 and Corol-\nlary 6.24 are the sparse variants of [BKP16, Proposition 4.4] and [BKP16,\nTheorem 4.12], in the unconstrained and constrained settings, respectively.\nAs in the dense case [BKP16, Algorithm 4.2], one can provide a ran-\ndomized algorithm to look for ﬂat optimal solutions for the constrained\neigenvalue problem (6.20). The underlying reason which motivates this\nrandomized approach is work by Nie, who derives in [Nie14] a hierarchy\nof SDP programs, with a random objective function, that converges to a\nﬂat solution (under mild assumptions).\nThe interested reader can ﬁnd more about trace minimization of sparse\npolynomials in [KMP21, § 6] and more detailed numerical experiments\nin [KMP21, § 7]. For a detailed account about maximal violation levels of\nBell inequalities (in particular the one mentioned in Table 6.2), we refer\nto [PV09]. CS can be exploited in a similar way to solve trace polynomial\noptimization problems [KMV22].\n\nBibliography\n[AL12]\nMiguel F. Anjos and Jean-Bernard Lasserre, editors. Handbook\non semideﬁnite, conic and polynomial optimization, volume 166\nof International Series in Operations Research & Management Sci-\nence. Springer, New York, 2012.\n[BCKP13]\nSabine Burgdorf, Kristijan Cafuta, Igor Klep, and Janez Povh.\nThe tracial moment problem and trace-optimization of poly-\nnomials. Math. Program., 137(1-2, Ser. A):557–578, 2013.\n[Bel64]\nJohn S Bell. On the Einstein Podolsky Rosen paradox. Physics\nPhysique Fizika, 1(3):195, 1964.\n[BKP16]\nSabine Burgdorf, Igor Klep, and Janez Povh.\nOptimization\nof polynomials in non-commuting variables.\nSpringerBriefs in\nMathematics. Springer, [Cham], 2016.\n[Bla78]\nBruce E. Blackadar.\nWeak expectations and nuclear C∗-\nalgebras. Indiana Univ. Math. J., 27(6):1021–1026, 1978.\n[BMV75]\nDaniel Bessis, Pierre Moussa, and Matteo Villani.\nMono-\ntonic converging variational approximations to the functional\nintegrals in quantum statistical mechanics.\nJ. Math. Phys.,\n16(11):2318–2325, 1975.\n[CGT88]\nAndrew R. Conn, Nicholas I. M. Gould, and Philippe L. Toint.\nTesting a class of methods for solving minimization prob-\nlems with simple bounds on the variables.\nMath. Comp.,\n50(182):399–430, 1988.\n[CHSH69]\nJohn F. Clauser, Michael A. Horne, Abner Shimony, and\nRichard A. Holt. Proposed experiment to test local hidden-\nvariable theories. Phys. rev. lett., 23(15):880, 1969.\n[CKP11]\nKristijan Cafuta, Igor Klep, and Janez Povh. NCSOStools: a\ncomputer algebra system for symbolic and numerical com-\nputation with noncommutative polynomials. Optim. Methods\nSoftw., 26(3):363–380, 2011.\n\n112\nBibliography\n[CKP12]\nKristijan Cafuta, Igor Klep, and Janez Povh.\nConstrained\npolynomial optimization problems with noncommuting vari-\nables. SIAM J. Optim., 22(2):363–383, 2012.\n[dOHMP09] Mauricio C. de Oliveira, J. William Helton, Scott A. Mc-\nCullough, and Mihai Putinar. Engineering systems and free\nsemi-algebraic geometry. In Emerging applications of algebraic\ngeometry, volume 149 of IMA Vol. Math. Appl., pages 17–61.\nSpringer, New York, 2009.\n[GdLL18]\nSander Gribling, David de Laat, and Monique Laurent.\nBounds on entanglement dimensions and quantum graph\nparameters via noncommutative polynomial optimization.\nMath. Program., 170(1, Ser. B):5–42, 2018.\n[GDLL19]\nSander Gribling, David De Laat, and Monique Laurent.\nLower bounds on matrix factorization ranks via noncommu-\ntative polynomial optimization. Foundations of Computational\nMathematics, 19(5):1013–1070, 2019.\n[Hel02]\nJ. William Helton. “Positive” noncommutative polynomials\nare sums of squares. Ann. of Math. (2), 156(2):675–694, 2002.\n[HKM12]\nJ. William Helton, Igor Klep, and Scott McCullough. The con-\nvex Positivstellensatz in a free algebra. Adv. Math., 231(1):516–\n534, 2012.\n[HKT17]\nFumio Hiai, Robert König, and Marco Tomamichel.\nGen-\neralized log-majorization and multivariate trace inequalities.\nAnn. Henri Poincaré, 18(7):2499–2521, 2017.\n[HLL09]\nD. Henrion, Jean-Bernard Lasserre, and J. Löfberg.\nGlop-\ntiPoly 3: moments, optimization and semideﬁnite program-\nming. Optimization Methods and Software, 24(4-5):pp. 761–779,\nAugust 2009.\n[HM04]\nJ. William Helton and Scott A. McCullough. A Positivstel-\nlensatz for non-commutative polynomials. Trans. Amer. Math.\nSoc., 356(9):3721–3737, 2004.\n[KMP21]\nIgor Klep, Victor Magron, and Janez Povh. Sparse noncom-\nmutative polynomial optimization.\nMathematical Program-\nming, pages 1–41, 2021.\n[KMV22]\nIgor Klep, Victor Magron, and Jurij Volˇciˇc.\nOptimization\nover trace polynomials. In Annales Henri Poincaré, volume 23,\npages 67–100. Springer, 2022.\n\nBibliography\n113\n[Las06]\nJean B Lasserre. Convergent sdp-relaxations in polynomial\noptimization with sparsity.\nSIAM Journal on Optimization,\n17(3):822–843, 2006.\n[Lax58]\nPeter D. Lax. Differential equations, difference equations and\nmatrix theory. Comm. Pure Appl. Math., 11:175–194, 1958.\n[LS04]\nElliott H. Lieb and Robert Seiringer.\nEquivalent forms of\nthe Bessis-Moussa-Villani conjecture. J. Statist. Phys., 115(1-\n2):185–190, 2004.\n[McC01]\nScott McCullough.\nFactorization of operator-valued poly-\nnomials in several non-commuting variables. Linear Algebra\nAppl., 326(1-3):193–203, 2001.\n[MP05]\nScott McCullough and Mihai Putinar. Noncommutative sums\nof squares. Paciﬁc J. Math., 218(1):167–171, 2005.\n[Nie14]\nJiawang Nie. The A-truncated K-moment problem. Found.\nComput. Math., 14(6):1243–1276, 2014.\n[NPA08]\nMiguel Navascués, Stefano Pironio, and Antonio Acín. A con-\nvergent hierarchy of semideﬁnite programs characterizing the\nset of quantum correlations. New J. Phys., 10(7):073013, 2008.\n[NT14]\nTim Netzer and Andreas Thom.\nHyperbolic polynomials\nand generalized Clifford algebras.\nDiscrete Comput. Geom.,\n51(4):802–814, 2014.\n[PKRR+19] Alejandro Pozas-Kerstjens, Rafael Rabelo, Łukasz Rudnicki,\nRafael Chaves, Daniel Cavalcanti, Miguel Navascués, and\nAntonio Acín. Bounding the sets of classical and quantum\ncorrelations in networks. Phys. Rev. Lett., 123(14):140503, 6,\n2019.\n[PNA10]\nStefano Pironio, Miguel Navascués, and Antonio Acín. Con-\nvergent relaxations of polynomial optimization problems\nwith noncommuting variables. SIAM J. Optim., 20(5):2157–\n2180, 2010.\n[PV09]\nKároly F. Pál and Tamás Vértesi. Quantum bounds on Bell\ninequalities. Phys. Rev. A (3), 79(2):022120, 12, 2009.\n[SBT17]\nDavid Sutter, Mario Berta, and Marco Tomamichel. Multivari-\nate trace inequalities. Comm. Math. Phys., 352(1):37–58, 2017.\n[SIG98]\nRobert E. Skelton, Tetsuya Iwasaki, and Karolos M. Grigori-\nadis. A uniﬁed algebraic approach to linear control design. The\nTaylor & Francis Systems and Control Book Series. Taylor &\nFrancis, Ltd., London, 1998.\n\n114\nBibliography\n[Sta13]\nHerbert R. Stahl. Proof of the BMV conjecture. Acta Math.,\n211(2):255–290, 2013.\n[Voi85]\nDan-Virgil Voiculescu.\nSymmetries of some reduced free\nproduct C∗-algebras. In Operator algebras and their connections\nwith topology and ergodic theory (Busteni, 1983), volume 1132 of\nLecture Notes in Math., pages 556–588. Springer, Berlin, 1985.\n[Wit15]\nPeter Wittek. Algorithm 950: Ncpol2sdpa-sparse semideﬁnite\nprogramming relaxations for polynomial optimization prob-\nlems of noncommuting variables. ACM Trans. Math. Software,\n41(3):Art. 21, 12, 2015.\n\nPart II\nTerm sparsity\n\n\nChapter 7\nThe moment-SOS hierarchy\nbased on term sparsity\nAs emphasized earlier for distinct applications, exploiting CS arising from\na POP may allow to signiﬁcantly reduce the computational cost of the re-\nlated hierarchy of SDP relaxations assuming that the csp is sufﬁciently\nsparse. Nevertheless a POP can be fairly sparse (namely, involving only\na small number of terms) whereas its csp is (nearly) dense. For instance,\nif some constraint (e.g., 1 −∥x∥2\n2 ≥0) involves all variables, then the csp\nis dense. On the other hand, instead of exploiting sparsity from the per-\nspective of variables, one can also exploit sparsity from the perspective of\nterms, which leads to the notions of term sparsity (TS) and term sparsity pat-\ntern (tsp).\nRoughly speaking, the tsp can be also represented by a graph, which\nis called the tsp graph. But unlike the csp graph, the nodes of the tsp\ngraph are monomials (coming from a monomial basis) and the edges of\nthe graph grasp the links between monomials emerging from the related\nSOS decomposition. We are able to design an iterative procedure to en-\nlarge the tsp graph in order to iteratively exploit TS for the given POP.\nEach iteration consists of two successive operations: (i) a support exten-\nsion followed by (ii) a chordal extension. In doing so we obtain a ﬁnite\nascending chain of graphs:\nG(1) ⊆G(2) ⊆· · · ⊆G(s) = G(s+1).\nThen combining this iterative procedure with the standard moment-SOS\nhierarchy results in a two-level moment-SOS hierarchy involving PSD blocks.\nWhen the sizes of the blocks are small, the associated SDP relaxations can\nbe drastically much cheaper to solve.\nTo some extent, TS (working on the monomial level) is ﬁner than CS\n\n118\nChapter 7. The moment-SOS hierarchy based on term sparsity\n(working on the variable level) in describing sparsity for a POP. A natural\nidea for solving large-scale POPs then is: ﬁrst exploiting CS to decompose\nvariables into a set of cliques, and second exploiting TS for each subsystem\ninvolving only one variable clique to further reduce the size of SDPs. This\nidea has been successfully carried out in [WMLM20] and will be presented\nlater on, in Chapter 8.\nChapter 7.1 focuses on exploiting TS for unconstrained POPs. We prove\nin Chapter 7.2 that the resulting scheme is always more accurate than the\nframework based on scaled diagonally dominant sum of squares (SDSOS).\nChapter 7.3 presents a TS variant of the moment-SOS hierarchy of SDP re-\nlaxations for general POPs with compact constraints. Next, we provide in\nChapter 7.4 more sophisticated algorithms to reduce even further the size\nof the resulting SDP relaxations. Chapter 7.5 explains the relation between\nthe block structures arising from the TS-adapted relaxations and the one\nprovided by sign symmetries. Eventually, we illustrate in Chapter 7.6 the\nefﬁciency and the accuracy of the TS-adapted relaxations on benchmarks\ncoming from the global optimization literature and networked systems\nstability.\n7.1\nThe TSSOS hierarchy for unconstrained POPs\nIn this section, we describe an iterative procedure to exploit TS for the\nprimal-dual moment-SOS relaxations of unconstrained POPs. Recall the\nformulation of an unconstrained POP:\nP :\nfmin := inf { f (x) : x ∈Rn} = sup {b : f −b ≥0},\n(7.1)\nwhere f = ∑α fαxα ∈R[x]. Suppose that f is of degree 2d with supp( f ) =\nA (w.l.o.g. assuming 0 ∈A ) and xB := (xβ)β∈B is a monomial basis ar-\nranged with resepct to any ﬁxed ordering. For convenience, we slightly\nabuse notation in the sequel and denote by B (resp. β) instead of xB\n(resp. xβ) a monomial basis (resp. a monomial). One may choose B to\nbe the standard monomial basis Nn\nd. But when f is sparse, the follow-\ning theorem due to Reznick [Rez78] allows us to use a (possibly) smaller\nmonomial basis by considering Newton polytopes. Recall that for a poly-\nnomial f ∈R[x], the Newton polytope of f, denoted by N ( f ), is the convex\nhull generated by its support.\nTheorem 7.1 If f, pi ∈R[x] and f = ∑i p2\ni , then N (pi) ⊆1\n2 N ( f ).\nAs an immediate corollary, we can take the integer points in half of the\nNewton polytope of f to form a monomial basis, i.e.,\nB = 1\n2 N ( f ) ∩Nn ⊆Nn\nd.\n(7.2)\n\n7.1. The TSSOS hierarchy for unconstrained POPs\n119\nExample 7.2 Let f = 4x4\n1x6\n2 + x2\n1 −x1x2\n2 + x2\n2. Then supp( f ) = {(4, 6), (2, 0), (1, 2),\n(0, 2)} and B = 1\n2 N ( f ) ∩Nn = {(1, 0), (2, 3), (0, 1), (1, 2), (1, 1)} (Figure\n7.1).\n1\n0\n2\n3\n4\n1\n2\n3\n4\n5\n6\nFigure 7.1: The monomial basis given by half of the Newton polytope\n(marked by red).\nGiven a monomial basis B and a sequence y ⊆R, the moment matrix\nMB(y) associated with B and y is the block of the moment matrix Md(y)\nindexed by B. Then the moment relaxation of P in the monomial basis B\nis given by\nPmom :\nfmom :=\ninf\ny\nLy( f )\ns.t.\nMB(y) ⪰0\ny0 = 1\n(7.3)\nFor a graph G(V, E) with V ⊆Nn, let the support of G be given by\nsupp(G) := {β + γ | β = γ or {β, γ} ∈E} .\n(7.4)\nWe deﬁne Gtsp to be the graph with nodes V = B and with edges\nE(Gtsp) = {{β, γ} | β ̸= γ ∈V, β + γ ∈A ∪(2 B)} ,\n(7.5)\nwhich is called the tsp graph associated with f.\nStarting with the initial graph G(0) = Gtsp, we now deﬁne a sequence\nof graphs (G(s))s≥1 by iteratively performing two successive operations:\n(1) support extension. Let F(s) be the graph with nodes V and with edges\nE(F(s)) =\nn\n{β, γ} | β ̸= γ ∈V, β + γ ∈supp(G(s−1))\no\n.\n(7.6)\n(2) chordal extension. Let G(s) =\n\u0000F(s)\u0001′.\n\n120\nChapter 7. The moment-SOS hierarchy based on term sparsity\n1\nx1\nx2\nx3\nx2x3\nx1x3\nx1x2\nFigure 7.2: The support extension of G in Example 7.3. The dashed edges\nare added after support extension.\nExample 7.3 Let us consider the graph G(V, E) with\nV = {1, x1, x2, x3, x2x3, x1x3, x1x2} and E = {{1, x2x3}, {x2, x1x3}} .\nFigure 7.2 illustrates the support extension of G.\nBy construction, one has G(s) ⊆G(s+1) for s ≥1 and therefore the se-\nquence of graphs (G(s))s≥1 stabilizes after a ﬁnite number of steps. Follow-\ning what we introduced in Chapter 1.2, we denote by ΠG(s)(S+\n| B |) the set\nof matrices in S(G(s)) that have a PSD completion, and denote by BG(s) the\nadjacency matrix of G(s). If f is sparse, by replacing MB(y) ⪰0 with the\nweaker condition MB(y) ∈ΠG(s)(S+\n| B |) in (7.3), we then obtain a sparse\nmoment relaxation of (7.1) for each s ≥1:\nPs\nts :\n\n\n\n\n\n\n\ninf\ny\nLy( f )\ns.t.\nBG(s) ◦MB(y) ∈ΠG(s)(S+\n| B |)\ny0 = 1\n(7.7)\nwith optimum denoted by f s\nts. We call (Ps\nts)s≥1 the TS-adpated moment-\nSOS (TSSOS) hierarchy for P and call s the sparse order.\nRemark 7.4 The intuition behind the support extension operation is that once\none position related to yα in the moment matrix MB(y) is “activated” in the\nsparsity pattern, then all positions related to yα in MB(y) should be “activated”.\nIn addition, Theorems 1.4 and 1.5 provide the rationale behind the mechanism of\nthe chordal extension operation.\nTheorem 7.5 The sequence ( f s\nts)s≥1 is monotonically nondecreasing and f s\nts ≤\nfmom for all s ≥1.\nPROOF The inclusion G(k) ⊆G(k+1) implies that each maximal clique of\nG(k) is a subset of some maximal clique of G(k+1). Thus by Theorem 1.5,\nwe see that Ps\nts is a relaxation of Ps+1\nts\n(and also a relaxation of Pmom). This\nyields the desired conclusions.\n2\n\n7.1. The TSSOS hierarchy for unconstrained POPs\n121\nAs a consequence of Theorem 7.5, we obtain the following hierarchy of\nlower bounds for the optimum of P:\nf 1\nts ≤f 2\nts ≤· · · ≤fmom ≤fmin.\n(7.8)\nIf the maximal chordal extension is chosen for the chordal extension\noperation, then we can show (see [WML21] for more details) that the se-\nquence ( f s\nts)s≥1 converges to the global optimum fmin. Otherwise, there is\nno guarantee of such convergence as illustrated by the following example.\nExample 7.6 Consider the commutative version of the polynomial from (6.13):\nf = x2\n1 −2x1x2 + 3x2\n2 −2x2\n1x2 + 2x2\n1x2\n2 −2x2x3 + 6x2\n3\n+ 18x2\n2x3 −54x2x2\n3 + 142x2\n2x2\n3.\nThe monomial basis computed from the Newton polytope is {1, x1, x2, x3, x1x2,\nx2x3}.\nFigure 7.3 shows the tsp graph Gtsp (without dashed edges) and its\nsmallest chordal extension G(1) (with dashed edges) for f. The graph sequence\n(G(s))s≥1 stabilizes at s = 1. Solving P1\nts, we obtain f 1\nts ≈−0.00355 while\nfmom = fmin = 0. On the other hand, note that Gtsp has only one connected com-\nponent. So with the maximal chordal extension, we immediately get the complete\ngraph and it follows f 1\nts = fmom = 0 in this case.\nx1\nx2\nx3\nx1x2\n1\nx2x3\nx1\nx2\nx3\nx1x2\n1\nx2x3\nFigure 7.3: The tsp graph Gtsp and a smallest chordal extension (left) as\nwell as the maximal chordal extension (right) for Example 7.6.\nFor each s ≥1, the dual SDP of (7.7) is\n\n\n\n\n\n\n\nsup\nG,b\nb\ns.t.\n⟨G, Bα⟩= fα −b1α=0,\n∀α ∈supp(G(s))\nG ∈S+\n| B | ∩S(G(s))\n(7.9)\nwhere Bα has been deﬁned after (2.8).\nProposition 7.7 For each s ≥1, there is no duality gap between Ps\nts and its dual\n(7.9).\nPROOF This easily follows from the fact that Ps\nts satisﬁes Slater’s condition\nby Proposition 3.1 of [Las01] and Theorem 1.5.\n2\n\n122\nChapter 7. The moment-SOS hierarchy based on term sparsity\n7.2\nComparison with SDSOS\nSDSOS polynomials were introduced and studied in [AM14] as cheaper\nalternatives to SOS polynomials in the context of polynomial optimiza-\ntion. More concretely, a symmetric matrix G ∈St is diagonally dominant if\nGii ≥∑j̸=i |Gij| for i ∈[t], and is scaled diagonally dominant if there exists\na positive deﬁnite t × t diagonal matrix D such that DGD is diagonally\ndominant. We say that a polynomial f ∈R[x] is a scaled diagonally domi-\nnant sum of squares (SDSOS) polynomial if it admits a Gram matrix repre-\nsentation as in (7.9) (with b = 0) with a scaled diagonally dominant Gram\nmatrix G. We denote the set of SDSOS polynomials by SDSOS.\nBy replacing the nonnegativity condition in P with the SDSOS condi-\ntion, one obtains the SDSOS relaxation of P:\n(SDSOS) :\nfsdsos := sup {b : f −b ∈SDSOS}.\nIt turns out that the ﬁrst value of the TSSOS hierarchy is already better\nthan or equal to the bound provided by the SDSOS relaxation.\nTheorem 7.8 With the above notation, one has f 1\nts ≥fsdsos.\nPROOF Let A\n= supp( f ) and B be a monomial basis.\nAssume f ∈\nSDSOS, i.e., f admits a scaled diagonally dominant Gram matrix G ∈S+\n| B |\nindexed by B. We then construct a Gram matrix ˜G for f by\n˜Gβγ =\n(\nGβγ,\nif β + γ ∈A ∪(2 B),\n0,\notherwise.\nIt is easy to see that we still have f = (xB)⊺˜GxB. Note that we only\nreplace off-diagonal entries by zeros in G to obtain ˜G and replacing off-\ndiagonal entries by zeros does not affect the scaled diagonal dominance of\na matrix. Hence ˜G is also a scaled diagonally dominant matrix. Moreover,\nwe have ˜G ∈S+\n| B | ∩S(G(1)) by construction. It follows that (SDSOS) is a\nrelaxation of (7.9). Hence f 1\nts ≥fsdsos by Proposition 7.7.\n2\n7.3\nThe TSSOS hierarchy for constrained POPs\nIn this section, we describe an iterative procedure to exploit TS for the\nprimal-dual moment-SOS hierarchy of constrained POPs:\nP :\nfmin := inf { f (x) : x ∈X},\n(7.10)\nwith\nX = {x ∈Rn | g1(x) ≥0, . . . , gm(x) ≥0}.\n(7.11)\n\n7.3. The TSSOS hierarchy for constrained POPs\n123\nLet A denote the union of supports involved in P, i.e.,\nA = supp( f ) ∪\nm\n[\nj=1\nsupp(gj).\n(7.12)\nLet rmin := max {⌈deg( f )/2⌉, d1, . . . , dm} with dj := ⌈deg(gj)/2⌉for\nj ∈[m]. Fix a relaxation order r ≥rmin. Let g0 = 1, d0 = 0 and Br,j =\nNn\nr−dj be the standard monomial basis for j = 0, 1, . . . , m. We deﬁne a\ngraph Gtsp with nodes Br,0 and edges\nE(Gtsp) = {{β, γ} | β ̸= γ ∈Br,0, β + γ ∈A ∪(2 Br,0)} ,\n(7.13)\nwhich is called the tsp graph associated with P or essentially A .\nNow let us initialize with G(0)\nr,0 := Gtsp and G(0)\nr,j being an empty graph\nfor j ∈[m]. Then for each j ∈{0} ∪[m], we deﬁne a sequence of graphs\n(G(s)\nr,j )s≥1 by iteratively performing two successive operations:\n(1) support extension. Let F(s)\nr,j be the graph with nodes Br,j and edges\nE(F(s)\nr,j ) =\nn\n{β, γ} | β ̸= γ ∈Br,j,\n(supp(gj) + β + γ) ∩\nm\n[\ni=0\n\u0000supp(gi) + supp(G(s−1)\nr,i\n)\n\u0001 ̸= ∅\no\n.\n(7.14)\n(2) chordal extension. Let\nG(s)\nr,j :=\n\u0000F(s)\nr,j\n\u0001′,\nj ∈{0} ∪[m].\n(7.15)\nRecall that the dense moment relaxation of order r for P is given by\nPr :\nf r\nmom :=\ninf\ny\nLy( f )\ns.t.\nMr−dj(gj y) ⪰0,\nj ∈{0} ∪[m]\ny0 = 1\n(7.16)\nLet tj := | Br,j | = (n+r−dj\nr−dj ). Therefore by replacing Mr−dj(gjy) ⪰0 with\nthe weaker condition BG(s)\nr,j ◦Mr−dj(gjy) ∈ΠG(s)\nr,j (S+\ntj ) for j ∈{0} ∪[m] in\n(7.16), we obtain the following sparse moment relaxation of Pr and P for\neach s ≥1:\nPr,s\nts :\nf r,s\nts :=\ninf\ny\nLy( f )\ns.t.\nBG(s)\nr,0 ◦Mr(y) ∈ΠG(s)\nr,0 (S+\nt0)\nBG(s)\nr,j ◦Mr−dj(gjy) ∈ΠG(s)\nr,j (S+\ntj ),\nj ∈[m]\ny0 = 1\n(7.17)\n\n124\nChapter 7. The moment-SOS hierarchy based on term sparsity\nAs in the unconstrained case, we call s the sparse order. By construction,\none has G(s)\nr,j ⊆G(s+1)\nr,j\nfor all j, s. Therefore, for each j ∈{0} ∪[m], the\nsequence of graphs (G(s)\nr,j )s≥1 stabilizes after a ﬁnite number of steps. We\ndenote the stabilized graph by G(•)\nr,j for all j and the corresponding moment\nrelaxation by Pr,•\nts with optimum f r,•\nts .\nFor each s ≥1, the dual SDP of Pr,s\nts reads as\n\n\n\n\n\n\n\n\n\n\n\nsup\nGj,b\nb\ns.t.\n∑m\nj=0⟨Cj\nα, Gj⟩= fα −b1α=0,\n∀α ∈Sm\nj=0\n\u0000supp(gj) + supp(G(s)\nr,j )\n\u0001\nGj ∈S+\ntj ∩S(G(s)\nr,j ),\nj ∈{0} ∪[m]\n(7.18)\nwhere Cj\nα is deﬁned after (2.8). The primal-dual SDP relaxations (7.17)–\n(7.18) are called the TSSOS hierarchy associated with P, which is indexed\nby two parameters: the relaxation order r and the sparse order s.\nTheorem 7.9 With the above notation, the following hold:\n(i) Assume that X has a nonempty interior. Then there is no duality gap\nbetween Pr,s\nts and its dual (7.18) for any r ≥rmin and s ≥1.\n(ii) Fixing a relaxation order r ≥rmin, the sequence ( f r,s\nts )s≥1 is monoton-\nically nondecreasing and f r,s\nts ≤f r\nmom for all s ≥1.\n(iii) When the maximal chordal extension is used for the chordal extension\noperation, the sequence ( f r,s\nts )s≥1 converges to f r\nmom in ﬁnitely many\nsteps.\n(iv) Fixing a sparse order s ≥1, the sequence ( f r,s\nts )r≥rmin is monotonically\nnondecreasing.\nPROOF (i). This easily follows from the fact that Pr,s\nts satisﬁes Slater’s con-\ndition by Theorem 4.2 of [Las01] and Theorem 1.5.\n(ii). For all j, s, the inclusion G(s)\nr,j ⊆G(s+1)\nr,j\nimplies that each maximal\nclique of G(s)\nr,j is a subset of some maximal clique of G(s+1)\nr,j\n. Hence by Theo-\nrem 1.5, Pr,s\nts is a relaxation of Pr,s+1\nts\n(and also a relaxation of Pr) from which\nwe have that ( f r,s\nts )s≥1 is monotonically nondecreasing and f r,s\nts ≤f r\nmom for\nall s ≥1.\n(iii). Let y = (yα) be an arbitrary feasible solution of Pr,•\nts . We note that\n\n7.3. The TSSOS hierarchy for constrained POPs\n125\n\b\nyα | α ∈S\nj∈{0}∪[m]\n\u0000supp(gj) + supp(G(•)\nr,j )\n\u0001\t\nis the set of decision vari-\nables involved in Pr,•\nts , and {yα | α ∈Nn\n2r} is the set of decision variables\ninvolved in Pr (7.16). We then deﬁne a vector y = (yα)α∈Nn\n2r as follows:\nyα =\n(\nyα,\nif α ∈S\nj∈{0}∪[m]\n\u0000supp(gj) + supp(G(•)\nr,j )\n\u0001\n,\n0,\notherwise.\nBy construction and since G(•)\nr,j stabilizes under support extension for all\nj, we immediately have Mr−dj(gjy) = BG(•)\nr,j ◦Mr−dj(gjy). As we use the\nmaximal chordal extension for the chordal extension operation, the matrix\nBG(•)\nr,j ◦Mr−dj(gjy) is block-diagonal up to permutation. So from BG(•)\nr,j ◦\nMr−dj(gjy) ∈ΠG(•)\nr,j (S\ntj\n+) it follows Mr−dj(gjy) ⪰0 for j ∈{0} ∪[m].\nTherefore y is a feasible solution of Pr and so Ly( f ) = Ly( f ) ≥f r\nmom.\nHence f r,•\nts ≥f r\nmom as y is an arbitrary feasible solution of Pr,•\nts . By (ii), we\nalready have f r,•\nts ≤f r\nmom. Therefore, f r,•\nts = f r\nmom as desired.\n(iv). The conclusion follows if we can show that G(s)\nr,j ⊆G(s)\nr+1,j for all\nj, r since by Theorem 1.5 this implies that Pr,s\nts is a relaxation of Pr+1,s\nts\n. Let\nus prove G(s)\nr,j\n⊆G(s)\nr+1,j by induction on s. For s = 1, from (7.13), we\nhave G(0)\nr,0 ⊆G(0)\nr+1,0, which implies G(1)\nr,j ⊆G(1)\nr+1,j for j ∈{0} ∪[m]. Now\nassume that G(s)\nr,j ⊆G(s)\nr+1,j, j ∈{0} ∪[m] hold for a given s ≥1. Then\nfrom (7.14) and by the induction hypothesis, we have G(s+1)\nr,j\n⊆G(s+1)\nr+1,j for\nj ∈{0} ∪[m], which completes the induction and also completes the proof.\n2\nBy Theorem 7.9, we have the following two-level hierarchy of lower\nbounds for the optimum fmin of P:\nf rmin,1\nts\n≤\nf rmin,2\nts\n≤\n· · ·\n≤\nf rmin\nmom\n≥\n≥\n≥\nf rmin+1,1\nts\n≤\nf rmin+1,2\nts\n≤\n· · ·\n≤\nf rmin+1\nmom\n≥\n≥\n≥\n...\n...\n...\n...\n≥\n≥\n≥\nf r,1\nts\n≤\nf r,2\nts\n≤\n· · ·\n≤\nf r\nmom\n≥\n≥\n≥\n...\n...\n...\n...\n(7.19)\nThe TSSOS hierarchy entails a trade-off between the computational\ncost and the quality of the obtained lower bound via the two tunable pa-\n\n126\nChapter 7. The moment-SOS hierarchy based on term sparsity\nrameters r and s. Besides, one has the freedom to choose a speciﬁc chordal\nextension in (7.15) (e.g., maximal chordal extensions, approximately small-\nest chordal extensions and so on). This choice could affect the resulting\nsizes of PSD blocks and the quality of the related lower bound.\nIntu-\nitively, chordal extensions with smaller clique numbers would lead to PSD\nblocks of smaller sizes and lower bounds of (possibly) lower quality while\nchordal extensions with larger clique numbers would lead to PSD blocks\nwith larger sizes and lower bounds of (possibly) higher quality.\nRemark 7.10 If P is a QCQP, then P1,1\nts and P1 yield the same lower bound, i.e.,\nf 1,1\nts = f 1\nmom. Indeed, for a QCQP, the moment relaxation P1 reads as\n\n\n\n\n\n\n\n\n\n\n\ninf\ny\nLy( f )\ns.t.\nM1(y) ⪰0\nLy(gj) ≥0,\nj ∈[m]\ny0 = 1\nNote that the objective function and the afﬁne constraints of P1 involve only\nthe decision variables {y0} ∪{yα}α∈A with A = supp( f ) ∪Sm\nj=1 supp(gj).\nHence there is no discrepancy of optima in replacing P1 with P1,1\nts by construction.\n7.4\nObtaining a possibly smaller monomial ba-\nsis\nThe size of SDPs arising form the TSSOS hierarchy heavily depends on\nthe chosen monomial basis B or Br,0. As we already saw in Chapter 7.1,\nfor unconstrained POPs the Newton polytope method usually produces\na monomial basis smaller than the standard monomial basis. However,\nthis method does not apply to constrained POPs. Here as an optional\npre-treatment, we present an iterative procedure which not only allows\nus to obtain a monomial basis smaller than the one given by the Newton\npolytope method for unconstrained POPs in many cases, but can also be\napplied to constrained POPs.\nWe start with the unconstrained case. Let f ∈R[x] with A = supp( f )\nand B be the monomial basis given by the Newton polytope method. Ini-\ntializing with B0 := ∅, we iteratively deﬁne a sequence of monomial sets\n(Bp)p≥1 by\nBp := {β ∈B | ∃γ ∈B s.t. β + γ ∈A ∪(2 Bp−1)}.\n(7.20)\nConsequently, we obtain an ascending chain of monomial sets:\nB1 ⊆B2 ⊆B3 ⊆· · · ⊆B .\n\n7.4. Obtaining a possibly smaller monomial basis\n127\nThis procedure is formulated in Algorithm 4. Each Bp in the chain can\nserve as a candidate monomial basis. In practice, if indexing the unknown\nGram matrix by Bp leads to an infeasible SDP, then we turn to Bp+1 until\na feasible SDP is retrieved.\nAlgorithm 4 GenerateBasis\nRequire: A support set A and an initial monomial basis B\nEnsure: An ascending chain of potential monomial bases (Bp)p≥1\n1: B0 ←∅\n2: p ←0\n3: while p = 0 or Bp ̸= Bp−1 do\n4:\np ←p + 1\n5:\nBp ←∅\n6:\nfor each pair β, γ in B do\n7:\nif β + γ ∈A ∪(2 Bp−1) then\n8:\nBp ←Bp ∪{β, γ}\n9:\nend if\n10:\nend for\n11: end while\n12: return (Bp)p≥1\nProposition 7.11 Let f ∈R[x] and B∗= ∪p≥1 Bp with Bp being deﬁned by\n(7.20). If f ∈SDSOS, then f is an SDSOS polynomial in the monomial basis\nB∗.\nPROOF Let B be the monomial basis given by the Newton polytope method.\nIf f ∈SDSOS, then there exists a scaled diagonally dominant Gram matrix\nG ∈S| B |\n+\nindexed by B such that f = (xB)⊺GxB. We then construct a\nGram matrix ˜G ∈S| B∗|\n+\nindexed by B∗for f as follows:\n˜Gβγ =\n(\nGβγ,\nif β + γ ∈A ∪(2 B∗),\n0,\notherwise.\nOne can easily check that we still have f = (xB∗)⊺˜GxB∗. Let ˆG be the\nprincipal submatrix of G by deleting the rows and columns whose indices\nare not in B∗, which is also a scaled diagonally dominant matrix. By con-\nstruction, ˜G is obtained from ˆG by replacing certain off-diagonal entries\nby zeros. Since replacing off-diagonal entries by zeros does not affect the\nscaled diagonal dominance of a matrix, ˜G is also a scaled diagonally dom-\ninant matrix. It follows that f is an SDSOS polynomial in the monomial\nbasis B∗.\n2\n\n128\nChapter 7. The moment-SOS hierarchy based on term sparsity\nRemark 7.12 By Proposition 7.11, if we use the monomial basis B∗for (7.7)–\n(7.9), we still have the hierarchy of optima:\nfsdsos ≤f 1\nts ≤f 2\nts ≤· · · ≤fmom ≤fmin.\nThe algorithm GenerateBasis may provide a smaller monomial basis\nthan the one given by the Newton polytope method as the following ex-\nample illustrates.\nExample 7.13 Consider the polynomial f = 1 + x + x8. The monomial basis\ngiven by the Newton polytope method is B = {1, x, x2, x3, x4}. By the algo-\nrithm GenerateBasis, we obtain B1 = {1, x, x4} and B2 = {1, x, x2, x4}. It\nturns out that f admits no SOS decomposition with B1 while B2 can serve as a\nmonomial basis to represent f as an SOS.\nFor the constrained case we follow the notation of Chapter 7.3. Fix a\nrelaxation order r and a sparse order s of the TSSOS hierarchy. Initializing\nwith Br,0 = Nn\nr , we iteratively perform the following two steps:\nFor Step 1, let the maximal cliques of G(s)\nr,j be Cj,1, Cj,2, . . . , Cj,tj for j ∈\n{0} ∪[m]. Let\nF = supp( f ) ∪\nm\n[\nj=1\n\nsupp(gj) +\ntj[\ni=1\n(Cj,i + Cj,i)\n\n.\n(7.21)\nThen call the algorithm GenerateBasis with A = F and B = Br,0 to\ngenerate a new monomial basis B′\nr,0.\nFor Step 2, with the new monomial basis B′\nr,0, recompute the graph\nG(s)\nr,j for j ∈{0} ∪[m] as in Chapter 7.3 and then go back to Step 1.\nContinue the iterative procedure until B′\nr,0 = Br,0, which is the desired\nmonomial basis.\n7.5\nSign symmetries and a sparse representation\ntheorem for positive polynomials\nThe exploitation of TS developed in the previous sections is closely related\nto sign symmetries. Intuitively, a polynomial is said to have sign symme-\ntries if it is invariant when we change signs of some variables. For instance,\nthe polynomial f (x1, x2) = x2\n1 + x2\n2 + x1x2 has the sign symmetry associ-\nated to (x1, x2) 7→(−x1, −x2) as f (−x1, −x2) = f (x1, x2). To be more\nprecise, we give the following deﬁnition of sign symmetries in terms of\nsupport sets.\n\n7.5. Sign symmetries and a sparse representation theorem\n129\nDeﬁnition 7.14 (sign symmetry) Given a ﬁnite set A ⊆Nn, the sign sym-\nmetries of A are deﬁned by all vectors s ∈Zn\n2 := {0, 1}n such that s⊺α ≡0\n(mod 2) for all α ∈A .\nAssume that the maximal chordal extension is chosen for the chordal\nextension operation in Chapter 7.3. As mentioned earlier, for any j the\nsequence of graphs (G(s)\nr,j )s≥1 ends up with G(•)\nr,j in ﬁnitely many steps.\nNote that the graph G(•)\nr,j induces a partition of the monomial basis Nn\nr−dj:\ntwo monomials β, γ ∈Nn\nr−dj belong to the same block if and only if they\nbelong to the same connected component of G(•)\nr,j . The following theorem\nprovides an interpretation of this partition in terms of sign symmetries.\nTheorem 7.15 Notations are as in the previous sections. Fix the relaxation\norder r ≥rmin. Assume that the maximal chordal extension is chosen for\nthe chordal extension operation and the sign symmetries of A are given by\nthe columns of a binary matrix denoted by R. Then for each j ∈{0} ∪[m],\nβ, γ belong to the same block in the partition of Nn\nr−dj induced by G(•)\nr,j if and\nonly if R⊺(β + γ) ≡0 (mod 2). In other words, for a ﬁxed relaxation order\nthe block structures arising from the TSSOS hierarchy converge to the block\nstructure determined by the sign symmetries of the POP assuming that the\nmaximal chordal extension is used for the chordal extension operation.\nTheorem 7.15 is applied for the standard monomial basis Nn\nr−dj. If a\nsmaller monomial basis is chosen, then we only have the \"only if\" part of\nthe conclusion in Theorem 7.15.\nExample 7.16 Let f = 1 + x2\n1x4\n2 + x4\n1x2\n2 + x4\n1x4\n2 −x1x2\n2 −3x2\n1x2\n2 and A =\nsupp( f ). The monomial basis given by the Newton polytope method is B =\n{1, x1x2, x1x2\n2, x2\n1x2, x2\n1x2\n2}. The sign symmetries of A consist of two elements:\n(0, 0) and (0, 1). According to the sign symmetries, B is partitioned into {1, x1x2\n2,\nx2\n1x2\n2} and {x1x2, x2\n1x2}. On the other hand, the partition of B induced by G(•)\nis {1, x1x2\n2, x2\n1x2\n2}, {x1x2} and {x2\n1x2}, which is a reﬁnement of the partition\ndetermined by the sign symmetries.\nAs a corollary of Theorem 7.15, we can prove a sparse representation\ntheorem for positive polynomials on compact basic semialgebraic sets.\n\n130\nChapter 7. The moment-SOS hierarchy based on term sparsity\nTheorem 7.17 Let X be deﬁned as in (7.11). Assume that the quadratic\nmodule M(g) is Archimedean and that the polynomial f is positive on X.\nLet A = supp( f ) ∪Sm\nj=1 supp(gj) and let the sign symmetries of A be\ngiven by the columns of the binary matrix R. Then f can be decomposed as\nf = σ0 +\nm\n∑\nj=1\nσjgj,\nfor some SOS polynomials σ0, σ1, . . . , σm satisfying R⊺α ≡0 (mod 2) for\nany α ∈supp(σj), j = 0, . . . , m.\nPROOF By Putinar’s Positivstellensatz (Theorem 2.3), there exist SOS poly-\nnomials σ′\n0, σ′\n1, . . . , σ′\nm such that\nf = σ′\n0 +\nm\n∑\nj=1\nσ′\nj gj.\n(7.22)\nLet dj = ⌈deg(gj)/2⌉, j = 0, 1, . . . , m and\nr = max {⌈deg(σ′\nj gj)/2⌉: j = 0, 1, . . . , m}\nwith g0 = 1. Let Gj be a Gram matrix associated with σ′\nj and indexed by\nthe monomial basis Nn\nr−dj, j = 0, 1, . . . , m. Then deﬁne σj = (x\nNn\nr−dj )⊺(BG(•)\nr,j ◦\nGj)x\nNn\nr−dj for j = 0, 1, . . . , m, where G(•)\nr,j is deﬁned in Chapter 7.3. For any\nj ∈{0} ∪[m], since BG(•)\nr,j ◦Gj is block-diagonal (up to permutation) and\nGj is positive semideﬁnite, we see that σj is an SOS polynomial.\nSuppose α ∈supp(σj) with j ∈{0} ∪[m]. Then we can write α =\nα′ + β + γ for some α′ ∈supp(gj) and some β, γ belonging to the same\nconnected component of G(•)\nr,j . By Theorem 7.15, we have R⊺(β + γ) ≡0\n(mod 2) and therefore, R⊺α ≡0 (mod 2). Moreover, for any α′ ∈supp(gj)\nand β, γ not belonging to the same connected component of G(•)\nr,j , we have\nR⊺(β + γ) ̸≡0 (mod 2) by Theorem 7.15 and so R⊺(α′ + β + γ) ̸≡0\n(mod 2). From these facts we deduce that substituting σ′\nj with σj in (7.22)\nis just removing the terms whose exponents α do not satisfy R⊺α ≡0\n(mod 2) from the right-hand side of (7.22). Doing so, one does not change\nthe match of coefﬁcients on both sides of the equality. Thus we have\nf = σ0 +\nm\n∑\nj=1\nσjgj,\n\n7.6. Numerical experiments\n131\nwith the desired property.\n2\n7.6\nNumerical experiments\nWe present some numerical results of the proposed TSSOS hierarchy in\nthis section. The related algorithms were implemented in the Julia package\nTSSOS1. For the numerical experiments, we use MOSEK [ART03] as an SDP\nsolver. All examples were computed on an Intel Core i5-8265U@1.60GHz\nCPU with 8GB RAM memory. The timing (in seconds) includes the time\nfor pre-processing (to get the block structure), the time for modeling SDP\nand the time for solving SDP.\n7.6.1\nUnconstrained polynomial optimization\nWe ﬁrst consider Lyapunov functions emerging from some networked sys-\ntems. The following polynomial is from Example 2 in [HP11]:\nf =\nn\n∑\ni=1\nai(x2\ni + x4\ni ) −\nn\n∑\ni=1\nn\n∑\nk=1\nbikx2\ni x2\nk,\nwhere ai are randomly chosen from [1, 2] and bik are randomly chosen from\n[ 0.5\nn , 1.5\nn ]. Here, n is the number of nodes in the network. The task is to\ndetermine whether f is globally nonnegative.\nThe sizes of SDPs corresponding to the TSSOS (with sparse order s = 1)\nand dense relaxations are listed in Table 7.1. In the column “#PSD blocks”,\ni × j means j PSD blocks of size i. The column “#equality constraints”\nindicates the number of equality constraints involved in SDPs.\nTable 7.1: The sizes of SDPs for the sparse and dense relaxations.\n#PSD blocks\n#equality constraints\nTSSOS\n3 × n(n−1)\n2\n, 1 × n, (n + 1) × 1\n3n(n−1)\n2\n+ 2n + 1\nDense\n(n+2\n2 ) × 1\n(n+4\n4 )\nWe solve the TSSOS relaxation with s = 1 for n ∈{10, 20, . . . , 80}. The\nresults are displayed in Table 7.2 in which “mb” stands for the maximal\nsize of PSD blocks.\nFor this example, the size of the system that can be handled in [HP11]\nis up to n = 50 nodes while TSSOS can easily handle the system with up to\nn = 80 nodes.\n1https://github.com/wangjie212/TSSOS\n\n132\nChapter 7. The moment-SOS hierarchy based on term sparsity\nTable 7.2: Results for the ﬁrst networked system.\nn\n10\n20\n30\n40\n50\n60\n70\n80\nmb\n11\n31\n31\n41\n51\n61\n71\n81\ntime\n0.006\n0.03\n0.10\n0.34\n0.92\n1.9\n4.7\n12\nThe following polynomial is from Example 3 in [HP11]:\nV =\nn\n∑\ni=1\nai(1\n2x2\ni −1\n4x4\ni ) + 1\n2\nn\n∑\ni=1\nn\n∑\nk=1\nbik\n1\n4(xi −xk)4,\n(7.23)\nwhere ai are randomly chosen from [0.5, 1.5] and bik are randomly chosen\nfrom [ 0.5\nn , 1.5\nn ]. The task is to analyze the domain on which the Hamiltonian\nfunction V for a network of Dufﬁng oscillators is positive deﬁnite. We use\nthe following condition to establish an inner approximation of the domain\non which V is positive deﬁnite:\nf = V −\nn\n∑\ni=1\nλix2\ni (g −x2\ni ) ≥0,\n(7.24)\nwhere λi > 0 are scalar decision variables and g is a ﬁxed positive scalar.\nClearly, the condition (7.24) ensures that V is positive deﬁnite when x2\ni <\ng.\nWe illustrate the tsp graph G(0) of this example in Figure 7.4, which has\n1 maximal clique of size n + 1 (involving the nodes 1, x2\n1, . . . , x2\nn), n(n−1)\n2\nmaximal cliques of size 3 (involving the nodes x2\ni , x2\nj , xixj for each pair\n{i, j}, i ̸= j) and n maximal cliques of size 1 (involving the node xi for each\ni). Since G(0) is already a chordal graph, we have G(1) = G(0).\nHere we solve the TSSOS relaxation with s = 1 for n ∈{10, 20, . . . , 50}.\nThis example was also examined in [MAT14] to demonstrate the advan-\ntage of SDSOS programming compared to dense SOS programming. The\napproach based on SDSOS programming was implemented in SPOT [Meg10]\nwith MOSEK as a second-order cone programming solver. We display the\nresults for both TSSOS and SDSOS in Table 7.3 in which the row “#SDP\nvars\" indicates the number of decision variables used in SDPs.\nFrom the table we see that the TSSOS approach uses much less deci-\nsion variables than the SDSOS approach, and hence is more efﬁcient. On\nthe other hand, the TSSOS approach computes a positive deﬁnite form V\nafter selecting a value for g up to 2 (which is the same as the maximal value\nobtained by the dense SOS approach) while the method in [HP11] can se-\nlect g up to 1.8 and the SDSOS approach only works out for a maximal\nvalue of g up to around 1.5.\n\n7.6. Numerical experiments\n133\nx2\nj\nx2\nk\nx2\ni\n1\nxixj\nxixk\nxjxk\nx1\nx2\nxn\n· · ·\nFigure 7.4: The tsp graph G(0) for the second networked system. What\nis displayed is merely a subgraph of G(0). The whole graph G(0) can be\nobtained by putting all such subgraphs together.\nTable 7.3: Results for the second networked system.\nn\n10\n20\n30\n40\n50\n#PSD blocks\nTSSOS\n3 × 45,\n3 × 190,\n3 × 435,\n3 × 780,\n3 × 1225,\n1 × 10,\n1 × 20,\n1 × 30,\n1 × 40,\n1 × 50,\n11 × 1\n21 × 1\n31 × 1\n41 × 1\n51 × 1\nSDSOS\n2 × 2145\n2 × 26565\n2 × 122760\n2 × 370230\n2 × 878475\n#SDP vars\nTSSOS\n346\n1391\n3136\n5581\n8726\nSDSOS\n6435\n79695\n368280\n1110690\n2635425\ntime\nTSSOS\n0.01\n0.06\n0.17\n0.50\n0.89\nSDSOS\n0.47\n1.14\n5.47\n20\n70\n7.6.2\nConstrained polynomial optimization\nNow we present the numerical results for constrained polynomial opti-\nmization problems. As a ﬁrst example, we minimize randomly generated2\nsparse polynomials H2, H4, H6 over the unit ball\nX =\nn\n(x1, . . . , xn) ∈Rn \f\f\f g1 = 1 −(x2\n1 + · · · + x2\nn) ≥0\no\n.\nWe solve these instances with TSSOS as well as GLOPTIPOLY. The related\nresults are shown in Table 7.4. Note that approximately smallest chordal\nextensions are used for TS and only the results of the ﬁrst three steps of\nthe TSSOS hierarchy (for a ﬁxed relaxation order) are displayed. From the\ntable it can be seen that for each instance TSSOS is signiﬁcantly faster than\nGLOPTIPOLY3 without compromising accuracy.\nNext we present the numerical results (Table 7.5) for minimizing the\n2These polynomials can be downloaded at https://wangjie212.github.io/jiewang/code.html.\n3GLOPTIPOLY also uses MOSEK as an SDP solver.\n\n134\nChapter 7. The moment-SOS hierarchy based on term sparsity\nTable 7.4: Results for minimizing randomly generated polynomials over\nthe unit ball; d denotes the polynomial degree and t denotes the number\nof terms.\n(n, d, t)\nr\nTSSOS\nGLOPTIPOLY\ns\nmb\nopt\ntime\nmb\nopt\ntime\nH2\n(7,8,12)\n4\n1\n36\n0.1373\n0.36\n330\n0.1373\n34\n2\n36\n0.52\n3\n38\n1.6\n5\n1\n36\n1.9\n792\n-\n-\n2\n45\n3.9\n3\n59\n34\nH4\n(9,6,15)\n3\n1\n10\n0.8704\n0.15\n220\n0.8704\n16\n2\n10\n0.22\n3\n10\n0.25\n4\n1\n55\n1.3\n715\n-\n-\n2\n55\n2.0\n3\n56\n2.8\nH6\n(11,6,20)\n3\n1\n12\n0.1171\n0.28\n364\n0.1171\n115\n2\n15\n0.36\n3\n16\n0.60\n4\n1\n78\n4.4\n1365\n-\n-\n2\n78\n4.7\n3\n78\n7.5\ngeneralized Rosenbrock function over the unit ball:\nfgR(x) = 1 +\nn\n∑\ni=1\n\u0010\n100(xi −x2\ni−1)2 + (1 −xi)2\u0011\n.\nWe approach this problem by solving the TSSOS hierarchy with r =\n2.\nHere we compare the results obtained with approximately smallest\nchordal extensions (min) and maximal chordal extensions (max). In Table\n7.5, we can see that TSSOS scales much better with approximately smallest\nchordal extensions than with maximal chordal extensions while providing\nthe same optimum.\n\n7.7. Notes and sources\n135\nTable 7.5: Results for the generalized Rosenbrock function.\nn\nmin\nmax\ns\nmb\nopt\ntime\ns\nmb\nopt\ntime\n20\n1\n21\n18.25\n0.19\n1\n58\n18.25\n8.2\n2\n211\n18.25\n45\n60\n1\n61\n57.85\n6.6\n1\n178\n-\n-\n2\n1831\n-\n-\n100\n1\n101\n97.45\n85\n1\n308\n-\n-\n2\n5051\n-\n-\n140\n1\n141\n137.05\n448\n1\n428\n-\n-\n2\n9871\n-\n-\n180\n1\n181\n176.65\n1495\n1\n548\n-\n-\n2\n16291\n-\n-\n7.7\nNotes and sources\nBesides the Newton polytope method and the approach given in Chap-\nter 7.4, there are also other algorithms that provide a possibly smaller\nmonomial basis; see for instance [KKW05] and [YC20].\nThe results on the TSSOS hierarchy presented in this chapter have been\npublished in [WML21, WML21a]. The idea of exploiting TS in SOS decom-\npositions was initially proposed in [WLX19] for the unconstrained case\nand sooner after extended to the constrained case in [WML21, WML21a].\nThe exploitation of sign symmetries in SOS decompositions was ﬁrstly dis-\ncussed in [Lof09] in the unconstrained setting. Theorem 7.15, stated in\n[WML21, Theorem 6.5], relates the convergence of block structures arising\nfrom the TSSOS hierarchy (for a ﬁxed relaxation order) to sign symme-\ntries. For more extensive comparison of TSSOS with the polynomial op-\ntimization solvers GLOPTIPOLY [HLL09], YALMIP [Lö04], and SparsePOP\n[WKK+08], the reader is referred to [WML21, WML21a].\n\n\nBibliography\n[AM14]\nAmir Ali Ahmadi and Anirudha Majumdar. Dsos and sd-\nsos optimization: Lp and socp-based alternatives to sum of\nsquares optimization. In 2014 48th annual conference on infor-\nmation sciences and systems (CISS), pages 1–5. IEEE, 2014.\n[ART03]\nErling D Andersen, Cornelis Roos, and Tamas Terlaky.\nOn implementing a primal-dual interior-point method for\nconic quadratic optimization.\nMathematical Programming,\n95(2):249–277, 2003.\n[HLL09]\nD. Henrion, Jean-Bernard Lasserre, and J. Löfberg.\nGlop-\ntiPoly 3: moments, optimization and semideﬁnite program-\nming. Optimization Methods and Software, 24(4-5):pp. 761–779,\nAugust 2009.\n[HP11]\nEdward J Hancock and Antonis Papachristodoulou. Struc-\ntured sum of squares for networked systems analysis. In 2011\n50th IEEE Conference on Decision and Control and European Con-\ntrol Conference, pages 7236–7241. IEEE, 2011.\n[KKW05]\nMasakazu Kojima, Sunyoung Kim, and Hayato Waki. Spar-\nsity in sums of squares of polynomials. Mathematical Program-\nming, 103(1):45–62, 2005.\n[Las01]\nJean-Bernard Lasserre. Global Optimization with Polynomi-\nals and the Problem of Moments. SIAM Journal on Optimiza-\ntion, 11(3):796–817, 2001.\n[Lof09]\nJohan Lofberg. Pre-and post-processing sum-of-squares pro-\ngrams in practice.\nIEEE transactions on automatic control,\n54(5):1007–1011, 2009.\n[Lö04]\nJ. Löfberg. Yalmip : A toolbox for modeling and optimization\nin MATLAB. In Proceedings of the CACSD Conference, Taipei,\nTaiwan, 2004.\n\n138\nBibliography\n[MAT14]\nAnirudha Majumdar, Amir Ali Ahmadi, and Russ Tedrake.\nControl and veriﬁcation of high-dimensional systems with\ndsos and sdsos programming. In 53rd IEEE Conference on De-\ncision and Control, pages 394–401. IEEE, 2014.\n[Meg10]\nA Megretski. Systems polynomial optimization tools (spot).\nMassachusetts Inst. Technol., Cambridge, MA, USA, 2010.\n[Rez78]\nBruce Reznick. Extremal psd forms with few terms. Duke\nmathematical journal, 45(2):363–374, 1978.\n[WKK+08]\nHayato Waki, Sunyoung Kim, Masakazu Kojima, Masakazu\nMuramatsu,\nand\nHiroshi\nSugimoto.\nAlgorithm\n883:\nSparsepop—a sparse semideﬁnite programming relaxation\nof polynomial optimization problems. ACM Transactions on\nMathematical Software (TOMS), 35(2):1–13, 2008.\n[WLX19]\nJie Wang, Haokun Li, and Bican Xia. A new sparse sos de-\ncomposition algorithm based on term sparsity. In Proceedings\nof the 2019 on International Symposium on Symbolic and Algebraic\nComputation, pages 347–354, 2019.\n[WML21a]\nJie\nWang,\nVictor\nMagron,\nand\nJean-Bernard\nLasserre.\nChordal-tssos: a moment-sos hierarchy that exploits term\nsparsity with chordal extension. SIAM Journal on Optimiza-\ntion, 31(1):114–141, 2021.\n[WML21b]\nJie Wang, Victor Magron, and Jean-Bernard Lasserre. Tssos:\nA moment-sos hierarchy that exploits term sparsity. SIAM\nJournal on Optimization, 31(1):30–58, 2021.\n[WMLM20] Jie Wang,\nVictor Magron,\nJean B Lasserre,\nand Ngoc\nHoang Anh Mai.\nCs-tssos:\nCorrelative and term spar-\nsity for large-scale polynomial optimization. arXiv preprint\narXiv:2005.02828, 2020.\n[YC20]\nHeng Yang and Luca Carlone.\nOne ring to rule them all:\nCertiﬁably robust geometric perception with outliers. arXiv\npreprint arXiv:2006.06769, 2020.\n\nChapter 8\nExploiting both correlative\nand term sparsity\nIn previous chapters, we have seen that how CS or TS of POPs helps to\nreduce the size of SDP relaxations arising from the moment-SOS hierarchy.\nFor large-scale POPs, it is natural to ask whether one can exploit CS and\nTS simultaneously to further reduce the size of SDP relaxations. As we\nshall see in this chapter, the answer is afﬁrmative.\n8.1\nThe CS-TSSOS hierarchy\nThe underlying idea to exploit CS and TS simultaneously in the moment-SOS\nhierarchy consists of the following two steps:\n(1) decomposing the set of variables into a tuple of cliques {Ik}k∈[p] by\nexploiting CS as in Chapter 3;\n(2) applying the iterative procedure for exploiting TS to each decoupled\nsubsystem involving variables x(Ik) as in Chapter 7.\nMore concretely, let us ﬁx a relaxation order r ≥rmin. Suppose that\nGcsp is the csp graph associated to POP (2.4) deﬁned as in Chapter 3,\n(Gcsp)′ is a chordal extension of Gcsp, and Ik, k ∈[p] are the maximal\ncliques of (Gcsp)′ with cardinality being denoted by nk, k ∈[p]. As in\nChapter 3, the set of variables x is decomposed into x(I1), x(I2), . . . , x(Ip)\nby exploiting CS. In addition, assume that the constraints are assigned to\nthe variable cliques according to J1, . . . , Jp, J′ as deﬁned in Chapter 3.\n\n140\nChapter 8. Exploiting both correlative and term sparsity\nNow we apply the iterative procedure for exploiting TS to each sub-\nsystem involving variables x(Ik), k ∈[p] in the following way. Let\nA := supp( f ) ∪\nm\n[\nj=1\nsupp(gj) and Ak := {α ∈A | supp(α) ⊆Ik}\n(8.1)\nfor k ∈[p]. Let Nnk\nr−dj be the standard monomial basis for j ∈{0} ∪Jk, k ∈\n[p]. Let Gtsp\nr,k be the tsp graph with nodes Nnk\nr\nassociated to the support\nAk deﬁned as in Chapter 7, i.e., its node set is Nnk\nr\nand {β, γ} is an edge\nif β + γ ∈Ak ∪2Nnk\nr . Note that here we embed Nnk\nr into Nn\nr via the map\nα = (αi) ∈Nnk\nr 7→α′ = (α′\ni) ∈Nn\nr satisfying\nα′\ni =\n(\nαi,\nif i ∈Ik,\n0,\notherwise.\nLet us deﬁne G(0)\nr,k,0 := Gtsp\nr,k and G(0)\nr,k,j, j ∈Jk, k ∈[p] are all empty\ngraphs. Next for each j ∈{0} ∪Jk and each k ∈[p], we iteratively deﬁne\nan ascending chain of graphs (G(s)\nr,k,j(Vr,k,j, E(s)\nr,k,j))s≥1 with Vr,k,j := Nnk\nr−dj\nvia two successive operations:\n(1) support extension. Deﬁne F(s)\nr,k,j to be the graph with nodes Vr,k,j and\nwith edges\nE(F(s)\nr,k,j) =\nn\n{β, γ} | β ̸= γ ∈Vr,k,j,\n\u0000β + γ + supp(gj)\n\u0001 ∩C (s−1)\nr\n̸= ∅\no\n,\n(8.2)\nwhere\nC (s−1)\nr\n:=\np[\nk=1\n\n\n[\nj∈{0}∪Jk\n\u0010\nsupp(gj) + supp(G(s−1)\nr,k,j )\n\u0011\n\n.\n(8.3)\n(2) chordal extension. Let\nG(s)\nr,k,j := (F(s)\nr,k,j)′,\nj ∈{0} ∪Jk, k ∈[p].\n(8.4)\nIt is clear by construction that the sequences of graphs (G(s)\nr,k,j)s≥1 stabilize\nfor all j ∈{0} ∪Jk, k ∈[p] after ﬁnitely many steps.\nExample 8.1 Let f = 1 + x2\n1 + x2\n2 + x2\n3 + x1x2 + x2x3 + x3 and consider the\nunconstrained POP: min{ f (x) : x ∈Rn}. Take the relaxation order r = rmin =\n1. The variables are decomposed into two cliques: {x1, x2} and {x2, x3}. The tsp\ngraphs with respect to these two cliques are illustrated in Figure 8.1. The left\ngraph corresponds to the ﬁrst clique: x1 and x2 are connected because of the term\n\n8.1. The CS-TSSOS hierarchy\n141\nx1x2. The right graph corresponds to the second clique: 1 and x3 are connected\nbecause of the term x3; x2 and x3 are connected because of the term x2x3. It is\nnot hard to see that the graph sequences (G(s)\n1,k)s≥1, k = 1, 2 (the subscript j is\nomitted here since there is no constraint) stabilize at s = 2 if the maximal chordal\nextension is used in (8.4).\n1\nx2\nx1\n1\nx3\nx2\nFigure 8.1: The tsp graphs of Example 8.1.\nLet tk,j := |Nnk\nr−dj| = (nk+r−dj\nr−dj ) for all k, j. Then with s ≥1, the moment\nrelaxation based on correlative-term sparsity for POP (2.4) is given by\nPr,s\ncs-ts :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\ny\nLy( f )\ns.t.\nBG(s)\nr,k,0\n◦Mr(y, Ik) ∈ΠG(s)\nr,k,0\n(S+\ntk,0),\nk ∈[p]\nBG(s)\nr,k,j\n◦Mr−dj(gjy, Ik) ∈ΠG(s)\nr,k,j\n(S+\ntk,j),\nj ∈Jk, k ∈[p]\nLy(gj) ≥0,\nj ∈J′\ny0 = 1\n(8.5)\nwith optimum denoted by f r,s\ncs-ts.\nFor all k, j, let us write Mr−dj(gjy, Ik) = ∑α Dk,j\nα yα for appropriate sym-\nmetry matrices {Dk,j\nα } and gj = ∑α gj,αxα. Then for each s ≥1, the dual of\nPr,s\ncs-ts (8.5) reads as\n(Pr,s\ncs-ts)∗:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsup\nGk,j,λj,b\nb\ns.t.\n∑\np\nk=1 ∑j∈{0}∪Jk⟨Gk,j, Dk,j\nα ⟩+ ∑j∈J′ λjgj,α\n+bδ0α = fα,\n∀α ∈C (s)\nr\nGk,j ∈S\ntk,j\n+ ∩SG(s)\nr,k,j\n,\nj ∈{0} ∪Jk, k ∈[p]\nλj ≥0,\nj ∈J′\n(8.6)\nwhere C (s)\nr\nis deﬁned in (8.3).\nThe primal-dual SDP relaxations (8.5)–(8.6) is called the CS-TS adpated\nmoment-SOS (CS-TSSOS) hierarchy associated with P (2.4), which is in-\ndexed by two parameters: the relaxation order r and the sparse order s.\n\n142\nChapter 8. Exploiting both correlative and term sparsity\n1\n2\n3\n4\n5\n6\nFigure 8.2: The csp graph of Example 8.2.\n1\nx2\n3\nx2\n2\nx2\n1\nx1\nx2\nx3\nx2x3\nx1x3\nx1x2\nFigure 8.3: The tsp graph for the ﬁrst clique of Example 8.2.\nExample 8.2 Let f = 1 + ∑6\ni=1 x4\ni + x1x2x3 + x3x4x5 + x3x4x6 + x3x5x6 +\nx4x5x6, and consider the unconstrained POP: min{ f (x) : x ∈Rn}. Let us\napply the CS-TSSOS hierarchy (using the maximal chordal extension in (8.4))\nto this problem by taking r = rmin = 2, s = 1. First, according to the csp\ngraph (Figure 8.2), we decompose the variables into two cliques: {x1, x2, x3} and\n{x3, x4, x5, x6}. The tsp graphs for the ﬁrst clique and the second clique are shown\nin Figure 8.3 and Figure 8.4, respectively. For the ﬁrst clique one obtains four\nblocks of SDP matrices with respective sizes 4, 2, 2, 2. For the second clique one\nobtains two blocks of SDP matrices with respective sizes 5, 10. As a result, the\noriginal SDP matrix of size 28 has been reduced to six blocks of maximal size 10.\nAlternatively, if one applies the TSSOS hierarchy (using the maximal chordal\nextension in (7.15)) directly to this problem by taking r = rmin = 2, s = 1 (i.e.,\nwithout decomposing variables), then the tsp graph is shown in Figure 8.5 and\none thereby obtains 11 PSD blocks with respective sizes 7, 2, 2, 2, 1, 1, 1, 1, 1, 1, 10.\nCompared to the CS-TSSOS case, there are six additional blocks of size one and\nthe two blocks with respective sizes 4, 5 are replaced by a single block of size 7.\nWe summarize the basic properties of the CS-TSSOS hierarchy in the\nnext theorem.\nTheorem 8.3 Let f ∈R[x] and X be deﬁned as in (2.1). Then the following\nhold:\n\n8.1. The CS-TSSOS hierarchy\n143\n1\nx2\n6\nx2\n5\nx2\n4\nx2\n3\nx3\nx5x6\nx4x6\nx4x5\nx3x6\nx3x5\nx3x4\nx6\nx4\nx5\nFigure 8.4: The tsp graph for the second clique of Example 8.2\n.\n1\nx2\n1\nx2\n2\nx2\n3\nx2\n4\nx2\n5\nx2\n6\nx1\nx2\nx3\nx2x3\nx1x3\nx1x2\nx1x4\nx1x5\nx1x6\nx2x4\nx2x5\nx2x6\nx3\nx5x6\nx4x6\nx4x5\nx3x6\nx3x5\nx3x4\nx6\nx4\nx5\nFigure 8.5: The tsp graph without decomposing variables of Example 8.2.\n1. If X has a nonempty interior, then there is no duality gap between Pr,s\ncs-ts\nand (Pr,s\ncs-ts)∗for any r ≥rmin and s ≥1.\n2. For any r ≥rmin, the sequence ( f r,s\ncs-ts)s≥1 is monotonically non-\ndecreasing and f r,s\ncs-ts ≤f r\ncs for all s with f r\ncs being deﬁned in Sec-\ntion 3.3.\n3. For any s ≥1, the sequence ( f r,s\ncs-ts)r≥rmin is monotonically non-\ndecreasing.\nPROOF 1. By the duality theory of convex programming, this easily fol-\nlows from Theorem 3.6 of [Las06] and Theorem 1.4.\n2. By construction, we have G(s)\nr,k,j ⊆G(s+1)\nr,k,j\nfor all r, k, j and for all s.\n\n144\nChapter 8. Exploiting both correlative and term sparsity\nIt follows that each maximal clique of G(s)\nr,k,j is contained in some maximal\nclique of G(s+1)\nr,k,j . Hence by Theorem 1.4, Pr,s\ncs-ts is a relaxation of Pr,s+1\ncs-ts and\nis clearly also a relaxation of Pr\ncs. Therefore, ( f r,s\ncs-ts)s≥1 is monotonically\nnon-decreasing and f r,s\ncs-ts ≤f r\ncs for all s.\n3. The conclusion follows if we can show that the inclusion G(s)\nr,k,j ⊆\nG(s)\nr+1,k,j holds for all r, k, j, s, since by Theorem 1.4 this implies that Pr,s\ncs-ts\nis a relaxation of Pr+1,s\ncs-ts . Let us prove G(s)\nr,k,j ⊆G(s)\nr+1,k,j by induction on s.\nFor s = 1, we have G(0)\nr,k,0 = Gtsp\nr,k ⊆Gtsp\nr+1,k = G(0)\nr+1,k,0, which together\nwith (8.2)-(8.3) implies that F(1)\nr,k,j ⊆F(1)\nr+1,k,j for j ∈{0} ∪Jk, k ∈[p]. It\nthen follows that G(1)\nr,k,j = (F(1)\nr,k,j)′ ⊆(F(1)\nr+1,k,j)′ = G(1)\nr+1,k,j. Now assume\nthat G(s)\nr,k,j ⊆G(s)\nr+1,k,j, j ∈{0} ∪Jk, k ∈[p], hold for some s ≥1. Then by\n(8.2)-(8.3) and by the induction hypothesis, we have F(s+1)\nr,k,j\n⊆F(s+1)\nr+1,k,j for\nj ∈{0} ∪Jk, k ∈[p]. Thus G(s+1)\nr,k,j\n= (F(s+1)\nr,k,j\n)′ ⊆(F(s+1)\nr+1,k,j)′ = G(s+1)\nr+1,k,j which\ncompletes the induction.\n2\nFrom Theorem 8.3, we deduce the following two-level hierarchy of\nlower bounds for the optimum fmin of P (2.4):\nf rmin,1\ncs-ts\n≤\nf rmin,2\ncs-ts\n≤\n· · ·\n≤\nf rmin\ncs-ts\n≥\n≥\n≥\nf rmin+1,1\ncs-ts\n≤\nf rmin+1,2\ncs-ts\n≤\n· · ·\n≤\nf rmin+1\ncs-ts\n≥\n≥\n≥\n...\n...\n...\n...\n≥\n≥\n≥\nf r,1\ncs-ts\n≤\nf r,2\ncs-ts\n≤\n· · ·\n≤\nf r\ncs-ts\n≥\n≥\n≥\n...\n...\n...\n...\n(8.7)\nAs we have known for the TSSOS hierarchy, the block structure arising\nfrom the CS-TSSOS hierarchy is consistent with the sign symmetries of the\nPOP. More precisely, we have the following theorem.\nTheorem 8.4 Let A be deﬁned as in (8.1), C (s)\nr\nbe deﬁned as in (8.3), and\nassume that the sign symmetries of A are represented by the column vectors\nof the binary matrix R. Then for any r ≥rmin, s ≥1 and any α ∈C (s)\nr\n, it\n\n8.2. Global convergence\n145\nholds R⊺α ≡0 (mod 2). As a consequence, if β, γ belong to the same block\nin the CS-TSSOS relaxations, then R⊺(β + γ) ≡0 (mod 2).\n8.2\nGlobal convergence\nWe next show that if the chordal extension in (8.5) is chosen to be maximal,\nthen for any relaxation order r ≥rmin, the sequence of optima ( f r,s\ncs-ts)s≥1\narising from the CS-TSSOS hierarchy converges to the optimum f r\ncs of the\nCSSOS relaxation.\nIt is clear by construction that the sequences of graphs (G(s)\nr,k,j)s≥1 sta-\nbilize for all j ∈{0} ∪Jk, k ∈[p] after ﬁnitely many steps. We denote the\nresulting stabilized graphs by G(•)\nr,k,j, j ∈{0} ∪Jk, k ∈[p] and the corre-\nsponding SDP (8.5) by Pr,•\ncs-ts.\nTheorem 8.5 If one uses the maximal chordal extension in (8.4), then for\nany r ≥rmin, the sequence ( f r,s\ncs-ts)s≥1 converges to f r\ncs in ﬁnitely many\nsteps.\nPROOF Let y = (yα) be an arbitrary feasible solution of Pr,•\ncs-ts and f r,•\ncs-ts\nbe the optimum of Pr,•\ncs-ts. Note that {yα | α ∈Sp\nk=1(∪j∈{0}∪Jk(supp(gj) +\nsupp(G(•)\nr,k,j)))} is the set of decision variables involved in Pr,•\ncs-ts. Let R be\nthe set of decision variables involved in Pr\ncs (3.6). We then deﬁne a vector\ny = (yα)α∈R as follows:\nyα =\n(\nyα,\nif α ∈Sp\nk=1(∪j∈{0}∪Jk(supp(gj) + supp(G(•)\nr,k,j))),\n0,\notherwise.\nBy construction and since G(•)\nr,k,j stabilizes under support extension for all\nk, j, we have Mr−dj(gjy, Ik) = BG(•)\nr,k,j\n◦Mr−dj(gjy, Ik).\nAs the maximal\nchordal extension is chosen for (8.4), the matrix BG(•)\nr,k,j\n◦Mr−dj(gjy, Ik) is\nblock diagonal up to permutation. It follows from BG(•)\nr,k,j\n◦Mr−dj(gjy, Ik) ∈\nΠG(•)\nr,k,j\n(S\ntk,j\n+ ) that Mr−dj(gjy, Ik) ⪰0 for j ∈{0} ∪Jk, k ∈[p]. Therefore y is\na feasible solution of Pr\ncs and so Ly( f ) = Ly( f ) ≥f r\ncs. Hence f r,•\ncs-ts ≥f r\ncs\n\n146\nChapter 8. Exploiting both correlative and term sparsity\nsince y is an arbitrary feasible solution of Pr,•\ncs-ts. By Theorem 8.3, we al-\nready have f r,•\ncs-ts ≤f r\ncs. Therefore, f r,•\ncs-ts = f r\ncs.\n2\nBy Theorem 3.6 in [Las06], the sequence ( f r\ncs)r≥rmin converges to the\nglobal optimum fmin of POP (2.4) (after adding some redundant quadratic\nconstraints). Therefore, this together with Theorem 8.5 offers the global\nconvergence of the CS-TSSOS hierarchy.\nProceeding along Theorem 8.3, we are able to provide a sparse repre-\nsentation theorem based on both CS and TS for a polynomial positive on a\ncompact basic semialgebraic set.\nTheorem 8.6 Let f ∈R[x], X ⊆Rn and {Ik}p\nk=1, {Jk}p\nk=1 be deﬁned in\nAssumption (3.1). Assume that the sign symmetries of A = supp( f ) ∪\nSm\nj=1 supp(gj) are represented by the columns of the binary matrix R. If f\nis positive on X, then f admits a representation of form\nf =\np\n∑\nk=1\n \nσk,0 + ∑\nj∈Jk\nσk,jgj\n!\n,\n(8.8)\nfor some polynomials σk,j ∈Σ[x(Ik)], j ∈{0} ∪Jk, k ∈[p], satisfying\nR⊺α ≡0 (mod 2) for any α ∈supp(σk,j).\nPROOF By Corollary 3.9 of [Las06] (or Theorem 5 of [GNS07]), there exist\npolynomials σ′\nk,j ∈Σ[x(Ik)], j ∈{0} ∪Jk, k ∈[p] such that\nf =\np\n∑\nk=1\n \nσ′\nk,0 + ∑\nj∈Jk\nσ′\nk,jgj\n!\n.\n(8.9)\nLet r = max{⌈deg(σ′\nk,jgj)/2⌉: j ∈{0} ∪Jk, k ∈[p]}.\nLet G′\nk,j be a\nPSD Gram matrix associated with σ′\nk,j and indexed by the monomial basis\nNnk\nr−dj. Then for all k, j, we deﬁne Gk,j ∈Stk,j (indexed by Nnk\nr−dj) by\n[Gk,j]βγ :=\n(\n[Q′\nk,j]βγ,\nif R⊺(β + γ) ≡0 (mod 2),\n0,\notherwise,\nand let σk,j = (x\nN\nnk\nr−dj )⊺Gk,jx\nN\nnk\nr−dj . One can easily verify that Gk,j is block\ndiagonal up to permutation (see also [WML21]) and each block is a prin-\ncipal submatrix of G′\nk,j. Then the positive semideﬁniteness of G′\nk,j implies\nthat Gk,j is also positive semideﬁnite. Thus σk,j ∈Σ[x(Ik)].\n\n8.3. Extracting a solution\n147\nBy construction, substituting σ′\nk,j with σk,j in (8.9) boils down to remov-\ning the terms with exponents α that do not satisfy R⊺α ≡0 (mod 2) from\nthe right hand side of (8.9). Since any α ∈supp( f ) satisﬁes R⊺α ≡0\n(mod 2), this does not change the match of coefﬁcients on both sides of\nthe equality. Thus we obtain\nf =\np\n∑\nk=1\n \nσk,0 + ∑\nj∈Jk\nσk,jgj\n!\nwith the desired property.\n2\n8.3\nExtracting a solution\nIn the case of the dense moment-SOS hierarchy, there is a standard pro-\ncedure described in [HL05] to extract globally optimal solutions when\nthe moment matrix satisﬁes the so-called ﬂatness condition. This proce-\ndure was generalized to the correlative sparse setting in [Las06, § 3.3] and\n[ND09]. In the term sparse setting, however, the corresponding proce-\ndure cannot be applied because the information on the moment matrix\nis incomplete. In order to extract a solution in this case, we may add an\norder-one (dense) moment matrix for each clique in (8.5):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\ny\nLy( f )\ns.t.\nMr(y, Ik) ∈ΠG(s)\nr,k,0\n(S+\ntk,0),\nk ∈[p]\nM1(y, Ik) ⪰0,\nk ∈[p]\nMr−dj(gjy, Ik) ∈ΠG(s)\nr,k,j\n(S+\ntk,j),\nj ∈Jk,\nk ∈[p]\nLy(gj) ≥0,\nj ∈J′\ny0 = 1\n(8.10)\nLet yopt be an optimal solution of (8.10). Typically, M1(yopt, Ik) (after\nidentifying sufﬁciently small entries with zeros) is a block diagonal matrix\n(up to permutation). If for all k every block of M1(yopt, Ik) is of rank one,\nthen a globally optimal solution xopt to P (2.4) which is unique up to sign\nsymmetries can be extracted ([Las06, Theorem 3.7]), and the global opti-\nmality is certiﬁed. Otherwise, the relaxation might be not exact or yield\nmultiple global solutions.\nRemark 8.7 Note that (8.10) is a tighter relaxation of P (2.4) than Pr,s\ncs-ts (8.5),\nand so might provide a better lower bound for P. In particular, if P is a QCQP,\nthen (8.10) is always tighter than Shor’s relaxation of P.\n\n148\nChapter 8. Exploiting both correlative and term sparsity\n8.4\nA minimal initial relaxation step\nFor POP (2.4), suppose that f is not a homogeneous polynomial or the\nconstraint polynomials {gj}j∈[m] are of different degrees. Then instead of\nusing the uniform minimum relaxation order rmin, it might be more bene-\nﬁcial, from the computational point of view, to assign different relaxation\norders to different subsystems obtained from the csp for the initial relax-\nation step of the CS-TSSOS hierarchy. To this end, we redeﬁne the csp\ngraph Gicsp(V, E) as follows: V = [n] and {i, j} ∈E whenever there exists\nα ∈A such that {i, j} ⊆supp(α). This is clearly a subgraph of Gcsp de-\nﬁned in Chapter 3 and hence typically admits a smaller chordal extension.\nLet (Gicsp)′ be a chordal extension of Gicsp and {Ik}k∈[p] be the list of max-\nimal cliques of (Gicsp)′ with nk := |Ik|. Now we partition the constraint\npolynomials {gj}j∈[m] into groups {gj | j ∈Jk}k∈[p] and {gj | j ∈J′} which\nsatisfy\n(1) J1, . . . , Jp, J′ ⊆[m] are pairwise disjoint and Sp\nk=1 Jk ∪J′ = [m];\n(2) for any j ∈Jk, S\nα∈supp(gj) supp(α) ⊆Ik, k ∈[p];\n(3) for any j ∈J′, S\nα∈supp(gj) supp(α) ⊈Ik for all k ∈[p].\nSuppose f decomposes as f = ∑k∈[p] fk such that S\nα∈supp( fk) supp(α) ⊆\nIk for k ∈[p]. We deﬁne the vector of minimum relaxation orders o =\n(ok)k ∈Np with ok := max{{dj : j ∈Jk} ∪{⌈deg( fk)/2⌉}}. Then with s ≥\n1, we deﬁne the following minimal initial relaxation step of the CS-TSSOS\nhierarchy:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\ny\nLy( f )\ns.t.\nBG(s)\nok,k,0\n◦Mok(y, Ik) ∈ΠG(s)\nok,k,0\n(S\ntk,0\n+ ),\nk ∈[p]\nBG(s)\nok,k,j\n◦Mok−dj(gjy, Ik) ∈ΠG(s)\nok,k,j\n(S\ntk,j\n+ ),\nj ∈Jk, k ∈[p]\nLy(gj) ≥0,\nj ∈J′\ny0 = 1\n(8.11)\nwhere G(s)\nok,k,j, j ∈Jk, k ∈[p] are deﬁned in the same spirit with Chapter 8.1\nand tk,j := (nk+ok−dj\nok−dj ) for all k, j.\n8.5\nNumerical experiments\nIn this section, we report some numerical results of the proposed CS-TSSOS\nhierarchy. All numerical examples were computed on an Intel Core i5-\n8265U@1.60GHz CPU with 8GB RAM memory.\n\n8.5. Numerical experiments\n149\n8.5.1\nBenchmarks for constrained POPs\nConsider the following POP:\n\n\n\ninf\nx\nfgR\n(resp. fBt or fcW)\ns.t.\n1 −(∑\n20j\ni=20j−19 x2\ni ) ≥0,\nj = 1, 2, . . . , n/20\n(8.12)\nwith 20|n, where the objective function is respectively given by\n• the generalized Rosenbrock function\nfgR(x) = 1 +\nn\n∑\ni=2\n(100(xi −x2\ni−1)2 + (1 −xi)2),\n• the Broyden tridiagonal function\nfBt(x) = ((3 −2x1)x1 −2x2 + 1)2 +\nn−1\n∑\ni=2\n((3 −2xi)xi −xi−1 −2xi+1 + 1)2\n+ ((3 −2xn)xn −xn−1 + 1)2,\n• the chained Wood function\nfcW(x) = 1 + ∑\ni∈J\n(100(xi+1 −x2\ni )2 + (1 −xi)2 + 90(xi+3 −x2\ni+2)2\n+ (1 −xi+2)2 + 10(xi+1 + xi+3 −2)2 + 0.1(xi+1 −xi+3)2),\nwhere J = {1, 3, 5, . . . , n −3} and 4|n.\nWe solve the CSSOS relaxation with r = 2, the TSSOS relaxation with\nr = 2, s = 1, and the CS-TSSOS relaxation with r = 2, s = 1, where approx-\nimately smallest chordal extensions are used for TS. The results are pre-\nsented in Tables 8.1–8.3, in which “mb” denotes the maximal size of PSD\nblocks involved in the relaxations, “opt” denotes the optimum, “time” de-\nnotes running time in seconds, and “-” indicates an out of memory error.\nWe see that CSSOS, TSSOS and CS-TSSOS yield almost the same optimum\nwhile CS-TSSOS is the most efﬁcient and scalable approach among them.\n8.5.2\nThe Max-Cut problem\nThe Max-Cut problem is one of the basic combinatorial optimization prob-\nlems, which is known to be NP-hard. Let G(V, E) be an undirected graph\nwith V = {1, . . . , n} and with edge weights wij for {i, j} ∈E. Then the\nMax-Cut problem for G can be formulated as a QCQP in binary variables:\n(\nmax\nx\n1\n2 ∑{i,j}∈E wij(1 −xixj)\ns.t.\n1 −x2\ni = 0,\ni = 1, . . . , n.\n(8.13)\n\n150\nChapter 8. Exploiting both correlative and term sparsity\nTable 8.1: Results for the generalized Rosenbrock function (r = 2).\nn\nCSSOS\nTSSOS\nCS-TSSOS\nmb\nopt\ntime\nmb\nopt\ntime\nmb\nopt\ntime\n100\n231\n97.445\n377\n101\n97.436\n31.3\n21\n97.436\n0.54\n200\n231\n-\n-\n201\n196.41\n1327\n21\n196.41\n1.27\n300\n231\n-\n-\n-\n-\n-\n21\n295.39\n2.26\n400\n231\n-\n-\n-\n-\n-\n21\n394.37\n3.36\n500\n231\n-\n-\n-\n-\n-\n21\n493.35\n4.65\n1000\n231\n-\n-\n-\n-\n-\n21\n988.24\n15.8\nTable 8.2: Results for the Broyden tridiagonal function (r = 2).\nn\nCSSOS\nTSSOS\nCS-TSSOS\nmb\nopt\ntime\nmb\nopt\ntime\nmb\nopt\ntime\n100\n231\n79.834\n519\n103\n79.834\n104\n23\n79.834\n1.96\n200\n231\n-\n-\n-\n-\n-\n23\n160.83\n4.88\n300\n231\n-\n-\n-\n-\n-\n23\n241.83\n8.67\n400\n231\n-\n-\n-\n-\n-\n23\n322.83\n13.3\n500\n231\n-\n-\n-\n-\n-\n23\n403.83\n19.9\n1000\n231\n-\n-\n-\n-\n-\n23\n808.83\n57.5\nTable 8.3: Results for the chained Wood function (r = 2).\nn\nCSSOS\nTSSOS\nCS-TSSOS\nmb\nopt\ntime\nmb\nopt\ntime\nmb\nopt\ntime\n100\n231\n1485.8\n505\n101\n1485.8\n43.2\n21\n1485.8\n0.73\n200\n231\n-\n-\n201\n3004.5\n1238\n21\n3004.5\n1.91\n300\n231\n-\n-\n-\n-\n-\n21\n4523.6\n3.39\n400\n231\n-\n-\n-\n-\n-\n21\n6042.0\n5.72\n500\n231\n-\n-\n-\n-\n-\n21\n7560.7\n7.88\n1000\n231\n-\n-\n-\n-\n-\n21\n15155\n23.0\n\n8.6. Notes and sources\n151\nFor the numerical experiments, we construct random Max-Cut instances\nwith a block-band sparsity pattern (illustrated in Figure 8.6) which consists\nof l blocks of size 25 and two bands of width 5. There are ten such Max-Cut\ninstances with l = 20, 40, 60, 80, 100, 120, 140, 160, 180, 200, respectively1.\nh\nh\nb\nb\nl blocks\nFigure 8.6: The block-band sparsity pattern. l: the number of blocks, b: the\nsize of blocks, h: the width of bands.\nFor each instance, we solve Shor’s relaxation, the CSSOS hierarchy\nwith r = 2, and the CS-TSSOS hierarchy with r = 2, s = 1, where the\nmaximal chordal extension is used for TS. The results are reported in Ta-\nble 8.4. We see that for each instance, both CSSOS and CS-TSSOS signif-\nicantly improve the bound obtained from Shor’s relaxation. Meanwhile,\nCS-TSSOS is several times faster than CSSOS at the cost of possibly pro-\nviding a sightly weaker bound.\n8.6\nNotes and sources\nThe material from this chapter is issued from [WMLM20]. A proof of The-\norem 8.4 can be found in Section 3.2 of [WMLM20].\nNewton and Papachristodoulou have used the CS-TSSOS hierarchy for\nneural network veriﬁcation in [NP22].\n1The instances are available at https://wangjie212.github.io/jiewang/code.html.\n\n152\nChapter 8. Exploiting both correlative and term sparsity\nTable 8.4: Results for Max-Cut instances. Only integer parts of optima are\npreserved. “mc” denotes the maximal size of variable cliques.\nname\nnode\nedge\nmc\nShor\nCSSOS\nCS-TSSOS\nopt\nmb\nopt\ntime\nmb\nopt\ntime\ng20\n505\n2045\n14\n570\n120\n488\n51.2\n92\n488\n19.6\ng40\n1005\n3441\n14\n1032\n120\n885\n134\n92\n893\n41.1\ng60\n1505\n4874\n14\n1439\n120\n1227\n183\n92\n1247\n71.3\ng80\n2005\n6035\n15\n1899\n136\n1638\n167\n106\n1669\n84.8\ng100\n2505\n7320\n14\n2398\n120\n2073\n262\n92\n2128\n112\ng120\n3005\n8431\n14\n2731\n120\n2358\n221\n79\n2443\n127\ng140\n3505\n9658\n13\n3115\n105\n2701\n250\n79\n2812\n153\ng160\n4005\n10677\n14\n3670\n120\n3202\n294\n79\n3404\n166\ng180\n4505\n12081\n13\n4054\n105\n3525\n354\n79\n3666\n246\ng200\n5005\n13240\n13\n4584\n105\n4003\n374\n79\n4218\n262\n\nBibliography\n[GNS07]\nDavid Grimm, Tim Netzer, and Markus Schweighofer. A note\non the representation of positive polynomials with structured\nsparsity. Archiv der Mathematik, 89(5):399–403, 2007.\n[HL05]\nD. Henrion and Jean-Bernard Lasserre. Detecting Global Op-\ntimality and Extracting Solutions in GloptiPoly, pages 293–310.\nSpringer Berlin Heidelberg, Berlin, Heidelberg, 2005.\n[Las06]\nJean B Lasserre. Convergent sdp-relaxations in polynomial\noptimization with sparsity.\nSIAM Journal on Optimization,\n17(3):822–843, 2006.\n[ND09]\nJiawang Nie and James Demmel. Sparse sos relaxations for\nminimizing functions that are summations of small polyno-\nmials. SIAM Journal on Optimization, 19(4):1534–1558, 2009.\n[NP22]\nMatthew Newton and Antonis Papachristodoulou.\nSparse\npolynomial optimisation for neural network veriﬁcation.\narXiv preprint arXiv:2202.02241, 2022.\n[WML21]\nJie Wang, Victor Magron, and Jean-Bernard Lasserre. Tssos:\nA moment-sos hierarchy that exploits term sparsity. SIAM\nJournal on Optimization, 31(1):30–58, 2021.\n[WMLM20] Jie Wang,\nVictor Magron,\nJean B Lasserre,\nand Ngoc\nHoang Anh Mai.\nCs-tssos:\nCorrelative and term spar-\nsity for large-scale polynomial optimization. arXiv preprint\narXiv:2005.02828, 2020.\n\n\nChapter 9\nApplication in optimal\npower ﬂow\nIn this chapter, we apply the CS-TSSOS hierarchy to the famous alternating\ncurrent optimal power ﬂow (AC-OPF) problem, which can be formulized\nas a POP either with real variables [BEGL20, GMM15] or with complex\nvariables [JM18]. To tackle POPs in complex variables, we ﬁrst provide\ningredients for extending the moment-SOS hierarchy to the complex case.\n9.1\nExtension to complex polynomial optimiza-\ntion\nWe start by introducing some notations. Let z = (z1, . . . , zn) be a tuple\nof complex variables and ¯z = (¯z1, . . . , ¯zn) be its conjugate. We denote\nby C[z] := C[z1, . . . , zn], C[z, ¯z] := C[z1, . . . , zn, ¯z1, . . . , ¯zn] the complex\npolynomial ring in z, the complex polynomial ring in z, ¯z, respectively.\nA polynomial f ∈C[z, ¯z] can be written as f = ∑(β,γ)∈A fβ,γzβ ¯zγ with\nA ⊆Nn × Nn and fβ,γ ∈C, zβ = zβ1\n1 · · · zβn\nn , ¯zγ = ¯zγ1\n1 · · · ¯zγn\nn . The support\nof f is deﬁned by supp( f ) = {(β, γ) ∈A | fβ,γ ̸= 0}. The conjugate of f\nis ¯f = ∑(β,γ)∈A ¯fβ,γzγ ¯zβ. A polynomial σ = ∑(β,γ) σβ,γzβ ¯zγ ∈C[z, ¯z]\nis called an Hermitian sum of squares (HSOS) if there exist polynomials\nfi ∈C[z], i ∈[t] such that σ = ∑t\ni=1 fi ¯fi. For a positive integer m, the\nset of m × m Hermitian matrices is denoted by Hm and the set of m × m\nPSD Hermitian matrices is denoted by H+\nm.\nA complex polynomial optimization problem (CPOP) is given by\n(\ninf\nz∈Cn\nf (z, ¯z) := ∑α,β fα,βzα ¯zβ\ns.t.\ngj(z, ¯z) := ∑α,β gj,α,βzα ¯zβ ≥0,\nj ∈[m]\n(9.1)\n\n156\nChapter 9. Application in optimal power ﬂow\nwhere the functions f, g1, . . . , gm are real-valued polynomials and their co-\nefﬁcients satisfy fα,β = ¯fβ,α, and gj,α,β = ¯gj,β,α. There are two ways to\nconstruct a “moment-SOS” hierarchy for CPOP (9.1). The ﬁrst way is in-\ntroducing real variables for both real and imaginary parts of each complex\nvariable in (9.1), i.e., letting zi = xi + xi+ni for i ∈[n]. Then one can\nconvert CPOP (9.1) to a POP involving only real variables at the price of\ndoubling the number of variables. Hence the usual real moment-SOS hier-\narchy applies to the resulting real POP. On the other hand, as the second\nway, it might be advantageous to handle CPOP (9.1) directly with the com-\nplex moment-HSOS hierarchy introduced in [JM18]. To this end, we deﬁne\nthe complex moment matrix Mc\nr(y) (r ∈N) by\n[Mc\nr(y)]β,γ := Lc\ny(zβ ¯zγ) = yβ,γ,\n∀β, γ ∈Nn\nr ,\nwhere y = (yβ,γ)(β,γ)∈Nn×Nn ⊆C is a sequence indexed by (β, γ) ∈Nn ×\nNn satisfying yβ,γ = ¯yγ,β, and Lc\ny : C[z, ¯z] →R is the linear functional\nsuch that\nf = ∑\n(β,γ)\nfβ,γzβ ¯zγ 7→Lc\ny( f ) = ∑\n(β,γ)\nfβ,γyβ,γ.\nSuppose that g = ∑(β′,γ′) gβ′,γ′zβ′ ¯zγ′ ∈C[z, ¯z] is an Hermitian polynomial,\ni.e., ¯g = g. The complex localizing matrix Mc\nr(gy) associated with g and y\nis deﬁned by\n[Mc\nr(g y)]β,γ := Lc\ny(g zβ ¯zγ) = ∑\n(β′,γ′)\ngβ′,γ′yβ+β′,γ+γ′,\n∀β, γ ∈Nn\nr .\nBoth the complex moment matrix and the complex localizing matrix are\nHermitian matrices. Note that a distinguished difference between the usual\n(real) moment matrix and the complex moment matrix is that the former\nhas the Hankel property (i.e., [Mr(y)]β,γ is a function of β + γ), whereas\nthe latter does not have.\nLet dj = ⌈deg(gj)/2⌉, j ∈[m] and rmin = max{⌈deg( f )/2⌉, d1, . . . , dm}\nas before. Then the complex moment hierarchy indexed by r ≥rmin for\nCPOP (9.1) is given by\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\ny\nLc\ny( f )\ns.t.\nMc\nr(y) ⪰0\nMc\nr−dj(gjy) ⪰0,\nj ∈[m]\ny0,0 = 1\n(9.2)\n\n9.2. Applications to optimal power ﬂow\n157\nThe dual of (9.2) can be formulized as the following HSOS relaxation:\n\n\n\n\n\n\n\n\n\n\n\n\n\nsup\nσj,b\nb\ns.t.\nf −b = σ0 + σ1g1 + . . . + σmgm\nσj is an HSOS,\nj ∈{0} ∪[m]\ndeg(σ0) ≤2r, deg(σjgj) ≤2r,\nj ∈[m]\n(9.3)\nRemark 9.1 In (9.2), the expression “X ⪰0\" means an Hermitian matrix X to\nbe PSD. Since popular SDP solvers deal with only real SDPs, it is then necessary\nto convert this constraint to a constraint involving only real matrices. This can\nbe done by introducing the real part A and the imaginary part B of X respectively\nsuch that X = A + Bi. Then,\nX ⪰0\n⇐⇒\n\u0014\nA\n−B\nB\nA\n\u0015\n⪰0.\nRemark 9.2 The ﬁrst-order moment-(H)SOS relaxation for QCQPs is also known\nas Shor’s relaxation. It was proved in [JM15] that the real Shor’s relaxation and\nthe complex Shor’s relaxation for homogeneous QCQPs yield the same bound.\nHowever, in general the complex hierarchy is weaker (i.e., producing looser bounds)\nthan the real hierarchy at the same relaxation order r > 1 as Hermitian sums of\nsquares are a special case of real sums of squares; see [JM18].\nRemark 9.3 By the complex Positivstellensatz theorem due to D’Angelo and\nPutinar [DP09], global convergence of the complex hierarchy is guaranteed when\na sphere constraint is present.\nAs for the usual moment-SOS hierarchy, we can reduce the size of SDP\nrelaxations arising from the complex moment-HSOS hierarchy by exploit-\ning CS and/or TS. The procedures are quite similar. The only signiﬁ-\ncant difference is on the deﬁnitions of tsp graphs: in the real case, we use\nA ∪2Nn\nr while in the complex case we use A instead due to the absence\nof the Hankel structure of complex moment matrices; see [WM22].\n9.2\nApplications to optimal power ﬂow\nThe AC-OPF problem aims to minimize the generation cost of an alternat-\ning current transmission network under the physical constraints (Kirch-\nhoff’s laws, Ohm’s law) as well as operational constraints, which can be\n\n158\nChapter 9. Application in optimal power ﬂow\nformulated as the following POP in complex variables:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninf\nVi,Sg\nk ∈C\n∑k∈G(c2k(ℜ(Sg\nk))2 + c1kℜ(Sg\nk) + c0k)\ns.t.\n∠Vref = 0\nSgl\nk ≤Sg\nk ≤Sgu\nk ,\n∀k ∈G\nυl\ni ≤|Vi| ≤υu\ni ,\n∀i ∈N\n∑k∈Gi Sg\nk −Sd\ni −Ys\ni |Vi|2 = ∑(i,j)∈Ei∪ER\ni Sij,\n∀i ∈N\nSij = ( ¯Yij −i\nbc\nij\n2 ) |Vi|2\n|Tij|2 −¯Yij\nVi ¯Vj\nTij ,\n∀(i, j) ∈E\nSji = ( ¯Yij −i\nbc\nij\n2 )|Vj|2 −¯Yij\n¯ViVj\n¯Tij ,\n∀(i, j) ∈E\n|Sij| ≤su\nij,\n∀(i, j) ∈E ∪ER\nθ∆l\nij ≤∠(Vi ¯Vj) ≤θ∆u\nij ,\n∀(i, j) ∈E\n(9.4)\nIn (9.4), Vi denotes the voltage, Sg\nk denotes the power generation, N de-\nnotes the set of buses, and G denotes the set of generators. Besides, ℜ(·),\n∠(·), | · | stand for the real part, the angle, the magnitude of a complex\nnumber, respectively. All symbols in boldface are constants. For a full\ndescription on the AC-OPF problem, the reader is referred to [BBC+19].\nNote that by introducing real variables for both real and imaginary parts\nof each complex variable, we can convert the AC-OPF problem to a POP\ninvolving only real variables1.\nTo tackle an AC-OPF instance, we ﬁrst compute a locally optimal so-\nlution with nonlinear programming tools whose global optimality is how-\never unknown. And we then rely on certain convex relaxation of (9.4) to\ncertify global optimality of the local solution. Suppose that the optimum\nreported by the local solver is AC and the optimum of the convex relax-\nation is opt. The optimality gap between the locally optimal solution and\nthe convex relaxation is deﬁned by\ngap := AC −opt\nAC\n× 100%.\nIf the optimality gap is less than 1.00%, then we accept the locally optimal\nsolution as globally optimal.\nWe perform two classes of numerical experiments on AC-OPF instances\nissued from PGLiB. For the ﬁrst class, we compare the complex hierarchy\nwith the real hierarchy in terms of strength and efﬁciency. The results are\nreported in Table 9.1 where “mb” denotes the maximal size of PSD blocks\ninvolved in the relaxations, “opt” denotes the optimum, “time” denotes\nrunning time in seconds, and “-” indicates an out of memory error. We re-\nfer to Shor’s relaxation (which applies when we convert (9.4) to a QCQP)\n1The expressions involving angles of complex variables can be converted to polynomials\nby using tan(∠z) = y/x for z = x + iy ∈C.\n\n9.2. Applications to optimal power ﬂow\n159\nas the 1st order relaxation and refer to the minimal initial relaxation de-\nﬁned in Chapter 8.4 as the 1.5th order relaxation.\nAs one can see from Table 9.1, the complex 1st order relaxation and\nthe real 1st order relaxation give the same lower bound (up to a given\nprecision) while the former runs slightly faster. The complex 1.5th order\nrelaxation typically gives a looser bound than the real 1.5th order relax-\nation whereas it runs faster by a factor of 1 ∼8. In addition, the 1st order\nrelaxation is able to certify global optimality for 4 out of all 9 instances.\nFor the remaining 5 instances, the complex 1.5th order relaxation is able to\ncertify global optimality for 3 instances and the real 1.5th order relaxation\nis able to certify global optimality for 4 instances.\nTable 9.1: The complex versus real hierarchy on AC-OPF instances under\ntypical operating conditions.\ncase name\norder\nComplex\nReal\nmb\nopt\ntime\ngap\nmb\nopt\ntime\ngap\n30_ieee\n1st\n8\n7.5472e3\n0.12\n8.06%\n8\n7.5472e3\n0.15\n8.06%\n1.5th\n12\n8.2073e3\n0.66\n0.02%\n22\n8.2085e3\n0.97\n0.00%\n39_epri\n1st\n8\n1.3565e4\n0.17\n2.00%\n8\n1.3565e4\n0.22\n2.00%\n1.5th\n14\n1.3765e4\n1.08\n0.55%\n25\n1.3842e4\n1.12\n0.00%\n89_pegase\n1st\n24\n1.0670e5\n0.72\n0.55%\n24\n1.0670e5\n0.74\n0.55%\n1.5th\n96\n1.0709e5\n263\n0.19%\n184\n1.0715e5\n1232\n0.13%\n118_ieee\n1st\n10\n9.6900e4\n0.49\n0.32%\n10\n9.6901e4\n0.57\n0.32%\n1.5th\n20\n9.7199e4\n5.22\n0.02%\n37\n9.7214e4\n8.78\n0.00%\n162_ieee_dtc\n1st\n28\n1.0164e5\n1.49\n5.96%\n28\n1.0164e5\n1.51\n5.96%\n1.5th\n40\n1.0249e5\n17.1\n5.17%\n74\n1.0645e5\n87.5\n1.51%\n179_goc\n1st\n10\n7.5016e5\n0.72\n0.55%\n10\n7.5016e5\n0.77\n0.55%\n1.5th\n20\n7.5078e5\n6.77\n0.46%\n37\n7.5382e5\n10.6\n0.06%\n300_ieee\n1st\n14\n5.5424e5\n1.41\n1.94%\n16\n5.5424e5\n1.49\n1.94%\n1.5th\n22\n5.6455e5\n19.1\n0.12%\n40\n5.6522e5\n27.3\n0.00%\n1354_pegase\n1st\n26\n1.2172e6\n10.9\n3.30%\n26\n1.2172e6\n13.1\n3.30%\n1.5th\n26\n1.2304e6\n255\n2.29%\n49\n1.2514e6\n392\n0.59%\n2869_pegase\n1st\n26\n2.4387e6\n47.2\n0.98%\n26\n2.4388e6\n67.3\n0.97%\n1.5th\n98\n2.4586e6\n1666\n0.17%\n191\n-\n-\n-\nAs the 1st order relaxation is already able to certify global optimality\nfor a large number of AC-OPF instances, we now focus on more challeng-\ning AC-OPF instances for which the 1st order relaxation yields an opti-\nmality gap greater than 1.00%. The related data of these selected AC-OPF\ninstances are displayed in Table 9.2, in which “var” denotes the number\nof variables, “cons” denotes the number of constraints, and “mc” denotes\nthe maximal size of variable cliques.\nSince the real relaxations typically yield tighter lower bounds when\nthe relaxation order is greater than one, we only investigate the real relax-\n\n160\nChapter 9. Application in optimal power ﬂow\nations here. Particularly, we solve the CSSOS hierarchy with r = 2 and the\nCS-TSSOS hierarchy with r = 2, s = 1 for these AC-OPF instances, and re-\nport the results in Table 9.3. As the table shows, CS-TSSOS is more efﬁcient\nand scales much better with the problem size than CSSOS. In particular,\nCS-TSSOS succeeds in reducing the optimality gap to less than 1.00% for\nall instances.\nTable 9.2: The data of selected AC-OPF instances.\ncase name\nvar\ncons\nmc\nAC\nShor\nopt\ngap\n3_lmbd_api\n12\n28\n6\n1.1242e4\n1.0417e4\n7.34%\n5_pjm\n20\n55\n6\n1.7552e4\n1.6634e4\n5.22%\n24_ieee_rts_sad\n114\n315\n14\n7.6943e4\n7.3592e4\n4.36%\n30_as_api\n72\n297\n8\n4.9962e3\n4.9256e3\n1.41%\n73_ieee_rts_sad\n344\n971\n16\n2.2775e5\n2.2148e5\n2.75%\n162_ieee_dtc_api\n348\n1809\n21\n1.2100e5\n1.1928e5\n1.42%\n240_pserc\n766\n3322\n16\n3.3297e6\n3.2818e6\n1.44%\n500_tamu_api\n1112\n4613\n20\n4.2776e4\n4.2286e4\n1.14%\n793_goc\n1780\n7019\n18\n2.6020e5\n2.5636e5\n1.47%\n1888_rte\n4356\n18257\n26\n1.4025e6\n1.3748e6\n1.97%\n3022_goc\n6698\n29283\n50\n6.0143e5\n5.9278e5\n1.44%\n9.3\nNotes and sources\nThe complex moment-HSOS hierarchy was initially introduced and stud-\nied in [JM15, JM18], which has been shown to have advantages over the\nreal hierarchy for certain CPOPs (e.g., a simpliﬁed version of the AC-OPF\nproblem).\nThe AC-OPF is a fundamental problem in power systems, which has\nbeen extensively studied in recent years; for a detailed introduction and\nrecent developments, the reader is referred to the survey [BEGL20] and\nreferences therein. Since 2006, several convex relaxation schemes (e.g.,\nsecond-order cone relaxations (SOCR) [Jab06], quadratic convex relaxations\n(QCR) [CHVH15], tight-and-cheap conic relaxations (TCR) [BALD18] and\nsemideﬁnite relaxations (SDR) [BWFW08]) have been proposed to provide\nlower bounds for the AC-OPF which can be then used to certify global op-\ntimality of local optimal solutions. While these relaxations (SOCR, QCR,\nTCR, SDR) could be scalable to problems of large size and prove to be tight\n\n9.3. Notes and sources\n161\nTable 9.3: The CSSOS versus CS-TSSOS hierarchy on AC-OPF instances.\ncase name\nCSSOS\nCS-TSSOS\nmb\nopt\ntime\ngap\nmb\nopt\ntime\ngap\n3_lmbd_api\n28\n1.1242e4\n0.21\n0.00%\n22\n1.1242e4\n0.09\n0.00%\n5_pjm\n28\n1.7543e4\n0.56\n0.05%\n22\n1.7543e4\n0.30\n0.05%\n24_ieee_rts_sad\n120\n7.6943e4\n94.9\n0.00%\n39\n7.6942e4\n14.8\n0.00%\n30_as_api\n45\n4.9927e3\n4.43\n0.07%\n22\n4.9920e3\n2.69\n0.08%\n73_ieee_rts_sad\n153\n2.2775e5\n504\n0.00%\n44\n2.2766e5\n71.5\n0.04%\n162_ieee_dtc_api\n253\n-\n-\n-\n34\n1.2096e5\n201\n0.03%\n240_pserc\n153\n3.3072e6\n585\n0.68%\n44\n3.3042e6\n33.9\n0.77%\n500_tamu_api\n231\n4.2413e4\n3114\n0.85%\n39\n4.2408e4\n46.6\n0.86%\n793_goc\n190\n2.5938e5\n563\n0.31%\n33\n2.5932e5\n66.1\n0.34%\n1888_rte\n378\n-\n-\n-\n27\n1.3953e6\n934\n0.51%\n3022_goc\n1326\n-\n-\n-\n76\n5.9858e5\n1886\n0.47%\nfor quite a few cases [BBC+19, CHVH15, EDA19], they yield signiﬁcant\noptimality gaps on a large number of cases2. To tackle these more chal-\nlenging cases, it is then mandatory to go to higher steps of the moment-SOS\nhierarchy which can provide tighter lower bounds. Along with this line re-\ncently in [GHW+20], Gopinath et al. certiﬁed 1% global optimality for all\nAC-OPF instances with up to 300 buses from the AC-OPF library PGLiB\nusing an SDP-based bound tightening approach. Relying on the complex\nmoment-SOS hierarchy combined with a multi-order technique, Josz and\nMolzahn certiﬁed 0.05% global optimality for certain 2000-bus cases on a\nsimpliﬁed AC-OPF model [JM18]. A comprehensive numerical study on\nAC-OPF instances from PGLiB with up to tens of thousands of variables\nand constraints via the CS-TSSOS hierarchy could be found in [WML22].\n2The\nreader\nmay\nﬁnd\nrelated\nresults\non\nbenchmarking\nSOCR\nand\nQR\nat\nhttps://github.com/power-grid-lib/pglib-opf/blob/master/BASELINE.md.\n\n\nBibliography\n[BALD18]\nChristian Bingane, Miguel F Anjos, and Sébastien Le Diga-\nbel. Tight-and-cheap conic relaxation for the ac optimal power\nﬂow problem. IEEE Transactions on Power Systems, 33(6):7181–\n7188, 2018.\n[BBC+19]\nSogol Babaeinejadsarookolaee, Adam Birchﬁeld, Richard D\nChristie, Carleton Coffrin, Christopher DeMarco, Ruisheng\nDiao, Michael Ferris, Stephane Fliscounakis, Scott Greene,\nRenke Huang, et al.\nThe power grid library for bench-\nmarking AC optimal power ﬂow algorithms. arXiv preprint\narXiv:1908.02788, 2019.\n[BEGL20]\nDan Bienstock, Mauro Escobar, Claudio Gentile, and Leo Lib-\nerti. Mathematical programming formulations for the alter-\nnating current optimal power ﬂow problem. 4OR, 18(3):249–\n292, 2020.\n[BWFW08] Xiaoqing Bai, Hua Wei, Katsuki Fujisawa, and Yong Wang.\nSemideﬁnite programming for optimal power ﬂow problems.\nInternational Journal of Electrical Power & Energy Systems, 30(6-\n7):383–392, 2008.\n[CHVH15] Carleton Coffrin, Hassan L Hijazi, and Pascal Van Henten-\nryck.\nThe QC relaxation: A theoretical and computational\nstudy on optimal power ﬂow. IEEE Transactions on Power Sys-\ntems, 31(4):3008–3018, 2015.\n[DP09]\nJohn P D’Angelo and Mihai Putinar. Polynomial optimization\non odd-dimensional spheres. In Emerging applications of alge-\nbraic geometry, pages 1–15. Springer, 2009.\n[EDA19]\nAnders Eltved, Joachim Dahl, and Martin S Andersen. On the\nrobustness and scalability of semideﬁnite relaxation for opti-\nmal power ﬂow problems. Optimization and Engineering, pages\n1–18, 2019.\n\n164\nBibliography\n[GHW+20] S Gopinath, Hassan L Hijazi, Tillmann Weisser, Harsha Na-\ngarajan, Mertcan Yetkin, Kaarthik Sundar, and Russel W Bent.\nProving global optimality of ACOPF solutions. Electric Power\nSystems Research, 189, 2020.\n[GMM15]\nBissan Ghaddar, Jakub Marecek, and Martin Mevissen. Opti-\nmal power ﬂow as a polynomial optimization problem. IEEE\nTransactions on Power Systems, 31(1):539–546, 2015.\n[Jab06]\nRabih A Jabr. Radial distribution load ﬂow using conic pro-\ngramming.\nIEEE transactions on power systems, 21(3):1458–\n1459, 2006.\n[JM15]\nCédric Josz and Daniel K Molzahn. Moment/sum-of-squares\nhierarchy for complex polynomial optimization. arXiv preprint\narXiv:1508.02068, 2015.\n[JM18]\nCédric Josz and Daniel K Molzahn.\nLasserre hierarchy for\nlarge scale polynomial optimization in real and complex vari-\nables. SIAM Journal on Optimization, 28(2):1017–1048, 2018.\n[WM22]\nJie Wang and Victor Magron. Exploiting sparsity in complex\npolynomial optimization. Journal of Optimization Theory and\nApplications, 192(1):335–359, 2022.\n[WML22]\nJie Wang, Victor Magron, and Jean B. Lasserre.\nCertifying\nglobal optimality of ac-opf solutions via sparse polynomial\noptimization. Electric Power Systems Research, 213:108683, 2022.\n\nChapter 10\nExploiting term sparsity in\nnoncommutative\npolynomial optimization\nIn this chapter, we generalize the methodology of exploiting TS to non-\ncommutative polynomial optimization. For the sake of conciseness, we\nconsider the problem of eigenvalue optimization over noncommutative\npolynomials and omit the proofs.\n10.1\nEigenvalue optimization with term sparsity\nRecall that the eigenvalue optimization problem is deﬁned by\nλmin( f, g) := inf{⟨f (A)v, v⟩: A ∈D∞\ng , ∥v∥= 1},\n(10.1)\nfor f ∈Sym R⟨x⟩and g = {g1, . . . , gm} ⊆Sym R⟨x⟩. Let\nA = supp( f ) ∪\nm\n[\nj=1\nsupp(gj).\n(10.2)\nAs before, we set g0 := 1, and let dj = ⌈deg(gj)/2⌉for j ∈{0} ∪[m] and\nrmin = max{⌈deg( f )/2⌉, d1, . . . , dm}. Fixing a relaxation order r ≥rmin,\nwe deﬁne a graph Gtsp\nr\nwith nodes Wr1 and edges\nE(Gtsp\nr ) = {{u, v} | (u, v) ∈Wr × Wr, u ̸= v, u⋆v ∈A ∪W2\nr},\n(10.3)\n1If g = ∅, then we may replace the monomial basis Wr with the one returned by the\nNewton chip method; see [BKP16, §2.3].\n\n166\nChapter 10. Exploiting term sparsity in noncommutative optimization\nwhere W2\nr := {u⋆u | u ∈Wr}. We call Gtsp\nr\nthe tsp graph associated with\nthe support A .\nFor a graph G(V, E) with V ⊆⟨x⟩and g ∈R⟨x⟩, let us deﬁne\nsuppg(G) := {u⋆wv | u = v ∈V or {u, v} ∈E, w ∈supp(g)}.\n(10.4)\nLet G(0)\nr,0 = Gtsp\nr\nand G(0)\nr,j be the empty graph with nodes Vr,j := Wr−dj for\nj ∈[m]. Then for each j ∈{0} ∪[m], we iteratively deﬁne a sequence of\ngraphs (G(s)\nr,j (Vr,j, E(s)\nr,j ))s≥1 via two successive operations:\n(1) support extension. Let F(s)\nr,j be the graph with nodes Vr,j and\nE(F(s)\nr,j ) ={{u, v} | (u, v) ∈Vr,j × Vr,j, u ̸= v,\nu⋆supp(gj)v ∩\nm\n[\nj=0\nsuppgj(G(s−1)\nr,j\n) ̸= ∅},\n(10.5)\nwhere u⋆supp(gj)v := {u⋆wv | w ∈supp(gj)}.\n(2) chordal extension. Let\nG(s)\nr,j := (F(s)\nr,j )′.\n(10.6)\nBy construction, one has G(s)\nr,j ⊆G(s+1)\nr,j\nfor all j, s. Therefore, for every j,\nthe sequence of graphs (G(s)\nr,j )s≥1 stabilizes after a ﬁnite number of steps.\nLet tj = |Wr−dj| for j ∈{0} ∪[m]. Then by replacing the PSD constraint\nMr−dj(gjy) ⪰0 with the weaker constraint BG(s)\nr,j ◦Mr−dj(gjy) ∈ΠG(s)\nr,j (S+\ntj )\nfor j ∈{0} ∪[m] in (6.23), we obtain the following series of sparse moment\nrelaxations for (10.1) indexed by s ≥1:\nλr,s\nts ( f, g) :=\ninf\ny\nLy( f )\ns.t.\nBG(s)\nr,0 ◦Mr(y) ∈ΠG(s)\nr,0 (S+\nt0)\nBG(s)\nr,j ◦Mr−dj(gjy) ∈ΠG(s)\nr,j (S+\ntj ),\nj ∈[m]\ny1 = 1\n(10.7)\nWe call s the sparse order. For each s ≥1, the dual of (10.7) reads as\n\n\n\n\n\n\n\n\n\n\n\nsup\nGj,b\nb\ns.t.\n∑m\nj=0⟨Gj, Dj\nw⟩+ bδ1w = fw,\n∀w ∈Sm\nj=0 suppgj(G(s)\nr,j )\nGj ∈S+\ntj ∩SG(s)\nr,j\n,\nj ∈{0} ∪[m]\n(10.8)\n\n10.1. Eigenvalue optimization with term sparsity\n167\nwhere {Dj\nw}j,w are appropriate matrices satisfying Mr−dj(gjy) = ∑w Dj\nwyw.\nWe call the TS-adapted moment-SOHS relaxations (10.7)–(10.8) the NCTSSOS\nhierarchy associated with (10.1).\nTheorem 10.1 Let { f } ∪g ⊆Sym R⟨x⟩. Then the following hold:\n(1) Suppose that Dg contains an nc ε-neighborhood of 0. Then for all r, s,\nthere is no duality gap between (10.7) and its dual (10.8).\n(2) Fixing a relaxation order r ≥rmin, the sequence (λr,s\nts ( f, g))s≥1 is\nmonotonically non-decreasing and λr,s\nts ( f, g) ≤λr( f, g) for all s (with\nλr( f, g) being deﬁned in (6.23)).\n(3) Fixing a sparse order s ≥1, the sequence (λr,s\nts ( f, g))r≥rmin is mono-\ntonically non-decreasing.\n(4) If\nthe\nmaximal\nchordal\nextension\nis\nchosen\nin\n(10.6),\nthen\n(λr,s\nts ( f, g))s≥1 converges to λr( f, g) in ﬁnitely many steps.\nFollowing from Theorem 10.1, we have the following two-level hierar-\nchy of lower bounds for the optimum λmin( f, g) of (10.1):\nλrmin,1\nts\n( f, g)\n≤\nλrmin,2\nts\n( f, g)\n≤\n· · ·\n≤\nλrmin( f, g)\n≥\n≥\n≥\nλrmin+1,1\nts\n( f, g)\n≤\nλrmin+1,2\nts\n( f, g)\n≤\n· · ·\n≤\nλrmin+1( f, g)\n≥\n≥\n≥\n...\n...\n...\n...\n≥\n≥\n≥\nλr,1\nts ( f, g)\n≤\nλr,2\nts ( f, g)\n≤\n· · ·\n≤\nλr( f, g)\n≥\n≥\n≥\n...\n...\n...\n...\n(10.9)\nExample 10.2 Consider f = 2 −x2 + xy2x −y2 + xyxy + yxyx + x3y +\nyx3 + xy3 + y3x and g = {1 −x2, 1 −y2}. The graph sequence (G(s)\n2,0)s≥1\nfor f and g is given in Figure 10.1. In fact the graph sequence (G(s)\n2,j )s≥1 stabi-\nlizes at s = 2 for j = 0, 1, 2 (with approximately smallest chordal extensions).\nUsing TSSOS, we obtain that λ2,1\nts ( f, g) ≈−2.55482, λ2,2\nts ( f, g) = λ2( f, g) ≈\n−2.05111.\n\n168\nChapter 10. Exploiting term sparsity in noncommutative optimization\n1\nx2\nxy\nyx\ny2\nx\ny\n1\nx2\nxy\nyx\ny2\nx\ny\nFigure 10.1: The graph sequence (G(s)\n2,0)s≥1 in Example 10.2: left for s = 1;\nright for s = 2. The dashed edges are added after a chordal extension.\n10.2\nCombining correlative and term sparsity\nCombining CS with TS for eigenvalue optimization proceeds in a similar\nmanner as for the commutative case in Chapter 8.1.\nLet f = ∑w fww ∈Sym R⟨x⟩and g = {g1, . . . , gm} ⊆Sym R⟨x⟩.\nSuppose that Gcsp is the csp graph associated with f and g, and (Gcsp)′ is\na chordal extension of Gcsp. Let {Ik}k∈[p] be the maximal cliques of (Gcsp)′\nwith cardinality being denoted by nk, k ∈[p]. Then the set of variables x\nis decomposed into x(I1), x(I2), . . . , x(Ip). Let J1, . . . , Jp be deﬁned as in\nChapter 6.2.\nNow we consider the tsp for each subsystem involving the variables\nx(Ik), k ∈[p] respectively as follows. Let\nA := supp( f ) ∪\nm\n[\nj=1\nsupp(gj) and Ak := {w ∈A | var(w) ⊆x(Ik)},\n(10.10)\nfor k ∈[p]. As before, let g0 = 1, dj = ⌈deg(gj)/2⌉, j ∈{0} ∪[m] and\nrmin = max{⌈deg( f )/2⌉, d1, . . . , dm}. Fix a relaxation order r ≥rmin. Let\nWr−dj,k be the standard monomial basis of degree ≤r −dj with respect to\nthe variables x(Ik) and Gtsp\nr,k be the tsp graph with nodes Wr,k associated\nwith Ak deﬁned as in Chapter 10.1. Let G(0)\nr,k,0 = Gtsp\nr,k and G(0)\nr,k,j be the\nempty graph with nodes Vr,k,j := Wr−dj,k for j ∈Jk, k ∈[p]. Letting\nC (s)\nr\n:=\np[\nk=1\n[\nj∈{0}∪Jk\nsuppgj(G(s)\nr,k,j),\n(10.11)\n\n10.2. Combining correlative and term sparsity\n169\nwe iteratively deﬁne a sequence of graphs (G(s)\nr,k,j(Vr,k,j, E(s)\nr,k,j))s≥1 for each\nj ∈{0} ∪Jk, k ∈[p] by\nG(s)\nr,k,j := (F(s)\nr,k,j)′,\n(10.12)\nwhere F(s)\nr,k,j is the graph with nodes Vr,k,j and edges\nE(F(s)\nr,k,j) = {{u, v} | u ̸= v ∈Vr,k,j, u⋆supp(gj)v ∩C (s−1)\nr\n̸= ∅}.\n(10.13)\nLet tk,j = |Wr−dj,k| for all k, j. Then for each s ≥1 (called the sparse\norder), the moment relaxation based on correlative-term sparsity for (10.1)\nis given by\nλr,s\ncs-ts( f, g) :=\ninf\ny\nLy( f )\ns.t.\nBG(s)\nr,k,0\n◦Mr(y, Ik) ∈ΠG(s)\nr,k,0\n(S+\nrk,0),\nk ∈[p]\nBG(s)\nr,k,j\n◦Mr−dj(gjy, Ik) ∈ΠG(s)\nr,k,j\n(S+\nrk,j),\nj ∈Jk, k ∈[p]\ny1 = 1\n(10.14)\nFor all k, j, let us write Mr−dj(gjy, Ik) = ∑w Dk,j\nw yw for appropriate ma-\ntrices {Dk,j\nw }k,j,w. Then for each s ≥1, the dual of (10.14) reads as\n\n\n\n\n\n\n\n\n\n\n\nsup\nGk,j,b\nb\ns.t.\n∑\np\nk=1 ∑j∈{0}∪Jk⟨Gk,j, Dk,j\nw ⟩+ bδ1w = fw,\n∀w ∈C (s)\nr\nGk,j ∈S+\ntk,j ∩SG(s)\nr,k,j\n,\nj ∈{0} ∪Jk, k ∈[p]\n(10.15)\nwhere C (s)\nr\nis deﬁned in (10.11).\nThe properties of the relaxations (10.14)–(10.15) are summarized in the\nfollowing theorem.\nTheorem 10.3 Assume that { f } ∪g ⊆Sym R⟨x⟩. Then the following\nhold:\n(1) Fixing a relaxation order r ≥rmin, the sequence (λr,s\ncs-ts( f, g))s≥1 is\nmonotonically non-decreasing and λr,s\ncs-ts( f, g) ≤λr\ncs( f, g) for all s ≥\n1 (with λr\ncs( f, g) being deﬁned in Chapter 6.25).\n(2) Fixing a sparse order s ≥1, the sequence (λr,s\ncs-ts( f, g))r≥rmin is mono-\ntonically non-decreasing.\n\n170\nChapter 10. Exploiting term sparsity in noncommutative optimization\n(3) If the maximal chordal extension is chosen in (10.12),\nthen\n(λr,s\ncs-ts( f, g))s≥1 converges to λr\ncs( f, g) in ﬁnitely many steps.\n10.3\nNumerical experiments\nIn this section, we present numerical results of the proposed NCTSSOS hi-\nerarchies. The tool NCTSSOS to implement these hierarchies is available at\nhttps://github.com/wangjie212/NCTSSOS\nNCTSSOS employs MOSEK as an SDP solver. All numerical examples were\ncomputed on an Intel Core i5-8265U@1.60GHz CPU with 8GB RAM mem-\nory. In the following, “mb” denotes the maximal size of PSD blocks, “opt”\ndenotes the optimum, “time” denotes running time in seconds, and “-”\nindicates an out of memory error.\nLet D be the semialgebraic set deﬁned by g = {1 −X2\n1, . . . , 1 −X2\nn, X1 −\n1/3, . . . , Xn −1/3}, and consider the optimization problem of minimizing\nthe eigenvalue of the nc Broyden banded function on D, where the nc\nBroyden banded function is deﬁned by\nfBb(x) =\nn\n∑\ni=1\n(2xi + 5x3\ni + 1 −∑\nj∈Ji\n(xj + x2\nj ))⋆(2xi + 5x3\ni + 1 −∑\nj∈Ji\n(xj + x2\nj )),\nwhere Ji = {j | j ̸= i, max(1, i −5) ≤j ≤min(n, i + 1)}.\nWe compute λ3,1\ncs-ts( f, g) using approximately smallest chordal exten-\nsions for TS and present the results in Table 10.1, indicated by “CS+TS”.\nTo show the beneﬁts of NCTSSOS against the CS-based approach devel-\noped in Chapter 6, we also display the results for the latter approach in\nthe table, indicated by “CS”. It is evident from the table that NCTSSOS is\nmuch more scalable than the CS-based approach. Actually, the latter can\nnever be executed due to the memory limit even when the problem has\nonly 6 variables.\nNow we construct randomly generated examples whose csp graph\nconsists of p maximal cliques of size 15 as follows: let f = ∑\np\nk=1(hk +\nh⋆\nk)/2 where hk ∈R⟨x10k−9, . . . , x10k+5⟩is a random quartic nc polynomi-\nals with 15 terms and coefﬁcients being taken from [−1, 1], and let g =\n{gk}p\nk=1 where gk = 1 −x2\n10k−9 −· · · −x2\n10k+5. We consider the eigen-\nvalue minimization problem for f on the multi-ball deﬁned by g.\nLet\np = 100, 200, 300, 400 so that we obtain 4 such instances2. We compute\nthe sequence (λr,s\ncs-ts( f, g))s≥1 with r = 2 and present the results of the ﬁrst\n2The polynomials are available at https://wangjie212.github.io/jiewang/code.\nhtml.\n\n10.4. Polynomial Bell inequalities\n171\nTable 10.1: The eigenvalue minimization for the nc Broyden banded func-\ntion on D with r = 3, s = 1.\nn\nCS+TS\nCS\nmb\nopt\ntime\nmb\nopt\ntime\n5\n11\n3.113\n0.50\n156\n3.113\n70.7\n10\n15\n3.011\n2.78\n400\n-\n-\n20\n15\n9.658\n11.4\n400\n-\n-\n40\n15\n22.93\n38.1\n400\n-\n-\n60\n15\n36.21\n80.5\n400\n-\n-\n80\n15\n49.49\n138\n400\n-\n-\n100\n15\n62.77\n180\n400\n-\n-\nthree steps (where we use the maximal chordal extension for the ﬁrst step\nand use approximate smallest chordal extensions for the second and third\nsteps, respectively) in Table 10.2. Again we see that NCTSSOS is much\nmore scalable than the CS-based approach.\n10.4\nPolynomial Bell inequalities\nThe above framework can be extended to minimize the (normalized) trace\nof either noncommutative polynomials or a so-called trace polynomial. Let\nus denote by tr the normalized trace operator. A trace polynomial is a\npolynomial in symmetric noncommutative variables x1, . . . , xn and traces\nof their products. Thus naturally each trace polynomial f has an adjoint\nf ⋆. A pure trace polynomial is a trace polynomial that is made only of traces,\ni.e., has no free variables xj. For instance, the trace of a trace polynomial is\na pure trace polynomial, e.g.,\nf = x1x2x2\n1 −tr(x2) tr(x1x2) tr(x2\n1x2)x2x1,\ntr( f ) = tr(x3\n1x2) −tr(x2) tr(x1x2)2 tr(x2\n1x2),\nf ⋆= x2\n1x2x1 −tr(x2) tr(x1x2) tr(x2\n1x2)x1x2.\nIn this section we connect trace polynomial optimization to violations of\nnonlinear Bell inequalities, outline a few examples and prove the optimal\nbound on maximal violation of a covariance Bell inequality considered in\nthe quantum information literature.\nWe already introduced classical (linear) Bell inequalities in Chapter\n6.7.2, and saw that v∗(A1 ⊗B1 + A1 ⊗B2 + A2 ⊗B1 −A2 ⊗B2)v is at\nmost 2 for all separable states v ∈Ck ⊗Ck and Aj, Bj ∈Ck×k satisfy-\ning A∗\nj = Aj, A2\nj = I, B∗\nj = Bj, B2\nj = I. Tsirelson’s bound implies that\n\n172\nChapter 10. Exploiting term sparsity in noncommutative optimization\nTable 10.2: The eigenvalue minimization for randomly generated exam-\nples over multi-balls with r = 2.\nn\nCS+TS\nCS\ns\nmb\nopt\ntime\nmb\nopt\ntime\n1005\n1\n25\n−32.58\n9.71\n241\n-\n-\n2\n25\n−31.91\n24.5\n3\n25\n−31.71\n40.9\n2005\n1\n25\n−63.58\n33.7\n241\n-\n-\n2\n25\n−62.05\n85.8\n3\n25\n−61.76\n149\n3005\n1\n23\n−95.73\n74.8\n241\n-\n-\n2\n23\n−93.13\n212\n3\n23\n−92.71\n396\n4005\n1\n25\n−131.1\n122\n241\n-\n-\n2\n25\n−127.5\n375\n3\n25\n−126.8\n687\nthe value is at most 2\n√\n2, which is attained in particular when k = 2 and\nv =\n1\n√\n2(e1 ⊗e1 + e2 ⊗e2), with (e1, e2) being an orthonormal basis of C2.\nIn general, if vk is the generalized Bell state,\nvk =\n1\n√\nk\nk\n∑\nj=1\nej ⊗ej ∈Rk ⊗Rk,\nwhich is a maximally entangled bipartite state on Ck ⊗Ck, unique up to\nbipartite unitary equivalence, then\nv∗\nk(X ⊗Y)vk = tr(XY)\n(10.16)\nfor all X, Y ∈Sk.\nWhile linear Bell inequalities are linear in expectation values of (prod-\nucts of) observables, polynomial Bell inequalities contain multivariate poly-\nnomials in expectation values of (products of) observables. For this reason,\nnoncommutative polynomial optimization is not suitable for studying vi-\nolations of nonlinear Bell inequalities. In contrast, trace polynomial opti-\nmization gives upper bounds on violations of polynomial Bell inequalities,\nat least for certain families of states, e.g., the maximally entangled bipartite\nstates via (10.16). Consider a simple quadratic Bell inequality\n\u0000v∗(A1 ⊗B2 + A2 ⊗B1)v\n\u00012 +\n\u0000v∗(A2 ⊗B1 −A2 ⊗B2)v\n\u00012 ≤4.\n(10.17)\n\n10.4. Polynomial Bell inequalities\n173\nAn automatized proof of (10.17) for maximally entangled states of arbi-\ntrary dimension can be obtained by solving the optimization problem:\n(\nsup\n(tr(a1b2 + a2b1))2 + (tr(a1b1 −a2b2))2\ns.t.\na2\nj = 1, b2\nj = 1 for j = 1, 2.\n(10.18)\nWe compare the value of the dense relaxation of (10.18) with the ones ob-\ntained after exploiting TS. At relaxation order r = 2, we obtain a bound\nof 4 (the optimal one) in the dense setting. The number of SDP equality\nconstraints is 222 and the size of the SDP matrix is 53. At the sparse or-\nder s = 1 and using the maximal chordal extension, we obtain the same\nbound but the maximal block size is only 6 and the number of equality\nconstraints is only 30.\nAnother class of polynomial Bell inequalities arises from covariances of\nquantum correlations. Let covv(X, Y) := v∗(X ⊗Y)v −v∗(X ⊗I)v · v∗(I ⊗\nY)v. and let us show that the value of\ncovv(A1, B1) + covv(A1, B2) + covv(A1, B3)\n+ covv(A2, B1) + covv(A2, B2) −covv(A2, B3)\n+ covv(A3, B1) −covv(A3, B2)\n(10.19)\nis at most 5 for every maximally entangled state. Let\nt = tr(a1b1) −tr(a1) tr(b1) + tr(a1b2) −tr(a1) tr(b2) + tr(a1b3) + tr(a2b1)\n−tr(a1) tr(b3) −tr(a2) tr(b1) + tr(a2b2) −tr(a2) tr(b2) −tr(a2b3)\n+ tr(a2) tr(b3) + tr(a3b1) −tr(a3) tr(b1) −tr(a3b2) + tr(a3) tr(b2).\nThe dense relaxation of\nsup t s.t. a2\nj = 1, b2\nj = 1 for j = 1, 2, 3\n(10.20)\nwith r = 2 returns 5. Therefore the value of (10.19) is at most 5 for every\nmaximally entangled state, regardless of the local dimension k. This dense\nrelaxation involves 1010 SDP equality constraints and an SDP matrix of\nsize 115. Exploiting TS at the sparse order s = 1 with the maximal chordal\nextension yields the same bound but requires to solve an SDP with max-\nimal block size 83 and 797 equality constraints. We can obtain an even\nbetter computational gain by relying on approximately smallest chordal\nextensions as it yields an SDP with maximal block size 25 and only 115\nequality constraints, 10 times less compared to the dense case. We refer\nthe interested programmer to the Julia scripts displayed in Appendix B.2,\nallowing one to retrieve these results.\n\n174\nChapter 10. Exploiting term sparsity in noncommutative optimization\n10.5\nNotes and sources\nThe material from this chapter is mainly issued from [WM21]. Our frame-\nwork can be extended to minimize the trace of a noncommutative poly-\nnomial over a noncommutative semialgebraic set; see [WM21, § 5]. Opti-\nmization over trace polynomials has been recently developed in [KMV22],\nwhere the authors present a novel Positivstellensatz certifying positiv-\nity of trace polynomials subject to trace constraints, and a hierarchy of\nsemideﬁnite relaxations converging monotonically to the optimum of a\ntrace polynomial subject to tracial constraints is provided. Trace poly-\nnomial optimization can be used to detect entanglement in multipartite\nWerner states [HKMV21]. The interested reader can ﬁnd more information\nabout bilocal models in [BRGP12, Cha16], covariance of quantum correla-\ntions in [PHBB17] and detection of partial separability in [Uff02]. Inequal-\nity (10.17) is given in [Uff02], where it is shown to hold for all separable\nstates, and for all 2-dimensional states. In [NKI02], (10.17) is shown to\nhold for arbitrary states, meaning it admits no quantum violations.\nIn [PHBB17] it is shown that while the value of (10.19) is at most 9\n2 for\nseparable states, it attains the value 5 with the Bell state v2. The authors\nalso performed extensive numerical search within entangled states for lo-\ncal dimensions k ≤5, but no higher value of (10.19) was found. They leave\nit as an open question whether higher dimensional entangled states could\nlead to larger violations [PHBB17, Appendix D.1(b)].\n\nBibliography\n[BKP16]\nSabine Burgdorf, Igor Klep, and Janez Povh.\nOptimization\nof polynomials in non-commuting variables.\nSpringerBriefs in\nMathematics. Springer, [Cham], 2016.\n[BRGP12]\nCyril Branciard, Denis Rosset, Nicolas Gisin, and Stefano Piro-\nnio. Bilocal versus nonbilocal correlations in entanglement-\nswapping experiments. Phys. Rev. A, 85:032119, Mar 2012.\n[Cha16]\nRafael Chaves. Polynomial Bell inequalities. Phys. Rev. Lett.,\n116(1):010402, 6, 2016.\n[HKMV21] Felix Huber, Igor Klep, Victor Magron, and Jurij Volˇciˇc.\nDimension-free\nentanglement\ndetection\nin\nmultipartite\nwerner states. arXiv preprint arXiv:2108.08720, 2021.\n[KMV22]\nIgor Klep, Victor Magron, and Jurij Volˇciˇc. Optimization over\ntrace polynomials. In Annales Henri Poincaré, volume 23, pages\n67–100. Springer, 2022.\n[NKI02]\nKoji Nagata, Masato Koashi, and Nobuyuki Imoto. Conﬁgu-\nration of separability and tests for multipartite entanglement\nin Bell-type experiments.\nPhys. Rev. Lett., 89(26):260401, 4,\n2002.\n[PHBB17]\nVictor Pozsgay, Flavien Hirsch, Cyril Branciard, and Nico-\nlas Brunner.\nCovariance Bell inequalities.\nPhys. Rev. A,\n96(6):062128, 13, 2017.\n[Uff02]\nJos Ufﬁnk. Quadratic Bell inequalities as tests for multipartite\nentanglement. Phys. Rev. Lett., 88(23):230406, 4, 2002.\n[WM21]\nJie Wang and Victor Magron. Exploiting term sparsity in non-\ncommutative polynomial optimization.\nComputational Opti-\nmization and Applications, 80(2):483–521, 2021.\n\n\nChapter 11\nApplication in stability of\ncontrol-systems\nThe concept of joint spectral radius (JSR) can be viewed as a generaliza-\ntion of the usual spectral radius to the case of multiple matrices. The exact\ncomputation and even the approximation of the JSR for a set of matrices\nare, however, notoriously difﬁcult. In this chapter, we discuss how to ef-\nﬁciently compute upper bounds on JSR via the SOS approach when the\nmatrices possess certain sparsity.\n11.1\nApproximating JSR via SOS relaxations\nThe JSR for a set of matrices A = {A1, . . . , Am} ⊆Rn×n is given by\nρ(A) := lim\nk→∞\nmax\nσ∈{1,...,m}k ∥Aσ1 Aσ2 · · · Aσk∥\n1\nk .\n(11.1)\nNote that the value of ρ(A) is independent of the choice of the norm used\nin (11.1). Parrilo and Jadbabaie proposed to compute a sequence of upper\nbounds for ρ(A) via SOS relaxations. The underlying idea is based on the\nfollowing theorem.\nTheorem 11.1 ([PJ08], Theorem 2.2) Let A = {A1, . . . , Am} ⊆Rn×n be a\nset of matrices, and let p be a strictly positive form of degree 2r that satisﬁes\np(Aix) ≤γ2rp(x),\n∀x ∈Rn,\ni = 1, . . . , m.\nThen, ρ(A) ≤γ.\nBy replacing positive forms with more tractable SOS forms, Theorem\n11.1 immediately suggests the following SOS relaxations indexed by r ∈\n\n178\nChapter 11. Application in stability of control-systems\nN∗to compute a sequence of upper bounds for ρ(A):\nρSOS,2r(A) :=\ninf\np∈R[x]2r,γ\nγ\ns.t.\np(x) −∥x∥2r\n2 ∈Σn,2r\nγ2rp(x) −p(Aix) ∈Σn,2r,\ni = 1, . . . , m\n(11.2)\nThe term “∥x∥2r\n2 ” appearing in the ﬁrst constraint of (11.2) is added to\nmake sure that p is strictly positive. The optimization problem (11.2) can\nbe solved via SDP by bisection on γ. It was shown in [PJ08] that the upper\nbound ρSOS,2r(A) satisﬁes the inequalties stated in the following theorem.\nTheorem 11.2 ([PJ08]) Let A = {A1, . . . , Am} ⊆Rn×n. For any integer r ≥\n1, one has m−1\n2r ρSOS,2r(A) ≤ρ(A) ≤ρSOS,2r(A).\nIt is immediate from Theorem 11.2 that (ρSOS,2r(A))r≥1 converges to ρ(A)\nas r goes to inﬁnity.\n11.2\nThe SparseJSR Algorithm\nIn this section, we propose an algorithm SparseJSR for bounding JSR from\nabove based on the sparse SOS decomposition when the matrices A pos-\nsess certain sparsity. The ﬁrst step is to establish a hierarchy of sparse\nsupports for the auxiliary form p(x) used in the SOS program (11.2).\nLet us ﬁx a relaxation order r. Let p0(x) = ∑n\ni=1 cix2r\ni\nwith random\ncoefﬁcients ci ∈(0, 1) and let A (0) = supp(p0). Then for s ∈N∗, we\niteratively deﬁne\nA (s) := A (s−1) ∪\nm\n[\ni=1\nsupp(ps−1(Aix)),\n(11.3)\nwhere ps−1(x) = ∑α∈A (s−1) cαxα with random coefﬁcients cα ∈(0, 1). Note\nthat here the particular form of p0(x) is chosen such that A (s) contains all\npossible homogeneous monomials of degree 2r that are “compatible” with\nthe couplings between variables x1, . . . , xn introduced by the mappings\nx 7→Aix for all i. It is clear that\nA (1) ⊆· · · ⊆A (s) ⊆A (s+1) ⊆· · · ⊆Nn\n2r\n(11.4)\nand the sequence (A (s))s≥1 stabilizes in ﬁnitely many steps. We emphasis\nthat it is not guaranteed a hierarchy of sparse supports is always retrieved\nby (11.4) even if all Ai are sparse. For instance, if some matrix Ai ∈A has\na fully dense row, then by deﬁnition, one immediately has A (1) = Nn\n2r. In\nthis case, the sparsity of A cannot be exploited by the present method.\n\n11.2. The SparseJSR Algorithm\n179\nOn the other hand, if the matrices in A have some common zero columns,\nthen a hierarchy of sparse supports must be retrieved by (11.4) as shown\nin the next proposition.\nProposition 11.3 Let A = {A1, . . . , Am} ⊆Rn×n and assume that the ma-\ntrices in A have common zero columns indexed by J ⊆[n]. Let ˜Nn−|J|\n2r\n:=\n{(αi)i∈[n] ∈Nn | (αi)i∈[n]\\J ∈Nn−|J|\n2r\n, αi = 0 for i ∈J} and bj := {(αi)i∈[n] ∈\nNn | αj = 2r, αi = 0 for i ̸= j} for j ∈[n]. Then A (s) ⊆˜Nn−|J|\n2r\n∪{bj}j∈J for\nall s ≥1.\nPROOF Let us do induction on s.\nIt is obvious that A (0) ⊆\n˜Nn−|J|\n2r\n∪\n{bj}j∈J. Now assume A (s) ⊆\n˜Nn−|J|\n2r\n∪{bj}j∈J for some s ≥0. Since\nthe variables effectively involved in ps(Ajx) are contained in {xi}i∈[n]\\J,\nwe have supp(ps(Ajx)) ⊆˜Nn−|J|\n2r\nfor j = 1, . . . , m. This combined with\nthe induction hypothesis yields A (s+1) ⊆˜Nn−|J|\n2r\n∪{bj}j∈J as desired.\n2\nFor each s ≥1, by restricting p(x) to forms with the sparse support\nA (s), (11.2) now reads as\n\n\n\n\n\n\n\ninf\np∈R[A (s)],γ\nγ\ns.t.\np(x) −∥x∥2r\n2 ∈Σn,2r\nγ2rp(x) −p(Aix) ∈Σn,2r,\ni ∈[m]\n(11.5)\nLet A (s)\ni\n= A (s) ∪supp(ps(Aix) for i = 1, . . . , m. In order to exploit\nthe sparsity present in (11.5), for a sparse support A ⊆Nn\n2r we deﬁne\nΣ(A ) :=\nn\nf ∈R[A ] | ∃G ∈S+\n| B | ∩SGtsp s.t. f = (xB)⊺GxBo\n,\n(11.6)\nwhere B is a monomial basis and Gtsp is the tsp graph with respect to B\ndeﬁned as in Chapter 7.1. Then by replacing Σn,2d with Σ(A (s)) or Σ(A (s)\ni\n)\nin (11.5), we therefore obtain a hierarchy of sparse SOS relaxations indexed\nby s for a ﬁxed r:\nρs,2r(A) :=\ninf\np∈R[A (s)],γ\nγ\ns.t.\np(x) −∥x∥2r\n2 ∈Σ(A (s))\nγ2rp(x) −p(Aix) ∈Σ(A (s)\ni\n),\ni ∈[m]\n(11.7)\nAs in the dense case, the optimization problem (11.7) can be solved via\nSDP by bisection on γ. We call the index s the sparse order of (11.7), and we\nget a hierarchy of upper bounds on the JSR indexed by the sparse order s\nwhen the relaxation order r is ﬁxed.\n\n180\nChapter 11. Application in stability of control-systems\nTheorem 11.4 Let A = {A1, . . . , Am} ⊆Rn×n. For any integer r ≥1,\none has ρSOS,2r(A) ≤· · · ≤ρs,2r(A) ≤· · · ≤ρ2,2r(A) ≤ρ1,2r(A).\nPROOF For any ﬁxed r ∈N∗, because of (11.4), it is clear that the feasible\nset of (11.7) with the sparse order s is contained in the feasible set of (11.7)\nwith the sparse order s + 1, which is in turn contained in the feasible set of\n(11.2). This yields the desired conclusion.\nTherefore, the algorithm SparseJSR computes a non-increasing sequence\nof upper bounds for the JSR of a tuple of matrices via solving (11.7) for any\nﬁxed r. By tuning the relaxation order r and the sparse order s, SparseJSR\noffers a trade-off between the computational cost and the quality of the\nobtained upper bound.\n11.3\nNumerical Experiments\nIn this section, we present numerical experiments for the proposed algo-\nrithm SparseJSR, which was implemented in the Julia package also named\nSparseJSR. In Appendix B.3, we provide a Julia script to illustrate how to\nuse SparseJSR.\nThe examples were computed on an Intel Core i5-8265U@1.60GHz CPU\nwith 8GB RAM memory, where the sparse order s was set to 1, the tol-\nerance for bisection was set to ε = 1 × 10−5, and the initial interval for\nbisection was set to [0, 2]. To measure the quality of upper bounds (ub)\nprovided by SparseJSR, we also compute lower bounds (lb) on JSR us-\ning Gripenberg’s algorithm [Gri96]. In the following, “mb” denotes the\nmaximal size of PSD blocks, “time” denotes running time in seconds, ∗\nindicates running time exceeding one hour, and “-” indicates an out of\nmemory error.\n11.3.1\nRandomly generated examples\nWe generate random sparse matrices as follows1: generate a random di-\nrected graph G with n nodes and n + 10 edges; for each edge (i, j) of G,\nput a random number in [−1, 1] on the position (i, j) of the matrix and\nput zeros on the other positions. We compute an upper bound on JSR for\npairs of such matrices using the ﬁrst-order SOS relaxation and report the\nresults in Table 11.1. It is evident that the sparse approach is much more\nefﬁcient than the dense approach. For instance, the dense approach takes\n1Available at https://wangjie212.github.io/jiewang/code.html.\n\n11.3. Numerical Experiments\n181\nover 3600 s when the size of matrices is greater than 100 while the sparse\napproach can handle matrices of size 120 within 12 s. The upper bound\nproduced by the sparse approach is slightly weaker, but is still close to the\nlower bound.\nTable 11.1: Randomly generated examples with r = 1 and m = 2.\nSparse\nDense\nn\nlb\ntime\nub\nmb\ntime\nub\nmb\n20\n0.7894\n0.74\n0.8192\n10\n1.88\n0.7967\n20\n40\n0.9446\n2.68\n0.9446\n14\n25.6\n0.9446\n40\n60\n0.7612\n3.64\n0.7843\n13\n171\n0.7612\n60\n80\n0.9345\n5.95\n0.9399\n15\n743\n0.9345\n80\n100\n0.8642\n8.15\n0.9132\n13\n2568\n0.8659\n100\n120\n0.7483\n11.7\n0.7735\n16\n∗\n∗\n∗\n11.3.2\nExamples from control systems\nHere we consider examples from [MHMJZ20], where the dynamics of closed-\nloop systems are given by the combination of a plant and a one-step delay\ncontroller that stabilizes the plant. The closed-loop system evolves accord-\ning to either a completed or a missed computation. In the case of a dead-\nline hit, the closed-loop state matrix is AH. In the case of a deadline miss,\nthe associated closed-loop state matrix is AM. The computational plat-\nform (hardware and software) ensures that no more than m −1 deadlines\nare missed consecutively. The set of possible realisations A of such a sys-\ntem contains either a single hit or at most m −1 misses followed by a hit,\nnamely A := {AHAi\nM | 0 ≤i ≤m −1}. Then, the closed-loop system\nthat can switch between the realisations included in A is asymptotically\nstable if and only if ρ(A) < 1. This gives an indication for scheduling\nand control co-design, in which the hardware and software platform must\nguarantee that the maximum number of deadlines missed consecutively\ndoes not interfere with stability requirements.\nIn Tables 11.2 and 11.3, we report the results obtained for various con-\ntrol systems with n states, under m −1 deadline misses, by applying the\ndense and sparse SOS approaches with relaxation orders r = 1 and r = 2,\nrespectively. The examples are randomly generated, i.e., our script gener-\nates a random system and then tries to control it2.\nIn Table 11.2, we ﬁx m = 5 and vary n from 20 to 110. For these ex-\namples, surprisingly the dense and sparse approaches with the relaxation\n2Available at https://wangjie212.github.io/jiewang/code.html.\n\n182\nChapter 11. Application in stability of control-systems\norder r = 1 always produce the same upper bounds. As one can see from\nthe table, the sparse approach is more efﬁcient and scalable than the dense\none.\nIn Table 11.3, we vary m from 3 to 11 and vary n from 8 to 24. The\ncolumn “ub” indicates the upper bound given by the dense approach with\nthe relaxation order r = 1. For these examples, with the relaxation order\nr = 2, the sparse approach produces upper bounds that are very close\nto those given by the dense approach. And again the sparse approach is\nmore efﬁcient and scalable than the dense one.\nTable 11.2: Results for control systems with r = 1 and m = 5.\nSparse\nDense\nn\nlb\ntime\nub\nmb\ntime\nub\nmb\n30\n1.4682\n4.30\n1.5132\n14\n57.8\n1.5131\n30\n30\n1.0924\n4.42\n1.0961\n14\n65.4\n1.0961\n30\n50\n1.3153\n17.3\n1.3248\n18\n660\n1.3248\n50\n50\n1.1884\n17.5\n1.1884\n18\n680\n1.1884\n50\n70\n1.8135\n54.2\n1.8578\n22\n∗\n∗\n∗\n70\n1.2727\n53.9\n1.2727\n22\n∗\n∗\n∗\n90\n1.8745\n133\n1.9020\n26\n∗\n∗\n∗\n90\n1.4452\n132\n1.4452\n26\n∗\n∗\n∗\n110\n2.3597\n280\n2.3943\n30\n-\n-\n-\n110\n1.5753\n287\n1.5753\n30\n-\n-\n-\nTable 11.3: Results for control systems with r = 2.\nSparse\nDense\nm\nn\nlb\nub\ntime\nub\nmb\ntime\nub\nmb\n3\n8\n0.7218\n0.7467\n0.60\n0.7310\n10\n13.4\n0.7305\n36\n4\n10 0.7458\n0.7738\n0.75\n0.7564\n10\n107\n0.7554\n55\n5\n12 0.8601\n0.8937\n1.08\n0.8706\n10\n1157 0.8699\n78\n6\n14 0.7875\n0.8107\n1.32\n0.7958\n10\n∗\n∗\n∗\n7\n16 1.1110\n1.1531\n1.81\n1.1182\n10\n-\n-\n-\n8\n18 1.0487\n1.0881\n2.05\n1.0569\n10\n-\n-\n-\n9\n20 0.7570\n0.7808\n2.52\n0.7660\n10\n-\n-\n-\n10 22 0.9911\n1.0315\n2.70\n1.0002\n10\n-\n-\n-\n11 24 0.7339\n0.7530\n3.67\n0.7418\n10\n-\n-\n-\n\n11.4. Notes and sources\n183\n11.4\nNotes and sources\nThe material from this chapter is issued from [WMM21]. The JSR was ﬁrst\nintroduced by Rota and Strang in [RS60] and since then has found appli-\ncations in many areas such as the stability of switched linear dynamical\nsystems, the continuity of wavelet functions, combinatorics and language\ntheory, the capacity of some codes, the trackability of graphs. We refer the\nreader to [Jun09] for a survey of the theory and applications of JSR.\n\n\nBibliography\n[Gri96]\nGustaf Gripenberg. Computing the joint spectral radius. Lin-\near Algebra and its Applications, 234:43–60, 1996.\n[Jun09]\nRaphaël Jungers. The joint spectral radius: theory and applica-\ntions, volume 385. Springer Science & Business Media, 2009.\n[MHMJZ20] Martina Maggio, Arne Hamann, Eckart Mayer-John, and\nDirk Ziegenbein. Control-system stability under consecutive\ndeadline misses constraints. In 32nd Euromicro Conference on\nReal-Time Systems (ECRTS 2020). Schloss Dagstuhl-Leibniz-\nZentrum für Informatik, 2020.\n[PJ08]\nPablo A Parrilo and Ali Jadbabaie.\nApproximation of the\njoint spectral radius using sum of squares. Linear Algebra and\nits Applications, 428(10):2385–2402, 2008.\n[RS60]\nGian-Carlo Rota and W Strang. A note on the joint spectral\nradius. In Gian-Carlo Rota on Analysis and Probability: Selected\nPapers and Commentaries, 1960.\n[WMM21]\nJie Wang, Martina Maggio, and Victor Magron. Sparsejsr: A\nfast algorithm to compute joint spectral radius via sparse sos\ndecompositions. In 2021 American Control Conference (ACC),\npages 2254–2259. IEEE, 2021.\n\n\nChapter 12\nMiscellaneous\nThe goal of this chapter is to propose alternative schemes to methods\nbased on sparse SOS polynomials. First, we focus in Section 12.1 on sum\nof nonnegative circuits (SONC) polynomials, a set of new nonnegativity\ncertiﬁcates, independent of the set of SOS polynomials described earlier in\nthis book. We present a characterization of SONC polynomials in terms\nof sums of binomial squares with rational exponents. Next, we present\nin Section 12.2 a framework to speed-up the resolution of SDP relaxations\narising from the moment-SOS hierarchy. This framework is based on the\nuse of ﬁrst-order methods.\n12.1\nNonnegative circuits and binomial squares\nA lattice point α ∈Nn is said to be even if it is in (2N)n. A subset T =\n{α1, . . . , αm} ⊆(2N)n is called a trellis when T comprises the vertices of a\nsimplex. Given a trellis T , a circuit polynomial is of the form ∑α∈T cαxα −\ndxβ ∈R[x], where cα > 0 for all α ∈T , and β lies in the relative interior of\nthe simplex associated to T . The name “circuit polynomial” stems from\nthe fact that (T , β) consists of a circuit. Given a circuit polynomial f =\n∑α∈T cαxα −dxβ ∈R[x], there exist unique barycentric coordinates (λ)m\nj=1\nsatisfying\nβ =\nm\n∑\nj=1\nλjα(j) with λj > 0 and\nm\n∑\nj=1\nλj = 1.\n(12.1)\nWe then deﬁne the related circuit number as Θ f = ∏m\nj=1\n\u0010\ncα(j)/λj\n\u0011λj.\nCircuit polynomials are proper building blocks for nonnegativity cer-\ntiﬁcates since the circuit number alone determines whether they are non-\nnegative.\n\n188\nChapter 12. Miscellaneous\nTheorem 12.1 A circuit polynomial f is nonnegative if and only if f is a sum of\nmonomial squares or |d| ≤Θ f .\nExample 12.2 Let f = x4\n1x2\n2 + x2\n1x4\n2 + 1 −3x2\n1x2\n2 be the Motzkin polynomial\nand T = {α1 = (0, 0), α2 = (4, 2), α3 = (2, 4)}, β = (2, 2). Then β =\n1\n3α1 + 1\n3α2 + 1\n3α3.\n(0, 0)\nα1\n(2, 4)\nα3\n(4, 2)\nα2\n(2, 2)\nβ\nOne easily checks that | −3| ≤Θ f = 3, proving that f is nonnegative.\nAn explicit representation of a polynomial being a SONC provides a\ncertiﬁcate for its nonnegativity, which is called a SONC decomposition.\nPolynomial optimization via SONC decompositions can be solved by means\nof geometric programming. Here we introduce a potentially cheaper ap-\nproach based on second-order cone programming.\nFor a subset of points M ⊆Nn, let\nA(M) :=\n\u001a1\n2(v + w) | v ̸= w, v, w ∈M ∩(2N)n\n\u001b\nbe the set of averages of distinct even points in M. For a trellis T , we call\nM a T -mediated set if T ⊆M ⊆A(M) ∪T .\nTheorem 12.3 Let f = ∑α∈T cαxα −dxβ ∈R[x] with d ̸= 0 be a non-\nnegative circuit polynomial. Then f is a sum of binomial squares if and\nonly if there exists a T -mediated set containing β. Moreover, suppose that\nβ belongs to a T -mediated set M and for each u ∈M \\ T , let us write\nu = 1\n2(vu + wu) for some vu ̸= wu ∈M ∩(2N)n. Then we can rewrite\nf as f = ∑u∈M\\T (aux\n1\n2 vu −bux\n1\n2 wu)2 for some au, bu ∈R.\n\n12.1. Nonnegative circuits and binomial squares\n189\nBy Theorem 12.3, to represent a nonnegative circuit polynomial as a\nsum of binomial squares, we need to ﬁrst decide if there exists a T -mediated\nset containing a given lattice point, and then compute one if there exists.\nHowever, such a T -mediated set may not exist in general. In order to cir-\ncumvent this obstacle, we introduce the concept of T -rational mediated\nsets as a replacement of T -mediated sets by admitting rational numbers\nin coordinates.\nConcretely, for a subset of points M ⊆Qn, let us deﬁne\neA(M) :=\n\u001a1\n2(v + w) | v ̸= w, v, w ∈M\n\u001b\nas the set of averages of distinct rational points in M. For a trellis T ⊆Nn,\nwe say that M is a T -rational mediated set if T ⊆M ⊆eA(M) ∪T . Given\na trellis T and a lattice point β ∈conv(T )◦, there always exists a T -\nrational mediated set containing β as stated in the following Proposition.\nProposition 12.4 Given a trellis T and a lattice point β ∈conv(T )◦, there\nexists a T -rational mediated set MT β containing β such that the denominators\n(resp. numerators) of coordinates of points in MT β are odd (resp. even) numbers.\nThe proof of Proposition 12.4 is constructive, and yields an algorithm\nto compute such a T -rational mediated set MT β. We refer the reader to\n[MW22] for the details. By virtue of Proposition 12.4, we are able to de-\ncompose SONC polynomials into sums of binomial squares with rational\nexponents.\nTheorem 12.5 Let f = ∑α∈T cαxα −dxβ ∈R[x] with d ̸= 0 be a circuit\npolynomial. Assume that MT β is a T -rational mediated set containing β\nprovided by Proposition 12.4. For each u ∈MT β \\ T , let u = 1\n2(vu + wu)\nfor some vu ̸= wu ∈MT β. Then f is nonnegative if and only if f can be\nwritten as f = ∑u∈MT β\\T (aux\n1\n2 vu −bux\n1\n2 wu)2 for some au, bu ∈R.\nExample 12.6 As in Example 12.2, let f = x4\n1x2\n2 + x2\n1x4\n2 + 1 −3x2\n1x2\n2 be the\nMotzkin polynomial, T = {α1 = (0, 0), α2 = (4, 2), α3 = (2, 4)} and β =\n(2, 2). Let β1 = 1\n3α1 + 2\n3α2 and β2 = 1\n3α1 + 2\n3α3 such that β = 1\n2 β1 + 1\n2 β2. Let\nβ3 = 2\n3α1 + 1\n3α2 and β4 = 2\n3α1 + 1\n3α3. Then M = {α1, α2, α3, β, β1, β2, β3, β4}\nis a T -rational mediated set containing β. By Theorem 12.5, one has\nf = (a1x\n2\n3\n1 x\n4\n3\n2 −b1x\n4\n3\n1 x\n2\n3\n2 )2 + (a2x1x2\n2 −b2x\n1\n3\n1 x\n2\n3\n2 )2 + (a3x\n2\n3\n1 x\n4\n3\n2 −b3)2\n+ (a4x2\n1x2 −b4x\n2\n3\n1 x\n1\n3\n2 )2 + (a5x\n4\n3\n1 x\n2\n3\n2 −b5)2.\n\n190\nChapter 12. Miscellaneous\n(0, 0)\nα1\n(2, 4)\nα3\n(4, 2)\nα2\n(2, 2)\nβ\n( 4\n3, 8\n3)\nβ2\n( 8\n3, 4\n3)\nβ1\n( 2\n3, 4\n3)\nβ4\n( 4\n3, 2\n3)\nβ3\nComparing coefﬁcients yields\nf = 3\n2(x\n2\n3\n1 x\n4\n3\n2 −x\n4\n3\n1 x\n2\n3\n2 )2 + (x1x2\n2 −x\n1\n3\n1 x\n2\n3\n2 )2 + 1\n2(x\n2\n3\n1 x\n4\n3\n2 −1)2\n+ (x2\n1x2 −x\n2\n3\n1 x\n1\n3\n2 )2 + 1\n2(x\n4\n3\n1 x\n2\n3\n2 −1)2,\na sum of ﬁve binomial squares with rational exponents.\nFor a polynomial f = ∑α∈A fαxα ∈R[x], let Λ( f ) := {α ∈A | α ∈\n(2N)n and fα > 0} and Γ( f ) := supp( f ) \\ Λ( f ) so that f = ∑α∈Λ( f ) cαxα −\n∑β∈Γ( f ) dβxβ. For each β ∈Γ( f ), let\nC (β) := {T | T ⊆Λ( f ) and (T , β) consists of a circuit}.\n(12.2)\nAs a consequence of Theorem 5.5 from [Wan22], if f is a SONC polyno-\nmial, then it admits a decomposition\nf = ∑\nβ∈Γ( f )\n∑\nT ∈C (β)\nfT β + ∑\nα∈˜\nA\ncαxα,\n(12.3)\nwhere fT β is a nonnegative circuit polynomial supported on T ∪{β} and\n˜\nA = {α ∈Λ( f ) | α /∈∪β∈Γ( f ) ∪T ∈C (β) T }.\nTheorem 12.7 Let f = ∑α∈Λ( f ) cαxα −∑β∈Γ( f ) dβxβ ∈R[x]. For every\nβ ∈Γ( f ) and every trellis T ∈C (β), let MT β be a T -rational mediated\nset containing β provided by Proposition 12.4. Let M = ∪β∈Γ( f ) ∪T ∈C (β)\nMT β. For each u ∈M \\ Λ( f ), let u =\n1\n2(vu + wu) for some vu ̸=\nwu ∈M. Then f is a SONC polynomial if and only if f can be written as\nf = ∑u∈M\\Λ( f )(aux\n1\n2 vu −bux\n1\n2 wu)2 + ∑α∈˜\nA cαxα for some au, bu ∈R.\n\n12.1. Nonnegative circuits and binomial squares\n191\nIn order to obtain a SONC decomposition of f, we use all simplices\ncovering β for each β ∈Γ( f ) in Theorem 12.5. In practice, we do not need\nthat many simplices as illustrated by the following example.\nExample 12.8 Let f = 50x4\n1x4\n2 + x4\n1 + 3x4\n2 + 800 −100x1x2\n2 −100x2\n1x2. Let\nα1 = (0, 0), α2 = (4, 0), α3 = (0, 4), α4 = (4, 4) and β1 = (2, 1), β2 = (1, 2).\nThere are two simplices covering β1: the one with vertices {α1, α2 α3} (denoted\nby ∆1), and the one with vertices {α1, α2, α4} (denoted by ∆2). There are two\nsimplices covering β2: ∆1 and the one with vertices {α1, α3, α4} (denoted by\n∆3). One can check that f admits a SONC decomposition f = g1 + g2, where\ng1 = 20x4\n1x4\n2 + x4\n1 + 400 −100x2\n1x2 supported on ∆2 and g2 = 30x4\n1x4\n2 + 3x4\n2 +\n400 −100x1x2\n2 supported on ∆3 are both nonnegative circuit polynomials. So the\nsimplex ∆1 is not needed in this SONC decomposition of f.\nα1\nα2\nα3\nα4\nβ1\nβ2\nHere we rely on a heuristics to compute a set of simplices with vertices\ncoming from Λ( f ) and that covers Γ( f ). For β ∈Γ( f ) and α0 ∈Λ( f ), let\nus deﬁne an auxiliary linear program:\nSimSel(β, Λ( f ), α0) :=\n\n\n\n\n\n\n\n\n\n\n\n\n\narg max\nλα\nλα0\ns.t.\n∑α∈Λ( f ) λα · α = β\n∑α∈Λ( f ) λα = 1\nλα ≥0,\n∀α ∈Λ( f )\nIf β and α0 lie on the same face of N ( f ), then the output of SimSel(β, Λ( f ), α0)\ncorresponds to a trellis which contains α0 and covers β.\nSuppose f = ∑α∈Λ( f ) cαxα −∑β∈Γ( f ) dβxβ ∈R[x] and assume that\n{α ∈Λ( f ) | α /∈∪β∈Γ( f ) ∪T ∈C (β) T } = ∅. We ﬁrst compute a sim-\nplex cover {(Tk, βk)}l\nk=1 for f by repeatedly running the program SimSel\nwith appropriate β and α0. Then, for each k let Mk be a Tk-rational me-\ndiated set containing βk and sk be the cardinality of Mk \\ Tk. For each\nuk\ni ∈Mk \\ Tk, let us write uk\ni = 1\n2(vk\ni + wk\ni ). Let K be the 3-dimensional\nrotated second-order cone, i.e.,\nK := {(a, b, c) ∈R3 | 2ab ≥c2, a ≥0, b ≥0}.\n(12.4)\n\n192\nChapter 12. Miscellaneous\nThen we can approximate fmin from below with the following second-\norder cone program:\n\n\n\n\n\nsup\nb\ns.t.\nf (x) −b = ∑l\nk=1 ∑sk\ni=1(2ak\ni xvk\ni + bk\ni xwk\ni −2ck\ni xuk\ni )\n(ak\ni , bk\ni , ck\ni ) ∈K,\n∀i, k\n(12.5)\n12.2\nFirst-order SDP solvers\nIn the previous chapters, we have explained how to reduce the size of\nthe moment-SOS relaxations by exploiting certain sparsity structures in-\nduced by the input polynomials. A complementary framework consists of\nexploiting the speciﬁc properties of the matrices involved in the moment\nSDP relaxations, in order to speed-up their resolution via speciﬁc ﬁrst-\norder algorithms. Here, we prove that every moment relaxation of a POP\nwith a sphere or ball constraint can be reformulated as an SDP involv-\ning a PSD matrix with constant trace property (CTP). As a result, such\nmoment relaxations can be solved efﬁciently by ﬁrst-order methods that\nexploit CTP, e.g., the conditional gradient-based augmented Lagrangian\nmethod.\nFirst let us deﬁne CTP for a POP. Given f, g1, . . . , gm, h1, . . . , hl ∈R[x],\nlet us consider the following POP with n variables, m inequality constraints\nand l equality constraints:\nfmin = min { f (x) : gj(x) ≥0, j ∈[m], hi(x) = 0, i ∈[l]}.\n(12.6)\nLet rmin := max {⌈deg( f )\n2\n⌉, ⌈deg(g1)\n2\n⌉, . . . , ⌈deg(gm)\n2\n⌉, ⌈deg(h1)\n2\n⌉, . . . , ⌈deg(hl)\n2\n⌉}.\nFor each r ≥rmin, recall that the moment relaxation associated to POP\n(12.6) is\nf r = inf\ny\n\n\nLy( f )\n\f\f\f\f\f\f\nMr(y) ⪰0, y0 = 1\nMr−⌈deg(gj)/2⌉(gjy) ⪰0, j ∈[m]\nMr−⌈deg(hi)/2⌉(hiy) = 0, i ∈[l]\n\n\n.\n(12.7)\nIf we denote\nDr(y) := Diag(Mr(y), Mr−⌈deg(g1)/2⌉(g1y), . . . , Mr−⌈deg(gm)/2⌉(gmy)),\nthen SDP (12.7) can be rewritten as\nf r = inf\ny\n\u001a\nLy( f )\n\f\f\f\f\nDr(y) ⪰0, y0 = 1\nMr−⌈deg(hi)/2⌉(hiy) = 0, i ∈[l]\n\u001b\n.\n(12.8)\n\n12.2. First-order SDP solvers\n193\nDeﬁnition 12.9 (CTP for a POP) We say that POP (12.6) has CTP if for every\nr ≥rmin, there exists ar > 0 and a positive deﬁnite matrix Tr such that\nMr−⌈deg(hi)/2⌉(hiy) = 0, i ∈[l]\ny0 = 1\n\u001b\n⇒trace(TrDr(y)Tr) = ar.\n(12.9)\nIn other words, we say that a POP has CTP if each moment relaxation\n(12.8) has an equivalent form involving a PSD matrix whose trace is con-\nstant. In this case, we call ar the constant trace and Tr the basis transfor-\nmation matrix. We illustrate this conversion to an SDP with CTP.\nExample 12.10 Consider the following univariate POP\n−1 = inf {x : 1 −x2 = 0}.\nThen the second-order moment relaxation is\nf 2 =\ninf\ny\ny1\ns.t.\n\n\ny0\ny1\ny2\ny1\ny2\ny3\ny2\ny3\ny4\n\n⪰0,\n\u0014\ny0 −y2\ny1 −y3\ny1 −y3\ny2 −y4\n\u0015\n= 0, y0 = 1.\nIt can be rewritten as\nf 2 =\ninf\ny\ny1\ns.t.\n\n\n1\ny1\n1\ny1\n1\ny1\n1\ny1\n1\n\n⪰0,\nby removing equality constraints, in which the PSD matrix has trace 3. Alterna-\ntively, with D2(y) = M2(y) and\nT2 =\n\n\n1\n0\n0\n0\n√\n2\n0\n0\n0\n1\n\n,\nY = T2D2(y)T2 = T2M2(y)T2,\nwe have\n−f 2 = sup\nY\n{⟨C, Y⟩: ⟨Ai, Y⟩= bi, i ∈[5], Y ⪰0},\nwhere b1 = · · · = b4 = 0, b5 = 1 and\nC = −\n√\n2\n4\n\n\n0\n1\n0\n1\n0\n0\n0\n0\n0\n\n, A1 =\n√\n2\n2\n\n\n0\n0\n1\n0\n−1\n0\n1\n0\n0\n\n, A2 = 1\n2\n\n\n2\n0\n−1\n0\n0\n0\n−1\n0\n0\n\n,\nA3 =\n√\n2\n4\n\n\n0\n1\n0\n1\n0\n−1\n0\n−1\n0\n\n, A4 = 1\n2\n\n\n0\n0\n1\n0\n0\n0\n1\n0\n−2\n\n, A5 =\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n.\n\n194\nChapter 12. Miscellaneous\nWe then obtain that\n⟨Ai, Y⟩= bi, i ∈[5] ⇒trace(Y) = 4.\nFor the minimization of a polynomial on the unit sphere, one can show\nthat POP (12.6) has CTP with ar = 2r and Tr = Diag((θ1/2\nr,α )α∈Nnr ), where\n(θr,α)α∈Nnr ⊆R>0 satisﬁes (1 + ∥x∥2\n2)r = ∑α∈Nnr θr,αx2α, for all r ≥1.\nNext, we provide a sufﬁcient condition for POP (12.6) to have CTP.\nFor r ≥rmin, let M(g)r be the truncated quadratic module associated to\ng = {g1, . . . , gm}. Let M◦(g)r be the interior of the truncated quadratic\nmodule M(g)r, which is deﬁned by\nM◦(g)r := {v⊺\nr G0vr+ ∑\nj∈[m]\ngjv⊺\nr−⌈deg(gj)/2⌉Gjvr−⌈deg(gj)/2⌉\n| Gj ≻0, j ∈{0} ∪[m]}.\nTheorem 12.11 If 1 ∈M◦(g)r for all r ≥rmin, then POP (12.6) has CTP.\nThis condition holds in particular when the set of constraints includes either\nball or annulus constraints.\nOnce we have the knowledge of the constant ar, the r-th order moment\nrelaxation can be cast as follows:\n−f r = sup\nY\n{⟨Cr, Y⟩: ArY = br, Y ⪰0, trace(Y) = ar},\n(12.10)\nwhere Ar is a linear operator (used here to encode afﬁne constraints of the\nSDP). Afterwards, it turns out that SDP (12.10) is equivalent to minimizing\nthe largest eigenvalue of a matrix pencil:\n−f r = inf\nz {arλmax(Cr −A⊺\nr z) + b⊺\nr z},\n(12.11)\nwhere A⊺\nk denotes the adjoint operator of Ak. Hence (12.11) forms what\nwe call a hierarchy of (nonsmooth, convex) spectral relaxations.\nTo solve large-scale instances of this maximal eigenvalue minimization\nproblem, a plethora of ﬁrst-order methods are available, including sub-\ngradient descent or variants of the mirror-prox algorithm, spectral bundle\nmethods, the conditional gradient based augmented Lagrangian (CGAL)\nalgorithm, and variants relying on limited memory and arithmetic.\n12.3\nNotes and sources\nThe material presented in Chapter 12.1 has been published in [WM20,\nMW22]. The set of SONC polynomials was introduced by [IDW16]. The\n\n12.3. Notes and sources\n195\ncondition linking nonnegativity of a circuit polynomial with its circuit\nnumber, stated in Theorem 12.1, can be found in [IDW16, Theorem 3.8].\nThe interested reader can ﬁnd more details on mediated sets in [PR21,\nRez89]. Theorem 12.3, Proposition 12.4, Theorem 12.5 correspond to Theo-\nrem 5.2 in [IDW16], Lemma 3.5 and Theorem 3.6 in [WM20], respectively.\nThe heuristics used to compute the simplex cover can be found, e.g., in\n[SdW18]. Detailed numerical experiments comparing the usual geomet-\nric programming approach and the second-order cone programming ap-\nproach stated in Chapter 12.1 can be found in [WM20, § 6]. The corre-\nsponding Julia package is available at github:SONCSOCP. A recent study\nby [Pap19] proposes a systematic method to compute an optimal simplex\ncover.\nThe fact that SONC cones admit a second-order cone characterization\nwas ﬁrstly stated in [Ave19, Theorem 17], but the related proof does not\nprovide an explicit construction.\nAnother recently introduced alterna-\ntive certiﬁcates by [CS16] are sums of arithmetic geometric exponentials\n(SAGE), which can be obtained via relative entropy programming. The\nconnection between SONC and SAGE polynomials has been recently stud-\nied in [MCW21, Wan22, KNT21]. It happens that SONC polynomials and\nSAGE polynomials are actually equivalent, and that both have a cancellation-\nfree representation in terms of generators.\nThe framework by [DHNdW20] relies on the dual SONC cone to com-\npute lower bounds of polynomials by means of linear programming in-\nstead of geometric programming. Note that there are no general guaran-\ntees that the bounds obtained with this framework are always more or\nless accurate than the approach based on geometric programming from\n[SdW18], and the same holds for performance. One of the similar fea-\ntures shared by SOS/SONC-based frameworks is their intrinsic connec-\ntions with conic programming: SOS decompositions are computed via\nsemideﬁnite programming and SONC decompositions via geometric pro-\ngramming or second-order cone programming. In both cases, the resulting\noptimization problems are solved with interior-point algorithms, thus out-\nput approximate nonnegativity certiﬁcates. However, one can still obtain\nan exact certiﬁcate from such output via hybrid numerical-symbolic al-\ngorithms when the input polynomial lies in the interior of the SOS/SONC\ncone. One way is to rely on rounding-projection algorithms adapted to the\nSOS cone by [PP08] and the SONC cone by [MSdW19], or alternatively on\nperturbation-compensation schemes as in [MSEDS19, MSED18, MDSV22,\nMDV21, MW22].\nThe material presented in Chapter 12.2 is issued from [MML20, MLMW22].\nTheorem 12.11 is proved in [MLMW22, § 3]. The equivalence between SDP\n(12.10) and (12.11) follows from the framework by Helmberg and Rendl\n[HR00]. The CTP has been also studied in the context of eigenvalue opti-\nmization; see [MBM21].\n\n196\nChapter 12. Miscellaneous\nCGAL is a ﬁrst-order method that exploits CTP. In [YTF+21], the au-\nthors combined CGAL with the Nyström sketch (named SketchyCGAL),\nwhich requires dramatically less storage than other methods and is very\nefﬁcient for solving the ﬁrst-order relaxation of large-scale Max-Cut in-\nstances.\nWhen solving the second and higher order relaxations, SDP solvers\noften encounter the following issues:\n• Storage: Interior-point methods are often chosen by users because\nof their highly accurate output. These methods are efﬁcient for solv-\ning medium-scale SDPs. However they frequently fail due to lack of\nmemory when solving large-scale SDPs. First-order methods (e.g.,\nADMM, SBM, CGAL) provide alternatives to interior-point methods\nto avoid the memory issue. This is due to the fact that the cost per\niteration of ﬁrst-order methods is much lower than that of interior-\npoint methods. At the price of losing convexity one can also rely\non heuristic methods and replace the full matrix Y in the moment\nrelaxation by a simpler one, in order to save memory. For instance,\nthe Burer-Monteiro method [BM05] considers a low rank factoriza-\ntion of Y. However, to get correct results the rank cannot be too low\n[WW20] and therefore this limitation makes it useless for the sec-\nond and higher order relaxations of POPs. Not suffering from such\na limitation, CGAL not only maintains the convexity of the moment\nrelaxation but also possibly runs with an implicit matrix Y; see Re-\nmarks A.12 and A.17 in [MLMW22].\n• Accuracy: Nevertheless, ﬁrst-order methods have low convergence\nrates compared to interior-point methods. Their performance de-\npends heavily on the problem scaling and conditioning. As a result,\nwhen solving large-scale SDPs with ﬁrst-order methods it is often\ndifﬁcult to obtain numerical results with high accuracy.\nExtensive numerical comparisons between some of these methods have\nbeen performed in § 4 and Appendix A.3 from [MLMW22].\nWe close this chapter by emphasizing that there are other issues to be\naddressed to improve the scalability of polynomial optimization meth-\nods. A ﬁrst complementary research track is to overcome the issue that\nSDP relaxations arising from the moment-SOS hierarchy can often be ill-\nconditioned. Possible remedies include the use of other bases of vector\nspaces of polynomials, for instance Chebyshev polynomials instead of the\nstandard monomial basis as done in the univariate case in [Hen12, § 5],\nand encoding polynomial identities by evaluating them on suitably (per-\nhaps randomly) chosen points, seeing [LP04, LTY17] for preliminary at-\ntempts.\nAnother complementary research track is to exploit symmetries in poly-\nnomial optimization. For instance the invariance of all input polynomi-\n\n12.3. Notes and sources\n197\nals under the action of a subgroup of the general linear group has been\nstudied in [RTAL13]. Previous works focused on exploiting the knowl-\nedge of the group action at the SDP level [GP04]. These frameworks have\nbeen applied to compute correlation bounds for ﬁnite-dimensional quan-\ntum systems [TRR19] and bounds of packing problems [dLV15, DGV+17,\nDDLM21]. Exploiting symmetries has also been investigated for polyno-\nmial optimization based on sums of arithmetic-geometric-exponentials in\n[MNR+21]. It would be deﬁnitely worth extending these frameworks to\nother (non-)discrete groups, dynamical systems and to noncommutative\npolynomial optimization in the future. This research direction of symme-\ntry exploitation is still to be pursued and shall hopefully lead to publishing\nanother complementary book in the upcoming years!\n\n\nBibliography\n[Ave19]\nG. Averkov.\nOptimal size of linear matrix inequalities in\nsemideﬁnite approaches to polynomial optimization. SIAM\nJournal on Applied Algebra and Geometry, 3(1):128–151, 2019.\n[BM05]\nSamuel Burer and Renato DC Monteiro. Local minima and\nconvergence in low-rank semideﬁnite programming. Math-\nematical Programming, 103(3):427–444, 2005.\n[CS16]\nV. Chandrasekaran and P. Shah.\nRelative Entropy Re-\nlaxations for Signomial Optimization.\nSIAM J. Optim.,\n26(2):1147–1173, 2016.\n[DDLM21]\nMaria Dostert, David De Laat, and Philippe Moustrou. Ex-\nact semideﬁnite programming bounds for packing prob-\nlems. SIAM Journal on Optimization, 31(2):1433–1458, 2021.\n[DGV+17]\nMaria Dostert, Cristóbal Guzmán, Frank Vallentin, et al.\nNew upper bounds for the density of translative packings of\nthree-dimensional convex bodies with tetrahedral symme-\ntry. Discrete & Computational Geometry, 58(2):449–481, 2017.\n[DHNdW20] Mareike Dressler, Janin Heuer, Helen Naumann, and Timo\nde Wolff. Global optimization via the dual sonc cone and\nlinear programming. In Proceedings of the 45th International\nSymposium on Symbolic and Algebraic Computation, pages\n138–145, 2020.\n[dLV15]\nDavid de Laat and Frank Vallentin. A semideﬁnite program-\nming hierarchy for packing problems in discrete geometry.\nMathematical Programming, 151(2):529–553, 2015.\n[GP04]\nKarin Gatermann and Pablo A Parrilo. Symmetry groups,\nsemideﬁnite programs, and sums of squares. Journal of Pure\nand Applied Algebra, 192(1-3):95–128, 2004.\n\n200\nBibliography\n[Hen12]\nDidier Henrion. Semideﬁnite characterisation of invariant\nmeasures for one-dimensional discrete dynamical systems.\nKybernetika, 48(6):1089–1099, 2012.\n[HR00]\nChristoph Helmberg and Franz Rendl. A spectral bundle\nmethod for semideﬁnite programming. SIAM Journal on Op-\ntimization, 10(3):673–696, 2000.\n[IDW16]\nS. Iliman and T. De Wolff. Amoebas, nonnegative polyno-\nmials and sums of squares supported on circuits. Research in\nthe Mathematical Sciences, 3(1):9, 2016.\n[KNT21]\nLukas Katthän, Helen Naumann, and Thorsten Theobald. A\nuniﬁed framework of sage and sonc polynomials and its du-\nality theory. Mathematics of Computation, 90(329):1297–1322,\n2021.\n[LP04]\nJohan Lofberg and Pablo A Parrilo.\nFrom coefﬁcients to\nsamples: a new approach to sos optimization. In 2004 43rd\nIEEE Conference on Decision and Control (CDC)(IEEE Cat. No.\n04CH37601), volume 3, pages 3154–3159. IEEE, 2004.\n[LTY17]\nJeanB Lasserre, Kim-Chuan Toh, and Shouguang Yang. A\nbounded degree sos hierarchy for polynomial optimization.\nEURO Journal on Computational Optimization, 5(1-2):87–117,\n2017.\n[MBM21]\nNgoc Hoang Anh Mai, Abhishek Bhardwaj, and Victor Ma-\ngron. The constant trace property in noncommutative opti-\nmization. In Proceedings of the 2021 on International Sympo-\nsium on Symbolic and Algebraic Computation, pages 297–304,\n2021.\n[MCW21]\nRiley Murray, Venkat Chandrasekaran, and Adam Wier-\nman. Newton polytopes and relative entropy optimization.\nFoundations of Computational Mathematics, pages 1–35, 2021.\n[MDSV22]\nVictor Magron, Mohab Safey El Din, Markus Schweighofer,\nand Trung Hieu Vu. Exact sohs decompositions of trigono-\nmetric univariate polynomials with gaussian coefﬁcients.\narXiv preprint arXiv:2202.06544, 2022.\n[MDV21]\nVictor Magron, Mohab Safey El Din, and Trung-Hieu Vu.\nSum of squares decompositions of polynomials over their\ngradient ideals with rational coefﬁcients.\narXiv preprint\narXiv:2107.11825, 2021.\n\nBibliography\n201\n[MLMW22]\nNgoc Hoang Anh Mai, Jean-Bernard Lasserre, Victor Ma-\ngron, and Jie Wang.\nExploiting constant trace property\nin large-scale polynomial optimization. ACM Trans. Math.\nSoftw., 2022.\n[MML20]\nNgoc Hoang Anh Mai, Victor Magron, and Jean-Bernard\nLasserre. A hierarchy of spectral relaxations for polynomial\noptimization. arXiv preprint arXiv:2007.09027, 2020.\n[MNR+21]\nPhilippe Moustrou,\nHelen Naumann,\nCordian Riener,\nThorsten Theobald, and Hugues Verdure.\nSymmetry re-\nduction in am/gm-based optimization.\narXiv preprint\narXiv:2102.12913, 2021.\n[MSdW19]\nV. Magron, H. Seidler, and T. de Wolff. Exact optimization\nvia sums of nonnegative circuits and arithmetic-geometric-\nmean-exponentials. In Proceedings of the 2019 on International\nSymposium on Symbolic and Algebraic Computation, ISSAC ’19,\npage 291–298, New York, NY, USA, 2019.\n[MSED18]\nV. Magron and M. Safey El Din. On Exact Polya and Puti-\nnar’s Representations. In ISSAC’18: Proceedings of the 2018\nACM International Symposium on Symbolic and Algebraic Com-\nputation. ACM, New York, NY, USA, 2018.\n[MSEDS19]\nV. Magron, M. Safey El Din, and M. Schweighofer. Algo-\nrithms for weighted sum of squares decomposition of non-\nnegative univariate polynomials. Journal of Symbolic Compu-\ntation, 93:200–220, 2019.\n[MW22]\nVictor Magron and Jie Wang. Sonc optimization and exact\nnonnegativity certiﬁcates via second-order cone program-\nming. Journal of Symbolic Computation, 2022.\n[Pap19]\nD. Papp. Duality of sum of nonnegative circuit polynomials\nand optimal SONC bounds. arXiv preprint arXiv:1912.04718,\n2019.\n[PP08]\nH. Peyrl and P.A. Parrilo. Computing sum of squares de-\ncompositions with rational coefﬁcients. Theoretical Computer\nScience, 409(2):269–281, 2008.\n[PR21]\nVictoria Powers and Bruce Reznick.\nA note on mediated\nsimplices. Journal of Pure and Applied Algebra, 225(7):106608,\n2021.\n[Rez89]\nB. Reznick. Forms derived from the arithmetic-geometric\ninequality. Mathematische Annalen, 283(3):431–464, 1989.\n\n202\nBibliography\n[RTAL13]\nCordian Riener, Thorsten Theobald, Lina Jansson Andrén,\nand Jean B Lasserre.\nExploiting symmetries in sdp-\nrelaxations for polynomial optimization. Mathematics of Op-\nerations Research, 38(1):122–141, 2013.\n[SdW18]\nH. Seidler and T. de Wolff.\nAn experimental comparison\nof sonc and sos certiﬁcates for unconstrained optimization.\narXiv preprint arXiv:1808.08431, 2018.\n[TRR19]\nArmin Tavakoli, Denis Rosset, and Marc-Olivier Renou.\nEnabling computation of correlation bounds for ﬁnite-\ndimensional quantum systems via symmetrization. Physical\nreview letters, 122(7):070501, 2019.\n[Wan22]\nJie Wang. Nonnegative polynomials and circuit polynomi-\nals. SIAM Journal on Applied Algebra and Geometry, 6(2):111–\n133, 2022.\n[WM20]\nJie Wang and Victor Magron. A second order cone charac-\nterization for sums of nonnegative circuits. In Proceedings\nof the 45th International Symposium on Symbolic and Algebraic\nComputation, pages 450–457, 2020.\n[WW20]\nIrene Waldspurger and Alden Waters. Rank optimality for\nthe burer–monteiro factorization. SIAM Journal on Optimiza-\ntion, 30(3):2577–2602, 2020.\n[YTF+21]\nAlp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine\nUdell, and Volkan Cevher. Scalable semideﬁnite program-\nming. SIAM Journal on Mathematics of Data Science, 3(1):171–\n200, 2021.\n\nPart III\nAppendix: software\nlibraries\n\n\nAppendix A\nProgramming with\nMATLAB\nA.1\nSparse moment relaxations with GLOPTIPOLY\nGLOPTIPOLY [HLL09] is a MATLAB library designed to solve the general-\nized problem of moments. We can rely on this library to solve the (primal)\nmoment relaxation arising after exploiting CS for a given POP. Next we\ngive two scripts to obtain the dense and sparse bounds given in Exam-\nple 3.9. In this example, we approximate the minimum of f = f1 + f2 + f3\non the basic compact semialgebraic set X with\nf1 = −x1x4,\nf2 = −x2\n1 + x1x2 + x1x3 −x2x3 + x2x5,\nf3 = −x5x6 + x1x5 + x1x6 + x3x6,\nand\nX = {x ∈Rn | (6.36 −x1)(x1 −4) ≥0, . . . , (6.36 −x6)(x6 −4) ≥0}.\nBy exploiting CS, the set of variables is decomposed as I1 = {1, 4}, I2 =\n{1, 2, 3, 5}, I3 = {1, 3, 5, 6}. The ﬁrst script below allows one to retrieve the\ndense bound f 2 of the dense primal moment relaxation (2.7) at the second\nrelaxation order r = 2. Our problem involves six polynomial variables\nand a measure µ (deﬁned in Line 3) supported on X (deﬁned in Lines 8-\n13). At Line 15, GLOPTIPOLY calls the SDP solver SeDuMi [Stu99] and\nreturns the objective value f 2 = 20.8608. In addition, GLOPTIPOLY is able\nto extract a minimizer of f on X through Algorithm 1, which then certiﬁes\nthat fmin = f 2.\n\n206\nAppendix A. Programming with MATLAB\n1\nr = 2 % relaxation order\n2\nmpol x1 x2 x3 x4 x5 x6\n3\nmu = meas([x1 x2 x3 x4 x5 x6]);\n4\nf1 = mom(-x1*x4);\n5\nf2 = mom(-x1^2 + x1*x2 + x1* x3 - x2*x3 + x2*x5);\n6\nf3 = mom(-x5*x6 + x1*x5 + x1*x6 + x3*x6);\n7\nf = f1 + f2 + f3; % objective function\n8\nX = [(6.36 - x1)*(x1-4)≥0,...\n9\n(6.36 - x2)*(x2-4)≥0,...\n10\n(6.36 - x3)*(x3-4)≥0,...\n11\n(6.36 - x4)*(x4-4)≥0,...\n12\n(6.36 - x5)*(x5-4)≥0,...\n13\n(6.36 - x6)*(x6-4)≥0]; % support\n14\nPdense = msdp(min(f), X, mass(mu)==1, r);\n15\n[stat,obj] = msol(Pdense);\nFor more details on the six GLOPTIPOLY commands (mpol, meas, mom, msdp,\nmass, msol), we refer the interested reader to the online tutorial:\nhttps://homepages.laas.fr/henrion/papers/gloptipoly3.pdf.\nThe second script below allows one to retrieve the sparse bound f 2\ncs corre-\nsponding to the second relaxation order r = 2. By contrast with the dense\ncase, one needs to deﬁne three measures µ1, µ2, µ3 (Lines 6-8), associated\nto I1, I2 and I3, respectively, and ensure that their marginals satisfy the\nequality constraints given in (3.5). For instance, the marginals of µ1 and µ2\nmust have equal moments for monomials in x1 since I12 = I1 ∩I2 = {1}\n(see Lines 14-15). At Line 34, GLOPTIPOLY returns the objective value\nf 2\ncs = f 2 = 20.8608 and is again able to extract a minimizer of f on X.\n1\nr = 2 % relaxation order\n2\nmpol x1 3\n3\nmpol x2 x4 x6\n4\nmpol x3 2\n5\nmpol x5 2\n6\nmu(1) = meas([x1(1) x4]); % first measure on I1 = {1, 4}\n7\nmu(2) = meas([x1(2) x2 x3(1) x5(1)]); % second measure on I2 ...\n= {1, 2, 3, 5}\n8\nmu(3) = meas([x1(3) x3(2) x5(2) x6]); % third measure on I3 ...\n= {1, 3, 5, 6}\n9\nf1 = mom(-x1(1)*x4) ;\n10\nf2 = mom(-x1(2)^2 + x1(2)*x2(1) + x1(2)* x3(1) - x2(1)*x3(1) ...\n+ x2(1)*x5(1));\n11\nf3 = mom(-x5(2)*x6 + x1(3)*x5(2) + x1(3)*x6 + x3(2)*x6);\n12\nf = f1 + f2 + f3; % objective function\n13\n14\nm12_1 = mom(mmon(x1(1),2*r)); % moments of the marginal of ...\nmu1 on monomials in x1, corresponding to I12 = {1}\n15\nm12_2 = mom(mmon(x1(2),2*r)); % moments of the marginal of ...\nmu2 on monomials in x1, corresponding to I12 = {1}\n16\n\nA.2. Sparse SOS relaxations with Yalmip\n207\n17\nm23_2 = mom(mmon([x1(2) x3(1) x5(1)],2*r)); % moments of the ...\nmarginal of mu2 on monomials in x1, x3, x5, ...\ncorresponding to I23 = {1, 3, 5}\n18\nm23_3 = mom(mmon([x1(3) x3(2) x5(2)],2*r)); % moments of the ...\nmarginal of mu3 on monomials in x1, x3, x5, ...\ncorresponding to I23 = {1, 3, 5}\n19\n20\nm13_1 = mom(mmon(x1(1),2*r)); % moments of the marginal of ...\nmu1 on monomials in x1, corresponding to I13 = {1}\n21\nm13_3 = mom(mmon(x1(3),2*r)); % moments of the marginal of ...\nmu3 on monomials in x1, corresponding to I13 = {1}\n22\n23\nK = [(6.36 - x1(1))*(x1(1)-4)≥0,...\n24\n(6.36 - x1(2))*(x1(2)-4)≥0,...\n25\n(6.36 - x1(3))*(x1(3)-4)≥0,...\n26\n(6.36 - x2)*(x2-4)≥0,...\n27\n(6.36 - x4)*(x4-4)≥0,...\n28\n(6.36 - x6)*(x6-4)≥0,...\n29\n(6.36 - x3(1))*(x3(1)-4)≥0,...\n30\n(6.36 - x3(2))*(x3(2)-4)≥0,...\n31\n(6.36 - x5(1))*(x5(1)-4)≥0,...\n32\n(6.36 - x5(2))*(x5(2)-4)≥0]; % supports\n33\nPsparse = msdp(min(f),m12_1-m12_2==0,m23_2-m23_3==0, ...\nm13_1-m13_3==0, K,mass(mu)==1);\n34\n[stat,obj] = msol(Psparse);\nOther similar scripts for approximating minima of rational functions are\ngiven in [BHL16, § 7].\nA.2\nSparse SOS relaxations with Yalmip\nYalmip [Lö04] is a MATLAB toolbox to model optimization problems and\nto solve them using external solvers. In particular, we can rely on this\ntoolbox to solve the SOS program (3.7), which is the dual of the moment\nrelaxation (3.6) encoded in the prior section. Next we give two scripts to\nobtain the dense and sparse SOS bounds given in Example 3.9.\nThe ﬁrst script below allows to retrieve the dense bound, correspond-\ning to SDP (2.9) at the second relaxation order r = 2. For each of the six\nbox constraints, we need to deﬁne an SOS multiplier si (see Line 11), cor-\nresponding to σi in (2.9). Note that the ﬁrst argument si refers to the SOS\npolynomial multiplier itself while the second argument ci refers to its (un-\nknown) vector of coefﬁcients. Here, we want to maximize the lower bound\nb of f such that f −b has a Putinar’s representation of degree 2r = 4, and\nthus the degree of each SOS multiplier is 2r −2 = 2. As with GLOPTIPOLY,\nYalmip calls at Line 15 the same SDP solver SeDuMi and returns the ob-\njective value f 2 = 20.8608.\n1\nr = 2; % relaxation order\n\n208\nAppendix A. Programming with MATLAB\n2\nsdpvar x1 x2 x3 x4 x5 x6 b;\n3\nx = [x1; x2; x3; x4; x5; x6];\n4\nf1 = -x1*x4;\n5\nf2 = -x1^2 + x1*x2 + x1* x3 - x2*x3 + x2*x5;\n6\nf3 = -x5*x6 + x1*x5 + x1*x6 + x3*x6;\n7\nf = f1 + f2 + f3; % objective function\n8\ng = (6.36 - x).*(x - 4); % the 6 polynomials used to define ...\nthe box constraints\n9\ns = []; c = [b]; F = [];\n10\nfor i=1:6\n11\n[si,ci] = polynomial(x,2*r-2); % SOS multiplier si of ...\ndegree 2 in the dense Putinar's representation\n12\ns = [s si]; c = [c; ci]; F = [F, sos(si)];\n13\nend\n14\nF = [F, sos(f - b - s*g)]; % SOS multiplier s0 = f - b - ...\nsum_i si gi of degree 4\n15\nsolvesos(F,-b,[],c); % solves the SOS program: maximize b ...\nsuch that f - b = s0 + sum_i si gi\nFor more details on the four Yalmip commands (sdpvar, polynomial, sos,\nsolvesos), we refer the interested reader to the online tutorial github:yalmip.\nThe second script below allows one to retrieve the sparse bound f 2\ncs\ncorresponding to the second relaxation order r = 2. By contrast with the\ndense case, one needs to deﬁne three SOS multipliers s01, s02, s03 (Lines\n20-22), corresponding to σ01, σ02, σ03 in (2.9), associated to I1, I2 and I3,\nrespectively.\nAs in the dense case, one needs a single SOS multiplier for each con-\nstraint but the support is more restricted. For instance, the SOS multiplier\nassociated to the polynomial (6.36 −x1)(4 −x1) (deﬁned in Line 13) de-\npends only on the variables x1, x4, related to the index subset I1 = {1, 4}.\nAt Line 28, Yalmip returns the objective value f 2\ncs = f 2 = 20.8608.\n1\nr = 2; % relaxation order\n2\nsdpvar x1 x2 x3 x4 x5 x6 b;\n3\nx = [x1; x2; x3; x4; x5; x6];\n4\nxI1 = [x1; x4];\n5\nxI2 = [x1; x2; x3; x5];\n6\nxI3 = [x1; x3; x5; x6];\n7\nf1 = -x1*x4;\n8\nf2 = -x1^2 + x1*x2 + x1* x3 - x2*x3 + x2*x5;\n9\nf3 = -x5*x6 + x1*x5 + x1*x6 + x3*x6;\n10\nf = f1 + f2 + f3; % objective function\n11\ng = (6.36 - x).*(x - 4); % the 6 polynomials used to define ...\nthe box constraints\n12\n13\n[s1,c1] = polynomial(xI1,2*r-2); % SOS multiplier of degree ...\n2 in the sparse Putinar's representation, depending only ...\non x1, x4\n14\n[s2,c2] = polynomial(xI2,2*r-2); % SOS multiplier of degree ...\n2 in the sparse Putinar's representation, depending only ...\non x1, x2, x3, x5\n\nA.2. Sparse SOS relaxations with Yalmip\n209\n15\n[s3,c3] = polynomial(xI2,2*r-2); % SOS multiplier of degree ...\n2 in the sparse Putinar's representation, depending only ...\non x1, x2, x3, x5\n16\n[s4,c4] = polynomial(xI1,2*r-2); % SOS multiplier of degree ...\n2 in the sparse Putinar's representation, depending only ...\non x1, x4\n17\n[s5,c5] = polynomial(xI2,2*r-2); % SOS multiplier of degree ...\n2 in the sparse Putinar's representation, depending only ...\non x1, x2, x3, x5\n18\n[s6,c6] = polynomial(xI3,2*r-2); % SOS multiplier of degree ...\n2 in the sparse Putinar's representation, depending only ...\non x1, x3, x5, x6\n19\n20\n[s01, c01] = polynomial(xI1,2*r); % SOS multiplier of degree ...\n4 in the sparse Putinar's representation, depending only ...\non x1, x4\n21\n[s02, c02] = polynomial(xI2,2*r); % SOS multiplier of degree ...\n4 in the sparse Putinar's representation, depending only ...\non x1, x2, x3, x5\n22\n[s03, c03] = polynomial(xI3,2*r); % SOS multiplier of degree ...\n4 in the sparse Putinar's representation, depending only ...\non x1, x3, x5, x6\n23\n24\ns = [s1 s2 s3 s4 s5 s6]; c = [b; c1; c2; c3; c4; c5; c6; ...\nc01; c02; c03];\n25\nF = [sos(s1), sos(s2), sos(s3), sos(s4), sos(s5), sos(s6), ...\nsos(s01), sos(s02), sos(s03)];\n26\n27\nF = [F, coefficients(f - b - s01 - s02 - s03 - s*g,x) == 0];\n28\nsolvesos(F,-b,[],c); % solves the SOS program: maximize b ...\nsuch that f - b = s01 + s02 + s03 + sum_i si gi\n\n\nAppendix B\nProgramming with Julia\nThe goal of this chapter is to present our Julia library, called TSSOS, which\naims at helping polynomial optimizers to solve large-scale problems with\nsparse input data. The underlying algorithmic framework is based on ex-\nploiting TS (see Chapter 7) as well as the combination of CS and TS (see\nChapter 8). As emphasized in the different chapters of Part II, TS can be\napplied to numerous problems ranging from power networks to eigen-\nvalue optimization of noncommutative polynomials, involving up to tens\nof thousands of variables and constraints. A complete documentation of\nthe TSSOS library is available at github:TSSOS. TSSOS depends on the fol-\nlowing Julia packages:\n• MultivariatePolynomials to manipulate multivariate polynomials;\n• JuMP [DHL17] to model the SDP problem;\n• Graphs [FBS+21] to handle graphs;\n• MetaGraphs to handle weighted graphs;\n• ChordalGraph [Wan20] to generate approximately smallest chordal\nextensions;\n• SemialgebraicSets to compute Gröbner bases.\nBesides, TSSOS requires an SDP solver, which can be MOSEK [ART03] or\nCOSMO [GCG19]. Once one of the SDP solvers has been installed, the instal-\nlation of TSSOS is straightforward:\nPkg.add(\"https://github.com/wangjie212/TSSOS\")\n\n212\nAppendix B. Programming with Julia\nB.1\nTSSOS for polynomial optimization\nTSSOS provides an easy way to deﬁne a POP and to solve it by sparsity-\nadapted SDP relaxations, including the relaxation Pr,s\nts given in (7.17) ex-\nploiting TS, the relaxation Pr\ncs given in (3.6) exploiting CS, as well as the re-\nlaxation Pr,s\ncs-ts given in (8.5) exploiting both CS and TS. The tunable param-\neters (e.g., the relaxation order r, the sparse order s, the types of chordal\nextensions) allow the user to ﬁnd the best compromise between the com-\nputational cost and the solution accuracy. The following Julia script is a\nsimple example to illustrate the usage of TSSOS.\n1 using TSSOS\n2 using DynamicPolynomials\n3 r = 2 /* relaxation order */\n4 @polyvar x[1:6]\n5 f = x[1]^4 + x[2]^4 - 2x[1]^2*x[2] - 2x[1] + 2x[2]*x[3] -\n2x[1]^2*x[3] - 2x[2]^2*x[3] - 2x[2]^2*x[4] - 2x[2] + 2x[1]^2 +\n2.5x[1]*x[2] - 2x[4] + 2x[1]*x[4] + 3x[2]^2 + 2x[2]*x[5] +\n2x[3]^2 + 2x[3]*x[4] + 2x[4]^2 + x[5]^2 - 2x[5] + 2 /* objective\nfunction */\n,→\n,→\n,→\n,→\n6 g = 1 - sum(x[1:2].^2) /* inequality constraints */\n7 h = 1 - sum(x[3:5].^2) /* equality constraints */\n8 nh = 1 /* number of equality constraints */\nTo solve the ﬁrst step of the TSSOS hierarchy with approximately smallest\nchordal extensions (option TS=\"MD\"), run\nopt,sol,data = tssos_first([f;g;h], x, r, numeq=nh, TS=\"MD\")\nWe obtain f 2,1\nts = 0.2096.\nTo solve higher steps of the TSSOS hierarchy, repeatedly run\nopt,sol,data = tssos_higher!(data, TS=\"MD\")\nFor instance, at the second step of the TSSOS hierarchy we obtain f 2,2\nts\n=\n0.2123.\nTo solve the ﬁrst step of the CS-TSSOS hierarchy, run\nopt,sol,data = cs_tssos_first([f;g;h], x, r, numeq=nh, TS=\"MD\")\nto obtain the lower bound f 2,1\ncs-ts = 0.2092.\nTo solve higher steps of the CS-TSSOS hierarchy, repeatedly run\n\nB.1. TSSOS for polynomial optimization\n213\nopt,sol,data = cs_tssos_higher!(data, TS=\"MD\")\nFor instance, at the second step of the CS-TSSOS hierarchy we obtain the\nlower bound f 2,2\ncs-ts = 0.2097.\nTSSOS also employs other techniques to gain more speed-up.\nBinary variables\nTSSOS supports binary variables. By setting nb = a, one can specify that\nthe ﬁrst a variables (i.e., x1, . . . , xa) are binary variables which satisfy the\nequations x2\ni = 1, i ∈[a]. The speciﬁcation is helpful to reduce the num-\nber of decision variables of SDP relaxations since one can identify xj with\nxj (mod 2) for a binary variable x.\nEquality constraints\nIf there are equality constraints in the description of POP (2.4), then one\ncan reduce the number of decision variables of SDP relaxations by working\nin the quotient ring R[x]/(h1, . . . , ht), where {h1 = 0, . . . , ht = 0} is the set\nof equality constraints. To conduct the elimination, we need to compute a\nGröbner basis GB of the ideal (h1, . . . , ht). Then any monomial xα can be\nreplaced by its normal form NF(xα, GB) with respect to the Gröbner basis\nGB when constructing SDP relaxations.\nAdding extra ﬁrst-order moment matrices\nWhen POP (2.4) is a QCQP, the ﬁrst-order moment-SOS relaxation is also\nknown as Shor’s relaxation. In this case, P1, P1\ncs and P1,1\nts yield the same\noptimum. To ensure that any higher order CS-TSSOS relaxation (i.e., Pr,s\ncs-ts\nwith r > 1) provides a tighter lower bound compared to the one given\nby Shor’s relaxation, we may add an extra ﬁrst-order moment matrix for\neach variable clique in Pr,s\ncs-ts. In TSSOS, this is accomplished by setting\nMomentOne = true.\nChordal extensions\nFor TS, TSSOS supports two types of chordal extensions: the maximal\nchordal extension (option TS=\"block\") and approximately smallest chordal\nextensions. TSSOS generates approximately smallest chordal extensions\nthrough two heuristics: the Minimum Degree heuristic (option TS=\"MD\")\nand the Minimum Fillin heuristic (option TS=\"MF\"). To use relaxations\nwithout TS, set TS = false. Similarly for CS there are options for the ﬁeld\n\n214\nAppendix B. Programming with Julia\n\n\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n\n\n−→\n\n\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n\n\nFigure B.1: Merge two 4 × 4 blocks into a single 5 × 5 block.\nCS which can be \"MF\" by default, or \"MD\", or \"NC\" (without chordal exten-\nsion), or false (without CS).\nSee [BK10] for a full description of the two chordal extension heuristics.\nThe Minimum Degree heuristic is slightly faster in practice, whereas the\nMinimum Fillin heuristic yields on average slightly smaller clique num-\nbers. Hence for CS, the Minimum Fillin heuristic is recommended and for\nTS, the Minimum Degree heuristic is recommended.\nMerging PSD blocks\nIn case that two PSD blocks have a large portion of overlaps, it might be\nbeneﬁcial to merge these two blocks into a single block for efﬁciency. See\nFigure B.1 for such an example. TSSOS supports PSD block merging in-\nspired by the strategy proposed in [GCG19]. To activate the merging pro-\ncess, one just needs to set the option Merge = True. The parameter md (= 3\nby default) can be used to tune the merging strength.\nRepresenting polynomials with supports and coefﬁcients\nThe Julia package DynamicPolynomials provides an efﬁcient way to de-\nﬁne polynomials symbolically. But for large-scale polynomial optimiza-\ntion (say, n > 500), it is more efﬁcient to represent polynomials by their\nsupports and coefﬁcients. For instance, we can represent f = x4\n1 + x4\n2 +\nx4\n3 + x1x2x3 in terms of its support and coefﬁcients as follows:\nsupp = [[1; 1; 1; 1], [2; 2; 2 ;2], [3; 3; 3; 3], [1; 2; 3]]\n/* support array of f */\ncoe = [1; 1; 1; 1] /* coefficient vector of f */\nThe above representation of polynomials is natively supported by TSSOS.\nHence the user can deﬁne the polynomial optimization problem directly\nby the support data and the coefﬁcient data to speed up the modeling pro-\ncess.\n\nB.2. TSSOS for noncommutative optimization\n215\nSOS + sparse + RIP ⇏sparse SOS\nTo ﬁnish this section, we provide a TSSOS script showing that the relax-\nations based on CS can be strictly more conservative than the dense ones,\neven when the RIP holds. Let f1 = x4\n1 + (x1x2 −1)2, f2 = x2\n2x2\n3 + (x2\n3 −1)2\nand f = f1 + f2. Here the RIP trivially holds as we have only two subsets\nof variables. After running the following commands:\n1 using TSSOS, DynamicPolynomials\n2 @polyvar x[1:3]\n3 f1 = x[1]^4 + (x[1]*x[2] - 1)^2\n4 f2 = x[2]^2*x[3]^2 + (x[3]^2 - 1)^2\n5 f = f1 + f2\n6 opt,sol,data = cs_tssos_first([f], x, 2, CS=false, TS=false)\n7 opt,sol,data = cs_tssos_first([f], x, 2, TS=false)\nwe obtain f 2\ncs = 0.0005 < 0.8498 = f 2. One can also compute lower\nbounds based on TS as follows:\n1 opt,sol,data = tssos_first([f], x, 2, TS=\"block\")\n2 opt,sol,data = tssos_higher!(data, TS=\"block\")\nto obtain f 2,1\nts = 0.0004 < 0.8498 = f 2,2\nts = f 2.\nB.2\nTSSOS for noncommutative optimization\nAs seen previously in Chapters 6 and 10, the whole framework of ex-\nploiting CS and TS for (commutative) polynomial optimization can be\nextended to handle noncommutative polynomial optimization (including\neigenvalue and trace optimization), which leads to the submodule NCTSSOS\nin TSSOS, available at github:NCTSSOS. The corresponding commands are\nsimilar to TSSOS. To illustrate the use of NCTSSOS, we consider the eigen-\nvalue minimization problem given in Example 6.22. After running the\nfollowing commands:\n1 @ncpolyvar x1 x2 x3 x4\n2 x = [x1; x2; x3; x4]\n3 f1 = 4 - x1 + 3 * x2 - 3 * x3 - 3 * x1^2 - 7 * x1 * x2 + 6 * x1 * x3\n- x2 * x1 -5 * x3 * x1 + 5 * x3 * x2 - 5 * x1^3 - 3 * x1^2 * x3\n+ 4 * x1 * x2 * x1 - 6 * x1 * x2 * x3 + 7 * x1 * x3 * x1 + 2 *\nx1 * x3 * x2 - x1 * x3^2 - x2 * x1^2 + 3 * x2 * x1 * x2 - x2 *\nx1 * x3 - 2 * x2^3 - 5 * x2^2 * x3 - 4 * x2 * x3^2 - 5 * x3 *\nx1^2 + 7 * x3 * x1 * x2 + 6 * x3 * x2 * x1 - 4 * x3 * x2 * x2 -\nx3^2 * x1 - 2 * x3^2 * x2 + 7 * x3^3\n,→\n,→\n,→\n,→\n,→\n,→\n\n216\nAppendix B. Programming with Julia\n4 f2 = -1 + 6 * x2 + 5 * x3 + 3 * x4 - 5 * x2^2 + 2 * x2 * x3 + 4 * x2\n* x4 - 4 * x3 * x2 + x3^2 - x3 * x4 + x4 * x2 - x4 * x3 + 2 *\nx4^2 - 7 * x2^3 + 4 * x2 * x3^2 + 5 * x2 * x3 * x4 - 7 * x2 * x4\n* x3 - 7 * x2 * x4^2 + x3 * x2^2 + 6 * x3 * x2 * x3 - 6 * x3 *\nx2 * x4 - 3 * x3^2 * x2 - 7 * x3^2 * x4 + 6 * x3 * x4 * x2\n- 3\n* x3 * x4 * x3 - 7 * x3 * x4^2 + 3 * x4 * x2^2 - 7 * x4 * x2 *\nx3 - x4 * x2 * x4 - 5 * x4 * x3^2 + 7 * x4 * x3 * x4 + 6 * x4^2\n* x2 - 4 * x4^3\n,→\n,→\n,→\n,→\n,→\n,→\n,→\n5 f = f1 + f2\n6 ncball = [1 - x1^2 - x2^2 - x3^2; 1 - x2^2 - x3^2 - x4^2]\n7 cs_nctssos_first([f; ncball], x, 2, CS=false, TS=false, obj=\"eigen\")\n8 cs_nctssos_first([f; ncball], x, 2, TS=false, obj=\"eigen\")\n9 cs_nctssos_first([f; ncball], x, 3, TS=false, obj=\"eigen\")\nwe obtain λ2\ncs( f, g) ≃−13.7680 < −13.7333 ≃λ3\ncs( f, g) ≃λ2( f, g) =\nλmin( f, g).\nOne can also compute lower bounds based on TS as follows:\n1 opt,data = nctssos_first([f; ncball], x, 2, TS=\"MD\", obj=\"eigen\")\n2 opt,data = nctssos_higher!(data, TS=\"MD\")\nAfter repeating the last command twice, we obtain λ2,1\nts ( f, g) ≃−14.0922,\nλ2,2\nts ( f, g) ≃−13.7618, λ2,3\nts ( f, g) ≃−13.7393, λ2,4\nts ( f, g) = λ2( f, g) =\nλmin( f, g) ≃−13.7333.\nNCTSSOS can also handle optimization over trace polynomials. To il-\nlustrate this, let us consider the two polynomial Bell inequalities given in\nChapter 10.4. For the ﬁrst one, we use the set of noncommutative variables\n(x1, x2, x3, x4) := (a1, a2, b1, b2), and so the objective function is\n(tr(a1b2 + a2b1))2 + (tr(a1b1 −a2b2))2\n= (tr(x1x4 + x2x3))2 + (tr(x1x3 −x2x4))2\n= (tr(x1x4))2 + (tr(x2x3))2 + 2 tr(x1x4) tr(x2x3)\n+ (tr(x1x3))2 + (tr(x2x4))2 + 2 tr(x1x3) tr(x2x4).\nAs seen earlier in the commutative setting, we represent trace polynomials\nby their supports and coefﬁcients. Note that we minimize the opposite\nexpression, so the resulting vector of coefﬁcients is [−1; −1; −2; −1; −1; 2]\n(see Line 4). The option constraint=\"unipotent\" (Line 5) allows us to\nencode the constraints x2\ni = 1 for i ∈[4]. The next script provides an\nupper bound for the maximization problem given in (10.18) at relaxation\norder r = 2 and sparse order s = 1 with maximal chordal extensions.\n1 n = 4\n2 r = 2\n\nB.3. TSSOS for dynamical systems\n217\n3 tr_supp = [[[1;4], [1;4]], [[2;3], [2;3]], [[1;4], [2;3]], [[1;3],\n[1;3]], [[2;4], [2;4]], [[1;3], [2;4]]]\n,→\n4 coe = [-1; -1; -2; -1; -1; 2]\n5 opt,data = ptraceopt_first(tr_supp, coe, n, r, TS=\"block\",\nconstraint=\"unipotent\")\n,→\nSimilarly, the next script provides an upper bound for the maximization\nproblem (10.20) at relaxation order r = 2 and sparse order s = 1 with\napproximately smallest chordal extensions.\n1 n = 6\n2 r = 2\n3 tr_supp = [[[1;4]], [[1], [4]], [[1;5]], [[1], [5]], [[1;6]], [[1],\n[6]], [[2;4]], [[2], [4]], [[2;5]], [[2], [5]], [[2;6]], [[2],\n[6]], [[3;4]], [[3], [4]], [[3;5]], [[3], [5]]]\n,→\n,→\n4 coe = [-1; 1; -1; 1; -1; 1; -1; 1; -1; 1; 1; -1; -1; 1; 1; -1]\n5 opt,data = ptraceopt_first(tr_supp, coe, n, r, TS=\"MD\",\nconstraint=\"unipotent\")\n,→\nTo obtain bounds at higher sparse orders, one should run repeatedly\nopt,data = ptraceopt_higher!(data, TS=\"block\")\nB.3\nTSSOS for dynamical systems\nSparseJSR is an efﬁcient tool to compute bounds on the JSR of a set of\nmatrices, based on the sparse SOS relaxations provided in Chapter 11. To\nuse it in Julia, run\nadd https://github.com/wangjie212/SparseJSR\nThe following simple example illustrates how to compute upper bounds:\n1 A = [[1 -1 0; -0.5 1 0; 1 1 0], [0.5 1 0; -1 1 0; -1 -0.5 0]]\n2 r = 2 /* relaxation order */\n3 ub = SparseJSR(A, r, TS = \"block\") /* computing an upper bound on\nthe JSR of A via sparse SOS */\n,→\n4 ub = JSR(A, r) /* computing an upper bound on the JSR of A via dense\nSOS */\n,→\n\n218\nAppendix B. Programming with Julia\nAs explained in Chapter 11.2, the JSR upper bounds are obtained via a\nbisection procedure. By default, the initial lower bound and the initial up-\nper bound for bisection are 0 and 2, respectively. The default tolerance is\n1e-5. The default sparse order is 1. One can also set TS=\"MD\" to use ap-\nproximately smallest chordal extensions, which is recommended for the\nrelaxation order r = 1. In addition, Gripenberg’s algorithm can be em-\nployed to produce a lower bound and an upper bound on the JSR with\ndifference at most δ.\n1 lb,ub = gripenberg(A, δ = 0.2)\nFinally, the SparseDynamicSystem library allows one to approximate re-\ngions of attraction, maximum positively invariant sets, global attractors\nfor polynomial dynamic systems via the TS-based moment-SOS hierar-\nchy. For more details, the reader is referred to the dedicated webpage:\ngithub:SparseDynamicSystem.\n\nBibliography\n[ART03]\nErling D Andersen, Cornelis Roos, and Tamas Terlaky.\nOn\nimplementing a primal-dual interior-point method for conic\nquadratic optimization. Mathematical Programming, 95(2):249–\n277, 2003.\n[BHL16]\nFlorian Bugarin, Didier Henrion, and Jean Bernard Lasserre.\nMinimizing the sum of many rational functions. Mathematical\nProgramming Computation, 8(1):83–111, 2016.\n[BK10]\nHans L Bodlaender and Arie MCA Koster. Treewidth computa-\ntions i. upper bounds. Information and Computation, 208(3):259–\n275, 2010.\n[DHL17] Iain Dunning, Joey Huchette, and Miles Lubin. Jump: A mod-\neling language for mathematical optimization.\nSIAM review,\n59(2):295–320, 2017.\n[FBS+21] James Fairbanks, Mathieu Besançon, Schölly Simon, Júlio Hof-\nﬁman, Nick Eubank, and Stefan Karpinski.\nJuliagraphs/-\ngraphs.jl: an optimized graphs package for the julia program-\nming language, 2021.\n[GCG19] Michael Garstka, Mark Cannon, and Paul Goulart. Cosmo: A\nconic operator splitting method for large convex problems. In\n2019 18th European Control Conference (ECC), pages 1951–1956.\nIEEE, 2019.\n[HLL09]\nD. Henrion, Jean-Bernard Lasserre, and J. Löfberg. GloptiPoly\n3: moments, optimization and semideﬁnite programming. Opti-\nmization Methods and Software, 24(4-5):pp. 761–779, August 2009.\n[Lö04]\nJ. Löfberg. Yalmip : A toolbox for modeling and optimization\nin MATLAB. In Proceedings of the CACSD Conference, Taipei, Tai-\nwan, 2004.\n\n220\nBibliography\n[Stu99]\nJos F Sturm. Using sedumi 1.02, a matlab toolbox for optimiza-\ntion over symmetric cones. Optimization methods and software,\n11(1-4):625–653, 1999.\n[Wan20]\nJie Wang. ChordalGraph: A Julia Package to Handle Chordal\nGraphs. 2020.",
    "pdf_filename": "Sparse Polynomial Optimization - Theory and Practice.pdf"
}