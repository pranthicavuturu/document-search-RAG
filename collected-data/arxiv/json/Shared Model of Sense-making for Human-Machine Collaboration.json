{
    "title": "Shared Model of Sense-making for Human-Machine Collaboration",
    "context": "We present a model of sense-making that greatly facilitates the collaboration between an intelligent analyst and a knowledge-based agent. It is a general model grounded in the science of evidence and the scientific method of hypothesis generation and testing, where sense-making hypotheses that explain an observation are generated, relevant evidence is then discovered, and the hypotheses are tested based on the discovered evidence. We illustrate how the model enables an analyst to directly instruct the agent to understand situations involving the possible production of weapons (e.g., chemical warfare agents) and how the agent becomes increasingly more competent in understanding other situations from that domain (e.g., possible production of centrifuge-enriched uranium or of stealth fighter aircraft). Sense-making is the intelligence analysis process of situation understanding, prediction of the behavior and intent of the entities of interest, and identifying the threats as early as possible, in the context of a dynamic world, based on data that is sparse, noisy, and uncertain (Moore, 2011). The prevailing approach to sense-making in intelligence analysis is the holistic approach where the analysts, after reviewing large amounts of information and performing the reasoning in their heads, reach a conclusion (Marrin, 2011). A complementary approach uses very simple structured analytic techniques, such as those described by Heuer and Pherson (2011), that provide general guidelines for hypothesis generation and testing. Most of the time sense- making is the result of shallow arguments using the Toulmin intuitive model (Toulmin 1963; van Gelder, 2007), where each claim is backed by evidence. There is no systematic process to determine the probabilities of hypotheses based on the available evidence (Pherson and Pherson, 2021). More advanced methods build Bayesian probabilistic inference networks using analytical tools, such as Netica (2019), but modeling a situation with a Bayesian network is a very complex task for an intelligence analyst. This paper presents a more advanced system for sense- making in intelligence analysis, the Multi-Agent System for Sensemaking through Hypothesis Generation and Analysis (MASH). MASH builds on a series of analytical tools that includes Disciple-LTA (Tecuci et al., 2008; Schum et al., 2009), TIACRITIS (Tecuci et al., 2011), Disciple-CD (Tecuci et al., 2016a) and Cogent (Tecuci et al., 2015; 2018). MASH also builds on the Disciple multistrategy apprenticeship learning approach (Boicu et al., 2001; Tecuci, 1988; 1998; Tecuci and Hieb, 1996; Tecuci et al., 2000; 2002; 2005; 2007a; 2019; Huang et al., 2020). Shared Model of Sense-making Figure 1 is an overview of a human-machine shared model of sense-making that facilitates the synergistic integration of the analyst’s imagination and expertise with the computer’s domain knowledge and formal reasoning. It is a general model grounded in the science of evidence (Schum, 2009) and the scientific method of hypothesis generation and testing. Evidence is any observable sign, datum, or item of information that is relevant in deciding whether a hypothesis is true or false (Schum, 2009). The sense-making model consists of three recursive collaborative processes: Evidence in search of hypotheses; Hypotheses in search of evidence; and Evidentiary assessment of hypotheses. The sense-making process starts with an “alerting observation” that may indicate an event of interest. Through abductive (imaginative) reasoning which shows that something is possibly true, the analyst and MASH generate competing hypotheses that may explain the observation (Peirce, 1955; Eco, 1983; Schum, 2001a; Langley, 2019). To determine which of these competing hypotheses is true, they use each hypothesis and deductive reasoning which shows that something is necessarily true, to discover new evidence. The question is: What evidence would need to be observed if this hypothesis were true? The reasoning might go as follows: If H were true then the sub-hypotheses H1, H2, and H3 would also need to be true. But if H1 were true then one would need to observe evidence E1, and so on. A broader question that guides the discovery of evidence is, What evidence would favor or disfavor hypothesis H? The decomposition of H is done through a sequence of  Figure 1: Human-machine shared model of sense-making. Probability of Hypotheses New Evidence Evidence in search of hypotheses Abduction E possibly H Evidentiary assessment of hypotheses Induction E probably H Hypotheses in search of evidence Deduction H necessarily E Competing Hypotheses Alerting Observation What is the probability of each hypothesis? What evidence would favor or disfavor this hypothesis? What hypotheses would explain this observation?",
    "body": "In the Proceedings of the 2021 AAAI Fall Symposium “Cognitive Assistance in \nGovernment and Public Sector Applications”, Arlington, VA, November 4-6 \n1 \n \nShared Model of Sense-making for Human-Machine Collaboration \nGheorghe Tecuci, Dorin Marcu, Louis Kaiser, Mihai Boicu \nLearning Agents Center, School of Computing, George Mason University, Fairfax, VA 22030, USA \n{ tecuci; dmarcu; lkaiser4; mboicu }@gmu.edu \nAbstract \nWe present a model of sense-making that greatly facilitates \nthe collaboration between an intelligent analyst and a \nknowledge-based agent. It is a general model grounded in the \nscience of evidence and the scientific method of hypothesis \ngeneration and testing, where sense-making hypotheses that \nexplain an observation are generated, relevant evidence is \nthen discovered, and the hypotheses are tested based on the \ndiscovered evidence. We illustrate how the model enables an \nanalyst to directly instruct the agent to understand situations \ninvolving the possible production of weapons (e.g., chemical \nwarfare agents) and how the agent becomes increasingly \nmore competent in understanding other situations from that \ndomain (e.g., possible production of centrifuge-enriched \nuranium or of stealth fighter aircraft).   \nIntroduction \nSense-making is the intelligence analysis process of \nsituation understanding, prediction of the behavior and \nintent of the entities of interest, and identifying the threats \nas early as possible, in the context of a dynamic world, based \non data that is sparse, noisy, and uncertain (Moore, 2011).  \n The prevailing approach to sense-making in intelligence \nanalysis is the holistic approach where the analysts, after \nreviewing large amounts of information and performing the \nreasoning in their heads, reach a conclusion (Marrin, 2011). \n A complementary approach uses very simple structured \nanalytic techniques, such as those described by Heuer and \nPherson (2011), that provide general guidelines for \nhypothesis generation and testing. Most of the time sense-\nmaking is the result of shallow arguments using the Toulmin \nintuitive model (Toulmin 1963; van Gelder, 2007), where \neach claim is backed by evidence. There is no systematic \nprocess to determine the probabilities of hypotheses based \non the available evidence (Pherson and Pherson, 2021). \nMore advanced methods build Bayesian probabilistic \ninference networks using analytical tools, such as Netica \n(2019), but modeling a situation with a Bayesian network is \na very complex task for an intelligence analyst.  \n This paper presents a more advanced system for sense-\nmaking in intelligence analysis, the Multi-Agent System for \nSensemaking through Hypothesis Generation and Analysis \n(MASH). MASH builds on a series of analytical tools that \nincludes Disciple-LTA (Tecuci et al., 2008; Schum et al., \n2009), TIACRITIS (Tecuci et al., 2011), Disciple-CD \n(Tecuci et al., 2016a) and Cogent (Tecuci et al., 2015; 2018). \nMASH also builds on the Disciple multistrategy \napprenticeship learning approach (Boicu et al., 2001; \nTecuci, 1988; 1998; Tecuci and Hieb, 1996; Tecuci et al., \n2000; 2002; 2005; 2007a; 2019; Huang et al., 2020). \nShared Model of Sense-making \nFigure 1 is an overview of a human-machine shared model \nof sense-making that facilitates the synergistic integration of \nthe analyst’s imagination and expertise with the computer’s \ndomain knowledge and formal reasoning. It is a general \nmodel grounded in the science of evidence (Schum, 2009) \nand the scientific method of hypothesis generation and \ntesting. Evidence is any observable sign, datum, or item of \ninformation that is relevant in deciding whether a hypothesis \nis true or false (Schum, 2009). The sense-making model \nconsists of three recursive collaborative processes: Evidence \nin search of hypotheses; Hypotheses in search of evidence; \nand Evidentiary assessment of hypotheses.  \n The sense-making process starts with an “alerting \nobservation” that may indicate an event of interest. Through \nabductive (imaginative) reasoning which shows that \nsomething is possibly true, the analyst and MASH generate \ncompeting hypotheses that may explain the observation \n(Peirce, 1955; Eco, 1983; Schum, 2001a; Langley, 2019). \n To determine which of these competing hypotheses is \ntrue, they use each hypothesis and deductive reasoning \nwhich shows that something is necessarily true, to discover \nnew evidence. The question is: What evidence would need \nto be observed if this hypothesis were true? The reasoning \nmight go as follows: If H were true then the sub-hypotheses \nH1, H2, and H3 would also need to be true. But if H1 were \ntrue then one would need to observe evidence E1, and so on. \n A broader question that guides the discovery of evidence \nis, What evidence would favor or disfavor hypothesis H? \nThe decomposition of H is done through a sequence of \n \nFigure 1: Human-machine shared model of sense-making. \nProbability of Hypotheses\nNew Evidence\nEvidence in search\nof hypotheses\nAbduction\nE possibly H\nEvidentiary assessment\nof hypotheses\nInduction\nE probably H\nHypotheses in\nsearch of evidence\nDeduction\nH necessarily E\nCompeting Hypotheses\nAlerting Observation\nWhat is the \nprobability of \neach hypothesis?\nWhat evidence\nwould favor or\ndisfavor this hypothesis?\nWhat hypotheses \nwould explain\nthis observation?     \n\n \n2 \n \nfavoring and disfavoring arguments. These arguments will \nend in evidence collection requests that will return evidence \nto test the top hypothesis. \n Once the evidence is discovered, the analyst and/or \nMASH use inductive reasoning which shows that something \nis probably true, to test each hypothesis. They employ the \nWigmorean probabilistic inference network developed \nduring evidence collection, where the probabilities of the \nbottom hypotheses are assessed based on the collected \nevidence, and the probabilities of the upper level hypotheses \nare assessed based on the probabilities of their \nsubhypotheses (Wigmore 1937; Schum, 2001b; Tecuci et \nal., 2016a). These Wigmorean networks naturally integrate \nlogic and Baconian probability (Cohen, 1977; 1989) with \nFuzzy qualifiers (Negoita and Ralescu, 1975; Zadeh, 1983), \nsuch as “barely likely,” “likely,” or “almost certain,” being \nable to deal with all the five characteristics of evidence, \nnamely \nincompleteness, \ninconclusiveness, \nambiguity, \ndissonance, and credibility level (Schum, 2001b; Tecuci et \nal., 2016b, pp. 159-172). This integrated logic and \nprobability \nsystem \nuses \nthe \nmin/max \nprobability \ncombination rules common to the Baconian and the Fuzzy \nprobability views. These rules are much simpler than the \nBayesian probability combination rule, which is important \nfor the human understandability of the analysis. \n Evidence in search of hypotheses, hypotheses in search of \nevidence, and evidentiary assessment of hypotheses are \ncollaborative processes that support each other in recursive \ncalls, as shown in the bottom part of Figure 1. For example, \nthe discovery of new evidence may lead to the modification \nof the existing hypotheses or the generation of new ones \nthat, in turn, lead to the search and discovery of new \nevidence. Also, inconclusive testing of the hypotheses leads \nto the need of discovering additional evidence. \nMulti-Agent System Architecture \nWe have developed MASH as a proof of concept multi-\nagent system that an analyst can instruct to perform sense-\nmaking, as a teacher would instruct a student, through a \nprocess that is significantly easier and faster than the typical \nknowledge engineering approach where the agent is \ndeveloped by a knowledge engineer who acquires the \nknowledge from the analyst and encodes it into the agent’s \nknowledge base. Figure 2 shows the overall architecture of \nMASH, for both its training and its use.  \n First the analyst demonstrates to the Mixed-Initiative \nLearning and Reasoning Assistant how to determine \nwhether certain activities of interest are taking place. As a \nresult the agent learns general reasoning rules that are stored \nin the Knowledge Base. The Autonomous Multi-Agent \nSystem uses this knowledge base to automatically reason \nabout other situations as the analyst would. The MASH-\ngenerated analysis is reviewed and possibly revised by the \nanalyst. As a result, MASH refines the previously learned \nrules and learns additional ones, becoming increasingly \nmore competent. \n The process of teaching and using the system is \nsummarized in Figure 3, and illustrated in the next sections \nthat will show how MASH will be instructed to \nautomatically recognize when a country is producing a \ncertain type of weapon or weapons-related material, such as \nchemical \nwarfare \nagents, \ncentrifuge-\nenriched uranium, or \nstealth \nfighter \naircraft. \n Another important \ncomponent \nof \nthe \narchitecture is the \nSimulated \nISR \nEnvironment that not \nonly \nenables \nthe \ntesting of automatic \nsense-making \nbut \nalso facilitates the \ntransition to real data \nsources and real environments. \nDemonstration of Sensemaking  \nAgent instruction starts by demonstrating to MASH how to \nanalyze a specific scenario, such as the one in Table 1. \nTable 1. The Bogustan scenario. \nThe country Bogustan was building a new chemical plant at Tanan \nthat was nearing completion; the plant’s purpose was not known. \nBogustan was suspected of harboring weapons of mass destruction \nambitions. A reconnaissance asset conducting a routine quarterly \noverflight detected heat signatures at the Tanan facility on \n2/25/2020.   \n \n \nFigure 2. The overall architecture of MASH. \nCollection and\nMonitoring \nManager\nCollection\nAgent\nCollection\nAgent\nCollection\nAgents\nSurveillance\nAgent\nSurveillance\nAgent\nSurveillance\nAgents\nSimulated ISR \nEnvironment\n(Sensors and Sources)\nSurveillance\nManager\nKnowledge Base\nMixed-Initiative Learning \nand Reasoning Assistant\nAutonomous \nMulti-Agent System\nEvidence\nAgent\nHypothesis \nGeneration Agent\nHypothesis \nAnalysis Agent\nAlert\nAgent\nNew Evidence\nHypotheses\nAlert \nProbabilities\nLearning\nto generate \nhypotheses\nLearning\nto discover and \nmonitor evidence\nLearning \nto assess \nhypotheses \n \nFigure 3. Teaching and using \nMASH in a new domain. \nManual Analysis of Initial Scenario A\nOntology Development\nSemi-automatic Rule Learning\nAutomatic Analysis of \nScenarios Similar to A\nScenario Representation\nSemi-Automatic Analysis \nof a Novel Scenario B\nSemi-automatic\nAnalysis and Learning\nAutomatic Analysis of \nScenarios Similar to A or B\n\n \n3 \n \nMASH supports the analyst in developing the analysis for \nanswering the question: \nIs Bogustan producing Tanan chemical-warfare agents at \nTanan chemical plant as of 2/25/2020?   \nFigure 4 shows the system’s interface for hypothesis \nanalysis. The upper left-hand side pane is the Whiteboard \narea where the analysis is constructed. The upper right-hand \nside pane is the Assistants area that includes several \nassistants, each helping the user perform a group of related \noperations. The currently selected one is the Evidence \nassistant that lists the current evidence items. The analyst \nclicks on an item, then drags and drops it on the relevant \nhypothesis, either on the green square (if the evidence item \nfavors the truthfulness of the hypothesis) or on the pink \nsquare (if it disfavors its truthfulness). The analyst then \ndouble-clicks on the NS (Not Set) values of relevance and \ncredibility, and selects the corresponding probability values \nfrom the displayed list. For example, the analyst selected BL \n(Barely Likely, 50-55%) for the relevance of E25 Drone (A \ncollection drone operating near Tanan did not detect any \nchemical warfare agents on 1/15/2020) because the sensor \nreported this information a month ago. The information is \nsomewhat dated. More current information would have a \nhigher relevance.  \n As shown in Figure 4, the analyst considered two \ncompeting hypotheses: \nBogustan is producing Tanan chemical-warfare agents at \nTanan chemical plant as of 2/25/2020. \nBogustan is not yet producing Tanan chemical-warfare \nagents at the Tanan chemical plant as of 2/25/2020. \nThere are two favoring arguments that would support a \nconclusion that Bogustan is producing chemical warfare \nagents: \nBogustan has the intent to produce chemical warfare \nagents and the expertise and materials to do so, and the \nplant at Tanan is complete and was built to produce such \nagents. \nChemical warfare agents have been detected in the \nvicinity of the plant. \nThere is also a disfavoring argument: \nSource reporting stating that this is not the case. \nThe analysis fragment that is visible in the whiteboard from \nFigure 4 is a small part of the entire analysis. MASH \nsupports the analyst in developing a comprehensive, \ndefensible, and persuasive analysis by: \n• Making reasoned judgments based on all the available \ninformation; \n• Rigorously considering favoring and disfavoring \nevidence in the context of large and complex \narguments; \n• Distinguishing between circumstantial evidence and \n \nFigure 4. The system’s interface for hypothesis analysis. \n\n \n4 \n \nmore conclusive (direct) evidence; \n• Taking into account the credibility and \nrelevance of evidence; \n• Responding to new information without \nstarting over, while minimizing the \npotential for certain cognitive biases to \ndismiss \nor \namplify \nthe \nanalytic \nimportance of the new information. \nTo enable MASH to automatically discover \nrelevant evidence, the analyst needs to insert \nevidence collection requests under each \nhypothesis that may have evidence that \ndirectly supports that hypothesis. This is \ndone through the simple process illustrated in Figure 5 The \nanalyst right-clicks on the hypothesis “Several areas of the \nTanan chemical plant are emitting heat as of 2/25/2020” and \nselects “Add Collection Task.” MASH generates the \ncollection task pattern “Collect evidence from <collection \nagent> using <function> to determine whether Several areas \nof the Tanan chemical plant are emitting heat as of \n2/25/2020” that the analyst needs to concretize by selecting \nthe collection agent (e.g., thermal imagery sensor) and its \ncollection function (i.e., heat detection).  \nAnalysis-Driven Ontology Development \nThe developed analysis of Bogustan answered the question: \nIs Bogustan producing Tanan chemical-warfare agents at \nTanan chemical plant as of 2/25/2020? \nFrom this analysis MASH learns general rules that will \nenable it to automatically generate analyses for answering \nquestions of the type \nIs country producing weapons-related product at plant as \nof date?  \nin other scenarios, such as \nIs Wokistan producing Wokistan chemical-warfare \nagents at Bandar chemical plant as of 3/12/2020? \nFor this, however, it needs an ontology specifying the \nconcepts to which instances (entities) from the question, \nsuch as Bogustan, may be generalized (e.g., to country). \nThis has to be done for all the entities appearing in the \nanalysis. The entire ontology for the Bogustan scenario is \nshown in Figure 6. Notice that it also contains the necessary \nrelationships between the various instances, for example \n \nFigure 5. Adding evidence collection tasks. \n \nFigure 6. The ontology for the Bogustan scenario. \nchemical weapon\nchemical \nwarfare agent\nTanan chemical-\nwarfare agents\nchemical plant \nTanan \nchemical plant \nchemical precursor\nBogustan chemical \nprecursors\ntanker truck\nBogustan \ntanker truck\nchemical weapons program\nBogustan chemical \nweapons program\nBogustan\nHalifaza\nBogustan chemical \nweapons department\nchemical weapons \ndepartment\nproduct\nplant\norganization\nobject\noutgoing \ntransportation utility\nweapon-related \nproduct\nmaterial\ndevelopment program\naction\nactor\ncountry\nconventional \nweapons-\nrelated product\nWMD-related \nproduct\nweapons-\nrelated plant\nconventional weapons \ndevelopment program\nWMD development \nprogram\nweapons development program\ngovernment \ndepartment\nweapons-related \ndepartment\nuses as material\nmay have as product\nuses for transportation\nbelongs to\nmay have as department\nhas as enemy\nmay have as development program\n\n \n5 \n \nthat Tanan chemical plant belongs to Bogustan. In all, there \nare only eight instances, seven relationships, and 26 \nconcepts that are shown in Figure 6.  \n The ontology is developed using the Ontology editor. The \nontology language is an extension of RDFS (Allemang et \nal., 2020; W3C, 2004) with additional features to facilitate \nlearning and evidence representation (Tecuci et a., 2016b). \nRule Learning \nAutomatic Rule Learning \nAll the 97 analysis rules corresponding to the developed \nBogustan analysis are learned with a single click on “Learn \nAll” from the drop-down menu invoked with a right-click \non the intelligence question, as shown in Figure 7. \nEach rule is an ontology-based generalization of an \nargument for or against a hypothesis. \nMixed-Initiative Rule Refinement \nThe analyst uses the Rule Analysis assistant to \nidentify the arguments containing new instances in \nsub-hypotheses whose presence need to be \nexplained by connecting them with instances from \nthe \ntop \nhypothesis, \nusing \nmixed-initiative \ninteraction (Tecuci et al., 2007b). For example, the \nleft-hand side of Figure 8 shows such an argument \nwhere its sub-hypothesis contains the new instance \nHalifaza. The learned rule is shown in the middle of \nFigure 8. Notice that this rule contains the variable ?O3 \n(generated for Halifaza) in the sub-hypothesis, whose value \nis not restricted in any way by the values of the variables in \nthe top hypothesis. Therefore, when applying this rule, \nMASH can instantiate ?O3 with any country or actor. \n When the analyst double-clicks on this argument in the \nRule Analysis assistant, MASH selects the argument in the \nWhiteboard and displays the instance that needs to be \nexplained (Halifaza) together with the possible explanation \n(Bogustan has as enemy Halifaza) in the Explanations \nbrowser of the Learning assistant, as shown in Figure 9.  \nThe analyst selects the explanation by right-clicking on the \nfeature has as enemy and selecting “Accept.” As a result, \nMASH refines the rule from the middle of Figure 8 as \n \nFigure 8. Rule learning and refinement. \n \nFigure 7. With a single right-click on the intelligence question, MASH automatically learns all the analysis rules. \n\n \n6 \n \nindicated in the right hand side of the figure. Now, MASH \nwill only apply the refined rule when it can instantiate ?O1 \nwith a country or actor that has as enemy the country or actor \ninstantiating ?O3.  \nOnly nine out of the 97 learned rules needed to be refined. \nAutomatic Analysis of Similar Scenarios \nThe rules learned from the Bogustan scenario enable the \nsystem to automatically analyze similar scenarios, such as \nthe Wokistan scenario from Table 2. \n First the analyst uses the Ontology editor to represent the \nWokistan scenario. This is similar to the Bogustan scenario \nin Figure 6, where Bogustan, Tanan, and Halifaza are \nreplaced with Wokistan, Bandar, and Valeria, respectively.  \n Then, with a single click on “Solve” from the drop-down \nmenu invoked with a right-click on the intelligence \nquestion, MASH automatically generates the entire \nWokistan analysis in the Whiteboard area. \nTable 2. The Wokistan scenario. \nThe country Wokistan was building a new chemical plant at \nBandar that was nearing completion; the plant’s purpose was not \nknown.  Wokistan was suspected of harboring weapons of mass \ndestruction ambitions. A reconnaissance asset conducting a routine \nquarterly overflight detected heat signatures at the Bandar facility \non 3/12/2020. \nSemi-Automatic Analysis of a Novel Scenario \nThe rules learned from the Bogustan scenario enable the \nsystem to also generate the analysis for a novel scenario, \nsuch as the Shamland scenario from Table 3 on the \nproduction of centrifuge-enriched uranium.  \n The analyst uses the Ontology editor to represent the \nShamland scenario shown in Figure 10. This is similar to the \nBogustan scenario, where Bogustan, Tanan and Halifaza are \nreplaced with Shamland, Destructville and Agressia. Also, \nthe types of objects are correspondingly updated (e.g., \nchemical plant is replaced with centrifuge-enriched uranium \nplant) and Shamland uses two types of critical material \ninputs, instead of one. Then, with a single click on “Solve” \nMASH automatically generates the entire Shamland \nanalysis in the Whiteboard area.  \n The analyst browses the automatically generated analysis \nand can easily modify it where necessary. One update of the \nShamland analysis was the addition of an incentive for \nenriching uranium: economic reasons (electricity shortages \nand the need of enriched uranium for nuclear power plants).  \nOnly 12 new rules where needed to correctly assess the \nproduction of centrifuge-enriched uranium. \nTable 3. The Shamland scenario. \nThe country Shamland was building a large plant at Destructville, \nwhose purpose was not known. Shamland was suspected of \nwanting to develop nuclear weapons. A reconnaissance asset \nconducting a routine quarterly overflight detected heat at the \nDestructville facility on 5/2/2020.   \nContinued Increase of System’s Competence  \nWith every novel scenario, the system learns a few \nadditional rules, continuously increasing its competence \nacross a broader spectrum of applications. Over time, the \nsystem’s efficiency also steadily improves because the \nanalyst’s effort in addressing new scenarios will \ncorrespondingly decrease as the system learns how to \nreasons about the production of new weapons. \nSemi-Automatic Analysis of Another Novel Scenario \nTable 4 represents the novel scenario of a country suspected \nof producing stealth fighter aircraft. The semantic \nrepresentation of this scenario is similar to that in Figure 6, \nwhere Bogustan, Tanan, and \nHalifaza are replaced with \nViolenta, Bemoana, and \nSmoldera, \nrespectively.  \nAlso, the types of involved \nentities are correspondingly \nupdated \n(e.g., \nchemical \nplant \nis \nreplaced \nwith \naircraft plant) and Violenta \ndoes \nnot \nuse \nany \ntransportation \nutility \nbecause a produced aircraft \ncan move itself. \n \nFigure 9. Mixed-initiative explanation of an argument. \n \nFigure 10. The semantic representation of the Shamland scenario. \nShamland\nAgressia\nmay have as product\nuses for \ntransportation\nbelongs to\nhas as enemy\nmay have as \ndevelopment program\nnuclear program\nShamland nuclear \nprogram\ncentrifuge-enriched \nuranium plant \nDestructville centrifuge-\nenriched uranium plant \nspecial container\nShamland special \ncontainer\nuranium special \ncontainer\nenriched uranium\nDestructville centrifuge-\nenriched uranium\nspecialty steel\nShamland specialty \nsteel for centrifuge-\nenriched uranium\nspecialty steel for \ncentrifuge-enriched \nuranium\nShamland uranium ore\nuranium ore\nuses as material\nShamland chemical \ndepartment\nchemical weapons \ndepartment\nmay have as department\n\n \n7 \n \nTable 4. The Violenta scenario. \nDuring the past four decades, several conflicts have occurred \nbetween Violenta and Smoldera in which air power has played an \nincreasingly important role. In the last conflict two years ago, \nSmoldera’s air force, after destroying Violenta’s air force, \nextensively bombed Violenta’s armored units and forced Violenta \nto make major border concessions. On 6/10/2020 several areas of \na new plant that Violenta was building at Bemoana were emitting \nheat signatures. Is Violenta producing stealth fighter aircraft at \nBemoana? \n \nThen, with a single click on “Solve” MASH automatically \ngenerates the entire Violenta analysis in the Whiteboard \narea. The analyst browses the automatically generated \nanalysis and can modify it where necessary. One update was \nthe addition of two new areas of expertise needed to \nmanufacture stealth aircraft. Only 20 new rules where \nneeded to instruct MASH to assess the production of stealth \nfighter aircraft. \nAutomatic Analysis of a Novel Scenario \nTable 5 represents the novel scenario of a country suspected \nof producing long-range stealth bomber aircraft. \nTable 5. The Malicia scenario. \nGoodlanda and Malicia are nuclear-weapon powers who vie for \nglobal influence and see each other as arch enemies. Both have \nlong-range strategic bombers that can travel the 6,000-mile \ndistance that separates Goodlanda and Malicia. The bombers, \nhowever, are easily detected by long-range radars positioned along \nthe periphery of both countries that provide ample early warning \nof a possible attack. On 7/15/2020 several areas of a new plant that \nMalicia was building at Tirinta were emitting heat. Is Malicia \nproducing long-range stealth bombers at Tirinta? \n \nThe semantic representation of this scenario is similar to the \nViolenta scenario, where Violenta, Bemoana, Smoldera, and \nstealth fighter aircraft, are replaced with Malicia, Tirinta, \nand Goodlanda, and long-range stealth bomber aircraft \nrespectively.  \n Then, with a single click on “Solve” MASH \nautomatically generates the entire Malicia analysis in the \nWhiteboard area. This analysis was complete and correct, \nno adaptation being necessary. \nCognitive Augmentation \nAs discussed in the previous sections, by using MASH the \nanalyst follows a systematic analysis process that \nsynergistically integrates the user’s imaginative reasoning \nand expertise with the agent’s formal reasoning and learned \nexpertise. For example, the analyst imagines the questions \nto ask and hypothesizes possible answers. MASH helps with \ndeveloping the arguments by reusing previously learned \nrules, and guides the evidence collection. The jointly-\ndeveloped analysis makes very clear the logic, what \nevidence was used and how, what is not known, and what \nassumptions have been made. It can be shared with other \nusers, subjected to critical review, and correspondingly \nimproved. As a result, this systematic process leads to the \ndevelopment of defensible and persuasive conclusions. \nMASH also enables rapid analysis, not only through the \nreuse of patterns, but also through a drill-down process \nwhere a hypothesis may be decomposed to different levels \nof detail, depending on the available time. It facilitates the \nanalysis of what-if scenarios, where the user may make \nvarious assumptions and the assistant automatically \ndetermines their influence on the analytic conclusion. The \nassistant also makes possible the rapid updating of the \nanalysis based on new (or revised) evidence from monitored \nsources, and assumptions. \nTrust in the Machine-Generated Analysis \nMachine-generated analyses are similar to the ones \ndeveloped by the analyst. The transparency and defensibility \nof the developed analysis facilitate its review by the analyst. \nThe probabilistic assessments are based on the simple min-\nmax rules common to the Fuzzy and Baconian systems that \nare intuitive and easy to understand. Moreover, the analyst \nmay review each argument from the analysis, may accept, \nmay revise, or may even reject it. MASH uses the analyst’s \nfeedback to further improve the previously learned rules and \nto learn new ones, to replicate better and better the reasoning \nof the analyst, and thus insuring increased trust in the \ngenerated analyses. \nConclusions and Future Research \nThis paper showed how a shared model of sense-making \nfacilitated the synergistic integration of the analyst’s \nimagination and expertise with the computer’s knowledge \nand critical reasoning.  \n But there are more opportunities for human-machine \ncollaboration with respect to this model, including: mixed-\ninitiative ontology learning, fully-automatic rule learning, \ndeep sense-making through iterative (multi-step) abduction, \ndeduction and induction; advanced analytics (e.g., detection \nand mitigation of cognitive biases, automatic identification \nof key evidence and assumptions); natural language \ninteraction, and automatic evidence collection using edge \nprocessing with convolutional neural networks.  \n\n \n8 \n \nReferences \nAllemang, D., Hendler, J., Gandon, F. (2020). Semantic Web for \nthe Working Ontologist: Effective Modeling for Linked Data, \nRDFS and OWL, Morgan Kaufmann Publishers. \nBoicu, M., Tecuci, G., Stanescu, B, Marcu, D., Cascaval, C.E. \n(2001). Automatic Knowledge Acquisition from Subject Matter \nExperts, in Proc. of the 13th Int. Conf. on Tools with Artificial \nIntelligence (ICTAI), pp. 69-78, November 7-9, Dallas, TX. \nCohen, L.J. (1977). The Probable and the Provable, Clarendon \nPress, Oxford. \nCohen, L.J. (1989). An Introduction to the Philosophy of Induction \nand Probability, Clarendon Press, Oxford. \nEco, U. (1983). Horns, Hooves, Insteps: Some Hypotheses on \nThree Types of Abduction, in Eco, U. and Sebeok, T. (eds.), The \nSign of Three: Dupin, Holmes, Peirce, pp. 198-220, Indiana \nUniversity Press, Bloomington, IN. \nHeuer, R.J. and Pherson, R.H. (2011). Structured Analytic \nTechniques for Intelligence Analysis, CQ Press, Washington, DC. \nHuang, J., An, Z., Meckl, S., Tecuci, G., Marcu, D. \n(2020), Complementary Approaches to Instructable Agents for \nAdvanced Persistent Threats Detection, Studies in Informatics and \nControl, 29 (3). \nLangley, P. (2019). Heuristic Construction of Explanations \nThrough Associative Abduction, in Proc. of the 7th Annual Conf. \non Advances in Cognitive Systems, Technical Report Number \nCOLAB2-TR-4, pp. 21-36, August 2-5, MIT, Cambridge, MA. \nMarrin, S. (2011). Improving Intelligence Analysis: Bridging the \ngap between scholarship and practice, Routlege, London & New \nYork. \nMoore, D.T. (2011). Sensemaking: A Structure for Intelligence \nRevolution, NDIC Press. \nNegoita, C.V. and Ralescu, D.A. (1975). Applications of Fuzzy Sets \nto Systems Analysis, Wiley, New York, NY. \nNetica (2019). https://www.norsys.com/. Accessed on 08/08/2021. \nPeirce, C.S. (1901). Abduction and Induction, in Buchler, J. (ed.) \n(1955). Philosophical Writings of Peirce, pp. 150-156, Dover, \nNew York, NY. \nPherson, K.H. and Pherson, R.H. (2021). Critical Thinking for \nStrategic Intelligence, SAGE Publishing. \nSchum, D.A. (2009). Science of Evidence: Contributions from \nLaw and Probability, Law, Probab. and Risk, 8, pp. 197-231. \nSchum, D.A. (2001a). Species of Abductive Reasoning in Fact \nInvestigation in Law, Cardozo Law Rev., 22 (5-6), pp. 1645-1681. \nSchum, D.A. (2001b). The Evidential Foundations of Probabilistic \nReasoning, Northwestern University Press. \nSchum, D., Tecuci, G., Boicu, M. (2009). Analyzing Evidence and \nIts Chain of Custody: A Mixed-Initiative Computational \nApproach, \nInternational \nJournal \nof \nIntelligence \nand \nCounterIntelligence, 22 (2), pp. 298-319. \nTecuci, G. (1988). DISCIPLE: A Theory, Methodology and \nSystem for Learning Expert Knowledge, These de Docteur en \nScience (in English), 197 pages, University of Paris-South, France. \nTecuci, G. (1998). Building Intelligent Agents: An Apprenticeship \nMultistrategy Learning Theory, Methodology, Tool and Case \nStudies, Academic Press, San Diego, CA. \nTecuci G. and Hieb M.H (1996). Teaching Intelligent Agents: The \nDisciple Approach, International Journal of Human-Computer \nInteraction, 8(3), pp.259-285 \nTecuci, G., Boicu, M., Bowman, M., Marcu, D., Shyr, P., Cascaval, \nC. (2000). An Experiment in Agent Teaching by Subject Matter \nExperts, International Journal of Human-Computer Studies, 53, \npp. 583-610. \nTecuci, G., Boicu, M., Marcu, D., Stanescu, B., Boicu, C., \nComello, J., Lopez, A., Donlon, J., Cleckner W. (2002). \nDevelopment and Deployment of a Disciple Agent for Center of \nGravity Analysis, in Proc. of the 18th National Conf. of Artificial \nIntelligence and the 14th Conf. on Innovative Applications of \nArtificial Intelligence, pp. 853-860, Edmonton, Alberta. \nTecuci, G., Boicu, M., Boicu, C., Marcu, D., Stanescu, B., \nBarbulescu, M. \n(2005). The \nDisciple-RKF \nLearning \nand \nReasoning Agent, Computational Intelligence, 21, pp. 462-479.  \nTecuci, G., Boicu, M., Marcu, D., Boicu, C., Barbulescu, M., \nAyers, C., Cammons, D. (2007a). Cognitive Assistants for \nAnalysts, Journal of Intelligence Community Research and \nDevelopment (JICRD). Also in Auger, J. and Wimbish, W.  (eds.), \nProteus Futures Digest: A Compilation of Selected Works Derived \nfrom the 2006 Proteus Workshop, pp. 303-329. \nTecuci, G., Boicu, M., Cox, M.T. (2007b). Seven Aspects of \nMixed-Initiative Reasoning: An Introduction to the Special Issue \non Mixed-Initiative Assistants, AI Magazine, 28 (2), pp. 11-18. \nTecuci, G., Boicu, M., Marcu, D., Boicu, C., Barbulescu, M. \n(2008). \nDisciple-LTA: \nLearning, \nTutoring \nand \nAnalytic \nAssistance, Journal of Intelligence Community Research and \nDevelopment.  \nTecuci, G., Marcu, D., Boicu, M., Schum, D.A., Russell, K. \n(2011). Computational Theory and Cognitive Assistant for \nIntelligence Analysis, in Proc. of the 6th International Conference \non Semantic Technologies for Intelligence, Defense, and Security \n- STIDS 2011, pp. 68-75, November 16-18, Fairfax, VA. \nTecuci, G., Marcu, D., Boicu, M., Schum, D.A. (2015). Cogent: \nCognitive Agent for Cogent Analysis, in Proc. of the 2015 AAAI \nFall Symp. “Cognitive Assistance in Government and Public \nSector Applications,” pp. 58-65, Arlington, VA, Technical Report \nFS-15-02, AAAI Press, Palo Alto, CA.  \nTecuci, G., Schum, D.A., Marcu, D., Boicu, M. (2016a). \nIntelligence Analysis as Discovery of Evidence, Hypotheses, and \nArguments: Connecting the Dots, Cambridge University Press. \nTecuci, G., Marcu, D., Boicu, M., Schum, D.A. (2016b). \nKnowledge Engineering: Building Cognitive Assistants for \nEvidence-based Reasoning, Cambridge University Press. \nTecuci, G., Kaiser, L., Marcu, D., Uttamsingh, C., Boicu, M. \n(2018). Evidence-based Reasoning in Intelligence Analysis: \nStructured Methodology and System, Computing in Science and \nEngineering, 20 (6), pp. 9-21, November/December. \nTecuci, G., Meckl, S., Marcu, D., Boicu, M. (2019). Instructable \nCognitive Agents for Autonomous Evidence-Based Reasoning, \nAdvances in Cognitive Systems, Vol. 8. \nToulmin, S. E. (1963). The Uses of Argument, Cambridge \nUniversity Press. \nvan Gelder, T.J. (2007). The Rationale for Rationale, Law, \nProbability and Risk, 6, pp. 23-42. \nWigmore, J. H. (1937). The Science of Judicial Proof: As Given by \nLogic, Psychology, and General Experience and Illustrated in \nJudicial Trials, 3rd edition, Little, Brown & Co, Boston, MA. \nW3C (2004). http://www.w3.org/TR/rdf-schema/.  \nZadeh, L. (1983). The Role of Fuzzy Logic in the Management of \nUncertainty in Expert Systems, Fuzzy Sets and Systems, 11, pp. \n199-227.",
    "pdf_filename": "Shared Model of Sense-making for Human-Machine Collaboration.pdf"
}