{
    "title": "Analysing Explanation-Related Interactions in Collaborative Perception-Cognition-Communication-Actio",
    "context": "rative tasks, so AI-equipped robots working alongside humans need to be able to explain their behaviour in order to cooperate effectively and earn trust. We analyse and classify communi- cations among human participants collaborating to complete a simulated emergency response task. The analysis identifies messages that relate to various kinds of interactive explanations identified in the explainable AI literature. This allows us to understand what type of explanations humans expect from their teammates in such settings, and thus where AI-equipped robots most need explanation capabilities. We find that most explanation-related messages seek clarification in the decisions or actions taken. We also confirm that messages have an impact on the performance of our simulated task. Effective human-robot teaming is seen as a key enabler of future ‘front line’ situations including emergency re- sponse and disaster relief. In such highly dynamic settings, coordination among team members is a critical success factor. The increasing sophistication of modern artificial intelligence (AI) has led to significantly improved perception, cognition, communication and action (PCCA) capabilities embodied in robots. However, many of the key technologies are ‘black box’ in nature, making it hard to engineer robots that operate in a sufficiently transparent manner to their human collaborators. This work is a step towards analysing human expectations in terms of explainability in relation to task coordination. The setting is a simulation environment, TeamCollab [1], in which humans and AI-equipped robots collaborate to clear an area of dangerous objects. The envi- ronment is designed to highlight PCCA capabilities; humans and robots are intended to work as peer agents, and inter- agent communication is a key factor in task success. We analyse results from TeamCollab experiments with human participants to better understand what humans expect from their teammates in relation to explanation. We adopt a recent explainable AI (XAI) framework [2] that takes a dialogue-centric view of explanation, labelling the exchanged messages in terms of their relationship to elements of the framework. We show that there is a positive relationship between message exchange and team performance: volume of communication correlates with task success. This anal- ysis seeks to answer the research question: What types of explanations do AI-equipped robots need based on human 1Cardiff University, UK. Emails: {RoigVilamalaM, furbyjl, preecead, fuentestoroc}@cardiff.ac.uk 2University of California, Los Angeles, USA {julian700, mbs}@ucla.edu teammate expectations? This is important, as AI agents that communicate effectively have been shown to earn more trust and cooperate better with humans [3]. II. RELATED WORK As AI systems will be required to perform in highly cog- nitively demanding environments, it has become critical to understand the communication mechanisms that would better support human-AI teams. Transparency and explainability are key components of situational awareness to allow agents to understand better the dynamically changing world they constantly perceive [4]. Thus, XAI aims to improve the understanding and interpretation of the decision processes and results of machine learning algorithms [5], [6] to support, e.g., human-robot teaming. Current approaches to XAI have mostly focused on static explainability, with a single message to cover, without input or user preferences involved in the process [7]. Due to the social nature of XAI, interactive explanations are gaining attention [8], as this involves an iterative process that considers user’s information needs and is more similar to people’s patterns on how explanations are expected to be provided [2], [6]. Some taxonomies have been proposed to explore the interactive aspect of XAI. Liao et. al [5] present an XAI question bank framework with a set of prototypical questions that users may ask when requesting an explanation from an AI system. Authors in [2] synthesize 48 empirical studies to create a two-level taxonomy of interactive techniques in XAI based on their cognitive processes and tasks. During communication in critical timing scenarios, asking for an explanation may not always be explicit. The theory behind team communication in human groups identifies that a significant amount of communication goes through non- explicit channels, meaning that messages are interpreted in context, and there are additional communicative channels used as eye-gaze, gestures or non-verbal statements critical for performance that complement the message [9]. There- fore, to better understand implicit explanation dialogues in context, we analyse the communication interaction of human- human data in a team environment and map awareness factors into a taxonomy of interactive techniques in XAI. III. METHODS A. Experiment Design The experiment was designed to run in TeamCollab [1], a simulated environment based on the ThreeDWorld physics arXiv:2411.12483v1  [cs.HC]  19 Nov 2024",
    "body": "Analysing Explanation-Related Interactions in Collaborative\nPerception-Cognition-Communication-Action\nMarc Roig Vilamala1, Jack Furby1, Julian de Gortari Briseno2, Mani Srivastava2, Alun Preece1 and\nCarolina Fuentes Toro1\nAbstract— Effective communication is essential in collabo-\nrative tasks, so AI-equipped robots working alongside humans\nneed to be able to explain their behaviour in order to cooperate\neffectively and earn trust. We analyse and classify communi-\ncations among human participants collaborating to complete\na simulated emergency response task. The analysis identifies\nmessages that relate to various kinds of interactive explanations\nidentified in the explainable AI literature. This allows us to\nunderstand what type of explanations humans expect from\ntheir teammates in such settings, and thus where AI-equipped\nrobots most need explanation capabilities. We find that most\nexplanation-related messages seek clarification in the decisions\nor actions taken. We also confirm that messages have an impact\non the performance of our simulated task.\nI. INTRODUCTION\nEffective human-robot teaming is seen as a key enabler\nof future ‘front line’ situations including emergency re-\nsponse and disaster relief. In such highly dynamic settings,\ncoordination among team members is a critical success\nfactor. The increasing sophistication of modern artificial\nintelligence (AI) has led to significantly improved perception,\ncognition, communication and action (PCCA) capabilities\nembodied in robots. However, many of the key technologies\nare ‘black box’ in nature, making it hard to engineer robots\nthat operate in a sufficiently transparent manner to their\nhuman collaborators. This work is a step towards analysing\nhuman expectations in terms of explainability in relation to\ntask coordination. The setting is a simulation environment,\nTeamCollab [1], in which humans and AI-equipped robots\ncollaborate to clear an area of dangerous objects. The envi-\nronment is designed to highlight PCCA capabilities; humans\nand robots are intended to work as peer agents, and inter-\nagent communication is a key factor in task success.\nWe analyse results from TeamCollab experiments with\nhuman participants to better understand what humans expect\nfrom their teammates in relation to explanation. We adopt\na recent explainable AI (XAI) framework [2] that takes a\ndialogue-centric view of explanation, labelling the exchanged\nmessages in terms of their relationship to elements of the\nframework. We show that there is a positive relationship\nbetween message exchange and team performance: volume\nof communication correlates with task success. This anal-\nysis seeks to answer the research question: What types of\nexplanations do AI-equipped robots need based on human\n1Cardiff University, UK. Emails: {RoigVilamalaM, furbyjl,\npreecead, fuentestoroc}@cardiff.ac.uk\n2University\nof\nCalifornia,\nLos\nAngeles,\nUSA\n{julian700,\nmbs}@ucla.edu\nteammate expectations? This is important, as AI agents that\ncommunicate effectively have been shown to earn more trust\nand cooperate better with humans [3].\nII. RELATED WORK\nAs AI systems will be required to perform in highly cog-\nnitively demanding environments, it has become critical to\nunderstand the communication mechanisms that would better\nsupport human-AI teams. Transparency and explainability\nare key components of situational awareness to allow agents\nto understand better the dynamically changing world they\nconstantly perceive [4]. Thus, XAI aims to improve the\nunderstanding and interpretation of the decision processes\nand results of machine learning algorithms [5], [6] to support,\ne.g., human-robot teaming. Current approaches to XAI have\nmostly focused on static explainability, with a single message\nto cover, without input or user preferences involved in the\nprocess [7]. Due to the social nature of XAI, interactive\nexplanations are gaining attention [8], as this involves an\niterative process that considers user’s information needs and\nis more similar to people’s patterns on how explanations\nare expected to be provided [2], [6]. Some taxonomies have\nbeen proposed to explore the interactive aspect of XAI. Liao\net. al [5] present an XAI question bank framework with\na set of prototypical questions that users may ask when\nrequesting an explanation from an AI system. Authors in\n[2] synthesize 48 empirical studies to create a two-level\ntaxonomy of interactive techniques in XAI based on their\ncognitive processes and tasks.\nDuring communication in critical timing scenarios, asking\nfor an explanation may not always be explicit. The theory\nbehind team communication in human groups identifies that\na significant amount of communication goes through non-\nexplicit channels, meaning that messages are interpreted in\ncontext, and there are additional communicative channels\nused as eye-gaze, gestures or non-verbal statements critical\nfor performance that complement the message [9]. There-\nfore, to better understand implicit explanation dialogues in\ncontext, we analyse the communication interaction of human-\nhuman data in a team environment and map awareness\nfactors into a taxonomy of interactive techniques in XAI.\nIII. METHODS\nA. Experiment Design\nThe experiment was designed to run in TeamCollab [1],\na simulated environment based on the ThreeDWorld physics\narXiv:2411.12483v1  [cs.HC]  19 Nov 2024\n\nFig. 1.\nA top view and first-person view of the simulation environment\nTeamCollab [1]. Agents only have access to their avatar’s first-person view.\nsimulator [10]. Participating agents are tasked with identi-\nfying dangerous objects distributed around a scenario and\ncarrying them to a goal area, as shown in Fig. 1. Each\nobject has two attributes: (i) weight, indicating the number of\nagents required to carry that object and (ii) danger, a boolean\nvalue indicating whether the object is dangerous or benign.\nTo obtain these attributes, each agent is equipped with a\nsensor. This sensor always accurately predicts the weight,\nbut it can be inaccurate regarding the danger. The accuracy\nof the sensor regarding the danger varies between agents.\nAgents know how accurate their sensors are, but they can\nonly learn if their prediction was correct for a given object\nby carrying it to the goal area. This is designed to encourage\ncommunication between agents to confirm which objects are\nactually dangerous. In addition, many of the objects are too\nheavy for a single agent to carry, requiring coordination\nto move them to the goal. The simulation thus emphasises\nperception, cognition, communication and action elements. It\nalso facilitates collaboration between human and AI agents to\nachieve the shared goal of collecting all dangerous objects as\nfast as possible or, failing that, to collect as many dangerous\nobjects as possible by a deadline.\nIn our experiments, we used a deadline of 20 minutes, after\nwhich agents could no longer act. We considered groups of\n2 to 4 human agents, with the intent of better understanding\nhow we should implement robot agents so that they interact\ncorrectly with humans.\nB. Data Collection\nAll participants were asked to connect to the simulation\nthrough their web browser and to avoid the use of any outside\ncommunication with other participants for the duration of\nthe session. Participants were provided with a proximity-\nbased text chat, where communications were broadcast to\nall agents within a range of 5 metres. The system recorded\nall communications, capturing the following parameters:\n• File & session ID: identify each individual session;\n• Timestamp: time in seconds when the message was sent;\n• Sender: the agent that sent the message.\n• Receivers: the agents that received the message;\n• Message: the text that the agent sent.\nWe ran 20 sessions, resulting in 2,607 messages.\nC. Data Analysis\nFollowing a qualitative approach, we conducted a linguis-\ntic analysis of communication logs to identify explanation-\nrelated messages. Previous approaches have been followed\nto understand in-depth interactions that emerge from col-\nlaborative dialogues and spoken instructions [11], [12]. We\nanalyse the messages from the perspective of the recent\nXAI taxonomy presented by [2]. This framework was chosen\nbecause it offers a synthesis of findings from 48 empirical\nstudies evaluating interactive explanations with human users.\nThe resulting taxonomy classifies XAI techniques according\nto the type of support interaction: (i) select, which allows\nusers to choose the information they want to see, (ii) mutate,\nwhich considers hypotheses or different situations, and (iii)\ndialogue with, which provides interactivity. Each cognitive\nsupport type is divided into three task-oriented categories.\nFor our experiment, we focus on four categories: (i) se-\nlect/clarify, which gives additional information on demand,\n(ii) mutate/simulate, which considers predictions for a given\nset of inputs, (iii) dialogue/progress, which guides the user\nthrough an explanation sequence and (iv) dialogue/answer,\nwhich gives feedback or edits explanation components. In\nour analysis, we are not only looking for communications\nthat explicitly request explanation (e.g., questions asking,\n“Why...?”) but also identifying parts of the human-human\ndialogues where our participants are, in the judgement of\nthe annotators, implicitly calling for explanation within the\nfour sub-categories. The purpose here is to focus on the\ncollaborative activities where robots will most need to be\nequipped to offer explanations in response to implicit as well\nas explicit requests. Following an iterative process guided\nby a CodeBook [13], four researchers labelled 1,000 of the\ncollected messages, which came from 15 different sessions.\n1) Procedure: Researchers used a 3-stage qualitative anal-\nysis to label the communication logs. First, 100 messages\nwere individually labelled and checked for inter-code agree-\nment, adjusting the CodeBook where necessary. Then, 150\nmore messages were labelled and discussed to ensure inter-\ncode agreement. Finally, a total of 1,000 messages were\nlabelled, including the re-labelled first 100 messages.\n2) CodeBook: The team worked through the codes to\nlabel the messages, classifying and refining them until all the\nresearchers agreed. The labels were then used as themes to\nclassify the messages. Table I shows the final labels, along\nwith a definition and the corresponding category from the\ntaxonomy in [2] interpreted as noted above.\nIV. RESULTS\nA. Communication Effect on Team Performance\nBefore analyzing the messages sent, we first wanted to\nevaluate whether team communication had any impact on\nthe performance of the team. For that purpose, in this section\nwe compare the number of messages sent by the team as a\nwhole with different performance indicators.\nFig. 2a shows that the total number of collected objects\ntends to have an inverse correlation to the number of mes-\nsages sent by the team as a whole, with a Pearson Correlation\n\nTABLE I\nCODEBOOK DESCRIPTION\nLabel\nCategory\nDefinition\ndoing\nselect/clarify\nThe message is either a question that can be interpreted as “What are you doing?” or a statement that can\nbe interpreted as an answer to “What are you doing?”\nwhy\nselect/clarify\nThe message is either a question that asks for information to justify why some action has (or has not) been\ntaken or a statement that provides such a justification. The action can be an observation or a decision.\nfeatures\nselect/clarify\nThe message (question or statement) specifically refers to features of an object or agent.\nauto\nselect/clarify\nAutocompleted messages used to share all sensed details for a given object via a button on the UI.\ndangerous\nmutate/simulate\nThe message refers to actions / observations / decisions under an assumption that the object is dangerous.\nsafe\nmutate/simulate\nAs above but the assumption is the object is benign.\nmake-safe\nmutate/simulate\nThe message specifically refers to actions to make an object safe.\norder\ndialogue/progress\nThe message is an order / instruction / command / tell.\nconfirm\ndialogue/answer\nThe message is a confirmation.\nCoefficient (PCC) of -0.28. Intuitively, this makes sense, as\nparticipants need to spend time typing the messages, which\nprevents them from taking other actions in the environment.\nAs such, it might seem that sending messages is detrimental\nto the performance of the team as a whole. However,\nFig. 2b shows that the number of actually dangerous objects\ncollected by the teams does not seem to depend on the\nnumber of messages sent (PCC of 0.05). We believe this\nis thanks to the knowledge that each agent gains from\ncommunicating with the others, which allows everyone to\ncompare the predictions from different sensors. This means\nagents can more accurately predict which objects are actually\ndangerous, leading to less wasted time on trips carrying\nbenign objects. This is reflected in Fig. 2c, which shows\nthat a higher percentage of the objects collected by more\ncommunicative teams are actually dangerous (PCC of 0.30).\nAs such, it seems that, while there may be some downsides\nto too much communication, the advantages of getting input\nfrom other teammates can make up for the time spent typing\nmessages. It is also worth considering that in a real scenario,\nthe disposal of dangerous objects might have a monetary\nor time cost beyond the time spent moving them to the\ngoal area represented in the simulation. In such cases, being\nmore accurate in identifying which objects are considered\ndangerous might have even further benefits.\nB. Understanding XAI-Related Language\nWe present the results of the qualitative linguist analy-\nsis conducted to understand what XAI language emerges\nfrom the communications. Four annotators categorised and\nlabelled 1,000 messages with a good level of inter-annotator\nagreement (61% of samples were labelled with at least\nthree annotators agreeing on the label, and 5% of samples\nwere unclassified or with no similarities). Fig. 3 shows the\nmajority label distribution for the annotated messages. That\nis, the labels where at least 3 annotators agreed. The most\nfrequent label is autocompleted messages, which participants\ncan easily send by pressing a UI button to share all sensed\nattributes for an object. An example of this type of message\nis “Object 18 (weight: 1) Last seen in (-7.5,-5.5) at 04:06\n-Status Danger: dangerous,Prob. Correct: 72.5%”. The ease\nwith which these messages can be generated likely had\nan influence on their frequency. The second most common\nlabel is confirm, for messages confirming communications,\nsuch as “K sounds good”, and “Ok 9”. This is followed by\ndoing, for messages that either ask what actions are being\ntaken or can be interpreted as answering such questions.\nSome examples are “I’ll explore in the next room”, “do\nyou want to pick up another one, or should we call it a\nday”. Messages discussing the features of an object are also\ncommon, including cases such as: “Nothing here for me.\nOnly 2 but that one is too heavy”; “I see, that’s high”. All\nother labels are significantly less common, with why and\nmake-safe having no cases of majority agreement. There\nalso were no cases where a majority of annotators agreed on\nassigning multiple labels to a single message.\nIt is worth noting that most of the labelled messages\nfall into the select/clarify category (auto, doing, features),\nfollowed by dialogue/answer (confirm). Annotators also re-\nmarked that, while more traditional XAI questions such\nas “Why do you think object X is dangerous?” do not\ntend to appear explicitly, participants did discuss why they\nthought an object was dangerous or not. This is likely\nwhy messages labelled with auto and features are quite\nfrequent, as participants shared their opinions on how to\nact based on shared sensor outputs. Some messages from\nconfirm were also part of such discussion, as conclusions\nwere reached. Most of the other messages labelled confirm\nwere in response to doing messages, which tended to be used\nto inform teammates of actions and coordinate.\nV. DISCUSSION AND CONCLUSION\nIn this work, we sought to gain a better understanding of\nwhat explanation capabilities are required of PCCA robots\nin team collaboration tasks in dynamic emergency response\ntype settings. Our expectation that effective communication\nmatters in such settings is borne out by data showing that the\nvolume of dialogue correlates with the performance of the\nteam. More specifically, we have shown that, while teams\nthat communicate more tend to collect fewer objects, they\nare more accurate, with a higher percentage of collected\nobjects being actually dangerous. As a result, we see that\nthe implementation of TeamCollab is successful in rewarding\nteams that use effective communication while having the\npotential to penalise teams that communicate ineffectively.\nOur analysis of the content of the communications re-\n\n(a) Number of objects collected\n(b) Number of dangerous objects collected\n(c) Percentage of collected objects that are\nactually dangerous\nFig. 2.\nDifferent evaluations of performance by the number of team messages sent.\nFig. 3.\nMajority Label Distribution (≥3 agreement level)\nvealed that a high proportion of messages discussed the\nattributes of the objects and what to do with them as a result.\nThere was also a significant proportion of messages intended\nto coordinate the team, either as a high-level strategy or\nfor a specific task. In terms of the PCCA capabilities, our\nanalysis focused directly on communication. The content\nof a high proportion of messages concerned perception\n(auto/features) and action (doing). We identify cognition\nin the discussions deliberating whether an object is actually\ndangerous based on multiple sensor readings and in the\nmessages trying to achieve team coordination.\nThese\nfindings\nwill\ninform\nthe\nimplementation\nof\nexplanation-equipped AI agents that can effectively commu-\nnicate with their human teammates. In particular, it will help\nus focus on the XAI capabilities that humans expect from\ntheir teammates. In immediate future work, we will extend\nthe TeamCollab experiments to human-robot interaction and\nanalyse whether there are any significant differences in how\nhumans communicate with teammates depending on whether\nthey are human or robot. We would also like to explore any\ndifferences for larger teams and other tasks. Furthermore, we\nwould like to run similar experiments in the real world to\nallow us to capture non-verbal communication aspects.\nACKNOWLEDGMENT\nThe research reported in this paper was sponsored in part\nby the DEVCOM Army Research Laboratory via cooperative\nagreement W911NF2220243. Any opinions, findings, and\nconclusions or recommendations expressed in this material\nare those of the author(s) and do not necessarily reflect the\nviews of the United States government.\nREFERENCES\n[1] J. de Gortari Briseno, R. Para´c, L. Ardon, M. Roig Vilamala,\nD. Furelos-Blanco, L. Kaplan, V. K. Mishra, F. Cerutti, A. Preece,\nA. Russo, and M. Srivastava, “TeamCollab: A framework for col-\nlaborative Perception-Cognition-Communication-Action,” in Proc 27th\nInternational Conference on Information Fusion, in press 2024.\n[2] A. Bertrand, T. Viard, R. Belloum, J. R. Eagan, and W. Maxwell,\n“On selective, mutable and dialogic XAI: a review of what users say\nabout different types of interactive explanations,” in Proc 2023 CHI\nConference on Human Factors in Computing Systems, April 2023, pp.\n1–21.\n[3] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu,\nand C. Gan, “Building Cooperative Embodied Agents Modularly\nwith Large Language Models,” arXiv preprint, vol. arXiv:2307.02485,\n2023.\n[4] M. R. Endsley, “Supporting human-AI teams: Transparency, explain-\nability, and situation awareness,” Computers in Human Behavior, vol.\n140, p. 107574, 2023.\n[5] Q. V. Liao, D. Gruen, and S. Miller, “Questioning the AI: informing\ndesign practices for explainable AI user experiences,” in Proc 2020\nCHI conference on human factors in computing systems, 2020, pp.\n1–15.\n[6] S. T. Mueller, E. S. Veinott, R. R. Hoffman, G. Klein, L. Alam,\nT. Mamun, and W. J. Clancey, “Principles of explanation in human-AI\nsystems,” arXiv preprint arXiv:2102.04972, 2021.\n[7] K. Sokol and P. Flach, “One explanation does not fit all: The promise\nof interactive explanations for machine learning transparency,” KI-\nK¨unstliche Intelligenz, vol. 34, no. 2, pp. 235–250, 2020.\n[8] T. Miller, “Explanation in artificial intelligence: Insights from the\nsocial sciences,” Artificial intelligence, vol. 267, pp. 1–38, 2019.\n[9] C. Liang, J. Proft, E. Andersen, and R. A. Knepper, “Implicit commu-\nnication of actionable information in human-AI teams,” in Proc 2019\nCHI conference on human factors in computing systems, 2019, pp.\n1–13.\n[10] C. Gan et al, “ThreeDWorld: A platform for interactive multi-modal\nphysical simulation,” in Proc Advances in Neural Information Pro-\ncessing Systems (NeurIPS) Track on Datasets and Benchmarks, 2021.\n[11] C. Fuentes, M. Porcheron, and J. E. Fischer, “Roboclean: Contextual\nlanguage grounding for human-robot interactions in specialised low-\nresource environments,” in Proc 5th International Conference on\nConversational User Interfaces, 2023, pp. 1–11.\n[12] M. Kullasaar, E. Vutt, and M. Koit, “Developing a natural language\ndialogue system: Wizard of Oz studies,” in Proc First International\nIEEE Symposium Intelligent Systems, vol. 1.\nIEEE, 2002, pp. 202–\n207.\n[13] J. T. DeCuir-Gunby, P. L. Marshall, and A. W. McCulloch, “De-\nveloping and using a codebook for the analysis of interview data:\nAn example from a professional development research project,” Field\nmethods, vol. 23, no. 2, pp. 136–155, 2011.",
    "pdf_filename": "Analysing_Explanation-Related_Interactions_in_Collaborative_Perception-Cognition-Communication-Actio.pdf"
}