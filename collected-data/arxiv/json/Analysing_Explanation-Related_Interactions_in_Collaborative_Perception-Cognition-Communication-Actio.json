{
    "title": "Analysing Explanation-Related Interactions in Collaborative",
    "abstract": "rative tasks, so AI-equipped robots working alongside humans communicateeffectivelyhavebeenshowntoearnmoretrust needtobeabletoexplaintheirbehaviourinordertocooperate and cooperate better with humans [3]. effectively and earn trust. We analyse and classify communi- cations among human participants collaborating to complete II. RELATEDWORK a simulated emergency response task. The analysis identifies messagesthatrelatetovariouskindsofinteractiveexplanations As AI systems will be required to perform in highly cog- identified in the explainable AI literature. This allows us to nitively demanding environments, it has become critical to understand what type of explanations humans expect from understandthecommunicationmechanismsthatwouldbetter their teammates in such settings, and thus where AI-equipped robots most need explanation capabilities. We find that most support human-AI teams. Transparency and explainability explanation-related messages seek clarification in the decisions are key components of situational awareness to allow agents oractionstaken.Wealsoconfirmthatmessageshaveanimpact to understand better the dynamically changing world they on the performance of our simulated task. constantly perceive [4]. Thus, XAI aims to improve the understanding and interpretation of the decision processes I. INTRODUCTION andresultsofmachinelearningalgorithms[5],[6]tosupport, Effective human-robot teaming is seen as a key enabler e.g., human-robot teaming. Current approaches to XAI have of future ‘front line’ situations including emergency re- mostlyfocusedonstaticexplainability,withasinglemessage sponse and disaster relief. In such highly dynamic settings, to cover, without input or user preferences involved in the coordination among team members is a critical success process [7]. Due to the social nature of XAI, interactive factor. The increasing sophistication of modern artificial explanations are gaining attention [8], as this involves an intelligence(AI)hasledtosignificantlyimprovedperception, iterative process that considers user’s information needs and cognition, communication and action (PCCA) capabilities is more similar to people’s patterns on how explanations embodied in robots. However, many of the key technologies are expected to be provided [2], [6]. Some taxonomies have are ‘black box’ in nature, making it hard to engineer robots beenproposedtoexploretheinteractiveaspectofXAI.Liao that operate in a sufficiently transparent manner to their et. al [5] present an XAI question bank framework with human collaborators. This work is a step towards analysing a set of prototypical questions that users may ask when human expectations in terms of explainability in relation to requesting an explanation from an AI system. Authors in task coordination. The setting is a simulation environment, [2] synthesize 48 empirical studies to create a two-level TeamCollab [1], in which humans and AI-equipped robots taxonomy of interactive techniques in XAI based on their collaborate to clear an area of dangerous objects. The envi- cognitive processes and tasks. ronmentisdesignedtohighlightPCCAcapabilities;humans Duringcommunicationincriticaltimingscenarios,asking and robots are intended to work as peer agents, and inter- for an explanation may not always be explicit. The theory agent communication is a key factor in task success. behind team communication in human groups identifies that We analyse results from TeamCollab experiments with a significant amount of communication goes through non- humanparticipantstobetterunderstandwhathumansexpect explicit channels, meaning that messages are interpreted in from their teammates in relation to explanation. We adopt context, and there are additional communicative channels a recent explainable AI (XAI) framework [2] that takes a used as eye-gaze, gestures or non-verbal statements critical dialogue-centricviewofexplanation,labellingtheexchanged for performance that complement the message [9]. There- messages in terms of their relationship to elements of the fore, to better understand implicit explanation dialogues in framework. We show that there is a positive relationship context,weanalysethecommunicationinteractionofhuman- between message exchange and team performance: volume human data in a team environment and map awareness of communication correlates with task success. This anal- factors into a taxonomy of interactive techniques in XAI. ysis seeks to answer the research question: What types of explanations do AI-equipped robots need based on human III. METHODS 1CardiffUniversity,UK.Emails:{RoigVilamalaM, furbyjl, A. Experiment Design preecead, fuentestoroc}@cardiff.ac.uk The experiment was designed to run in TeamCollab [1], 2University of California, Los Angeles, USA {julian700, mbs}@ucla.edu a simulated environment based on the ThreeDWorld physics 4202 voN 91 ]CH.sc[ 1v38421.1142:viXra",
    "body": "Analysing Explanation-Related Interactions in Collaborative\nPerception-Cognition-Communication-Action\nMarc Roig Vilamala1, Jack Furby1, Julian de Gortari Briseno2, Mani Srivastava2, Alun Preece1 and\nCarolina Fuentes Toro1\nAbstract—Effective communication is essential in collabo- teammate expectations? This is important, as AI agents that\nrative tasks, so AI-equipped robots working alongside humans communicateeffectivelyhavebeenshowntoearnmoretrust\nneedtobeabletoexplaintheirbehaviourinordertocooperate\nand cooperate better with humans [3].\neffectively and earn trust. We analyse and classify communi-\ncations among human participants collaborating to complete\nII. RELATEDWORK\na simulated emergency response task. The analysis identifies\nmessagesthatrelatetovariouskindsofinteractiveexplanations As AI systems will be required to perform in highly cog-\nidentified in the explainable AI literature. This allows us to\nnitively demanding environments, it has become critical to understand what type of explanations humans expect from\nunderstandthecommunicationmechanismsthatwouldbetter\ntheir teammates in such settings, and thus where AI-equipped\nrobots most need explanation capabilities. We find that most support human-AI teams. Transparency and explainability\nexplanation-related messages seek clarification in the decisions are key components of situational awareness to allow agents\noractionstaken.Wealsoconfirmthatmessageshaveanimpact to understand better the dynamically changing world they\non the performance of our simulated task.\nconstantly perceive [4]. Thus, XAI aims to improve the\nunderstanding and interpretation of the decision processes I. INTRODUCTION\nandresultsofmachinelearningalgorithms[5],[6]tosupport,\nEffective human-robot teaming is seen as a key enabler\ne.g., human-robot teaming. Current approaches to XAI have\nof future ‘front line’ situations including emergency re-\nmostlyfocusedonstaticexplainability,withasinglemessage\nsponse and disaster relief. In such highly dynamic settings,\nto cover, without input or user preferences involved in the\ncoordination among team members is a critical success\nprocess [7]. Due to the social nature of XAI, interactive\nfactor. The increasing sophistication of modern artificial\nexplanations are gaining attention [8], as this involves an\nintelligence(AI)hasledtosignificantlyimprovedperception,\niterative process that considers user’s information needs and\ncognition, communication and action (PCCA) capabilities\nis more similar to people’s patterns on how explanations\nembodied in robots. However, many of the key technologies\nare expected to be provided [2], [6]. Some taxonomies have\nare ‘black box’ in nature, making it hard to engineer robots\nbeenproposedtoexploretheinteractiveaspectofXAI.Liao\nthat operate in a sufficiently transparent manner to their\net. al [5] present an XAI question bank framework with\nhuman collaborators. This work is a step towards analysing\na set of prototypical questions that users may ask when\nhuman expectations in terms of explainability in relation to\nrequesting an explanation from an AI system. Authors in\ntask coordination. The setting is a simulation environment,\n[2] synthesize 48 empirical studies to create a two-level\nTeamCollab [1], in which humans and AI-equipped robots\ntaxonomy of interactive techniques in XAI based on their\ncollaborate to clear an area of dangerous objects. The envi-\ncognitive processes and tasks.\nronmentisdesignedtohighlightPCCAcapabilities;humans\nDuringcommunicationincriticaltimingscenarios,asking\nand robots are intended to work as peer agents, and inter-\nfor an explanation may not always be explicit. The theory\nagent communication is a key factor in task success.\nbehind team communication in human groups identifies that\nWe analyse results from TeamCollab experiments with\na significant amount of communication goes through non-\nhumanparticipantstobetterunderstandwhathumansexpect\nexplicit channels, meaning that messages are interpreted in\nfrom their teammates in relation to explanation. We adopt\ncontext, and there are additional communicative channels\na recent explainable AI (XAI) framework [2] that takes a\nused as eye-gaze, gestures or non-verbal statements critical\ndialogue-centricviewofexplanation,labellingtheexchanged\nfor performance that complement the message [9]. There-\nmessages in terms of their relationship to elements of the\nfore, to better understand implicit explanation dialogues in\nframework. We show that there is a positive relationship\ncontext,weanalysethecommunicationinteractionofhuman-\nbetween message exchange and team performance: volume\nhuman data in a team environment and map awareness\nof communication correlates with task success. This anal-\nfactors into a taxonomy of interactive techniques in XAI.\nysis seeks to answer the research question: What types of\nexplanations do AI-equipped robots need based on human III. METHODS\n1CardiffUniversity,UK.Emails:{RoigVilamalaM, furbyjl, A. Experiment Design\npreecead, fuentestoroc}@cardiff.ac.uk\nThe experiment was designed to run in TeamCollab [1],\n2University of California, Los Angeles, USA {julian700,\nmbs}@ucla.edu a simulated environment based on the ThreeDWorld physics\n4202\nvoN\n91\n]CH.sc[\n1v38421.1142:viXra\nC. Data Analysis\nFollowing a qualitative approach, we conducted a linguis-\ntic analysis of communication logs to identify explanation-\nrelated messages. Previous approaches have been followed\nto understand in-depth interactions that emerge from col-\nlaborative dialogues and spoken instructions [11], [12]. We\nanalyse the messages from the perspective of the recent\nXAItaxonomypresentedby[2].Thisframeworkwaschosen\nbecause it offers a synthesis of findings from 48 empirical\nstudiesevaluatinginteractiveexplanationswithhumanusers.\nFig.1. Atopviewandfirst-personviewofthesimulationenvironment\nThe resulting taxonomy classifies XAI techniques according\nTeamCollab[1].Agentsonlyhaveaccesstotheiravatar’sfirst-personview.\nto the type of support interaction: (i) select, which allows\nuserstochoosetheinformationtheywanttosee,(ii)mutate,\nsimulator [10]. Participating agents are tasked with identi- which considers hypotheses or different situations, and (iii)\nfying dangerous objects distributed around a scenario and dialogue with, which provides interactivity. Each cognitive\ncarrying them to a goal area, as shown in Fig. 1. Each support type is divided into three task-oriented categories.\nobjecthastwoattributes:(i)weight,indicatingthenumberof For our experiment, we focus on four categories: (i) se-\nagentsrequiredtocarrythatobjectand(ii)danger,aboolean lect/clarify, which gives additional information on demand,\nvalue indicating whether the object is dangerous or benign. (ii) mutate/simulate, which considers predictions for a given\nTo obtain these attributes, each agent is equipped with a set of inputs, (iii) dialogue/progress, which guides the user\nsensor. This sensor always accurately predicts the weight, through an explanation sequence and (iv) dialogue/answer,\nbut it can be inaccurate regarding the danger. The accuracy which gives feedback or edits explanation components. In\nof the sensor regarding the danger varies between agents. our analysis, we are not only looking for communications\nAgents know how accurate their sensors are, but they can that explicitly request explanation (e.g., questions asking,\nonly learn if their prediction was correct for a given object “Why...?”) but also identifying parts of the human-human\nbycarryingittothegoalarea.Thisisdesignedtoencourage dialogues where our participants are, in the judgement of\ncommunicationbetweenagentstoconfirmwhichobjectsare the annotators, implicitly calling for explanation within the\nactually dangerous. In addition, many of the objects are too four sub-categories. The purpose here is to focus on the\nheavy for a single agent to carry, requiring coordination collaborative activities where robots will most need to be\nto move them to the goal. The simulation thus emphasises equippedtoofferexplanationsinresponsetoimplicitaswell\nperception,cognition,communicationandactionelements.It as explicit requests. Following an iterative process guided\nalsofacilitatescollaborationbetweenhumanandAIagentsto by a CodeBook [13], four researchers labelled 1,000 of the\nachievethesharedgoalofcollectingalldangerousobjectsas collected messages, which came from 15 different sessions.\n1) Procedure: Researchersuseda3-stagequalitativeanal-\nfast as possible or, failing that, to collect as many dangerous\nysis to label the communication logs. First, 100 messages\nobjects as possible by a deadline.\nwere individually labelled and checked for inter-code agree-\nInourexperiments,weusedadeadlineof20minutes,after\nment, adjusting the CodeBook where necessary. Then, 150\nwhich agents could no longer act. We considered groups of\nmore messages were labelled and discussed to ensure inter-\n2 to 4 human agents, with the intent of better understanding\ncode agreement. Finally, a total of 1,000 messages were\nhow we should implement robot agents so that they interact\nlabelled, including the re-labelled first 100 messages.\ncorrectly with humans.\n2) CodeBook: The team worked through the codes to\nlabelthemessages,classifyingandrefiningthemuntilallthe\nB. Data Collection\nresearchers agreed. The labels were then used as themes to\nAll participants were asked to connect to the simulation classify the messages. Table I shows the final labels, along\nthroughtheirwebbrowserandtoavoidtheuseofanyoutside with a definition and the corresponding category from the\ncommunication with other participants for the duration of taxonomy in [2] interpreted as noted above.\nthe session. Participants were provided with a proximity- IV. RESULTS\nbased text chat, where communications were broadcast to\nA. Communication Effect on Team Performance\nall agents within a range of 5 metres. The system recorded\nBefore analyzing the messages sent, we first wanted to\nall communications, capturing the following parameters:\nevaluate whether team communication had any impact on\n• File & session ID: identify each individual session; theperformanceoftheteam.Forthatpurpose,inthissection\n• Timestamp:timeinsecondswhenthemessagewassent; we compare the number of messages sent by the team as a\n• Sender: the agent that sent the message. whole with different performance indicators.\n• Receivers: the agents that received the message; Fig. 2a shows that the total number of collected objects\n• Message: the text that the agent sent. tends to have an inverse correlation to the number of mes-\nWe ran 20 sessions, resulting in 2,607 messages. sagessentbytheteamasawhole,withaPearsonCorrelation\nTABLEI\nCODEBOOKDESCRIPTION\nLabel Category Definition\ndoing select/clarify Themessageiseitheraquestionthatcanbeinterpretedas“Whatareyoudoing?”orastatementthatcan\nbeinterpretedasananswerto“Whatareyoudoing?”\nwhy select/clarify Themessageiseitheraquestionthatasksforinformationtojustifywhysomeactionhas(orhasnot)been\ntakenorastatementthatprovidessuchajustification.Theactioncanbeanobservationoradecision.\nfeatures select/clarify Themessage(questionorstatement)specificallyreferstofeaturesofanobjectoragent.\nauto select/clarify AutocompletedmessagesusedtoshareallsenseddetailsforagivenobjectviaabuttonontheUI.\ndangerous mutate/simulate Themessagereferstoactions/observations/decisionsunderanassumptionthattheobjectisdangerous.\nsafe mutate/simulate Asabovebuttheassumptionistheobjectisbenign.\nmake-safe mutate/simulate Themessagespecificallyreferstoactionstomakeanobjectsafe.\norder dialogue/progress Themessageisanorder/instruction/command/tell.\nconfirm dialogue/answer Themessageisaconfirmation.\nCoefficient (PCC) of -0.28. Intuitively, this makes sense, as label is confirm, for messages confirming communications,\nparticipants need to spend time typing the messages, which such as “K sounds good”, and “Ok 9”. This is followed by\nprevents them from taking other actions in the environment. doing, for messages that either ask what actions are being\nAs such, it might seem that sending messages is detrimental taken or can be interpreted as answering such questions.\nto the performance of the team as a whole. However, Some examples are “I’ll explore in the next room”, “do\nFig. 2b shows that the number of actually dangerous objects you want to pick up another one, or should we call it a\ncollected by the teams does not seem to depend on the day”.Messagesdiscussingthefeaturesofanobjectarealso\nnumber of messages sent (PCC of 0.05). We believe this common, including cases such as: “Nothing here for me.\nis thanks to the knowledge that each agent gains from Only 2 but that one is too heavy”; “I see, that’s high”. All\ncommunicating with the others, which allows everyone to other labels are significantly less common, with why and\ncompare the predictions from different sensors. This means make-safe having no cases of majority agreement. There\nagentscanmoreaccuratelypredictwhichobjectsareactually alsowerenocaseswhereamajorityofannotatorsagreedon\ndangerous, leading to less wasted time on trips carrying assigning multiple labels to a single message.\nbenign objects. This is reflected in Fig. 2c, which shows It is worth noting that most of the labelled messages\nthat a higher percentage of the objects collected by more fall into the select/clarify category (auto, doing, features),\ncommunicative teams are actually dangerous (PCC of 0.30). followed by dialogue/answer (confirm). Annotators also re-\nAssuch,itseemsthat,whiletheremaybesomedownsides marked that, while more traditional XAI questions such\nto too much communication, the advantages of getting input as “Why do you think object X is dangerous?” do not\nfromotherteammatescanmakeupforthetimespenttyping tend to appear explicitly, participants did discuss why they\nmessages.Itisalsoworthconsideringthatinarealscenario, thought an object was dangerous or not. This is likely\nthe disposal of dangerous objects might have a monetary why messages labelled with auto and features are quite\nor time cost beyond the time spent moving them to the frequent, as participants shared their opinions on how to\ngoal area represented in the simulation. In such cases, being act based on shared sensor outputs. Some messages from\nmore accurate in identifying which objects are considered confirm were also part of such discussion, as conclusions\ndangerous might have even further benefits. were reached. Most of the other messages labelled confirm\nwereinresponsetodoingmessages,whichtendedtobeused\nB. Understanding XAI-Related Language to inform teammates of actions and coordinate.\nWe present the results of the qualitative linguist analy-\nV. DISCUSSIONANDCONCLUSION\nsis conducted to understand what XAI language emerges\nfrom the communications. Four annotators categorised and In this work, we sought to gain a better understanding of\nlabelled 1,000 messages with a good level of inter-annotator what explanation capabilities are required of PCCA robots\nagreement (61% of samples were labelled with at least in team collaboration tasks in dynamic emergency response\nthree annotators agreeing on the label, and 5% of samples type settings. Our expectation that effective communication\nwere unclassified or with no similarities). Fig. 3 shows the mattersinsuchsettingsisborneoutbydatashowingthatthe\nmajority label distribution for the annotated messages. That volume of dialogue correlates with the performance of the\nis, the labels where at least 3 annotators agreed. The most team. More specifically, we have shown that, while teams\nfrequentlabelisautocompletedmessages,whichparticipants that communicate more tend to collect fewer objects, they\ncan easily send by pressing a UI button to share all sensed are more accurate, with a higher percentage of collected\nattributes for an object. An example of this type of message objects being actually dangerous. As a result, we see that\nis “Object 18 (weight: 1) Last seen in (-7.5,-5.5) at 04:06 theimplementationofTeamCollabissuccessfulinrewarding\n-StatusDanger:dangerous,Prob.Correct:72.5%”.Theease teams that use effective communication while having the\nwith which these messages can be generated likely had potential to penalise teams that communicate ineffectively.\nan influence on their frequency. The second most common Our analysis of the content of the communications re-\n(a)Numberofobjectscollected (b)Numberofdangerousobjectscollected (c) Percentage of collected objects that are\nactuallydangerous\nFig.2. Differentevaluationsofperformancebythenumberofteammessagessent.\nconclusions or recommendations expressed in this material\nare those of the author(s) and do not necessarily reflect the\nviews of the United States government.\nREFERENCES\n[1] J. de Gortari Briseno, R. Parac´, L. Ardon, M. Roig Vilamala,\nD. Furelos-Blanco, L. Kaplan, V. K. Mishra, F. Cerutti, A. Preece,\nA. Russo, and M. Srivastava, “TeamCollab: A framework for col-\nlaborativePerception-Cognition-Communication-Action,”inProc27th\nInternationalConferenceonInformationFusion,inpress2024.\n[2] A. Bertrand, T. Viard, R. Belloum, J. R. Eagan, and W. Maxwell,\n“Onselective,mutableanddialogicXAI:areviewofwhatuserssay\nFig.3. MajorityLabelDistribution(≥3agreementlevel) about different types of interactive explanations,” in Proc 2023 CHI\nConferenceonHumanFactorsinComputingSystems,April2023,pp.\n1–21.\n[3] H.Zhang,W.Du,J.Shan,Q.Zhou,Y.Du,J.B.Tenenbaum,T.Shu,\nvealed that a high proportion of messages discussed the\nand C. Gan, “Building Cooperative Embodied Agents Modularly\nattributesoftheobjectsandwhattodowiththemasaresult. withLargeLanguageModels,”arXivpreprint,vol.arXiv:2307.02485,\nTherewasalsoasignificantproportionofmessagesintended 2023.\n[4] M.R.Endsley,“Supportinghuman-AIteams:Transparency,explain-\nto coordinate the team, either as a high-level strategy or\nability,andsituationawareness,”ComputersinHumanBehavior,vol.\nfor a specific task. In terms of the PCCA capabilities, our 140,p.107574,2023.\nanalysis focused directly on communication. The content [5] Q.V.Liao,D.Gruen,andS.Miller,“QuestioningtheAI:informing\ndesign practices for explainable AI user experiences,” in Proc 2020\nof a high proportion of messages concerned perception\nCHI conference on human factors in computing systems, 2020, pp.\n(auto/features) and action (doing). We identify cognition 1–15.\nin the discussions deliberating whether an object is actually [6] S. T. Mueller, E. S. Veinott, R. R. Hoffman, G. Klein, L. Alam,\nT.Mamun,andW.J.Clancey,“Principlesofexplanationinhuman-AI\ndangerous based on multiple sensor readings and in the\nsystems,”arXivpreprintarXiv:2102.04972,2021.\nmessages trying to achieve team coordination. [7] K.SokolandP.Flach,“Oneexplanationdoesnotfitall:Thepromise\nThese findings will inform the implementation of of interactive explanations for machine learning transparency,” KI-\nKu¨nstlicheIntelligenz,vol.34,no.2,pp.235–250,2020.\nexplanation-equipped AI agents that can effectively commu-\n[8] T. Miller, “Explanation in artificial intelligence: Insights from the\nnicatewiththeirhumanteammates.Inparticular,itwillhelp socialsciences,”Artificialintelligence,vol.267,pp.1–38,2019.\nus focus on the XAI capabilities that humans expect from [9] C.Liang,J.Proft,E.Andersen,andR.A.Knepper,“Implicitcommu-\nnicationofactionableinformationinhuman-AIteams,”inProc2019\ntheir teammates. In immediate future work, we will extend\nCHI conference on human factors in computing systems, 2019, pp.\nthe TeamCollab experiments to human-robot interaction and 1–13.\nanalyse whether there are any significant differences in how [10] C. Gan et al, “ThreeDWorld: A platform for interactive multi-modal\nphysical simulation,” in Proc Advances in Neural Information Pro-\nhumanscommunicatewithteammatesdependingonwhether\ncessingSystems(NeurIPS)TrackonDatasetsandBenchmarks,2021.\nthey are human or robot. We would also like to explore any [11] C.Fuentes,M.Porcheron,andJ.E.Fischer,“Roboclean:Contextual\ndifferencesforlargerteamsandothertasks.Furthermore,we language grounding for human-robot interactions in specialised low-\nresource environments,” in Proc 5th International Conference on\nwould like to run similar experiments in the real world to\nConversationalUserInterfaces,2023,pp.1–11.\nallow us to capture non-verbal communication aspects. [12] M. Kullasaar, E. Vutt, and M. Koit, “Developing a natural language\ndialogue system: Wizard of Oz studies,” in Proc First International\nIEEESymposiumIntelligentSystems,vol.1. IEEE,2002,pp.202–\nACKNOWLEDGMENT\n207.\nThe research reported in this paper was sponsored in part [13] J. T. DeCuir-Gunby, P. L. Marshall, and A. W. McCulloch, “De-\nveloping and using a codebook for the analysis of interview data:\nbytheDEVCOMArmyResearchLaboratoryviacooperative\nAnexamplefromaprofessionaldevelopmentresearchproject,”Field\nagreement W911NF2220243. Any opinions, findings, and methods,vol.23,no.2,pp.136–155,2011.",
    "pdf_filename": "Analysing_Explanation-Related_Interactions_in_Collaborative_Perception-Cognition-Communication-Actio.pdf"
}