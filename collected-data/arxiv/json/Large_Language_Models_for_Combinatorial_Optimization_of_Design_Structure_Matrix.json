{
    "title": "Large Language Models for Combinatorial Optimization of Design Structure Matrix",
    "context": "Combinatorial optimization (CO) is essential for improving efficiency and performance in engineering applications. As complexity increases with larger problem sizes and more intricate dependencies, identifying the optimal solution become challenging. When it comes to real-world engineering problems, algorithms based on pure mathematical reasoning are limited and incapable to capture the contextual nuances necessary for optimization. This study explores the potential of Large Language Models (LLMs) in solving engineering CO problems by leveraging their reasoning power and contextual knowledge. We propose a novel LLM-based framework that integrates network topology and domain knowledge to optimize the sequencing of Design Structure Matrix (DSM)—a common CO problem. Our experiments on various DSM cases demonstrate that the proposed method achieves faster convergence and higher solution quality than benchmark methods. Moreover, results show that incorporating contextual domain knowledge significantly improves performance despite the choice of LLMs. These findings highlight the potential of LLMs to address complex CO problems by combining semantic and mathematical reasoning. This approach paves the way for a new paradigm in real-world engineering combinatorial optimization. Keywords Large Language Models · Combinatorial Optimization · Artificial Intelligence · Knowledge-based Reasoning · Design Structure Matrix · Systems Engineering 1 Combinatorial optimization (CO) problems are ubiquitous across fields, where finding an optimal solution from a finite set often drives improvements in efficiency, cost, and performance [1]. For instance, applications such as DNA barcoding and DNA assembly in synthetic biology [2], as well as job scheduling in manufacturing [3], rely heavily on effective CO solutions. However, due to their NP-hard nature, these problems present substantial challenges, especially as complexity increases with larger problem sizes and more intricate dependencies. Traditionally, CO problems in engineering are usually approached through the following process: the problem is first modelled mathematically, then solved using specific algorithms or heuristics, and finally interpreted within the context of practical engineering [4]. ∗Comments are welcome: shuojiangcn@gmail.com arXiv:2411.12571v1  [cs.CE]  19 Nov 2024",
    "body": "LARGE LANGUAGE MODELS FOR COMBINATORIAL\nOPTIMIZATION OF DESIGN STRUCTURE MATRIX\nShuo Jiang∗\nDepartment of Systems Engineering\nCity University of Hong Kong\n83 Tat Chee Ave, Kowloon Tong, Hong Kong\nshuo.jiang@cityu.edu.hk\nMin Xie\nDepartment of Systems Engineering\nCity University of Hong Kong\n83 Tat Chee Ave, Kowloon Tong, Hong Kong\nxiemin@cityu.edu.hk\nJianxi Luo\nDepartment of Systems Engineering\nCity University of Hong Kong\n83 Tat Chee Ave, Kowloon Tong, Hong Kong\njianxi.luo@cityu.edu.hk\nA Preprint, Version: Nov 19, 2024\nABSTRACT\nCombinatorial optimization (CO) is essential for improving efficiency and performance in engineering\napplications. As complexity increases with larger problem sizes and more intricate dependencies,\nidentifying the optimal solution become challenging. When it comes to real-world engineering\nproblems, algorithms based on pure mathematical reasoning are limited and incapable to capture the\ncontextual nuances necessary for optimization. This study explores the potential of Large Language\nModels (LLMs) in solving engineering CO problems by leveraging their reasoning power and\ncontextual knowledge. We propose a novel LLM-based framework that integrates network topology\nand domain knowledge to optimize the sequencing of Design Structure Matrix (DSM)—a common\nCO problem. Our experiments on various DSM cases demonstrate that the proposed method achieves\nfaster convergence and higher solution quality than benchmark methods. Moreover, results show\nthat incorporating contextual domain knowledge significantly improves performance despite the\nchoice of LLMs. These findings highlight the potential of LLMs to address complex CO problems by\ncombining semantic and mathematical reasoning. This approach paves the way for a new paradigm\nin real-world engineering combinatorial optimization.\nKeywords Large Language Models · Combinatorial Optimization · Artificial Intelligence · Knowledge-based\nReasoning · Design Structure Matrix · Systems Engineering\n1\nIntroduction\nCombinatorial optimization (CO) problems are ubiquitous across fields, where finding an optimal solution from a\nfinite set often drives improvements in efficiency, cost, and performance [1]. For instance, applications such as DNA\nbarcoding and DNA assembly in synthetic biology [2], as well as job scheduling in manufacturing [3], rely heavily on\neffective CO solutions. However, due to their NP-hard nature, these problems present substantial challenges, especially\nas complexity increases with larger problem sizes and more intricate dependencies. Traditionally, CO problems in\nengineering are usually approached through the following process: the problem is first modelled mathematically, then\nsolved using specific algorithms or heuristics, and finally interpreted within the context of practical engineering [4].\n∗Comments are welcome: shuojiangcn@gmail.com\narXiv:2411.12571v1  [cs.CE]  19 Nov 2024\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nThis separation of problem-solving and interpretation stages is limited and incapable to capture the contextual nuances\nnecessary for optimization of real-world problems.\nRecent advancements in Large Language Models (LLMs) have demonstrated their powerful capabilities in natural\nlanguage generation, semantic understanding, instruction following, and complex reasoning [5, 6]. Furthermore,\npioneering studies have shown that LLMs can be used for continuous and concrete optimization [7, 8, 9]. For instance,\nresearchers from DeepMind utilized LLMs as optimizers and evaluated their effectiveness on classic CO problems [8],\nsuch as Traveling Salesman Problems (TSP). Additionally, prior studies also highlight that LLMs possess extensive\ndomain knowledge pretrained across a wide range of engineering-related data, which enhances their applicability in\nengineering fields [10, 11, 12, 13]. Therefore, the ability of LLMs to combine mathematical and semantic reasoning,\nalong with their possession of extensive knowledge, motivated us to explore their potential for solving engineering\nCO problems while integrating contextual domain knowledge relevant to their network typology. Our hypotheses are:\n(1) LLMs can be effectively applied to solve CO problems in engineering, and (2) incorporating contextual domain\nknowledge can further enhance LLM performance by supporting mathematical reasoning with semantic insights. This\nparadigm, which leverages both semantic and mathematical reasoning, introduces a novel approach to combinatorial\noptimization that traditional pure mathematical methods cannot achieve for empirical problems. On this basis, we\npropose a novel LLM-based framework that integrates both network topology and domain context into the optimization\nprocess.\nTo evaluate our proposed method, we focus on the Design Structure Matrix (DSM) sequencing task, as an example\nof CO problems. DSM is a modelling tool in engineering design, which represents dependency relationships among\ntasks or components within a system [14]. Reordering the node sequence of DSMs can significantly reduce feedback\nloops and improve modularization [15, 16]. The DSM sequencing is also an NP-hard problem, and traditional methods\ntypically approach it using heuristics-based algorithms [17, 18]. Figure 1 illustrates a design activity DSM before and\nafter sequencing [19]. In this paper, we conduct extensive experiments on various DSM cases to demonstrate that our\nLLM-based method achieves better convergence speed and solution quality compared to benchmark methods. Notably,\nresults show that incorporating contextual domain knowledge significantly enhances the performance despite the choice\nof backbone LLMs.\nFigure 1: Illustration of a Design Activity DSM: (A) Pre-Sequencing; (B) Post-Sequencing\n2\nMethodology\nIn this section, we introduce our proposed LLM-based framework for solving CO problems. The framework is\ndesigned to harness the generative and reasoning powers of LLMs in combination with domain knowledge and objective\nevaluation. The framework begins with the initialization of a solution randomly sampled in the total solution space.\nEach solution is evaluated based on predefined criteria by an evaluator, which quantifies the quality of a solution.\nUsing this evaluation, the framework iteratively updates the solution base through few-shot learning and suggesting\nnew candidates, guided by crafted prompts that include both network information in mathematical form and domain\nknowledge in natural language description. The newly generated solutions are appended into the solution base, together\nwith their evaluation results. When the iteration time is reached, the solution base returns the best one as the final output.\nIn following, we focus on DSM sequencing as a common CO problem to illustrate the pipeline. The framework is\ndepicted in Figure 2.\n2\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nFigure 2: Overview of the Proposed Framework\nInitialization and solution sampling\nThe Solution Base serves as an essential module to (1) enable storage of explored solutions and their evaluation results,\n(2) provide historical solutions to the backend LLM for few-shot learning, and (3) return the top-performing solution\nwhen iteration ends.\nThe initialization involves generating an initial solution that is randomly sampled from the entire solution space. In\nthe DSM sequencing task, a solution represents a complete and non-repetitive sequence of nodes (see Figure 2). This\ninitial solution is then evaluated and added to the Solution Base for future use. In subsequent iterations, we design\na sampling rule that selects Kp top-performance solutions and randomly samples Kq solutions from the remaining\nKn −Kp solutions to form a solution set, where Kn is the total number of solutions in the Solution Base. Kp and Kq\nare adjustable parameters. The obtained solution set is further refined and crafted into prompts.\nLLM-driven optimization using network information and domain knowledge\nIn each iteration of the optimization process, we prompt backend LLM with the following information: (i) Typology\nInformation: These two elements complete the mathematical description of a DSM. It is noteworthy that there\nare multiple equivalent representations for describing a network mathematically, such as an edge list, a dependency\nrelationship list according to node sequence, or an adjacency matrix. In this research, we choose the edge list as the\nrepresentation of the network’s topology and shuffle all edges to avoid any possible bias. (ii) Contextual Domain\nKnowledge: This includes the name of each node and an overall description of the network, which conveys the domain\nknowledge underlying the DSM’s mathematical structure to the LLM. For instance, in an activity DSM, each node\nrepresents the name of an activity in the entire design process. (iii) Meta-instructions: We adopt some frequently used\nprompt engineering strategies [20], including role-playing, task specification, and output format specification. These\nstrategies allow the LLM to follow the guidance to perform reasoning and generate solutions in a specific format. (iv)\nSelected historical solutions: As described in the last section, we obtain a set of up to Kp + Kq solutions through\nsampling from the Solution Base for the LLM to use in few-shot learning.\nOnce receiving inputs described above, the backend LLM combines the network topology information with domain\nknowledge to infer and suggest new solutions. The generated solution must pass the checker, which verifies that all\nnodes are present exactly once in the sequence. Once validated, the solution is then evaluated and appended to the\nSolution Base. The detailed input prompts are included in Appendix 1.\nEvaluation of DSM sequencing solutions\nThe evaluator is used to quantify each newly generated solution. For the DSM sequencing task, the goal is to reorder\nthe rows and columns of the DSM to minimize feedback loops. To achieve this objective, the evaluator calculates the\nnumber of backward dependencies in the corresponding sequence.\n3\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nWe formally describe this problem as follows: Given a sequence s, we can use the edge list of the network to obtain an\nn × n asymmetric adjacency matrix A, representing a directed network of n nodes, where aij is a typical binary entry.\nSpecifically, aij = 1 represents that node i depends on node j. The task is to reorder the n correlated rows and columns\nto minimize the number of entries above the main diagonal in the corresponding adjacency matrix. On this basis, the\nobjective function can be formally expressed as: mins\nPn\ni=1\nPn\nj=i+1 aij, where i < j.\nFor each sequence generated by the LLM (including the initial randomly created one), we calculate its evaluation score\nusing the formula above. In this task, a lower score indicates a higher quality solution.\n3\nExperiments\n3.1\nData\nWe collected four DSM cases for our experiments, which can be categorized into two types [21]: (1) Activity-based\nDSMs, which represent the input-output relationships between different tasks or activities within a project; and (2)\nParameter-based DSMs, which illustrate the relationships among design parameters of a product.\nThe DSM of the Unmanned Combat Aerial Vehicle (UCAV) includes 12 conceptual design activities conducted at\nBoeing [22]. The DSM of the Microfilm Cartridge was derived from Kodak’s Cheetah project including 13 major\ntasks [23]. These two activity-based DSMs were constructed based on interviews with relevant engineers, followed\nby review, verification, and calibration to ensure accuracy. For the parameter-based DSMs, the Heat Exchanger [19]\nand the Automobile Brake System [24], researchers first identified the key components from the product and then\ninterviewed the corresponding designers to define design parameters and establish precedence relationships. The Heat\nExchanger DSM contains 17 components related to core thermal exchange elements, while the Brake System DSM\nincludes 14 main parameters covering braking mechanisms and their dependencies.\nFrom the referential documents, we extracted: (1) the name of each node, (2) the edge list obtained from the adjacency\nmatrix, and (3) the overall description of each network. All data were kept consistent with the original references. The\nspecific data formats are shown in Appendix 1. The characteristics of four DSMs are summarized in Table 1. In general,\nthe node count (N) of DSMs ranges from 12 to 17, and the edge count (E) varies between 32 and 47. The network\ndiameter, representing the longest shortest path between any two nodes, spans from 2 to 7. We also present measures\nsuch as network density and clustering coefficient to highlight the complexity. For instance, the UCAV DSM has a high\nnetwork density of 0.712 and a clustering coefficient of 0.773, suggesting strong interconnections, while measures of\nthe Heat Exchanger DSM indicate a sparser network with more distinct relationships.\nTable 1: Characteristics of Four DSMs\nN\nE\nNetwork\nDiameter\nNetwork\nDensity\nAverage\nDegree\nClustering\nCoefficient\nAverage\nPath Length\nActivity-Based DSMs\nUnmanned Aerial Vehicle [22]\nMicrofilm Cartridge [23]\n12\n47\n2\n0.712\n7.833\n0.773\n1.288\n13\n41\n3\n0.526\n6.308\n0.682\n1.577\nParameter-Based DSMs\nHeat Exchanger [19]\nAutomobile Brake System [24]\n17\n41\n7\n0.302\n4.824\n0.457\n2.397\n14\n32\n4\n0.352\n4.571\n0.414\n1.824\n3.2\nExperiment Setup\nExperimental setting\nIn all our experiments, we ran each method 10 times with different random seeds to ensure robustness. For solution\nsampling, we set Kp = 5 and Kq = 5. For the index of each node, we use a string composed of 5 randomly generated\ncharacters (a mix of numbers and letters) to represent the node uniquely. The maximum number of iterations was set to\n20. We selected Claude-3.5-Sonnet-20241022 as the backbone LLM and kept all other settings of the LLM as their\ndefault values [25].\n4\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nFigure 3: Comparison of Convergence Speed\nComparative methods for benchmarking\nWe consider two types of approaches for benchmarking.\nThe first type is stochastic methods, which rely on probabilistic rules to explore the solution space. We chose the classic\nGenetic Algorithm (GA) for comparison, as it has been successfully applied to various optimization problems [26]. We\nimplemented GA using the DEAP library [27]. We considered three different settings for GA: exploration-focused,\nexploitation-focused, and balanced setting. Specific parameter settings of three variants are detailed in Appendix 2.\nThe second type is deterministic methods, which rank nodes based on a specific measure and then use this ranking for\nreordering. Since the calculation of each measure is deterministic, the resulting solution is consistent. We considered\nfive deterministic methods for comparison, each reordering nodes based on different criterion: (i) Out-In Degree [28]:\nReordering based on the difference between out-degree and in-degree of each node. (ii) Eigenvector [29]: Calculating\nthe values of components in the Perron vector of the adjacency matrix and sorting nodes accordingly. (iii) Walk-based\n(Exponential) [30]: Considering in-depth connectivity patterns by involving the power of the adjacency matrix A. It is\ncalculated by: F(A) = exp(A). (iv) Walk-based (Resolvent) [31]: Calculated by: F(A) = (I −δA)−1, where δ\nrepresents the probability that a message will successfully traverse an edge. We set δ = 0.025 in experiments following\nthe original reference. (v) Visibility [32]: Involving the visibility matrix, showing the dependencies between all system\nelements for all possible path lengths up to the matrix size. It is calculated by: F(A) = Pn\nk=0 Ak, where n represents\nthe number of nodes. The resulting visibility matrix is then binarized. For methods (iii), (iv), and (v), once the F(A)\nis obtained, nodes are reordered by the sum of rows of F(A). If nodes share the same value, they are then reordered\nbased on the sum of columns of F(A).\nTo further investigate the effectiveness of incorporating domain knowledge, we also consider a variant of our proposed\nmethod for comparison. In this variant, all settings are kept the same as in the main method, except that the contextual\nknowledge about nodes and the entire network is removed. Therein, only the topological information of the network\nis engineered into the input prompt, guiding the LLM to search for an optimal solution solely through mathematical\nreasoning.\n4\nResults and Discussion\nWe first compared the convergence speed among our LLM-based methods and the three variants of GA, as shown in\nFigure 3. In GA, due to different parameter settings (e.g., population size, crossover, and mutation rates), each iteration\nexplores multiple unique solutions. In contrast, our methods suggest only one unique solution per iteration. Therefore,\n5\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nwe standardized the iteration time (generation) for GA as the number of unique solutions explored and used this as the\nmeasure for convergence speed. Although the maximum number of generations for each GA run was set to 2,000 (refer\nto Appendix 2), we truncated the comparison to the first 10,000 unique solutions explored for clearer visualization.\nCompared to all three GA settings, our methods show significantly higher convergence speed, reaching lower objective\nvalues faster across all four cases. Overall, our LLM-based methods demonstrate a consistent advantage over GA,\nfinding high-quality solutions multiple orders of magnitude faster in all cases. In each of the four cases, our methods\ncan always identify an initial solution of good quality within the first attempt and continue optimizing the solution\nthrough subsequent iterations, approximating the optimal solution. This demonstrates that LLMs can leverage in-context\nlearning to adapt and improve solutions efficiently.\nMoreover, in Figures 3 (A), (C), and (D), we observe that incorporating domain knowledge enhances the LLM’s\nperformance in both convergence speed and solution quality, as indicated by the red line consistently lying below the\ngrey line. This suggests that LLMs combining semantic reasoning with mathematical reasoning can solve CO problems\nmore effectively and efficiently. It is noteworthy that in the cartridge development case (Figure 3 (B)), both LLM-based\nmethods (with and without knowledge) found the best solution within the first step. Consequently, the two lines overlap,\nindicating identical performance in this case.\nTable 2: Comparison of Solution Quality\nMethods\nActivity-Based DSMs\nParameter-Based DSMs\nUnmanned\nAerial Vehicle\nMicrofilm\nCartridge\nHeat\nExchanger\nAutomobile\nBrake System\nStochastic Methods1\nGA (Exploration-focused setting)\n6.0±0.0\n8.1±0.3\n5.7±1.0\n3.8±0.7\nGA (Exploitation-focused setting)\n7.4±1.5\n9.9±1.4\n7.6±1.4\n6.6±1.7\nGA (Balanced setting)\n6.0±0.0\n8.4±0.5\n6.2±1.2\n4.2±1.2\nDeterministic Methods2\nOut-In Degree [28]\n10.0±0.0\n12.3±0.5\n10.3±0.6\n5.7±0.9\nEigenvector [29]\n15.0±0.0\n13.8±0.7\n13.1±1.7\n11.1±1.4\nWalk-based (Exponential) [30]\n15.0±0.0\n12.0±0.0\n8.0±0.0\n11.0±0.0\nWalk-based (Resolvent) [31]\n9.0±0.0\n12.0±0.0\n8.0±0.0\n11.0±0.0\nVisibility [32]\n25.6±2.2\n8.8±0.7\n6.0±1.3\n3.0±0.0\nLLM-driven Methods (Ours)\nSingle-trial with knowledge\n6.6±0.7\n8.0±0.0\n4.8±0.6\n4.4±0.9\nSingle-trial without knowledge\n11.9±3.4\n8.1±0.3\n5.8±1.1\n5.9±1.4\n5-trial with knowledge\n6.1±0.3\n8.0±0.0\n4.0±0.4\n3.0±0.0\n5-trial without knowledge\n7.5±0.9\n8.0±0.0\n4.9±0.5\n4.0±1.0\n20-trial with knowledge\n6.0±0.0\n8.0±0.0\n3.6±0.5\n3.0±0.0\n20-trial without knowledge\n6.4±0.7\n8.0±0.0\n4.1±0.3\n3.4±0.7\nWe also compared the solution quality among different methods, as presented in Table 2. We experimented with our\nmethods over single-trial, 5-trial, and 20-trial runs to evaluate their effectiveness. Compared to stochastic methods and\ndeterministic methods, our methods significantly outperformed the corresponding benchmarks.\nOn the one hand, since all deterministic methods can be regarded as single-trial approaches (static algorithms), we first\ncompared our single-trial results with those methods. As shown in Table 2, except for the Visibility-based method\n1Consistent with Figure 3, all three GA settings show optimization performance with number of unique sequences explored =\n10,000.\n2All deterministic methods can be regarded as single-trial approaches, as they produce consistent outcomes for identical inputs\nacross repeated executions. However, certain nodes might share the same measures. For these nodes, a random order is applied, and\nthe final statistical evaluation results are obtained from 10 runs.\n6\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nfinding the optimal solution directly in one case (brake system design DSM), our methods outperformed all other\ndeterministic methods across all cases. Even in the brake system case, our 5-trial method matched the effectiveness of\nthe Visibility-based method.\nOn the other hand, when comparing with stochastic algorithms, our 20-trial method consistently found better solutions\nthan benchmarks and achieved the optimal solution in 3 out of 4 cases. It is important to note that our experiments\nonly tested up to 20 iterations. If we were to continue with longer iterations, the heat exchanger design case might also\nreach the optimal solution. Moreover, even with the 5-trial runs, our methods outperformed all three variants of GA\nin 3 out of 4 cases and were close in the UCAV case (6.1±0.3 vs. 6.0±0.0). This demonstrates that our LLM-based\napproach, leveraging contextual domain knowledge and iterative in-context learning, can effectively solve CO problems,\nsuggesting high-quality solutions in a few attempts.\nTable 3: Ablation on the Backbone LLM\n# of Trial\nBackbone LLM\nKnowledge\nActivity-Based DSMs\nParameter-Based DSMs\nUnmanned\nAerial Vehicle\nMicrofilm\nCartridge\nHeat\nExchanger\nAutomobile\nBrake System\n1\nMixtral-7x8B\nwith\n12.5±2.2\n19.5±5.0\n16.1±2.2\n12.1±3.1\nwithout\n15.8±4.6\n17.7±3.3\n16.8±1.9\n16.0±2.3\nLlama3-70B\nwith\n7.6±0.8\n8.3±0.5\n7.9±2.0\n6.1±1.1\nwithout\n12.5±3.8\n10.2±1.2\n10.1±1.8\n7.0±2.2\nGPT-4-Turbo\nwith\n9.6±3.2\n10.0±1.6\n9.2±2.1\n5.0±1.2\nwithout\n13.9±4.3\n11.0±1.5\n12.9±3.1\n7.5±2.7\nClaude-3.5-Sonnet\nwith\n6.6±0.7\n8.0±0.0\n4.8±0.6\n4.4±0.9\nwithout\n11.9±3.4\n8.1±0.3\n5.8±1.1\n5.9±1.4\n5\nMixtral-7x8B\nwith\n10.6±1.4\n13.1±2.7\n13.0±2.1\n9.7±2.5\nwithout\n12.3±2.6\n15.0±2.2\n15.4±2.3\n13.8±1.7\nLlama3-70B\nwith\n6.7±0.5\n8.1±0.3\n5.5±0.9\n5.4±1.1\nwithout\n8.4±1.7\n9.3±0.5\n8.7±1.2\n5.4±1.4\nGPT-4-Turbo\nwith\n6.8±0.6\n8.4±0.5\n6.6±1.3\n4.6±0.9\nwithout\n8.9±0.8\n9.6±0.8\n10.1±2.7\n5.7±1.8\nClaude-3.5-Sonnet\nwith\n6.1±0.3\n8.0±0.0\n4.0±0.4\n3.0±0.0\nwithout\n7.5±0.9\n8.0±0.0\n4.9±0.5\n4.0±1.0\n20\nMixtral-7x8B\nwith\n10.1±1.2\n11.2±2.3\n10.7±1.2\n8.8±2.0\nwithout\n11.3±1.7\n13.0±1.2\n14.0±1.3\n13.1±1.7\nLlama3-70B\nwith\n6.7±0.5\n8.0±0.0\n4.7±0.5\n5.2±1.2\nwithout\n7.4±0.8\n8.8±0.6\n7.7±1.3\n4.6±0.9\nGPT-4-Turbo\nwith\n6.1±0.3\n8.2±0.4\n5.1±0.8\n4.1±0.7\nwithout\n7.3±0.6\n9.1±0.3\n7.7±1.8\n5.0±1.5\nClaude-3.5-Sonnet\nwith\n6.0±0.0\n8.0±0.0\n3.6±0.5\n3.0±0.0\nwithout\n6.4±0.7\n8.0±0.0\n4.1±0.3\n3.4±0.7\nTo evaluate the effect of different backbone LLMs on our proposed method, we conducted an ablation study. In addition\nto Claude-3.5-Sonnet [25], we repeated the experiments in Table 2 using three different backbones: two open-source\nLLMs (Mixtral-7x8B [33] and Llama3-70B [34]) and one closed-source LLM (GPT-4-Turbo [35]), accessed via its API.\nWe report the results for single-trial, 5-trial, and 20-trial runs, and also examine the impact of removing contextual\ndomain knowledge from the prompts. As indicated by Table 3, Claude-3.5-Sonnet, our choice, statistically outperforms\nthe other LLMs across all four cases. Llama3-70B follows as the second top performer in most cases, demonstrating that\nopen-source LLMs can be effective alternatives. This suggests that our framework is compatible with different LLMs.\nWe recommend users or researchers may start with Claude-3.5-Sonnet for optimal performance, but may consider\nopen-source models like Llama for budget or customizability. Furthermore, in all cases, the results across all LLMs\nshow that incorporating contextual domain knowledge consistently improves solution quality compared to the ones\n7\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nwithout knowledge. This finding aligns with our previous observation in Figure 1, highlighting the robustness of our\nmethods and the significant role of domain knowledge in enhancing LLMs’ reasoning performance in such CO tasks\nwith real-world contexts.\n5\nConcluding Remarks\nExperimental results demonstrate that our proposed method, particularly when incorporating contextual domain\nknowledge, offer significant advantages in both convergence speed and solution quality compared to benchmarking\nstochastic and deterministic approaches. Across all experiments and backbone LLMs, the inclusion of domain\nknowledge consistently improved performance. While Claude-3.5-Sonnet is the most effective backbone LLM, open-\nsource models such as Llama3-70B also shows strong performance. Overall, these findings illustrate the capability\nof our proposed method to effectively address DSM sequencing tasks, leveraging both semantic and mathematical\nreasoning of LLMs for real-world combinatorial optimization problems.\nDespite the promising results, there are some limitations. First, the scope of the current experiments could be expanded\nby collecting more diverse DSM cases, including larger and more complex networks, to achieve more robust statistical\nresults. Second, while we focused on DSM sequencing tasks, the effectiveness of our proposed method could be\nexplored across a wider range of real-world combinatorial optimization tasks to assess its generalizability. Third, we\nplan to delve into the intermediate changes in LLM outputs during optimization in future work, which may provide\ngreater interpretability and insights into the LLMs’ reasoning processes.\nIn conclusion, our study highlights the potential of LLMs for combinatorial optimization of design structure matrix,\nshowing that including domain knowledge can significantly enhance the performance of our methods. Future research\ncan build on these findings to further refine LLM-driven methods and extend applications to more complex and diverse\ncombinatorial optimization problems.\nReferences\n[1] Bernhard H. Korte and Jens Vygen. Combinatorial optimization. Springer, 2011.\n[2] Gita Naseri and Mattheos A. G. Koffas. Application of combinatorial optimization strategies in synthetic biology.\nNature Communications, 11:2446, 2020.\n[3] Elias Xidias and Philip Azariadis. Energy efficient motion design and task scheduling for an autonomous vehicle.\nIn Proceedings of the International Conference on Engineering Design (ICED), pages 2853–2862, 2019.\n[4] Michael L. Pinedo. Scheduling. Springer, New York, 2012.\n[5] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean,\nand William Fedus. Emergent abilities of large language models. Transactions on Machine Learning Research,\n2022.\n[6] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang\nWang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on\nevaluation of large language models. ACM Transactions on Intelligent Systems and Technology, pages 1–45, 2024.\n[7] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar,\nEmilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and\nAlhussein Fawzi. Mathematical discoveries from program search with large language models. Nature, pages\n468–475, 2024.\n[8] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large\nlanguage models as optimizers. In The Twelfth International Conference on Learning Representations (ICLR),\n2024.\n[9] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models as evolutionary\noptimizers. In IEEE Congress on Evolutionary Computation (CEC), pages 1–8, 2024.\n[10] Qihao Zhu and Jianxi Luo. Generative transformers for design concept generation. Journal of Computing and\nInformation Science in Engineering, page 041003, 2023.\n[11] Liane Makatura, Michael Foshey, Bohan Wang, Felix Hähnlein, Pingchuan Ma, Bolei Deng, Megan Tjandra-\nsuwita, Andrew Spielberg, Crystal Elaine Owens, Peter Yichen Chen, Allan Zhao, Amy Zhu, Edward Gu Wil\nJ. Norton, Joshua Jacob, Yifei Li, Adriana Schulz, and Wojciech Matusik. Large language models for design and\nmanufacturing. An MIT Exploration of Generative AI, 2024.\n8\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\n[12] Shuo Jiang and Jianxi Luo. Autotriz: Artificial ideation with triz and large language models. In International De-\nsign Engineering Technical Conferences & Computers and Information in Engineering Conference (IDETC/CIE).\nASME, 2024.\n[13] Aoran Mei, Guo-Niu Zhu, Huaxiang Zhang, and Zhongxue Gan. Replanvlm: Replanning robotic tasks with visual\nlanguage models. IEEE Robotics and Automation Letters, 2024.\n[14] Donald V. Steward. The design structure system: A method for managing the design of complex systems. IEEE\nTransactions on Engineering Management, pages 71–74, 1981.\n[15] Young Mi Choi. Effective scheduling of user input during the design process. In Proceedings of the International\nConference on Engineering Design (ICED), pages 116–122, 2011.\n[16] Steven D. Eppinger and Tyson R. Browning. Design structure matrix methods and applications. MIT Press, 2012.\n[17] Steven D. Eppinger, Daniel E. Whitney, Robert P. Smith, and David A. Gebala. A model-based method for\norganizing tasks in product development. Research in Engineering Design, pages 1–13, 1994.\n[18] Yanjun Qian, Jun Lin, Thong Ngee Goh, and Min Xie. A novel approach to dsm-based activity sequencing\nproblem. IEEE Transactions on Engineering Management, pages 688–705, 2011.\n[19] Rafael Amen, Ingvar Rask, and Staffan Sunnersjö. Matching design tasks to knowledge-based software tools:\nWhen intuition does not suffice. In International Design Engineering Technical Conferences and Computers and\nInformation in Engineering Conference (IDETC/CIE), pages 1165–1174, 1999.\n[20] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,\nJunjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan\nLi, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models.\narXiv preprint, 2023. arXiv:2303.18223.\n[21] Tyson R. Browning. Applying the design structure matrix to system decomposition and integration problems: a\nreview and new directions. IEEE Transactions on Engineering Management, pages 292–306, 2001.\n[22] Tyson R. Browning. Modeling and analyzing cost, schedule, and performance in complex system product\ndevelopment. PhD thesis, Massachusetts Institute of Technology, 1998.\n[23] Karl T. Ulrich and Steven D. Eppinger. Product Design and Development. McGraw-Hill, 2016.\n[24] Thomas Andrew Black, Charles H. Fine, and Emanuel M. Sachs. A method for systems design using prece-\ndence relationships: An application to automotive brake systems. Working Paper of Sloan School of Manage-\nment,Massachusetts Institute of Technology, 1990.\n[25] Anthropic. Introducing computer use, a new claude 3.5 sonnet, and claude 3.5 haiku, 2024. Accessed Oct 2024,\nhttps://www.anthropic.com/news/3-5-models-and-computer-use.\n[26] Bushra Alhijawi and Arafat Awajan. Genetic algorithms: Theory, genetic operators, solutions, and applications.\nEvolutionary Intelligence, pages 1245–1256, 2024.\n[27] Felix-Antoine Fortin, Francois-Michel De Rainville, Marc-Andre Gardner, Marc Parizeau, and Christian Gagne.\nDeap: Evolutionary algorithms made easy. Journal of Machine Learning Research, pages 2171–2175, 2012.\n[28] Jonathan J. Crofts and Desmond J. Higham. Googling the brain: Discovering hierarchical and asymmetric network\nstructures, with applications in neuroscience. Internet Mathematics, pages 233–254, 2011.\n[29] Erik Dietzenbacher. The measurement of interindustry linkages: Key sectors in the netherlands. Economic\nModelling, pages 419–437, 1992.\n[30] Ernesto Estrada and Juan A. Rodriguez-Velazquez. Subgraph centrality in complex networks. Physical Review\nE-Statistical, Nonlinear, and Soft Matter Physics, 56(103), 2005.\n[31] Ernesto Estrada and Desmond J. Higham. Network properties revealed through matrix functions. SIAM Review,\npages 696–714, 2010.\n[32] Alan MacCormack, Carliss Baldwin, and John Rusnak. Exploring the duality between product and organizational\narchitectures: A test of the “mirroring” hypothesis. Research Policy, pages 1309–1324, 2012.\n[33] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, and\nDevendra Singh Chaplot. Mixtral of experts. arXiv preprint, 2024. arXiv:2401.04088.\n[34] Meta.\nIntroducing\nmeta\nllama\n3:\nThe\nmost\ncapable\nopenly\navailable\nllm\nto\ndate,\n2024.\nhttps://ai.meta.com/blog/meta-llama-3/.\n[35] OpenAI. Gpt-4 turbo, 2024. https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo.\n9\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nAppendix 1. Full prompts\nPrompt for our proposed method (the main method):\nYou are an expert in the domain of combinational optimization.\nPlease assist me to find an optimal sequential order that minimizes feedback cycles in the dependency\nnetwork described below. Your task is to propose a new order that differs from previous attempts and has fewer\nfeedback cycles than any listed.\n<Description of the entire Network> {network_description} </Description of the entire\nNetwork>\n<Nodes with Descriptions> {node_list_with_description} </Nodes with Descriptions>\n<Edges> {edge_list} </Edges>\nBelow are some previous sequential orders arranged in descending order of feedback cycles (lower is\nbetter): {selected_historical_solutions}\nPlease suggest a new order that:\n- Is different from all prior orders.\n- Has fewer feedback cycles than any previous order.\n- Covers all nodes exactly once.\n- Starts with <order> and ends with </order>.\n- You can use the descriptions of nodes and networks to support your suggestion.\nOutput Format:\n<order> ......\n</order>\nPlease provide only the order and nothing else.\nPrompt for our proposed method (removing contextual domain knowledge):\nYou are an expert in the domain of combinational optimization.\nPlease assist me to find an optimal sequential order that minimizes feedback cycles in the dependency\nnetwork described below. Your task is to propose a new order that differs from previous attempts and has fewer\nfeedback cycles than any listed.\n<Nodes> {node_list} </Nodes>\n<Edges> {edge_list} </Edges>\nBelow are some previous sequential orders arranged in descending order of feedback cycles (lower is\nbetter): {selected_historical_solutions}\nPlease suggest a new order that:\n- Is different from all prior orders.\n- Has fewer feedback cycles than any previous order.\n- Covers all nodes exactly once.\n- Starts with <order> and ends with </order>.\nOutput Format:\n<order> ......\n</order>\nPlease provide only the order and nothing else.\n10\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nExample of {network_description} in the UCAV design activity DSM case:\nThis network represents the dependency relationships among conceptual design\nactivities for UCAV development at Boeing.\nEach node corresponds to a specific task\nor analysis, and directed edges indicate the prerequisite relationships between these\ntasks.\nNodes:\nEach node is a task or analysis in the conceptual design process.\nEdges:\nDirected edges show the prerequisite relationships between these tasks and\nanalyses.\nExample of {node_list_with_description} in the UCAV design activity DSM case:\n[\n{’id’:\n’lzOtR’, ’name’:\n’Create Configuration Concepts’},\n{’id’:\n’yLlKi’, ’name’:\n’Prepare UCAV Conceptual DR&O’},\n{’id’:\n’Swvi2’, ’name’:\n’Prepare 3-View Drawing & Geometry Data’},\n{’id’:\n’CDcxF’, ’name’:\n’Perform Weights Analyses & Evaluation’},\n{’id’:\n’0KGDm’, ’name’:\n’Perform Aerodynamics Analyses & Evaluation’},\n{’id’:\n’4wHtv’, ’name’:\n’Perform Multidisciplinary Analyses & Evaluation’},\n{’id’:\n’AgIBP’, ’name’:\n’Prepare & Distribute Choice Config.\nData Set’},\n{’id’:\n’gRtHi’, ’name’:\n’Perform S&C Characteristics Analyses & Eval.’},\n{’id’:\n’GV9RJ’, ’name’:\n’Make Concept Assessment and Variant Decisions’},\n{’id’:\n’I1j2m’, ’name’:\n’Perform Performance Analyses & Evaluation’},\n{’id’:\n’Vzzm7’, ’name’:\n’Perform Propulsion Analyses & Evaluation’},\n{’id’:\n’B0BFG’, ’name’:\n’Perform Mechanical & Electrical Analyses & Eval.’}\n]\nExample of {node_list} in the UCAV design activity DSM case:\n[’lzOtR’, ’yLlKi’, ’Swvi2’, ’CDcxF’, ’0KGDm’, ’4wHtv’, ’AgIBP’, ’gRtHi’, ’GV9RJ’,\n’I1j2m’, ’Vzzm7’, ’B0BFG’]\nExample of {edge_list} in the UCAV design activity DSM case:\n[\n{’dependent’:\n’0KGDm’, ’predecessor’:\n’Swvi2’},\n{’dependent’:\n’AgIBP’, ’predecessor’:\n’lzOtR’},\n{’dependent’:\n’0KGDm’, ’predecessor’:\n’yLlKi’},\n{’dependent’:\n’Swvi2’, ’predecessor’:\n’lzOtR’},\n...\n]\nExample of {selected_historical_solutions} in the UCAV design activity DSM case:\n[\n{’solution’:\n’lzOtR, yLlKi, GV9RJ, AgIBP, B0BFG, Vzzm7, Swvi2, CDcxF, 0KGDm, I1j2m,\ngRtHi, 4wHtv’, ’score’:\n15.0},\n{’solution’:\n’B0BFG, yLlKi, Vzzm7, lzOtR, Swvi2, CDcxF, AgIBP, 0KGDm, GV9RJ, I1j2m,\ngRtHi, 4wHtv’, ’score’:\n13.0},\n...\n]\n11\n\nLARGE LANGUAGE MODELS FOR COMBINATORIAL OPTIMIZATION OF DSM\nAppendix 2. Parameter settings for three variants of GA\nIn all three variants of the GA used for DSM sequencing tasks, we employed the following shared settings: the number\nof generations was set to 2,000, with a selection mechanism using tournament selection and mutation using shuffled\nindexes. The GA was implemented using the DEAP library with standard configurations for initial population generation\n[27]. The different configurations for each GA setting are shown in the table below. The meaning of each parameter\nalso refers to [27].\nPopulation\nIndividual Mutation\nProbability\nTournament\nSize\nCrossover\nProbability\nMutation\nProbability\nExploration-focused\n50\n0.05\n5\n0.6\n0.4\nExploitation-focused\n10\n0.01\n20\n0.9\n0.1\nBalanced\n20\n0.02\n10\n0.7\n0.3\n12",
    "pdf_filename": "Large_Language_Models_for_Combinatorial_Optimization_of_Design_Structure_Matrix.pdf"
}