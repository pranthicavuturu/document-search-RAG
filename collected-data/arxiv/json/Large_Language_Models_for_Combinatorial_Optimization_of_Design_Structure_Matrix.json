{
    "title": "LARGE LANGUAGE MODELS FOR COMBINATORIAL",
    "abstract": "Combinatorialoptimization(CO)isessentialforimprovingefficiencyandperformanceinengineering applications. Ascomplexityincreaseswithlargerproblemsizesandmoreintricatedependencies, identifying the optimal solution become challenging. When it comes to real-world engineering problems,algorithmsbasedonpuremathematicalreasoningarelimitedandincapabletocapturethe contextualnuancesnecessaryforoptimization. ThisstudyexploresthepotentialofLargeLanguage Models (LLMs) in solving engineering CO problems by leveraging their reasoning power and contextualknowledge. WeproposeanovelLLM-basedframeworkthatintegratesnetworktopology anddomainknowledgetooptimizethesequencingofDesignStructureMatrix(DSM)—acommon COproblem. OurexperimentsonvariousDSMcasesdemonstratethattheproposedmethodachieves fasterconvergenceandhighersolutionqualitythanbenchmarkmethods. Moreover,resultsshow that incorporating contextual domain knowledge significantly improves performance despite the choiceofLLMs. ThesefindingshighlightthepotentialofLLMstoaddresscomplexCOproblemsby combiningsemanticandmathematicalreasoning. Thisapproachpavesthewayforanewparadigm inreal-worldengineeringcombinatorialoptimization.",
    "body": "LARGE LANGUAGE MODELS FOR COMBINATORIAL\nOPTIMIZATION OF DESIGN STRUCTURE MATRIX\nShuoJiang∗ MinXie\nDepartmentofSystemsEngineering DepartmentofSystemsEngineering\nCityUniversityofHongKong CityUniversityofHongKong\n83TatCheeAve,KowloonTong,HongKong 83TatCheeAve,KowloonTong,HongKong\nshuo.jiang@cityu.edu.hk xiemin@cityu.edu.hk\nJianxiLuo\nDepartmentofSystemsEngineering\nCityUniversityofHongKong\n83TatCheeAve,KowloonTong,HongKong\njianxi.luo@cityu.edu.hk\nAPreprint,Version: Nov19,2024\nABSTRACT\nCombinatorialoptimization(CO)isessentialforimprovingefficiencyandperformanceinengineering\napplications. Ascomplexityincreaseswithlargerproblemsizesandmoreintricatedependencies,\nidentifying the optimal solution become challenging. When it comes to real-world engineering\nproblems,algorithmsbasedonpuremathematicalreasoningarelimitedandincapabletocapturethe\ncontextualnuancesnecessaryforoptimization. ThisstudyexploresthepotentialofLargeLanguage\nModels (LLMs) in solving engineering CO problems by leveraging their reasoning power and\ncontextualknowledge. WeproposeanovelLLM-basedframeworkthatintegratesnetworktopology\nanddomainknowledgetooptimizethesequencingofDesignStructureMatrix(DSM)—acommon\nCOproblem. OurexperimentsonvariousDSMcasesdemonstratethattheproposedmethodachieves\nfasterconvergenceandhighersolutionqualitythanbenchmarkmethods. Moreover,resultsshow\nthat incorporating contextual domain knowledge significantly improves performance despite the\nchoiceofLLMs. ThesefindingshighlightthepotentialofLLMstoaddresscomplexCOproblemsby\ncombiningsemanticandmathematicalreasoning. Thisapproachpavesthewayforanewparadigm\ninreal-worldengineeringcombinatorialoptimization.\nKeywords Large Language Models · Combinatorial Optimization · Artificial Intelligence · Knowledge-based\nReasoning·DesignStructureMatrix·SystemsEngineering\n1 Introduction\nCombinatorial optimization (CO) problems are ubiquitous across fields, where finding an optimal solution from a\nfinitesetoftendrivesimprovementsinefficiency,cost,andperformance[1]. Forinstance,applicationssuchasDNA\nbarcodingandDNAassemblyinsyntheticbiology[2],aswellasjobschedulinginmanufacturing[3],relyheavilyon\neffectiveCOsolutions. However,duetotheirNP-hardnature,theseproblemspresentsubstantialchallenges,especially\nascomplexityincreaseswithlargerproblemsizesandmoreintricatedependencies. Traditionally,COproblemsin\nengineeringareusuallyapproachedthroughthefollowingprocess: theproblemisfirstmodelledmathematically,then\nsolvedusingspecificalgorithmsorheuristics,andfinallyinterpretedwithinthecontextofpracticalengineering[4].\n∗Commentsarewelcome:shuojiangcn@gmail.com\n4202\nvoN\n91\n]EC.sc[\n1v17521.1142:viXra\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nThisseparationofproblem-solvingandinterpretationstagesislimitedandincapabletocapturethecontextualnuances\nnecessaryforoptimizationofreal-worldproblems.\nRecentadvancementsinLargeLanguageModels(LLMs)havedemonstratedtheirpowerfulcapabilitiesinnatural\nlanguage generation, semantic understanding, instruction following, and complex reasoning [5, 6]. Furthermore,\npioneeringstudieshaveshownthatLLMscanbeusedforcontinuousandconcreteoptimization[7,8,9]. Forinstance,\nresearchersfromDeepMindutilizedLLMsasoptimizersandevaluatedtheireffectivenessonclassicCOproblems[8],\nsuchasTravelingSalesmanProblems(TSP).Additionally,priorstudiesalsohighlightthatLLMspossessextensive\ndomainknowledgepretrainedacrossawiderangeofengineering-relateddata,whichenhancestheirapplicabilityin\nengineeringfields[10,11,12,13]. Therefore,theabilityofLLMstocombinemathematicalandsemanticreasoning,\nalongwiththeirpossessionofextensiveknowledge,motivatedustoexploretheirpotentialforsolvingengineering\nCOproblemswhileintegratingcontextualdomainknowledgerelevanttotheirnetworktypology. Ourhypothesesare:\n(1)LLMscanbeeffectivelyappliedtosolveCOproblemsinengineering,and(2)incorporatingcontextualdomain\nknowledgecanfurtherenhanceLLMperformancebysupportingmathematicalreasoningwithsemanticinsights. This\nparadigm,whichleveragesbothsemanticandmathematicalreasoning,introducesanovelapproachtocombinatorial\noptimizationthattraditionalpuremathematicalmethodscannotachieveforempiricalproblems. Onthisbasis, we\nproposeanovelLLM-basedframeworkthatintegratesbothnetworktopologyanddomaincontextintotheoptimization\nprocess.\nToevaluateourproposedmethod,wefocusontheDesignStructureMatrix(DSM)sequencingtask,asanexample\nofCOproblems. DSMisamodellingtoolinengineeringdesign,whichrepresentsdependencyrelationshipsamong\ntasksorcomponentswithinasystem[14]. ReorderingthenodesequenceofDSMscansignificantlyreducefeedback\nloopsandimprovemodularization[15,16]. TheDSMsequencingisalsoanNP-hardproblem,andtraditionalmethods\ntypicallyapproachitusingheuristics-basedalgorithms[17,18]. Figure1illustratesadesignactivityDSMbeforeand\naftersequencing[19]. Inthispaper,weconductextensiveexperimentsonvariousDSMcasestodemonstratethatour\nLLM-basedmethodachievesbetterconvergencespeedandsolutionqualitycomparedtobenchmarkmethods. Notably,\nresultsshowthatincorporatingcontextualdomainknowledgesignificantlyenhancestheperformancedespitethechoice\nofbackboneLLMs.\nFigure1: IllustrationofaDesignActivityDSM:(A)Pre-Sequencing;(B)Post-Sequencing\n2 Methodology\nIn this section, we introduce our proposed LLM-based framework for solving CO problems. The framework is\ndesignedtoharnessthegenerativeandreasoningpowersofLLMsincombinationwithdomainknowledgeandobjective\nevaluation. Theframeworkbeginswiththeinitializationofasolutionrandomlysampledinthetotalsolutionspace.\nEach solution is evaluated based on predefined criteria by an evaluator, which quantifies the quality of a solution.\nUsingthisevaluation,theframeworkiterativelyupdatesthesolutionbasethroughfew-shotlearningandsuggesting\nnewcandidates,guidedbycraftedpromptsthatincludebothnetworkinformationinmathematicalformanddomain\nknowledgeinnaturallanguagedescription. Thenewlygeneratedsolutionsareappendedintothesolutionbase,together\nwiththeirevaluationresults. Whentheiterationtimeisreached,thesolutionbasereturnsthebestoneasthefinaloutput.\nInfollowing,wefocusonDSMsequencingasacommonCOproblemtoillustratethepipeline. Theframeworkis\ndepictedinFigure2.\n2\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nFigure2: OverviewoftheProposedFramework\nInitializationandsolutionsampling\nTheSolutionBaseservesasanessentialmoduleto(1)enablestorageofexploredsolutionsandtheirevaluationresults,\n(2)providehistoricalsolutionstothebackendLLMforfew-shotlearning,and(3)returnthetop-performingsolution\nwheniterationends.\nTheinitializationinvolvesgeneratinganinitialsolutionthatisrandomlysampledfromtheentiresolutionspace. In\ntheDSMsequencingtask,asolutionrepresentsacompleteandnon-repetitivesequenceofnodes(seeFigure2). This\ninitialsolutionisthenevaluatedandaddedtotheSolutionBaseforfutureuse. Insubsequentiterations,wedesign\nasamplingrulethatselectsK top-performancesolutionsandrandomlysamplesK solutionsfromtheremaining\np q\nK −K solutionstoformasolutionset,whereK isthetotalnumberofsolutionsintheSolutionBase. K andK\nn p n p q\nareadjustableparameters. Theobtainedsolutionsetisfurtherrefinedandcraftedintoprompts.\nLLM-drivenoptimizationusingnetworkinformationanddomainknowledge\nIneachiterationoftheoptimizationprocess,wepromptbackendLLMwiththefollowinginformation: (i)Typology\nInformation: These two elements complete the mathematical description of a DSM. It is noteworthy that there\naremultipleequivalentrepresentationsfordescribinganetworkmathematically,suchasanedgelist,adependency\nrelationshiplistaccordingtonodesequence,oranadjacencymatrix. Inthisresearch,wechoosetheedgelistasthe\nrepresentationofthenetwork’stopologyandshufflealledgestoavoidanypossiblebias. (ii)ContextualDomain\nKnowledge: Thisincludesthenameofeachnodeandanoveralldescriptionofthenetwork,whichconveysthedomain\nknowledgeunderlyingtheDSM’smathematicalstructuretotheLLM.Forinstance,inanactivityDSM,eachnode\nrepresentsthenameofanactivityintheentiredesignprocess. (iii)Meta-instructions: Weadoptsomefrequentlyused\npromptengineeringstrategies[20],includingrole-playing,taskspecification,andoutputformatspecification. These\nstrategiesallowtheLLMtofollowtheguidancetoperformreasoningandgeneratesolutionsinaspecificformat. (iv)\nSelectedhistoricalsolutions: Asdescribedinthelastsection,weobtainasetofuptoK +K solutionsthrough\np q\nsamplingfromtheSolutionBasefortheLLMtouseinfew-shotlearning.\nOncereceivinginputsdescribedabove,thebackendLLMcombinesthenetworktopologyinformationwithdomain\nknowledgetoinferandsuggestnewsolutions. Thegeneratedsolutionmustpassthechecker,whichverifiesthatall\nnodesarepresentexactlyonceinthesequence. Oncevalidated,thesolutionisthenevaluatedandappendedtothe\nSolutionBase. ThedetailedinputpromptsareincludedinAppendix1.\nEvaluationofDSMsequencingsolutions\nTheevaluatorisusedtoquantifyeachnewlygeneratedsolution. FortheDSMsequencingtask,thegoalistoreorder\ntherowsandcolumnsoftheDSMtominimizefeedbackloops. Toachievethisobjective,theevaluatorcalculatesthe\nnumberofbackwarddependenciesinthecorrespondingsequence.\n3\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nWeformallydescribethisproblemasfollows: Givenasequences,wecanusetheedgelistofthenetworktoobtainan\nn×nasymmetricadjacencymatrixA,representingadirectednetworkofnnodes,wherea isatypicalbinaryentry.\nij\nSpecifically,a =1representsthatnodeidependsonnodej. Thetaskistoreorderthencorrelatedrowsandcolumns\nij\ntominimizethenumberofentriesabovethemaindiagonalinthecorrespondingadjacencymatrix. Onthisbasis,the\nobjectivefunctioncanbeformallyexpressedas: min\n(cid:80)n (cid:80)n\na ,wherei<j.\ns i=1 j=i+1 ij\nForeachsequencegeneratedbytheLLM(includingtheinitialrandomlycreatedone),wecalculateitsevaluationscore\nusingtheformulaabove. Inthistask,alowerscoreindicatesahigherqualitysolution.\n3 Experiments\n3.1 Data\nWecollectedfourDSMcasesforourexperiments,whichcanbecategorizedintotwotypes[21]: (1)Activity-based\nDSMs,whichrepresenttheinput-outputrelationshipsbetweendifferenttasksoractivitieswithinaproject;and(2)\nParameter-basedDSMs,whichillustratetherelationshipsamongdesignparametersofaproduct.\nTheDSMoftheUnmannedCombatAerialVehicle(UCAV)includes12conceptualdesignactivitiesconductedat\nBoeing[22]. TheDSMoftheMicrofilmCartridgewasderivedfromKodak’sCheetahprojectincluding13major\ntasks[23]. Thesetwoactivity-basedDSMswereconstructedbasedoninterviewswithrelevantengineers,followed\nbyreview,verification,andcalibrationtoensureaccuracy. Fortheparameter-basedDSMs,theHeatExchanger[19]\nandtheAutomobileBrakeSystem[24], researchersfirstidentifiedthekeycomponentsfromtheproductandthen\ninterviewedthecorrespondingdesignerstodefinedesignparametersandestablishprecedencerelationships. TheHeat\nExchangerDSMcontains17componentsrelatedtocorethermalexchangeelements,whiletheBrakeSystemDSM\nincludes14mainparameterscoveringbrakingmechanismsandtheirdependencies.\nFromthereferentialdocuments,weextracted: (1)thenameofeachnode,(2)theedgelistobtainedfromtheadjacency\nmatrix,and(3)theoveralldescriptionofeachnetwork. Alldatawerekeptconsistentwiththeoriginalreferences. The\nspecificdataformatsareshowninAppendix1. ThecharacteristicsoffourDSMsaresummarizedinTable1. Ingeneral,\nthenodecount(N)ofDSMsrangesfrom12to17,andtheedgecount(E)variesbetween32and47. Thenetwork\ndiameter,representingthelongestshortestpathbetweenanytwonodes,spansfrom2to7. Wealsopresentmeasures\nsuchasnetworkdensityandclusteringcoefficienttohighlightthecomplexity. Forinstance,theUCAVDSMhasahigh\nnetworkdensityof0.712andaclusteringcoefficientof0.773,suggestingstronginterconnections,whilemeasuresof\ntheHeatExchangerDSMindicateasparsernetworkwithmoredistinctrelationships.\nTable1: CharacteristicsofFourDSMs\nNetwork Network Average Clustering Average\nN E\nDiameter Density Degree Coefficient PathLength\nActivity-BasedDSMs\n12 47 2 0.712 7.833 0.773 1.288\nUnmannedAerialVehicle[22]\nMicrofilmCartridge[23] 13 41 3 0.526 6.308 0.682 1.577\nParameter-BasedDSMs\n17 41 7 0.302 4.824 0.457 2.397\nHeatExchanger[19]\nAutomobileBrakeSystem[24] 14 32 4 0.352 4.571 0.414 1.824\n3.2 ExperimentSetup\nExperimentalsetting\nInallourexperiments,weraneachmethod10timeswithdifferentrandomseedstoensurerobustness. Forsolution\nsampling,wesetK =5andK =5. Fortheindexofeachnode,weuseastringcomposedof5randomlygenerated\np q\ncharacters(amixofnumbersandletters)torepresentthenodeuniquely. Themaximumnumberofiterationswassetto\n20. WeselectedClaude-3.5-Sonnet-20241022asthebackboneLLMandkeptallothersettingsoftheLLMastheir\ndefaultvalues[25].\n4\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nFigure3: ComparisonofConvergenceSpeed\nComparativemethodsforbenchmarking\nWeconsidertwotypesofapproachesforbenchmarking.\nThefirsttypeisstochasticmethods,whichrelyonprobabilisticrulestoexplorethesolutionspace. Wechosetheclassic\nGeneticAlgorithm(GA)forcomparison,asithasbeensuccessfullyappliedtovariousoptimizationproblems[26]. We\nimplementedGAusingtheDEAPlibrary[27]. WeconsideredthreedifferentsettingsforGA:exploration-focused,\nexploitation-focused,andbalancedsetting. SpecificparametersettingsofthreevariantsaredetailedinAppendix2.\nThesecondtypeisdeterministicmethods,whichranknodesbasedonaspecificmeasureandthenusethisrankingfor\nreordering. Sincethecalculationofeachmeasureisdeterministic,theresultingsolutionisconsistent. Weconsidered\nfivedeterministicmethodsforcomparison,eachreorderingnodesbasedondifferentcriterion: (i)Out-InDegree[28]:\nReorderingbasedonthedifferencebetweenout-degreeandin-degreeofeachnode. (ii)Eigenvector[29]: Calculating\nthevaluesofcomponentsinthePerronvectoroftheadjacencymatrixandsortingnodesaccordingly. (iii)Walk-based\n(Exponential)[30]: Consideringin-depthconnectivitypatternsbyinvolvingthepoweroftheadjacencymatrixA.Itis\ncalculatedby: F(A) = exp(A). (iv)Walk-based(Resolvent)[31]: Calculatedby: F(A) = (I−δA)−1,whereδ\nrepresentstheprobabilitythatamessagewillsuccessfullytraverseanedge. Wesetδ =0.025inexperimentsfollowing\ntheoriginalreference. (v)Visibility[32]: Involvingthevisibilitymatrix,showingthedependenciesbetweenallsystem\nelementsforallpossiblepathlengthsuptothematrixsize. Itiscalculatedby: F(A)=(cid:80)n Ak,wherenrepresents\nk=0\nthenumberofnodes. Theresultingvisibilitymatrixisthenbinarized. Formethods(iii),(iv),and(v),oncetheF(A)\nisobtained,nodesarereorderedbythesumofrowsofF(A). Ifnodessharethesamevalue,theyarethenreordered\nbasedonthesumofcolumnsofF(A).\nTofurtherinvestigatetheeffectivenessofincorporatingdomainknowledge,wealsoconsideravariantofourproposed\nmethodforcomparison. Inthisvariant,allsettingsarekeptthesameasinthemainmethod,exceptthatthecontextual\nknowledgeaboutnodesandtheentirenetworkisremoved. Therein,onlythetopologicalinformationofthenetwork\nisengineeredintotheinputprompt,guidingtheLLMtosearchforanoptimalsolutionsolelythroughmathematical\nreasoning.\n4 ResultsandDiscussion\nWefirstcomparedtheconvergencespeedamongourLLM-basedmethodsandthethreevariantsofGA,asshownin\nFigure3. InGA,duetodifferentparametersettings(e.g.,populationsize,crossover,andmutationrates),eachiteration\nexploresmultipleuniquesolutions. Incontrast,ourmethodssuggestonlyoneuniquesolutionperiteration. Therefore,\n5\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nwestandardizedtheiterationtime(generation)forGAasthenumberofuniquesolutionsexploredandusedthisasthe\nmeasureforconvergencespeed. AlthoughthemaximumnumberofgenerationsforeachGArunwassetto2,000(refer\ntoAppendix2),wetruncatedthecomparisontothefirst10,000uniquesolutionsexploredforclearervisualization.\nComparedtoallthreeGAsettings,ourmethodsshowsignificantlyhigherconvergencespeed,reachinglowerobjective\nvaluesfasteracrossallfourcases. Overall, ourLLM-basedmethodsdemonstrateaconsistentadvantageoverGA,\nfindinghigh-qualitysolutionsmultipleordersofmagnitudefasterinallcases. Ineachofthefourcases,ourmethods\ncanalwaysidentifyaninitialsolutionofgoodqualitywithinthefirstattemptandcontinueoptimizingthesolution\nthroughsubsequentiterations,approximatingtheoptimalsolution.ThisdemonstratesthatLLMscanleveragein-context\nlearningtoadaptandimprovesolutionsefficiently.\nMoreover, in Figures 3 (A), (C), and (D), we observe that incorporating domain knowledge enhances the LLM’s\nperformanceinbothconvergencespeedandsolutionquality,asindicatedbytheredlineconsistentlylyingbelowthe\ngreyline. ThissuggeststhatLLMscombiningsemanticreasoningwithmathematicalreasoningcansolveCOproblems\nmoreeffectivelyandefficiently. Itisnoteworthythatinthecartridgedevelopmentcase(Figure3(B)),bothLLM-based\nmethods(withandwithoutknowledge)foundthebestsolutionwithinthefirststep. Consequently,thetwolinesoverlap,\nindicatingidenticalperformanceinthiscase.\nTable2: ComparisonofSolutionQuality\nMethods Activity-BasedDSMs Parameter-BasedDSMs\nUnmanned Microfilm Heat Automobile\nAerialVehicle Cartridge Exchanger BrakeSystem\nStochasticMethods1\nGA(Exploration-focusedsetting) 6.0±0.0 8.1±0.3 5.7±1.0 3.8±0.7\nGA(Exploitation-focusedsetting) 7.4±1.5 9.9±1.4 7.6±1.4 6.6±1.7\nGA(Balancedsetting) 6.0±0.0 8.4±0.5 6.2±1.2 4.2±1.2\nDeterministicMethods2\nOut-InDegree[28] 10.0±0.0 12.3±0.5 10.3±0.6 5.7±0.9\nEigenvector[29] 15.0±0.0 13.8±0.7 13.1±1.7 11.1±1.4\nWalk-based(Exponential)[30] 15.0±0.0 12.0±0.0 8.0±0.0 11.0±0.0\nWalk-based(Resolvent)[31] 9.0±0.0 12.0±0.0 8.0±0.0 11.0±0.0\nVisibility[32] 25.6±2.2 8.8±0.7 6.0±1.3 3.0±0.0\nLLM-drivenMethods(Ours)\nSingle-trialwithknowledge 6.6±0.7 8.0±0.0 4.8±0.6 4.4±0.9\nSingle-trialwithoutknowledge 11.9±3.4 8.1±0.3 5.8±1.1 5.9±1.4\n5-trialwithknowledge 6.1±0.3 8.0±0.0 4.0±0.4 3.0±0.0\n5-trialwithoutknowledge 7.5±0.9 8.0±0.0 4.9±0.5 4.0±1.0\n20-trialwithknowledge 6.0±0.0 8.0±0.0 3.6±0.5 3.0±0.0\n20-trialwithoutknowledge 6.4±0.7 8.0±0.0 4.1±0.3 3.4±0.7\nWealsocomparedthesolutionqualityamongdifferentmethods,aspresentedinTable2. Weexperimentedwithour\nmethodsoversingle-trial,5-trial,and20-trialrunstoevaluatetheireffectiveness. Comparedtostochasticmethodsand\ndeterministicmethods,ourmethodssignificantlyoutperformedthecorrespondingbenchmarks.\nOntheonehand,sincealldeterministicmethodscanberegardedassingle-trialapproaches(staticalgorithms),wefirst\ncomparedoursingle-trialresultswiththosemethods. AsshowninTable2,exceptfortheVisibility-basedmethod\n1ConsistentwithFigure3,allthreeGAsettingsshowoptimizationperformancewithnumberofuniquesequencesexplored=\n10,000.\n2Alldeterministicmethodscanberegardedassingle-trialapproaches,astheyproduceconsistentoutcomesforidenticalinputs\nacrossrepeatedexecutions.However,certainnodesmightsharethesamemeasures.Forthesenodes,arandomorderisapplied,and\nthefinalstatisticalevaluationresultsareobtainedfrom10runs.\n6\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nfinding the optimal solution directly in one case (brake system design DSM), our methods outperformed all other\ndeterministicmethodsacrossallcases. Eveninthebrakesystemcase,our5-trialmethodmatchedtheeffectivenessof\ntheVisibility-basedmethod.\nOntheotherhand,whencomparingwithstochasticalgorithms,our20-trialmethodconsistentlyfoundbettersolutions\nthanbenchmarksandachievedtheoptimalsolutionin3outof4cases. Itisimportanttonotethatourexperiments\nonlytestedupto20iterations. Ifweweretocontinuewithlongeriterations,theheatexchangerdesigncasemightalso\nreachtheoptimalsolution. Moreover,evenwiththe5-trialruns,ourmethodsoutperformedallthreevariantsofGA\nin3outof4casesandwerecloseintheUCAVcase(6.1±0.3vs. 6.0±0.0). ThisdemonstratesthatourLLM-based\napproach,leveragingcontextualdomainknowledgeanditerativein-contextlearning,caneffectivelysolveCOproblems,\nsuggestinghigh-qualitysolutionsinafewattempts.\nTable3: AblationontheBackboneLLM\nActivity-BasedDSMs Parameter-BasedDSMs\n#ofTrial BackboneLLM Knowledge\nUnmanned Microfilm Heat Automobile\nAerialVehicle Cartridge Exchanger BrakeSystem\nwith 12.5±2.2 19.5±5.0 16.1±2.2 12.1±3.1\nMixtral-7x8B\nwithout 15.8±4.6 17.7±3.3 16.8±1.9 16.0±2.3\nwith 7.6±0.8 8.3±0.5 7.9±2.0 6.1±1.1\nLlama3-70B\nwithout 12.5±3.8 10.2±1.2 10.1±1.8 7.0±2.2\n1\nwith 9.6±3.2 10.0±1.6 9.2±2.1 5.0±1.2\nGPT-4-Turbo\nwithout 13.9±4.3 11.0±1.5 12.9±3.1 7.5±2.7\nwith 6.6±0.7 8.0±0.0 4.8±0.6 4.4±0.9\nClaude-3.5-Sonnet\nwithout 11.9±3.4 8.1±0.3 5.8±1.1 5.9±1.4\nwith 10.6±1.4 13.1±2.7 13.0±2.1 9.7±2.5\nMixtral-7x8B\nwithout 12.3±2.6 15.0±2.2 15.4±2.3 13.8±1.7\nwith 6.7±0.5 8.1±0.3 5.5±0.9 5.4±1.1\nLlama3-70B\nwithout 8.4±1.7 9.3±0.5 8.7±1.2 5.4±1.4\n5\nwith 6.8±0.6 8.4±0.5 6.6±1.3 4.6±0.9\nGPT-4-Turbo\nwithout 8.9±0.8 9.6±0.8 10.1±2.7 5.7±1.8\nwith 6.1±0.3 8.0±0.0 4.0±0.4 3.0±0.0\nClaude-3.5-Sonnet\nwithout 7.5±0.9 8.0±0.0 4.9±0.5 4.0±1.0\nwith 10.1±1.2 11.2±2.3 10.7±1.2 8.8±2.0\nMixtral-7x8B\nwithout 11.3±1.7 13.0±1.2 14.0±1.3 13.1±1.7\nwith 6.7±0.5 8.0±0.0 4.7±0.5 5.2±1.2\nLlama3-70B\nwithout 7.4±0.8 8.8±0.6 7.7±1.3 4.6±0.9\n20\nwith 6.1±0.3 8.2±0.4 5.1±0.8 4.1±0.7\nGPT-4-Turbo\nwithout 7.3±0.6 9.1±0.3 7.7±1.8 5.0±1.5\nwith 6.0±0.0 8.0±0.0 3.6±0.5 3.0±0.0\nClaude-3.5-Sonnet\nwithout 6.4±0.7 8.0±0.0 4.1±0.3 3.4±0.7\nToevaluatetheeffectofdifferentbackboneLLMsonourproposedmethod,weconductedanablationstudy. Inaddition\ntoClaude-3.5-Sonnet[25],werepeatedtheexperimentsinTable2usingthreedifferentbackbones: twoopen-source\nLLMs(Mixtral-7x8B[33]andLlama3-70B[34])andoneclosed-sourceLLM(GPT-4-Turbo[35]),accessedviaitsAPI.\nWereporttheresultsforsingle-trial,5-trial,and20-trialruns,andalsoexaminetheimpactofremovingcontextual\ndomainknowledgefromtheprompts. AsindicatedbyTable3,Claude-3.5-Sonnet,ourchoice,statisticallyoutperforms\ntheotherLLMsacrossallfourcases. Llama3-70Bfollowsasthesecondtopperformerinmostcases,demonstratingthat\nopen-sourceLLMscanbeeffectivealternatives. ThissuggeststhatourframeworkiscompatiblewithdifferentLLMs.\nWerecommendusersorresearchersmaystartwithClaude-3.5-Sonnet foroptimalperformance, butmayconsider\nopen-sourcemodelslikeLlamaforbudgetorcustomizability. Furthermore,inallcases,theresultsacrossallLLMs\nshowthatincorporatingcontextualdomainknowledgeconsistentlyimprovessolutionqualitycomparedtotheones\n7\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nwithoutknowledge. ThisfindingalignswithourpreviousobservationinFigure1,highlightingtherobustnessofour\nmethodsandthesignificantroleofdomainknowledgeinenhancingLLMs’reasoningperformanceinsuchCOtasks\nwithreal-worldcontexts.\n5 ConcludingRemarks\nExperimental results demonstrate that our proposed method, particularly when incorporating contextual domain\nknowledge,offersignificantadvantagesinbothconvergencespeedandsolutionqualitycomparedtobenchmarking\nstochastic and deterministic approaches. Across all experiments and backbone LLMs, the inclusion of domain\nknowledgeconsistentlyimprovedperformance. WhileClaude-3.5-SonnetisthemosteffectivebackboneLLM,open-\nsourcemodelssuchasLlama3-70Balsoshowsstrongperformance. Overall,thesefindingsillustratethecapability\nofourproposedmethodtoeffectivelyaddressDSMsequencingtasks,leveragingbothsemanticandmathematical\nreasoningofLLMsforreal-worldcombinatorialoptimizationproblems.\nDespitethepromisingresults,therearesomelimitations. First,thescopeofthecurrentexperimentscouldbeexpanded\nbycollectingmorediverseDSMcases,includinglargerandmorecomplexnetworks,toachievemorerobuststatistical\nresults. Second, while we focused on DSM sequencing tasks, the effectiveness of our proposed method could be\nexploredacrossawiderrangeofreal-worldcombinatorialoptimizationtaskstoassessitsgeneralizability. Third,we\nplantodelveintotheintermediatechangesinLLMoutputsduringoptimizationinfuturework,whichmayprovide\ngreaterinterpretabilityandinsightsintotheLLMs’reasoningprocesses.\nInconclusion,ourstudyhighlightsthepotentialofLLMsforcombinatorialoptimizationofdesignstructurematrix,\nshowingthatincludingdomainknowledgecansignificantlyenhancetheperformanceofourmethods. Futureresearch\ncanbuildonthesefindingstofurtherrefineLLM-drivenmethodsandextendapplicationstomorecomplexanddiverse\ncombinatorialoptimizationproblems.\nReferences\n[1] BernhardH.KorteandJensVygen. Combinatorialoptimization. Springer,2011.\n[2] GitaNaseriandMattheosA.G.Koffas. Applicationofcombinatorialoptimizationstrategiesinsyntheticbiology.\nNatureCommunications,11:2446,2020.\n[3] EliasXidiasandPhilipAzariadis. Energyefficientmotiondesignandtaskschedulingforanautonomousvehicle.\nInProceedingsoftheInternationalConferenceonEngineeringDesign(ICED),pages2853–2862,2019.\n[4] MichaelL.Pinedo. Scheduling. Springer,NewYork,2012.\n[5] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,Maarten\nBosma,DennyZhou,DonaldMetzler,EdH.Chi,TatsunoriHashimoto,OriolVinyals,PercyLiang,JeffDean,\nandWilliamFedus. Emergentabilitiesoflargelanguagemodels. TransactionsonMachineLearningResearch,\n2022.\n[6] YupengChang,XuWang,JindongWang,YuanWu,LinyiYang,KaijieZhu,HaoChen,XiaoyuanYi,Cunxiang\nWang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on\nevaluationoflargelanguagemodels. ACMTransactionsonIntelligentSystemsandTechnology,pages1–45,2024.\n[7] BernardinoRomera-Paredes,MohammadaminBarekatain,AlexanderNovikov,MatejBalog,M.PawanKumar,\nEmilienDupont,FranciscoJ.R.Ruiz,JordanS.Ellenberg,PengmingWang,OmarFawzi,PushmeetKohli,and\nAlhusseinFawzi. Mathematicaldiscoveriesfromprogramsearchwithlargelanguagemodels. Nature,pages\n468–475,2024.\n[8] ChengrunYang,XuezhiWang,YifengLu,HanxiaoLiu,QuocV.Le,DennyZhou,andXinyunChen. Large\nlanguagemodelsasoptimizers. InTheTwelfthInternationalConferenceonLearningRepresentations(ICLR),\n2024.\n[9] ShengcaiLiu,CaishunChen,XinghuaQu,KeTang,andYew-SoonOng. Largelanguagemodelsasevolutionary\noptimizers. InIEEECongressonEvolutionaryComputation(CEC),pages1–8,2024.\n[10] QihaoZhuandJianxiLuo. Generativetransformersfordesignconceptgeneration. JournalofComputingand\nInformationScienceinEngineering,page041003,2023.\n[11] LianeMakatura,MichaelFoshey,BohanWang,FelixHähnlein,PingchuanMa,BoleiDeng,MeganTjandra-\nsuwita,AndrewSpielberg,CrystalElaineOwens,PeterYichenChen,AllanZhao,AmyZhu,EdwardGuWil\nJ.Norton,JoshuaJacob,YifeiLi,AdrianaSchulz,andWojciechMatusik. Largelanguagemodelsfordesignand\nmanufacturing. AnMITExplorationofGenerativeAI,2024.\n8\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\n[12] ShuoJiangandJianxiLuo. Autotriz: Artificialideationwithtrizandlargelanguagemodels. InInternationalDe-\nsignEngineeringTechnicalConferences&ComputersandInformationinEngineeringConference(IDETC/CIE).\nASME,2024.\n[13] AoranMei,Guo-NiuZhu,HuaxiangZhang,andZhongxueGan. Replanvlm: Replanningrobotictaskswithvisual\nlanguagemodels. IEEERoboticsandAutomationLetters,2024.\n[14] DonaldV.Steward. Thedesignstructuresystem: Amethodformanagingthedesignofcomplexsystems. IEEE\nTransactionsonEngineeringManagement,pages71–74,1981.\n[15] YoungMiChoi. Effectiveschedulingofuserinputduringthedesignprocess. InProceedingsoftheInternational\nConferenceonEngineeringDesign(ICED),pages116–122,2011.\n[16] StevenD.EppingerandTysonR.Browning. Designstructurematrixmethodsandapplications. MITPress,2012.\n[17] Steven D. Eppinger, Daniel E. Whitney, Robert P. Smith, and David A. Gebala. A model-based method for\norganizingtasksinproductdevelopment. ResearchinEngineeringDesign,pages1–13,1994.\n[18] Yanjun Qian, Jun Lin, Thong Ngee Goh, and Min Xie. A novel approach to dsm-based activity sequencing\nproblem. IEEETransactionsonEngineeringManagement,pages688–705,2011.\n[19] RafaelAmen,IngvarRask,andStaffanSunnersjö. Matchingdesigntaskstoknowledge-basedsoftwaretools:\nWhenintuitiondoesnotsuffice. InInternationalDesignEngineeringTechnicalConferencesandComputersand\nInformationinEngineeringConference(IDETC/CIE),pages1165–1174,1999.\n[20] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,BeichenZhang,\nJunjieZhang,ZicanDong,YifanDu,ChenYang,YushuoChen,ZhipengChen,JinhaoJiang,RuiyangRen,Yifan\nLi,XinyuTang,ZikangLiu,PeiyuLiu,Jian-YunNie,andJi-RongWen. Asurveyoflargelanguagemodels.\narXivpreprint,2023. arXiv:2303.18223.\n[21] TysonR.Browning. Applyingthedesignstructurematrixtosystemdecompositionandintegrationproblems: a\nreviewandnewdirections. IEEETransactionsonEngineeringManagement,pages292–306,2001.\n[22] Tyson R. Browning. Modeling and analyzing cost, schedule, and performance in complex system product\ndevelopment. PhDthesis,MassachusettsInstituteofTechnology,1998.\n[23] KarlT.UlrichandStevenD.Eppinger. ProductDesignandDevelopment. McGraw-Hill,2016.\n[24] Thomas Andrew Black, Charles H. Fine, and Emanuel M. Sachs. A method for systems design using prece-\ndencerelationships: Anapplicationtoautomotivebrakesystems. WorkingPaperofSloanSchoolofManage-\nment,MassachusettsInstituteofTechnology,1990.\n[25] Anthropic. Introducingcomputeruse,anewclaude3.5sonnet,andclaude3.5haiku,2024. AccessedOct2024,\nhttps://www.anthropic.com/news/3-5-models-and-computer-use.\n[26] BushraAlhijawiandArafatAwajan. Geneticalgorithms: Theory,geneticoperators,solutions,andapplications.\nEvolutionaryIntelligence,pages1245–1256,2024.\n[27] Felix-AntoineFortin,Francois-MichelDeRainville,Marc-AndreGardner,MarcParizeau,andChristianGagne.\nDeap: Evolutionaryalgorithmsmadeeasy. JournalofMachineLearningResearch,pages2171–2175,2012.\n[28] JonathanJ.CroftsandDesmondJ.Higham. Googlingthebrain:Discoveringhierarchicalandasymmetricnetwork\nstructures,withapplicationsinneuroscience. InternetMathematics,pages233–254,2011.\n[29] Erik Dietzenbacher. The measurement of interindustry linkages: Key sectors in the netherlands. Economic\nModelling,pages419–437,1992.\n[30] ErnestoEstradaandJuanA.Rodriguez-Velazquez. Subgraphcentralityincomplexnetworks. PhysicalReview\nE-Statistical,Nonlinear,andSoftMatterPhysics,56(103),2005.\n[31] ErnestoEstradaandDesmondJ.Higham. Networkpropertiesrevealedthroughmatrixfunctions. SIAMReview,\npages696–714,2010.\n[32] AlanMacCormack,CarlissBaldwin,andJohnRusnak. Exploringthedualitybetweenproductandorganizational\narchitectures: Atestofthe“mirroring”hypothesis. ResearchPolicy,pages1309–1324,2012.\n[33] AlbertQ.Jiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBamford,and\nDevendraSinghChaplot. Mixtralofexperts. arXivpreprint,2024. arXiv:2401.04088.\n[34] Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024.\nhttps://ai.meta.com/blog/meta-llama-3/.\n[35] OpenAI. Gpt-4turbo,2024. https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo.\n9\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nAppendix1. Fullprompts\nPromptforourproposedmethod(themainmethod):\nYouareanexpertinthedomainofcombinationaloptimization.\nPlease assist me to find an optimal sequential order that minimizes feedback cycles in the dependency\nnetworkdescribedbelow. Yourtaskistoproposeaneworderthatdiffersfrompreviousattemptsandhasfewer\nfeedbackcyclesthananylisted.\n<Description of the entire Network> {network_description} </Description of the entire\nNetwork>\n<Nodes with Descriptions>{node_list_with_description}</Nodes with Descriptions>\n<Edges>{edge_list}</Edges>\nBelow are some previous sequential orders arranged in descending order of feedback cycles (lower is\nbetter): {selected_historical_solutions}\nPleasesuggestaneworderthat:\n-Isdifferentfromallpriororders.\n-Hasfewerfeedbackcyclesthananypreviousorder.\n-Coversallnodesexactlyonce.\n-Startswith<order>andendswith</order>.\n-Youcanusethedescriptionsofnodesandnetworkstosupportyoursuggestion.\nOutputFormat:\n<order> ...... </order>\nPleaseprovideonlytheorderandnothingelse.\nPromptforourproposedmethod(removingcontextualdomainknowledge):\nYouareanexpertinthedomainofcombinationaloptimization.\nPlease assist me to find an optimal sequential order that minimizes feedback cycles in the dependency\nnetworkdescribedbelow. Yourtaskistoproposeaneworderthatdiffersfrompreviousattemptsandhasfewer\nfeedbackcyclesthananylisted.\n<Nodes>{node_list}</Nodes>\n<Edges>{edge_list}</Edges>\nBelow are some previous sequential orders arranged in descending order of feedback cycles (lower is\nbetter): {selected_historical_solutions}\nPleasesuggestaneworderthat:\n-Isdifferentfromallpriororders.\n-Hasfewerfeedbackcyclesthananypreviousorder.\n-Coversallnodesexactlyonce.\n-Startswith<order>andendswith</order>.\nOutputFormat:\n<order> ...... </order>\nPleaseprovideonlytheorderandnothingelse.\n10\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nExampleof{network_description}intheUCAVdesignactivityDSMcase:\nThis network represents the dependency relationships among conceptual design\nactivities for UCAV development at Boeing. Each node corresponds to a specific task\nor analysis, and directed edges indicate the prerequisite relationships between these\ntasks. Nodes: Each node is a task or analysis in the conceptual design process.\nEdges: Directed edges show the prerequisite relationships between these tasks and\nanalyses.\nExampleof{node_list_with_description}intheUCAVdesignactivityDSMcase:\n[\n{’id’: ’lzOtR’, ’name’: ’Create Configuration Concepts’},\n{’id’: ’yLlKi’, ’name’: ’Prepare UCAV Conceptual DR&O’},\n{’id’: ’Swvi2’, ’name’: ’Prepare 3-View Drawing & Geometry Data’},\n{’id’: ’CDcxF’, ’name’: ’Perform Weights Analyses & Evaluation’},\n{’id’: ’0KGDm’, ’name’: ’Perform Aerodynamics Analyses & Evaluation’},\n{’id’: ’4wHtv’, ’name’: ’Perform Multidisciplinary Analyses & Evaluation’},\n{’id’: ’AgIBP’, ’name’: ’Prepare & Distribute Choice Config. Data Set’},\n{’id’: ’gRtHi’, ’name’: ’Perform S&C Characteristics Analyses & Eval.’},\n{’id’: ’GV9RJ’, ’name’: ’Make Concept Assessment and Variant Decisions’},\n{’id’: ’I1j2m’, ’name’: ’Perform Performance Analyses & Evaluation’},\n{’id’: ’Vzzm7’, ’name’: ’Perform Propulsion Analyses & Evaluation’},\n{’id’: ’B0BFG’, ’name’: ’Perform Mechanical & Electrical Analyses & Eval.’}\n]\nExampleof{node_list}intheUCAVdesignactivityDSMcase:\n[’lzOtR’, ’yLlKi’, ’Swvi2’, ’CDcxF’, ’0KGDm’, ’4wHtv’, ’AgIBP’, ’gRtHi’, ’GV9RJ’,\n’I1j2m’, ’Vzzm7’, ’B0BFG’]\nExampleof{edge_list}intheUCAVdesignactivityDSMcase:\n[\n{’dependent’: ’0KGDm’, ’predecessor’: ’Swvi2’},\n{’dependent’: ’AgIBP’, ’predecessor’: ’lzOtR’},\n{’dependent’: ’0KGDm’, ’predecessor’: ’yLlKi’},\n{’dependent’: ’Swvi2’, ’predecessor’: ’lzOtR’},\n...\n]\nExampleof{selected_historical_solutions}intheUCAVdesignactivityDSMcase:\n[\n{’solution’: ’lzOtR, yLlKi, GV9RJ, AgIBP, B0BFG, Vzzm7, Swvi2, CDcxF, 0KGDm, I1j2m,\ngRtHi, 4wHtv’, ’score’: 15.0},\n{’solution’: ’B0BFG, yLlKi, Vzzm7, lzOtR, Swvi2, CDcxF, AgIBP, 0KGDm, GV9RJ, I1j2m,\ngRtHi, 4wHtv’, ’score’: 13.0},\n...\n]\n11\nLARGELANGUAGEMODELSFORCOMBINATORIALOPTIMIZATIONOFDSM\nAppendix2. ParametersettingsforthreevariantsofGA\nInallthreevariantsoftheGAusedforDSMsequencingtasks,weemployedthefollowingsharedsettings: thenumber\nofgenerationswassetto2,000,withaselectionmechanismusingtournamentselectionandmutationusingshuffled\nindexes.TheGAwasimplementedusingtheDEAPlibrarywithstandardconfigurationsforinitialpopulationgeneration\n[27]. ThedifferentconfigurationsforeachGAsettingareshowninthetablebelow. Themeaningofeachparameter\nalsorefersto[27].\nIndividualMutation Tournament Crossover Mutation\nPopulation\nProbability Size Probability Probability\nExploration-focused 50 0.05 5 0.6 0.4\nExploitation-focused 10 0.01 20 0.9 0.1\nBalanced 20 0.02 10 0.7 0.3\n12",
    "pdf_filename": "Large_Language_Models_for_Combinatorial_Optimization_of_Design_Structure_Matrix.pdf"
}