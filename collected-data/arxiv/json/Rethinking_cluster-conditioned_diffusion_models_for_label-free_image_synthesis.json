{
    "title": "Rethinking cluster-conditioned diffusion models for label-free image synthesis",
    "context": "Diffusion-based image generation models can enhance image quality when conditioned on ground truth labels. Here, we conduct a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We investigate how individual clustering de- terminants, such as the number of clusters and the cluster- ing method, impact image synthesis across three different datasets. Given the optimal number of clusters with respect to image synthesis, we show that cluster-conditioning can achieve state-of-the-art performance, with an FID of 1.67 for CIFAR10 and 2.17 for CIFAR100, along with a strong increase in training sample efficiency. We further propose a novel empirical method to estimate an upper bound for the optimal number of clusters. Unlike existing approaches, we find no significant association between clustering per- formance and the corresponding cluster-conditional FID scores. Code is available at https://github.com/ HHU-MMBS/cedm-official-wavc2025 Diffusion models have enabled significant progress in many visual generative tasks, such as image synthesis [29, 39, 57] and manipulation [80]. Conditioning diffusion models on human-annotated data is today’s standard prac- tice as it significantly improves the image fidelity [8,23,25]. Image-level conditioning is typically realized by using as- sociated text captions [65, 66] or class labels, if available [29, 53]. Nonetheless, human-annotated labels are costly and often contain inaccuracies [7,10], while publicly avail- able image-text pairs can be non-descriptive [9]. Large, human-annotated datasets are prohibitively costly [59] and hard to obtain in a plethora of real-life applications such as medical image synthesis [41]. Human annotation is subject to the label collection procedure and can have vary- Cluster 0 willow_tree oak_tree maple_tree Cluster 1 telephone telephone telephone Cluster 2 telephone telephone telephone Figure 1. An ideal image-level conditioning should group images based on shared patterns, shown in the same row, which do not always align with human labels, indicated above each image (CI- FAR100 [45] samples). ing annotation hierarchies [3, 11, 31]. An ideal condition- ing signal would be based solely on shared characteristics corresponding to general visual concepts, as illustrated in Fig. 1. To provide a ground truth (GT) label-free alternative that is applicable to unlabelled datasets, we focus on cluster- based conditioning on diffusion models. Image clustering refers to algorithmically assigning a semantic group to an image, called a cluster, given an a priori number of such groups [14,18,38]. Maximizing cluster alignment with ground truth (GT) labels does not guarantee optimal cluster-conditional im- age generation. To create visually distinguishable groups (Fig. 1), a clustering algorithm would sometimes group im- ages into more fine-grained groups than the GT labels, i.e., sub-grouping analog and digital devices that map to the arXiv:2403.00570v2  [cs.CV]  19 Nov 2024",
    "body": "Rethinking cluster-conditioned diffusion models for label-free image synthesis\nNikolas Adaloglou\nHeinrich Heine University of Dusseldorf\nadaloglo@hhu.de\nTim Kaiser\nHeinrich Heine University of Dusseldorf\ntikai103@hhu.de\nFelix Michels\nHeinrich Heine University of Dusseldorf\nfelix.michels@hhu.de\nMarkus Kollmann\nHeinrich Heine University of Dusseldorf\nmarkus.kollmann@hhu.de\nAbstract\nDiffusion-based image generation models can enhance\nimage quality when conditioned on ground truth labels.\nHere, we conduct a comprehensive experimental study on\nimage-level conditioning for diffusion models using cluster\nassignments. We investigate how individual clustering de-\nterminants, such as the number of clusters and the cluster-\ning method, impact image synthesis across three different\ndatasets. Given the optimal number of clusters with respect\nto image synthesis, we show that cluster-conditioning can\nachieve state-of-the-art performance, with an FID of 1.67\nfor CIFAR10 and 2.17 for CIFAR100, along with a strong\nincrease in training sample efficiency. We further propose\na novel empirical method to estimate an upper bound for\nthe optimal number of clusters. Unlike existing approaches,\nwe find no significant association between clustering per-\nformance and the corresponding cluster-conditional FID\nscores. Code is available at https://github.com/\nHHU-MMBS/cedm-official-wavc2025\n1. Introduction\nDiffusion models have enabled significant progress in\nmany visual generative tasks, such as image synthesis\n[29, 39, 57] and manipulation [80]. Conditioning diffusion\nmodels on human-annotated data is today’s standard prac-\ntice as it significantly improves the image fidelity [8,23,25].\nImage-level conditioning is typically realized by using as-\nsociated text captions [65, 66] or class labels, if available\n[29, 53]. Nonetheless, human-annotated labels are costly\nand often contain inaccuracies [7,10], while publicly avail-\nable image-text pairs can be non-descriptive [9].\nLarge, human-annotated datasets are prohibitively costly\n[59] and hard to obtain in a plethora of real-life applications\nsuch as medical image synthesis [41]. Human annotation is\nsubject to the label collection procedure and can have vary-\nCluster 0\nwillow_tree\noak_tree\nmaple_tree\nCluster 1\ntelephone\ntelephone\ntelephone\nCluster 2\ntelephone\ntelephone\ntelephone\nFigure 1. An ideal image-level conditioning should group images\nbased on shared patterns, shown in the same row, which do not\nalways align with human labels, indicated above each image (CI-\nFAR100 [45] samples).\ning annotation hierarchies [3, 11, 31]. An ideal condition-\ning signal would be based solely on shared characteristics\ncorresponding to general visual concepts, as illustrated in\nFig. 1. To provide a ground truth (GT) label-free alternative\nthat is applicable to unlabelled datasets, we focus on cluster-\nbased conditioning on diffusion models. Image clustering\nrefers to algorithmically assigning a semantic group to an\nimage, called a cluster, given an a priori number of such\ngroups [14,18,38].\nMaximizing cluster alignment with ground truth (GT)\nlabels does not guarantee optimal cluster-conditional im-\nage generation. To create visually distinguishable groups\n(Fig. 1), a clustering algorithm would sometimes group im-\nages into more fine-grained groups than the GT labels, i.e.,\nsub-grouping analog and digital devices that map to the\narXiv:2403.00570v2  [cs.CV]  19 Nov 2024\n\nsame label or merge similarly looking images with differ-\nent labels (merging different types of trees).\nDeep image clustering has recently seen significant\nprogress [4, 78, 81], especially when utilizing off-the-shelf\nfeatures learned from large-scale pre-training [2,3,76]. Cur-\nrently, the relationship between clustering performance and\ncluster-conditional image synthesis performance remains\nunclear for several reasons. First, clustering metrics cap-\nture the alignment to GT labels, whereas generative models\nmay benefit more from visually distinguishable groups [34].\nSecond, clustering metrics cannot be computed for unla-\nbeled datasets. Third, existing metrics are not suitable for\nfairly evaluating cluster assignments with varying numbers\nof clusters. Therefore, they cannot help to determine the op-\ntimal cluster granularity w.r.t. generative performance for an\narbitrary dataset. This aspect has not been sufficiently ex-\nplored to date [25,50].\nExisting cluster-conditioned generative approaches [25,\n34, 50] often adopt simple generative baselines [29] and\nhave mainly investigated k-means clustering on balanced\nclassification datasets. The adoption of k-means is justi-\nfied due to its ability to work with off-the-shelf feature ex-\ntractors and its non-parametric nature, apart from setting\nthe number of clusters. Conversely, it is well-established\nthat k-means is suboptimal for image clustering because its\ncluster assignments are highly imbalanced [34, 76]. While\ncluster-conditioned approaches are sensitive to the num-\nber of clusters [25], there is currently no method to pre-\ndetermine this number. Previous methods have overlooked\nthis [25, 34, 50], significantly hindering their applicability\nto unlabeled datasets.\nRecent feature-based image clus-\ntering methods have demonstrated superior performance\ncompared to k-means [2, 76], and their efficacy in condi-\ntional image generation is yet to be investigated. For the\nabove reasons, cluster-conditioning methods underperform\nGT conditional ones to date.\nIn this paper, we systematically study clustering as a\nlabel-free condition for diffusion models. We demonstrate\nthat optimal cluster granularity achieves state-of-the-art FID\nscores (1.67 for CIFAR10 and 2.17 for CIFAR100) while\nenhancing training sample efficiency. We propose a compu-\ntationally efficient method to derive an upper cluster bound\nusing feature-based clustering, which narrows the search\nspace for optimal image synthesis performance. We vali-\ndate this upper bound across three datasets and two gen-\nerative metrics. Finally, we find no significant association\nbetween clustering performance and cluster-conditional im-\nage synthesis performance across various clustering meth-\nods, performance metrics, and pre-trained models.\n2. Related Work\n2.1. Conditional generative models\nEarly attempts on controllable image synthesis adopted\ngenerative adversarial networks (GANs) [52]. By condi-\ntioning both the generator and discriminator on human la-\nbels, GANs can produce images for a specific GT label,\neven at the scale of ImageNet [13].\nRecently, diffusion\nmodels (DMs) have emerged as an expressive and flexi-\nble category of generative models [30, 40, 63, 68, 69]. In-\nternal guidance-based methods [24, 30, 32, 33] further im-\nproved the flexibility and visual fidelity of DMs during sam-\npling such as classifier and classifier-free guidance. DMs\nexhibit an enormous number of label-conditional variants\n[29, 39, 44, 53, 58].\nRecently, Stein et al. [71] proposed\nthe adoption of Fr´echet DINOv2 [54] distance as it aligns\nbetter with human preferences and demonstrated that DMs\nachieve the highest perceptual realism.\n2.2. Alternative conditioning of generative models\nUnlike internal guidance [30,32,33], external condition-\ning signals are computed from the training set using ad-\nditional models, which is commonly referred to as self-\nconditioning. Self-conditioning signals can be roughly di-\nvided into image-level and sub-image-level. Image-level\nconditioning refers to a single condition for all pixels in an\nimage, such as cluster assignments or text captions [63].\nSub-image-level conditions refer to specific parts or regions\nof an image [6,34]. For instance, Hu et al. [34] extend DMs\nto incorporate conditioning of bounding boxes or segmen-\ntation masks.\nFeature-based conditioning.\nUsing image or text fea-\ntures can also provide an informative conditioning signal\n[12, 49, 61, 67]. Bordes et al. [12] condition DMs directly\non image representations (oracle features from real im-\nages) and reveal their impact on visual fidelity. Instance-\nconditioned GAN combines GT labels with features from\neach image’s NN set [16]. Ramesh et al. [61] leverage the\nzero-shot capability of CLIP to condition DMs on language-\nguided image representations to improve diversity. Zhou\net al. provide a language-free framework [84] that directly\ngenerates text features for images using CLIP. Still, CLIP\nmodels require an a priori set of descriptive candidate cap-\ntions, such as label names.\nCluster-based conditioning. Similar to label conditioning,\nconditioning on cluster assignments facilitates DMs by al-\nlowing them to specialize on a distinct set of shared visual\nfeatures [25, 34, 50]. In [25], Bao et al. computed the k-\nmeans clusters offline using contrastive learned features on\nthe training data. However, their approach could not outper-\nform the label-conditioned models. The first cluster-based\napproach that achieves competitive performance compared\nto GT labels leverages pre-trained feature extractors [34],\n\nwhich is the closest to our work. In [34], the authors at-\ntempt to weak establish a correlation between clustering\nand cluster-conditional generative performance for different\nself-supervised models. The observed correlation cannot be\ntested on unlabeled datasets, and its sensitivity to the num-\nber of clusters is unknown. While it was demonstrated that\nDINO [15] provides the most informative k-means clus-\nters [34], their method lacks a strategy for choosing the clus-\nter granularity for unlabeled datasets.\n3. Method\n3.1. Notations and prerequisites\nWe consider the frequent scenario where we have access\nto an unlabeled dataset D and a pre-trained feature extrac-\ntor g(·). We denote the number of ground truth (GT) labels\nas CGT , which we assume to be unknown. To distinguish\ncluster-based conditioning from the image clustering task,\nwe denote the number of visual groups CV of D as the\noptimal number of clusters w.r.t. image synthesis (e.g. as\nmeasured by FID [28]). We adopt the diffusion model ap-\nproach of Karras et al. (EDM [39]) as a baseline, which in-\ntroduced various improvements to the standard DDPM [29].\nWe provide an overview of EDM in the supplementary ma-\nterial. Although we focus on EDM throughout this work,\nour method can be applied to any conditional generative\nmodel.\nTEMI clustering. Given an a priori determined number of\nclusters C and a feature extractor g(.), TEMI [2] first mines\nthe m nearest neighbors (NN) of all x ∈D in the feature\nspace of g(·) based on their cosine similarity. We denote the\nset of NN for x by Sx. During training, TEMI randomly\nsamples x from D and x′ from Sx to generate image pairs\nwith similar (visual) features. A self-distillation framework\nis introduced to learn the cluster assignments with a teacher\nand student head ht(·) and hs(·) that share the same archi-\ntecture (i.e. 3-layer MLP) but differ w.r.t. their parameters.\nThe features of the image pair are fed to the student and\nteacher heads hi\ns(z), hi\nt(z′), where i ∈{1, . . . , H} is the\nhead index and z = g(x), z′ = g(x′). The outputs of the\nheads are converted to probabilities qs(c|x) and qt(c|x′) us-\ning a softmax function. The TEMI objective,\n  \n\\begin {sp li t }\n \n\\\nt\nemi\nl\no\nss ^\ni(\n\\ rvx ,\\rv\nx  ') := \n- \\\nf\nr\nac \n{\n1}\n{H} \\sum\n _{j=1}\n^H\n \\s\num _\n{c'=1}^C \\pteach ^j(c'|\\rvx )\\pteach ^j(c'|\\rvx ') \\\\ \\log \\sum _{c=1}^C \\frac { \\left ( \\pstud ^i(c|\\rvx ) \\pteach ^i(c|\\rvx ')\\right )^{\\gamma }} {\\tilde q_t^i(c)} \\quad , \\end {split} \\label {eq:temi-loss} \n(1)\nmaximizes the pointwise mutual information between im-\nages x and x′, using the clustering index c as information\nbottleneck [38]. Here, ˜qt\ni(c) is an estimate of the teacher\ncluster distribution Ex∼pdata\n\u0002\nqi\nt(c|x)\n\u0003\n, which can be com-\nputed by an exponential moving average over mini-batches\nB defined as ˜qt\ni(c) ←λ ˜qt(c) + (1 −λ) 1\n|B|\nP\nx∈B qi\nt(c|x).\nThe hyperparameter γ ∈(0.5, 1] avoids the collapse of\nall sample pairs into a single cluster, and λ ∈(0, 1) is a\nmomentum hyperparameter. The loss function Eq. (1) is\nfurther symmetrized and averaged over heads to obtain the\nfinal training loss. After training, the cluster assignments\nc∗(x) = arg maxc qt(c|x) and the empirical cluster distri-\nbution q(c) on the training set is computed from the TEMI\nhead with the lowest loss. We use c∗(x) as a condition of\nthe generative model during training and sample from q(c)\nto generate images. Adaloglou et al. [2] show that the value\nγ = 0.6 enforces a close to uniform cluster utilization and\nachieves state-of-the-art clustering accuracy for CGT [2].\n3.2. Cluster-conditional EDM (C-EDM)\nWe consider k-means and TEMI clusters using off-the-\nshelf feature extractors with CV clusters to compute cluster\nassignments. We experimentally observed that CV > CGT ,\neven though this is not a requirement of our approach. We\nhighlight that clustering using C > CGT while enforcing\na uniform cluster utilization (such as TEMI with γ = 0.6)\nhas not been previously explored [2,35,48,76] as it reduces\nthe cluster alignment with the GT labels. We denote the\ncluster-conditioned EDM model as C-EDM.\nA direct estimation of CV is hard to obtain for unlabeled\ndatasets. In principle, CV can be found using a hyperpa-\nrameter search with FID as an evaluation metric, which\nis computationally expensive. In contrast to existing ap-\nproaches that perform a restricted hyperparameter search\naround CGT , we propose a new metric that allows us to\nderive an upper bound for CV (Sec. 3.3), which requires no\nprior knowledge about the dataset.\n3.3. Estimating the upper cluster bound\nWe aim to find an upper cluster bound Cmax using TEMI\n(γ = 0.6), such that CV < Cmax. During this computation,\nwe neither use the generative model nor any additional in-\nformation. We highlight that previous works [25, 34, 50]\nhave overlooked this design choice and iterate within a\nsmall range around CGT . We denote the number of utilized\nclusters (one training sample is assigned to it) as Cu after\nTEMI clustering. The TEMI cluster utilization ratio is de-\nfined as rC := Cu\nC ≤1. Importantly, there is no guarantee\nthat the full cluster spectrum will be utilized. Unlike k-\nmeans that always has rC = 1, we observe that as the num-\nber of clusters C increases, the TEMI cluster utilization ra-\ntio rC typically decreases, which provides dataset-specific\ninformation. Intuitively, TEMI clustering with γ = 0.6 is\nenforcing rC →1, and the observation of rC < 1 is an\nindication that the maximum number of TEMI clusters is\nreached for γ = 0.6.\nThe search for Cmax involves doubling the number of\n\nCIFAR10\nCIFAR100\nFFHQ-64\nFFHQ-128\nGenerative methods\nUnlabelled\nGT\nUnlabelled\nGT\nUnlabelled\nUnlabelled\nDDPM [29,34]\n3.17\n-\n10.7\n9.7\n-\n-\nPFGM++ [79]\n-\n1.74\n-\n-\n2.43\n-\nEDM [39]\n1.97\n1.79\n-\n-\n2.39\n-\nEDM (our reproduction)\n2.07\n1.81\n3.41\n2.21\n2.53\n5.93\nSelf-conditional methods\nIDDPM+k-means (k=10) [25]\n2.23\n-\n-\n-\n-\n-\nDDPM+k-means† (k=400) [34]\n-\n-\n9.6\n-\n-\n-\nC-EDM+k-means† (Ours)\n1.69\n-\n2.21\n-\n1.99\n-\nC-EDM+TEMI† (Ours)\n1.67\n-\n2.17\n-\n2.09\n4.40\nEDM+oracle features† (Ours)\n2.21\n-\n2.25\n-\n1.77\n-\nTable 1. State-of-the-art generative model comparison: FID (↓) for various GT labeled and unlabelled benchmarks. We use CV =100,\n200, 400 clusters for CIFAR10, CIFAR100, and FFHQ-64, respectively. Self-conditional methods with † use the pre-trained DINO ViT-B\nfeature extractor. Ground truth and oracle feature conditioning results are marked in gray color as they are not a fair comparison.\nclusters C at each iteration until the rC falls below a\nthreshold, rC ≤α, followed by a more fine-grained grid\nsearch. Cmax is then defined as the highest C for which\nrC > α. This results in a worst-case time complexity of\nO(log2( Cmax\nCstart )). This is conceptually similar to the elbow\nmethod [42,75]. In this sense, the proposed heuristic has the\nsame limitations as the elbow method. Yet, it is currently\nthe only method that provides a practitioner with a starting\npoint for an unlabelled dataset. After detecting Cmax, we\nperform a bounded grid search to find CV ∈[2, Cmax) us-\ning the generative model. To obtain a simple cross-dataset\nestimate, we empirically find a cutoff threshold rC ≤0.96\nto work well across three datasets.\n4. Experimental evaluation\n4.1. Datasets, models, and metrics\nFollowing prior works [2, 34, 83], we use DINO ViT-\nB pre-trained on ImageNet [22] for image clustering. We\nreport the Fr´echet inception distance (FID) [28] to quan-\ntify the image generation performance as it simultaneously\ncaptures visual fidelity and diversity. To facilitate future\ncomparisons, we also compute the Fr´echet DINOv2 dis-\ntance (FDD) [71] that replaces the features of InceptionNet-\nv3 [73, 74] with DINOv2 ViT-L [54]. FID and FDD are\naveraged over three independently generated sample sets of\n50K images each. To measure the cluster alignment w.r.t.\nGT labels, we use the adjusted normalized mutual informa-\ntion (ANMI) as in [35].\nWe follow the default hyperparameter setup for EDM\n[39] and TEMI [2]. We denote the number of samples (in\nmillions) seen during training as Mimg. We use Mimg =\n200 when comparing C-EDM with other baselines and\nstate-of-the-art methods (Tab. 1), and Mimg = 100 when\ncomparing across different number of clusters. We train\nthe TEMI clustering heads for 200 epochs per dataset. All\nthe experiments were conducted on 4 NVIDIA A100 GPUs\nwith 40GB VRAM each. On this hardware, TEMI clus-\ntering was more than 50× faster than training EDM on\nFFHQ-64, which requires more than 2 days for 200 Mimg\nwith a batch size of 512. Additional implementation de-\ntails and hyperparameters can be found in the supplemen-\ntary material.\nWe verify our approach across CIFAR10,\nCIFAR100 [45] and FFHQ. CIFAR10 and CIFAR100 have\n50K samples and 322 resolution images, while we use the\n64x64 and 128x128 versions of FFHQ (FFHQ-64, FFHQ-\n128) consisting of 70K samples. Finally, diffusion sampling\nmethods are not included, as they can be incorporated into\nany diffusion model [30,43,64].\n4.2. State-of-the-art comparison for image synthesis\nComparison with state-of-the-art unconditional gen-\nerative models.\nWith respect to FID, we found CV\n=\n100, 200, 400 to be close to optimal cluster granularities for\nCIFAR10, CIFAR100, and FFHQ-64, respectively. We use\nthese values to report FIDs compared to GT conditional and\nunconditional generative models in Tab. 1. Compared to\nunconditional methods, C-EDM achieves an average rela-\ntive FID improvement of 24.4% and 24.9% using TEMI and\nk-means clusters, respectively. More importantly, previous\ncluster-conditional approaches did not achieve near-state-\nof-the-art FIDs because they: a) adopted non-competitive\ndiffusion baselines such as DDPM [29], b) did not consider\npre-trained feature extractors for clustering [25], c) did not\nuse the optimal cluster granularity [34]. For instance, Hu et\nal. [34] used 400 clusters on CIFAR100 using DINO ViT-B\nfor clustering, while Fig. 3 shows that 200 clusters lead to a\nsuperior FID using C-EDM (11.2% relative improvement).\nComparison with state-of-the-art GT conditional\n\nMimg\n40\n80\n120\n160\n200\n1.8\n2.0\n2.2\n2.4\n2.6\nFID\n5 ×\nCIFAR10\nMimg\n40\n80\n120\n160\n200\n2.5\n3.0\n3.5\n4.0\n4.5\nFID\n5 ×\nCIFAR100\nMimg\n40\n80\n120\n160\n200\n2.2\n2.5\n2.8\n3.1\n3.4\nFID\n3.3 ×\nFFHQ\nTEMI\nk-means\nUnconditional\nGT labels\nGT superclasses\nFigure 2. FID (y-axis) versus seen samples during training in millions (x-axis). TEMI and k-means clusters are computed using the\nrepresentations of DINO ViT-B [15]. We used CV = 100, 200, 400 for CIFAR10, CIFAR100 and FFHQ-64 respectively. The training\nsample efficiency compared to the unconditional baseline is indicated by the arrow. Best viewed in color.\ngenerative models. Intriguingly, using C-EDM with CV\nclusters, we report small improvements compared to GT\nlabel conditioning on CIFAR10 and CIFAR100 in Tab. 1.\nFor instance, 4.77% mean relative improvement in FID us-\ning TEMI. Even though cluster-conditioning is primarily\ndesigned for unconditional generation, such as FFHQ, we\ndemonstrate that it can match or outperform GT labels,\nwhich showcases the effectiveness of C-EDM.\nSample efficiency. In addition to the reported gains in FID,\nwe study the sample efficiency of C-EDM during training\nacross datasets in Fig. 2. On CIFAR10 and CIFAR100, the\ntraining sample efficiency for C = CV compared to the\nunconditional model (C = 1) peaks at 5×, where C-EDM\nwith Mimg = 40 outperforms the unconditional model at\nMimg = 200. On FFHQ-64, which is not a classification\ndataset, we report a sample efficiency of 3.3×. TEMI and\nk-means demonstrate identical sample efficiency compared\nto the unconditional model. Intuitivly, a more informative\nconditioning signal enables learning the data distribution\nfaster. Upon visual inspection, we could identify FFHQ\nclusters for C = CV with easily distinguishable visual pat-\nterns, such as groups of images with beanies, smiling faces,\nglasses, sunglasses, hats, and kids (see supplementary). Fi-\nnally, we find that the sample efficiency of C-EDM against\nGT conditional EDM heavily depends on the quality and\ngranularity of the GT labels.\n4.3. Cluster utilization ratio and discovered upper\nbounds\nWe depict how the TEMI cluster utilization rC changes\nin tandem with FID for different numbers of clusters in\nFig. 3. Crucially, this approach also applies to FFHQ, where\nno GT labels exist. Moreover, we can discern certain pat-\nterns across datasets: (i) CV always has a high utilization\nratio, (ii) the majority of experiments with C < Cmax out-\nperform the unconditional model at Mimg = 100, and (iii)\nCV > CGT for CIFAR10 and CIFAR100, which is in line\nwith [34]. Even though the choice of α = 0.96 is not guar-\nanteed to be generally applicable and is based on empirical\nevidence, a more or less strict choice can be used based on\nthe practitioner’s computational budget. The introduced up-\nper bound can be computed with negligible computational\noverhead and without access to GT labels or training the\ngenerative model, which on large scales (ImageNet) can re-\nquire up to 4MWh per experiment [40].\n4.4. Investigating the connection between clustering\nand cluster-conditional image synthesis\nFID and ANMI. On image clustering benchmarks, TEMI\n(γ = 0.6) outperforms k-means, where we respectively\nmeasure an ANMI of 60.6% versus 59.2% on CIFAR10 and\n72.3% versus 67.6% on CIFAR100. However, their genera-\ntive performance displays negligible differences in terms of\nFID for C = CV . We emphasize that the imbalanced clus-\nters of k-means are penalized when computing clustering\nmetrics such as ANMI on balanced classification datasets,\nwhile during image synthesis, this is naturally mitigated by\nsampling from q(c). Tab. 1 suggests that imbalanced clus-\nter assignments are beneficial for unlabelled datasets such\nas FFHQ, where we report a relative gain of 4.8% using\nk-means clusters compared to TEMI.\nC-EDM matches EDM when C = CGT . In Fig. 3, we\nobserve that the GT label conditioning closely follows the\nFID of TEMI clusters for C = CGT . In Tab. 2, we lever-\nage the two annotation levels of CIFAR100 [45], specif-\nically the 20 GT superclasses and the 100 GT labels to\nbenchmark how different grouping methods perform in im-\nage synthesis. Apart from image clustering, we create text-\nbased pseudo-labels with CLIP (OpenCLIP ViT-G/14 [36])\nsimilar to [51]. Then, CLIP pseudo-labels are derived from\nzero-shot classification [59] using the arg max of the text-\nimage similarity after softmax [1]. For reference, we pro-\n\nC = 1\n200\n400\n600\n800\n2.0\n2.4\n2.8\n3.2\nFID\nCIFAR10\nC = 1\n200\n400\n600\n800\n1000\n2.4\n2.8\n3.2\n3.6\nFID\nCIFAR100\nC = 1\n200\n400\n600\n800\n1000\n2.2\n2.4\n2.6\n2.8\nFID\nFFHQ\n0.94\n0.96\n0.98\n1.0\nrC\n0.91\n0.94\n0.97\n1.0\nrC\n0.91\n0.94\n0.97\n1.0\nrC\nFID\nTEMI cluster utilization ratio (rC)\nGT labels (FID)\nFigure 3. FID (left y-axis) and TEMI cluster utilization ratio rC (right y-axis) across different numbers of clusters C (x-axis) using C-EDM,\nevaluated at Mimg = 100. The green area indicates the discovered cluster range [2, Cmax) for rC ≤α = 0.96.\nC\nCIFAR100\n20\n100\n200\nHuman annotation (GT)\n3.10\n2.41\n-\nCLIP pseudo-labels\n3.38\n2.42\n-\nk-means clusters\n3.09\n2.41\n2.36\nTEMI clusters\n2.93\n2.37\n2.31\nTable 2. FID for different grouping methods as a condition to\nEDM (Mimg = 100). We use the 20 GT superclasses and 100\nGT labels of CIFAR100. CLIP pseudo-labels are computed us-\ning zero-shot classification pseudo-labels based on the GT label\nnames.\nvide the FID with the number of visual groups CV = 200\nfor k-means and TEMI clusters. Interestingly, all methods\nattain a similar FID for CGT = 100, while TEMI achieves\nthe best FID only for C = 20. This suggests that generative\nperformance is mostly invariant to the grouping method,\nincluding human annotation, given a feature extractor that\ncaptures general visual concepts. Next, we investigate the\nimpact of the pre-trained feature extractor.\nThe number of visual groups is insensitive to the chosen\nfeature extractor. The broader adoption of the discovered\nCV for each dataset requires our analysis to be insensitive\nto the chosen feature extractor. In Fig. 4, we show that the\npre-trained models considered in this work achieve similar\nFIDs across different numbers of clusters even though the\nANMI varies up to 5.4% on CIFAR10. Feature extractors\ndiffer in terms of the number of parameters (86M up to 1.8\nbillion), pre-training dataset size (1.2M up to 2B), and ob-\njective (MoCO [19], DINO [15], iBOT [82], CLIP [36,59]).\nOur finding is consistent with concurrent work on the scale\nof ImageNet [49] that shows that linear probing accuracy is\nnot indicative of generative performance for feature-based\nC = 1\n50\n100\n150\n200\n250\n300\n350\n400\n1.8\n2.0\n2.2\nFID\nCIFAR10\nMoCov3 (59.3)\nDINO (60.7)\nDINOv2 (63.8)\niBOT (62.7)\nOpenCLIP (64.7)\nFigure 4. FID (y-axis) across different numbers of clusters C (x-\naxis) using C-EDM with TEMI with different feature extractors.\nThe ANMI is shown in parentheses for CV =100.\nconditioning.\nThe fact that differences in discriminative\nperformance do not translate to improvements in FID is in\ncontrast with Hu et al. [34].\nDataset-specific features do not improve generative per-\nformance. To realize dataset-dependent adaptation of the\nfeature extractor, we perform self-supervised fine-tuning\non CIFAR10 and CIFAR100, starting from the ImageNet\nweights in Tab. 3.\nWe train all layers for 15 epochs\nsimilar to [62] using the DINO framework, resulting in\n“DINO adapted weights”. We find large improvements in\nclassification accuracy and ANMI, which is in line with\nthe fact that the learned features are more dataset-specific\n[60, 62, 77]. However, the gains in classification and clus-\ntering do not translate into conditional image generation,\nwhich suggests that image synthesis requires features that\ncapture general visual concepts.\n\nCIFAR10 (C = 100)\nCIFAR100 (C = 200)\nC-EDM+TEMI\nACC↑\nANMI↑\nFID↓\nFDD↓\nACC↑\nANMI↑\nFID↓\nFDD↓\nDINO ImageNet weights\n96.4\n60.6\n1.73\n153.5\n82.3\n72.3\n2.31\n250.7\nDINO adapted weights\n98.5\n63.41\n1.74\n152.8\n87.8\n78.2\n2.39\n250.8\nTable 3. Effect of self-supervised fine-tuning (adapted weights) on the pre-trained DINO ViT-B. ACC stands for the 20-NN classification\naccuracy using the DINO features. We measure FID and FDD for both models at Mimg = 100.\nC=1\n200\n400\n600\n800\n0.59\n0.6\n0.61\nAUROC\nCIFAR10\nC=1\n200\n400\n600\n800\n1000\n0.63\n0.64\n0.65\n0.66\nAUROC\nCIFAR100\n1.7\n2.4\n3.1\nuFID\n1.9\n2.3\n2.7\n3.1\nuFID\nAUROC\nuFID\n Unconditional AUROC\nFigure 5. Top 1-NN cosine similarity AUROC (left y-axis) and Frechet distance between the C-EDM and unconditional samples (uFID)\nfor different cluster sizes C (x-axis). For the computation of AUROC, we use the official test splits.\n5. Discussion\nHighly\nfine-grained\nclusters\nlead\nto\nout-of-\ndistribution samples.\nTo quantify the degree of out-\nof-distribution for the generated C-EDM samples, we\nmeasure the AUROC using the top-1 NN cosine similarity\n(1-NN) using DINO ViT-B [72] and compare with the\ntest split of CIFAR10 and CIFAR100 (Fig. 5). We note\nthat 1-NN is independent of the data distribution of the\ngenerated samples and thus does not account for diversity.\nThe highest C considered for CIFAR10 and CIFAR100\nproduces the highest AUROC, suggesting that the features\nof the generated samples are pushed away from the training\ndata.\nWe hypothesize that this behavior originates from\nhighly specialized clusters that cannot always be generated\nfrom the initial noise, resulting in visual artifacts.\nIn\nparallel, we measure how similar the C-EDM samples are\ncompared to the unconditional ones using FID. We call this\nmetric unconditional FID (uFID). We observe that uFID\nincreases as C increases, suggesting that the generated\nsamples for larger C are farther away in feature space than\nthe unconditional samples.\nThe clusters’ granularity level determines conditional\ngenerative performance. The quality of feature represen-\ntations is typically determined by linear separability w.r.t.\nGT labels [55]. However, our experimental analysis shows\nthat in image synthesis, the quality of image-level condi-\ntioning primarily depends on the granularity of the clus-\nter assignments (Tab. 2 and Fig. 3). Overall, we find no\nsignificant connection between the task of feature-based\nclustering and cluster-conditional image generation (Fig. 4\nand Tab. 3). The pre-trained model and the categorization\nmethod do not severely affect generative performance for\nthe considered datasets.\nGenerative metric and optimal number of clusters.\nHere, we investigate the impact of the choice of genera-\ntive metric by comparing FDD and FID. We highlight that\nthe only difference is that FID and FDD use InceptionV3\nand DINOv2 features, respectively. The cross-dataset eval-\nuation in Fig. 6 shows that FID and FDD do not always\nagree with respect to the number of visual groups.\nThe\nlargest disagreement is observed on CIFAR10. Still, the\ndiscovered upper bound Cmax always includes the num-\nber of visual groups. Moreover, we evaluate both metrics\nacross training iterations and found that while FID fluc-\ntuates after Mimg = 120, FDD decreases monotonically\n(supplementary material). Finally, we measured additional\nfeature-based metrics such as precision, recall, diversity,\ncoverage, MSS, and inception score without success. Simi-\nlar to [21,37,47,56,71], we argue that new generative met-\nrics must be developed.\nVisual comparison of C-EDM to EDM. In Fig. 7, we map\nclusters to their respective CIFAR100 classes and produce\nsamples with C-EDM and GT conditional EDM using the\nsame noise. Samples exhibit high visual similarity, corrob-\norating with Tab. 2 and Fig. 2. More visualizations are pro-\nvided in the supplementary material.\n\nC = 1\n200\n400\n600\n800\n2.0\n2.3\n2.6\n2.9\nFID\nCIFAR10\nC = 1\n200\n400\n600\n800\n1000\n2.5\n2.8\n3.1\n3.4\nFID\nCIFAR100\nC = 1\n200\n400\n600\n800\n1000\n2.3\n2.5\n2.7\n2.9\nFID\nFFHQ-64\n182\n214\n246\n278\nFDD\n266\n287\n308\n329\nFDD\n206\n214\n222\n230\nFDD\nTEMI clusters (FID)\nTEMI clusters (FDD)\nFigure 6. FID (left y-axis) and FDD (right y-axis) versus number of clusters (x-axis) using C-EDM at Mimg = 100. The non-filled markers\nindicate the unconditional EDM.\nFigure 7.\nComparing C-EDM samples generated with learned\ncluster assignments (top row, C=100) to EDM samples generated\nwith GT labels, on CIFAR100. The clusters are mapped to GT\nclasses using the Hungarian algorithm.\nMeasuring the confidence of the generated samples us-\ning TEMI. In Fig. 8, we show the generated examples\nwith the lowest and highest maximum softmax probabil-\nity [27] of the TEMI head as a measure of confidence. For\ncomparison, we show unconditional samples generated us-\ning the same initial noise in the denoising process. Visual\ninspection shows that low-confidence C-EDM samples do\nnot have coherent semantics compared to the unconditional\nones, leading to inferior image quality.\nWe hypothesize\nthat the sampled condition for the low-confidence samples\nis in conflict with the existing patterns in the initial noise.\nIncreasing the number of conditions likely leads to worse\nimage-condition alignment [30, 32, 65]. We argue that in-\nternal guidance methods could be employed to increase the\nimage quality in such cases, which is left for future work.\nBy contrast, highly confident C-EDM samples show\nmore clearly defined semantics than unconditional ones.\nWhen the low frequencies, such as the object’s shape, re-\nmain intact, cluster conditioning aids in refining local pixel\npatterns. Confident C-EDM samples consist of simple pixel\n(a) High-confidence C-EDM samples\n(b) Low-confidence C-EDM samples\nFigure 8. Generated high (a) and low (b) confidence CIFAR10\nsamples. The top row depicts the unconditional (Uncond.) sam-\nples, while the bottom row shows the generated samples using C-\nEDM with TEMI (C = 100). Images on the same column are\nproduced with the same initial noise.\npatterns that are easy to generate, such as a white back-\nground. Confidence can also be leveraged in future works\nin rejection sampling schemes [17]. More samples are pro-\nvided in the supplementary material.\n6. Conclusion\nIn this paper, a systematic empirical study was con-\nducted focusing on conditioning diffusion models with clus-\nter assignments. It was demonstrated that cluster condi-\ntioning achieves state-of-the-art FID on three generative\nbenchmarks while attaining strong sample efficiency. To\nreduce the search space for estimating the visual groups of\nthe dataset, a novel method that computes an upper cluster\nbound based solely on clustering was proposed. Finally, our\nexperimental study indicates that generative performance\nusing cluster assignments depends primarily on the gran-\nularity of the assignments.\n\nReferences\n[1] Nikolas Adaloglou, Felix Michels, Tim Kaiser, and Markus\nKollmann. Adapting contrastive language-image pretrained\n(clip) models for out-of-distribution detection.\narXiv e-\nprints, pages arXiv–2303, 2023. 5\n[2] Nikolas Adaloglou, Felix Michels, Hamza Kalisch, and\nMarkus Kollmann. Exploring the limits of deep image clus-\ntering using pretrained models. In 34th British Machine Vi-\nsion Conference 2023, BMVC 2023, Aberdeen, UK, Novem-\nber 20-24, 2023. BMVA, 2023. 2, 3, 4\n[3] Nikolas Adaloglou, Felix Michels, Kaspar Senft, Diana\nPetrusheva, and Markus Kollmann.\nScaling up deep\nclustering methods beyond imagenet-1k.\narXiv preprint\narXiv:2406.01203, 2024. 1, 2\n[4] Elad Amrani, Leonid Karlinsky, and Alex Bronstein. Self-\nsupervised classification network. In European Conference\non Computer Vision, pages 116–132. Springer, 2022. 2\n[5] Uri M. Ascher and Linda R. Petzold. Computer Methods for\nOrdinary Differential Equations and Differential-Algebraic\nEquations. Society for Industrial and Applied Mathematics,\nUSA, 1st edition, 1998. 18\n[6] Arpit\nBansal,\nHong-Min\nChu,\nAvi\nSchwarzschild,\nSoumyadip Sengupta,\nMicah Goldblum,\nJonas Geip-\ning, and Tom Goldstein. Universal guidance for diffusion\nmodels.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 843–852,\n2023. 2\n[7] Bj¨orn Barz and Joachim Denzler. Do we train on test data?\npurging cifar of near-duplicates. Journal of Imaging, 6(6):41,\n2020. 1\n[8] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch¨onlieb,\nand\nChristian\nEtmann.\nConditional\nimage\ngenera-\ntion with score-based diffusion models.\narXiv preprint\narXiv:2111.13606, 2021. 1\n[9] James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng\nWang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee,\nYufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu,\nYunxinJiao, and Aditya Ramesh. Improving image gener-\nation with better captions. 1\n[10] Lucas Beyer, Olivier J H´enaff, Alexander Kolesnikov, Xi-\naohua Zhai, and A¨aron van den Oord. Are we done with\nimagenet? arXiv preprint arXiv:2006.07159, 2020. 1\n[11] Lucas Beyer, Olivier J H´enaff, Alexander Kolesnikov, Xi-\naohua Zhai, and A¨aron van den Oord. Are we done with\nimagenet? arXiv preprint arXiv:2006.07159, 2020. 1\n[12] Florian Bordes, Randall Balestriero, and Pascal Vincent.\nHigh fidelity visualization of what your self-supervised rep-\nresentation knows about, 2022. 2\n[13] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis,\n2019. 2\n[14] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning\nof visual features. In Proceedings of the European confer-\nence on computer vision (ECCV), pages 132–149, 2018. 1\n[15] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650–9660, 2021. 3, 5, 6, 12, 13, 14\n[16] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal\nDrozdzal,\nand Adriana Romero Soriano.\nInstance-\nconditioned gan. Advances in Neural Information Process-\ning Systems, 34:27517–27529, 2021. 2\n[17] George Casella, Christian P Robert, and Martin T Wells.\nGeneralized accept-reject sampling schemes. Lecture Notes-\nMonograph Series, pages 342–347, 2004. 8\n[18] Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming\nXiang, and Chunhong Pan.\nDeep adaptive image cluster-\ning. In Proceedings of the IEEE international conference on\ncomputer vision, pages 5879–5887, 2017. 1\n[19] Xinlei Chen, Saining Xie, and Kaiming He.\nAn empiri-\ncal study of training self-supervised vision transformers. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 9640–9649, 2021. 6, 13, 14\n[20] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell\nWortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-\nmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-\ning laws for contrastive language-image learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2818–2829, 2023. 13, 14\n[21] Min Jin Chong and David Forsyth. Effectively unbiased fid\nand inception score and where to find them. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 6070–6079, 2020. 7\n[22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 4\n[23] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780–8794, 2021. 1\n[24] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780–8794, 2021. 2\n[25] Bao et al. Why are conditional generative models better than\nunconditional ones? In NeurIPS Workshop on Score-Based\nMethods, 2022. 1, 2, 3, 4\n[26] Hariprasath Govindarajan, Per Sid´en, Jacob Roll, and\nFredrik Lindsten. On partial prototype collapse in clustering-\nbased self-supervised learning, 2024. 12\n[27] Dan Hendrycks and Kevin Gimpel. A baseline for detect-\ning misclassified and out-of-distribution examples in neural\nnetworks. In International Conference on Learning Repre-\nsentations, 2017. 8\n[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 3, 4\n[29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840–6851, 2020. 1, 2, 3, 4\n[30] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2, 4, 8\n\n[31] Laura Hollink, Aysenur Bilgin, and Jacco Van Ossen-\nbruggen. Is it a fruit, an apple or a granny smith? predict-\ning the basic level in a concept hierarchy.\narXiv preprint\narXiv:1910.12619, 2019. 1\n[32] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungry-\nong Kim. Improving sample quality of diffusion models us-\ning self-attention guidance. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n7462–7471, October 2023. 2, 8\n[33] Vincent Tao Hu, Yunlu Chen, Mathilde Caron, Yuki M\nAsano, Cees GM Snoek, and Bjorn Ommer. Guided diffu-\nsion from self-supervised diffusion features. arXiv preprint\narXiv:2312.08825, 2023. 2\n[34] Vincent Tao Hu, David W Zhang, Yuki M Asano, Gertjan J\nBurghouts, and Cees GM Snoek. Self-guided diffusion mod-\nels. In CVPR, 2023. 2, 3, 4, 5, 6\n[35] Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming\nShan. Learning representation for clustering via prototype\nscattering and positive sampling. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 2022. 3, 4\n[36] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade\nGordon,\nNicholas Carlini,\nRohan Taori,\nAchal Dave,\nVaishaal Shankar, Hongseok Namkoong, John Miller, Han-\nnaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-\nclip, July 2021. If you use this software, please cite it as\nbelow. 5, 6, 19\n[37] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit,\nDaniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Re-\nthinking fid: Towards a better evaluation metric for image\ngeneration. arXiv preprint arXiv:2401.09603, 2023. 7\n[38] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant in-\nformation clustering for unsupervised image classification\nand segmentation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 9865–9874,\n2019. 1, 3\n[39] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 35:26565–26577, 2022. 1, 2, 3, 4, 14, 15, 17, 20\n[40] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten,\nTimo Aila, and Samuli Laine. Analyzing and improving the\ntraining dynamics of diffusion models, 2023. 2, 5, 12\n[41] Amirhossein\nKazerouni,\nEhsan\nKhodapanah\nAghdam,\nMoein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Haci-\nhaliloglu, and Dorit Merhof.\nDiffusion models for medi-\ncal image analysis: A comprehensive survey. arXiv preprint\narXiv:2211.07804, 2022. 1\n[42] David J Ketchen and Christopher L Shook.\nThe applica-\ntion of cluster analysis in strategic management research:\nan analysis and critique.\nStrategic management journal,\n17(6):441–458, 1996. 4\n[43] Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo\nKang, and Il-Chul Moon.\nRefining generative process\nwith discriminator guidance in score-based diffusion mod-\nels. arXiv preprint arXiv:2211.17091, 2022. 4\n[44] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. Advances in neural infor-\nmation processing systems, 34:21696–21707, 2021. 2\n[45] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 1, 4, 5\n[46] Harold W Kuhn. The hungarian method for the assignment\nproblem. Naval research logistics quarterly, 2(1-2):83–97,\n1955. 12\n[47] Tuomas Kynk¨a¨anniemi, Tero Karras, Miika Aittala, Timo\nAila,\nand Jaakko Lehtinen.\nThe role of imagenet\nclasses in fr\\’echet inception distance.\narXiv preprint\narXiv:2203.06026, 2022. 7\n[48] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi.\nPrototypical contrastive learning of unsupervised representa-\ntions. arXiv preprint arXiv:2005.04966, 2020. 3\n[49] Tianhong Li, Dina Katabi, and Kaiming He.\nSelf-\nconditioned image generation via generating representations.\narXiv preprint, 2023. 2, 6\n[50] Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu,\nand Antonio Torralba. Diverse image generation via self-\nconditioned gans.\nIn Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n14286–14295, 2020. 2, 3\n[51] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li,\nand Yixuan Li.\nDelving into out-of-distribution detection\nwith vision-language representations. Advances in Neural\nInformation Processing Systems, 35:35087–35102, 2022. 5,\n19\n[52] Mehdi Mirza and Simon Osindero. Conditional generative\nadversarial nets. arXiv preprint arXiv:1411.1784, 2014. 2\n[53] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162–8171. PMLR,\n2021. 1, 2\n[54] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 2, 4, 13, 14\n[55] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim,\nand Sangdoo Yun.\nWhat do self-supervised vision trans-\nformers learn?\nIn The Eleventh International Conference\non Learning Representations, 2023. 7\n[56] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn\naliased resizing and surprising subtleties in gan evaluation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11410–11420, 2022.\n7\n[57] William Peebles and Saining Xie. Scalable diffusion mod-\nels with transformers. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n4195–4205, October 2023. 1\n[58] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2\n[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\n\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 1, 5, 6\n[60] Nima Rafiee, Rahil Gholamipoor, Nikolas Adaloglou, Si-\nmon Jaxy, Julius Ramakers, and Markus Kollmann. Self-\nsupervised anomaly detection by self-distillation and nega-\ntive sampling. In International Conference on Artificial Neu-\nral Networks, pages 459–470. Springer, 2022. 6\n[61] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n1(2):3, 2022. 2\n[62] Tal Reiss, Niv Cohen, Eliahu Horwitz, Ron Abutbul, and\nYedid Hoshen. Anomaly detection requires better represen-\ntations. In European Conference on Computer Vision, pages\n56–68. Springer, 2022. 6\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models. 2022 ieee. In CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 10674–10685, 2021. 2\n[64] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradely, Otmar\nHilliges, and Romann M Weber. Cads: Unleashing the di-\nversity of diffusion models through condition-annealed sam-\npling. arXiv preprint arXiv:2310.17347, 2023. 4\n[65] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479–36494, 2022. 1, 8\n[66] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer,\nOran Gafni, Eliya Nachmani, and Yaniv Taigman.\nKnn-\ndiffusion: Image generation via large-scale retrieval, 2022.\n1\n[67] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer,\nOran Gafni, Eliya Nachmani, and Yaniv Taigman.\nKnn-\ndiffusion: Image generation via large-scale retrieval. arXiv\npreprint arXiv:2204.02849, 2022. 2\n[68] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256–2265. PMLR, 2015.\n2\n[69] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2\n[70] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. CoRR, abs/2011.13456, 2020. 14\n[71] George Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi\nSui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan\nLiu, Anthony L. Caterini, Eric Taylor, and Gabriel Loaiza-\nGanem. Exposing flaws of generative model evaluation met-\nrics and their unfair treatment of diffusion models. In Thirty-\nseventh Conference on Neural Information Processing Sys-\ntems, 2023. 2, 4, 7, 12\n[72] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-\nof-distribution detection with deep nearest neighbors. In In-\nternational Conference on Machine Learning, pages 20827–\n20840. PMLR, 2022. 7\n[73] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1–9, 2015.\n4\n[74] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n2818–2826, 2016. 4\n[75] Robert L Thorndike. Who belongs in the family? Psychome-\ntrika, 18(4):267–276, 1953. 4\n[76] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\nGeorgoulis, Marc Proesmans, and Luc Van Gool.\nScan:\nLearning to classify images without labels. In European con-\nference on computer vision, pages 268–285. Springer, 2020.\n2, 3, 13\n[77] Hualiang Wang, Yi Li, Huifeng Yao, and Xiaomeng Li.\nClipn for zero-shot ood detection: Teaching clip to say no.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 1802–1812, 2023. 6\n[78] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised\ndeep embedding for clustering analysis.\nIn International\nconference on machine learning, pages 478–487. PMLR,\n2016. 2\n[79] Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong,\nMax Tegmark, and Tommi Jaakkola. Pfgm++: Unlocking\nthe potential of physics-inspired generative models. arXiv\npreprint arXiv:2302.04265, 2023. 4\n[80] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18381–18391,\n2023. 1\n[81] Asano YM., Rupprecht C., and Vedaldi A. Self-labelling via\nsimultaneous clustering and representation learning. In In-\nternational Conference on Learning Representations, 2020.\n2\n[82] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang\nXie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training\nwith online tokenizer.\narXiv preprint arXiv:2111.07832,\n2021. 6, 12, 13, 14\n[83] Xingzhi Zhou and Nevin L Zhang.\nDeep clustering with\nfeatures from self-supervised pretraining.\narXiv preprint\narXiv:2207.13364, 2022. 4\n[84] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,\nChris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and\nTong Sun. Towards language-free training for text-to-image\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 17907–\n17917, 2022. 2\n\nCode. Code is available at https://github.com/\nHHU-MMBS/cedm-official-wavc2025.\nA. Visualizations\nIn Fig. 10, we map the TEMI clusters to classes using\nthe Hungarian mapping [46] on CIFAR100. For the map-\nping to be one-to-one, we set C = 100. We then generate\nsamples using the same initial noise with both C-EDM and\nGT-conditioned EDM and visualize the first 20 clusters on\nCIFAR100. Given the same initial noise and the cluster that\nis mapped to its respective GT class, we observe a lot of vi-\nsual similarities in the images, even though the two models\n(C-EDM and EDM) have different weights and have been\ntrained with different types of conditioning.\nIn Figs. 11 and 12, we visualize C-EDM samples gen-\nerated from the same initial noise on FFHQ-64 for diffu-\nsion models trained with varying cluster granularities. Each\nnoise gets a condition sampled from p(c). Similar to our\nquantitative analysis, the generated images from small clus-\nter sizes are closer to the unconditional prediction. Finally,\nin Fig. 9, we visualize cluster-conditioned and uncondi-\ntional FFHQ samples at Mimg = 100M.\nIn Fig. 13, we visualize real training FFHQ images that\nare grouped in the same TEMI cluster using the DINO fea-\ntures. We visually identify groups with shared character-\nistics such as sunglasses, hats, beanies, pictures of infants,\nand pictures of speakers talking to a microphone. Finally,\nin Fig. 14 we provide a more detailed visual comparison\nof low and high confidence samples using C-EDM on CI-\nFAR100.\nB. Additional discussion points\nB.1. FID and FDD across training iterations\nIn Fig. 15, we report FID and FDD across training\nusing C-EDM with TEMI clusters.\nWe notice that FID\ntends to saturate faster than FDD and fluctuates more be-\ntween checkpoints. FDD keeps decreasing monotonically,\nwith minimal fluctuation and always prefers the samples at\nMimg = 200. Since both metrics compute the Frechet dis-\ntance, these tendencies can only be attributed to the super-\nvised InceptionV3 features. Even though the study of gen-\nerative metrics is out of the scope of this work and a human\nevaluation is necessary as in [71], we hope that our findings\nw.r.t. cluster-conditioning can facilitate future works.\nB.2. Image synthesis beyond ImageNet.\nImageNet is currently the largest labeled public dataset,\nand a single experiment using a recent state-of-the-art dif-\nfusion model on ImageNet requires up to 4MWh at 5122\nresolution [40]. Based on our experiments, clusters match\nor outperform the human-derived labels on image genera-\ntion by estimating the visual groups. Using the introduced\nupper bound, the search space of the visual groups is signifi-\ncantly reduced with minimal computational overhead, while\nno further hyperparameter tuning is required. Therefore,\nit allows future works to incorporate unlabelled data and\nexperiment at scales beyond ImageNet while being sample\nefficient. Additionally, the sample efficiency compared to\nnoisy or non-mutually exclusive labels could be investigated\nin future works.\nC. Deep image clustering with TEMI\nC.1. Intuition for γ\nIn the TEMI loss function, there are two parts inside\nthe log sum: the numerator\n\u0000qi\ns(c|x)qi\nt(c|x′)\n\u0001γ aligns the\ncluster assignment of a positive pair and is maximal when\neach individual assignment is one-hot. On the other hand,\nthe denominator ˜qi\nt(c) promotes a uniform cluster distribu-\ntion. By dividing element-wise with the cluster probability,\nit is effectively up-weighing the summand corresponding to\nclasses with low probability. In other words, when ˜qi\nt(c)\nis low. The hyperparameter γ reduces the influence of the\nnumerator, which leads to partial collapse [26] when γ = 1.\nC.2. What about the lower bound?\nTEMI with\nγ = 1 experiments.\nStarting with a high overestimation of the number of\nclusters (e.g. 1K for CIFAR10), we find that TEMI clus-\ntering with γ = 1 utilizes a subset of clusters, which could\nbe used as a lower cluster bound. More precisely, we find a\nmaximum standard deviation of 6.4 for Cu across datasets\nand feature extractors (see Supp.). Intuitively, Cu is the\nminimum amount of clusters TEMI (with γ = 1) uses to\ngroup all image pairs. This behavior is analogous to cluster-\nbased self-supervised learning (using image augmentations)\n[15, 82] and has been recently coined as partial prototype\ncollapse [26]. Nonetheless, the lower bound is more ap-\nplicable to large scales as the measured standard deviation\nmight exclude the optimal granularity for small, highly cu-\nrated datasets. Due to the above limitation, we leave this for\nfuture work.\nAs depicted in Tab. 4, the utilized number of clusters Cu\nis not sensitive to the pre-determined number of clusters nor\nthe choice of backbone for TEMI clustering when γ = 1.\nC.3. TEMI with different backbones.\nHere, we report ANMI across various cluster sizes based\non the result reported in the main paper (Fig. 5, main pa-\nper). For all the conducted experiments, we used TEMI\nwith γ = 0.6. Apart from having roughly the same FID,\nwe can observe the ranking of backbones w.r.t ANMI is not\nconsistent across cluster sizes.\n\nFigure 9. Visual comparison between C-EDM (C=400) and unconditional EDM (Uncond.) at Mimg = 100M on FFHQ at 128x128.\nTable 4. Number of utilised clusters Cu for different number of input clusters C (left) and different backbones (right) using TEMI with\nβ = 1 with the DINO ViT-B/16 backbone. We show the relatively small sensitivity of Cu to the choice of C and backbone; we report a\nstandard deviation of a maximum value of 6.37 across different cluster sizes and 6.44 across backbones on CIFAR10.\nTEMI\nCIFAR10\nFFHQ\nCIFAR100\nγ = 1\nCu\nCu\nCu\n100\n33\n36\n48\n400\n38\n48\n48\n500\n34\n54\n51\n800\n40\n45\n47\n1K\n34\n49\n47\n2K\n48\n52\n54\n5K\n28\n42\n51\nMean\n36.4\n46.6\n49.4\nStd\n6.37\n6.16\n2.63\nTEMI\nCIFAR10\nγ = 1, C = 500\nCu\nDINO ViT-B/16 [15]\n34\nMoCOv3 ViT-B/16 [19]\n39\niBOT ViT-L/14 [82]\n45\nOpenCLIP ViT-G/14 [20]\n47\nDINOv2 ViT-g/14 [54]\n50\nMean\n43\nStd\n6.44\nC.4. Dependence on q(c) during generative sam-\npling on balanced classification datasets.\nIt is well-established in the clustering literature that k-\nmeans clusters are highly imbalanced [76]. To illustrate this\n\nFigure 10. Visualizing generated images from CIFAR100 using C-EDM (even rows) and ground truth conditional EDM (odd rows) using\nthe same initial noise and deterministic noise sampling. We map the C=100 CIFAR100 cluster to the respective ground truth class as\ncomputed via the Hungarian one-to-one mapping.\nTable 5. CIFAR10 ANMI across different cluster sizes and state-\nof-the-art feature extractors used for TEMI clustering with γ =\n0.6.\nWe only reported the ANMI for C = 100 in the main\nmanuscript.\nTEMI (γ = 0.6)\nANMI\nANMI\nANMI\nC\n50\n100\n200\nMoCov3 ViT-B/16 [19]\n65.2\n59.3\n55.0\nDINO ViT-B/16 [15]\n65.7\n60.7\n55.8\nDINOv2 ViT-g/14 [54]\n66.1\n63.8\n59.3\niBOT ViT-L/14 [82]\n68.7\n62.7\n57.4\nCLIP ViT-G/14 [20]\n70.6\n64.7\n58.9\nin a generative context, we sample from a uniform clus-\nter distribution instead of q(c) for balanced classification\ndatasets (CIFAR10 and CIFAR100). As expected, k-means\nis more dependent to q(c) compared to TEMI, as its FID is\nsignificantly deteriorated.\nD. The EDM diffusion baseline.\nThis section briefly summarizes the EDM framework for\ndiffusion models, which was used extensively in this work.\nFor more details and the official EDM code, we refer the\nreader to the original paper by Karras et al. [39].\nGiven a data distribution pdata(x), consider the condi-\ntional distribution p(x; σ) of data samples noised with i.i.d.\nGaussian noise of variance σ2. Diffusion-based generative\nmodels learn to follow trajectories that connect noisy sam-\nples x ∼p(x; σ) with data points y ∼pdata(x). Song\net al. [70] introduced the idea of formulating the forward\ntrajectories (from data to noise) using stochastic differen-\ntial equations (SDE) that evolve samples x(σ) according to\np(x; σ) with σ = σ(t) as a function of time t. They also\n\nFigure 11. Visualizing generated images from FFHQ-64 using CEDM for different number of clusters C with the same random noise. We\nuse deterministic noise sampling. Each noise gets a condition sampled from p(c) for each individual clusters.\nUncond.\nC=10\nC=100\nC=200\nC=400\nC=600\nC=1K\nFigure 12. Generated FFHQ-64 samples using C-EDM and TEMI clusters with different granularity levels C as well as unconditional\nEDM (Uncond., first column). All samples in a row use the same initial noise. The cluster assignment is randomly sampled from q(c) for\neach C.\nTable 6. We report FID for k-means and TEMI with and without considering the training data’s cluster distribution q(c). U({1, .., C})\ndenotes the uniform cluster distribution. We use CV = 100, 200, 400 for CIFAR10, CIFAR100 and FFHQ, respectively. ∆quantifies the\nabsolute difference.\nEDM [39]\nSampling Distribution\nCIFAR10\nCIFAR100\nk-means\nU({1, .., C})\n2.75\n2.60\nk-means\nq(c)\n1.69\n2.21\n∆(↓)\n-\n0.79\n0.39\nTEMI\nU({1, .., C})\n1.86\n2.41\nTEMI\nq(c)\n1.67\n2.17\n∆(↓)\n-\n0.19\n0.24\n\nFigure 13. Visualizing training images from FFHQ that belong to the same TEMI cluster. Images that are grouped into the same cluster\nare shown in the same row. We use the trained TEMI model with CV = 400 using the DINO backbone. Cluster assignments are picked to\nillustrate that images with similar visual characteristics are grouped together (i.e., beanies, smiling faces, glasses, hats, kids, etc.). Images\nare randomly sampled from each cluster.\n\n(a) High-confident C-EDM samples\n(b) Low-confident C-EDM samples\nFigure 14. Generated low- (a) and high-confident (b) CIFAR100 samples . The top row depicts the unconditional (Uncond.) samples,\nwhile the bottom row shows the generated samples using C-EDM with TEMI (C = 200). Images on the same column are produced with\nthe same initial noise. Confidence is quantified using maximum softmax probability (MSP). MSP is measured using TEMI trained on\nCIFAR100 without annotated data.\nMimg\n40\n80\n120\n160\n200\n1.7\n1.8\n1.9\n2.0\n2.1\nFID\nCIFAR10\nMimg\n40\n80\n120\n160\n200\n2.3\n2.6\n2.9\n3.2\nFID\nCIFAR100\nMimg\n40\n80\n120\n160\n200\n2.2\n2.4\n2.6\n2.8\nFID\nFFHQ-64\n140\n160\n180\n200\n220\nFDD\n240\n270\n300\n330\nFDD\n190\n220\n250\n280\nFDD\nFID (Inception-v3)\nFDD (DINOv2 ViT-L/14)\nFigure 15. FID score (y-axis, left) and FDD (y-axis, right) during training samples seen (Mimg, x-axis). We used CV = 100, 200, 400 for\nCIFAR10, CIFAR100 and FFHQ-64 respectively.\nproposed a corresponding “probability flow” ordinary dif-\nferential equation (ODE), which is fully deterministic and\nmaps the data distribution pdata(x) to the same noise distri-\nbution p(x; σ(t)) as the SDE, for a given time t. The ODE\ncontinuously adds or removes noise as the sample evolves\nthrough time. To formulate the ODE in its simplest form,\nwe need to set a noise schedule σ(t) and obtain the score\nfunction ∇x log p(x; σ):\n  \\ l abel {eq:od e} \\dd \\rvx = -\\dot {\\sigma }(t) \\sigma (t) \\nabla _\\rvx \\log p(\\rvx ; \\sigma ) \\dd t. \n(2)\nWhile mathematical motivations exist for the choice of\nschedule σ(t), empirically motivated choices were shown to\nbe superior [39]. The main component here, the score func-\ntion, is learned by a neural network through what is known\nas denoising score matching. The core observation here is\nthat the score does not depend on the intractable normaliza-\ntion constant of p(x, σ(t)), which is the reason that diffu-\nsion models in their current formulation work at all (maybe\nremove this side-note). Given a denoiser D(x, σ) and the\nL2-denoising error\n  \\label {eq :L2-obj} \\ma t hb b { E}_{\\rvy \\sim p_{\\text {data}}} \\mathbb {E}_{\\rvn \\sim \\mathcal {N}(0, \\sigma ^2 I)}[\\norm {D(\\rvy + \\rvn , \\sigma ) - \\rvy }^2], \n(3)\nwe can recover the score function via ∇x log p(x, σ) =\n(D(x, σ) −x)/σ2.\nThus, parametrizing the denoiser as\na neural network and training it on Eq. (3) allows us to\nlearn the score function needed for Eq. (2). To solve the\n\nODE in Eq. (2), we can put the recovered score function\ninto Eq. (2) and apply numerical ODE solvers, like Euler’s\nmethod or Heun’s method [5]. The ODE is discretized into\na finite number of sampling times t0, ..., tN and then solved\nthrough iteratively computing the score and taking a step\nwith an ODE solver.\n\nE. Additional implementation details and hy-\nperparameters\nWhen searching for CV , we evaluate EDM after train-\ning with Mimg = 100 and for Mimg = 200 once CV is\nfound. We only report k-means cluster conditioning with\nk = CV . All our reported FID and FDD values are averages\nover 3 runs of 50k images each, each with different random\nseeds. Below, we show the hyperparameters we used for all\ndatasets to enable reproducibility. We always use the aver-\nage FID and FDD for three sets of 50K generated images.\nThe used hyperparameters can be found in Tabs. 7 and 8\nTo assign the CLIP pseudo-labels (Sec. 4.4) to the train-\ning set, we compute the cosine similarity of the image and\nlabel embeddings using openclip’s ViT-G/14 [36]. The la-\nbel embeddings use prompt ensembling and use the five\nprompts: a photo of a <label>, a blurry photo of a <label>,\na photo of many <label>, a photo of the large <label>, and\na photo of the small <label> as in [51].\n\nTable 7. Hyperparameters used for training EDM and C-EDM. Bold signifies that the value is changing across datasets. All other parame-\nters of the training setup were identical to the specifications of Karras et. al [39], which are detailed there.\nHyperparameter\nCIFAR10/CIFAR100\nFFHQ-64/AFHQ-64\nOptimization\noptimizer\nAdam\nAdam\nlearning rate\n0.001\n0.001\nbetas\n0.9, 0.999\n0.9, 0.999\nbatch size\n1024\n512\nFP16\ntrue\ntrue\nSongUNet\nmodel channels\n128\n128\nchannel multiplier\n2-2-2\n1-2-2-2\ndropout\n13%\n5% / 25%\nAugmentation\naugment dim\n9\n9\nprobability\n12%\n15%\n\nTable 8. TEMI hyperparameters\nHyperparameter\nValue\nHead hyperparameters\nMLP hidden layers\n2\nhidden dim\n512\nbottleneck dim\n256\nHead final gelu\nfalse\nNumber of heads (H)\n50\nLoss\nTEMI\nγ\n0.6\nMomentum λ\n0.996\nUse batch normalization\nfalse\nDropout\n0.0\nTemperature\n0.1\nNearest neibohrs (NN)\n50\nNorm last layer\nfalse\nOptimization\nFP16 (mixed precision)\nfalse\nWeight decay\n0.0001\nClip grad\n0\nBatch size\n512\nEpochs\n200\nLearning rate\n0.0001\nOptimizer\nAdamW\nDrop path rate\n0.1\nImage size\n224",
    "pdf_filename": "Rethinking_cluster-conditioned_diffusion_models_for_label-free_image_synthesis.pdf"
}