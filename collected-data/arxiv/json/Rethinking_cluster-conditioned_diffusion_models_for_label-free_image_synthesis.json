{
    "title": "Rethinking cluster-conditioned diffusion models for label-free image synthesis",
    "abstract": "willow_tree oak_tree maple_tree Diffusion-based image generation models can enhance image quality when conditioned on ground truth labels. Here, we conduct a comprehensive experimental study on image-levelconditioningfordiffusionmodelsusingcluster telephone telephone telephone assignments. We investigate how individual clustering de- terminants,suchasthenumberofclustersandthecluster- ing method, impact image synthesis across three different datasets. Giventheoptimalnumberofclusterswithrespect to image synthesis, we show that cluster-conditioning can telephone telephone telephone achieve state-of-the-art performance, with an FID of 1.67 for CIFAR10 and 2.17 for CIFAR100, along with a strong increase in training sample efficiency. We further propose a novel empirical method to estimate an upper bound for theoptimalnumberofclusters. Unlikeexistingapproaches, we find no significant association between clustering per- formance and the corresponding cluster-conditional FID Figure1. Anidealimage-levelconditioningshouldgroupimages scores. Code is available at https://github.com/ based on shared patterns, shown in the same row, which do not HHU-MMBS/cedm-official-wavc2025 alwaysalignwithhumanlabels,indicatedaboveeachimage(CI- FAR100[45]samples). 1.Introduction ing annotation hierarchies [3,11,31]. An ideal condition- Diffusion models have enabled significant progress in ing signal would be based solely on shared characteristics many visual generative tasks, such as image synthesis corresponding to general visual concepts, as illustrated in [29,39,57] and manipulation [80]. Conditioning diffusion Fig.1. Toprovideagroundtruth(GT)label-freealternative models on human-annotated data is today’s standard prac- thatisapplicabletounlabelleddatasets,wefocusoncluster- ticeasitsignificantlyimprovestheimagefidelity[8,23,25]. based conditioning on diffusion models. Image clustering Image-level conditioning is typically realized by using as- refers to algorithmically assigning a semantic group to an sociated text captions [65,66] or class labels, if available image, called a cluster, given an a priori number of such [29,53]. Nonetheless, human-annotated labels are costly groups[14,18,38]. andoftencontaininaccuracies[7,10],whilepubliclyavail- Maximizing cluster alignment with ground truth (GT) ableimage-textpairscanbenon-descriptive[9]. labels does not guarantee optimal cluster-conditional im- Large,human-annotateddatasetsareprohibitivelycostly age generation. To create visually distinguishable groups [59]andhardtoobtaininaplethoraofreal-lifeapplications (Fig.1),aclusteringalgorithmwouldsometimesgroupim- suchasmedicalimagesynthesis[41]. Humanannotationis agesintomorefine-grainedgroupsthantheGTlabels,i.e., subjecttothelabelcollectionprocedureandcanhavevary- sub-grouping analog and digital devices that map to the 4202 voN 91 ]VC.sc[ 2v07500.3042:viXra 0 retsulC 1 retsulC 2 retsulC",
    "body": "Rethinking cluster-conditioned diffusion models for label-free image synthesis\nNikolasAdaloglou TimKaiser\nHeinrichHeineUniversityofDusseldorf HeinrichHeineUniversityofDusseldorf\nadaloglo@hhu.de tikai103@hhu.de\nFelixMichels MarkusKollmann\nHeinrichHeineUniversityofDusseldorf HeinrichHeineUniversityofDusseldorf\nfelix.michels@hhu.de markus.kollmann@hhu.de\nAbstract\nwillow_tree oak_tree maple_tree\nDiffusion-based image generation models can enhance\nimage quality when conditioned on ground truth labels.\nHere, we conduct a comprehensive experimental study on\nimage-levelconditioningfordiffusionmodelsusingcluster\ntelephone telephone telephone\nassignments. We investigate how individual clustering de-\nterminants,suchasthenumberofclustersandthecluster-\ning method, impact image synthesis across three different\ndatasets. Giventheoptimalnumberofclusterswithrespect\nto image synthesis, we show that cluster-conditioning can\ntelephone telephone telephone achieve state-of-the-art performance, with an FID of 1.67\nfor CIFAR10 and 2.17 for CIFAR100, along with a strong\nincrease in training sample efficiency. We further propose\na novel empirical method to estimate an upper bound for\ntheoptimalnumberofclusters. Unlikeexistingapproaches,\nwe find no significant association between clustering per-\nformance and the corresponding cluster-conditional FID Figure1. Anidealimage-levelconditioningshouldgroupimages\nscores. Code is available at https://github.com/ based on shared patterns, shown in the same row, which do not\nHHU-MMBS/cedm-official-wavc2025 alwaysalignwithhumanlabels,indicatedaboveeachimage(CI-\nFAR100[45]samples).\n1.Introduction\ning annotation hierarchies [3,11,31]. An ideal condition-\nDiffusion models have enabled significant progress in ing signal would be based solely on shared characteristics\nmany visual generative tasks, such as image synthesis corresponding to general visual concepts, as illustrated in\n[29,39,57] and manipulation [80]. Conditioning diffusion Fig.1. Toprovideagroundtruth(GT)label-freealternative\nmodels on human-annotated data is today’s standard prac- thatisapplicabletounlabelleddatasets,wefocusoncluster-\nticeasitsignificantlyimprovestheimagefidelity[8,23,25]. based conditioning on diffusion models. Image clustering\nImage-level conditioning is typically realized by using as- refers to algorithmically assigning a semantic group to an\nsociated text captions [65,66] or class labels, if available image, called a cluster, given an a priori number of such\n[29,53]. Nonetheless, human-annotated labels are costly groups[14,18,38].\nandoftencontaininaccuracies[7,10],whilepubliclyavail- Maximizing cluster alignment with ground truth (GT)\nableimage-textpairscanbenon-descriptive[9]. labels does not guarantee optimal cluster-conditional im-\nLarge,human-annotateddatasetsareprohibitivelycostly age generation. To create visually distinguishable groups\n[59]andhardtoobtaininaplethoraofreal-lifeapplications (Fig.1),aclusteringalgorithmwouldsometimesgroupim-\nsuchasmedicalimagesynthesis[41]. Humanannotationis agesintomorefine-grainedgroupsthantheGTlabels,i.e.,\nsubjecttothelabelcollectionprocedureandcanhavevary- sub-grouping analog and digital devices that map to the\n4202\nvoN\n91\n]VC.sc[\n2v07500.3042:viXra\n0\nretsulC\n1\nretsulC\n2\nretsulC\nsame label or merge similarly looking images with differ- 2.RelatedWork\nentlabels(mergingdifferenttypesoftrees).\n2.1.Conditionalgenerativemodels\nEarly attempts on controllable image synthesis adopted\nDeep image clustering has recently seen significant\ngenerative adversarial networks (GANs) [52]. By condi-\nprogress [4,78,81], especially when utilizing off-the-shelf\ntioning both the generator and discriminator on human la-\nfeatureslearnedfromlarge-scalepre-training[2,3,76].Cur-\nbels, GANs can produce images for a specific GT label,\nrently,therelationshipbetweenclusteringperformanceand\neven at the scale of ImageNet [13]. Recently, diffusion\ncluster-conditional image synthesis performance remains\nmodels (DMs) have emerged as an expressive and flexi-\nunclear for several reasons. First, clustering metrics cap-\nble category of generative models [30,40,63,68,69]. In-\nturethealignmenttoGTlabels,whereasgenerativemodels\nternal guidance-based methods [24,30,32,33] further im-\nmaybenefitmorefromvisuallydistinguishablegroups[34].\nprovedtheflexibilityandvisualfidelityofDMsduringsam-\nSecond, clustering metrics cannot be computed for unla-\npling such as classifier and classifier-free guidance. DMs\nbeled datasets. Third, existing metrics are not suitable for\nexhibit an enormous number of label-conditional variants\nfairlyevaluatingclusterassignmentswithvaryingnumbers\n[29,39,44,53,58]. Recently, Stein et al. [71] proposed\nofclusters.Therefore,theycannothelptodeterminetheop-\nthe adoption of Fre´chet DINOv2 [54] distance as it aligns\ntimalclustergranularityw.r.t.generativeperformanceforan\nbetterwithhumanpreferencesanddemonstratedthatDMs\narbitrary dataset. This aspect has not been sufficiently ex-\nachievethehighestperceptualrealism.\nploredtodate[25,50].\n2.2.Alternativeconditioningofgenerativemodels\nExisting cluster-conditioned generative approaches [25,\nUnlikeinternalguidance[30,32,33],externalcondition-\n34,50] often adopt simple generative baselines [29] and\ning signals are computed from the training set using ad-\nhave mainly investigated k-means clustering on balanced\nditional models, which is commonly referred to as self-\nclassification datasets. The adoption of k-means is justi-\nconditioning. Self-conditioning signals can be roughly di-\nfiedduetoitsabilitytoworkwithoff-the-shelffeatureex-\nvided into image-level and sub-image-level. Image-level\ntractors and its non-parametric nature, apart from setting\nconditioningreferstoasingleconditionforallpixelsinan\nthe number of clusters. Conversely, it is well-established\nimage, such as cluster assignments or text captions [63].\nthatk-meansissuboptimalforimageclusteringbecauseits\nSub-image-levelconditionsrefertospecificpartsorregions\ncluster assignments are highly imbalanced [34,76]. While\nofanimage[6,34]. Forinstance,Huetal.[34]extendDMs\ncluster-conditioned approaches are sensitive to the num-\nto incorporate conditioning of bounding boxes or segmen-\nber of clusters [25], there is currently no method to pre-\ntationmasks.\ndeterminethisnumber. Previousmethodshaveoverlooked\nFeature-based conditioning. Using image or text fea-\nthis [25,34,50], significantly hindering their applicability\ntures can also provide an informative conditioning signal\nto unlabeled datasets. Recent feature-based image clus-\n[12,49,61,67]. Bordes et al. [12] condition DMs directly\ntering methods have demonstrated superior performance\non image representations (oracle features from real im-\ncompared to k-means [2,76], and their efficacy in condi-\nages) and reveal their impact on visual fidelity. Instance-\ntional image generation is yet to be investigated. For the\nconditioned GAN combines GT labels with features from\nabovereasons, cluster-conditioningmethodsunderperform\neachimage’sNNset[16]. Rameshetal.[61]leveragethe\nGTconditionalonestodate.\nzero-shotcapabilityofCLIPtoconditionDMsonlanguage-\nguided image representations to improve diversity. Zhou\nIn this paper, we systematically study clustering as a etal. providealanguage-freeframework[84]thatdirectly\nlabel-free condition for diffusion models. We demonstrate generates text features for images using CLIP. Still, CLIP\nthatoptimalclustergranularityachievesstate-of-the-artFID modelsrequireanapriorisetofdescriptivecandidatecap-\nscores (1.67 for CIFAR10 and 2.17 for CIFAR100) while tions,suchaslabelnames.\nenhancingtrainingsampleefficiency.Weproposeacompu- Cluster-basedconditioning. Similartolabelconditioning,\ntationallyefficientmethodtoderiveanupperclusterbound conditioning on cluster assignments facilitates DMs by al-\nusing feature-based clustering, which narrows the search lowingthemtospecializeonadistinctsetofsharedvisual\nspace for optimal image synthesis performance. We vali- features [25,34,50]. In [25], Bao et al. computed the k-\ndate this upper bound across three datasets and two gen- meansclustersofflineusingcontrastivelearnedfeatureson\nerative metrics. Finally, we find no significant association thetrainingdata.However,theirapproachcouldnotoutper-\nbetweenclusteringperformanceandcluster-conditionalim- form the label-conditioned models. The first cluster-based\nage synthesis performance across various clustering meth- approachthatachievescompetitiveperformancecompared\nods,performancemetrics,andpre-trainedmodels. to GT labels leverages pre-trained feature extractors [34],\nwhich is the closest to our work. In [34], the authors at- putedbyanexponentialmovingaverageovermini-batches\ntempt to weak establish a correlation between clustering Bdefinedasq˜i(c)←λq˜(c)+(1−λ) 1 (cid:80) qi(c|x).\nt t |B| x∈B t\nandcluster-conditionalgenerativeperformancefordifferent The hyperparameter γ ∈ (0.5,1] avoids the collapse of\nself-supervisedmodels. Theobservedcorrelationcannotbe all sample pairs into a single cluster, and λ ∈ (0,1) is a\ntestedonunlabeleddatasets,anditssensitivitytothenum- momentum hyperparameter. The loss function Eq. (1) is\nberofclustersisunknown. Whileitwasdemonstratedthat furthersymmetrizedandaveragedoverheadstoobtainthe\nDINO [15] provides the most informative k-means clus- final training loss. After training, the cluster assignments\nters[34],theirmethodlacksastrategyforchoosingtheclus- c∗(x) = argmax q (c|x) and the empirical cluster distri-\nc t\ntergranularityforunlabeleddatasets. butionq(c)onthetrainingsetiscomputedfromtheTEMI\nhead with the lowest loss. We use c∗(x) as a condition of\n3.Method thegenerativemodelduringtrainingandsamplefromq(c)\ntogenerateimages. Adaloglouetal.[2]showthatthevalue\n3.1.Notationsandprerequisites\nγ = 0.6enforcesaclosetouniformclusterutilizationand\nWeconsiderthefrequentscenariowherewehaveaccess achievesstate-of-the-artclusteringaccuracyforC GT [2].\ntoanunlabeleddatasetDandapre-trainedfeatureextrac-\n3.2.Cluster-conditionalEDM(C-EDM)\ntorg(·). Wedenotethenumberofgroundtruth(GT)labels\nas C GT, which we assume to be unknown. To distinguish We consider k-means and TEMI clusters using off-the-\ncluster-based conditioning from the image clustering task, shelffeatureextractorswithC clusterstocomputecluster\nV\nwe denote the number of visual groups C V of D as the assignments. WeexperimentallyobservedthatC V >C GT,\noptimal number of clusters w.r.t. image synthesis (e.g. as eventhoughthisisnotarequirementofourapproach. We\nmeasured by FID [28]). We adopt the diffusion model ap- highlight that clustering using C > C while enforcing\nGT\nproachofKarrasetal. (EDM[39])asabaseline,whichin- auniformclusterutilization(suchasTEMIwithγ = 0.6)\ntroducedvariousimprovementstothestandardDDPM[29]. hasnotbeenpreviouslyexplored[2,35,48,76]asitreduces\nWeprovideanoverviewofEDMinthesupplementaryma- the cluster alignment with the GT labels. We denote the\nterial. Although we focus on EDM throughout this work, cluster-conditionedEDMmodelasC-EDM.\nour method can be applied to any conditional generative AdirectestimationofC ishardtoobtainforunlabeled\nV\nmodel. datasets. In principle, C can be found using a hyperpa-\nV\nTEMIclustering. Givenanapriorideterminednumberof rameter search with FID as an evaluation metric, which\nclustersCandafeatureextractorg(.),TEMI[2]firstmines is computationally expensive. In contrast to existing ap-\nthe m nearest neighbors (NN) of all x ∈ D in the feature proaches that perform a restricted hyperparameter search\nspaceofg(·)basedontheircosinesimilarity.Wedenotethe around C , we propose a new metric that allows us to\nGT\nset of NN for x by S x. During training, TEMI randomly deriveanupperboundforC\nV\n(Sec.3.3),whichrequiresno\nsamplesxfromD andx′ fromS x togenerateimagepairs priorknowledgeaboutthedataset.\nwithsimilar(visual)features. Aself-distillationframework\n3.3.Estimatingtheupperclusterbound\nisintroducedtolearntheclusterassignmentswithateacher\nandstudentheadh (·)andh (·)thatsharethesamearchi-\nt s WeaimtofindanupperclusterboundC usingTEMI\nmax\ntecture(i.e.3-layerMLP)butdifferw.r.t.theirparameters.\n(γ =0.6),suchthatC <C .Duringthiscomputation,\nV max\nThe features of the image pair are fed to the student and\nwe neither use the generative model nor any additional in-\nteacher heads hi(z), hi(z′), where i ∈ {1,...,H} is the\ns t formation. We highlight that previous works [25,34,50]\nhead index and z = g(x), z′ = g(x′). The outputs of the\nhave overlooked this design choice and iterate within a\nheadsareconvertedtoprobabilitiesq (c|x)andq (c|x′)us-\ns t smallrangearoundC . Wedenotethenumberofutilized\nGT\ningasoftmaxfunction. TheTEMIobjective, clusters (one training sample is assigned to it) as Cu after\nTEMI clustering. The TEMI clusterutilization ratio is de-\nLi (x,x′):=− 1 (cid:88)H (cid:88)C qj(c′|x)qj(c′|x′) finedasr C := C Cu ≤ 1. Importantly,thereisnoguarantee\nTEMI H t t that the full cluster spectrum will be utilized. Unlike k-\nj=1c′=1\n(1) meansthatalwayshasr =1,weobservethatasthenum-\nlog(cid:88)C (cid:0) q si(c|x)q ti(c|x′)(cid:1)γ\n,\nberofclustersC increasC es,theTEMIclusterutilizationra-\nq˜i(c) tio r typically decreases, which provides dataset-specific\nc=1 t C\ninformation. Intuitively, TEMI clustering with γ = 0.6 is\nmaximizes the pointwise mutual information between im- enforcing r → 1, and the observation of r < 1 is an\nC C\nages x and x′, using the clustering index c as information indication that the maximum number of TEMI clusters is\nbottleneck [38]. Here, q˜i(c) is an estimate of the teacher reachedforγ =0.6.\nt\ncluster distribution E (cid:2) qi(c|x)(cid:3) , which can be com- The search for C involves doubling the number of\nx∼pdata t max\nCIFAR10 CIFAR100 FFHQ-64 FFHQ-128\nGenerativemethods Unlabelled GT Unlabelled GT Unlabelled Unlabelled\nDDPM[29,34] 3.17 - 10.7 9.7 - -\nPFGM++[79] - 1.74 - - 2.43 -\nEDM[39] 1.97 1.79 - - 2.39 -\nEDM(ourreproduction) 2.07 1.81 3.41 2.21 2.53 5.93\nSelf-conditionalmethods\nIDDPM+k-means(k=10)[25] 2.23 - - - - -\nDDPM+k-means†(k=400)[34] - - 9.6 - - -\nC-EDM+k-means†(Ours) 1.69 - 2.21 - 1.99 -\nC-EDM+TEMI†(Ours) 1.67 - 2.17 - 2.09 4.40\nEDM+oraclefeatures†(Ours) 2.21 - 2.25 - 1.77 -\nTable1. State-of-the-artgenerativemodelcomparison: FID(↓)forvariousGTlabeledandunlabelledbenchmarks. WeuseC =100,\nV\n200,400clustersforCIFAR10,CIFAR100,andFFHQ-64,respectively. Self-conditionalmethodswith†usethepre-trainedDINOViT-B\nfeatureextractor.Groundtruthandoraclefeatureconditioningresultsaremarkedingraycolorastheyarenotafaircomparison.\nclusters C at each iteration until the r falls below a theTEMIclusteringheadsfor200epochsperdataset. All\nC\nthreshold, r ≤ α, followed by a more fine-grained grid theexperimentswereconductedon4NVIDIAA100GPUs\nC\nsearch. C is then defined as the highest C for which with 40GB VRAM each. On this hardware, TEMI clus-\nmax\nr > α. This results in a worst-case time complexity of tering was more than 50× faster than training EDM on\nC\nO(log 2(C Cm staa rtx)). This is conceptually similar to the elbow FFHQ-64, whichrequiresmorethan2daysfor200M img\nmethod[42,75].Inthissense,theproposedheuristichasthe with a batch size of 512. Additional implementation de-\nsame limitations as the elbow method. Yet, it is currently tails and hyperparameters can be found in the supplemen-\ntheonlymethodthatprovidesapractitionerwithastarting tary material. We verify our approach across CIFAR10,\npoint for an unlabelled dataset. After detecting C , we CIFAR100[45]andFFHQ.CIFAR10andCIFAR100have\nmax\nperformaboundedgridsearchtofindC ∈ [2,C )us- 50K samples and 322 resolution images, while we use the\nV max\ningthegenerativemodel. Toobtainasimplecross-dataset 64x64 and 128x128 versions of FFHQ (FFHQ-64, FFHQ-\nestimate,weempiricallyfindacutoffthresholdr ≤ 0.96 128)consistingof70Ksamples.Finally,diffusionsampling\nC\ntoworkwellacrossthreedatasets. methodsarenotincluded, astheycanbeincorporatedinto\nanydiffusionmodel[30,43,64].\n4.Experimentalevaluation\n4.2.State-of-the-artcomparisonforimagesynthesis\n4.1.Datasets,models,andmetrics\nFollowing prior works [2,34,83], we use DINO ViT- Comparisonwithstate-of-the-artunconditionalgen-\nB pre-trained on ImageNet [22] for image clustering. We erative models. With respect to FID, we found C V =\nreport the Fre´chet inception distance (FID) [28] to quan- 100,200,400tobeclosetooptimalclustergranularitiesfor\ntifytheimagegenerationperformanceasitsimultaneously CIFAR10,CIFAR100,andFFHQ-64,respectively. Weuse\ncaptures visual fidelity and diversity. To facilitate future thesevaluestoreportFIDscomparedtoGTconditionaland\ncomparisons, we also compute the Fre´chet DINOv2 dis- unconditional generative models in Tab. 1. Compared to\ntance(FDD)[71]thatreplacesthefeaturesofInceptionNet- unconditional methods, C-EDM achieves an average rela-\nv3 [73,74] with DINOv2 ViT-L [54]. FID and FDD are tiveFIDimprovementof24.4%and24.9%usingTEMIand\naveragedoverthreeindependentlygeneratedsamplesetsof k-meansclusters,respectively. Moreimportantly,previous\n50K images each. To measure the cluster alignment w.r.t. cluster-conditional approaches did not achieve near-state-\nGTlabels,weusetheadjustednormalizedmutualinforma- of-the-art FIDs because they: a) adopted non-competitive\ntion(ANMI)asin[35]. diffusionbaselinessuchasDDPM[29],b)didnotconsider\nWe follow the default hyperparameter setup for EDM pre-trainedfeatureextractorsforclustering[25],c)didnot\n[39] and TEMI [2]. We denote the number of samples (in usetheoptimalclustergranularity[34]. Forinstance,Huet\nmillions) seen during training as M . We use M = al.[34]used400clustersonCIFAR100usingDINOViT-B\nimg img\n200 when comparing C-EDM with other baselines and forclustering,whileFig.3showsthat200clustersleadtoa\nstate-of-the-art methods (Tab. 1), and M = 100 when superiorFIDusingC-EDM(11.2%relativeimprovement).\nimg\ncomparing across different number of clusters. We train Comparison with state-of-the-art GT conditional\nCIFAR10 CIFAR100 FFHQ\nFID FID FID\n2.6 4.5 3.4\n2.4 4.0 3.1\n2.2 3.5 2.8\n5×\n2.0 5× 3.0 2.5 3.3×\n1.8 2.5 2.2\nMimg 40 80 120 160 200 Mimg 40 80 120 160 200 Mimg 40 80 120 160 200\nTEMI k-means Unconditional GT labels GT superclasses\nFigure 2. FID (y-axis) versus seen samples during training in millions (x-axis). TEMI and k-means clusters are computed using the\nrepresentationsofDINOViT-B[15]. WeusedC = 100,200,400forCIFAR10,CIFAR100andFFHQ-64respectively. Thetraining\nV\nsampleefficiencycomparedtotheunconditionalbaselineisindicatedbythearrow.Bestviewedincolor.\ngenerative models. Intriguingly, using C-EDM with C C > C forCIFAR10andCIFAR100, whichisinline\nV V GT\nclusters, we report small improvements compared to GT with[34]. Eventhoughthechoiceofα =0.96isnotguar-\nlabel conditioning on CIFAR10 and CIFAR100 in Tab. 1. anteedtobegenerallyapplicableandisbasedonempirical\nForinstance,4.77%meanrelativeimprovementinFIDus- evidence,amoreorlessstrictchoicecanbeusedbasedon\ning TEMI. Even though cluster-conditioning is primarily thepractitioner’scomputationalbudget.Theintroducedup-\ndesigned for unconditional generation, such as FFHQ, we per bound can be computed with negligible computational\ndemonstrate that it can match or outperform GT labels, overhead and without access to GT labels or training the\nwhichshowcasestheeffectivenessofC-EDM. generativemodel,whichonlargescales(ImageNet)canre-\nSampleefficiency. InadditiontothereportedgainsinFID, quireupto4MWhperexperiment[40].\nwe study the sample efficiency of C-EDM during training\n4.4.Investigatingtheconnectionbetweenclustering\nacrossdatasetsinFig.2. OnCIFAR10andCIFAR100,the\nandcluster-conditionalimagesynthesis\ntraining sample efficiency for C = C compared to the\nV\nunconditionalmodel(C = 1)peaksat5×, whereC-EDM FID and ANMI. On image clustering benchmarks, TEMI\nwith M img = 40 outperforms the unconditional model at (γ = 0.6) outperforms k-means, where we respectively\nM img = 200. On FFHQ-64, which is not a classification measureanANMIof60.6%versus59.2%onCIFAR10and\ndataset, we report a sample efficiency of 3.3×. TEMI and 72.3%versus67.6%onCIFAR100. However,theirgenera-\nk-meansdemonstrateidenticalsampleefficiencycompared tiveperformancedisplaysnegligibledifferencesintermsof\nto the unconditional model. Intuitivly, a more informative FIDforC = C . Weemphasizethattheimbalancedclus-\nV\nconditioning signal enables learning the data distribution ters of k-means are penalized when computing clustering\nfaster. Upon visual inspection, we could identify FFHQ metrics such as ANMI on balanced classification datasets,\nclustersforC =C V witheasilydistinguishablevisualpat- whileduringimagesynthesis,thisisnaturallymitigatedby\nterns,suchasgroupsofimageswithbeanies,smilingfaces, samplingfromq(c). Tab.1suggeststhatimbalancedclus-\nglasses,sunglasses,hats,andkids(seesupplementary). Fi- ter assignments are beneficial for unlabelled datasets such\nnally,wefindthatthesampleefficiencyofC-EDMagainst as FFHQ, where we report a relative gain of 4.8% using\nGT conditional EDM heavily depends on the quality and k-meansclusterscomparedtoTEMI.\ngranularityoftheGTlabels. C-EDM matches EDM when C = C . In Fig. 3, we\nGT\nobserve that the GT label conditioning closely follows the\n4.3. Cluster utilization ratio and discovered upper\nFID of TEMI clusters for C = C . In Tab. 2, we lever-\nbounds GT\nage the two annotation levels of CIFAR100 [45], specif-\nWedepicthowtheTEMIclusterutilizationr changes ically the 20 GT superclasses and the 100 GT labels to\nC\nin tandem with FID for different numbers of clusters in benchmarkhowdifferentgroupingmethodsperforminim-\nFig.3.Crucially,thisapproachalsoappliestoFFHQ,where agesynthesis. Apartfromimageclustering,wecreatetext-\nno GT labels exist. Moreover, we can discern certain pat- basedpseudo-labelswithCLIP(OpenCLIPViT-G/14[36])\nterns across datasets: (i) C always has a high utilization similarto[51]. Then,CLIPpseudo-labelsarederivedfrom\nV\nratio,(ii)themajorityofexperimentswithC < C out- zero-shotclassification[59]using theargmaxofthetext-\nmax\nperformtheunconditionalmodelatM = 100,and(iii) image similarity after softmax [1]. For reference, we pro-\nimg\nCIFAR10 CIFAR100 FFHQ\nFID rC FID rC FID rC\n3.2 1.0 3.6 1.0 2.8 1.0\n2.8 0.98 3.2 0.97 2.6 0.97\n2.4 0.96 2.8 0.94 2.4 0.94\n2.0 0.94 2.4 0.91 2.2 0.91\nC = 1 200 400 600 800 C = 1 200 400 600 800 1000 C = 1 200 400 600 800 1000\nFID TEMI cluster utilization ratio (r ) GT labels (FID)\nC\nFigure3.FID(lefty-axis)andTEMIclusterutilizationratior (righty-axis)acrossdifferentnumbersofclustersC(x-axis)usingC-EDM,\nC\nevaluatedatM =100.Thegreenareaindicatesthediscoveredclusterrange[2,C )forr ≤α=0.96.\nimg max C\nC CIFAR10\nFID\nCIFAR100 20 100 200\n2.2\nHumanannotation(GT) 3.10 2.41 -\nCLIPpseudo-labels 3.38 2.42 -\n2.0\nk-meansclusters 3.09 2.41 2.36\nTEMIclusters 2.93 2.37 2.31 1.8\nTable 2. FID for different grouping methods as a condition to C = 1 50 100 150 200 250 300 350 400\nEDM (M = 100). We use the 20 GT superclasses and 100\nimg\nMoCov3 (59.3) iBOT (62.7)\nGT labels of CIFAR100. CLIP pseudo-labels are computed us-\ning zero-shot classification pseudo-labels based on the GT label DINO (60.7) OpenCLIP (64.7)\nnames. DINOv2 (63.8)\nFigure4. FID(y-axis)acrossdifferentnumbersofclustersC (x-\naxis)usingC-EDMwithTEMIwithdifferentfeatureextractors.\nvide the FID with the number of visual groups C\nV\n= 200 TheANMIisshowninparenthesesforC V=100.\nfor k-meansand TEMIclusters. Interestingly, allmethods\nattainasimilarFIDforC = 100,whileTEMIachieves\nGT\nthebestFIDonlyforC =20. Thissuggeststhatgenerative\nperformance is mostly invariant to the grouping method,\nconditioning. The fact that differences in discriminative\nincluding human annotation, given a feature extractor that\nperformancedonottranslatetoimprovementsinFIDisin\ncaptures general visual concepts. Next, we investigate the\ncontrastwithHuetal.[34].\nimpactofthepre-trainedfeatureextractor.\nThenumberofvisualgroupsisinsensitivetothechosen Dataset-specificfeaturesdonotimprovegenerativeper-\nfeatureextractor. Thebroaderadoptionofthediscovered formance. To realize dataset-dependent adaptation of the\nC for each dataset requires our analysis to be insensitive feature extractor, we perform self-supervised fine-tuning\nV\ntothechosenfeatureextractor. InFig.4,weshowthatthe on CIFAR10 and CIFAR100, starting from the ImageNet\npre-trainedmodelsconsideredinthisworkachievesimilar weights in Tab. 3. We train all layers for 15 epochs\nFIDs across different numbers of clusters even though the similar to [62] using the DINO framework, resulting in\nANMI varies up to 5.4% on CIFAR10. Feature extractors “DINO adapted weights”. We find large improvements in\ndifferintermsofthenumberofparameters(86Mupto1.8 classification accuracy and ANMI, which is in line with\nbillion), pre-trainingdatasetsize(1.2Mupto2B),andob- the fact that the learned features are more dataset-specific\njective(MoCO[19],DINO[15],iBOT[82],CLIP[36,59]). [60,62,77]. However, the gains in classification and clus-\nOurfindingisconsistentwithconcurrentworkonthescale tering do not translate into conditional image generation,\nofImageNet[49]thatshowsthatlinearprobingaccuracyis which suggests that image synthesis requires features that\nnot indicative of generative performance for feature-based capturegeneralvisualconcepts.\nCIFAR10(C =100) CIFAR100(C =200)\nC-EDM+TEMI ACC↑ ANMI↑ FID↓ FDD↓ ACC↑ ANMI↑ FID↓ FDD↓\nDINOImageNetweights 96.4 60.6 1.73 153.5 82.3 72.3 2.31 250.7\nDINOadaptedweights 98.5 63.41 1.74 152.8 87.8 78.2 2.39 250.8\nTable3. Effectofself-supervisedfine-tuning(adaptedweights)onthepre-trainedDINOViT-B.ACCstandsforthe20-NNclassification\naccuracyusingtheDINOfeatures.WemeasureFIDandFDDforbothmodelsatM =100.\nimg\nCIFAR10 CIFAR100\nAUROC uFIDAUROC uFID\n0.66 3.1\n0.61 3.1\n0.65 2.7\n0.6 2.4\n0.64 2.3\n0.59 1.7\n0.63 1.9\nC=1 200 400 600 800 C=1 200 400 600 800 1000\nAUROC uFID Unconditional AUROC\nFigure5. Top1-NNcosinesimilarityAUROC(lefty-axis)andFrechetdistancebetweentheC-EDMandunconditionalsamples(uFID)\nfordifferentclustersizesC(x-axis).ForthecomputationofAUROC,weusetheofficialtestsplits.\n5.Discussion significant connection between the task of feature-based\nclusteringandcluster-conditionalimagegeneration(Fig.4\nHighly fine-grained clusters lead to out-of-\nand Tab. 3). The pre-trained model and the categorization\ndistribution samples. To quantify the degree of out-\nmethod do not severely affect generative performance for\nof-distribution for the generated C-EDM samples, we\ntheconsidereddatasets.\nmeasure the AUROC using the top-1 NN cosine similarity\nGenerative metric and optimal number of clusters.\n(1-NN) using DINO ViT-B [72] and compare with the\nHere, we investigate the impact of the choice of genera-\ntest split of CIFAR10 and CIFAR100 (Fig. 5). We note\ntivemetricbycomparingFDDandFID.Wehighlightthat\nthat 1-NN is independent of the data distribution of the\nthe only difference is that FID and FDD use InceptionV3\ngenerated samples and thus does not account for diversity.\nandDINOv2features,respectively. Thecross-dataseteval-\nThe highest C considered for CIFAR10 and CIFAR100\nuation in Fig. 6 shows that FID and FDD do not always\nproduces the highest AUROC, suggesting that the features\nagree with respect to the number of visual groups. The\nofthegeneratedsamplesarepushedawayfromthetraining\nlargest disagreement is observed on CIFAR10. Still, the\ndata. We hypothesize that this behavior originates from\ndiscovered upper bound C always includes the num-\nhighlyspecializedclustersthatcannotalwaysbegenerated max\nber of visual groups. Moreover, we evaluate both metrics\nfrom the initial noise, resulting in visual artifacts. In\nacross training iterations and found that while FID fluc-\nparallel, we measure how similar the C-EDM samples are\ntuates after M = 120, FDD decreases monotonically\ncomparedtotheunconditionalonesusingFID.Wecallthis img\n(supplementarymaterial). Finally, wemeasuredadditional\nmetric unconditional FID (uFID). We observe that uFID\nfeature-based metrics such as precision, recall, diversity,\nincreases as C increases, suggesting that the generated\ncoverage,MSS,andinceptionscorewithoutsuccess. Simi-\nsamplesforlargerC arefartherawayinfeaturespacethan\nlarto[21,37,47,56,71],wearguethatnewgenerativemet-\ntheunconditionalsamples.\nricsmustbedeveloped.\nThe clusters’ granularity level determines conditional\ngenerativeperformance. Thequalityoffeaturerepresen- VisualcomparisonofC-EDMtoEDM.InFig.7,wemap\ntations is typically determined by linear separability w.r.t. clusters to their respective CIFAR100 classes and produce\nGTlabels[55]. However,ourexperimentalanalysisshows samples with C-EDM and GT conditional EDM using the\nthat in image synthesis, the quality of image-level condi- samenoise. Samplesexhibithighvisualsimilarity,corrob-\ntioning primarily depends on the granularity of the clus- oratingwithTab.2andFig.2. Morevisualizationsarepro-\nter assignments (Tab. 2 and Fig. 3). Overall, we find no videdinthesupplementarymaterial.\nCIFAR10 CIFAR100 FFHQ-64\nFID FDDFID FDDFID FDD\n2.9 278 3.4 329 2.9 230\n2.6 246 3.1 308 2.7 222\n2.3 214 2.8 287 2.5 214\n2.0 182 2.5 266 2.3 206\nC = 1 200 400 600 800 C = 1 200 400 600 800 1000 C = 1 200 400 600 800 1000\nTEMI clusters (FID) TEMI clusters (FDD)\nFigure6.FID(lefty-axis)andFDD(righty-axis)versusnumberofclusters(x-axis)usingC-EDMatM =100.Thenon-filledmarkers\nimg\nindicatetheunconditionalEDM.\n(a)High-confidenceC-EDMsamples\nFigure 7. Comparing C-EDM samples generated with learned\n(b)Low-confidenceC-EDMsamples\nclusterassignments(toprow,C=100)toEDMsamplesgenerated\nwith GT labels, on CIFAR100. The clusters are mapped to GT\nFigure 8. Generated high (a) and low (b) confidence CIFAR10\nclassesusingtheHungarianalgorithm.\nsamples. Thetoprowdepictstheunconditional(Uncond.) sam-\nples,whilethebottomrowshowsthegeneratedsamplesusingC-\nEDM with TEMI (C = 100). Images on the same column are\nMeasuring the confidence of the generated samples us- producedwiththesameinitialnoise.\ning TEMI. In Fig. 8, we show the generated examples\nwith the lowest and highest maximum softmax probabil-\npatterns that are easy to generate, such as a white back-\nity[27]oftheTEMIheadasameasureofconfidence. For\nground. Confidence can also be leveraged in future works\ncomparison,weshowunconditionalsamplesgeneratedus-\ninrejectionsamplingschemes[17]. Moresamplesarepro-\ning the same initial noise in the denoising process. Visual\nvidedinthesupplementarymaterial.\ninspection shows that low-confidence C-EDM samples do\nnothavecoherentsemanticscomparedtotheunconditional\n6.Conclusion\nones, leading to inferior image quality. We hypothesize\nthat the sampled condition for the low-confidence samples\nIn this paper, a systematic empirical study was con-\nis in conflict with the existing patterns in the initial noise.\nductedfocusingonconditioningdiffusionmodelswithclus-\nIncreasing the number of conditions likely leads to worse\nter assignments. It was demonstrated that cluster condi-\nimage-condition alignment [30,32,65]. We argue that in-\ntioning achieves state-of-the-art FID on three generative\nternalguidancemethodscouldbeemployedtoincreasethe\nbenchmarks while attaining strong sample efficiency. To\nimagequalityinsuchcases,whichisleftforfuturework.\nreducethesearchspaceforestimatingthevisualgroupsof\nBy contrast, highly confident C-EDM samples show thedataset,anovelmethodthatcomputesanuppercluster\nmore clearly defined semantics than unconditional ones. boundbasedsolelyonclusteringwasproposed.Finally,our\nWhen the low frequencies, such as the object’s shape, re- experimental study indicates that generative performance\nmainintact,clusterconditioningaidsinrefininglocalpixel using cluster assignments depends primarily on the gran-\npatterns.ConfidentC-EDMsamplesconsistofsimplepixel ularityoftheassignments.\nReferences ingpropertiesinself-supervisedvisiontransformers.InPro-\nceedingsoftheIEEE/CVFinternationalconferenceoncom-\n[1] NikolasAdaloglou,FelixMichels,TimKaiser,andMarkus\nputervision,pages9650–9660,2021. 3,5,6,12,13,14\nKollmann. Adaptingcontrastivelanguage-imagepretrained\n[16] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal\n(clip) models for out-of-distribution detection. arXiv e-\nDrozdzal, and Adriana Romero Soriano. Instance-\nprints,pagesarXiv–2303,2023. 5\nconditionedgan. AdvancesinNeuralInformationProcess-\n[2] Nikolas Adaloglou, Felix Michels, Hamza Kalisch, and\ningSystems,34:27517–27529,2021. 2\nMarkusKollmann. Exploringthelimitsofdeepimageclus-\n[17] George Casella, Christian P Robert, and Martin T Wells.\nteringusingpretrainedmodels. In34thBritishMachineVi-\nGeneralizedaccept-rejectsamplingschemes.LectureNotes-\nsionConference2023,BMVC2023,Aberdeen,UK,Novem-\nMonographSeries,pages342–347,2004. 8\nber20-24,2023.BMVA,2023. 2,3,4\n[18] Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming\n[3] Nikolas Adaloglou, Felix Michels, Kaspar Senft, Diana\nXiang, and Chunhong Pan. Deep adaptive image cluster-\nPetrusheva, and Markus Kollmann. Scaling up deep\ning. InProceedingsoftheIEEEinternationalconferenceon\nclustering methods beyond imagenet-1k. arXiv preprint\ncomputervision,pages5879–5887,2017. 1\narXiv:2406.01203,2024. 1,2\n[19] Xinlei Chen, Saining Xie, and Kaiming He. An empiri-\n[4] EladAmrani, LeonidKarlinsky, andAlexBronstein. Self-\ncalstudyoftrainingself-supervisedvisiontransformers. In\nsupervisedclassificationnetwork. InEuropeanConference\nProceedingsoftheIEEE/CVFInternationalConferenceon\nonComputerVision,pages116–132.Springer,2022. 2\nComputerVision,pages9640–9649,2021. 6,13,14\n[5] UriM.AscherandLindaR.Petzold. ComputerMethodsfor\n[20] MehdiCherti,RomainBeaumont,RossWightman,Mitchell\nOrdinary Differential Equations and Differential-Algebraic\nWortsman,GabrielIlharco,CadeGordon,ChristophSchuh-\nEquations. SocietyforIndustrialandAppliedMathematics,\nmann,LudwigSchmidt,andJeniaJitsev.Reproduciblescal-\nUSA,1stedition,1998. 18 ing laws for contrastive language-image learning. In Pro-\n[6] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, ceedingsoftheIEEE/CVFConferenceonComputerVision\nSoumyadip Sengupta, Micah Goldblum, Jonas Geip- andPatternRecognition,pages2818–2829,2023. 13,14\ning, and Tom Goldstein. Universal guidance for diffusion [21] MinJinChongandDavidForsyth. Effectivelyunbiasedfid\nmodels. In Proceedings of the IEEE/CVF Conference on andinceptionscoreandwheretofindthem. InProceedings\nComputer Vision and Pattern Recognition, pages 843–852, oftheIEEE/CVFconferenceoncomputervisionandpattern\n2023. 2 recognition,pages6070–6079,2020. 7\n[7] Bjo¨rnBarzandJoachimDenzler. Dowetrainontestdata? [22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\npurgingcifarofnear-duplicates.JournalofImaging,6(6):41, andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage\n2020. 1 database. In2009IEEEconferenceoncomputervisionand\n[8] GeorgiosBatzolis,JanStanczuk,Carola-BibianeScho¨nlieb, patternrecognition,pages248–255.Ieee,2009. 4\nand Christian Etmann. Conditional image genera- [23] PrafullaDhariwalandAlexanderNichol. Diffusionmodels\ntion with score-based diffusion models. arXiv preprint beatgansonimagesynthesis. Advancesinneuralinforma-\narXiv:2111.13606,2021. 1 tionprocessingsystems,34:8780–8794,2021. 1\n[9] James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng [24] PrafullaDhariwalandAlexanderNichol. Diffusionmodels\nWang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee, beatgansonimagesynthesis. Advancesinneuralinforma-\nYufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu, tionprocessingsystems,34:8780–8794,2021. 2\nYunxinJiao, and Aditya Ramesh. Improving image gener- [25] Baoetal.Whyareconditionalgenerativemodelsbetterthan\nationwithbettercaptions. 1 unconditionalones? InNeurIPSWorkshoponScore-Based\n[10] Lucas Beyer, Olivier J He´naff, Alexander Kolesnikov, Xi- Methods,2022. 1,2,3,4\naohua Zhai, and Aa¨ron van den Oord. Are we done with [26] Hariprasath Govindarajan, Per Side´n, Jacob Roll, and\nimagenet? arXivpreprintarXiv:2006.07159,2020. 1 FredrikLindsten.Onpartialprototypecollapseinclustering-\n[11] Lucas Beyer, Olivier J He´naff, Alexander Kolesnikov, Xi- basedself-supervisedlearning,2024. 12\naohua Zhai, and Aa¨ron van den Oord. Are we done with [27] Dan Hendrycks and Kevin Gimpel. A baseline for detect-\nimagenet? arXivpreprintarXiv:2006.07159,2020. 1 ingmisclassifiedandout-of-distributionexamplesinneural\n[12] Florian Bordes, Randall Balestriero, and Pascal Vincent. networks. InInternationalConferenceonLearningRepre-\nHighfidelityvisualizationofwhatyourself-supervisedrep- sentations,2017. 8\nresentationknowsabout,2022. 2 [28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\n[13] AndrewBrock,JeffDonahue,andKarenSimonyan. Large Bernhard Nessler, and Sepp Hochreiter. Gans trained by a\nscalegantrainingforhighfidelitynaturalimagesynthesis, twotime-scaleupdateruleconvergetoalocalnashequilib-\n2019. 2 rium. Advances in neural information processing systems,\n[14] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and 30,2017. 3,4\nMatthijsDouze. Deepclusteringforunsupervisedlearning [29] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif-\nofvisualfeatures. InProceedingsoftheEuropeanconfer- fusionprobabilisticmodels. Advancesinneuralinformation\nenceoncomputervision(ECCV),pages132–149,2018. 1 processingsystems,33:6840–6851,2020. 1,2,3,4\n[15] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou, [30] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nJulienMairal,PiotrBojanowski,andArmandJoulin.Emerg- guidance. arXivpreprintarXiv:2207.12598,2022. 2,4,8\n[31] Laura Hollink, Aysenur Bilgin, and Jacco Van Ossen- [45] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple\nbruggen. Isitafruit, anappleoragrannysmith? predict- layersoffeaturesfromtinyimages. 2009. 1,4,5\ning the basic level in a concept hierarchy. arXiv preprint [46] HaroldWKuhn. Thehungarianmethodfortheassignment\narXiv:1910.12619,2019. 1 problem. Navalresearchlogisticsquarterly, 2(1-2):83–97,\n[32] SusungHong,GyuseongLee,WooseokJang,andSeungry- 1955. 12\nongKim. Improvingsamplequalityofdiffusionmodelsus-\n[47] Tuomas Kynka¨a¨nniemi, Tero Karras, Miika Aittala, Timo\ningself-attentionguidance.InProceedingsoftheIEEE/CVF\nAila, and Jaakko Lehtinen. The role of imagenet\nInternationalConferenceonComputerVision(ICCV),pages\nclasses in fr\\’echet inception distance. arXiv preprint\n7462–7471,October2023. 2,8\narXiv:2203.06026,2022. 7\n[33] Vincent Tao Hu, Yunlu Chen, Mathilde Caron, Yuki M\n[48] JunnanLi,PanZhou,CaimingXiong,andStevenCHHoi.\nAsano, CeesGMSnoek, andBjornOmmer. Guideddiffu-\nPrototypicalcontrastivelearningofunsupervisedrepresenta-\nsionfromself-superviseddiffusionfeatures. arXivpreprint\ntions. arXivpreprintarXiv:2005.04966,2020. 3\narXiv:2312.08825,2023. 2\n[49] Tianhong Li, Dina Katabi, and Kaiming He. Self-\n[34] VincentTaoHu,DavidWZhang,YukiMAsano,GertjanJ\nconditionedimagegenerationviageneratingrepresentations.\nBurghouts,andCeesGMSnoek.Self-guideddiffusionmod-\narXivpreprint,2023. 2,6\nels. InCVPR,2023. 2,3,4,5,6\n[35] ZhizhongHuang,JieChen,JunpingZhang,andHongming [50] Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu,\nShan. Learning representation for clustering via prototype and Antonio Torralba. Diverse image generation via self-\nscatteringandpositivesampling. IEEETransactionsonPat- conditioned gans. In Proceedings of the IEEE/CVF con-\nternAnalysisandMachineIntelligence,2022. 3,4 ference on computer vision and pattern recognition, pages\n14286–14295,2020. 2,3\n[36] GabrielIlharco,MitchellWortsman,RossWightman,Cade\nGordon, Nicholas Carlini, Rohan Taori, Achal Dave, [51] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li,\nVaishaalShankar,HongseokNamkoong,JohnMiller,Han- and Yixuan Li. Delving into out-of-distribution detection\nnanehHajishirzi,AliFarhadi,andLudwigSchmidt. Open- with vision-language representations. Advances in Neural\nclip, July 2021. If you use this software, please cite it as InformationProcessingSystems,35:35087–35102,2022. 5,\nbelow. 5,6,19 19\n[37] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, [52] Mehdi Mirza and Simon Osindero. Conditional generative\nDaniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Re- adversarialnets. arXivpreprintarXiv:1411.1784,2014. 2\nthinking fid: Towards a better evaluation metric for image [53] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ngeneration. arXivpreprintarXiv:2401.09603,2023. 7 denoising diffusion probabilistic models. In International\n[38] XuJi,JoaoFHenriques,andAndreaVedaldi. Invariantin- ConferenceonMachineLearning,pages8162–8171.PMLR,\nformation clustering for unsupervised image classification 2021. 1,2\nand segmentation. In Proceedings of the IEEE/CVF Inter-\n[54] Maxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy\nnationalConferenceonComputerVision,pages9865–9874,\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\n2019. 1,3\nDanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.\n[39] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nDinov2:Learningrobustvisualfeatureswithoutsupervision.\nElucidating the design space of diffusion-based generative\narXivpreprintarXiv:2304.07193,2023. 2,4,13,14\nmodels. Advances in Neural Information Processing Sys-\n[55] NamukPark,WonjaeKim,ByeonghoHeo,TaekyungKim,\ntems,35:26565–26577,2022. 1,2,3,4,14,15,17,20\nand Sangdoo Yun. What do self-supervised vision trans-\n[40] TeroKarras,MiikaAittala,JaakkoLehtinen,JanneHellsten,\nformers learn? In The Eleventh International Conference\nTimoAila,andSamuliLaine. Analyzingandimprovingthe\nonLearningRepresentations,2023. 7\ntrainingdynamicsofdiffusionmodels,2023. 2,5,12\n[56] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On\n[41] Amirhossein Kazerouni, Ehsan Khodapanah Aghdam,\naliased resizing and surprising subtleties in gan evaluation.\nMoein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Haci-\nIn Proceedings of the IEEE/CVF Conference on Computer\nhaliloglu, and Dorit Merhof. Diffusion models for medi-\nVisionandPatternRecognition,pages11410–11420,2022.\ncalimageanalysis:Acomprehensivesurvey. arXivpreprint\n7\narXiv:2211.07804,2022. 1\n[57] WilliamPeeblesandSainingXie. Scalablediffusionmod-\n[42] David J Ketchen and Christopher L Shook. The applica-\nelswithtransformers. InProceedingsoftheIEEE/CVFIn-\ntion of cluster analysis in strategic management research:\nan analysis and critique. Strategic management journal, ternational Conference on Computer Vision (ICCV), pages\n4195–4205,October2023. 1\n17(6):441–458,1996. 4\n[43] Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo [58] WilliamPeeblesandSainingXie. Scalablediffusionmodels\nKang, and Il-Chul Moon. Refining generative process with transformers. In Proceedings of the IEEE/CVF Inter-\nwith discriminator guidance in score-based diffusion mod- nationalConferenceonComputerVision,pages4195–4205,\nels. arXivpreprintarXiv:2211.17091,2022. 4 2023. 2\n[44] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nHo. Variationaldiffusionmodels. Advancesinneuralinfor- Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nmationprocessingsystems,34:21696–21707,2021. 2 AmandaAskell,PamelaMishkin,JackClark,etal.Learning\ntransferable visual models from natural language supervi- [72] YiyouSun, YifeiMing, XiaojinZhu, andYixuanLi. Out-\nsion.InInternationalconferenceonmachinelearning,pages of-distributiondetectionwithdeepnearestneighbors. InIn-\n8748–8763.PMLR,2021. 1,5,6 ternationalConferenceonMachineLearning,pages20827–\n[60] Nima Rafiee, Rahil Gholamipoor, Nikolas Adaloglou, Si- 20840.PMLR,2022. 7\nmon Jaxy, Julius Ramakers, and Markus Kollmann. Self- [73] ChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet,\nsupervised anomaly detection by self-distillation and nega- Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\ntivesampling.InInternationalConferenceonArtificialNeu- Vanhoucke, and Andrew Rabinovich. Going deeper with\nralNetworks,pages459–470.Springer,2022. 6 convolutions. In Proceedings of the IEEE conference on\n[61] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, computer vision and pattern recognition, pages 1–9, 2015.\nand Mark Chen. Hierarchical text-conditional image gen- 4\nerationwithcliplatents. arXivpreprintarXiv:2204.06125, [74] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\n1(2):3,2022. 2 Shlens,andZbigniewWojna.Rethinkingtheinceptionarchi-\n[62] Tal Reiss, Niv Cohen, Eliahu Horwitz, Ron Abutbul, and tectureforcomputervision.InProceedingsoftheIEEEcon-\nYedidHoshen. Anomalydetectionrequiresbetterrepresen- ference on computer vision and pattern recognition, pages\ntations. InEuropeanConferenceonComputerVision,pages 2818–2826,2016. 4\n56–68.Springer,2022. 6 [75] RobertLThorndike.Whobelongsinthefamily? Psychome-\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz, trika,18(4):267–276,1953. 4\nPatrick Esser, and Bjo¨rn Ommer. High-resolution image [76] Wouter Van Gansbeke, Simon Vandenhende, Stamatios\nsynthesis with latent diffusion models. 2022 ieee. In CVF Georgoulis, Marc Proesmans, and Luc Van Gool. Scan:\nConference on Computer Vision and Pattern Recognition Learningtoclassifyimageswithoutlabels.InEuropeancon-\n(CVPR),pages10674–10685,2021. 2 ferenceoncomputervision,pages268–285.Springer,2020.\n[64] SeyedmortezaSadat,JakobBuhmann,DerekBradely,Otmar 2,3,13\nHilliges, andRomannMWeber. Cads: Unleashingthedi- [77] Hualiang Wang, Yi Li, Huifeng Yao, and Xiaomeng Li.\nversityofdiffusionmodelsthroughcondition-annealedsam- Clipnforzero-shotooddetection: Teachingcliptosayno.\npling. arXivpreprintarXiv:2310.17347,2023. 4 In Proceedings of the IEEE/CVF International Conference\n[65] Chitwan Saharia, William Chan, Saurabh Saxena, Lala onComputerVision,pages1802–1812,2023. 6\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour, [78] JunyuanXie,RossGirshick,andAliFarhadi. Unsupervised\nRaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans, deep embedding for clustering analysis. In International\netal.Photorealistictext-to-imagediffusionmodelswithdeep conference on machine learning, pages 478–487. PMLR,\nlanguage understanding. Advances in Neural Information 2016. 2\nProcessingSystems,35:36479–36494,2022. 1,8\n[79] Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong,\n[66] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer,\nMax Tegmark, and Tommi Jaakkola. Pfgm++: Unlocking\nOran Gafni, Eliya Nachmani, and Yaniv Taigman. Knn-\nthe potential of physics-inspired generative models. arXiv\ndiffusion: Image generation via large-scale retrieval, 2022.\npreprintarXiv:2302.04265,2023. 4\n1\n[80] Binxin Yang, Shuyang Gu, Bo Zhang, TingZhang, Xuejin\n[67] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer,\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nOran Gafni, Eliya Nachmani, and Yaniv Taigman. Knn-\nexample:Exemplar-basedimageeditingwithdiffusionmod-\ndiffusion: Imagegenerationvialarge-scaleretrieval. arXiv\nels. InProceedingsoftheIEEE/CVFConferenceonCom-\npreprintarXiv:2204.02849,2022. 2\nputerVisionandPatternRecognition, pages18381–18391,\n[68] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,\n2023. 1\nand Surya Ganguli. Deep unsupervised learning using\n[81] AsanoYM.,RupprechtC.,andVedaldiA.Self-labellingvia\nnonequilibrium thermodynamics. In International confer-\nsimultaneousclusteringandrepresentationlearning. InIn-\nenceonmachinelearning,pages2256–2265.PMLR,2015.\nternationalConferenceonLearningRepresentations,2020.\n2\n2\n[69] Jiaming Song, Chenlin Meng, and Stefano Ermon.\n[82] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang\nDenoising diffusion implicit models. arXiv preprint\nXie,AlanYuille,andTaoKong.ibot:Imagebertpre-training\narXiv:2010.02502,2020. 2\nwith online tokenizer. arXiv preprint arXiv:2111.07832,\n[70] YangSong,JaschaSohl-Dickstein,DiederikP.Kingma,Ab-\n2021. 6,12,13,14\nhishekKumar,StefanoErmon,andBenPoole. Score-based\n[83] Xingzhi Zhou and Nevin L Zhang. Deep clustering with\ngenerative modeling through stochastic differential equa-\nfeatures from self-supervised pretraining. arXiv preprint\ntions. CoRR,abs/2011.13456,2020. 14\narXiv:2207.13364,2022. 4\n[71] George Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi\n[84] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,\nSui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan\nChris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and\nLiu, AnthonyL.Caterini, EricTaylor, andGabrielLoaiza-\nTongSun. Towardslanguage-freetrainingfortext-to-image\nGanem.Exposingflawsofgenerativemodelevaluationmet-\ngeneration. In Proceedings of the IEEE/CVF Conference\nricsandtheirunfairtreatmentofdiffusionmodels.InThirty-\nonComputerVisionandPatternRecognition,pages17907–\nseventhConferenceonNeuralInformationProcessingSys-\n17917,2022. 2\ntems,2023. 2,4,7,12\nCode. Codeisavailableathttps://github.com/ upperbound,thesearchspaceofthevisualgroupsissignifi-\nHHU-MMBS/cedm-official-wavc2025. cantlyreducedwithminimalcomputationaloverhead,while\nno further hyperparameter tuning is required. Therefore,\nA.Visualizations it allows future works to incorporate unlabelled data and\nexperimentatscalesbeyondImageNetwhilebeingsample\nIn Fig. 10, we map the TEMI clusters to classes using\nefficient. Additionally, the sample efficiency compared to\nthe Hungarian mapping [46] on CIFAR100. For the map-\nnoisyornon-mutuallyexclusivelabelscouldbeinvestigated\npingtobeone-to-one, wesetC = 100. Wethengenerate\ninfutureworks.\nsamplesusingthesameinitialnoisewithbothC-EDMand\nGT-conditionedEDMandvisualizethefirst20clusterson\nCIFAR100. Giventhesameinitialnoiseandtheclusterthat C.DeepimageclusteringwithTEMI\nismappedtoitsrespectiveGTclass,weobservealotofvi-\nC.1.Intuitionforγ\nsualsimilaritiesintheimages,eventhoughthetwomodels\n(C-EDM and EDM) have different weights and have been\nIn the TEMI loss function, there are two parts inside\ntrainedwithdifferenttypesofconditioning.\nthe log sum: the numerator\n(cid:0) qi(c|x)qi(c|x′)(cid:1)γ\naligns the\ns t\nIn Figs. 11 and 12, we visualize C-EDM samples gen-\ncluster assignment of a positive pair and is maximal when\nerated from the same initial noise on FFHQ-64 for diffu-\neach individual assignment is one-hot. On the other hand,\nsionmodelstrainedwithvaryingclustergranularities. Each thedenominatorq˜i(c)promotesauniformclusterdistribu-\nnoise gets a condition sampled from p(c). Similar to our t\ntion. Bydividingelement-wisewiththeclusterprobability,\nquantitativeanalysis,thegeneratedimagesfromsmallclus-\nitiseffectivelyup-weighingthesummandcorrespondingto\ntersizesareclosertotheunconditionalprediction. Finally, classes with low probability. In other words, when q˜i(c)\nt\nin Fig. 9, we visualize cluster-conditioned and uncondi-\nis low. The hyperparameter γ reduces the influence of the\ntionalFFHQsamplesatM =100M.\nimg numerator,whichleadstopartialcollapse[26]whenγ =1.\nInFig.13,wevisualizerealtrainingFFHQimagesthat\naregroupedinthesameTEMIclusterusingtheDINOfea-\nC.2. What about the lower bound? TEMI with\ntures. We visually identify groups with shared character-\nγ =1experiments.\nisticssuch assunglasses, hats, beanies, picturesof infants,\nand pictures of speakers talking to a microphone. Finally, Starting with a high overestimation of the number of\nin Fig. 14 we provide a more detailed visual comparison clusters (e.g. 1K for CIFAR10), we find that TEMI clus-\nof low and high confidence samples using C-EDM on CI- teringwithγ = 1utilizesasubsetofclusters,whichcould\nFAR100. beusedasalowerclusterbound. Moreprecisely,wefinda\nmaximumstandarddeviationof6.4forCu acrossdatasets\nB.Additionaldiscussionpoints and feature extractors (see Supp.). Intuitively, Cu is the\nminimum amount of clusters TEMI (with γ = 1) uses to\nB.1.FIDandFDDacrosstrainingiterations\ngroupallimagepairs.Thisbehaviorisanalogoustocluster-\nIn Fig. 15, we report FID and FDD across training basedself-supervisedlearning(usingimageaugmentations)\nusing C-EDM with TEMI clusters. We notice that FID [15,82] and has been recently coined as partial prototype\ntends to saturate faster than FDD and fluctuates more be- collapse [26]. Nonetheless, the lower bound is more ap-\ntween checkpoints. FDD keeps decreasing monotonically, plicable to large scales as the measured standard deviation\nwithminimalfluctuationandalwaysprefersthesamplesat mightexcludetheoptimalgranularityforsmall,highlycu-\nM img = 200. SincebothmetricscomputetheFrechetdis- rateddatasets.Duetotheabovelimitation,weleavethisfor\ntance, thesetendenciescanonlybeattributedtothesuper- futurework.\nvisedInceptionV3features. Eventhoughthestudyofgen- AsdepictedinTab.4,theutilizednumberofclustersCu\nerativemetricsisoutofthescopeofthisworkandahuman\nisnotsensitivetothepre-determinednumberofclustersnor\nevaluationisnecessaryasin[71],wehopethatourfindings\nthechoiceofbackboneforTEMIclusteringwhenγ =1.\nw.r.t. cluster-conditioningcanfacilitatefutureworks.\nB.2.ImagesynthesisbeyondImageNet. C.3.TEMIwithdifferentbackbones.\nImageNetiscurrentlythelargestlabeledpublicdataset, Here,wereportANMIacrossvariousclustersizesbased\nand a single experiment using a recent state-of-the-art dif- on the result reported in the main paper (Fig. 5, main pa-\nfusion model on ImageNet requires up to 4MWh at 5122 per). For all the conducted experiments, we used TEMI\nresolution [40]. Based on our experiments, clusters match with γ = 0.6. Apart from having roughly the same FID,\nor outperform the human-derived labels on image genera- wecanobservetherankingofbackbonesw.r.tANMIisnot\ntionbyestimatingthevisualgroups. Usingtheintroduced consistentacrossclustersizes.\nFigure9.VisualcomparisonbetweenC-EDM(C=400)andunconditionalEDM(Uncond.)atM =100M onFFHQat128x128.\nimg\nTable4. NumberofutilisedclustersCu fordifferentnumberofinputclustersC (left)anddifferentbackbones(right)usingTEMIwith\nβ = 1withtheDINOViT-B/16backbone. WeshowtherelativelysmallsensitivityofCu tothechoiceofC andbackbone;wereporta\nstandarddeviationofamaximumvalueof6.37acrossdifferentclustersizesand6.44acrossbackbonesonCIFAR10.\nTEMI CIFAR10 FFHQ CIFAR100\nγ =1 Cu Cu Cu TEMI CIFAR10\nγ =1,C =500 Cu\n100 33 36 48\n400 38 48 48 DINOViT-B/16[15] 34\n500 34 54 51 MoCOv3ViT-B/16[19] 39\n800 40 45 47 iBOTViT-L/14[82] 45\n1K 34 49 47 OpenCLIPViT-G/14[20] 47\n2K 48 52 54 DINOv2ViT-g/14[54] 50\n5K 28 42 51 Mean 43\nMean 36.4 46.6 49.4 Std 6.44\nStd 6.37 6.16 2.63\nC.4. Dependence on q(c) during generative sam-\nplingonbalancedclassificationdatasets.\nIt is well-established in the clustering literature that k-\nmeansclustersarehighlyimbalanced[76].Toillustratethis\nFigure10. VisualizinggeneratedimagesfromCIFAR100usingC-EDM(evenrows)andgroundtruthconditionalEDM(oddrows)using\nthe same initial noise and deterministic noise sampling. We map the C=100 CIFAR100 cluster to the respective ground truth class as\ncomputedviatheHungarianone-to-onemapping.\nTable5. CIFAR10ANMIacrossdifferentclustersizesandstate- ismoredependenttoq(c)comparedtoTEMI,asitsFIDis\nof-the-art feature extractors used for TEMI clustering with γ = significantlydeteriorated.\n0.6. We only reported the ANMI for C = 100 in the main\nmanuscript.\nD.TheEDMdiffusionbaseline.\nTEMI(γ =0.6) ANMI ANMI ANMI ThissectionbrieflysummarizestheEDMframeworkfor\nC 50 100 200 diffusionmodels,whichwasusedextensivelyinthiswork.\nFor more details and the official EDM code, we refer the\nMoCov3ViT-B/16[19] 65.2 59.3 55.0\nreadertotheoriginalpaperbyKarrasetal.[39].\nDINOViT-B/16[15] 65.7 60.7 55.8\nDINOv2ViT-g/14[54] 66.1 63.8 59.3 Given a data distribution p data(x), consider the condi-\niBOTViT-L/14[82] 68.7 62.7 57.4 tionaldistributionp(x;σ)ofdatasamplesnoisedwithi.i.d.\nCLIPViT-G/14[20] 70.6 64.7 58.9 Gaussiannoiseofvarianceσ2. Diffusion-basedgenerative\nmodelslearntofollowtrajectoriesthatconnectnoisysam-\nples x ∼ p(x;σ) with data points y ∼ p (x). Song\ndata\net al. [70] introduced the idea of formulating the forward\nin a generative context, we sample from a uniform clus- trajectories (from data to noise) using stochastic differen-\nter distribution instead of q(c) for balanced classification tialequations(SDE)thatevolvesamplesx(σ)accordingto\ndatasets(CIFAR10andCIFAR100). Asexpected,k-means p(x;σ) with σ = σ(t) as a function of time t. They also\nFigure11.VisualizinggeneratedimagesfromFFHQ-64usingCEDMfordifferentnumberofclustersCwiththesamerandomnoise.We\nusedeterministicnoisesampling.Eachnoisegetsaconditionsampledfromp(c)foreachindividualclusters.\nUncond. C=10 C=100 C=200 C=400 C=600 C=1K\nFigure12. GeneratedFFHQ-64samplesusingC-EDM andTEMIclusterswithdifferentgranularitylevelsC aswellasunconditional\nEDM(Uncond.,firstcolumn). Allsamplesinarowusethesameinitialnoise. Theclusterassignmentisrandomlysampledfromq(c)for\neachC.\nTable6. WereportFIDfork-meansandTEMIwithandwithoutconsideringthetrainingdata’sclusterdistributionq(c). U({1,..,C})\ndenotestheuniformclusterdistribution. WeuseC =100,200,400forCIFAR10,CIFAR100andFFHQ,respectively. ∆quantifiesthe\nV\nabsolutedifference.\nEDM[39] SamplingDistribution CIFAR10 CIFAR100\nk-means U({1,..,C}) 2.75 2.60\nk-means q(c) 1.69 2.21\n∆(↓) - 0.79 0.39\nTEMI U({1,..,C}) 1.86 2.41\nTEMI q(c) 1.67 2.17\n∆(↓) - 0.19 0.24\nFigure13. VisualizingtrainingimagesfromFFHQthatbelongtothesameTEMIcluster. Imagesthataregroupedintothesamecluster\nareshowninthesamerow.WeusethetrainedTEMImodelwithC =400usingtheDINObackbone.Clusterassignmentsarepickedto\nV\nillustratethatimageswithsimilarvisualcharacteristicsaregroupedtogether(i.e.,beanies,smilingfaces,glasses,hats,kids,etc.). Images\narerandomlysampledfromeachcluster.\n(a)High-confidentC-EDMsamples\n(b)Low-confidentC-EDMsamples\nFigure14. Generatedlow-(a)andhigh-confident(b)CIFAR100samples. Thetoprowdepictstheunconditional(Uncond.) samples,\nwhilethebottomrowshowsthegeneratedsamplesusingC-EDMwithTEMI(C =200). Imagesonthesamecolumnareproducedwith\nthe same initial noise. Confidence is quantified using maximum softmax probability (MSP). MSP is measured using TEMI trained on\nCIFAR100withoutannotateddata.\nCIFAR10 CIFAR100 FFHQ-64\nFID FDDFID FDDFID FDD\n2.1 220\n3.2 330 2.8 280\n2.0 200\n2.9 300 2.6 250\n1.9 180\n2.6 270 2.4 220\n1.8 160\n2.3 240 2.2 190\n1.7 140\nMimg 40 80 120 160 200 Mimg 40 80 120 160 200 Mimg 40 80 120 160 200\nFID (Inception-v3) FDD (DINOv2 ViT-L/14)\nFigure15.FIDscore(y-axis,left)andFDD(y-axis,right)duringtrainingsamplesseen(M ,x-axis).WeusedC =100,200,400for\nimg V\nCIFAR10,CIFAR100andFFHQ-64respectively.\nproposed a corresponding “probability flow” ordinary dif- tion,islearnedbyaneuralnetworkthroughwhatisknown\nferential equation (ODE), which is fully deterministic and as denoising score matching. The core observation here is\nmapsthedatadistributionp (x)tothesamenoisedistri- thatthescoredoesnotdependontheintractablenormaliza-\ndata\nbutionp(x;σ(t))astheSDE,foragiventimet. TheODE tion constant of p(x,σ(t)), which is the reason that diffu-\ncontinuously adds or removes noise as the sample evolves sionmodelsintheircurrentformulationworkatall(maybe\nthrough time. To formulate the ODE in its simplest form, remove this side-note). Given a denoiser D(x,σ) and the\nwe need to set a noise schedule σ(t) and obtain the score L2-denoisingerror\nfunction∇ logp(x;σ):\nx E E [∥D(y+n,σ)−y∥2], (3)\ny∼pdata n∼N(0,σ2I)\ndx=−σ˙(t)σ(t)∇ logp(x;σ)dt. (2)\nx\nwe can recover the score function via ∇ logp(x,σ) =\nx\nWhile mathematical motivations exist for the choice of (D(x,σ) − x)/σ2. Thus, parametrizing the denoiser as\nscheduleσ(t),empiricallymotivatedchoiceswereshownto a neural network and training it on Eq. (3) allows us to\nbesuperior[39]. Themaincomponenthere,thescorefunc- learn the score function needed for Eq. (2). To solve the\nODE in Eq. (2), we can put the recovered score function\nintoEq.(2)andapplynumericalODEsolvers,likeEuler’s\nmethodorHeun’smethod[5]. TheODEisdiscretizedinto\nafinitenumberofsamplingtimest ,...,t andthensolved\n0 N\nthrough iteratively computing the score and taking a step\nwithanODEsolver.\nE. Additional implementation details and hy-\nperparameters\nWhen searching for C , we evaluate EDM after train-\nV\ning with M = 100 and for M = 200 once C is\nimg img V\nfound. We only report k-means cluster conditioning with\nk =C .AllourreportedFIDandFDDvaluesareaverages\nV\nover3runsof50kimageseach,eachwithdifferentrandom\nseeds. Below,weshowthehyperparametersweusedforall\ndatasetstoenablereproducibility. Wealwaysusetheaver-\nage FID and FDD for three sets of 50K generated images.\nTheusedhyperparameterscanbefoundinTabs.7and8\nToassigntheCLIPpseudo-labels(Sec. 4.4)tothetrain-\ningset,wecomputethecosinesimilarityoftheimageand\nlabel embeddings using openclip’s ViT-G/14 [36]. The la-\nbel embeddings use prompt ensembling and use the five\nprompts:aphotoofa<label>,ablurryphotoofa<label>,\naphotoofmany<label>,aphotoofthelarge<label>,and\naphotoofthesmall<label>asin[51].\nTable7.HyperparametersusedfortrainingEDMandC-EDM.Boldsignifiesthatthevalueischangingacrossdatasets.Allotherparame-\ntersofthetrainingsetupwereidenticaltothespecificationsofKarraset.al[39],whicharedetailedthere.\nHyperparameter CIFAR10/CIFAR100 FFHQ-64/AFHQ-64\nOptimization\noptimizer Adam Adam\nlearningrate 0.001 0.001\nbetas 0.9,0.999 0.9,0.999\nbatchsize 1024 512\nFP16 true true\nSongUNet\nmodelchannels 128 128\nchannelmultiplier 2-2-2 1-2-2-2\ndropout 13% 5%/25%\nAugmentation\naugmentdim 9 9\nprobability 12% 15%\nTable8.TEMIhyperparameters\nHyperparameter Value\nHeadhyperparameters\nMLPhiddenlayers 2\nhiddendim 512\nbottleneckdim 256\nHeadfinalgelu false\nNumberofheads(H) 50\nLoss TEMI\nγ 0.6\nMomentumλ 0.996\nUsebatchnormalization false\nDropout 0.0\nTemperature 0.1\nNearestneibohrs(NN) 50\nNormlastlayer false\nOptimization\nFP16(mixedprecision) false\nWeightdecay 0.0001\nClipgrad 0\nBatchsize 512\nEpochs 200\nLearningrate 0.0001\nOptimizer AdamW\nDroppathrate 0.1\nImagesize 224",
    "pdf_filename": "Rethinking_cluster-conditioned_diffusion_models_for_label-free_image_synthesis.pdf"
}