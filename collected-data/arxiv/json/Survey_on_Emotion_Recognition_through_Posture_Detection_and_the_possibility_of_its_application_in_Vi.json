{
    "title": "Survey on Emotion Recognition through Posture Detection and the possibility of its application in Vi",
    "context": "A survey is presented focused on using pose estimation techniques in Emotional recognition using various technologies normal cameras, and depth cameras for real-time, and the potential use of VR and inputs including images, videos, and 3-dimensional poses described in vector space. We discussed 19 research papers collected from selected journals and databases highlighting their methodology, classification algorithm, and the used datasets that relate to emotion recognition and pose estimation. A benchmark has been made according to their accuracy as it was the most common performance measurement metric used. We concluded that the multimodal Approaches overall made the best accuracy and then we mentioned futuristic concerns that can improve the development of this research topic. Emotion recognition is one of the main vital tasks essential for having an intelligent system or application. Dealing with humans requires understanding their own emotions so that the human feels comfortable and the communication becomes more spontaneous which reflects on the efficiency of the service provided by the system/application. Emotions can be measured from multiple modalities like reading facial expressions, gesture detection, static posture, movement behavior, vocal tones, and text. When interacting with another human, you might know his current emotions from only seeing his face and sometimes the eyes can do the trick, or from his vocal tone, his posture - the way he is standing- or from the pattern of his movements, the gestures he is making or the context of his words whether those words are said or written - you can read an article and still visualize the emotions the writer has been through- or you can combine two or more modalities together which increases the efficiency of the human’s predictions. Computer models are being trained to recognize the above models far above is the physical measurement which may include using sensors and actuators to measure physiological patterns that are hard for the computer to measure like measuring the heart rate, body temperature, and skin sensitivity(Picard, R.W. and Vyzas, E. and Healey, J., n.d.). Those extra modalities shall prepare the computer to be able to measure emotions accurately even more than humans, which is not currently reached. We will discuss the challenges being faced in this field and how some papers overcome those challenges. Some modalities can provide reliable measurements on their own or they may be used only to enhance the recognition of another 1",
    "body": "Journal of Artificial Intelligence Research 23 (2005) 533-585\nSubmitted _/2024; published _/2024\nSurvey on Emotion Recognition through Posture Detection\nand the possibility of its application in Virtual Reality\nLeina Elansary\nMaster's student,Faculty of Computer and Information\nSciences, Ain Shams University, Egypt.\nleina.saad@cis.asu.edu.eg\nZaki Taha\nProfessor of Computer Science, Faculty of Computer and\nInformation Sciences, Ain Shams University, Egypt.\nzaki.taha@cis.asu.edu.eg\nWalaa Gad\nProfessor of InformationSystems, Faculty of Computer\nand Information Sciences, Ain Shams University, Egypt\nwalaagad@cis.asu.edu.eg\nAbstract\nA survey is presented focused on using pose estimation techniques in Emotional recognition using\nvarious technologies normal cameras, and depth cameras for real-time, and the potential use of VR\nand inputs including images, videos, and 3-dimensional poses described in vector space. We\ndiscussed 19 research papers collected from selected journals and databases highlighting their\nmethodology, classification algorithm, and the used datasets that relate to emotion recognition and\npose estimation. A benchmark has been made according to their accuracy as it was the most\ncommon performance measurement metric used. We concluded that the multimodal Approaches\noverall made the best accuracy and then we mentioned futuristic concerns that can improve the\ndevelopment of this research topic.\nIntroduction\nEmotion recognition is one of the main vital tasks essential for having an intelligent system or\napplication. Dealing with humans requires understanding their own emotions so that the human\nfeels comfortable and the communication becomes more spontaneous which reflects on the\nefficiency of the service provided by the system/application. Emotions can be measured from\nmultiple modalities like reading facial expressions, gesture detection, static posture, movement\nbehavior, vocal tones, and text. When interacting with another human, you might know his\ncurrent emotions from only seeing his face and sometimes the eyes can do the trick, or from his\nvocal tone, his posture - the way he is standing- or from the pattern of his movements, the\ngestures he is making or the context of his words whether those words are said or written - you\ncan read an article and still visualize the emotions the writer has been through- or you can\ncombine two or more modalities together which increases the efficiency of the human’s\npredictions. Computer models are being trained to recognize the above models far above is the\nphysical measurement which may include using sensors and actuators to measure physiological\npatterns that are hard for the computer to measure like measuring the heart rate, body\ntemperature, and skin sensitivity(Picard, R.W. and Vyzas, E. and Healey, J., n.d.). Those extra\nmodalities shall prepare the computer to be able to measure emotions accurately even more than\nhumans, which is not currently reached. We will discuss the challenges being faced in this field\nand how some papers overcome those challenges. Some modalities can provide reliable\nmeasurements on their own or they may be used only to enhance the recognition of another\n1\n\nJournal of Artificial Intelligence Research 23 (2005) 533-585\nSubmitted _/2024; published _/2024\nmodality and may not produce accurate results once used by themselves. In this paper, Our main\nfocus will be on using the Pose estimation modality or posture recognition to measure the\nemotions of the human interacting with affective systems. The body posture or the pose can be\ndetected from static images taken by a camera, image sequences (captured from videos) whether\nthey are previously captured or provided in real-time, using a depth camera like Kinect which is\nusually used in providing real-time data, or finally using the Virtual reality technology which is\nusually real-time also. The images provide 2D coordinate system data unless a 2D to 3D\nconversion algorithm is implemented and that provides us with 3D coordinate system data or by\nusing simply the depth camera or a VR device and sometimes it shall be equipped with external\nsensors to provide a full body detection including the lower body.\nResearch Question: What techniques and methodologies are used in literature to detect emotions\nthrough posture recognition?\nObjectives:\n●\nObserve how frequently each technology is Used.\n●\nList the measurement metric of each methodology.\n●\nexplore the possibility of using Virtual Reality in the task of emotion recognition\nthrough posture detection.\nThose keywords were chosen while doing the systematic review to be used in the academic\ndatabases and journals: Emotion Recognition/Detection AND Posture/Pose. The Virtual Reality\nkeyword shall be used later in the paper classification step. The review shall be held from year\n2019 to 2023. After the systematic review, we noticed the absence of Virtual reality usage and\none of the main objectives of this survey paper was to explore the possibility of using Virtual\nreality technology in Pose detection so we added the Virtual Reality journal to the above, when\nthose queries were used \"Pose in Virtual Reality\", \"Pose estimation in Virtual Reality\", \"Pose\ndetection in Virtual Reality\" no results were found till 5/2024 but by combining the Pose and\nVirtual reality keywords we reached 184 research article which was refined for relevance\naccording to their title and abstract.\nReferences\nAjili, I., Mallem, M., & Didier, J.-Y. (2019). Human motions and emotions recognition inspired\nby LMA qualities. The Visual Computer, 35(10), 1411–1426.\nhttps://doi.org/10.1007/s00371-018-01619-w\nAmara, K., Kerdjidj, O., & Ramzan, N. (2023). Emotion Recognition for Affective Human\nDigital Twin by Means of Virtual Reality Enabling Technologies. IEEE Access, 11, 74216–74227.\nIEEE Access. https://doi.org/10.1109/ACCESS.2023.3285398\nAnnotations in the EMOTIC dataset. (n.d.). https://s3.sunai.uoc.edu/emotic/annotations.html\nAslanyan, M. (2024). On Mobile Pose Estimation and Action Recognition Design and\nImplementation. Pattern Recognition and Image Analysis, 34(1), 126–136.\nhttps://doi.org/10.1134/S1054661824010036\nBrain4Cars dataset. (n.d.). [Dataset]. http://brain4cars.com/\n2\n\nJournal of Artificial Intelligence Research 23 (2005) 533-585\nSubmitted _/2024; published _/2024\nBRED Dataset. (n.d.). [Dataset]. https://zenodo.org/records/3233060\nCeleghin A, Diano M, Bagnis A, Viola M and Tamietto M. (2017). Basic Emotions in Human\nNeuroscience: Neuroimaging and Beyond. Rontiers in Psychology.\nhttps://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.01432/full\nCrenn, A., Meyer, A., Konik, H., Khan, R. A., & Bouakaz, S. (2020). Generic Body Expression\nRecognition Based on Synthesis of Realistic Neutral Motion. IEEE Access, 8, 207758–207767.\nIEEE Access. https://doi.org/10.1109/ACCESS.2020.3038473\nCUI, M., FANG, J., & ZHAO, Y. (2020). Emotion recognition of human body’s posture in open\nenvironment. 2020 Chinese Control And Decision Conference (CCDC), 3294–3299.\nhttps://doi.org/10.1109/CCDC49329.2020.9164551\nDr Wanqing Li (UOW)—MSR Action3D. (n.d.). Retrieved May 26, 2024, from\nhttps://sites.google.com/view/wanqingli/data-sets/msr-action3d\nEkaterina Volkova ,Stephan de la Rosa,Heinrich H. Bülthoff ,Betty Mohler. (2014). The MPI\nbody expressions database [Dataset].\nhttps://figshare.com/articles/dataset/MPI_EMBM_Database_Mocap_Files/1220428\nEkman Paul. (1992). An argument for basic emotions. Cognitive and Emotion.\nhttps://www.tandfonline.com/doi/abs/10.1080/02699939208411068\nEWalk dataset. (n.d.). [Dataset].\nhttps://drive.google.com/drive/folders/1wWL0Yc7Oa7AMm2QqQ4lbtTIRYvMW0L2h\nExpressive Hands and Faces (EHF)—V7 Open Datasets. (n.d.). Retrieved May 26, 2024, from\nhttps://www.v7labs.com/open-datasets/expressive-hands-and-faces-ehf\nFace Tracking for Movement SDK for Unity: Unity | Oculus Developers. (n.d.). Retrieved May\n26, 2024, from https://developer.oculus.com/documentation/unity/move-face-tracking/\nFilntisis, P. P., Efthymiou, N., Koutras, P., Potamianos, G., & Maragos, P. (2019). Fusing Body\nPosture With Facial Expressions for Joint Recognition of Affect in Child–Robot Interaction. IEEE\nRobotics and Automation Letters, 4(4), 4011–4018. IEEE Robotics and Automation Letters.\nhttps://doi.org/10.1109/LRA.2019.2930434\nGemep. (n.d.). [Dataset]. https://www.unige.ch/cisa/gemep/\nGroupWalk dataset. (n.d.). [Dataset].\nhttps://drive.google.com/drive/folders/1tVoqBaQWa8bsoXr2brxNObaZ3g-FQ5QM\nGunes, H., & M. Piccardi. (2006). FABO [Dataset]. https://www.cl.cam.ac.uk/~hg410/fabo.html\nIEMOCAP. (n.d.). [Dataset]. https://sail.usc.edu/iemocap/\nKalampokas, T., Krinidis, S., Chatzis, V., & Papakostas, G. A. (2023). Performance benchmark of\n3\n\nJournal of Artificial Intelligence Research 23 (2005) 533-585\nSubmitted _/2024; published _/2024\ndeep learning human pose estimation for UAVs. Machine Vision and Applications, 34(6), 97.\nhttps://doi.org/10.1007/s00138-023-01448-5\nKeshari, T., & Palaniswamy, S. (2019). Emotion Recognition Using Feature-level Fusion of\nFacial Expressions and Body Gestures. 2019 International Conference on Communication and\nElectronics Systems (ICCES), 1184–1189. https://doi.org/10.1109/ICCES45898.2019.9002175\nKosti, Ronak and Alvarez, Jose M and Recasens, Adria and Lapedriza, Agata. (2019). EMOTIC\n[Dataset]. https://opendatalab.com/OpenDataLab/EMOTIC\nKumar, L., & Singh, D. K. (2023). Pose image generation for video content creation using\ncontrolled human pose image generation GAN. Multimedia Tools and Applications.\nhttps://doi.org/10.1007/s11042-023-17856-8\nLi, H., Yao, H., & Hou, Y. (2024). Hierarchical pose net: Spatial hierarchical body tree driven\nmulti-person pose estimation. Multimedia Tools and Applications, 83(2), 6373–6392.\nhttps://doi.org/10.1007/s11042-023-15320-1\nLiakopoulos, L., Stagakis, N., Zacharaki, E. I., & Moustakas, K. (2021). CNN-based stress and\nemotion recognition in ambulatory settings. 2021 12th International Conference on Information,\nIntelligence, Systems & Applications (IISA), 1–8.\nhttps://doi.org/10.1109/IISA52424.2021.9555508\nMalek–Podjaski, M., & Deligianni, F. (2021). Towards Explainable, Privacy-Preserved\nHuman-Motion Affect Recognition. 2021 IEEE Symposium Series on Computational Intelligence\n(SSCI), 01–09. https://doi.org/10.1109/SSCI50451.2021.9660129\nMittal, T., Bera, A., & Manocha, D. (2021). Multimodal and Context-Aware Emotion Perception\nModel With Multiplicative Fusion. IEEE MultiMedia, 28(2), 67–75. IEEE MultiMedia.\nhttps://doi.org/10.1109/MMUL.2021.3068387\nMoCap. (n.d.). [Dataset]. https://paperswithcode.com/dataset/mocap\nNesrine Fourati, Catherine Pelachaud. (2014). Emilya: Emotional body expression in daily\nactions database [Dataset]. European Language Resources Association (ELRA).\nhttp://www.lrec-conf.org/proceedings/lrec2014/pdf/334_Paper.pdf\nPapers with Code—AVA Dataset. (n.d.). Retrieved May 26, 2024, from\nhttps://paperswithcode.com/dataset/ava\nPapers with Code—MSRC-12 Dataset. (n.d.). Retrieved May 26, 2024, from\nhttps://paperswithcode.com/dataset/msrc-12\nPavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A. A., Tzionas, D., & Black, M. J.\n(2019). Expressive Body Capture: 3D Hands, Face, and Body From a Single Image. 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 10967–10977.\nhttps://doi.org/10.1109/CVPR.2019.01123\n4\n\nJournal of Artificial Intelligence Research 23 (2005) 533-585\nSubmitted _/2024; published _/2024\nPicard, R. (n.d.). Affective Computing. MIT Press.\nhttps://mitpress.mit.edu/9780262661157/affective-computing/\nPicard, R.W. and Vyzas, E. and Healey, J. (n.d.). Toward machine emotional intelligence:\nAnalysis of affective physiological state. IEEE.\nhttps://ieeexplore.ieee.org/abstract/document/954607\nPlutchik. (2017). Psychoevolutionary Theory of Emotion.\nhttps://link.springer.com/referenceworkentry/10.1007/978-3-319-28099-8_547-1\nPrakash, V. G., Kohli, M., Kohli, S., Prathosh, A. P., Wadhera, T., Das, D., Panigrahi, D., &\nKommu, J. V. S. (2023). Computer Vision-Based Assessment of Autistic Children: Analyzing\nInteractions, Emotions, Human Pose, and Life Skills. IEEE Access, 11, 47907–47929. IEEE\nAccess. https://doi.org/10.1109/ACCESS.2023.3269027\nRandhavane, T., Bhattacharya, U., Kapsaskis, K., Gray, K., Bera, A., & Manocha, D. (2019).\nLearning Perceived Emotion Using Affective and Deep Features for Mental Health Applications.\n2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),\n395–399. https://doi.org/10.1109/ISMAR-Adjunct.2019.000-2\nRazzaq, M. A., Bang, J., Kang, S. S., & Lee, S. (2020). UnSkEm: Unobtrusive Skeletal-based\nEmotion Recognition for User Experience. 2020 International Conference on Information\nNetworking (ICOIN), 92–96. https://doi.org/10.1109/ICOIN48656.2020.9016601\nRussell, J. (1980). A Complex Model of Affect.\nhttps://www.researchgate.net/publication/235361517_A_Circumplex_Model_of_Affect\nSanthoshkumar, R., & Geetha, M. K. (2019). Deep Learning Approach for Emotion Recognition\nfrom Human Body Movements with Feedforward Deep Convolution Neural Networks. Procedia\nComputer Science, 152, 158–165. https://doi.org/10.1016/j.procs.2019.05.038\nSpencer, M. (2022). EMOTIONS VS. FEELINGS VS. MOODS.\nhttps://dakotafamilyservices.org/resources/blog/archive/moods-feelings-emotions/#:~:text=While\n%20emotions%20start%20as%20sensations,both%20physical%20and%20emotional%20states.\nSWELL-KW. (n.d.). [Dataset]. http://cs.ru.nl/~skoldijk/SWELL-KW/Dataset.html\nThe Difference Between Feelings and Emotions. (n.d.). Wake Forest University. Retrieved\nJanuary 2, 2023, from https://counseling.online.wfu.edu/blog/difference-feelings-emotions/\nThe difference between Joy and happiness. (n.d.). Retrieved May 16, 2024, from\nhttps://www.embarkbh.com/blog/mental-health/joy-vs-happiness/#:~:text=Happiness%20is%20ty\npically%20a%20more,and%20satisfaction%20with%20life%20overall.\nThe motion capture library. (n.d.). [Dataset]. https://themotioncapturelibrary.co.uk/\nThe Ten Postulates of Plutchik’s (1980) psychoevolutionary theory of basic emotions. (n.d.).\nhttps://is.muni.cz/el/1421/jaro2011/PSA_033/um/plutchik.pdf\n5\n\nJournal of Artificial Intelligence Research 23 (2005) 533-585\nSubmitted _/2024; published _/2024\nUCLIC Affective Body Posture and Motion Database. (n.d.). [Dataset].\nhttp://web4.cs.ucl.ac.uk/uclic/people/n.berthouze/AffectivePostures/\nUTKinect-Action3D Dataset. (n.d.). Retrieved May 26, 2024, from\nhttps://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html\nWESAD. (n.d.). [Dataset].\nhttp://archive.ics.uci.edu/dataset/465/wesad+wearable+stress+and+affect+detection\nWu, J., Zhang, Y., & Ning, L. (2019). The Fusion Knowledge of Face, Body and Context for\nEmotion Recognition. 2019 IEEE International Conference on Multimedia & Expo Workshops\n(ICMEW), 108–113. https://doi.org/10.1109/ICMEW.2019.0-102\nXing, Y., Hu, Z., Huang, Z., Lv, C., Cao, D., & Velenis, E. (2020). Multi-Scale Driver Behaviors\nReasoning System for Intelligent Vehicles Based on a Joint Deep Learning Framework. 2020\nIEEE International Conference on Systems, Man, and Cybernetics (SMC), 4410–4415.\nhttps://doi.org/10.1109/SMC42975.2020.9283004\nYINGLIANG MA, HELENA M. PATERSON, and FRANK E. POLLICK. (2006). A motion\ncapture library for the study of identity, gender, and emotion perception from biological motion\n[Dataset]. https://link.springer.com/article/10.3758/BF03192758\nZacharatos, H., Gatzoulis, C., Charalambous, P., & Chrysanthou, Y. (2021). Emotion Recognition\nfrom 3D Motion Capture Data using Deep CNNs. 2021 IEEE Conference on Games (CoG), 1–5.\nhttps://doi.org/10.1109/CoG52621.2021.9619065\nZaghbani, S., & Bouhlel, M. S. (2022). Multi-task CNN for multi-cue affects recognition using\nupper-body gestures and facial expressions. International Journal of Information Technology,\n14(1), 531–538. https://doi.org/10.1007/s41870-021-00820-w\nZhang, X., Qi, G., Fu, X., & Zhang, N. (2023). Robust Emotion Recognition Across Diverse\nScenes: A Deep Neural Network Approach Integrating Contextual Cues. IEEE Access, 11,\n73959–73970. IEEE Access. https://doi.org/10.1109/ACCESS.2023.3296316\n6",
    "pdf_filename": "Survey_on_Emotion_Recognition_through_Posture_Detection_and_the_possibility_of_its_application_in_Vi.pdf"
}