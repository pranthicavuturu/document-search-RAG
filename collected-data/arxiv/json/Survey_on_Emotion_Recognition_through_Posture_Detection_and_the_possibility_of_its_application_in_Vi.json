{
    "title": "JournalofArtificialIntelligenceResearch23(2005) 533-585 Submitted_/2024; published_/2024",
    "abstract": "A survey is presentedfocusedonusingposeestimationtechniquesinEmotionalrecognitionusing varioustechnologiesnormalcameras,anddepthcamerasforreal-time,andthepotentialuseofVR and inputs including images, videos, and 3-dimensional poses described in vector space. We discussed 19 research papers collected from selected journals and databases highlighting their methodology, classification algorithm, and theuseddatasetsthatrelatetoemotionrecognitionand pose estimation. A benchmark has been made according to their accuracy as it was the most common performance measurement metric used. We concluded that the multimodal Approaches overall made the best accuracy and then we mentioned futuristic concerns that can improve the developmentofthisresearchtopic. Introduction Emotion recognition is one of the main vital tasks essential for having an intelligent system or application. Dealing with humans requires understanding their own emotions so that the human feels comfortable and the communication becomes more spontaneous which reflects on the efficiency of the service provided by the system/application. Emotions can be measured from multiple modalities like reading facial expressions, gesture detection, static posture, movement behavior, vocal tones, and text. When interacting with another human, you might know his current emotions from only seeing his face and sometimes the eyes can do the trick, orfromhis vocal tone, his posture - the way he is standing- or from the pattern of his movements, the gestures he is making or the context of his words whether those words are said or written - you can read an article and still visualize the emotions the writer has been through- or you can combine two or more modalities together which increases the efficiency of the human’s predictions. Computer models are being trained to recognize the above models far above is the physical measurement which may include using sensors and actuators to measure physiological patterns that are hard for the computer to measure like measuring the heart rate, body temperature, and skin sensitivity(Picard, R.W. and Vyzas, E. and Healey, J., n.d.). Those extra modalities shall prepare the computer to be able to measure emotions accuratelyevenmorethan humans, which is not currently reached. We will discuss the challenges being faced in this field and how some papers overcome those challenges. Some modalities can provide reliable measurements on their own or they may be used only to enhance the recognition of another 1",
    "body": "JournalofArtificialIntelligenceResearch23(2005) 533-585 Submitted_/2024; published_/2024\nSurvey on Emotion Recognition through Posture Detection\nand the possibility of its application in Virtual Reality\nLeinaElansary leina.saad@cis.asu.edu.eg\nMaster'sstudent,FacultyofComputerandInformation\nSciences,AinShamsUniversity,Egypt.\nZakiTaha zaki.taha@cis.asu.edu.eg\nProfessorofComputerScience,FacultyofComputerand\nInformationSciences,AinShamsUniversity,Egypt.\nWalaaGad walaagad@cis.asu.edu.eg\nProfessorofInformationSystems,FacultyofComputer\nandInformationSciences,AinShamsUniversity,Egypt\nAbstract\nA survey is presentedfocusedonusingposeestimationtechniquesinEmotionalrecognitionusing\nvarioustechnologiesnormalcameras,anddepthcamerasforreal-time,andthepotentialuseofVR\nand inputs including images, videos, and 3-dimensional poses described in vector space. We\ndiscussed 19 research papers collected from selected journals and databases highlighting their\nmethodology, classification algorithm, and theuseddatasetsthatrelatetoemotionrecognitionand\npose estimation. A benchmark has been made according to their accuracy as it was the most\ncommon performance measurement metric used. We concluded that the multimodal Approaches\noverall made the best accuracy and then we mentioned futuristic concerns that can improve the\ndevelopmentofthisresearchtopic.\nIntroduction\nEmotion recognition is one of the main vital tasks essential for having an intelligent system or\napplication. Dealing with humans requires understanding their own emotions so that the human\nfeels comfortable and the communication becomes more spontaneous which reflects on the\nefficiency of the service provided by the system/application. Emotions can be measured from\nmultiple modalities like reading facial expressions, gesture detection, static posture, movement\nbehavior, vocal tones, and text. When interacting with another human, you might know his\ncurrent emotions from only seeing his face and sometimes the eyes can do the trick, orfromhis\nvocal tone, his posture - the way he is standing- or from the pattern of his movements, the\ngestures he is making or the context of his words whether those words are said or written - you\ncan read an article and still visualize the emotions the writer has been through- or you can\ncombine two or more modalities together which increases the efficiency of the human’s\npredictions. Computer models are being trained to recognize the above models far above is the\nphysical measurement which may include using sensors and actuators to measure physiological\npatterns that are hard for the computer to measure like measuring the heart rate, body\ntemperature, and skin sensitivity(Picard, R.W. and Vyzas, E. and Healey, J., n.d.). Those extra\nmodalities shall prepare the computer to be able to measure emotions accuratelyevenmorethan\nhumans, which is not currently reached. We will discuss the challenges being faced in this field\nand how some papers overcome those challenges. Some modalities can provide reliable\nmeasurements on their own or they may be used only to enhance the recognition of another\n1\nJournalofArtificialIntelligenceResearch23(2005) 533-585 Submitted_/2024; published_/2024\nmodality and may not produce accurate results once used by themselves.Inthispaper,Ourmain\nfocus will be on using the Pose estimation modality or posture recognition to measure the\nemotions of the human interacting with affective systems. The body posture or the pose can be\ndetected from static images taken by acamera,imagesequences (capturedfromvideos)whether\nthey are previously captured or provided in real-time, using a depth camera like Kinectwhichis\nusually used in providing real-time data, or finally using the Virtual reality technology which is\nusually real-time also. The images provide 2D coordinate system data unless a 2D to 3D\nconversion algorithm is implemented and that provides us with 3D coordinate system dataorby\nusing simply the depth camera or a VR device and sometimes it shall be equipped withexternal\nsensorstoprovideafullbodydetectionincludingthelowerbody.\nResearch Question: Whattechniquesandmethodologiesareusedinliteraturetodetectemotions\nthroughposturerecognition?\nObjectives:\n● ObservehowfrequentlyeachtechnologyisUsed.\n● Listthemeasurementmetricofeachmethodology.\n● explorethepossibilityofusingVirtualRealityinthetaskofemotionrecognition\nthroughposturedetection.\nThose keywords were chosen while doing the systematic review to be used in the academic\ndatabases and journals: Emotion Recognition/Detection AND Posture/Pose. The Virtual Reality\nkeyword shall be used later in the paper classification step. The review shall be held from year\n2019 to 2023. After the systematic review, we noticed the absence of Virtual reality usage and\none of the main objectives of this survey paper was to explore the possibility of using Virtual\nreality technology in Pose detection so we added the Virtual Reality journal to the above, when\nthose queries were used \"Pose in Virtual Reality\", \"Pose estimation in Virtual Reality\", \"Pose\ndetection in Virtual Reality\" no results were found till 5/2024 but by combining the Pose and\nVirtual reality keywords we reached 184 research article which was refined for relevance\naccordingtotheirtitleandabstract.\nReferences\nAjili,I.,Mallem,M.,&Didier,J.-Y.(2019).Humanmotionsandemotionsrecognitioninspired\nbyLMAqualities.TheVisualComputer,35(10),1411–1426.\nhttps://doi.org/10.1007/s00371-018-01619-w\nAmara,K.,Kerdjidj,O.,&Ramzan,N.(2023).EmotionRecognitionforAffectiveHuman\nDigitalTwinbyMeansofVirtualRealityEnablingTechnologies.IEEEAccess,11,74216–74227.\nIEEEAccess.https://doi.org/10.1109/ACCESS.2023.3285398\nAnnotationsintheEMOTICdataset.(n.d.).https://s3.sunai.uoc.edu/emotic/annotations.html\nAslanyan,M.(2024).OnMobilePoseEstimationandActionRecognitionDesignand\nImplementation.PatternRecognitionandImageAnalysis,34(1),126–136.\nhttps://doi.org/10.1134/S1054661824010036\nBrain4Carsdataset.(n.d.).[Dataset].http://brain4cars.com/\n2\nJournalofArtificialIntelligenceResearch23(2005) 533-585 Submitted_/2024; published_/2024\nBREDDataset.(n.d.).[Dataset].https://zenodo.org/records/3233060\nCeleghinA,DianoM,BagnisA,ViolaMandTamiettoM.(2017).BasicEmotionsinHuman\nNeuroscience:NeuroimagingandBeyond.RontiersinPsychology.\nhttps://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.01432/full\nCrenn,A.,Meyer,A.,Konik,H.,Khan,R.A.,&Bouakaz,S.(2020).GenericBodyExpression\nRecognitionBasedonSynthesisofRealisticNeutralMotion.IEEEAccess,8,207758–207767.\nIEEEAccess.https://doi.org/10.1109/ACCESS.2020.3038473\nCUI,M.,FANG,J.,&ZHAO,Y.(2020).Emotionrecognitionofhumanbody’spostureinopen\nenvironment.2020ChineseControlAndDecisionConference(CCDC),3294–3299.\nhttps://doi.org/10.1109/CCDC49329.2020.9164551\nDrWanqingLi(UOW)—MSRAction3D.(n.d.).RetrievedMay26,2024,from\nhttps://sites.google.com/view/wanqingli/data-sets/msr-action3d\nEkaterinaVolkova,StephandelaRosa,HeinrichH.Bülthoff,BettyMohler.(2014).TheMPI\nbodyexpressionsdatabase[Dataset].\nhttps://figshare.com/articles/dataset/MPI_EMBM_Database_Mocap_Files/1220428\nEkmanPaul.(1992).Anargumentforbasicemotions.CognitiveandEmotion.\nhttps://www.tandfonline.com/doi/abs/10.1080/02699939208411068\nEWalkdataset.(n.d.).[Dataset].\nhttps://drive.google.com/drive/folders/1wWL0Yc7Oa7AMm2QqQ4lbtTIRYvMW0L2h\nExpressiveHandsandFaces(EHF)—V7OpenDatasets.(n.d.).RetrievedMay26,2024,from\nhttps://www.v7labs.com/open-datasets/expressive-hands-and-faces-ehf\nFaceTrackingforMovementSDKforUnity:Unity|OculusDevelopers.(n.d.).RetrievedMay\n26,2024,fromhttps://developer.oculus.com/documentation/unity/move-face-tracking/\nFilntisis,P.P.,Efthymiou,N.,Koutras,P.,Potamianos,G.,&Maragos,P.(2019).FusingBody\nPostureWithFacialExpressionsforJointRecognitionofAffectinChild–RobotInteraction.IEEE\nRoboticsandAutomationLetters,4(4),4011–4018.IEEERoboticsandAutomationLetters.\nhttps://doi.org/10.1109/LRA.2019.2930434\nGemep.(n.d.).[Dataset].https://www.unige.ch/cisa/gemep/\nGroupWalkdataset.(n.d.).[Dataset].\nhttps://drive.google.com/drive/folders/1tVoqBaQWa8bsoXr2brxNObaZ3g-FQ5QM\nGunes,H.,&M.Piccardi.(2006).FABO[Dataset].https://www.cl.cam.ac.uk/~hg410/fabo.html\nIEMOCAP.(n.d.).[Dataset].https://sail.usc.edu/iemocap/\nKalampokas,T.,Krinidis,S.,Chatzis,V.,&Papakostas,G.A.(2023).Performancebenchmarkof\n3\nJournalofArtificialIntelligenceResearch23(2005) 533-585 Submitted_/2024; published_/2024\ndeeplearninghumanposeestimationforUAVs.MachineVisionandApplications,34(6),97.\nhttps://doi.org/10.1007/s00138-023-01448-5\nKeshari,T.,&Palaniswamy,S.(2019).EmotionRecognitionUsingFeature-levelFusionof\nFacialExpressionsandBodyGestures.2019InternationalConferenceonCommunicationand\nElectronicsSystems(ICCES),1184–1189.https://doi.org/10.1109/ICCES45898.2019.9002175\nKosti,RonakandAlvarez,JoseMandRecasens,AdriaandLapedriza,Agata.(2019).EMOTIC\n[Dataset].https://opendatalab.com/OpenDataLab/EMOTIC\nKumar,L.,&Singh,D.K.(2023).Poseimagegenerationforvideocontentcreationusing\ncontrolledhumanposeimagegenerationGAN.MultimediaToolsandApplications.\nhttps://doi.org/10.1007/s11042-023-17856-8\nLi,H.,Yao,H.,&Hou,Y.(2024).Hierarchicalposenet:Spatialhierarchicalbodytreedriven\nmulti-personposeestimation.MultimediaToolsandApplications,83(2),6373–6392.\nhttps://doi.org/10.1007/s11042-023-15320-1\nLiakopoulos,L.,Stagakis,N.,Zacharaki,E.I.,&Moustakas,K.(2021).CNN-basedstressand\nemotionrecognitioninambulatorysettings.202112thInternationalConferenceonInformation,\nIntelligence,Systems&Applications(IISA),1–8.\nhttps://doi.org/10.1109/IISA52424.2021.9555508\nMalek–Podjaski,M.,&Deligianni,F.(2021).TowardsExplainable,Privacy-Preserved\nHuman-MotionAffectRecognition.2021IEEESymposiumSeriesonComputationalIntelligence\n(SSCI),01–09.https://doi.org/10.1109/SSCI50451.2021.9660129\nMittal,T.,Bera,A.,&Manocha,D.(2021).MultimodalandContext-AwareEmotionPerception\nModelWithMultiplicativeFusion.IEEEMultiMedia,28(2),67–75.IEEEMultiMedia.\nhttps://doi.org/10.1109/MMUL.2021.3068387\nMoCap.(n.d.).[Dataset].https://paperswithcode.com/dataset/mocap\nNesrineFourati,CatherinePelachaud.(2014).Emilya:Emotionalbodyexpressionindaily\nactionsdatabase[Dataset].EuropeanLanguageResourcesAssociation(ELRA).\nhttp://www.lrec-conf.org/proceedings/lrec2014/pdf/334_Paper.pdf\nPaperswithCode—AVADataset.(n.d.).RetrievedMay26,2024,from\nhttps://paperswithcode.com/dataset/ava\nPaperswithCode—MSRC-12Dataset.(n.d.).RetrievedMay26,2024,from\nhttps://paperswithcode.com/dataset/msrc-12\nPavlakos,G.,Choutas,V.,Ghorbani,N.,Bolkart,T.,Osman,A.A.,Tzionas,D.,&Black,M.J.\n(2019).ExpressiveBodyCapture:3DHands,Face,andBodyFromaSingleImage.2019\nIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),10967–10977.\nhttps://doi.org/10.1109/CVPR.2019.01123\n4\nJournalofArtificialIntelligenceResearch23(2005) 533-585 Submitted_/2024; published_/2024\nPicard,R.(n.d.).AffectiveComputing.MITPress.\nhttps://mitpress.mit.edu/9780262661157/affective-computing/\nPicard,R.W.andVyzas,E.andHealey,J.(n.d.).Towardmachineemotionalintelligence:\nAnalysisofaffectivephysiologicalstate.IEEE.\nhttps://ieeexplore.ieee.org/abstract/document/954607\nPlutchik.(2017).PsychoevolutionaryTheoryofEmotion.\nhttps://link.springer.com/referenceworkentry/10.1007/978-3-319-28099-8_547-1\nPrakash,V.G.,Kohli,M.,Kohli,S.,Prathosh,A.P.,Wadhera,T.,Das,D.,Panigrahi,D.,&\nKommu,J.V.S.(2023).ComputerVision-BasedAssessmentofAutisticChildren:Analyzing\nInteractions,Emotions,HumanPose,andLifeSkills.IEEEAccess,11,47907–47929.IEEE\nAccess.https://doi.org/10.1109/ACCESS.2023.3269027\nRandhavane,T.,Bhattacharya,U.,Kapsaskis,K.,Gray,K.,Bera,A.,&Manocha,D.(2019).\nLearningPerceivedEmotionUsingAffectiveandDeepFeaturesforMentalHealthApplications.\n2019IEEEInternationalSymposiumonMixedandAugmentedRealityAdjunct(ISMAR-Adjunct),\n395–399.https://doi.org/10.1109/ISMAR-Adjunct.2019.000-2\nRazzaq,M.A.,Bang,J.,Kang,S.S.,&Lee,S.(2020).UnSkEm:UnobtrusiveSkeletal-based\nEmotionRecognitionforUserExperience.2020InternationalConferenceonInformation\nNetworking(ICOIN),92–96.https://doi.org/10.1109/ICOIN48656.2020.9016601\nRussell,J.(1980).AComplexModelofAffect.\nhttps://www.researchgate.net/publication/235361517_A_Circumplex_Model_of_Affect\nSanthoshkumar,R.,&Geetha,M.K.(2019).DeepLearningApproachforEmotionRecognition\nfromHumanBodyMovementswithFeedforwardDeepConvolutionNeuralNetworks.Procedia\nComputerScience,152,158–165.https://doi.org/10.1016/j.procs.2019.05.038\nSpencer,M.(2022).EMOTIONSVS.FEELINGSVS.MOODS.\nhttps://dakotafamilyservices.org/resources/blog/archive/moods-feelings-emotions/#:~:text=While\n%20emotions%20start%20as%20sensations,both%20physical%20and%20emotional%20states.\nSWELL-KW.(n.d.).[Dataset].http://cs.ru.nl/~skoldijk/SWELL-KW/Dataset.html\nTheDifferenceBetweenFeelingsandEmotions.(n.d.).WakeForestUniversity.Retrieved\nJanuary2,2023,fromhttps://counseling.online.wfu.edu/blog/difference-feelings-emotions/\nThedifferencebetweenJoyandhappiness.(n.d.).RetrievedMay16,2024,from\nhttps://www.embarkbh.com/blog/mental-health/joy-vs-happiness/#:~:text=Happiness%20is%20ty\npically%20a%20more,and%20satisfaction%20with%20life%20overall.\nThemotioncapturelibrary.(n.d.).[Dataset].https://themotioncapturelibrary.co.uk/\nTheTenPostulatesofPlutchik’s(1980)psychoevolutionarytheoryofbasicemotions.(n.d.).\nhttps://is.muni.cz/el/1421/jaro2011/PSA_033/um/plutchik.pdf\n5\nJournalofArtificialIntelligenceResearch23(2005) 533-585 Submitted_/2024; published_/2024\nUCLICAffectiveBodyPostureandMotionDatabase.(n.d.).[Dataset].\nhttp://web4.cs.ucl.ac.uk/uclic/people/n.berthouze/AffectivePostures/\nUTKinect-Action3DDataset.(n.d.).RetrievedMay26,2024,from\nhttps://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html\nWESAD.(n.d.).[Dataset].\nhttp://archive.ics.uci.edu/dataset/465/wesad+wearable+stress+and+affect+detection\nWu,J.,Zhang,Y.,&Ning,L.(2019).TheFusionKnowledgeofFace,BodyandContextfor\nEmotionRecognition.2019IEEEInternationalConferenceonMultimedia&ExpoWorkshops\n(ICMEW),108–113.https://doi.org/10.1109/ICMEW.2019.0-102\nXing,Y.,Hu,Z.,Huang,Z.,Lv,C.,Cao,D.,&Velenis,E.(2020).Multi-ScaleDriverBehaviors\nReasoningSystemforIntelligentVehiclesBasedonaJointDeepLearningFramework.2020\nIEEEInternationalConferenceonSystems,Man,andCybernetics(SMC),4410–4415.\nhttps://doi.org/10.1109/SMC42975.2020.9283004\nYINGLIANGMA,HELENAM.PATERSON,andFRANKE.POLLICK.(2006).Amotion\ncapturelibraryforthestudyofidentity,gender,andemotionperceptionfrombiologicalmotion\n[Dataset].https://link.springer.com/article/10.3758/BF03192758\nZacharatos,H.,Gatzoulis,C.,Charalambous,P.,&Chrysanthou,Y.(2021).EmotionRecognition\nfrom3DMotionCaptureDatausingDeepCNNs.2021IEEEConferenceonGames(CoG),1–5.\nhttps://doi.org/10.1109/CoG52621.2021.9619065\nZaghbani,S.,&Bouhlel,M.S.(2022).Multi-taskCNNformulti-cueaffectsrecognitionusing\nupper-bodygesturesandfacialexpressions.InternationalJournalofInformationTechnology,\n14(1),531–538.https://doi.org/10.1007/s41870-021-00820-w\nZhang,X.,Qi,G.,Fu,X.,&Zhang,N.(2023).RobustEmotionRecognitionAcrossDiverse\nScenes:ADeepNeuralNetworkApproachIntegratingContextualCues.IEEEAccess,11,\n73959–73970.IEEEAccess.https://doi.org/10.1109/ACCESS.2023.3296316\n6",
    "pdf_filename": "Survey_on_Emotion_Recognition_through_Posture_Detection_and_the_possibility_of_its_application_in_Vi.pdf"
}