{
    "title": "Reference-free Hallucination Detection for",
    "abstract": "Largevision-languagemodels(LVLMs)have made significant progress in recent years. WhileLVLMsexhibitexcellentabilityinlan- guageunderstanding,questionanswering,and conversationsofvisualinputs,theyareproneto producinghallucinations. Whileseveralmeth- odsareproposedtoevaluatethehallucinations in LVLMs, most are reference-based and de- pendonexternaltools,whichcomplicatestheir practicalapplication. Toassesstheviabilityof alternativemethods,itiscriticaltounderstand whetherthereference-freeapproaches,which donotrelyonanyexternaltools,canefficiently detecthallucinations. Therefore,weinitiatean exploratorystudytodemonstratetheeffective- nessofdifferentreference-freesolutionsinde- tectinghallucinationsinLVLMs. Inparticular, weconductanextensivestudyonthreekinds oftechniques: uncertainty-based,consistency- based, and supervised uncertainty quantifica- tion methods on four representative LVLMs across two different tasks. The empirical re- Figure1: Reference-freeHallucinationDetectionMeth- sultsshowthatthereference-freeapproaches odsusedinthiswork. arecapableofeffectivelydetectingnon-factual responsesinLVLMs,withthesuperviseduncer- taintyquantificationmethodoutperformingthe others,achievingthebestperformanceacross isnotpresentintheinputimage. Thisphenomenon differentsettings. poseschallengesforthereliabilityofLVLMs,high- lightingintrinsiclimitationsofcurrentmodels in maintainingcoherencebetweenvisualinputsand 1 Introduction textualdescriptions(Huangetal.,2023a). Large vision-language models (LVLMs) like Most existing methods of assessing hallucina- LLaVA(Liuetal.,2024),MiniGPT-4(Zhuetal., tionsrelyonexternalmodels. Lietal.(2023)pro- 2024b)havedemonstratedremarkablecapabilities posePOPE,whichrequiresautomatedsegmenta- in understanding and generating complex visual tion tools like SEEM (Zou et al., 2024) to iden- and textual content. However, an emerging con- tify present and absent objects in images. Faith- cern with these models is their tendency towards score(Jingetal.,2023)utilizesavisualentailment hallucination(Zhouetal.,2024;Zhuetal.,2024a; model(Xieetal.,2019)tovalidatethefactuality, Gengetal.,2024b). Forinstance, amodelmight focusingonobjectattributesandtheirrelationships. describeanobjectoreventinthegeneratedtextthat Nonetheless,theseapproachesincreasethecompu- tationalcostandarerestrictedbythecapabilities *Equalcontribution. †Correspondingauthor. ofthesemodels. 4202 voN 91 ]LC.sc[ 2v76750.8042:viXra",
    "body": "Reference-free Hallucination Detection for\nLarge Vision-Language Models\nQingLi1∗,JiahuiGeng1∗,ChenyangLyu1,DeruiZhu2,MaximPanov1,FakhriKarray1†\n1 MohamedbinZayedUniversityofArtificialIntelligence\n{qing.li,jiahui.geng,chenyang.lyu,maxim.panov,fakhri.karray}@mbzuai.ac.ae\n2 TechnicalUniversityofMunich\nderui.zhu@tum.de\nAbstract\nLargevision-languagemodels(LVLMs)have\nmade significant progress in recent years.\nWhileLVLMsexhibitexcellentabilityinlan-\nguageunderstanding,questionanswering,and\nconversationsofvisualinputs,theyareproneto\nproducinghallucinations. Whileseveralmeth-\nodsareproposedtoevaluatethehallucinations\nin LVLMs, most are reference-based and de-\npendonexternaltools,whichcomplicatestheir\npracticalapplication. Toassesstheviabilityof\nalternativemethods,itiscriticaltounderstand\nwhetherthereference-freeapproaches,which\ndonotrelyonanyexternaltools,canefficiently\ndetecthallucinations. Therefore,weinitiatean\nexploratorystudytodemonstratetheeffective-\nnessofdifferentreference-freesolutionsinde-\ntectinghallucinationsinLVLMs. Inparticular,\nweconductanextensivestudyonthreekinds\noftechniques: uncertainty-based,consistency-\nbased, and supervised uncertainty quantifica-\ntion methods on four representative LVLMs\nacross two different tasks. The empirical re-\nFigure1: Reference-freeHallucinationDetectionMeth-\nsultsshowthatthereference-freeapproaches\nodsusedinthiswork.\narecapableofeffectivelydetectingnon-factual\nresponsesinLVLMs,withthesuperviseduncer-\ntaintyquantificationmethodoutperformingthe\nothers,achievingthebestperformanceacross isnotpresentintheinputimage. Thisphenomenon\ndifferentsettings.\nposeschallengesforthereliabilityofLVLMs,high-\nlightingintrinsiclimitationsofcurrentmodels in\nmaintainingcoherencebetweenvisualinputsand\n1 Introduction\ntextualdescriptions(Huangetal.,2023a).\nLarge vision-language models (LVLMs) like Most existing methods of assessing hallucina-\nLLaVA(Liuetal.,2024),MiniGPT-4(Zhuetal., tionsrelyonexternalmodels. Lietal.(2023)pro-\n2024b)havedemonstratedremarkablecapabilities posePOPE,whichrequiresautomatedsegmenta-\nin understanding and generating complex visual tion tools like SEEM (Zou et al., 2024) to iden-\nand textual content. However, an emerging con- tify present and absent objects in images. Faith-\ncern with these models is their tendency towards score(Jingetal.,2023)utilizesavisualentailment\nhallucination(Zhouetal.,2024;Zhuetal.,2024a; model(Xieetal.,2019)tovalidatethefactuality,\nGengetal.,2024b). Forinstance, amodelmight focusingonobjectattributesandtheirrelationships.\ndescribeanobjectoreventinthegeneratedtextthat Nonetheless,theseapproachesincreasethecompu-\ntationalcostandarerestrictedbythecapabilities\n*Equalcontribution.\n†Correspondingauthor. ofthesemodels.\n4202\nvoN\n91\n]LC.sc[\n2v76750.8042:viXra\nLLaVA- LLaVA-\nMini- LLaVA-\nv1.6- v1.6-\nMethod GPT- v1.5-\nvicuna- mistral-\n4v 7b\n7b 7b\nRandom 48.73 48.73 48.73 48.73\nUncertainty-basedMethods\nFigure2: Twodatasources: self-generatedandhand-\nAvgProb 77.43 86.82 91.80 92.04\ncrafteddata. Thetestmodelassignsaprobabilityof0.9\nAvgEnt 48.89 50.05 48.90 48.92\nto“cat”inself-generatedinput,whileinhand-crafted\nSupervisedUncertaintyQuantificationMethod\ninput,“dog”receivesaprobabilityof0.1. SUQ 87.23 92.35 93.32 93.50\nBaseline 48.73 48.73 48.73 48.73\nUncertainty-basedMethods\nReference-free methods are a promising ap-\nAvgProb 86.62 95.89 96.23 96.99\nproach to address this issue, primarily utilizing AvgEnt 48.76 50.25 49.91 50.41\nthemodel’sownknowledge. Theyhaveattracted SupervisedUncertaintyQuantificationMethod\nSUQ 94.44 97.86 98.68 98.33\ngreat research interest in Large Language Mod-\nels (LLMs), with typical methods including the Baseline 48.73 48.73 48.73 48.73\nUncertainty-basedMethods\nuncertainty-basedmethods(Huetal.,2023;Geng\nAvgProb 82.11 92.73 93.57 95.06\netal.,2024a;Huangetal.,2023b;Vazhentsevetal., AvgEnt 48.50 49.76 50.63 51.39\n2023; Fadeeva et al., 2023), consistency-based SupervisedUncertaintyQuantificationMethod\nSUQ 91.02 97.85 98.11 98.50\nmethods (Manakul et al., 2023) and supervised\nuncertaintyquantification(SUQ)approach(Chen Baseline 48.47 48.47 48.47 48.47\nUncertainty-basedMethods\netal.,2023;AzariaandMitchell,2023). However,\nAvgProb 66.87 78.45 80.31 79.15\ntheirpotentialinthecontextofLVLMsisunknown. AvgEnt 48.75 49.88 48.95 49.33\nTheyhavenotyetbeensystematicallystudied,pos- SupervisedUncertaintyQuantificationMethod\nSUQ 69.14 82.84 84.57 83.25\nsiblyduetothecomplexityofmultimodaldata. We\narecommittedtobridgingthisgap.\nTable1: AUC-PRforYes-or-Notasks.\nIn this work, we collect and implement a bat-\nteryofstate-of-the-artmethods,includingfourmet-\nricsofuncertainty-basedmethods,fourvariantsof uncertainty, shown in Figure 1(b). Uncertainty-\nconsistency-basedmethods,andthesupervisedun- based methods are extensively studied for hal-\ncertaintyquantificationmethod. Theexperiments lucination detection in LLMs (Hu et al., 2023;\nareconductedonfiveLVLMswithdifferenttypes, Gengetal.,2024a;Huangetal.,2023b;Fadeeva\nversions,andsizesonYes-and-NoandOpen-ended etal.,2024),andarealsousedasbenchmarktech-\ntasks. The main contributions of this paper are niques (Zhu et al., 2024a; Manakul et al., 2023).\nsummarizedasfollows: We adopt the four proposed metrics: AvgProb,\nAvgEnt, MaxProb, MaxEnt to aggregate token-\n1. We comprehensively measure the perfor-\nlevel uncertainty to measure sentence-level and\nmanceofdifferentreference-freeapproaches\npassage-leveluncertainty. AppendixA.1provides\nindetectinghallucination.\ndetaileddescriptionsofthesemetrics.\n2. We demonstrate that the supervised uncer- Consistency-basedmethodsmainlyincludeself-\ntaintyquantification(SUQ)methodperforms consistency,cross-questionconsistency,andcross-\nbestacrossvarioussettings. model consistency (Zhang et al., 2023). In this\nwork, we focus on four variants – BERTScore,\n3. WecontributetheImage-HallucinationAnno-\nQuestionAnswering(QA),Unigram,NaturalLan-\ntationDataset(IHAD),amanuallyannotated,\nguage Inference (NLI) – of the self-consistency\nsentence-level dataset created by prompting\nmethodproposedin(Manakuletal.,2023)tode-\nLLaVA-v1.5-7b.\ntectnon-factualinformationforLVLMs,asdemon-\nstratedinFigure1(c). Thehallucinationscorefor\n2 Reference-freeHallucinationDetection\nthei-thsentence,denotedbyS(i),rangesfrom0.0\nMethods\nto1.0,whereascorecloseto0.0indicatesthesen-\nUncertainty-basedmethodshavethehypothesis tenceisaccurate,andascorenear1.0suggeststhe\nthat when LVLMs are uncertain about generated sentenceishallucinated. Moredetailedinformation\ninformation, generated tokens often have higher canbefoundinAppendixA.2.\nvdA\nnaR\npoP\nAQG\nLLaVA-LLaVA-LLaVA- LLaVA-LLaVA-LLaVA-\nMini- LLaVA- Mini- LLaVA-\nv1.6- v1.6- v1.6- v1.6- v1.6- v1.6-\nMethod GPT- v1.5- Method GPT- v1.5-\nvicuna- mistral- vicuna- vicuna- mistral- vicuna-\n4v 7b 4v 7b\n7b 7b 13b 7b 7b 13b\nBaseline 29.29 29.29 29.29 29.29 29.29 Baseline 64.05 64.05 64.05 64.05 64.05\nUncertainty-basedMethods Uncertainty-basedMethods\nAvgProb 30.52 37.22 38.61 37.54 43.30 AvgProb 70.28 72.78 72.89 73.32 81.23\nAvgEnt 29.92 32.18 39.13 37.22 40.11 AvgEnt 64.89 64.44 70.83 70.05 72.97\nMaxProb 33.45 32.05 34.86 34.73 40.03 MaxProb 66.68 68.97 72.67 70.37 73.56\nMaxEnt 29.96 29.71 39.71 39.62 33.14 MaxEnt 68.75 64.99 77.17 78.62 80.23\nConsistency-basedMethods SupervisedUncertaintyQuantificationMethod\nQA 35.08 40.27 41.44 39.48 46.38 SUQ 84.92 89.41 85.91 86.54 91.22\nBERTScore 37.21 39.13 51.16 51.64 54.12\nBaseline 83.00 83.00 83.00 83.00 83.00\nUnigram\n37.63 38.68 39.62 40.08 49.23 Uncertainty-basedMethods\n(max)\nAvgProb 84.29 87.50 87.38 87.63 90.23\nNLI 42.14 44.09 52.39 51.91 57.36\nAvgEnt 83.06 85.64 88.41 87.45 89.41\nSupervisedUncertaintyQuantificationMethod\nMaxProb 84.17 89.40 89.89 89.39 91.20\nSUQ 62.53 61.23 65.26 64.78 70.12\nMaxEnt 83.03 87.18 88.21 88.44 92.10\nBaseline 35.71 35.71 35.71 35.71 35.71 SupervisedUncertaintyQuantificationMethod\nUncertainty-basedMethods SUQ 85.34 94.08 93.09 94.07 96.03\nAvgProb 36.16 42.35 50.88 50.75 53.42\nAvgEnt 35.81 37.18 48.38 46.86 50.04\nTable3: AUC-PRforpassage-levelOpen-endedtasks.\nMaxProb 35.97 41.89 46.24 45.96 45.21\nMaxEnt 35.53 35.88 50.85 46.65 50.33\nConsistency-basedMethods\nscriptionofthegivenimage”. Ran,Pop,Adv,and\nQA 41.34 54.72 48.15 49.19 58.12\nBERTScore 40.29 42.37 49.03 47.87 50.33 asubsetofGQAusedinthisworkevaluateobject\nUnigram 39.26 39.06 43.13 44.10 49.79 hallucinationasabinaryclassificationtask,prompt-\n(max)\ningLVLMstooutput“Yes”or“No”,categorizing\nNLI 49.59 50.80 50.94 58.41 62.58\nSupervisedUncertaintyQuantificationMethod them as Yes-or-No tasks. In contrast, M-Hal and\nSUQ 62.73 63.85 65.74 64.17 69.19 IHAD involve long text answers based on open-\nendedquestions,makingthemOpen-endedtasks.\nTable2: AUC-PRforsentence-levelOpen-endedtasks.\nMoredetaileddatasetinformationisshowninAp-\npendixB.1.\nModels. We evaluate five representative cate-\nSuperviseduncertaintyquantification(SUQ).To\ngoriesofLVLMs: MiniGPT-4v(Zhuetal.,2024b),\nanalyzeandunderstandtheseinternalstates,SUQ\nLLaVA-v1.5-7b (Liu et al., 2023), LLaVA-v1.6-\nmethod (Chen et al., 2023; Azaria and Mitchell,\nvicuna-7b, LLaVA-v1.6-mistral-7b (Liu et al.,\n2023) shown in Figure 1(a) train a classifier, re-\n2024) and LLaVA-v1.6-vicuna-13b. These mod-\nferredtoasaprobe,onadatasetcontaininglabeled\nels, featuring different architectures and strong\nexamples. Thisclassifieroutputsthelikelihoodof\nperformance, are used as benchmarks in various\nastatementbeingtruthfulbyanalyzingthehidden\nworks(Huangetal.,2023a;Zhouetal.,2024). We\nlayeractivationsoftheLVLMduringthereading\nonlytestmodelsofa7-billionand13-billionsize\norgenerationofstates.\nduetothelimitedcomputationalresources.\n3 StudyDesign ExperimentalSetup. Wecombinequestionsand\nresponses to create inputs for test models. If the\nDataset. Ourexperimentsareconductedonpublic response is from this test model, it is considered\ndatasets: POPE(Lietal.,2023),GQA(Hudsonand self-generated input; otherwise, it is regarded as\nManning,2019),andM-HalDetect(Gunjaletal., hand-craftedinputforthismodel. Inputsarethen\n2024). POPEcontainsthreesubsets: random,pop- processedbytestmodels, fromwhichweextract\nular, and adversarial. For simplicity, we refer to internalinformation,includingprobability,entropy,\nthemasRan,Pop,andAdv,andwealsoabbreviate and embedding states. In this work, all datasets\nM-HalDetect as M-Hal. In addition, we create a are hand-crafted except for IHAD, which is self-\nmanuallyannotated,sentence-leveldataset,IHAD, generateddataonlyforLLaVA-v1.5-7b. Figure2\nbyusingLLaVA-v1.5-7btogenerateconciseimage showstestmodelassignsdifferentinternalinforma-\ndescriptionsfor500imagesfromMSCOCO(Lin tiontotwodifferentinputs. Detailedexperimental\net al., 2014) using prompt “Provide a concise de- setupsarepresentedinAppendixB.2.\nlaH-M\nDAHI\nlaH-M\nDAHI\n(a)AvgProb (b)SUQ (c)NLI\nFigure3: Comparisonusingimageswithdifferentclarity.\n4 ExperimentalResultsandAnalysis of LLaVA-v1.6-7b exceeds the highest perform-\ninguncertaintymetric,MaxEnt,byapproximately\nThe comprehensive results on Open-ended tasks\n12%. BERTScoreandQAoutperformtheuncertain\narepresentedinTables1, 2and3. Thenumbers\nestimationinmostsetups.\nin the pink and yellow background, respectively,\nPassage-level Hallucination Detection. We cal-\nrepresentthebestresultsfortheuncertainty-based\nculate the metrics of uncertainty-based methods\nmethodsandconsistency-basedmethods,whilethe\nacross all tokens in a passage and take the em-\nnumbers in bold indicate the overall best results.\nbedding information of the passage’s last token\nWe utilize AUC-PR (Davis and Goadrich, 2006),\nforSUQ.Consistency-basedmethodsarenotused\nwhichstandsfortheareaundertheprecision-recall\nto detect non-factual information in passages as\ncurve,toobjectivelyevaluatetheeffectivenessof\nthe heavy computation. Our results in Table 3\neachmodel. ThehigherthevalueofAUC-PR,the\nshowthattheSUQmethodisstillbetterthanthe\nstrongertheabilityofthismethodforhallucination\nuncertainty-basedmethods,includingprobability\ndetection.\nandentropymethods. Besides,uncertainty-based\n4.1 EffectivenessofVariousMethods methodsareweaklybetterthantherandombase-\nlineatpassage-level.\nHallucinationDetectiononYes-or-NoTasks. We\nonlycomparetheuncertainty-basedandSUQmeth-\nods on Yes-or-No tasks because the model’s an- 4.2 RobustnessandLimitationsofSUQ\nswers exclusively contain “Yes” or “No”. The\nrandom baseline is calculated as the ratio of non- ToinvestigatetherobustnessofSUQmethods,we\nfactual examples to the total number of exam- employtwoclassifierstoassesstheimpactoneach\nples. OurresultsinTable1demonstratethatSUQ dataset. Eachdatasetissplitintoatrainingsetanda\nmethod outperforms the uncertainty-based meth- testsetina3:1ratio. Oneclassifieristrainedonthe\nods,exceedingAvgProbbyabout4.5%. AvgProb Poptrainingset, whiletheotheristrainedonthe\nachievesAUC-PRvaluesnearlydoublethoseofthe trainingsetofeachdatasetindividually. Figure4\nrandom baseline across all datasets and models, illustratesthattheclassifiertrainedonPopexhibits\nindicating it is an efficient method since it is an comparableperformancetoclassifierstrainedon\nunsupervisedmethod. Inaddition,theprobability separateddatasets,indicatingtherobustnessofthe\nSUQmethod.\nProboverallperformsbetterthantheentropyEnt\ninYes-or-Notasks. Althoughourexperimentsconfirmedtheeffec-\nSentence-level Hallucination Detection. Based tiveness of the SUQ method, it was found to be\non the data in Table 2, we find that in most in- ineffectiveindetectingsubtle,manuallycraftedhal-\nstances, the consistency-based approaches sur- lucinations. Forexample,thedifferencebetween\npassestheuncertainty-basedmetrics, yettheyre- a correct statement, “There is a red apple and a\nmainsignificantlyinferiortotheSUQmethodby cutekitten.” andanincorrectone,“Thereisared\napproximately 15%. Among consistency-based orangeandacutekitten.” hasanegligibleeffecton\nmethods, NLI outperforms best while Unigram themodel’shiddenstateatthefinalposition,pos-\nshowstheleasteffectivenessinmostcases. Specif- ingachallengeforSUQmethodstodifferentiate\nically,ontheM-Haldataset,theNLIperformance betweenthem.\nFigure4: Comparisonofusingdifferentclassifiers. Figure5: Comparisonofusingdifferentdatasources.\n4.3 ImpactofImageClarity\nOurresultsindicatethattheSUQmethodperforms\nToexplorethisquestion,weemploytheGaussian best as a supervised method, but it relies on suf-\nblurtechnique,whichsmoothstheimagebyassign- ficient training data, which could limit its appli-\ningaweightedaveragetothesurroundingpixelsof cability in certain scenarios. Consistency-based\neachpixel,resultinginanobscurepicture. Weset methods outperform uncertainty-based methods;\nradius = 10inourexperiments. Figure3showsthe however, they require more computation and are\neffectivenessofAvgProb,SUQ,andNLIinblurred moresuitableforblack-boxmodels. Uncertainty-\nimagesaresignificantlyreducedcomparedtoclear basedmethodsareparticularlywell-suitedfortasks\nimages. Thephenomenonsuggeststhatenhancing similartoYes-or-Noquestions.\nimageclarityisacrucialstrategyforboostingthe\neffectivenessofhallucinationdetectionmethods. Limitations\n4.4 ImpactofDataSource Threemajorlimitationsareidentifiedinthiswork.\nFirst, we did not compare with reference-based\nReference-free methods, particularly uncertain-\nmethods. The performance of reference-based\nbasedandSUQmethods,relyontheinternalinfor-\nmethods depends on the performance of external\nmation of models. A natural question is whether\nmodels and is specific to certain tasks. In this ar-\nthesemethodsareaffectedbydatasources. M-Hal\nticle, we primarily aim to explore the potential\nandIHADarehand-crafteddataandself-generated\nof reference-free methods, which have a broader\ndata,respectively,forLLaVA-v1.5-7b. Therefore,\nrangeofapplications. Second,wedidnotclassify\nwecomparevariousdetectionmethodsonM-Hal\ntypesofhallucinations;limitedbyresources,even\nandIHADbasedonLLaVA-v1.5-7b. Specifically,\nwith fine-grained annotations, we only marked\nwecalculate∆AUC-PR\nwhether sentences contained hallucinations with-\n∆AUC-PR i,j = AUC-PR i,j −AUC-PR i,baseline, out distinguishing types such as object existence\nerrors,attributeerrors,textrecognitionerrors,and\nwhereireferstoM-HalandIHAD,j denotesdif-\nfact-conflictingerrors. ThismightbewhereSUQ\nferentapproaches.\nmethodshaveanadvantage. Third,weonlystud-\nFigure 5 shows that in uncertainty-based ap-\nied white-box models. Recent research indicates\nproaches,somemetricsoutperforminM-Halwhile\nthatwhite-boxmodelscanbeproxiesforblack-box\nothers excel in IHAD, a trend also observed in\nmodelstocalculatetheirconfidenceregardingspe-\nconsistency-basedtechniques. Thisphenomenon\ncific issues. Unigram method is one such proxy\nillustratesthatthereference-freehallucinationde-\nmodelapproach. Inthefuture,wecouldusemore\ntectionmethodsareinsensitivetothedatasource.\ncomplex models, closer in architecture to the tar-\nOthers. Wehavealsoexploredtheimpactofunim-\ngetmodels,asproxiestoexploretheirpotentialin\nportanttokensforuncertainty-basedmethodsand\nhallucinationdetection.\nthe optimal hidden layers for SUQ. They are dis-\ncussedrespectivelyinAppendicesC.1andC.2.\nEthicsandBroaderImpact\n5 Conclusion\nWe sampled a portion of the data from existing\nThispapersystematicallystudiesreference-freeap- datasetsforourexperiments,whichmayaffectthe\nproachesfordetectingthehallucinationofLVLMs. accuracyofsomeofourconclusions.\nReferences EducationalAdvancesinArtificialIntelligence,EAAI\n2014, February 20-27, 2024, Vancouver, Canada,\nAmos Azaria and Tom Mitchell. 2023. The internal\npages18135–18143.AAAIPress.\nstate of an LLM knows when it’s lying. In Find-\ningsoftheAssociationforComputationalLinguistics: PengchengHe,JianfengGao,andWeizhuChen.2023.\nEMNLP2023,pages967–976,Singapore.Associa- Debertav3: Improving deberta using electra-style\ntionforComputationalLinguistics. pre-trainingwithgradient-disentangledembedding\nsharing. InTheEleventhInternationalConference\nNuoChen,NingWu,ShiningLiang,MingGong,Linjun\non Learning Representations, ICLR 2023, Kigali,\nShou,DongmeiZhang,andJiaLi.2023. Beyondsur-\nRwanda,May1-5,2023.OpenReview.net.\nface: Probingllamaacrossscalesandlayers. ArXiv\npreprint,abs/2312.04333. Mengting Hu, Zhen Zhang, Shiwan Zhao, Minlie\nHuang,andBingzheWu.2023. Uncertaintyinnatu-\nJesseDavisandMarkGoadrich.2006. Therelationship\nrallanguageprocessing: Sources,quantification,and\nbetweenprecision-recallandroccurves. InProceed-\napplications. ArXivpreprint,abs/2306.04459.\ningsofthe23rdinternationalconferenceonMachine\nlearning,pages233–240. Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,\nConghui He, Jiaqi Wang, Dahua Lin, Weiming\nHanyu Duan, Yi Yang, and Kar Yan Tam. 2024. Do\nZhang,andNenghaiYu.2023a. Opera: Alleviating\nllms know about hallucination? an empirical in-\nhallucinationinmulti-modallargelanguagemodels\nvestigation of llm’s hidden states. ArXiv preprint,\nvia over-trust penalty and retrospection-allocation.\nabs/2402.09733.\nArXivpreprint,abs/2311.17911.\nEkaterina Fadeeva, Aleksandr Rubashevskii, Artem\nYuheng Huang, Jiayang Song, Zhijie Wang, Huam-\nShelmanov, Sergey Petrakov, Haonan Li, Hamdy\ning Chen, and Lei Ma. 2023b. Look before you\nMubarak,EvgeniiTsymbalov,GlebKuzmin,Alexan-\nleap: Anexploratorystudyofuncertaintymeasure-\nder Panchenko, Timothy Baldwin, Preslav Nakov,\nment for large language models. ArXiv preprint,\nandMaximPanov.2024. Fact-checkingtheoutput\nabs/2307.10236.\noflargelanguagemodelsviatoken-leveluncertainty\nquantification. In Findings of the Association for Drew A. Hudson and Christopher D. Manning. 2019.\nComputationalLinguistics: ACL2024.Association GQA: A new dataset for real-world visual reason-\nforComputationalLinguistics. ingandcompositionalquestionanswering. InIEEE\nConferenceonComputerVisionandPatternRecogni-\nEkaterina Fadeeva, Roman Vashurin, Akim Tsvigun,\ntion,CVPR2019,LongBeach,CA,USA,June16-20,\nArtemVazhentsev,SergeyPetrakov,KirillFedyanin,\n2019,pages6700–6709.ComputerVisionFounda-\nDaniil Vasilev, Elizaveta Goncharova, Alexander\ntion/IEEE.\nPanchenko, Maxim Panov, Timothy Baldwin, and\nArtemShelmanov.2023. Lm-polygraph:Uncertainty LiqiangJing,RuosenLi,YunmoChen,MengzhaoJia,\nestimationforlanguagemodels. InProceedingsof and Xinya Du. 2023. Faithscore: Evaluating hal-\nthe2023ConferenceonEmpiricalMethodsinNat- lucinationsinlargevision-languagemodels. ArXiv\nuralLanguageProcessing, EMNLP2023-System preprint,abs/2311.01477.\nDemonstrations, Singapore, December6-10, 2023,\npages446–461.AssociationforComputationalLin- YifanLi,YifanDu,KunZhou,JinpengWang,XinZhao,\nguistics. andJi-RongWen.2023. Evaluatingobjecthallucina-\ntioninlargevision-languagemodels. InProceedings\nJiahuiGeng,FengyuCai,YuxiaWang,HeinzKoeppl, of the 2023 Conference on Empirical Methods in\nPreslavNakov,andIrynaGurevych.2024a. Asur- NaturalLanguageProcessing,pages292–305,Sin-\nveyofconfidenceestimationandcalibrationinlarge gapore.AssociationforComputationalLinguistics.\nlanguagemodels. InProceedingsofthe2024Con-\nferenceoftheNorthAmericanChapteroftheAsso- Tsung-YiLin,MichaelMaire,SergeBelongie,James\nciationforComputationalLinguistics: HumanLan- Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nguageTechnologies(Volume1: LongPapers),pages and C Lawrence Zitnick. 2014. Microsoft coco:\n6577–6595, Mexico City, Mexico. Association for Common objects in context. In Computer Vision–\nComputationalLinguistics. ECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nJiahuiGeng,YovaKementchedjhieva,PreslavNakov, PartV13,pages740–755.Springer.\nandIrynaGurevych.2024b. Multimodallargelan-\nguage models to support real-world fact-checking. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\narXivpreprintarXiv:2403.03627. Lee.2024. Improvedbaselineswithvisualinstruc-\ntiontuning. InProceedingsoftheIEEE/CVFCon-\nAnisha Gunjal, Jihan Yin, and Erhan Bas. 2024. De- ferenceonComputerVisionandPatternRecognition\ntectingandpreventinghallucinationsinlargevision (CVPR),pages26296–26306.\nlanguagemodels. InThirty-EighthAAAIConference\non Artificial Intelligence, AAAI 2024, Thirty-Sixth HaotianLiu,ChunyuanLi,QingyangWu,andYongJae\nConferenceonInnovativeApplicationsofArtificial Lee.2023. Visualinstructiontuning. InAdvancesin\nIntelligence,IAAI2024,FourteenthSymposiumon NeuralInformationProcessingSystems36: Annual\nConferenceonNeuralInformationProcessingSys- A Reference-freeHallucinationDetection\ntems2023, NeurIPS2023, NewOrleans, LA,USA, Methods\nDecember10-16,2023.\nWe collect uncertainty-based and consistency-\nPotsaweeManakul,AdianLiusie,andMarkGales.2023. basedmethodsproposedin(Manakuletal.,2023)\nSelfCheckGPT:Zero-resourceblack-boxhallucina- totesttheeffectivenessonLVLMs.\ntiondetectionforgenerativelargelanguagemodels.\nIn Proceedings of the 2023 Conference on Empiri-\nA.1 Uncertainty-basedmethods\ncalMethodsinNaturalLanguageProcessing,pages\n9004–9017, Singapore. Association for Computa- Toaggregatetheuncertaintyinformationobtained\ntionalLinguistics.\natthetokenlevel,weemployfourmetricstoaggre-\ngatetoken-leveluncertaintyintosentencelevel. In\nArtem Vazhentsev, Gleb Kuzmin, Akim Tsvigun,\nparticular, a sentence-level uncertainty score can\nAlexanderPanchenko,MaximPanov,MikhailBurt-\nbe obtained by taking either the maximum or av-\nsev, and Artem Shelmanov. 2023. Hybrid uncer-\ntaintyquantificationforselectivetextclassification erage of the negative loglikelihood −logp ij in a\ninambiguoustasks. InProceedingsofthe61stAn- sentence:\nnualMeetingoftheAssociationforComputational\nLinguistics(Volume1: LongPapers),pages11659–\nMaxProb(i) = max(−logp ), (1)\n11681,Toronto,Canada.AssociationforComputa- j ij\ntionalLinguistics.\n1\n(cid:88)Ji\nAvgProb(i) = − logp , (2)\nij\nNing Xie, Farley Lai, Derek Doran, and Asim Ka- J i\nj=1\ndav. 2019. Visual entailment: A novel task for\nfine-grained image understanding. ArXiv preprint,\nwherep istheprobabilityofatokenatapositionj\nabs/1901.06706. ij\ninthesentenceiandJ isthetotalnumberoftokens\ni\nintheconsideredsentence. Additionally,onecan\nJiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley\nMalin,andSricharanKumar.2023. SAC3: Reliable also replace the negative loglikelihood −logp ij\nhallucinationdetectioninblack-boxlanguagemodels withtheentropyH :\nij\nviasemantic-awarecross-checkconsistency. InFind-\ningsoftheAssociationforComputationalLinguis-\nMaxEnt(i) = maxH , (3)\nij\ntics: EMNLP2023,pages15445–15458,Singapore. j\nAssociationforComputationalLinguistics.\n1\n(cid:88)Ji\nAvgEnt(i) = H , (4)\nij\nJ\nYiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun i\nj=1\nZhang,ZhunDeng,ChelseaFinn,MohitBansal,and\nHuaxiuYao.2024. Analyzingandmitigatingobject whereH istheentropyofthetokendistribution\nij\nhallucination in large vision-language models. In\nforthej-thtokeninthesentencei.\nThe Twelfth International Conference on Learning\nRepresentations.\nA.2 Consistency-basedMethods\nDeruiZhu,DingfanChen,QingLi,ZongxiongChen, BERTScore denoted as S , finds the aver-\nBERT\nLei Ma, Jens Grossklags, and Mario Fritz. 2024a. age BERTScore of the considered sentence with\nPoLLMgraph: Unravelinghallucinationsinlargelan-\nthe most similar sentence from each drawn sam-\nguagemodelsviastatetransitiondynamics. InFind-\nple. LetB(.,.)denotetheBERTScorebetweentwo\ningsoftheAssociationforComputationalLinguis-\ntics: NAACL2024,pages4737–4751,MexicoCity, sentences. S BERT iscomputedbyB(.,.)as:\nMexico.AssociationforComputationalLinguistics.\nN\n1 (cid:88)\nDeyaoZhu, JunChen, XiaoqianShen, XiangLi, and S BERT(i) = 1−\nN\nm kaxB(r i,sn k), (5)\nMohamedElhoseiny.2024b. MiniGPT-4: Enhanc- n=1\ning vision-language understanding with advanced\nlargelanguagemodels. InTheTwelfthInternational where r i represents the i-th sentence in main re-\nConferenceonLearningRepresentations. sponse R, sn represents the k-th sentence in the\nk\nn-thsampleSn andN isthetotalnumberofsam-\nXueyanZou,JianweiYang,HaoZhang,FengLi,Lin- pledsentences.\njieLi,JianfengWang,LijuanWang,JianfengGao,\nQuestionAnswering(QA).Themultiple-choice\nandYongJaeLee.2024. Segmenteverythingevery-\nquestionansweringgeneration(MQAG)evaluates\nwhereallatonce. AdvancesinNeuralInformation\nProcessingSystems,36. consistencybycreatingmultiple-choicequestions\nfromtheprimarygeneratedresponseandanswer- sampledpassageSn concatenatedtothesentence\ning them using other sampled responses as refer- tobeassessedr intheresponseR. Onlythelogits\ni\nence. MQAGconsistsoftwostages: questiongen- associatedwiththe‘entailment’and‘contradiction’\nerationGandquestionansweringA. Forthesen- classesareconsidered,\ntence r i in the response R, we draw questions q exp(cid:0) zi,n(cid:1)\nandoptionso: P(contradict | r ,Sn) = e ,\ni exp(cid:0) zi,n(cid:1) +exp(cid:0) zi,n(cid:1)\ne c\nq,o ∼ P (q,o | r ,R). (6)\nG i\nwherezi,n = z (r ,Sn)andzi,n = z (r ,Sn)are\ne e i c c i\nTheansweringstageAselectstheanswers: the logits of the ‘entailment’ and ‘contradiction’\nclasses. NLI score for sentence r on samples\ni\na\nR\n= argmax[P A(o\nk\n| q,R,o)], (7) {S1,...,SN}isthendefinedas,\nk\na Sn = arg kmax[P A(o k | q,Sn,o)]. (8) S NLI(i) = 1 (cid:88)N P(contradict | r i,Sn). (12)\nN\nn=1\nWecomparewhethera isequaltoa foreach\nR Sn\nsample in {S1,...,SN}, yielding N matches B DatasetandImplementation\nm\nandN not-matches. Asimpleinconsistencyscore\nn B.1 Dataset\nfor the i-th sentence and question q based on the\nPOPE(Lietal.,2023)formulatestheevaluationof\nmatch/not-match counts is defined: S (i,q) =\nQA\nobjecthallucinationasabinaryclassificationtask\nNn . Manakuletal.(2023)modifytheincon-\nNn+Nm thatpromptsLVLMstooutput“Yes”or“No”,e,g.,\nsistency score S (i,q) by incorporating the an-\nQA\n“Isthereachairinthefigure?” POPEadoptsthree\nswerabilityofgeneratedquestions. Finally,S (i)\nQA\nsampling settings to construct negative samples:\nistheaverageofinconsistencyscoresacrossq,\nrandom, popular, and adversarial. The random\nsetting selects non-present objects at random. In\nS (i) = E [S (i,q)]. (9)\nQA q QA\ncontrast,thepopularsettingchoosesfromalistof\nUnigram. The concept behind Unigram is to de- frequently occurring but absent objects, and the\nvelopanewmodelthatapproximatestheLVLMs adversarial method selects based on common co-\nbysamples{S1,...,SN}andgettheLVLM’sto- occurrence in contexts despite the absence in the\nkenprobabilitiesusingthismodel. AsN increases, targetimage.\nthenewmodelgetsclosertoLVLMs. Duetotime GQA(HudsonandManning,2019)isproposedto\nandcostconstraints,wejusttrainasimplen-gram detecthallucinations,includingexistence,attribute,\nmodelusingthesamples{S1,...,SN}aswellas andrelation. ItcontainsnotonlyYes-or-Noprob-\nthe main response R. We then compare the av- lemsbutalsoOpen-endedproblems. Werandomly\nerageandmaximumofthenegativeprobabilities 9050 some binary problems for 397 images used\nofthesentenceinresponseRusingthefollowing inourexperiments.\nequations: M-HalDetect (Gunjal et al., 2024) consists of\n16k fine-grained annotations on visual question-\n1 (cid:88)Ji answering examples, including object hallucina-\nSAvg (i) = − logpˆ , (10)\nn-gram J ij tion, descriptions, and relationships. We utilize\ni\nj=1 about3000questionson764figuresforourOpen-\nS nM -ga rx am(i) = max(−logpˆ ij), (11) endedtasks,averaging4sentencesperresponse.\nj\nIHAD severs as the evaluation of description for\nwherepˆ istheprobabilityofatokenatpositionj Open-ended tasks. It is a sentence-level dataset\nij\nofasentencei. createdbypromptingLLaVA-V1.5-7btogenerate\nNatural Language Inference (NLI) determines concise image descriptions for 500 images from\nwhetherahypothesisfollowsapremise,classified MSCOCO(Linetal.,2014). Onaverage,eachre-\nintoeitherentailment/neutral/contradiction. Inthis sponsecontains5sentences. ThecreationofIHAD\nwork,weuseDeBERTa-v3-large(Heetal.,2023) aims to investigate if the data source impacts the\nfine-tunedtoMNLIastheNLImodel. Theinput internalinformationofLVLMsandfurtherimpacts\nfor NLI classifiers is typically the premise con- hallucinationdetectionmethods,whichrelyonthis\ncatenatedtothehypothesis, whichforNLIisthe internalinformation.\nMini LLaVA LLaVA LLaVA nal responses to “Yes” for positive answers (e.g.,\nMethod\nGPT- v1.5- v1.6- v1.6- “Yes,thereisacutecat.”) and“No”forallothers.\nvicuna- mistral-\nv4 7b Tobalancepositiveandnegativesamples,wecre-\n7b 7b\nate two entries for each original datum, one with\nBaseline 48.73 48.73 48.73 48.73\nUncertainty-basedMethods a “Yes” response and the other with a “No” re-\nAvgProb 55.87 58.53 54.27 81.70 sponse. Thus,theoriginaldatasetsAdv,Ran,and\nAvgEnt 51.74 50.87 48.82 49.41 Pop, each containing 3000 questions for 500 im-\nMaxProb 32.28 36.48 32.11 41.78\nages, are expanded to 6000 questions per set for\nMaxEnt 49.03 50.60 48.35 49.25\nthesameimages. Similarly, GQAisincreasedto\nBaseline 48.73 48.73 48.73 48.73\nUncertainty-basedMethods 9050questionsfor397figures. RegardingOpen-\nAvgProb 54.57 51.95 47.35 85.10 endedtasks,eachresponsecontainsanaverageof\nAvgEnt 50.49 51.13 53.51 50.80 4sentencesinM-HalDetect,while5sentencesin\nMaxProb 39.80 33.77 30.68 41.30\nIHAD.\nMaxEnt 48.37 50.79 51.43 50.75\nIntheconsistency-basedmethod,wesetthetem-\nBaseline 48.73 48.73 48.73 48.73\nperature to 0.0 to obtain the main resource and\nUncertainty-basedMethods\nAvgProb 55.87 57.65 51.52 81.74 use standard beam search decoding. In addition,\nAvgEnt 51.74 50.59 51.83 51.92 wesetthetemperatureto1.0forthestochastically\nMaxProb 32.28 35.90 31.41 40.97 generatedsamplesandgeneratedN = 10samples.\nMaxEnt 49.03 50.30 50.95 51.73\nTheclassifierusedintheSUQmethodemploysa\nBaseline 48.47 48.47 48.47 48.47\nfeedforwardneuralnetworkfeaturingthreehidden\nUncertainty-basedMethods\nlayers with decreasing numbers of hidden units\nAvgProb 55.72 56.56 57.93 65.06\nAvgEnt 48.96 50.12 49.55 48.38 (256,128,64),allutilizingReLUactivations. The\nMaxProb 33.20 39.64 37.27 46.25 finallayerisasigmoidoutput. WeusetheAdam\nMaxEnt 48.16 49.98 49.15 48.39\noptimizer. Wedonotfine-tuneanyofthesehyper-\nparameters for this task. The classifier is trained\nTable4: Yes-or-Notaskswithperiod. Theterms top\nfor20epochs. Weuseabouttwo-thirdsofadataset\ndenote the highest AUC-PR values for uncertainty-\nbasedmethods. totrainaclassifierbasedonaspecificmodeland\nthentestitsaccuracyontheotherthirdonthesame\nmodel. The trained well classifier is required to\nB.2 ExperimentalSetup\ndetermine which sentences the LVLM “believes”\nIn our experiments, we merge questions and re- aretrueandwhichit“believes”arefalse.\nsponses from humans or other models to create\nhand-crafted input. This input is then processed C ExperimentalResults\nby the evaluation model, from which we extract\nC.1 ImpactofUnimportantTokensfor\ninternalinformation,includingprobability,entropy,\nUncertainty-basedMethods\nand embedding states. The evaluation model, ca-\npable of recognizing hallucinations (Duan et al., Itisobservedthattheperformanceofuncertainty\n2024),canassignreasonableinternalinformation methods on the Open-ended tasks is not as good\nto responses not generated by itself. As shown asontheYes-or-Notasks. Toexplorethereasons,\ninFigure2, whengeneratingthe“cat”token, the weappendaperiod,resultingin“Yes.” or“No.” in\nevaluationmodelassignsprobabilitiestoalltokens theYes-or-Notask. Inthiscase,probabilitypand\nandselectsthehigherprobabilitytokenforoutput. entropyHoftwotokensneedtobeconsidered. Ta-\nSpecifically, the probability assigned to “cat” in ble4showsthedetailedperformanceforYes-or-No\nself-generated input is 0.9, while in hand-crafted taskswithperiodusingdifferentmethods. Specifi-\ninput,“dog”receivesaprobabilityof0.1. POPE, cally,forLLaVA-v1.6-mistral,theAvgProbscores\nGQA,andM-Halarehand-crafteddataforallfour declineto81.70from92.04forAdv,to85.10from\nevaluation models used in this work. IHAD’s re- 96.99 for Ran, to 81.74 from 95.06 for Pop, and\nsponse,however,isgeneratedbyLLaVA-v1.5-7b, to65.06from79.15forGQAdatasets,indicating\nmakingIHADself-generateddataspecificallyfor the performance of AvgProb is severely affected\nLLaVA-v1.5-7b. Fortheotherthreemodels,IHAD by unimportant tokens. The other three metrics,\nisconsideredhand-crafteddata. AvgEnt, MaxProb, and MaxEnt, nearly equal to\nIn the Yes-or-No tasks, we simplify the origi- thebaselineandplaynoroleinhallucinationdetec-\nvdA\nnaR\npoP\nAQG\ntion.\nC.2 OptimalHiddenLayersforEffective\nSUQMethod\nToexplorewhichhiddenlayerinformationshould\nbeused,weperform3DTSNEvisualizationsofthe\nembeddingstatesofLLaVAatthefifthandlastlay-\nersforpopulartasks,asshowninFigure6. These\nvisualizationsshowthatatthelastlayer,thereare\ndistinctseparationsbetweentherepresentationsof\ndifferentlabels. Conversely,atthefifthlayer,rep-\nresentationsofdifferentlabelsarerelativelyclose\nandalmostblendtogether,indicatingthattheshal-\nlow layers primarily preserve language-agnostic\nfeatures. Therefore,weusethelasthiddenlayerto\nexploretheSUQmethodinourexperiments.\n(a)FifthLayer (b)LastLayer\nFigure6: 3DTSNEplotoflanguageembeddingscom-\nputedfromthefifthandlastlayersofLLaVA-v1.6-7b\nonpopulartask.\nC.3 Thecomparisonofclearandblurred\nimages.\nThecomparisonofclearandblurredimagesused\nin4.3isillustratedinFigure7.\n(a)ClearImage (b)BlurredImage\nFigure7: Thecomparisonofclearandblurredimages.",
    "pdf_filename": "Reference-free_Hallucination_Detection_for_Large_Vision-Language_Models.pdf"
}