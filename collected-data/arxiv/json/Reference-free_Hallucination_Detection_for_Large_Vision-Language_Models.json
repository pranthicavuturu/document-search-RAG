{
    "title": "Reference-free Hallucination Detection for Large Vision-Language Models",
    "abstract": "Large vision-language models (LVLMs) have made significant progress in recent years. While LVLMs exhibit excellent ability in lan- guage understanding, question answering, and conversations of visual inputs, they are prone to producing hallucinations. While several meth- ods are proposed to evaluate the hallucinations in LVLMs, most are reference-based and de- pend on external tools, which complicates their practical application. To assess the viability of alternative methods, it is critical to understand whether the reference-free approaches, which do not rely on any external tools, can efficiently detect hallucinations. Therefore, we initiate an exploratory study to demonstrate the effective- ness of different reference-free solutions in de- tecting hallucinations in LVLMs. In particular, we conduct an extensive study on three kinds of techniques: uncertainty-based, consistency- based, and supervised uncertainty quantifica- tion methods on four representative LVLMs across two different tasks. The empirical re- sults show that the reference-free approaches are capable of effectively detecting non-factual responses in LVLMs, with the supervised uncer- tainty quantification method outperforming the others, achieving the best performance across different settings. 1 Introduction Large vision-language models (LVLMs) like LLaVA (Liu et al., 2024), MiniGPT-4 (Zhu et al., 2024b) have demonstrated remarkable capabilities in understanding and generating complex visual and textual content. However, an emerging con- cern with these models is their tendency towards hallucination (Zhou et al., 2024; Zhu et al., 2024a; Geng et al., 2024b). For instance, a model might describe an object or event in the generated text that *Equal contribution. †Corresponding author. Figure 1: Reference-free Hallucination Detection Meth- ods used in this work. is not present in the input image. This phenomenon poses challenges for the reliability of LVLMs, high- lighting intrinsic limitations of current models in maintaining coherence between visual inputs and textual descriptions (Huang et al., 2023a). Most existing methods of assessing hallucina- tions rely on external models. Li et al. (2023) pro- pose POPE, which requires automated segmenta- tion tools like SEEM (Zou et al., 2024) to iden- tify present and absent objects in images. Faith- score (Jing et al., 2023) utilizes a visual entailment model (Xie et al., 2019) to validate the factuality, focusing on object attributes and their relationships. Nonetheless, these approaches increase the compu- tational cost and are restricted by the capabilities of these models. arXiv:2408.05767v2  [cs.CL]  19 Nov 2024",
    "body": "Reference-free Hallucination Detection for\nLarge Vision-Language Models\nQing Li1∗, Jiahui Geng1∗, Chenyang Lyu1, Derui Zhu2, Maxim Panov1, Fakhri Karray1†\n1 Mohamed bin Zayed University of Artificial Intelligence\n{qing.li, jiahui.geng, chenyang.lyu, maxim.panov, fakhri.karray}@mbzuai.ac.ae\n2 Technical University of Munich\nderui.zhu@tum.de\nAbstract\nLarge vision-language models (LVLMs) have\nmade significant progress in recent years.\nWhile LVLMs exhibit excellent ability in lan-\nguage understanding, question answering, and\nconversations of visual inputs, they are prone to\nproducing hallucinations. While several meth-\nods are proposed to evaluate the hallucinations\nin LVLMs, most are reference-based and de-\npend on external tools, which complicates their\npractical application. To assess the viability of\nalternative methods, it is critical to understand\nwhether the reference-free approaches, which\ndo not rely on any external tools, can efficiently\ndetect hallucinations. Therefore, we initiate an\nexploratory study to demonstrate the effective-\nness of different reference-free solutions in de-\ntecting hallucinations in LVLMs. In particular,\nwe conduct an extensive study on three kinds\nof techniques: uncertainty-based, consistency-\nbased, and supervised uncertainty quantifica-\ntion methods on four representative LVLMs\nacross two different tasks. The empirical re-\nsults show that the reference-free approaches\nare capable of effectively detecting non-factual\nresponses in LVLMs, with the supervised uncer-\ntainty quantification method outperforming the\nothers, achieving the best performance across\ndifferent settings.\n1\nIntroduction\nLarge vision-language models (LVLMs) like\nLLaVA (Liu et al., 2024), MiniGPT-4 (Zhu et al.,\n2024b) have demonstrated remarkable capabilities\nin understanding and generating complex visual\nand textual content. However, an emerging con-\ncern with these models is their tendency towards\nhallucination (Zhou et al., 2024; Zhu et al., 2024a;\nGeng et al., 2024b). For instance, a model might\ndescribe an object or event in the generated text that\n*Equal contribution.\n†Corresponding author.\nFigure 1: Reference-free Hallucination Detection Meth-\nods used in this work.\nis not present in the input image. This phenomenon\nposes challenges for the reliability of LVLMs, high-\nlighting intrinsic limitations of current models in\nmaintaining coherence between visual inputs and\ntextual descriptions (Huang et al., 2023a).\nMost existing methods of assessing hallucina-\ntions rely on external models. Li et al. (2023) pro-\npose POPE, which requires automated segmenta-\ntion tools like SEEM (Zou et al., 2024) to iden-\ntify present and absent objects in images. Faith-\nscore (Jing et al., 2023) utilizes a visual entailment\nmodel (Xie et al., 2019) to validate the factuality,\nfocusing on object attributes and their relationships.\nNonetheless, these approaches increase the compu-\ntational cost and are restricted by the capabilities\nof these models.\narXiv:2408.05767v2  [cs.CL]  19 Nov 2024\n\nFigure 2: Two data sources: self-generated and hand-\ncrafted data. The test model assigns a probability of 0.9\nto “cat” in self-generated input, while in hand-crafted\ninput, “dog” receives a probability of 0.1.\nReference-free methods are a promising ap-\nproach to address this issue, primarily utilizing\nthe model’s own knowledge. They have attracted\ngreat research interest in Large Language Mod-\nels (LLMs), with typical methods including the\nuncertainty-based methods (Hu et al., 2023; Geng\net al., 2024a; Huang et al., 2023b; Vazhentsev et al.,\n2023; Fadeeva et al., 2023), consistency-based\nmethods (Manakul et al., 2023) and supervised\nuncertainty quantification (SUQ) approach (Chen\net al., 2023; Azaria and Mitchell, 2023). However,\ntheir potential in the context of LVLMs is unknown.\nThey have not yet been systematically studied, pos-\nsibly due to the complexity of multimodal data. We\nare committed to bridging this gap.\nIn this work, we collect and implement a bat-\ntery of state-of-the-art methods, including four met-\nrics of uncertainty-based methods, four variants of\nconsistency-based methods, and the supervised un-\ncertainty quantification method. The experiments\nare conducted on five LVLMs with different types,\nversions, and sizes on Yes-and-No and Open-ended\ntasks. The main contributions of this paper are\nsummarized as follows:\n1. We comprehensively measure the perfor-\nmance of different reference-free approaches\nin detecting hallucination.\n2. We demonstrate that the supervised uncer-\ntainty quantification (SUQ) method performs\nbest across various settings.\n3. We contribute the Image-Hallucination Anno-\ntation Dataset (IHAD), a manually annotated,\nsentence-level dataset created by prompting\nLLaVA-v1.5-7b.\n2\nReference-free Hallucination Detection\nMethods\nUncertainty-based methods have the hypothesis\nthat when LVLMs are uncertain about generated\ninformation, generated tokens often have higher\nMethod\nMini-\nGPT-\n4v\nLLaVA-\nv1.5-\n7b\nLLaVA-\nv1.6-\nvicuna-\n7b\nLLaVA-\nv1.6-\nmistral-\n7b\nAdv\nRandom\n48.73\n48.73\n48.73\n48.73\nUncertainty-based Methods\nAvgProb\n77.43\n86.82\n91.80\n92.04\nAvgEnt\n48.89\n50.05\n48.90\n48.92\nSupervised Uncertainty Quantification Method\nSUQ\n87.23\n92.35\n93.32\n93.50\nRan\nBaseline\n48.73\n48.73\n48.73\n48.73\nUncertainty-based Methods\nAvgProb\n86.62\n95.89\n96.23\n96.99\nAvgEnt\n48.76\n50.25\n49.91\n50.41\nSupervised Uncertainty Quantification Method\nSUQ\n94.44\n97.86\n98.68\n98.33\nPop\nBaseline\n48.73\n48.73\n48.73\n48.73\nUncertainty-based Methods\nAvgProb\n82.11\n92.73\n93.57\n95.06\nAvgEnt\n48.50\n49.76\n50.63\n51.39\nSupervised Uncertainty Quantification Method\nSUQ\n91.02\n97.85\n98.11\n98.50\nGQA\nBaseline\n48.47\n48.47\n48.47\n48.47\nUncertainty-based Methods\nAvgProb\n66.87\n78.45\n80.31\n79.15\nAvgEnt\n48.75\n49.88\n48.95\n49.33\nSupervised Uncertainty Quantification Method\nSUQ\n69.14\n82.84\n84.57\n83.25\nTable 1: AUC-PR for Yes-or-No tasks.\nuncertainty, shown in Figure 1(b). Uncertainty-\nbased methods are extensively studied for hal-\nlucination detection in LLMs (Hu et al., 2023;\nGeng et al., 2024a; Huang et al., 2023b; Fadeeva\net al., 2024), and are also used as benchmark tech-\nniques (Zhu et al., 2024a; Manakul et al., 2023).\nWe adopt the four proposed metrics: AvgProb,\nAvgEnt, MaxProb, MaxEnt to aggregate token-\nlevel uncertainty to measure sentence-level and\npassage-level uncertainty. Appendix A.1 provides\ndetailed descriptions of these metrics.\nConsistency-based methods mainly include self-\nconsistency, cross-question consistency, and cross-\nmodel consistency (Zhang et al., 2023). In this\nwork, we focus on four variants – BERTScore,\nQuestion Answering (QA), Unigram, Natural Lan-\nguage Inference (NLI) – of the self-consistency\nmethod proposed in (Manakul et al., 2023) to de-\ntect non-factual information for LVLMs, as demon-\nstrated in Figure 1(c). The hallucination score for\nthe i-th sentence, denoted by S(i), ranges from 0.0\nto 1.0, where a score close to 0.0 indicates the sen-\ntence is accurate, and a score near 1.0 suggests the\nsentence is hallucinated. More detailed information\ncan be found in Appendix A.2.\n\nMethod\nMini-\nGPT-\n4v\nLLaVA-\nv1.5-\n7b\nLLaVA-\nv1.6-\nvicuna-\n7b\nLLaVA-\nv1.6-\nmistral-\n7b\nLLaVA-\nv1.6-\nvicuna-\n13b\nM-Hal\nBaseline\n29.29\n29.29\n29.29\n29.29\n29.29\nUncertainty-based Methods\nAvgProb\n30.52\n37.22\n38.61\n37.54\n43.30\nAvgEnt\n29.92\n32.18\n39.13\n37.22\n40.11\nMaxProb\n33.45\n32.05\n34.86\n34.73\n40.03\nMaxEnt\n29.96\n29.71\n39.71\n39.62\n33.14\nConsistency-based Methods\nQA\n35.08\n40.27\n41.44\n39.48\n46.38\nBERTScore 37.21\n39.13\n51.16\n51.64\n54.12\nUnigram\n(max)\n37.63\n38.68\n39.62\n40.08\n49.23\nNLI\n42.14\n44.09\n52.39\n51.91\n57.36\nSupervised Uncertainty Quantification Method\nSUQ\n62.53\n61.23\n65.26\n64.78\n70.12\nIHAD\nBaseline\n35.71\n35.71\n35.71\n35.71\n35.71\nUncertainty-based Methods\nAvgProb\n36.16\n42.35\n50.88\n50.75\n53.42\nAvgEnt\n35.81\n37.18\n48.38\n46.86\n50.04\nMaxProb\n35.97\n41.89\n46.24\n45.96\n45.21\nMaxEnt\n35.53\n35.88\n50.85\n46.65\n50.33\nConsistency-based Methods\nQA\n41.34\n54.72\n48.15\n49.19\n58.12\nBERTScore 40.29\n42.37\n49.03\n47.87\n50.33\nUnigram\n(max)\n39.26\n39.06\n43.13\n44.10\n49.79\nNLI\n49.59\n50.80\n50.94\n58.41\n62.58\nSupervised Uncertainty Quantification Method\nSUQ\n62.73\n63.85\n65.74\n64.17\n69.19\nTable 2: AUC-PR for sentence-level Open-ended tasks.\nSupervised uncertainty quantification (SUQ). To\nanalyze and understand these internal states, SUQ\nmethod (Chen et al., 2023; Azaria and Mitchell,\n2023) shown in Figure 1(a) train a classifier, re-\nferred to as a probe, on a dataset containing labeled\nexamples. This classifier outputs the likelihood of\na statement being truthful by analyzing the hidden\nlayer activations of the LVLM during the reading\nor generation of states.\n3\nStudy Design\nDataset. Our experiments are conducted on public\ndatasets: POPE (Li et al., 2023), GQA (Hudson and\nManning, 2019), and M-HalDetect (Gunjal et al.,\n2024). POPE contains three subsets: random, pop-\nular, and adversarial. For simplicity, we refer to\nthem as Ran, Pop, and Adv, and we also abbreviate\nM-HalDetect as M-Hal. In addition, we create a\nmanually annotated, sentence-level dataset, IHAD,\nby using LLaVA-v1.5-7b to generate concise image\ndescriptions for 500 images from MSCOCO (Lin\net al., 2014) using prompt “Provide a concise de-\nMethod\nMini-\nGPT-\n4v\nLLaVA-\nv1.5-\n7b\nLLaVA-\nv1.6-\nvicuna-\n7b\nLLaVA-\nv1.6-\nmistral-\n7b\nLLaVA-\nv1.6-\nvicuna-\n13b\nM-Hal\nBaseline\n64.05\n64.05\n64.05\n64.05\n64.05\nUncertainty-based Methods\nAvgProb\n70.28\n72.78\n72.89\n73.32\n81.23\nAvgEnt\n64.89\n64.44\n70.83\n70.05\n72.97\nMaxProb\n66.68\n68.97\n72.67\n70.37\n73.56\nMaxEnt\n68.75\n64.99\n77.17\n78.62\n80.23\nSupervised Uncertainty Quantification Method\nSUQ\n84.92\n89.41\n85.91\n86.54\n91.22\nIHAD\nBaseline\n83.00\n83.00\n83.00\n83.00\n83.00\nUncertainty-based Methods\nAvgProb\n84.29\n87.50\n87.38\n87.63\n90.23\nAvgEnt\n83.06\n85.64\n88.41\n87.45\n89.41\nMaxProb\n84.17\n89.40\n89.89\n89.39\n91.20\nMaxEnt\n83.03\n87.18\n88.21\n88.44\n92.10\nSupervised Uncertainty Quantification Method\nSUQ\n85.34\n94.08\n93.09\n94.07\n96.03\nTable 3: AUC-PR for passage-level Open-ended tasks.\nscription of the given image”. Ran, Pop, Adv, and\na subset of GQA used in this work evaluate object\nhallucination as a binary classification task, prompt-\ning LVLMs to output “Yes” or “No”, categorizing\nthem as Yes-or-No tasks. In contrast, M-Hal and\nIHAD involve long text answers based on open-\nended questions, making them Open-ended tasks.\nMore detailed dataset information is shown in Ap-\npendix B.1.\nModels.\nWe evaluate five representative cate-\ngories of LVLMs: MiniGPT-4v (Zhu et al., 2024b),\nLLaVA-v1.5-7b (Liu et al., 2023), LLaVA-v1.6-\nvicuna-7b, LLaVA-v1.6-mistral-7b (Liu et al.,\n2024) and LLaVA-v1.6-vicuna-13b. These mod-\nels, featuring different architectures and strong\nperformance, are used as benchmarks in various\nworks (Huang et al., 2023a; Zhou et al., 2024). We\nonly test models of a 7-billion and 13-billion size\ndue to the limited computational resources.\nExperimental Setup. We combine questions and\nresponses to create inputs for test models. If the\nresponse is from this test model, it is considered\nself-generated input; otherwise, it is regarded as\nhand-crafted input for this model. Inputs are then\nprocessed by test models, from which we extract\ninternal information, including probability, entropy,\nand embedding states. In this work, all datasets\nare hand-crafted except for IHAD, which is self-\ngenerated data only for LLaVA-v1.5-7b. Figure 2\nshows test model assigns different internal informa-\ntion to two different inputs. Detailed experimental\nsetups are presented in Appendix B.2.\n\n(a) AvgProb\n(b) SUQ\n(c) NLI\nFigure 3: Comparison using images with different clarity.\n4\nExperimental Results and Analysis\nThe comprehensive results on Open-ended tasks\nare presented in Tables 1, 2 and 3. The numbers\nin the pink and yellow background, respectively,\nrepresent the best results for the uncertainty-based\nmethods and consistency-based methods, while the\nnumbers in bold indicate the overall best results.\nWe utilize AUC-PR (Davis and Goadrich, 2006),\nwhich stands for the area under the precision-recall\ncurve, to objectively evaluate the effectiveness of\neach model. The higher the value of AUC-PR, the\nstronger the ability of this method for hallucination\ndetection.\n4.1\nEffectiveness of Various Methods\nHallucination Detection on Yes-or-No Tasks. We\nonly compare the uncertainty-based and SUQ meth-\nods on Yes-or-No tasks because the model’s an-\nswers exclusively contain “Yes” or “No”. The\nrandom baseline is calculated as the ratio of non-\nfactual examples to the total number of exam-\nples. Our results in Table 1 demonstrate that SUQ\nmethod outperforms the uncertainty-based meth-\nods, exceeding AvgProb by about 4.5%. AvgProb\nachieves AUC-PR values nearly double those of the\nrandom baseline across all datasets and models,\nindicating it is an efficient method since it is an\nunsupervised method. In addition, the probability\nProb overall performs better than the entropy Ent\nin Yes-or-No tasks.\nSentence-level Hallucination Detection. Based\non the data in Table 2, we find that in most in-\nstances, the consistency-based approaches sur-\npasses the uncertainty-based metrics, yet they re-\nmain significantly inferior to the SUQ method by\napproximately 15%. Among consistency-based\nmethods, NLI outperforms best while Unigram\nshows the least effectiveness in most cases. Specif-\nically, on the M-Hal dataset, the NLI performance\nof LLaVA-v1.6-7b exceeds the highest perform-\ning uncertainty metric, MaxEnt, by approximately\n12%. BERTScore and QA outperform the uncertain\nestimation in most setups.\nPassage-level Hallucination Detection. We cal-\nculate the metrics of uncertainty-based methods\nacross all tokens in a passage and take the em-\nbedding information of the passage’s last token\nfor SUQ. Consistency-based methods are not used\nto detect non-factual information in passages as\nthe heavy computation. Our results in Table 3\nshow that the SUQ method is still better than the\nuncertainty-based methods, including probability\nand entropy methods. Besides, uncertainty-based\nmethods are weakly better than the random base-\nline at passage-level.\n4.2\nRobustness and Limitations of SUQ\nTo investigate the robustness of SUQ methods, we\nemploy two classifiers to assess the impact on each\ndataset. Each dataset is split into a training set and a\ntest set in a 3:1 ratio. One classifier is trained on the\nPop training set, while the other is trained on the\ntraining set of each dataset individually. Figure 4\nillustrates that the classifier trained on Pop exhibits\ncomparable performance to classifiers trained on\nseparated datasets, indicating the robustness of the\nSUQ method.\nAlthough our experiments confirmed the effec-\ntiveness of the SUQ method, it was found to be\nineffective in detecting subtle, manually crafted hal-\nlucinations. For example, the difference between\na correct statement, “There is a red apple and a\ncute kitten.” and an incorrect one, “There is a red\norange and a cute kitten.” has a negligible effect on\nthe model’s hidden state at the final position, pos-\ning a challenge for SUQ methods to differentiate\nbetween them.\n\nFigure 4: Comparison of using different classifiers.\n4.3\nImpact of Image Clarity\nTo explore this question, we employ the Gaussian\nblur technique, which smooths the image by assign-\ning a weighted average to the surrounding pixels of\neach pixel, resulting in an obscure picture. We set\nradius = 10 in our experiments. Figure 3 shows the\neffectiveness of AvgProb, SUQ, and NLI in blurred\nimages are significantly reduced compared to clear\nimages. The phenomenon suggests that enhancing\nimage clarity is a crucial strategy for boosting the\neffectiveness of hallucination detection methods.\n4.4\nImpact of Data Source\nReference-free methods, particularly uncertain-\nbased and SUQ methods, rely on the internal infor-\nmation of models. A natural question is whether\nthese methods are affected by data sources. M-Hal\nand IHAD are hand-crafted data and self-generated\ndata, respectively, for LLaVA-v1.5-7b. Therefore,\nwe compare various detection methods on M-Hal\nand IHAD based on LLaVA-v1.5-7b. Specifically,\nwe calculate ∆AUC-PR\n∆AUC-PRi,j = AUC-PRi,j −AUC-PRi,baseline,\nwhere i refers to M-Hal and IHAD, j denotes dif-\nferent approaches.\nFigure 5 shows that in uncertainty-based ap-\nproaches, some metrics outperform in M-Hal while\nothers excel in IHAD, a trend also observed in\nconsistency-based techniques. This phenomenon\nillustrates that the reference-free hallucination de-\ntection methods are insensitive to the data source.\nOthers. We have also explored the impact of unim-\nportant tokens for uncertainty-based methods and\nthe optimal hidden layers for SUQ. They are dis-\ncussed respectively in Appendices C.1 and C.2.\n5\nConclusion\nThis paper systematically studies reference-free ap-\nproaches for detecting the hallucination of LVLMs.\nFigure 5: Comparison of using different data sources.\nOur results indicate that the SUQ method performs\nbest as a supervised method, but it relies on suf-\nficient training data, which could limit its appli-\ncability in certain scenarios. Consistency-based\nmethods outperform uncertainty-based methods;\nhowever, they require more computation and are\nmore suitable for black-box models. Uncertainty-\nbased methods are particularly well-suited for tasks\nsimilar to Yes-or-No questions.\nLimitations\nThree major limitations are identified in this work.\nFirst, we did not compare with reference-based\nmethods.\nThe performance of reference-based\nmethods depends on the performance of external\nmodels and is specific to certain tasks. In this ar-\nticle, we primarily aim to explore the potential\nof reference-free methods, which have a broader\nrange of applications. Second, we did not classify\ntypes of hallucinations; limited by resources, even\nwith fine-grained annotations, we only marked\nwhether sentences contained hallucinations with-\nout distinguishing types such as object existence\nerrors, attribute errors, text recognition errors, and\nfact-conflicting errors. This might be where SUQ\nmethods have an advantage. Third, we only stud-\nied white-box models. Recent research indicates\nthat white-box models can be proxies for black-box\nmodels to calculate their confidence regarding spe-\ncific issues. Unigram method is one such proxy\nmodel approach. In the future, we could use more\ncomplex models, closer in architecture to the tar-\nget models, as proxies to explore their potential in\nhallucination detection.\nEthics and Broader Impact\nWe sampled a portion of the data from existing\ndatasets for our experiments, which may affect the\naccuracy of some of our conclusions.\n\nReferences\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an LLM knows when it’s lying. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023, pages 967–976, Singapore. Associa-\ntion for Computational Linguistics.\nNuo Chen, Ning Wu, Shining Liang, Ming Gong, Linjun\nShou, Dongmei Zhang, and Jia Li. 2023. Beyond sur-\nface: Probing llama across scales and layers. ArXiv\npreprint, abs/2312.04333.\nJesse Davis and Mark Goadrich. 2006. The relationship\nbetween precision-recall and roc curves. In Proceed-\nings of the 23rd international conference on Machine\nlearning, pages 233–240.\nHanyu Duan, Yi Yang, and Kar Yan Tam. 2024. Do\nllms know about hallucination?\nan empirical in-\nvestigation of llm’s hidden states. ArXiv preprint,\nabs/2402.09733.\nEkaterina Fadeeva, Aleksandr Rubashevskii, Artem\nShelmanov, Sergey Petrakov, Haonan Li, Hamdy\nMubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexan-\nder Panchenko, Timothy Baldwin, Preslav Nakov,\nand Maxim Panov. 2024. Fact-checking the output\nof large language models via token-level uncertainty\nquantification. In Findings of the Association for\nComputational Linguistics: ACL 2024. Association\nfor Computational Linguistics.\nEkaterina Fadeeva, Roman Vashurin, Akim Tsvigun,\nArtem Vazhentsev, Sergey Petrakov, Kirill Fedyanin,\nDaniil Vasilev, Elizaveta Goncharova, Alexander\nPanchenko, Maxim Panov, Timothy Baldwin, and\nArtem Shelmanov. 2023. Lm-polygraph: Uncertainty\nestimation for language models. In Proceedings of\nthe 2023 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2023 - System\nDemonstrations, Singapore, December 6-10, 2023,\npages 446–461. Association for Computational Lin-\nguistics.\nJiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl,\nPreslav Nakov, and Iryna Gurevych. 2024a. A sur-\nvey of confidence estimation and calibration in large\nlanguage models. In Proceedings of the 2024 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pages\n6577–6595, Mexico City, Mexico. Association for\nComputational Linguistics.\nJiahui Geng, Yova Kementchedjhieva, Preslav Nakov,\nand Iryna Gurevych. 2024b. Multimodal large lan-\nguage models to support real-world fact-checking.\narXiv preprint arXiv:2403.03627.\nAnisha Gunjal, Jihan Yin, and Erhan Bas. 2024. De-\ntecting and preventing hallucinations in large vision\nlanguage models. In Thirty-Eighth AAAI Conference\non Artificial Intelligence, AAAI 2024, Thirty-Sixth\nConference on Innovative Applications of Artificial\nIntelligence, IAAI 2024, Fourteenth Symposium on\nEducational Advances in Artificial Intelligence, EAAI\n2014, February 20-27, 2024, Vancouver, Canada,\npages 18135–18143. AAAI Press.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.\nDebertav3: Improving deberta using electra-style\npre-training with gradient-disentangled embedding\nsharing. In The Eleventh International Conference\non Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net.\nMengting Hu, Zhen Zhang, Shiwan Zhao, Minlie\nHuang, and Bingzhe Wu. 2023. Uncertainty in natu-\nral language processing: Sources, quantification, and\napplications. ArXiv preprint, abs/2306.04459.\nQidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,\nConghui He, Jiaqi Wang, Dahua Lin, Weiming\nZhang, and Nenghai Yu. 2023a. Opera: Alleviating\nhallucination in multi-modal large language models\nvia over-trust penalty and retrospection-allocation.\nArXiv preprint, abs/2311.17911.\nYuheng Huang, Jiayang Song, Zhijie Wang, Huam-\ning Chen, and Lei Ma. 2023b. Look before you\nleap: An exploratory study of uncertainty measure-\nment for large language models.\nArXiv preprint,\nabs/2307.10236.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: A new dataset for real-world visual reason-\ning and compositional question answering. In IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pages 6700–6709. Computer Vision Founda-\ntion / IEEE.\nLiqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia,\nand Xinya Du. 2023. Faithscore: Evaluating hal-\nlucinations in large vision-language models. ArXiv\npreprint, abs/2311.01477.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao,\nand Ji-Rong Wen. 2023. Evaluating object hallucina-\ntion in large vision-language models. In Proceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 292–305, Sin-\ngapore. Association for Computational Linguistics.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2024. Improved baselines with visual instruc-\ntion tuning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 26296–26306.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. In Advances in\nNeural Information Processing Systems 36: Annual\n\nConference on Neural Information Processing Sys-\ntems 2023, NeurIPS 2023, New Orleans, LA, USA,\nDecember 10 - 16, 2023.\nPotsawee Manakul, Adian Liusie, and Mark Gales. 2023.\nSelfCheckGPT: Zero-resource black-box hallucina-\ntion detection for generative large language models.\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n9004–9017, Singapore. Association for Computa-\ntional Linguistics.\nArtem Vazhentsev, Gleb Kuzmin, Akim Tsvigun,\nAlexander Panchenko, Maxim Panov, Mikhail Burt-\nsev, and Artem Shelmanov. 2023.\nHybrid uncer-\ntainty quantification for selective text classification\nin ambiguous tasks. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 11659–\n11681, Toronto, Canada. Association for Computa-\ntional Linguistics.\nNing Xie, Farley Lai, Derek Doran, and Asim Ka-\ndav. 2019.\nVisual entailment: A novel task for\nfine-grained image understanding. ArXiv preprint,\nabs/1901.06706.\nJiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley\nMalin, and Sricharan Kumar. 2023. SAC3: Reliable\nhallucination detection in black-box language models\nvia semantic-aware cross-check consistency. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 15445–15458, Singapore.\nAssociation for Computational Linguistics.\nYiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun\nZhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and\nHuaxiu Yao. 2024. Analyzing and mitigating object\nhallucination in large vision-language models. In\nThe Twelfth International Conference on Learning\nRepresentations.\nDerui Zhu, Dingfan Chen, Qing Li, Zongxiong Chen,\nLei Ma, Jens Grossklags, and Mario Fritz. 2024a.\nPoLLMgraph: Unraveling hallucinations in large lan-\nguage models via state transition dynamics. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2024, pages 4737–4751, Mexico City,\nMexico. Association for Computational Linguistics.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2024b. MiniGPT-4: Enhanc-\ning vision-language understanding with advanced\nlarge language models. In The Twelfth International\nConference on Learning Representations.\nXueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Lin-\njie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao,\nand Yong Jae Lee. 2024. Segment everything every-\nwhere all at once. Advances in Neural Information\nProcessing Systems, 36.\nA\nReference-free Hallucination Detection\nMethods\nWe collect uncertainty-based and consistency-\nbased methods proposed in (Manakul et al., 2023)\nto test the effectiveness on LVLMs.\nA.1\nUncertainty-based methods\nTo aggregate the uncertainty information obtained\nat the token level, we employ four metrics to aggre-\ngate token-level uncertainty into sentence level. In\nparticular, a sentence-level uncertainty score can\nbe obtained by taking either the maximum or av-\nerage of the negative loglikelihood −log pij in a\nsentence:\nMaxProb(i) = max\nj (−log pij),\n(1)\nAvgProb(i) = −1\nJi\nJi\nX\nj=1\nlog pij,\n(2)\nwhere pij is the probability of a token at a position j\nin the sentence i and Ji is the total number of tokens\nin the considered sentence. Additionally, one can\nalso replace the negative loglikelihood −log pij\nwith the entropy Hij:\nMaxEnt(i) = max\nj\nHij,\n(3)\nAvgEnt(i) = 1\nJi\nJi\nX\nj=1\nHij,\n(4)\nwhere Hij is the entropy of the token distribution\nfor the j-th token in the sentence i.\nA.2\nConsistency-based Methods\nBERTScore denoted as SBERT , finds the aver-\nage BERTScore of the considered sentence with\nthe most similar sentence from each drawn sam-\nple. Let B(., .) denote the BERTScore between two\nsentences. SBERT is computed by B(., .) as:\nSBERT(i) = 1 −1\nN\nN\nX\nn=1\nmax\nk\nB(ri, sn\nk),\n(5)\nwhere ri represents the i-th sentence in main re-\nsponse R, sn\nk represents the k-th sentence in the\nn-th sample Sn and N is the total number of sam-\npled sentences.\nQuestion Answering (QA). The multiple-choice\nquestion answering generation (MQAG) evaluates\nconsistency by creating multiple-choice questions\n\nfrom the primary generated response and answer-\ning them using other sampled responses as refer-\nence. MQAG consists of two stages: question gen-\neration G and question answering A. For the sen-\ntence ri in the response R, we draw questions q\nand options o:\nq, o ∼PG(q, o | ri, R).\n(6)\nThe answering stage A selects the answers:\naR = arg max\nk\n[PA(ok | q, R, o)],\n(7)\naSn = arg max\nk\n[PA(ok | q, Sn, o)].\n(8)\nWe compare whether aR is equal to aSn for each\nsample in {S1, . . . , SN}, yielding Nm matches\nand Nn not-matches. A simple inconsistency score\nfor the i-th sentence and question q based on the\nmatch/not-match counts is defined: SQA(i, q) =\nNn\nNn+Nm . Manakul et al. (2023) modify the incon-\nsistency score SQA(i, q) by incorporating the an-\nswerability of generated questions. Finally, SQA(i)\nis the average of inconsistency scores across q,\nSQA(i) = Eq[SQA(i, q)].\n(9)\nUnigram. The concept behind Unigram is to de-\nvelop a new model that approximates the LVLMs\nby samples {S1, . . . , SN} and get the LVLM’s to-\nken probabilities using this model. As N increases,\nthe new model gets closer to LVLMs. Due to time\nand cost constraints, we just train a simple n-gram\nmodel using the samples {S1, . . . , SN} as well as\nthe main response R. We then compare the av-\nerage and maximum of the negative probabilities\nof the sentence in response R using the following\nequations:\nSAvg\nn-gram(i) = −1\nJi\nJi\nX\nj=1\nlog ˆpij,\n(10)\nSMax\nn-gram(i) = max\nj (−log ˆpij),\n(11)\nwhere ˆpij is the probability of a token at position j\nof a sentence i.\nNatural Language Inference (NLI) determines\nwhether a hypothesis follows a premise, classified\ninto either entailment/neutral/contradiction. In this\nwork, we use DeBERTa-v3-large (He et al., 2023)\nfine-tuned to MNLI as the NLI model. The input\nfor NLI classifiers is typically the premise con-\ncatenated to the hypothesis, which for NLI is the\nsampled passage Sn concatenated to the sentence\nto be assessed ri in the response R. Only the logits\nassociated with the ‘entailment’ and ‘contradiction’\nclasses are considered,\nP(contradict | ri, Sn) =\nexp\n\u0000zi,n\ne\n\u0001\nexp\n\u0000zi,n\ne\n\u0001\n+ exp\n\u0000zi,n\nc\n\u0001,\nwhere zi,n\ne\n= ze(ri, Sn) and zi,n\nc\n= zc(ri, Sn) are\nthe logits of the ‘entailment’ and ‘contradiction’\nclasses. NLI score for sentence ri on samples\n{S1, . . . , SN} is then defined as,\nSNLI(i) = 1\nN\nN\nX\nn=1\nP(contradict | ri, Sn). (12)\nB\nDataset and Implementation\nB.1\nDataset\nPOPE (Li et al., 2023) formulates the evaluation of\nobject hallucination as a binary classification task\nthat prompts LVLMs to output “Yes” or “No”, e,g.,\n“Is there a chair in the figure?” POPE adopts three\nsampling settings to construct negative samples:\nrandom, popular, and adversarial. The random\nsetting selects non-present objects at random. In\ncontrast, the popular setting chooses from a list of\nfrequently occurring but absent objects, and the\nadversarial method selects based on common co-\noccurrence in contexts despite the absence in the\ntarget image.\nGQA (Hudson and Manning, 2019) is proposed to\ndetect hallucinations, including existence, attribute,\nand relation. It contains not only Yes-or-No prob-\nlems but also Open-ended problems. We randomly\n9050 some binary problems for 397 images used\nin our experiments.\nM-HalDetect (Gunjal et al., 2024) consists of\n16k fine-grained annotations on visual question-\nanswering examples, including object hallucina-\ntion, descriptions, and relationships. We utilize\nabout 3000 questions on 764 figures for our Open-\nended tasks, averaging 4 sentences per response.\nIHAD severs as the evaluation of description for\nOpen-ended tasks. It is a sentence-level dataset\ncreated by prompting LLaVA-V1.5-7b to generate\nconcise image descriptions for 500 images from\nMSCOCO (Lin et al., 2014). On average, each re-\nsponse contains 5 sentences. The creation of IHAD\naims to investigate if the data source impacts the\ninternal information of LVLMs and further impacts\nhallucination detection methods, which rely on this\ninternal information.\n\nMethod\nMini\nLLaVA\nLLaVA\nLLaVA\nGPT-\nv4\nv1.5-\n7b\nv1.6-\nvicuna-\n7b\nv1.6-\nmistral-\n7b\nAdv\nBaseline\n48.73\n48.73\n48.73\n48.73\nUncertainty-based Methods\nAvgProb\n55.87\n58.53\n54.27\n81.70\nAvgEnt\n51.74\n50.87\n48.82\n49.41\nMaxProb\n32.28\n36.48\n32.11\n41.78\nMaxEnt\n49.03\n50.60\n48.35\n49.25\nRan\nBaseline\n48.73\n48.73\n48.73\n48.73\nUncertainty-based Methods\nAvgProb\n54.57\n51.95\n47.35\n85.10\nAvgEnt\n50.49\n51.13\n53.51\n50.80\nMaxProb\n39.80\n33.77\n30.68\n41.30\nMaxEnt\n48.37\n50.79\n51.43\n50.75\nPop\nBaseline\n48.73\n48.73\n48.73\n48.73\nUncertainty-based Methods\nAvgProb\n55.87\n57.65\n51.52\n81.74\nAvgEnt\n51.74\n50.59\n51.83\n51.92\nMaxProb\n32.28\n35.90\n31.41\n40.97\nMaxEnt\n49.03\n50.30\n50.95\n51.73\nGQA\nBaseline\n48.47\n48.47\n48.47\n48.47\nUncertainty-based Methods\nAvgProb\n55.72\n56.56\n57.93\n65.06\nAvgEnt\n48.96\n50.12\n49.55\n48.38\nMaxProb\n33.20\n39.64\n37.27\n46.25\nMaxEnt\n48.16\n49.98\n49.15\n48.39\nTable 4: Yes-or-No tasks with period. The terms top\ndenote the highest AUC-PR values for uncertainty-\nbased methods.\nB.2\nExperimental Setup\nIn our experiments, we merge questions and re-\nsponses from humans or other models to create\nhand-crafted input. This input is then processed\nby the evaluation model, from which we extract\ninternal information, including probability, entropy,\nand embedding states. The evaluation model, ca-\npable of recognizing hallucinations (Duan et al.,\n2024), can assign reasonable internal information\nto responses not generated by itself. As shown\nin Figure 2, when generating the “cat” token, the\nevaluation model assigns probabilities to all tokens\nand selects the higher probability token for output.\nSpecifically, the probability assigned to “cat” in\nself-generated input is 0.9, while in hand-crafted\ninput, “dog” receives a probability of 0.1. POPE,\nGQA, and M-Hal are hand-crafted data for all four\nevaluation models used in this work. IHAD’s re-\nsponse, however, is generated by LLaVA-v1.5-7b,\nmaking IHAD self-generated data specifically for\nLLaVA-v1.5-7b. For the other three models, IHAD\nis considered hand-crafted data.\nIn the Yes-or-No tasks, we simplify the origi-\nnal responses to “Yes” for positive answers (e.g.,\n“Yes, there is a cute cat.”) and “No” for all others.\nTo balance positive and negative samples, we cre-\nate two entries for each original datum, one with\na “Yes” response and the other with a “No” re-\nsponse. Thus, the original datasets Adv, Ran, and\nPop, each containing 3000 questions for 500 im-\nages, are expanded to 6000 questions per set for\nthe same images. Similarly, GQA is increased to\n9050 questions for 397 figures. Regarding Open-\nended tasks, each response contains an average of\n4 sentences in M-HalDetect, while 5 sentences in\nIHAD.\nIn the consistency-based method, we set the tem-\nperature to 0.0 to obtain the main resource and\nuse standard beam search decoding. In addition,\nwe set the temperature to 1.0 for the stochastically\ngenerated samples and generated N = 10 samples.\nThe classifier used in the SUQ method employs a\nfeedforward neural network featuring three hidden\nlayers with decreasing numbers of hidden units\n(256, 128, 64), all utilizing ReLU activations. The\nfinal layer is a sigmoid output. We use the Adam\noptimizer. We do not fine-tune any of these hyper-\nparameters for this task. The classifier is trained\nfor 20 epochs. We use about two-thirds of a dataset\nto train a classifier based on a specific model and\nthen test its accuracy on the other third on the same\nmodel. The trained well classifier is required to\ndetermine which sentences the LVLM “believes”\nare true and which it “believes” are false.\nC\nExperimental Results\nC.1\nImpact of Unimportant Tokens for\nUncertainty-based Methods\nIt is observed that the performance of uncertainty\nmethods on the Open-ended tasks is not as good\nas on the Yes-or-No tasks. To explore the reasons,\nwe append a period, resulting in “Yes.” or “No.” in\nthe Yes-or-No task. In this case, probability p and\nentropy H of two tokens need to be considered. Ta-\nble 4 shows the detailed performance for Yes-or-No\ntasks with period using different methods. Specifi-\ncally, for LLaVA-v1.6-mistral, the AvgProb scores\ndecline to 81.70 from 92.04 for Adv, to 85.10 from\n96.99 for Ran, to 81.74 from 95.06 for Pop, and\nto 65.06 from 79.15 for GQA datasets, indicating\nthe performance of AvgProb is severely affected\nby unimportant tokens. The other three metrics,\nAvgEnt, MaxProb, and MaxEnt, nearly equal to\nthe baseline and play no role in hallucination detec-\n\ntion.\nC.2\nOptimal Hidden Layers for Effective\nSUQ Method\nTo explore which hidden layer information should\nbe used, we perform 3D TSNE visualizations of the\nembedding states of LLaVA at the fifth and last lay-\ners for popular tasks, as shown in Figure 6. These\nvisualizations show that at the last layer, there are\ndistinct separations between the representations of\ndifferent labels. Conversely, at the fifth layer, rep-\nresentations of different labels are relatively close\nand almost blend together, indicating that the shal-\nlow layers primarily preserve language-agnostic\nfeatures. Therefore, we use the last hidden layer to\nexplore the SUQ method in our experiments.\n(a) Fifth Layer\n(b) Last Layer\nFigure 6: 3D TSNE plot of language embeddings com-\nputed from the fifth and last layers of LLaVA-v1.6-7b\non popular task.\nC.3\nThe comparison of clear and blurred\nimages.\nThe comparison of clear and blurred images used\nin 4.3 is illustrated in Figure 7.\n(a) Clear Image\n(b) Blurred Image\nFigure 7: The comparison of clear and blurred images.",
    "pdf_filename": "Reference-free_Hallucination_Detection_for_Large_Vision-Language_Models.pdf"
}