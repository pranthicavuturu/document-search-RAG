{
    "title": "ControlNet++ Improving Conditional Controls with Efficient Consistency Feedback",
    "abstract": "els, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face signif- icant challenges in generating images that align with the image condi- tional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained dis- criminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward im- plementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various con- ditional controls. For example, it achieves improvements over ControlNet by 11.1% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmen- tation mask, line-art edge, and depth conditions. All the code, models, demo and organized data have been open sourced on our Github Repo.",
    "body": "ControlNet++: Improving Conditional Controls\nwith Efficient Consistency Feedback\nProject Page: liming-ai.github.io/ControlNet_Plus_Plus\nMing Li1, Taojiannan Yang1, Huafeng Kuang2, Jie Wu2,\nZhaoning Wang1, Xuefeng Xiao2, and Chen Chen1\n1 Center for Research in Computer Vision, University of Central Florida\n2 ByteDance\nAbstract. To enhance the controllability of text-to-image diffusion mod-\nels, existing efforts like ControlNet incorporated image-based conditional\ncontrols. In this paper, we reveal that existing methods still face signif-\nicant challenges in generating images that align with the image condi-\ntional controls. To this end, we propose ControlNet++, a novel approach\nthat improves controllable generation by explicitly optimizing pixel-level\ncycle consistency between generated images and conditional controls.\nSpecifically, for an input conditional control, we use a pre-trained dis-\ncriminative reward model to extract the corresponding condition of the\ngenerated images, and then optimize the consistency loss between the\ninput conditional control and extracted condition. A straightforward im-\nplementation would be generating images from random noises and then\ncalculating the consistency loss, but such an approach requires storing\ngradients for multiple sampling timesteps, leading to considerable time\nand memory costs. To address this, we introduce an efficient reward\nstrategy that deliberately disturbs the input images by adding noise,\nand then uses the single-step denoised images for reward fine-tuning.\nThis avoids the extensive costs associated with image sampling, allowing\nfor more efficient reward fine-tuning. Extensive experiments show that\nControlNet++ significantly improves controllability under various con-\nditional controls. For example, it achieves improvements over ControlNet\nby 11.1% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmen-\ntation mask, line-art edge, and depth conditions. All the code, models,\ndemo and organized data have been open sourced on our Github Repo.\nKeywords: Controllable Generation ¬∑ Diffusion Model ¬∑ ControlNet\n1\nIntroduction\nThe emergence and improvements of diffusion models [12, 43, 50], along with\nthe introduction of large-scale image-text datasets [48,49], has catalyzed signif-\nicant strides in text-to-image generation. Nonetheless, as the proverb ‚Äúan image\nis worth a thousand words‚Äù conveys, it‚Äôs challenging to depict an image accu-\nrately and in detail through language alone, and this dilemma also perplexes\nexisting text-to-image diffusion models [43,46]. To this end, many studies focus\non incorporating conditional controls such as segmentation mask into text-to-\nimage diffusion models [22,30,37,62,63]. Despite the diversity in these methods,\nthe core objective remains to facilitate more accurate and controllable image\ngeneration with explicit image-based conditional controls.\narXiv:2404.07987v4  [cs.CV]  19 Nov 2024\n\n2\nLi et al.\nLine-Art Image Condition\nText Prompt: Ryan Reynolds\n(d) ControlNet v1.1 (SSIM: 0.7377)\n(b) Ours (SSIM: 0.8714)\n(c) T2I-Adapter-SDXL (SSIM: 0.6840)\nMore Details\nLess Details\n(a) Inputs Conditions\n1\n2\n3\n1\n2\n3\n1\n2\n3\n1\n2\n3\nFig. 1: (a) Given the same input image condition and text prompt, (b) the extracted\nconditions of our generated images are more consistent with the inputs, (c,d) while\nother methods fail to achieve accurate controllable generation. SSIM scores measure\nthe similarity between all input edge conditions and the extracted edge conditions. All\nthe line edges are extracted by the same line detection model used by ControlNet [63].\nAchieving controllable image generation could involve retraining diffusion\nmodels from scratch [37,43], but this comes with high computational demands\nand a scarcity of large public datasets [63]. In light of this, a feasible strategy\nis fine-tuning pre-trained text-to-image models [23,61] or introducing trainable\nmodules [30, 62, 63] like ControlNet [63]. However, despite these studies have\nexplored the feasibility of controllability [30, 62, 63] in text-to-image diffusion\nmodels and expanded various applications [22,23,37], a significant gap remains\nin achieving precise and fine-grained control. As shown in Fig. 1, existing meth-\nods of controllable generation (e.g., ControlNet [63] and T2I-Adapter [30]) still\nstruggle to accurately generate images that are consistent with the input im-\nage condition. For example, T2I-Adapter-SDXL consistently produced incorrect\nwrinkles on the forehead in all generated images, while ControlNet v1.1 intro-\nduced many wrong details. Regrettably, current efforts lack specific methods for\nimproving controllability, which impedes progress in this research area.\nTo address this issue, we model image-based controllable generation as an\nimage translation task [17] from input conditional controls to output generated\nimages. Inspired by CycleGAN [71], we propose to employ pre-trained discrim-\ninative models to extract the condition from the generated images and directly\noptimize the cycle consistency loss for better controllability. The idea is that if\nwe translate images from one domain to the other (condition cv ‚Üígenerated\nimage x‚Ä≤\n0), and back again (generated image x‚Ä≤\n0 ‚Üícondition c‚Ä≤\nv) we should arrive\nwhere we started (c‚Ä≤\nv = cv), as shown in Fig. 2. For example, given a segmentation\nmask as a conditional control, we can employ existing methods such as Control-\n\nControlNet++\n3\nCondition\nPre-trained\nReward Models\nOutput\nGenerated Image \nPrompt: heart, mountains, \nand nature image\nCycle Consistency\nHed Model\nDepth Model\nCanny Model\nSegmentation Model\n¬∑¬∑¬∑¬∑¬∑¬∑\nControllable\nDiffusions\nFig. 2: Illustration of the cycle consistency. We first prompt the diffusion model\nG to generate an image x'\n_0 based on the given image condition c_v and text prompt ct,\nthen extract the corresponding image condition ÀÜcv from the generated image x‚Ä≤\n0 using\npre-trained discriminative models D. The cycle consistency is defined as the similarity\nbetween the extracted condition ÀÜcv and input condition cv.\nNet [63] to generate corresponding images. Then the predicted segmentation\nmasks of these generated images can be obtained by a pre-trained segmentation\nmodel. Ideally, the predicted segmentation masks and the input segmentation\nmasks should be consistent. Hence, the cycle consistency loss can be formulated\nas the per-pixel classification loss between the input and predicted segmentation\nmasks. Unlike existing related works [27, 30, 37, 63, 65] that implicitly achieve\ncontrollability by introducing conditional controls into the latent-space denois-\ning process, our method explicitly optimizes controllability at the pixel-space\nfor better performance, as demonstrated in Fig. 3.\nTo implement pixel-level loss within the context of diffusion models, an intu-\nitive approach involves executing the diffusion model‚Äôs inference process, starting\nfrom random Gaussian noise and performing multiple sampling steps to obtain\nthe final generated images, following recent works focusing on improving image\nquality with human feedback [11,36,60]. However, multiple samplings can lead\nto efficiency issues, and require the storage of gradients at every timestep and\nthus significant time and GPU memory consumption. We demonstrate that initi-\nating sampling from random Gaussian noise is unnecessary. Instead, by directly\nadding noise to training images to disturb their consistency with input condi-\ntional controls and then using single-step denoised images to reconstruct the\nconsistency, we can conduct more efficient reward fine-tuning. Our contributions\nare summarized as:\n‚Äì New Insight: We reveal that existing efforts in controllable generation still\nperform poorly in terms of controllability, with generated images significantly\ndeviating from input conditions and lacking a clear strategy for improvement.\n‚Äì Consistency Reward Feedback: We show that pre-trained discriminative mod-\nels can serve as powerful visual reward models to improve the controllability\nof controllable diffusion models in a cycle-consistency manner.\n‚Äì Efficient Reward Fine-tuning: We disrupt the consistency between input im-\nages and conditions, and enable the single-step denoising for efficient reward\nfine-tuning, avoiding time and memory overheads caused by image sampling.\n‚Äì Evaluation and Promising Results: We provide a unified and public evaluation\nof controllability under various conditional controls, and demonstrate that\nControlNet++ comprehensively outperforms existing methods.\n\n4\nLi et al.\nLatent-space \nDenoising Loss\nüî•\nDiffusion\nEncoder\nDecoder\nPixel-space Cycle\nConsistency Loss\n(a) Existing Methods\nüî•\nDiffusion\nEncoder\n(b) Our Solution\n     : A large building \nwith a pointed roof \nand several chimneys.\nFig. 3: (a) Existing methods achieve implicit controllability by introducing image-\nbased conditional control cv into the denoising process of diffusion models, with the\nguidance of latent-space denoising loss. (b) We utilize discriminative reward models D\nto explicitly optimize the controllability of G via pixel-level cycle consistency loss.\n2\nRelated Work\n2.1\nDiffusion-based Generative Models\nThe diffusion probabilistic model presented in [50] has undergone substantial ad-\nvancements [12,19,25], thanks to iterative refinements in training and sampling\nstrategies [18, 51, 52]. To alleviate the computational demands for training dif-\nfusion models, Latent Diffusion [43] maps the pixel space diffusion process into\nthe latent feature space. In the realm of text-to-image synthesis, diffusion mod-\nels [31,35,40,41,43,46] integrate cross-attention mechanisms between UNet [44]\ndenoisers and text embeddings from pre-trained language models like CLIP [38]\nand T5 [39] to facilitate reasonable text-to-image generation. Furthermore, diffu-\nsion models are employed across image editing tasks [3,14,24,29] by manipulat-\ning inputs [40], editing cross-attentions [16], and fine-tuning models [45]. Despite\nthe astonishing capabilities of diffusion models, language is a sparse and highly\nsemantic representation, unsuitable for describing dense, low-semantic images.\nFurthermore, existing methods [35,43] still struggle to understand detailed text\nprompts, posing a severe challenge to the controllable generation [63].\n2.2\nControllable Text-to-Image Diffusion Models\nTo achieve conditional control in pre-trained text-to-image diffusion models,\nControlNet [63] and T2I-Adapter [30] introduce additional trainable modules for\nguided image generation. Furthermore, recent research employs various prompt\nengineering [27,61,64] and cross-attention constraints [6,23,58] for a more reg-\nulated generation. Some methods also explore multi-condition or multi-modal\ngeneration within a single diffusion model [21, 37, 65] or focus on the instance-\nbased controllable generation [54,69]. However, despite these methods exploring\nfeasibility and applications, there still lacks a clear approach to enhance con-\ntrollability under various controls. Furthermore, existing works implicitly learn\ncontrollability by the denoising process of diffusion models, while our Control-\nNet++ achieves this in an explicit cycle-consistency manner, as shown in Fig. 3.\n2.3\nLinguistic and Visual Reward Models\nThe reward model is trained to evaluate how well the results of generative mod-\nels align with human expectations, and its quantified results will be used to\n\nControlNet++\n5\nfacilitate generative models for better and more controllable generation. It is\nusually trained with reinforcement learning from human feedback (RLHF) in\nNLP tasks [10,32,53], and has recently extended into the vision domain to im-\nprove the image quality for text-to-image diffusion models [1, 11, 13, 36, 56, 60].\nHowever, image quality is an exceedingly subjective metric, fraught with indi-\nvidual preferences, and requires the creation of new datasets with human prefer-\nences [26,55,56,60] and the training of reward models [36,55,60]. Diverging from\nthe pursuit of global image quality with subjective human preference in current\nresearch, we target the more fine-grained and objective goal of controllability.\nAlso, it‚Äôs more cost-effective to obtain AI feedback compared to human feedback.\n3\nMethod\nIn this section, we first introduce the background of diffusion models in Sec. 3.1.\nIn Sec. 3.2, we discuss how to design the cycle consistency loss for controllable\ndiffusion models to enhance the controllability. Finally, in Sec. 3.3, we examine\nthe efficiency issues with the straightforward solution and correspondingly pro-\npose an efficient reward strategy that utilizes the single-step denoised images for\nconsistency loss, instead of sampling images from random noise.\n3.1\nPreliminary\nThe diffusion models [18] define a Markovian chain of diffusion forward process\nq(xt|x0) by gradually adding noise to input data x0:\n  x _t=\\sq r\nt\n { \\bar \n{ \\ alph a }_t} x_0+\\sqrt {1-\\bar {\\alpha }_t} \\epsilon , \\quad \\epsilon \\sim \\mathcal {N}(\\mathbf {0}, {I}), \\label {eq:add_noise} \n(1)\nwhere œµ is a noise map sampled from a Gaussian distribution, and ¬ØŒ±t := Qt\ns=0 Œ±s.\nŒ±t = 1 ‚àíŒ≤t is a differentiable function of timestep t, which is determined by the\ndenoising sampler such as DDPM [18]. To this end, the diffusion training loss\ncan be represented by:\n  \\ma t\nh\nc\nal \n{L}\\left (\\ep silon\n _\\th\neta \\ri g\nh\nt  )=\\s\nu\nm  \n_{t\n=\n1\n}^T \\mathbb {E}_{x_0 \\sim q\\left (x_0\\right ), \\epsilon \\sim \\mathcal {N}(\\mathbf {0}, {I})}\\left [\\left \\|\\epsilon _\\theta \\left (\\sqrt {\\bar {\\alpha }_t} x_0+\\sqrt {1-\\bar {\\alpha }_t} \\epsilon \\right )-\\epsilon \\right \\|_2^2\\right ]. \n(2)\nIn the context of controllable generation [30,63], with given image condition cv\nand text prompt ct, the diffusion training loss at timestep t can be re-written\nas:\n  \\mat h cal {L}_{\\text {trai\nn\n}} = \\m at hbb  {E } _{x\n_\n0\n, t, c_t, c_v, \\epsilon \\sim \\mathcal {N}(0,1)} \\left [ \\| \\epsilon _\\theta \\left (x_t, t, c_t, c_v\\right ) - \\epsilon \\| _2^2 \\right ]. \\label {loss:diffusion} \n(3)\nDuring the inference, given a random noise xT ‚àºN(0, I), we can predict\nfinal denoised image x0 with the step-by-step denoising process [18]:\n  x_ {\nt\n-1}\n=\n\\f r a c  {\n1} { \\sq\nrt  {\\a lp\nh\na  _t}}\\left (x_t-\\frac {1-\\alpha _t}{\\sqrt {1-\\bar {\\alpha }_t}} {\\epsilon }_\\theta \\left (\\mathbf {x}_t, t\\right )\\right )+\\sigma _t \\epsilon , \n(4)\nwhere œµŒ∏ refers to the predicted noise at timestep t by U-Net [44] with parameters\nŒ∏, and œÉt = 1‚àí¬ØŒ±t‚àí1\n1‚àí¬ØŒ±t Œ≤t is the variance of posterior Gaussian distribution pŒ∏(x0).\n\n6\nLi et al.\n3.2\nReward Controllability with Consistency Feedback\nAs we model controllability as the consistency between input conditions and the\ngenerated images, we can naturally quantify this outcome through the discrim-\ninative reward models. Once we quantify the results of the generative model,\nwe can perform further optimization for more controllable generation based on\nthese quantified results in a unified manner for various conditional controls.\nTo be more specific, we minimize the consistency loss between the input\ncondition cv and the corresponding output condition ÀÜcv of the generated image\nx‚Ä≤\n0, as depicted in Fig. 2. The reward consistency loss can be formulated as:\n  \\begi n  {ali gned\n}  \\mat h cal\n {L\n} _\n{\n\\te x\nt\n { rewa rd }}  & \n=\\\nmathcal {L}\\left (c_v, \\hat {c}_v\\right ) \\\\ & =\\mathcal {L}\\left (c_v, \\mathbb {D}\\left (x_0'\\right )\\right ) \\\\ & =\\mathcal {L}\\left (c_v, \\mathbb {D}\\left [\\mathbb {G}^{T}\\left (c_t, c_v, x_T, t\\right )\\right ]\\right ), \\label {eq:default_reward} \\end {aligned} \n(5)\nwhere GT (ct, cv, xT , t) denotes the process that the model performs T denois-\ning steps\nto generate the image x‚Ä≤\n0 from random noise xT , as shown in the\nFig. 4 (a). Here, L is an abstract metric function that can take on different con-\ncrete forms for different visual conditions. For example, in the context of using\nsegmentation mask as the input conditional control, L could be the per-pixel\ncross-entropy loss. The reward model D is also dependent on the condition, and\nwe use the UperNet [57] for segmentation mask conditions. The details of loss\nfunctions and reward models are summarized in the supplementary material.\nIn addition to the reward loss, we also employ diffusion training loss in Eq. 3\nto ensure that the original image generation capability is not compromised since\nthey have different optimization goals. Finally, the total loss is the combination\nof Ltrain and Lreward:\n  \\mat h cal {L } _ { \\text {total}} = \\mathcal {L}_{\\text {train}} + \\lambda \\cdot \\mathcal {L}_{\\text {reward}}, \n(6)\nwhere Œª is a hyper-parameter to adjust the weight of the reward loss. Through\nthis approach, the consistency loss can guide the diffusion model on how to\nsample at different timesteps to obtain images more consistent with the input\ncontrols, thereby enhancing controllability. Nonetheless, directly applying such\nreward consistency still poses challenges in efficiency in real-world settings.\n3.3\nEfficient Reward Fine-tuning\nTo achieve the pixel-space consistency loss Lreward, it requires x0, the final dif-\nfused image, to calculate the reward consistency from the reward models. As\nmodern diffusion models, such as Stable Diffusion [43], require multiple steps,\ne.g., 50 steps, to render a full image, directly using such a solution is impractical\nin realistic settings: (1) multiple time-consuming samplings are required to de-\nrive images from random noise. (2) to enable gradient backpropagation, we have\nto store gradients at each timestep, meaning the GPU memory usage will in-\ncrease linearly with the number of time-steps. Taking ControlNet as an example,\n\nControlNet++\n7\n(a) Default Reward Strategy\n‚Ä¶\n‚Ä¶\n‚Ä¶\nAdd\nNoise\nSingle-step Sampling\n(b) Efficient Reward Strategy (Ours)\nDisturb Consistency\nEq. (5)\nEq. (8)\nEq. (1)\nEq. (7)\nEq. (4)\nMulti-step Sampling (e.g., 50 steps)\nEq. (4)\n‚Ä¶\n50x Inference \nTime & Memory\n1x Inference\nTime & Memory\nFig. 4: (a) Pipeline of default reward fine-tuning strategy. Reward fine-tuning requires\nsampling all the way to the full image. Such a method needs to keep all gradients for\neach timestep and the memory required is unbearable by current GPUs. (b) Pipeline\nof our efficient reward strategy. We add a small noise ( t ‚â§ t_{thre} ) to disturb the consis-\ntency between input images and conditions, then the single-step denoised image can\nbe directly used for efficient reward fine-tuning.\nwhen the batch size is 1 with FP16 mixed precision, the GPU memory required\nfor a single denoising step and storing all training gradients is approximately\n6.8GB. If we use the 50-step inference with the DDIM [51] scheduler, approxi-\nmately 340GB of memory is needed to perform reward fine-tuning on a single\nsample, which is nearly impossible to achieve with current hardware capabilities.\nAlthough GPU memory consumption can be reduced by employing techniques\nsuch as Low-Rank Adaptation (LoRA) [11,20], gradient checkpointing [7,11], or\nstop-gradient [60], the efficiency degradation caused by the number of sampling\nsteps required to generate images remains significant and cannot be overlooked.\nTherefore, an efficient reward fine-tuning approach is necessary.\nIn contrast to diffusing from random noise xT to obtain the final image\nx0, as illustrated in Fig. 4 (a), we instead propose an one-step efficient reward\nstrategy. Specifically, instead of randomly sampling from noise, we add noise to\nthe training images x0, thereby explicitly disturbing the consistency between\nthe diffusion inputs x‚Ä≤\nt and their conditional controls cv, by performing diffusion\nforward process q(xt|x0) in Eq. 1. We demonstrate this process as the Disturb\nConsistency in Fig. 4 (b) , which is the same procedure as the standard diffusion\ntraining process. When the added noise œµ is relatively small, we can predict the\noriginal image x‚Ä≤\n0 by performing single-step sampling3 on disturbed image x‚Ä≤\nt [18]:\n  x _0\n \\ ap\np r ox  x'_0 =\\f\nra c { x'_ {t\n}-\\\nsqrt {1-\\alpha _{t}} \\epsilon _\\theta \\left (x'_{t}, c_v, c_t, t\\right )}{\\sqrt {\\alpha _{t}}}, \n(7)\nand then we directly utilize the denoised image x‚Ä≤\n0 to perform reward fine-tuning:\n  \\begi n  {ali gned }  \\mat hcal\n {L } _{\\te xt {rew ard }}\n & = \\mathcal {L}({c}_{v}, \\hat {c}_{v})=\\mathcal {L}({c}_{v}, \\mathbb {D}(x'_0))=\\mathcal {L}({c}_{v}, \\mathbb {D}[\\mathbb {G}(c_{t}, {c}_{v}, x'_t, t)]). \\end {aligned} \\label {eq:reward_loss} \n(8)\nEssentially, the process of adding noise destroys the consistency between the\ninput image and its condition. Then the reward fine-tuning in Eq. 8 instructs\nthe diffusion model to generate images that can reconstruct the consistency, thus\nenhancing its ability to follow conditions during generation.\n3 We provide a more detailed proof in the supplementary material.\n\n8\nLi et al.\nPlease note that here we avoid the sampling process in Eq. 5. Finally, the\nloss is the combination of diffusion training loss and the reward loss:\n  \\mat h\nc\nal {L} _ { \\ text {t o\nta l  }}= \\b\negin { c\nases}\\mathcal {L}_{\\text {train }}+\\lambda \\cdot \\mathcal {L}_{\\text {reward }}, & \\text {if } t \\leq t_{\\text {thre},} \\\\ \\mathcal {L}_{\\text {train }}, & \\text {otherwise, }\\end {cases} \n(9)\nwhere tthre denotes the timestep threshold, which is a hyper-parameter used to\ndetermine whether a noised image xt should be utilized for reward fine-tuning.\nWe note that a small noise œµ (i.e., a relatively small timestep t) can disturb\nthe consistency and lead to effective reward fine-tuning. When the timestep t is\nlarge, xt is closer to the random noise xT , and predicting x‚Ä≤\n0 directly from xt\nresults in severe image distortion. The advantage of our efficient rewarding is\nthat xt can be employed both to train and reward the diffusion model without\nthe need for time and GPU memory costs caused by multiple sampling, thereby\nsignificantly improving the efficiency during the reward fine-tuning stage.\nDuring the reward fine-tuning phases, we freeze the pre-trained discrimina-\ntive reward model and text-to-image model, and only update the ControlNet\nfollowing its original implementation, which ensures the generative capabilities\nare not compromised. We also observe that using only the reward loss will lead\nto image distortion, aligning with the conclusions drawn in previous studies [60].\n4\nExperiments\n4.1\nExperimental Setup\nCondition Controls and Datasets. Given that existing text-image paired datasets\nfor generative models are unable to provide accurate conditional control data\npairs [48, 49], such as image-segmentation pairs, we endeavor to select specific\ndatasets for different tasks that can offer more precise image-label data pairs.\nMore specifically, ADE20K [67,68] and COCOStuff [4] are used for the segmen-\ntation mask condition following ControlNet [63]. For the canny edge map, hed\nedge map, lineart map, and depth map condition, we utilize the MultiGen-20M\ndataset proposed by UniControl [37], which is a subset of LAION-Aesthetics [48].\nFor the datasets without text caption such as ADE20K, we utilize MiniGPT-\n4 [70] to generate the image caption with the instruction ‚ÄúPlease briefly describe\nthis image in one sentence‚Äù. The training and inference resolution is 512√ó512 for\nall datasets and methods. Details are provided in the supplementary material.\nEvaluation and Metrics. We train ControlNet++ on the training set of each\ncorresponding dataset and evaluate all methods on the validation dataset. All\nthe experiments are evaluated under 512√ó512 resolution for fair comparison.\nFor each condition, we evaluate the controllability by measuring the similarity\nbetween the input conditions and the extracted conditions from generated images\nof diffusion models. For semantic segmentation and depth map controls, we use\nmIoU and RMSE as evaluation metrics respectively, which is a common practice\nin related research fields. For the edge task, we use F1-Score for hard edges (canny\n\nControlNet++\n9\nTable 1: Controllability comparison with state-of-the-art methods under different con-\nditional controls and datasets. ‚Üëdenotes higher result is better, while ‚Üìmeans lower\nis better. ControlNet++ achieves significant controllability improvements. ‚Äò-‚Äô indicates\nthat the method does not provide a public model for testing. We generate four groups\nof images in png format and report the average result to reduce random errors.\nCondition\n(Metric)\nSeg. Mask\n(mIoU ‚Üë)\nCanny Edge\n(F1 Score ‚Üë)\nHed Edge\n(SSIM ‚Üë)\nLineArt Edge\n(SSIM ‚Üë)\nDepth Map\n(RMSE ‚Üì)\nDataset\nT2I\nModel ADE20K COCO-Stuff MultiGen-20M MultiGen-20M MultiGen-20M MultiGen-20M\nControlNet\nSDXL\n-\n-\n-\n-\n-\n40.00\nT2I-Adapter\nSDXL\n-\n-\n28.01\n-\n0.6394\n39.75\nT2I-Adapter\nSD1.5\n12.61\n-\n23.65\n-\n-\n48.40\nGligen\nSD1.4\n23.78\n-\n26.94\n0.5634\n-\n38.83\nUni-ControlNet SD1.5\n19.39\n-\n27.32\n0.6910\n-\n40.65\nUniControl\nSD1.5\n25.44\n-\n30.82\n0.7969\n-\n39.18\nControlNet\nSD1.5\n32.55\n27.46\n34.65\n0.7621\n0.7054\n35.90\nOurs\nSD1.5\n43.64\n34.56\n37.04\n0.8097\n0.8399\n28.32\nedge) because it can be regarded as a binary classification problem of 0 (non-\nedge) and 1 (edge) and has a serious long-tail distribution, following the standard\nevaluation in edge detection [59]. The threshold used for evaluation is (100, 200)\nfor OpenCV, and (0.1, 0.2) for Kornia implementation. The SSIM metric is used\nfor the soft edges conditional controls (i.e., hed edge & lineart edge) following\nprevious works [65]. For ControlNet++, we use the UniPC [66] sampler with\n20 denoising steps to generate images with the original text prompt following\nControlNet v1.1 [63], without any negative prompts. For other methods beyond\nControlNet and ours, we utilized their open-source code to generate images and\nconducted fair evaluations under the same data, without changing their inference\nconfigures such as the number of inference steps or denoising sampler.\nBaselines. Our evaluation primarily focuses on T2I-Adapter [30], ControlNet\nv1.1 [63], GLIGEN [27], Uni-ControlNet [65], and UniControl [37], as these meth-\nods are pioneering in the realm of controllable text-to-image diffusion models and\noffer public model weights for various image conditions. To ensure fairness of\nevaluation, all methods use the same image conditions and text prompts. While\nmost methods employ the user-friendly SD1.5 as their text-to-image model for\ncontrollable generation, we have observed that recently there are a few mod-\nels based on SDXL [35]. Therefore, we also report the controllability results for\nControlNet-SDXL and T2I-Adapter-SDXL. Please note that ControlNet-SDXL\nmentioned here is not an officially released model as in ControlNet [63].\n4.2\nExperimental Results\nComparison of Controllability with State-of-the-art Methods. The experimental\nresults are shown in Tab. 1, which can be summarized as the following ob-\nservations: (1) Existing methods still underperform in terms of controllability,\nstruggling to achieve precise controlled generation. For instance, current meth-\nods (i.e., ControlNet) achieve only a 32.55 mIoU for images generated under\nthe condition of segmentation masks, which is far from its performance on real\ndatasets with a 50.7 mIoU, under the same evaluation from Mask2Former seg-\nmentation model [8]. (2) Our ControlNet++ significantly outperforms existing\n\n10\nLi et al.\nTable 2: FID (‚Üì) comparison with state-of-the-art methods under different conditional\ncontrols and datasets. All the results are conducted on 512√ó512 image resolution with\nClean-FID implementation [33] for fair comparisons. ‚Äò-‚Äô indicates that the method does\nnot provide a public model for testing. We generate four groups of images in png format\nand report the average result to reduce random errors.\nSeg. Mask\nCanny Edge\nHed Edge\nLineArt Edge\nDepth Map\nMethod\nT2I\nModel ADE20K COCO MultiGen-20M MultiGen-20M MultiGen-20M MultiGen-20M\nGligen\nSD1.4\n33.02\n-\n18.89\n-\n-\n18.36\nT2I-Adapter\nSD1.5\n39.15\n-\n15.96\n-\n-\n22.52\nUniControlNet SD1.5\n39.70\n-\n17.14\n17.08\n-\n20.27\nUniControl\nSD1.5\n46.34\n-\n19.94\n15.99\n-\n18.66\nControlNet\nSD1.5\n33.28\n21.33\n14.73\n15.41\n17.44\n17.76\nOurs\nSD1.5\n29.49\n19.29\n18.23\n15.01\n13.88\n16.66\nTable 3: CLIP-score (‚Üë) comparison with state-of-the-art methods under different\nconditional controls and datasets. ‚Äò-‚Äô indicates that the method does not provide a\npublic model for testing. We generate four groups of images in png format and report\nthe average result to reduce random errors.\nSeg. Mask\nCanny Edge\nHed Edge\nLineArt Edge\nDepth Map\nMethod\nT2I\nModel ADE20K COCO MultiGen-20M MultiGen-20M MultiGen-20M MultiGen-20M\nGligen\nSD1.4\n31.12\n-\n31.77\n-\n-\n31.75\nT2I-Adapter\nSD1.5\n30.65\n-\n31.71\n-\n-\n31.46\nUniControlNet SD1.5\n30.59\n-\n31.84\n31.94\n-\n31.66\nUniControl\nSD1.5\n30.92\n-\n31.97\n32.02\n-\n32.45\nControlNet\nSD1.5\n31.53\n13.31\n32.15\n32.33\n32.46\n32.45\nOurs\nSD1.5\n31.96\n13.13\n31.87\n32.05\n31.95\n32.09\nworks in terms of controllability across various conditional controls. For example,\nit achieves 11.1% RMSE improvements against previous state-of-the-art methods\nfor the depth map condition; (3) For controllable diffusion models, the strength\nof the text-to-image backbone does not affect its controllability. As shown in\nthe table, although SDXL-based [35] ControlNet and T2I-Adapter have better\ncontrollability on some specific tasks, the improvement is not large and is not\nsignificantly better than the counterparts with SD 1.5 [43].\nComparison of Image Quality with State-of-the-art Methods. To verify whether\nimproving controllability leads to a decline in image quality, we reported the\nFID (Fr√©chet Inception Distance) metrics of different methods under various\nconditional generation tasks in Tab. 2. We discovered that, compared to existing\nmethods, ControlNet++ generally exhibits superior FID values in most cases,\nindicating that our approach, while enhancing the controllability of conditional\ncontrols, does not result in a decrease in image quality. This can also be observed\nin Fig. 6. We provide more visual examples in the supplementary material.\nComparison of CLIP score with State-of-the-art Methods. Our ControlNet++\naims to improve the controllability of diffusion models using image-based condi-\ntions. Concerned about the potential adverse effects on text controllability, we\nevaluated various methods using CLIP-Score metrics across different datasets to\nmeasure the similarity between generated images and input text. As indicated in\nTab. 3, ControlNet++ achieved comparable or superior CLIP-Score outcomes on\nseveral datasets relative to existing approaches. This suggests that our method\nnot only markedly enhances conditional controllability but also preserves the\noriginal model‚Äôs text-to-image generation proficiency.\n\nControlNet++\n11\nGenerated Images\nGT + Generated Images\nGround Truth Images\nFig. 5: Training DeepLabv3 (MobileNetv2) from scratch with different images, includ-\ning ground truth images from ADE20K, and the generated images from ControlNet\nand ours. All the labels (i.e., segmentation masks) are ground truth labels in ADE20K.\nPlease note improvements here are non-trivial for semantic segmentation.\nEffectiveness of Generated Images. To further validate our improvements in\ncontrollability and their impact, we use the generated images along with real\nhuman-annotated labels to create a new dataset for training discriminative mod-\nels from scratch. Please note that the only difference from the original dataset\nused to train the discriminative model is that we have replaced the images with\nthose generated by the controllable diffusion model while keeping the labels un-\nchanged. If the generative model exhibits good controllability, the quality of the\nconstructed dataset will be higher, thereby enabling to train a stronger model.\nSpecifically, we conduct experiments on the ADE20K [67, 68] dataset on\nDeepLabv3 with MobileNetv2 backbone [5]. We use the standard training dataset\n(20210 training samples) to train the discriminative model and the validation\ndataset (5000 evaluation samples) for evaluation. We show the experimental re-\nsults in Fig. 5, the segmentation model trained on our images outperforms the\nbaseline results (ControlNet) by 1.19 mIoU. Please note that this improve-\nment is significant in segmentation tasks. For instance, Mask2Former [8]\nimproves previous SOTA MaskFormer [9] with around 1.1 mIoU in semantic\nsegmentation. In addition to conducting experiments solely on the generated\ndataset, we also combined generated data with real data to train the segmenta-\ntion model. The experimental results indicate that augmenting real ground truth\ndata with data generated by ControlNet does not yield additional performance\nimprovements (34.11 v.s. 34.08). In contrast, augmenting real data with our\ngenerated data results in significant performance enhancements (+1.76 mIoU).\nQualitative Comparison. Figs. 6 and 7 provide a qualitative comparison between\nour ControlNet++ and previous state-of-the-art methods across different con-\nditional controls. When given the same input text prompts and image-based\nconditional controls, we observe that existing methods often generate areas\n\n12\nLi et al.\nUni-ControlNet\nT2I-Adapter\nControlNet\nOurs\nImage & Condition\nUniControl\nGligen\nSegmentation\nDepth\nCanny\nHed\nUni-ControlNet\nControlNet\nOurs\nImage & Condition\nUniControl\nGligen\nFig. 6: Visualization comparison results in different conditional controls.\nOurs\nImage & Condition\nControlNet\nLine-Art\nT2I-Adapter\nFig. 7: Comparison on Line-Art Edge.\ninconsistent with the image conditions.\nFor instance, in the segmentation mask\ngeneration task, other methods often pro-\nduce extraneous picture frames on the\nwalls, resulting in a mismatch between the\nsegmentation masks extracted from the\ngenerated image and the inputs. A sim-\nilar situation occurs under depth conditions, where other methods fail to accu-\nrately represent the depth of different fingers. In contrast, images generated by\nControlNet++ maintain good consistency with the input depth map.\n\nControlNet++\n13\n4.3\nAblation Study\nLoss Settings. In Fig. 8, we find that maintaining the original diffusion train-\ning process is crucial for preserving the quality and controllability of generated\nimages. Relying solely on pixel-level consistency loss leads to severe image dis-\ntortion, whereas training the model with both this loss and the diffusion training\nloss can enhance controllability without affecting image quality.\nInput Condition\nPrompt: A pelican \ngracefully takes off \nfrom the calm water.\n100 Steps\n200 Steps\n400 Steps\n50 Steps\n100 Steps\n200 Steps\n400 Steps\n50 Steps\nWithout Diffusion Training Loss\nWith Diffusion Training Loss\nFig. 8: Ablation study on different loss settings during training. Using only pixel-level\nconsistency loss leads to severe image distortion, affecting both image quality and\ncontrollability. However, when combined with diffusion training loss, it is possible to\ngradually improve controllability without compromising image quality.\nGeneralizability of Efficient Reward Fine-tuning. Although the reward fine-\ntuning is used in a small subset of timesteps, it updates all the parameters\nof the ControlNet and therefore helps more timesteps to improve controllability\nduring sampling. To prove this, we divide the sampling process into two parts:\nthe unoptimized timesteps [T, tthre], and the optimized timesteps [tthre, 1] and\nuse ControlNet and our model for inference crossly, with 20-step sampling follow-\ning ControlNet. Table 4 shows that our reward finetuning performed on a small\nnumber of timesteps [tthre, 1] can be generalized to larger timesteps [T, tthre].\nTable 4: The impact of efficient re-\nward fine-tuning on different timesteps.\nUnoptimized\n[T, tthre]\nOptimized\n[tthre, 1]\nADE20K\nmIoU (‚Üë)\nControlNet\nControlNet\n32.55\nControlNet\nOurs\n38.03\nOurs\nControlNet\n41.46\nOurs\nOurs\n43.64\nTable 5: Stronger reward model (UperNet-\nR50) leads to better controllability than the\nweaker reward model (DeepLabv3-MBv2).\nReward Model (RM) RM mIoU‚ÜëEval mIoU‚Üë\n-\n-\n32.55\nDeepLabv3-MBv2\n34.02\n31.96\nFCN-R101\n39.91\n40.44\nUperNet-R50\n42.05\n43.64\nChoice of Different Reward Models. We demonstrate the effectiveness of different\nreward models in Tab. 5, all the evaluation results (i.e., Eval mIoU in the table)\nare evaluated by the most powerful segmentation model Mask2Former [8] with\n56.01 mIoU, on ADE20K dataset. We experiment with three different reward\nmodels, including DeepLabv3 [5] with MobileNetv2 [47] backbone (DeepLabv3-\nMBv2), FCN [28] with ResNet-101 [15] backbone (FCN-R101) and UperNet [57]\nwith ResNet-50 backbone. The results demonstrate that a more powerful reward\nmodel leads to better controllability for controllable diffusion models.\n\n14\nLi et al.\n5\nDiscussion\nHow to make Hed/LineArt Edge extraction methods differentiable? The Hed and\nLineArt Edge extraction models are neural networks without non-differentiable\noperations. Differentiability can be achieved by simply modifying forward code.\nSome conditions (e.g., Box/Sketch/Pose) are not available. Our reward fine-\ntuning leverages a pre-trained ControlNet and a differentiable reward model.\nCurrently, pre-trained ControlNet for object bounding boxes and differentiable\nreward models for sketches are lacking. In existing pose models, there are non-\ndifferentiable operations such as the NMS and keypoints grouping. We leave the\nquestion of how to extend consistency reward to more conditions to future work.\nInfluence of Text Prompt. We discuss how different types of text prompts (No\nPrompt, Conflicting Prompt, and Perfect Prompt) affect the final results. As\nshown in Fig. 9, when the text prompt is empty or there is a semantic conflict\nwith the image conditional control, ControlNet often struggles to generate accu-\nrate content. In contrast, our ControlNet++ manages to generate images that\ncomply with the input conditional controls under various text prompt scenarios.\nNo Prompt\nConflict Prompt\n‚Äúdelicious cake‚Äù\nPerfect Prompt\n‚Äúa house, high-quality, extremely detailed, 4K‚Äù\nOurs\nControlNet\nFig. 9: When the input text prompt is empty or conflicts with the image-based condi-\ntional controls (the segmentation map in the top left corner), ControlNet struggles to\ngenerate correct content (red boxes), whereas our method manages to generate it well.\n6\nConclusion\nIn this paper, we demonstrate from both quantitative and qualitative perspec-\ntives that existing works focusing on controllable generation still fail to achieve\nprecise conditional control, leading to inconsistency between generated images\nand input conditions. To address this issue, we introduce ControlNet++, it ex-\nplicitly optimizes the consistency between input conditions and generated images\nusing a pre-trained discriminative reward model in a cycle consistency manner,\nwhich is different from existing methods that implicitly achieve controllability\nthrough latent diffusion denoising. We also propose a novel and efficient reward\nstrategy that calculates consistency loss by adding noise to input images fol-\nlowed by single-step denoising, thus avoiding the significant computational and\nmemory costs associated with sampling from random Gaussian noise. Exper-\nimental results under multiple conditional controls show that ControlNet++\nsignificantly improves controllability without compromising image quality and\nimage-text alignment, offering new insights into controllable visual generation.\n\nControlNet++: Improving Conditional Controls\nwith Efficient Consistency Feedback\nSupplementary Material\nMing Li1, Taojiannan Yang1, Huafeng Kuang2, Jie Wu2,\nZhaoning Wang1, Xuefeng Xiao2, and Chen Chen1\n1 Center for Research in Computer Vision, University of Central Florida\n2 ByteDance\n1\nOverview of Supplementary\nThe supplementary material is organized into the following sections:\n‚Äì Section 2: Implementation details for all experiments.\n‚Äì Section 3: Proof for Eq.(7) in the main paper.\n‚Äì Section 4: More experiments and analysis.\n‚Ä¢ Section 4.1: Effectiveness of conditioning scale of existing methods.\n‚Ä¢ Section 4.2: Human evaluation on controllability, text guidance and im-\nage quaility.\n‚Äì Section 5: Discussion of broader impact and limitation.\n‚Äì Section 6: More visualization results.\n2\nImplementation Details\n2.1\nDataset Details\nConsidering that the training data for ControlNet [63] has not been publicly\nreleased, we need to construct our training dataset. In this paper, we adhere to\nthe dataset construction principles of ControlNet [63], which endeavor to select\ndatasets with more accurate conditional conditions wherever possible. Specifi-\ncally, for the segmentation condition, previous works have provided datasets with\naccurately labeled segmentation masks [4,67,68]. Therefore, we opt to train our\nmodel using these accurately labeled datasets following ControlNet [63]. For the\nHed, LineArt edge tasks, it is challenging to find datasets with real and accurate\nannotations. As a result, following ControlNet [63], we train the model using\nthe MultiGen20M dataset [37], which is annotated by models, to address this\nissue. Regarding the depth task, existing datasets include masks of certain pixels\nas having unknown depth values, making them incompatible with the current\nControlNet pipeline. Therefore, we also adapt the MultiGen20M depth dataset,\nwhich is similar to the dataset constructed by ControlNet [63]. In terms of the\ncanny edge task, no human labels are required in the process, so we also adapt\nthe MultiGen20M dataset. We provide details of the datasets in Table 1.\n\n16\nLi et al.\nTable 1: Dataset and evaluation details of different conditional controls. ‚Üëdenotes\nhigher is better, while ‚Üìmeans lower is better.\nSegmentation Mask\nCanny Edge\nHed Edge\nLineArt Edge\nDepth Map\nDataset\nADE20K [67,68], COCOStuff [4] MultiGen20M [37] MultiGen20M [37] MultiGen20M [37] MultiGen20M [37]\nTraining Samples\n20,210 & 118,287\n2,560,000\n2,560,000\n2,560,000\n2,560,000\nEvaluation Samples\n2,000 & 5,000\n5,000\n5,000\n5,000\n5,000\nEvaluation Metric\nmIoU ‚Üë\nF1 Score ‚Üë\nSSIM ‚Üë\nSSIM ‚Üë\nRMSE ‚Üì\nTable 2: Details of the reward model, evaluation model, and training loss under dif-\nferent conditional controls. ControlNet* denotes we use the same model to extract\nconditions as ControlNet [63]\n.\nSeg. Mask\nDepth Edge\nCanny Edge Hed Edge LineArt Edge\nReward Model (RM)\nUperNet-R50\nDPT-Hybrid\nKornia Canny ControlNet*\nControlNet*\nRM Performance\nADE20K(mIoU): 42.05 NYU(AbsRel): 8.69\n-\n-\n-\nEvaluation Model (EM)\nMask2Former\nDPT-Large\nKornia Canny ControlNet*\nControlNet*\nEM Performance\nADE20K(mIoU): 56.01 NYU(AbsRel): 8.32\n-\n-\n-\nConsistency Loss\nCrossEntropy Loss\nMSE Loss\nMSE Loss\nMSE Loss\nMSE Loss\nLoss Weight Œª\n0.5\n0.5\n1.0\n1.0\n10\n2.2\nReward Model and Evaluation Details\nIn general, we deliberately choose slightly weaker models as the reward model\nand opt for stronger models for evaluation. This practice not only ensures the\nfairness of the evaluation but also helps to determine whether performance im-\nprovements result from alignment with the reward model‚Äôs preferences or from\na genuine enhancement in controllability. While such an approach is feasible\nfor some tasks (Segmentation, Depth), it becomes challenging to implement for\nothers (Hed, Canny, LineArt Edge) due to the difficulty in finding two distinct\nreward models. In such cases, we use the same model as both the reward model\nand the evaluation model. We utilize standard evaluation schemes from their re-\nspective research fields to evaluate the input conditions and extracted conditions\nfrom the generated images, as demonstrated in Section 4.1 of the main paper.\nWe use the same Hed edge detection model and LineArt edge detection model as\nControlNet [63]. We provide details of reward models and evaluation in Table 2.\n2.3\nTraining Details\nThe loss weight Œª for reward consistency loss is different for each condition.\nSpecifically, Œª is 0.5, 0.5, 1.0, 1.0, and 10 for segmentation mask, depth, hed\nedge, canny edge, and LineArt edge condition, respectively. For all experiments,\nwe first fine-tune the pre-trained ControlNet until convergence using a batch\nsize of 256 and a learning rate of 1e-5. We then employ the same batch size and\nlearning rate for 10k iterations reward fine-tuning. To this end, the valid training\nsamples for reward fine-tuning is 256 √ó 10, 000 = 2, 560, 000. We set threshold\ntthre = 200 of Eq.8 in the main paper for all experiments. Diverging from existing\nmethods that use OpenCV‚Äôs [2] implementation of the canny algorithm, we have\nadopted Kornia‚Äôs [42] implementation to make it differentiable. Our codebase is\nbased on the implementation in HuggingFace‚Äôs Diffusers [34], and we do not use\nclassifier-free guidance during the reward fine-tuning process following diffusers.\n\nControlNet++\n17\nPredicted\nat different timestep\n1000\n900\n800\n700\n600\n500\n400\n300\n200\n100\nImage\nCondition\nFig. 1: Illustration of predicted image x‚Ä≤\n0 at different timesteps t. A small timestep t\n(i.e., small noise œµt) leads to more precise estimation x‚Ä≤\n0 ‚âàx0.\n3\nProof of Equation 7 in the Main Paper\nThe diffusion models define a Markovian chain of diffusion forward process\nq(xt|x0) by gradually adding noise to input data x0:\n  x _t=\\sq r\nt\n { \\bar \n{ \\ alph a }_t} x_0+\\sqrt {1-\\bar {\\alpha }_t} \\epsilon , \\quad \\epsilon \\sim \\mathcal {N}(\\mathbf {0}, I), \\label {eq:diffusion_forward} \n(1)\nat any timestep t we can use the predicted œµ(x‚Ä≤\nt, cv, ct, t ‚àí1) to estimate the real\nnoise œµ in Eq. 1, and the above equation can be transformed through straight-\nforward algebraic manipulation to the following form:\n  \\ begin {\na\nl i gned}  & \nx_ t \\ app r o x \\\nsq r t \n{ \\ ba\nr  {\\ a lpha  }_\nt}  x_ 0+\\ s q rt\n {1\n-\\bar {\\alpha }_t} \\epsilon _\\theta \\left (x_t^{\\prime }, c_v, c_t, t-1\\right ), \\\\ & x_0 \\approx x_0^{\\prime }=\\frac {x_t^{\\prime }-\\sqrt {1-\\alpha _t} \\epsilon _\\theta \\left (x_t^{\\prime }, c_v, c_t, t-1\\right )}{\\sqrt {\\alpha _t}}. \\end {aligned} \n(2)\nTo this end, we can obtain the predicted original image x‚Ä≤\n0 at any denoising\ntimestep t and use it as the input for reward consistency loss. However, previous\nwork demonstrates that this approximation only yields a smaller error when\nthe time step t is relatively small [18]. Here we find similar results as shown in\nFigure 1, which illustrates the predicted x‚Ä≤\n0 is significantly different at different\ntimesteps. We kindly encourage readers to refer to Section 4.3 and Figure 5 in\nthe DDPM [18] paper for more experimental results.\n4\nMore Experiments\nIn this section, we provide additional supplements to the experiments discussed\nin the main paper, including human evaluation on generated data samples on\nthe Segmentation Mask condition in Sec. 4.2, analysis on conditioning scale of\nexisting methods such as ControlNet [63] and T2I-Adapter [30] in Sec. 4.1.\n\n18\nLi et al.\nControlNet\nT2I-Adapter-SDXL\nImage & Condition\nControl Scale: 1.0\nControl Scale: 2.0\nControl Scale: 3.0\nControl Scale: 4.0\nControl Scale: 10.0\nControl Scale: 0.5\nGenerated\nImages\nExtracted\nConditions\nExtracted\nConditions\nGenerated\nImages\nFig. 2: Naively increasing the weight of image condition embedding compared to text\ncondition embedding in exiting methods (i.e., ControlNet and T2I-Adapter) cannot\nimprove controllability while ensuring image quality. The red boxes in the figures high-\nlight areas where the generated image is inconsistent with the input conditions. Please\nnote that we employ the same line detection model to extract conditions from images.\n4.1\nEffectiveness of Conditioning Scale\nTo simultaneously achieve control based on text prompts and image conditions,\nexisting controllable generation methods perform an addition operation between\nthe image condition features and the text embedding features. The strength of\ndifferent conditions can be adjusted through a weight value. Hence, an obvious\nquestion arises: can better controllability be achieved by increasing the weight of\nthe image condition features? To answer this question, we conduct experiments\nunder different control scales (The weight of image condition feature) in Figure 2.\nIt demonstrates that naively increasing the control ratio of image conditions does\nnot enhance controllability and may lead to severe image distortion.\n4.2\nHuman Evaluation\nFollowing ControlNet, we use a single condition for human evaluation. We ask 20\nusers (12 in ControlNet paper) to select the best image based on three distinct\ncriteria as shown in Table 3. Our ControlNet++ offers better controllability\nwithout sacrificing image quality or text guidance.\nTable 3: Win rate on ADE20K validation dataset (Segmentation).\n20 annotators in total\nOurs\nControlNet T2I-Adapter UniControl\nImage-Mask Alignment 76.8%\n16.7%\n2.0%\n4.5%\nImage Quality\n26.1%\n25.8 %\n23.6%\n24.5 %\nImage-Text Alignment 25.3%\n25.1%\n24.9%\n24.7%\n\nControlNet++\n19\n5\nBroader Impact and Limitation\nIn this paper, we use visual discriminative models to evaluate and improve the\ncontrollability of text-to-image models. However, we also realize that this work\nis still insufficient and discuss the following issues:\nConditions Expansion: While we have achieved notable improvements under\nsix control conditions, our future work aims to broaden the scope by incorporat-\ning additional control conditions such as Human Pose and Scribbles. Ultimately,\nour objective is to control everything.\nBeyond Controllability: While our current focus lies predominantly on con-\ntrollability, we acknowledge the significance of quality and aesthetic appeal in the\ngenerated outputs. To address this, we plan to leverage human feedback to an-\nnotate controllability images. Subsequently, we will optimize the controllability\nmodel to simultaneously enhance both controllability and aesthetics.\nJoint Optimization: To further enhance the overall performance, we intend\nto employ a larger set of controllable images for joint optimization of the con-\ntrol network and reward model. This holistic approach would facilitate their\nco-evolution, leading to further improvements in the final generated outputs.\nThrough our research, we aspire to provide insightful contributions to control-\nlability in text-to-image diffusion models. We hope that our work inspires and\nencourages more researchers to delve into this fascinating area.\nDiscussion on the necessity of controllability: Controllability is important\nsince it allows users to modify image conditions to achieve more flexible and\naccurate generation. Take LineArt Edge as an example: (1) Freely generating in\nforeground may change the appearance (e.g., adding a beard for women) that\nwe usually do not expect. (2) Freely generating in background will damage some\napplications (e.g., blur background). (3) Global free generating may destroy the\noverall artistic effect of the input image, such as lighting, composition, contrast,\netc. Furthermore, we show in Fig.5 of the main paper that more controllable dif-\nfusion can in return improve the performance of discriminative models. Beyond\nimage generation, the controllable conditional generation can also be combined\nwith ID preserving methods to perform controllable image editing.\n6\nMore Visualization\nMore visualization results across different conditional controls for our image\ngeneration are shown in Figures 3,4,5,6,7.\n\n20\nLi et al.\nImage & Condition\nGenerated Images & Extracted Conditions\nFig. 3: More visualization results of our ControlNet++ (LineArt Edge)\n\nControlNet++\n21\nImage & Condition\nGenerated Images & Extracted Conditions\nFig. 4: More visualization results of our ControlNet++ (Depth Map)\n\n22\nLi et al.\nImage & Condition\nGenerated Images & Extracted Conditions\nFig. 5: More visualization results of our ControlNet++ (Hed Edge)\n\nControlNet++\n23\nImage & Condition\nGenerated Images & Extracted Conditions\nFig. 6: More visualization results of our ControlNet++ (Canny Edge)\n\n24\nLi et al.\nImage & Condition\nGenerated Images & Extracted Conditions\nFig. 7: More visualization results of our ControlNet++ (Segmentation Mask)\n\nControlNet++\n25\nReferences\n1. Black, K., Janner, M., Du, Y., Kostrikov, I., Levine, S.: Training diffusion models\nwith reinforcement learning. arXiv preprint arXiv:2305.13301 (2023)\n2. Bradski, G.: The OpenCV Library. Dr. Dobb‚Äôs Journal of Software Tools (2000)\n3. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image\nediting instructions. In: CVPR (2023)\n4. Caesar, H., Uijlings, J., Ferrari, V.: Coco-stuff: Thing and stuff classes in context.\nIn: CVPR (2018)\n5. Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution\nfor semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)\n6. Chen, M., Laina, I., Vedaldi, A.: Training-free layout control with cross-attention\nguidance. arXiv preprint arXiv:2304.03373 (2023)\n7. Chen, T., Xu, B., Zhang, C., Guestrin, C.: Training deep nets with sublinear mem-\nory cost. arXiv (2016)\n8. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention\nmask transformer for universal image segmentation. In: CVPR (2022)\n9. Cheng, B., Schwing, A., Kirillov, A.: Per-pixel classification is not all you need for\nsemantic segmentation. NeurIPS (2021)\n10. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\nguage modeling with pathways. arXiv preprint arXiv:2204.02311 (2022)\n11. Clark, K., Vicol, P., Swersky, K., Fleet, D.J.: Directly fine-tuning diffusion models\non differentiable rewards. arXiv preprint arXiv:2309.17400 (2023)\n12. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS\n(2021)\n13. Fan, Y., Watkins, O., Du, Y., Liu, H., Ryu, M., Boutilier, C., Abbeel, P.,\nGhavamzadeh, M., Lee, K., Lee, K.: Dpok: Reinforcement learning for fine-tuning\ntext-to-image diffusion models. NeurIPS (2023)\n14. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,\nCohen-or, D.: An image is worth one word: Personalizing text-to-image genera-\ntion using textual inversion. In: ICLR (2023)\n15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016)\n16. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-or, D.:\nPrompt-to-prompt image editing with cross-attention control. In: ICLR (2023)\n17. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-\ngies. In: SIGGRAPH (2001)\n18. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS\n(2020)\n19. Ho,\nJ.,\nSalimans,\nT.:\nClassifier-free\ndiffusion\nguidance.\narXiv\npreprint\narXiv:2207.12598 (2022)\n20. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,\nW.: LoRA: Low-rank adaptation of large language models. In: ICLR (2022)\n21. Hu, M., Zheng, J., Liu, D., Zheng, C., Wang, C., Tao, D., Cham, T.J.: Cocktail:\nMixing multi-modality controls for text-conditional image generation. NeurIPS\n(2023)\n22. Huang, L., Chen, D., Liu, Y., Shen, Y., Zhao, D., Zhou, J.: Composer: Creative\nand controllable image synthesis with composable conditions. In: ICML (2015)\n\n26\nLi et al.\n23. Ju, X., Zeng, A., Zhao, C., Wang, J., Zhang, L., Xu, Q.: Humansd: A native\nskeleton-guided diffusion model for human image generation. In: ICCV (2023)\n24. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,\nM.: Imagic: Text-based real image editing with diffusion models. In: CVPR (2023)\n25. Kingma, D., Salimans, T., Poole, B., Ho, J.: Variational diffusion models. NeurIPS\n(2021)\n26. Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., Levy, O.: Pick-a-pic:\nAn open dataset of user preferences for text-to-image generation. arXiv preprint\narXiv:2305.01569 (2023)\n27. Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., Lee, Y.J.: Gligen: Open-set\ngrounded text-to-image generation. In: CVPR (2023)\n28. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: CVPR (2015)\n29. Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: Sdedit: Guided\nimage synthesis and editing with stochastic differential equations. In: ICLR (2022)\n30. Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-adapter:\nLearning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453 (2023)\n31. Nichol, A.Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B.,\nSutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. In: ICML (2022)\n32. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,\nC., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow\ninstructions with human feedback. NeurIPS (2022)\n33. Parmar, G., Zhang, R., Zhu, J.Y.: On aliased resizing and surprising subtleties in\ngan evaluation. In: CVPR (2022)\n34. von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K.,\nDavaadorj, M., Wolf, T.: Diffusers: State-of-the-art diffusion models. https://\ngithub.com/huggingface/diffusers (2022)\n35. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M√ºller, J., Penna,\nJ., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952 (2023)\n36. Prabhudesai, M., Goyal, A., Pathak, D., Fragkiadaki, K.: Aligning text-to-image\ndiffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739\n(2023)\n37. Qin, C., Zhang, S., Yu, N., Feng, Y., Yang, X., Zhou, Y., Wang, H., Niebles, J.C.,\nXiong, C., Savarese, S., et al.: Unicontrol: A unified diffusion model for controllable\nvisual generation in the wild. NeurIPS (2023)\n38. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\nG., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models\nfrom natural language supervision. In: ICML (2021)\n39. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,\nW., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text\ntransformer. JMLR (2020)\n40. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125\n(2022)\n41. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,\nSutskever, I.: Zero-shot text-to-image generation. In: ICML (2021)\n42. Riba, E., Mishkin, D., Ponsa, D., Rublee, E., Bradski, G.: Kornia: an open source\ndifferentiable computer vision library for pytorch. In: CVPR (2020)\n\nControlNet++\n27\n43. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: CVPR (2022)\n44. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: MICCAI (2015)\n45. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation.\nIn: CVPR (2023)\n46. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,\nK., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-\nto-image diffusion models with deep language understanding. NeurIPS (2022)\n47. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-\nverted residuals and linear bottlenecks. In: CVPR (2018)\n48. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M.,\nCoombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,\nS., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An open\nlarge-scale dataset for training next generation image-text models. ArXiv (2022)\n49. Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A.,\nCoombes, T., Jitsev, J., Komatsuzaki, A.: Laion-400m: Open dataset of clip-filtered\n400 million image-text pairs. ArXiv (2021)\n50. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In: ICML (2015)\n51. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: ICLR (2021)\n52. Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-\nbased generative modeling through stochastic differential equations. In: ICLR\n(2021)\n53. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-\nlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n54. Wang, X., Darrell, T., Rambhatla, S.S., Girdhar, R., Misra, I.: Instancediffusion:\nInstance-level control for image generation (2024)\n55. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., Li, H.: Human preference\nscore v2: A solid benchmark for evaluating human preferences of text-to-image\nsynthesis. arXiv preprint arXiv:2306.09341 (2023)\n56. Wu, X., Sun, K., Zhu, F., Zhao, R., Li, H.: Better aligning text-to-image models\nwith human preference. In: ICCV (2023)\n57. Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene\nunderstanding. In: ECCV (2018)\n58. Xie, J., Li, Y., Huang, Y., Liu, H., Zhang, W., Zheng, Y., Shou, M.Z.: Boxdiff: Text-\nto-image synthesis with training-free box-constrained diffusion. In: ICCV (2023)\n59. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV (2015)\n60. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: Imagere-\nward: Learning and evaluating human preferences for text-to-image generation.\nNeurIPS (2023)\n61. Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z., Liu, C., Zeng,\nM., et al.: Reco: Region-controlled text-to-image generation. In: CVPR (2023)\n62. Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compati-\nble image prompt adapter for text-to-image diffusion models. arXiv preprint\narXiv:2308.06721 (2023)\n63. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image\ndiffusion models. In: ICCV (2023)\n\n28\nLi et al.\n64. Zhang, T., Zhang, Y., Vineet, V., Joshi, N., Wang, X.: Controllable text-to-image\ngeneration with gpt-4. arXiv preprint arXiv:2305.18583 (2023)\n65. Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-\ncontrolnet: All-in-one control to text-to-image diffusion models. NeurIPS (2023)\n66. Zhao, W., Bai, L., Rao, Y., Zhou, J., Lu, J.: Unipc: A unified predictor-corrector\nframework for fast sampling of diffusion models. arXiv preprint arXiv:2302.04867\n(2023)\n67. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing\nthrough ade20k dataset. In: CVPR (2017)\n68. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.:\nSemantic understanding of scenes through the ade20k dataset. IJCV (2019)\n69. Zhou, D., Li, Y., Ma, F., Yang, Z., Yang, Y.: Migc: Multi-instance generation\ncontroller for text-to-image synthesis. In: CVPR (2024)\n70. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 (2023)\n71. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In: ICCV (2017)",
    "pdf_filename": "ControlNet++_Improving_Conditional_Controls_with_Efficient_Consistency_Feedback.pdf"
}