{
    "title": "ControlNet++: Improving Conditional Controls",
    "abstract": "els,existingeffortslikeControlNetincorporatedimage-basedconditional controls. In this paper, we reveal that existing methods still face signif- icant challenges in generating images that align with the image condi- tionalcontrols.Tothisend,weproposeControlNet++,anovelapproach thatimprovescontrollablegenerationbyexplicitlyoptimizingpixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained dis- criminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the inputconditionalcontrolandextractedcondition.Astraightforwardim- plementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. Thisavoidstheextensivecostsassociatedwithimagesampling,allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various con- ditionalcontrols.Forexample,itachievesimprovementsoverControlNet by11.1%mIoU,13.4%SSIM,and7.6%RMSE,respectively,forsegmen- tation mask, line-art edge, and depth conditions. All the code, models, demo and organized data have been open sourced on our Github Repo.",
    "body": "ControlNet++: Improving Conditional Controls\nwith Efficient Consistency Feedback\nProject Page: liming-ai.github.io/ControlNet_Plus_Plus\nMing Li1, Taojiannan Yang1, Huafeng Kuang2, Jie Wu2,\nZhaoning Wang1, Xuefeng Xiao2, and Chen Chen1\n1 Center for Research in Computer Vision, University of Central Florida\n2 ByteDance\nAbstract. Toenhancethecontrollabilityoftext-to-imagediffusionmod-\nels,existingeffortslikeControlNetincorporatedimage-basedconditional\ncontrols. In this paper, we reveal that existing methods still face signif-\nicant challenges in generating images that align with the image condi-\ntionalcontrols.Tothisend,weproposeControlNet++,anovelapproach\nthatimprovescontrollablegenerationbyexplicitlyoptimizingpixel-level\ncycle consistency between generated images and conditional controls.\nSpecifically, for an input conditional control, we use a pre-trained dis-\ncriminative reward model to extract the corresponding condition of the\ngenerated images, and then optimize the consistency loss between the\ninputconditionalcontrolandextractedcondition.Astraightforwardim-\nplementation would be generating images from random noises and then\ncalculating the consistency loss, but such an approach requires storing\ngradients for multiple sampling timesteps, leading to considerable time\nand memory costs. To address this, we introduce an efficient reward\nstrategy that deliberately disturbs the input images by adding noise,\nand then uses the single-step denoised images for reward fine-tuning.\nThisavoidstheextensivecostsassociatedwithimagesampling,allowing\nfor more efficient reward fine-tuning. Extensive experiments show that\nControlNet++ significantly improves controllability under various con-\nditionalcontrols.Forexample,itachievesimprovementsoverControlNet\nby11.1%mIoU,13.4%SSIM,and7.6%RMSE,respectively,forsegmen-\ntation mask, line-art edge, and depth conditions. All the code, models,\ndemo and organized data have been open sourced on our Github Repo.\nKeywords: Controllable Generation · Diffusion Model · ControlNet\n1 Introduction\nThe emergence and improvements of diffusion models [12,43,50], along with\nthe introduction of large-scale image-text datasets [48,49], has catalyzed signif-\nicant strides in text-to-image generation. Nonetheless, as the proverb “an image\nis worth a thousand words” conveys, it’s challenging to depict an image accu-\nrately and in detail through language alone, and this dilemma also perplexes\nexisting text-to-image diffusion models [43,46]. To this end, many studies focus\non incorporating conditional controls such as segmentation mask into text-to-\nimagediffusionmodels[22,30,37,62,63].Despitethediversityinthesemethods,\nthe core objective remains to facilitate more accurate and controllable image\ngeneration with explicit image-based conditional controls.\n4202\nvoN\n91\n]VC.sc[\n4v78970.4042:viXra\n2 Li et al.\n1 2 3\nLessDetails More Details\n1 2 3\nLine-ArtImageCondition\nText Prompt: Ryan Reynolds\n(a) Inputs Conditions (b)Ours(SSIM:0.8714)\n1 2 3 1 2 3\n(c) T2I-Adapter-SDXL (SSIM: 0.6840) (d) ControlNet v1.1 (SSIM: 0.7377)\nFig.1: (a)Giventhesameinputimageconditionandtextprompt,(b)theextracted\nconditions of our generated images are more consistent with the inputs, (c,d) while\nother methods fail to achieve accurate controllable generation. SSIM scores measure\nthesimilaritybetweenallinputedgeconditionsandtheextractededgeconditions.All\nthelineedgesareextractedbythesamelinedetectionmodelusedbyControlNet[63].\nAchieving controllable image generation could involve retraining diffusion\nmodels from scratch [37,43], but this comes with high computational demands\nand a scarcity of large public datasets [63]. In light of this, a feasible strategy\nis fine-tuning pre-trained text-to-image models [23,61] or introducing trainable\nmodules [30,62,63] like ControlNet [63]. However, despite these studies have\nexplored the feasibility of controllability [30,62,63] in text-to-image diffusion\nmodels and expanded various applications [22,23,37], a significant gap remains\nin achieving precise and fine-grained control. As shown in Fig. 1, existing meth-\nods of controllable generation (e.g., ControlNet [63] and T2I-Adapter [30]) still\nstruggle to accurately generate images that are consistent with the input im-\nage condition. For example, T2I-Adapter-SDXL consistently produced incorrect\nwrinkles on the forehead in all generated images, while ControlNet v1.1 intro-\nduced many wrong details. Regrettably, current efforts lack specific methods for\nimproving controllability, which impedes progress in this research area.\nTo address this issue, we model image-based controllable generation as an\nimage translation task [17] from input conditional controls to output generated\nimages. Inspired by CycleGAN [71], we propose to employ pre-trained discrim-\ninative models to extract the condition from the generated images and directly\noptimize the cycle consistency loss for better controllability. The idea is that if\nwe translate images from one domain to the other (condition c → generated\nv\nimagex′),andbackagain(generatedimagex′ →conditionc′)weshouldarrive\n0 0 v\nwherewestarted(c′ =c ),asshowninFig.2.Forexample,givenasegmentation\nv v\nmask as a conditional control, we can employ existing methods such as Control-\nControlNet++ 3\nCondition Generated Image Output\nControllable Pre-trained\nDiffusions RewardModels\nPrompt: heart, mountains, Hed Model\nand nature image Depth Model\nCanny Model\nSegmentation Model\nCycle Consistency ······\nFig.2: Illustration of the cycle consistency.Wefirstpromptthediffusionmodel\nG to generate an image x′ based on the given image condition c and text prompt c ,\n0 v t\nthen extract the corresponding image condition cˆ from the generated image x′ using\nv 0\npre-trained discriminative models D. The cycle consistency is defined as the similarity\nbetween the extracted condition cˆ and input condition c .\nv v\nNet [63] to generate corresponding images. Then the predicted segmentation\nmasks of these generated images can be obtained by a pre-trained segmentation\nmodel. Ideally, the predicted segmentation masks and the input segmentation\nmasks should be consistent. Hence, the cycle consistency loss can be formulated\nastheper-pixelclassificationlossbetweentheinputandpredictedsegmentation\nmasks. Unlike existing related works [27,30,37,63,65] that implicitly achieve\ncontrollability by introducing conditional controls into the latent-space denois-\ning process, our method explicitly optimizes controllability at the pixel-space\nfor better performance, as demonstrated in Fig. 3.\nToimplementpixel-levellosswithinthecontextofdiffusionmodels,anintu-\nitiveapproachinvolvesexecutingthediffusionmodel’sinferenceprocess,starting\nfrom random Gaussian noise and performing multiple sampling steps to obtain\nthe final generated images, following recent works focusing on improving image\nquality with human feedback [11,36,60]. However, multiple samplings can lead\nto efficiency issues, and require the storage of gradients at every timestep and\nthussignificanttimeandGPUmemoryconsumption.Wedemonstratethatiniti-\nating sampling from random Gaussian noise is unnecessary. Instead, by directly\nadding noise to training images to disturb their consistency with input condi-\ntional controls and then using single-step denoised images to reconstruct the\nconsistency,wecanconductmoreefficientrewardfine-tuning.Ourcontributions\nare summarized as:\n– New Insight: We reveal that existing efforts in controllable generation still\nperformpoorlyintermsofcontrollability,withgeneratedimagessignificantly\ndeviatingfrominputconditionsandlackingaclearstrategyforimprovement.\n– Consistency Reward Feedback: We show that pre-trained discriminative mod-\nels can serve as powerful visual reward models to improve the controllability\nof controllable diffusion models in a cycle-consistency manner.\n– Efficient Reward Fine-tuning: We disrupt the consistency between input im-\nages and conditions, and enable the single-step denoising for efficient reward\nfine-tuning, avoiding time and memory overheads caused by image sampling.\n– Evaluation and Promising Results:Weprovideaunifiedandpublicevaluation\nof controllability under various conditional controls, and demonstrate that\nControlNet++ comprehensively outperforms existing methods.\n4 Li et al.\n🔥 🔥\nEncoder Diffusion Encoder Diffusion Decoder\n: A large building Latent-space Pixel-spaceCycle\nwith a pointed roof Denoising Loss ConsistencyLoss\nand several chimneys.\n(a)ExistingMethods (b) Our Solution\nFig.3: (a) Existing methods achieve implicit controllability by introducing image-\nbased conditional control c into the denoising process of diffusion models, with the\nv\nguidanceoflatent-spacedenoisingloss.(b)WeutilizediscriminativerewardmodelsD\nto explicitly optimize the controllability of G via pixel-level cycle consistency loss.\n2 Related Work\n2.1 Diffusion-based Generative Models\nThediffusionprobabilisticmodelpresentedin[50]hasundergonesubstantialad-\nvancements [12,19,25], thanks to iterative refinements in training and sampling\nstrategies [18,51,52]. To alleviate the computational demands for training dif-\nfusion models, Latent Diffusion [43] maps the pixel space diffusion process into\nthe latent feature space. In the realm of text-to-image synthesis, diffusion mod-\nels [31,35,40,41,43,46] integrate cross-attention mechanisms between UNet [44]\ndenoisers and text embeddings from pre-trained language models like CLIP [38]\nandT5[39]tofacilitatereasonabletext-to-imagegeneration.Furthermore,diffu-\nsion models are employed across image editing tasks [3,14,24,29] by manipulat-\ninginputs[40],editingcross-attentions[16],andfine-tuningmodels[45].Despite\nthe astonishing capabilities of diffusion models, language is a sparse and highly\nsemantic representation, unsuitable for describing dense, low-semantic images.\nFurthermore, existing methods [35,43] still struggle to understand detailed text\nprompts, posing a severe challenge to the controllable generation [63].\n2.2 Controllable Text-to-Image Diffusion Models\nTo achieve conditional control in pre-trained text-to-image diffusion models,\nControlNet[63]andT2I-Adapter[30]introduceadditionaltrainablemodulesfor\nguided image generation. Furthermore, recent research employs various prompt\nengineering [27,61,64] and cross-attention constraints [6,23,58] for a more reg-\nulated generation. Some methods also explore multi-condition or multi-modal\ngeneration within a single diffusion model [21,37,65] or focus on the instance-\nbased controllable generation [54,69]. However, despite these methods exploring\nfeasibility and applications, there still lacks a clear approach to enhance con-\ntrollability under various controls. Furthermore, existing works implicitly learn\ncontrollability by the denoising process of diffusion models, while our Control-\nNet++achievesthisinanexplicitcycle-consistencymanner,asshowninFig.3.\n2.3 Linguistic and Visual Reward Models\nThe reward model is trained to evaluate how well the results of generative mod-\nels align with human expectations, and its quantified results will be used to\nControlNet++ 5\nfacilitate generative models for better and more controllable generation. It is\nusually trained with reinforcement learning from human feedback (RLHF) in\nNLP tasks [10,32,53], and has recently extended into the vision domain to im-\nprove the image quality for text-to-image diffusion models [1,11,13,36,56,60].\nHowever, image quality is an exceedingly subjective metric, fraught with indi-\nvidualpreferences,andrequiresthecreationofnewdatasetswithhumanprefer-\nences[26,55,56,60]andthetrainingofrewardmodels[36,55,60].Divergingfrom\nthe pursuit of global image quality with subjective human preference in current\nresearch, we target the more fine-grained and objective goal of controllability.\nAlso,it’smorecost-effectivetoobtainAIfeedbackcomparedtohumanfeedback.\n3 Method\nInthissection,wefirstintroducethebackgroundofdiffusionmodelsinSec.3.1.\nIn Sec. 3.2, we discuss how to design the cycle consistency loss for controllable\ndiffusion models to enhance the controllability. Finally, in Sec. 3.3, we examine\nthe efficiency issues with the straightforward solution and correspondingly pro-\nposeanefficientrewardstrategythatutilizesthesingle-stepdenoisedimagesfor\nconsistency loss, instead of sampling images from random noise.\n3.1 Preliminary\nThe diffusion models [18] define a Markovian chain of diffusion forward process\nq(x |x ) by gradually adding noise to input data x :\nt 0 0\n√ √\nx = α¯ x + 1−α¯ ϵ, ϵ∼N(0,I), (1)\nt t 0 t\nwhereϵisanoisemapsampledfromaGaussiandistribution,andα¯\n:=(cid:81)t\nα .\nt s=0 s\nα =1−β is a differentiable function of timestep t, which is determined by the\nt t\ndenoising sampler such as DDPM [18]. To this end, the diffusion training loss\ncan be represented by:\nL(ϵ θ)=(cid:88)T E x0∼q(x0),ϵ∼N(0,I)(cid:104)(cid:13) (cid:13)ϵ θ(cid:0)√ α¯ tx 0+√ 1−α¯ tϵ(cid:1) −ϵ(cid:13) (cid:13)2 2(cid:105) . (2)\nt=1\nIn the context of controllable generation [30,63], with given image condition c\nv\nand text prompt c , the diffusion training loss at timestep t can be re-written\nt\nas:\nL =E (cid:2) ∥ϵ (x ,t,c ,c )−ϵ∥2(cid:3) . (3)\ntrain x0,t,ct,cv,ϵ∼N(0,1) θ t t v 2\nDuring the inference, given a random noise x ∼ N(0,I), we can predict\nT\nfinal denoised image x with the step-by-step denoising process [18]:\n0\n(cid:18) (cid:19)\n1 1−α\nx = √ x − √ t ϵ (x ,t) +σ ϵ, (4)\nt−1 α t 1−α¯ θ t t\nt t\nwhereϵ referstothepredictednoiseattimesteptbyU-Net[44]withparameters\nθ\nθ, and σ = 1−α¯t−1β is the variance of posterior Gaussian distribution p (x ).\nt 1−α¯t t θ 0\n6 Li et al.\n3.2 Reward Controllability with Consistency Feedback\nAswemodelcontrollabilityastheconsistencybetweeninputconditionsandthe\ngenerated images, we can naturally quantify this outcome through the discrim-\ninative reward models. Once we quantify the results of the generative model,\nwe can perform further optimization for more controllable generation based on\nthese quantified results in a unified manner for various conditional controls.\nTo be more specific, we minimize the consistency loss between the input\ncondition c and the corresponding output condition cˆ of the generated image\nv v\nx′, as depicted in Fig. 2. The reward consistency loss can be formulated as:\n0\nL =L(c ,cˆ )\nreward v v\n=L(c ,D(x′)) (5)\nv 0\n=L(cid:0)\nc\n,D(cid:2)GT\n(c ,c ,x\n,t)(cid:3)(cid:1)\n,\nv t v T\nwhere GT (c ,c ,x ,t) denotes the process that the model performs T denois-\nt v T\ning steps to generate the image x′ from random noise x , as shown in the\n0 T\nFig. 4 (a). Here, L is an abstract metric function that can take on different con-\ncrete forms for different visual conditions. For example, in the context of using\nsegmentation mask as the input conditional control, L could be the per-pixel\ncross-entropy loss. The reward model D is also dependent on the condition, and\nwe use the UperNet [57] for segmentation mask conditions. The details of loss\nfunctions and reward models are summarized in the supplementary material.\nInadditiontotherewardloss,wealsoemploydiffusiontraininglossinEq.3\ntoensurethattheoriginalimagegenerationcapabilityisnotcompromisedsince\nthey have different optimization goals. Finally, the total loss is the combination\nof L and L :\ntrain reward\nL =L +λ·L , (6)\ntotal train reward\nwhere λ is a hyper-parameter to adjust the weight of the reward loss. Through\nthis approach, the consistency loss can guide the diffusion model on how to\nsample at different timesteps to obtain images more consistent with the input\ncontrols, thereby enhancing controllability. Nonetheless, directly applying such\nreward consistency still poses challenges in efficiency in real-world settings.\n3.3 Efficient Reward Fine-tuning\nTo achieve the pixel-space consistency loss L , it requires x , the final dif-\nreward 0\nfused image, to calculate the reward consistency from the reward models. As\nmodern diffusion models, such as Stable Diffusion [43], require multiple steps,\ne.g.,50steps,torenderafullimage,directlyusingsuchasolutionisimpractical\nin realistic settings: (1) multiple time-consuming samplings are required to de-\nriveimagesfromrandomnoise.(2)toenablegradientbackpropagation,wehave\nto store gradients at each timestep, meaning the GPU memory usage will in-\ncreaselinearlywiththenumberoftime-steps.TakingControlNetasanexample,\nControlNet++ 7\n… …\nAdd\nEq. (5) Noise Eq. (8)\n… …\nEq. (4) Eq.(4) 50x Inference Eq. (1) Eq.(7) 1x Inference\nMulti-step Sampling (e.g., 50 steps) Time & Memory DisturbConsistency Single-stepSampling Time & Memory\n(a) Default Reward Strategy (b)EfficientRewardStrategy(Ours)\nFig.4:(a)Pipelineofdefaultrewardfine-tuningstrategy.Rewardfine-tuningrequires\nsampling all the way to the full image. Such a method needs to keep all gradients for\neach timestep and the memory required is unbearable by current GPUs. (b) Pipeline\nofourefficientrewardstrategy.Weaddasmallnoise(t≤t )todisturbtheconsis-\nthre\ntency between input images and conditions, then the single-step denoised image can\nbe directly used for efficient reward fine-tuning.\nwhen the batch size is 1 with FP16 mixed precision, the GPU memory required\nfor a single denoising step and storing all training gradients is approximately\n6.8GB. If we use the 50-step inference with the DDIM [51] scheduler, approxi-\nmately 340GB of memory is needed to perform reward fine-tuning on a single\nsample,whichisnearlyimpossibletoachievewithcurrenthardwarecapabilities.\nAlthough GPU memory consumption can be reduced by employing techniques\nsuch as Low-Rank Adaptation (LoRA) [11,20], gradient checkpointing [7,11], or\nstop-gradient [60], the efficiency degradation caused by the number of sampling\nsteps required to generate images remains significant and cannot be overlooked.\nTherefore, an efficient reward fine-tuning approach is necessary.\nIn contrast to diffusing from random noise x to obtain the final image\nT\nx , as illustrated in Fig. 4 (a), we instead propose an one-step efficient reward\n0\nstrategy. Specifically, instead of randomly sampling from noise, we add noise to\nthe training images x , thereby explicitly disturbing the consistency between\n0\nthe diffusion inputs x′ and their conditional controls c , by performing diffusion\nt v\nforward process q(x |x ) in Eq. 1. We demonstrate this process as the Disturb\nt 0\nConsistencyinFig.4(b),whichisthesameprocedureasthestandarddiffusion\ntraining process. When the added noise ϵ is relatively small, we can predict the\noriginalimagex′ byperformingsingle-stepsampling3ondisturbedimagex′ [18]:\n0 t\n√\nx′ − 1−α ϵ (x′,c ,c ,t)\nx ≈x′ = t √t θ t v t , (7)\n0 0 α\nt\nandthenwedirectlyutilizethedenoisedimagex′ toperformrewardfine-tuning:\n0\nL =L(c ,cˆ )=L(c ,D(x′))=L(c ,D[G(c ,c ,x′,t)]). (8)\nreward v v v 0 v t v t\nEssentially, the process of adding noise destroys the consistency between the\ninput image and its condition. Then the reward fine-tuning in Eq. 8 instructs\nthediffusionmodeltogenerateimagesthatcanreconstructtheconsistency,thus\nenhancing its ability to follow conditions during generation.\n3 We provide a more detailed proof in the supplementary material.\n8 Li et al.\nPlease note that here we avoid the sampling process in Eq. 5. Finally, the\nloss is the combination of diffusion training loss and the reward loss:\n(cid:40)\nL +λ·L , if t≤t\nL = train reward thre, (9)\ntotal\nL , otherwise,\ntrain\nwhere t denotes the timestep threshold, which is a hyper-parameter used to\nthre\ndetermine whether a noised image x should be utilized for reward fine-tuning.\nt\nWe note that a small noise ϵ (i.e., a relatively small timestep t) can disturb\nthe consistency and lead to effective reward fine-tuning. When the timestep t is\nlarge, x is closer to the random noise x , and predicting x′ directly from x\nt T 0 t\nresults in severe image distortion. The advantage of our efficient rewarding is\nthat x can be employed both to train and reward the diffusion model without\nt\nthe need for time and GPU memory costs caused by multiple sampling, thereby\nsignificantly improving the efficiency during the reward fine-tuning stage.\nDuring the reward fine-tuning phases, we freeze the pre-trained discrimina-\ntive reward model and text-to-image model, and only update the ControlNet\nfollowing its original implementation, which ensures the generative capabilities\nare not compromised. We also observe that using only the reward loss will lead\ntoimagedistortion,aligningwiththeconclusionsdrawninpreviousstudies[60].\n4 Experiments\n4.1 Experimental Setup\nConditionControlsandDatasets. Giventhatexistingtext-imagepaireddatasets\nfor generative models are unable to provide accurate conditional control data\npairs [48,49], such as image-segmentation pairs, we endeavor to select specific\ndatasets for different tasks that can offer more precise image-label data pairs.\nMore specifically, ADE20K [67,68] and COCOStuff [4] are used for the segmen-\ntation mask condition following ControlNet [63]. For the canny edge map, hed\nedge map, lineart map, and depth map condition, we utilize the MultiGen-20M\ndatasetproposedbyUniControl[37],whichisasubsetofLAION-Aesthetics[48].\nFor the datasets without text caption such as ADE20K, we utilize MiniGPT-\n4[70]togeneratetheimagecaptionwiththeinstruction“Please briefly describe\nthisimageinonesentence”.Thetrainingandinferenceresolutionis512×512for\nall datasets and methods. Details are provided in the supplementary material.\nEvaluation and Metrics. We train ControlNet++ on the training set of each\ncorresponding dataset and evaluate all methods on the validation dataset. All\nthe experiments are evaluated under 512×512 resolution for fair comparison.\nFor each condition, we evaluate the controllability by measuring the similarity\nbetweentheinputconditionsandtheextractedconditionsfromgeneratedimages\nof diffusion models. For semantic segmentation and depth map controls, we use\nmIoUandRMSEasevaluationmetricsrespectively,whichisacommonpractice\ninrelatedresearchfields.Fortheedgetask,weuseF1-Scoreforhardedges(canny\nControlNet++ 9\nTable1:Controllabilitycomparisonwithstate-of-the-artmethodsunderdifferentcon-\nditional controls and datasets. ↑ denotes higher result is better, while ↓ means lower\nisbetter.ControlNet++achievessignificantcontrollabilityimprovements.‘-’indicates\nthatthemethoddoesnotprovideapublicmodelfortesting.Wegeneratefourgroups\nof images in png format and report the average result to reduce random errors.\nCondition Seg.Mask CannyEdge HedEdge LineArtEdge DepthMap\nT2I\n(Metric) (mIoU↑) (F1Score↑) (SSIM↑) (SSIM↑) (RMSE↓)\nModel\nDataset ADE20KCOCO-StuffMultiGen-20MMultiGen-20MMultiGen-20MMultiGen-20M\nControlNet SDXL - - - - - 40.00\nT2I-Adapter SDXL - - 28.01 - 0.6394 39.75\nT2I-Adapter SD1.5 12.61 - 23.65 - - 48.40\nGligen SD1.4 23.78 - 26.94 0.5634 - 38.83\nUni-ControlNet SD1.5 19.39 - 27.32 0.6910 - 40.65\nUniControl SD1.5 25.44 - 30.82 0.7969 - 39.18\nControlNet SD1.5 32.55 27.46 34.65 0.7621 0.7054 35.90\nOurs SD1.5 43.64 34.56 37.04 0.8097 0.8399 28.32\nedge) because it can be regarded as a binary classification problem of 0 (non-\nedge)and1(edge)andhasaseriouslong-taildistribution,followingthestandard\nevaluationinedgedetection[59].Thethresholdusedforevaluationis(100,200)\nforOpenCV,and(0.1,0.2)forKorniaimplementation.TheSSIMmetricisused\nfor the soft edges conditional controls (i.e., hed edge & lineart edge) following\nprevious works [65]. For ControlNet++, we use the UniPC [66] sampler with\n20 denoising steps to generate images with the original text prompt following\nControlNet v1.1 [63], without any negative prompts. For other methods beyond\nControlNet and ours, we utilized their open-source code to generate images and\nconductedfairevaluationsunderthesamedata,withoutchangingtheirinference\nconfigures such as the number of inference steps or denoising sampler.\nBaselines. Our evaluation primarily focuses on T2I-Adapter [30], ControlNet\nv1.1[63],GLIGEN[27],Uni-ControlNet[65],andUniControl[37],asthesemeth-\nodsarepioneeringintherealmofcontrollabletext-to-imagediffusionmodelsand\noffer public model weights for various image conditions. To ensure fairness of\nevaluation, all methods use the same image conditions and text prompts. While\nmost methods employ the user-friendly SD1.5 as their text-to-image model for\ncontrollable generation, we have observed that recently there are a few mod-\nels based on SDXL [35]. Therefore, we also report the controllability results for\nControlNet-SDXL and T2I-Adapter-SDXL. Please note that ControlNet-SDXL\nmentioned here is not an officially released model as in ControlNet [63].\n4.2 Experimental Results\nComparison of Controllability with State-of-the-art Methods. The experimental\nresults are shown in Tab. 1, which can be summarized as the following ob-\nservations: (1) Existing methods still underperform in terms of controllability,\nstruggling to achieve precise controlled generation. For instance, current meth-\nods (i.e., ControlNet) achieve only a 32.55 mIoU for images generated under\nthe condition of segmentation masks, which is far from its performance on real\ndatasets with a 50.7 mIoU, under the same evaluation from Mask2Former seg-\nmentation model [8]. (2) Our ControlNet++ significantly outperforms existing\n10 Li et al.\nTable2:FID(↓)comparisonwithstate-of-the-artmethodsunderdifferentconditional\ncontrolsanddatasets.Alltheresultsareconductedon512×512imageresolutionwith\nClean-FIDimplementation[33]forfaircomparisons.‘-’indicatesthatthemethoddoes\nnotprovideapublicmodelfortesting.Wegeneratefourgroupsofimagesinpngformat\nand report the average result to reduce random errors.\nT2I Seg.Mask CannyEdge HedEdge LineArtEdge DepthMap\nMethod\nModelADE20KCOCOMultiGen-20MMultiGen-20MMultiGen-20MMultiGen-20M\nGligen SD1.4 33.02 - 18.89 - - 18.36\nT2I-Adapter SD1.5 39.15 - 15.96 - - 22.52\nUniControlNet SD1.5 39.70 - 17.14 17.08 - 20.27\nUniControl SD1.5 46.34 - 19.94 15.99 - 18.66\nControlNet SD1.5 33.28 21.33 14.73 15.41 17.44 17.76\nOurs SD1.5 29.49 19.29 18.23 15.01 13.88 16.66\nTable 3: CLIP-score (↑) comparison with state-of-the-art methods under different\nconditional controls and datasets. ‘-’ indicates that the method does not provide a\npublic model for testing. We generate four groups of images in png format and report\nthe average result to reduce random errors.\nT2I Seg.Mask CannyEdge HedEdge LineArtEdge DepthMap\nMethod\nModelADE20KCOCOMultiGen-20MMultiGen-20MMultiGen-20MMultiGen-20M\nGligen SD1.4 31.12 - 31.77 - - 31.75\nT2I-Adapter SD1.5 30.65 - 31.71 - - 31.46\nUniControlNet SD1.5 30.59 - 31.84 31.94 - 31.66\nUniControl SD1.5 30.92 - 31.97 32.02 - 32.45\nControlNet SD1.5 31.53 13.31 32.15 32.33 32.46 32.45\nOurs SD1.5 31.96 13.13 31.87 32.05 31.95 32.09\nworksintermsofcontrollabilityacrossvariousconditionalcontrols.Forexample,\nitachieves11.1%RMSEimprovementsagainstpreviousstate-of-the-artmethods\nfor the depth map condition; (3) For controllable diffusion models, the strength\nof the text-to-image backbone does not affect its controllability. As shown in\nthe table, although SDXL-based [35] ControlNet and T2I-Adapter have better\ncontrollability on some specific tasks, the improvement is not large and is not\nsignificantly better than the counterparts with SD 1.5 [43].\nComparison of Image Quality with State-of-the-art Methods. To verify whether\nimproving controllability leads to a decline in image quality, we reported the\nFID (Fréchet Inception Distance) metrics of different methods under various\nconditionalgenerationtasksinTab.2.Wediscoveredthat,comparedtoexisting\nmethods, ControlNet++ generally exhibits superior FID values in most cases,\nindicating that our approach, while enhancing the controllability of conditional\ncontrols,doesnotresultinadecreaseinimagequality.Thiscanalsobeobserved\nin Fig. 6. We provide more visual examples in the supplementary material.\nComparison of CLIP score with State-of-the-art Methods. Our ControlNet++\naims to improve the controllability of diffusion models using image-based condi-\ntions. Concerned about the potential adverse effects on text controllability, we\nevaluatedvariousmethodsusingCLIP-Scoremetricsacrossdifferentdatasetsto\nmeasurethesimilaritybetweengeneratedimagesandinputtext.Asindicatedin\nTab.3,ControlNet++achievedcomparableorsuperiorCLIP-Scoreoutcomeson\nseveral datasets relative to existing approaches. This suggests that our method\nnot only markedly enhances conditional controllability but also preserves the\noriginal model’s text-to-image generation proficiency.\nControlNet++ 11\nGenerated Images Ground Truth Images GT + Generated Images\nFig.5: TrainingDeepLabv3(MobileNetv2)fromscratchwithdifferentimages,includ-\ning ground truth images from ADE20K, and the generated images from ControlNet\nandours.Allthelabels(i.e.,segmentationmasks)aregroundtruthlabelsinADE20K.\nPlease note improvements here are non-trivial for semantic segmentation.\nEffectiveness of Generated Images. To further validate our improvements in\ncontrollability and their impact, we use the generated images along with real\nhuman-annotatedlabelstocreateanewdatasetfortrainingdiscriminativemod-\nels from scratch. Please note that the only difference from the original dataset\nused to train the discriminative model is that we have replaced the images with\nthose generated by the controllable diffusion model while keeping the labels un-\nchanged. If the generative model exhibits good controllability, the quality of the\nconstructed dataset will be higher, thereby enabling to train a stronger model.\nSpecifically, we conduct experiments on the ADE20K [67,68] dataset on\nDeepLabv3withMobileNetv2backbone[5].Weusethestandardtrainingdataset\n(20210 training samples) to train the discriminative model and the validation\ndataset (5000 evaluation samples) for evaluation. We show the experimental re-\nsults in Fig. 5, the segmentation model trained on our images outperforms the\nbaseline results (ControlNet) by 1.19 mIoU. Please note that this improve-\nment is significant in segmentation tasks. For instance, Mask2Former [8]\nimproves previous SOTA MaskFormer [9] with around 1.1 mIoU in semantic\nsegmentation. In addition to conducting experiments solely on the generated\ndataset, we also combined generated data with real data to train the segmenta-\ntionmodel.Theexperimentalresultsindicatethataugmentingrealgroundtruth\ndata with data generated by ControlNet does not yield additional performance\nimprovements (34.11 v.s. 34.08). In contrast, augmenting real data with our\ngenerated data results in significant performance enhancements (+1.76 mIoU).\nQualitativeComparison. Figs.6and7provideaqualitativecomparisonbetween\nour ControlNet++ and previous state-of-the-art methods across different con-\nditional controls. When given the same input text prompts and image-based\nconditional controls, we observe that existing methods often generate areas\n12 Li et al.\nImage & Condition Ours Uni-ControlNet UniControl Gligen ControlNet T2I-Adapter\nImage & Condition Ours Uni-ControlNet UniControl Gligen ControlNet\nFig.6: Visualization comparison results in different conditional controls.\nImage & Condition Ours ControlNet T2I-Adapter\ninconsistent with the image conditions.\nFor instance, in the segmentation mask\ngenerationtask,othermethodsoftenpro-\nduce extraneous picture frames on the\nwalls,resultinginamismatchbetweenthe\nsegmentation masks extracted from the\ngenerated image and the inputs. A sim- Fig.7: Comparison on Line-Art Edge.\nilar situation occurs under depth conditions, where other methods fail to accu-\nrately represent the depth of different fingers. In contrast, images generated by\nControlNet++ maintain good consistency with the input depth map.\nnoitatnemgeS\nhtpeD\nynnaC\ndeH\ntrA-eniL\nControlNet++ 13\n4.3 Ablation Study\nLoss Settings. In Fig. 8, we find that maintaining the original diffusion train-\ning process is crucial for preserving the quality and controllability of generated\nimages. Relying solely on pixel-level consistency loss leads to severe image dis-\ntortion,whereastrainingthemodelwithboththislossandthediffusiontraining\nloss can enhance controllability without affecting image quality.\n50Steps 100Steps 200Steps 400Steps 50Steps 100Steps 200Steps 400Steps\nInputCondition\nPrompt:A pelican\ngracefully takes off\nfrom the calm water. WithoutDiffusionTrainingLoss WithDiffusionTrainingLoss\nFig.8: Ablationstudyondifferentlosssettingsduringtraining.Usingonlypixel-level\nconsistency loss leads to severe image distortion, affecting both image quality and\ncontrollability. However, when combined with diffusion training loss, it is possible to\ngradually improve controllability without compromising image quality.\nGeneralizability of Efficient Reward Fine-tuning. Although the reward fine-\ntuning is used in a small subset of timesteps, it updates all the parameters\nof the ControlNet and therefore helps more timesteps to improve controllability\nduring sampling. To prove this, we divide the sampling process into two parts:\nthe unoptimized timesteps [T,t ], and the optimized timesteps [t ,1] and\nthre thre\nuseControlNetandourmodelforinferencecrossly,with20-stepsamplingfollow-\ning ControlNet. Table 4 shows that our reward finetuning performed on a small\nnumber of timesteps [t ,1] can be generalized to larger timesteps [T,t ].\nthre thre\nTable 4: The impact of efficient re- Table 5: Stronger reward model (UperNet-\nwardfine-tuningondifferenttimesteps. R50) leads to better controllability than the\nweaker reward model (DeepLabv3-MBv2).\nUnoptimized Optimized ADE20K\n[T,t ] [t ,1] mIoU (↑) RewardModel(RM)RMmIoU↑EvalmIoU↑\nthre thre\nControlNet ControlNet 32.55 - - 32.55\nControlNet Ours 38.03 DeepLabv3-MBv2 34.02 31.96\nOurs ControlNet 41.46 FCN-R101 39.91 40.44\nUperNet-R50 42.05 43.64\nOurs Ours 43.64\nChoiceofDifferentRewardModels. Wedemonstratetheeffectivenessofdifferent\nreward models in Tab. 5, all the evaluation results (i.e., Eval mIoU in the table)\nare evaluated by the most powerful segmentation model Mask2Former [8] with\n56.01 mIoU, on ADE20K dataset. We experiment with three different reward\nmodels, including DeepLabv3 [5] with MobileNetv2 [47] backbone (DeepLabv3-\nMBv2),FCN[28]withResNet-101[15]backbone(FCN-R101)andUperNet[57]\nwithResNet-50backbone.Theresultsdemonstratethatamorepowerfulreward\nmodel leads to better controllability for controllable diffusion models.\n14 Li et al.\n5 Discussion\nHowtomakeHed/LineArtEdgeextractionmethodsdifferentiable? TheHedand\nLineArt Edge extraction models are neural networks without non-differentiable\noperations. Differentiability can be achieved by simply modifying forward code.\nSome conditions (e.g., Box/Sketch/Pose) are not available. Our reward fine-\ntuning leverages a pre-trained ControlNet and a differentiable reward model.\nCurrently, pre-trained ControlNet for object bounding boxes and differentiable\nreward models for sketches are lacking. In existing pose models, there are non-\ndifferentiableoperationssuchastheNMSandkeypointsgrouping.Weleavethe\nquestionofhowtoextendconsistencyrewardtomoreconditionstofuturework.\nInfluence of Text Prompt. We discuss how different types of text prompts (No\nPrompt, Conflicting Prompt, and Perfect Prompt) affect the final results. As\nshown in Fig. 9, when the text prompt is empty or there is a semantic conflict\nwiththeimageconditionalcontrol,ControlNetoftenstrugglestogenerateaccu-\nrate content. In contrast, our ControlNet++ manages to generate images that\ncomplywiththeinputconditionalcontrolsundervarioustextpromptscenarios.\nNoPrompt Conflict Prompt Perfect Prompt\n“delicious cake” “a house, high-quality, extremely detailed, 4K”\nFig.9: Whentheinputtextpromptisemptyorconflictswiththeimage-basedcondi-\ntional controls (the segmentation map in the top left corner), ControlNet struggles to\ngeneratecorrectcontent(redboxes),whereasourmethodmanagestogenerateitwell.\n6 Conclusion\nIn this paper, we demonstrate from both quantitative and qualitative perspec-\ntives that existing works focusing on controllable generation still fail to achieve\nprecise conditional control, leading to inconsistency between generated images\nand input conditions. To address this issue, we introduce ControlNet++, it ex-\nplicitlyoptimizestheconsistencybetweeninputconditionsandgeneratedimages\nusing a pre-trained discriminative reward model in a cycle consistency manner,\nwhich is different from existing methods that implicitly achieve controllability\nthrough latent diffusion denoising. We also propose a novel and efficient reward\nstrategy that calculates consistency loss by adding noise to input images fol-\nlowed by single-step denoising, thus avoiding the significant computational and\nmemory costs associated with sampling from random Gaussian noise. Exper-\nimental results under multiple conditional controls show that ControlNet++\nsignificantly improves controllability without compromising image quality and\nimage-text alignment, offering new insights into controllable visual generation.\nsruO\nteNlortnoC\nControlNet++: Improving Conditional Controls\nwith Efficient Consistency Feedback\nSupplementary Material\nMing Li1, Taojiannan Yang1, Huafeng Kuang2, Jie Wu2,\nZhaoning Wang1, Xuefeng Xiao2, and Chen Chen1\n1 Center for Research in Computer Vision, University of Central Florida\n2 ByteDance\n1 Overview of Supplementary\nThe supplementary material is organized into the following sections:\n– Section 2: Implementation details for all experiments.\n– Section 3: Proof for Eq.(7) in the main paper.\n– Section 4: More experiments and analysis.\n• Section 4.1: Effectiveness of conditioning scale of existing methods.\n• Section 4.2: Human evaluation on controllability, text guidance and im-\nage quaility.\n– Section 5: Discussion of broader impact and limitation.\n– Section 6: More visualization results.\n2 Implementation Details\n2.1 Dataset Details\nConsidering that the training data for ControlNet [63] has not been publicly\nreleased, we need to construct our training dataset. In this paper, we adhere to\nthe dataset construction principles of ControlNet [63], which endeavor to select\ndatasets with more accurate conditional conditions wherever possible. Specifi-\ncally,forthesegmentationcondition,previousworkshaveprovideddatasetswith\naccurately labeled segmentation masks [4,67,68]. Therefore, we opt to train our\nmodelusingtheseaccuratelylabeleddatasetsfollowingControlNet[63].Forthe\nHed,LineArtedgetasks,itischallengingtofinddatasetswithrealandaccurate\nannotations. As a result, following ControlNet [63], we train the model using\nthe MultiGen20M dataset [37], which is annotated by models, to address this\nissue.Regardingthedepthtask,existingdatasetsincludemasksofcertainpixels\nas having unknown depth values, making them incompatible with the current\nControlNet pipeline. Therefore, we also adapt the MultiGen20M depth dataset,\nwhich is similar to the dataset constructed by ControlNet [63]. In terms of the\ncanny edge task, no human labels are required in the process, so we also adapt\nthe MultiGen20M dataset. We provide details of the datasets in Table 1.\n16 Li et al.\nTable 1: Dataset and evaluation details of different conditional controls. ↑ denotes\nhigher is better, while ↓ means lower is better.\nSegmentationMask CannyEdge HedEdge LineArtEdge DepthMap\nDataset ADE20K[67,68],COCOStuff[4]MultiGen20M[37]MultiGen20M[37]MultiGen20M[37]MultiGen20M[37]\nTrainingSamples 20,210&118,287 2,560,000 2,560,000 2,560,000 2,560,000\nEvaluationSamples 2,000&5,000 5,000 5,000 5,000 5,000\nEvaluationMetric mIoU↑ F1Score↑ SSIM↑ SSIM↑ RMSE↓\nTable 2: Details of the reward model, evaluation model, and training loss under dif-\nferent conditional controls. ControlNet* denotes we use the same model to extract\nconditions as ControlNet [63]\nSeg.Mask DepthEdge CannyEdge HedEdge LineArtEdge\nRewardModel(RM) UperNet-R50 DPT-Hybrid KorniaCannyControlNet* ControlNet*\nRMPerformance ADE20K(mIoU):42.05NYU(AbsRel):8.69 - - -\n. EvaluationModel(EM) Mask2Former DPT-Large KorniaCannyControlNet* ControlNet*\nEMPerformance ADE20K(mIoU):56.01NYU(AbsRel):8.32 - - -\nConsistencyLoss CrossEntropyLoss MSELoss MSELoss MSELoss MSELoss\nLossWeightλ 0.5 0.5 1.0 1.0 10\n2.2 Reward Model and Evaluation Details\nIn general, we deliberately choose slightly weaker models as the reward model\nand opt for stronger models for evaluation. This practice not only ensures the\nfairness of the evaluation but also helps to determine whether performance im-\nprovements result from alignment with the reward model’s preferences or from\na genuine enhancement in controllability. While such an approach is feasible\nfor some tasks (Segmentation, Depth), it becomes challenging to implement for\nothers (Hed, Canny, LineArt Edge) due to the difficulty in finding two distinct\nreward models. In such cases, we use the same model as both the reward model\nandtheevaluationmodel.Weutilizestandardevaluationschemesfromtheirre-\nspectiveresearchfieldstoevaluatetheinputconditionsandextractedconditions\nfrom the generated images, as demonstrated in Section 4.1 of the main paper.\nWeusethesameHededgedetectionmodelandLineArtedgedetectionmodelas\nControlNet[63]. We providedetailsofrewardmodelsandevaluationinTable2.\n2.3 Training Details\nThe loss weight λ for reward consistency loss is different for each condition.\nSpecifically, λ is 0.5, 0.5, 1.0, 1.0, and 10 for segmentation mask, depth, hed\nedge, canny edge, and LineArt edge condition, respectively. For all experiments,\nwe first fine-tune the pre-trained ControlNet until convergence using a batch\nsize of 256 and a learning rate of 1e-5. We then employ the same batch size and\nlearningratefor10kiterationsrewardfine-tuning.Tothisend,thevalidtraining\nsamples for reward fine-tuning is 256×10,000 = 2,560,000. We set threshold\nt =200ofEq.8inthemainpaperforallexperiments.Divergingfromexisting\nthre\nmethodsthatuseOpenCV’s[2]implementationofthecannyalgorithm,wehave\nadopted Kornia’s [42] implementation to make it differentiable. Our codebase is\nbased on the implementation in HuggingFace’s Diffusers [34], and we do not use\nclassifier-free guidance during the reward fine-tuning process following diffusers.\nControlNet++ 17\nImage Condition 1000 900 800 700 600 500 400 300 200 100\nPredicted atdifferenttimestep\nFig.1: Illustration of predicted image x′ at different timesteps t. A small timestep t\n0\n(i.e., small noise ϵ ) leads to more precise estimation x′ ≈x .\nt 0 0\n3 Proof of Equation 7 in the Main Paper\nThe diffusion models define a Markovian chain of diffusion forward process\nq(x |x ) by gradually adding noise to input data x :\nt 0 0\n√ √\nx = α¯ x + 1−α¯ ϵ, ϵ∼N(0,I), (1)\nt t 0 t\nat any timestep t we can use the predicted ϵ(x′,c ,c ,t−1) to estimate the real\nt v t\nnoise ϵ in Eq. 1, and the above equation can be transformed through straight-\nforward algebraic manipulation to the following form:\n√ √\nx ≈ α¯ x + 1−α¯ ϵ (x′,c ,c ,t−1),\nt t 0 t θ t v t\n√\nx′ − 1−α ϵ (x′,c ,c ,t−1) (2)\nx ≈x′ = t t √θ t v t .\n0 0 α\nt\nTo this end, we can obtain the predicted original image x′ at any denoising\n0\ntimesteptanduseitastheinputforrewardconsistencyloss.However,previous\nwork demonstrates that this approximation only yields a smaller error when\nthe time step t is relatively small [18]. Here we find similar results as shown in\nFigure 1, which illustrates the predicted x′ is significantly different at different\n0\ntimesteps. We kindly encourage readers to refer to Section 4.3 and Figure 5 in\nthe DDPM [18] paper for more experimental results.\n4 More Experiments\nIn this section, we provide additional supplements to the experiments discussed\nin the main paper, including human evaluation on generated data samples on\nthe Segmentation Mask condition in Sec. 4.2, analysis on conditioning scale of\nexisting methods such as ControlNet [63] and T2I-Adapter [30] in Sec. 4.1.\nImage&Condition Control Scale: 0.5 ControlScale:1.0 ControlScale:2.0 Control Scale: 3.0 Control Scale: 4.0 ControlScale:10.0\n18 Li et al.\nFig.2: Naively increasing the weight of image condition embedding compared to text\ncondition embedding in exiting methods (i.e., ControlNet and T2I-Adapter) cannot\nimprovecontrollabilitywhileensuringimagequality.Theredboxesinthefigureshigh-\nlightareaswherethegeneratedimageisinconsistentwiththeinputconditions.Please\nnotethatweemploythesamelinedetectionmodeltoextractconditionsfromimages.\n4.1 Effectiveness of Conditioning Scale\nTo simultaneously achieve control based on text prompts and image conditions,\nexistingcontrollablegenerationmethodsperformanadditionoperationbetween\nthe image condition features and the text embedding features. The strength of\ndifferent conditions can be adjusted through a weight value. Hence, an obvious\nquestionarises:canbettercontrollabilitybeachievedbyincreasingtheweightof\nthe image condition features? To answer this question, we conduct experiments\nunderdifferentcontrolscales(Theweightofimageconditionfeature)inFigure2.\nItdemonstratesthatnaivelyincreasingthecontrolratioofimageconditionsdoes\nnot enhance controllability and may lead to severe image distortion.\n4.2 Human Evaluation\nFollowingControlNet,weuseasingleconditionforhumanevaluation.Weask20\nusers (12 in ControlNet paper) to select the best image based on three distinct\ncriteria as shown in Table 3. Our ControlNet++ offers better controllability\nwithout sacrificing image quality or text guidance.\nTable 3: Win rate on ADE20K validation dataset (Segmentation).\n20 annotators in total Ours ControlNet T2I-Adapter UniControl\nImage-Mask Alignment 76.8% 16.7% 2.0% 4.5%\nImage Quality 26.1% 25.8 % 23.6% 24.5 %\nImage-Text Alignment 25.3% 25.1% 24.9% 24.7%\nteNlortnoC\nLXDS-retpadA-I2T\ndetareneG\ndetcartxE\ndetareneG\ndetcartxE\nsegamI\nsnoitidnoC\nsegamI\nsnoitidnoC\nControlNet++ 19\n5 Broader Impact and Limitation\nIn this paper, we use visual discriminative models to evaluate and improve the\ncontrollability of text-to-image models. However, we also realize that this work\nis still insufficient and discuss the following issues:\nConditions Expansion: While we have achieved notable improvements under\nsixcontrolconditions,ourfutureworkaimstobroadenthescopebyincorporat-\ningadditionalcontrolconditionssuchasHumanPoseandScribbles.Ultimately,\nour objective is to control everything.\nBeyond Controllability: While our current focus lies predominantly on con-\ntrollability,weacknowledgethesignificanceofqualityandaestheticappealinthe\ngenerated outputs. To address this, we plan to leverage human feedback to an-\nnotate controllability images. Subsequently, we will optimize the controllability\nmodel to simultaneously enhance both controllability and aesthetics.\nJoint Optimization: To further enhance the overall performance, we intend\nto employ a larger set of controllable images for joint optimization of the con-\ntrol network and reward model. This holistic approach would facilitate their\nco-evolution, leading to further improvements in the final generated outputs.\nThrough our research, we aspire to provide insightful contributions to control-\nlability in text-to-image diffusion models. We hope that our work inspires and\nencourages more researchers to delve into this fascinating area.\nDiscussion on the necessity of controllability:Controllabilityisimportant\nsince it allows users to modify image conditions to achieve more flexible and\naccurate generation. Take LineArt Edge as an example: (1) Freely generating in\nforeground may change the appearance (e.g., adding a beard for women) that\nweusuallydonotexpect.(2)Freelygeneratinginbackgroundwilldamagesome\napplications (e.g., blur background). (3) Global free generating may destroy the\noverall artistic effect of the input image, such as lighting, composition, contrast,\netc.Furthermore,weshowinFig.5ofthemainpaperthatmorecontrollabledif-\nfusion can in return improve the performance of discriminative models. Beyond\nimage generation, the controllable conditional generation can also be combined\nwith ID preserving methods to perform controllable image editing.\n6 More Visualization\nMore visualization results across different conditional controls for our image\ngeneration are shown in Figures 3,4,5,6,7.\n20 Li et al.\nImage & Condition Generated Images & Extracted Conditions\nFig.3: More visualization results of our ControlNet++ (LineArt Edge)\nControlNet++ 21\nImage & Condition Generated Images & Extracted Conditions\nFig.4: More visualization results of our ControlNet++ (Depth Map)\n22 Li et al.\nImage & Condition GeneratedImages&ExtractedConditions\nFig.5: More visualization results of our ControlNet++ (Hed Edge)\nControlNet++ 23\nImage & Condition GeneratedImages&ExtractedConditions\nFig.6: More visualization results of our ControlNet++ (Canny Edge)\n24 Li et al.\nImage & Condition Generated Images & Extracted Conditions\nFig.7: More visualization results of our ControlNet++ (Segmentation Mask)\nControlNet++ 25\nReferences\n1. Black, K., Janner, M., Du, Y., Kostrikov, I., Levine, S.: Training diffusion models\nwith reinforcement learning. arXiv preprint arXiv:2305.13301 (2023)\n2. Bradski, G.: The OpenCV Library. Dr. Dobb’s Journal of Software Tools (2000)\n3. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image\nediting instructions. In: CVPR (2023)\n4. Caesar,H.,Uijlings,J.,Ferrari,V.:Coco-stuff:Thingandstuffclassesincontext.\nIn: CVPR (2018)\n5. Chen,L.C.,Papandreou,G.,Schroff,F.,Adam,H.:Rethinkingatrousconvolution\nfor semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)\n6. Chen, M., Laina, I., Vedaldi, A.: Training-free layout control with cross-attention\nguidance. arXiv preprint arXiv:2304.03373 (2023)\n7. Chen,T.,Xu,B.,Zhang,C.,Guestrin,C.:Trainingdeepnetswithsublinearmem-\nory cost. arXiv (2016)\n8. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention\nmask transformer for universal image segmentation. In: CVPR (2022)\n9. Cheng,B.,Schwing,A.,Kirillov,A.:Per-pixelclassificationisnotallyouneedfor\nsemantic segmentation. NeurIPS (2021)\n10. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\nguage modeling with pathways. arXiv preprint arXiv:2204.02311 (2022)\n11. Clark,K.,Vicol,P.,Swersky,K.,Fleet,D.J.:Directlyfine-tuningdiffusionmodels\non differentiable rewards. arXiv preprint arXiv:2309.17400 (2023)\n12. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS\n(2021)\n13. Fan, Y., Watkins, O., Du, Y., Liu, H., Ryu, M., Boutilier, C., Abbeel, P.,\nGhavamzadeh,M.,Lee,K.,Lee,K.:Dpok:Reinforcementlearningforfine-tuning\ntext-to-image diffusion models. NeurIPS (2023)\n14. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,\nCohen-or, D.: An image is worth one word: Personalizing text-to-image genera-\ntion using textual inversion. In: ICLR (2023)\n15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016)\n16. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-or, D.:\nPrompt-to-prompt image editing with cross-attention control. In: ICLR (2023)\n17. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-\ngies. In: SIGGRAPH (2001)\n18. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS\n(2020)\n19. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint\narXiv:2207.12598 (2022)\n20. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen,\nW.: LoRA: Low-rank adaptation of large language models. In: ICLR (2022)\n21. Hu, M., Zheng, J., Liu, D., Zheng, C., Wang, C., Tao, D., Cham, T.J.: Cocktail:\nMixing multi-modality controls for text-conditional image generation. NeurIPS\n(2023)\n22. Huang, L., Chen, D., Liu, Y., Shen, Y., Zhao, D., Zhou, J.: Composer: Creative\nand controllable image synthesis with composable conditions. In: ICML (2015)\n26 Li et al.\n23. Ju, X., Zeng, A., Zhao, C., Wang, J., Zhang, L., Xu, Q.: Humansd: A native\nskeleton-guided diffusion model for human image generation. In: ICCV (2023)\n24. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,\nM.:Imagic:Text-basedrealimageeditingwithdiffusionmodels.In:CVPR(2023)\n25. Kingma,D.,Salimans,T.,Poole,B.,Ho,J.:Variationaldiffusionmodels.NeurIPS\n(2021)\n26. Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., Levy, O.: Pick-a-pic:\nAn open dataset of user preferences for text-to-image generation. arXiv preprint\narXiv:2305.01569 (2023)\n27. Li,Y.,Liu,H.,Wu,Q.,Mu,F.,Yang,J.,Gao,J.,Li,C.,Lee,Y.J.:Gligen:Open-set\ngrounded text-to-image generation. In: CVPR (2023)\n28. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: CVPR (2015)\n29. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:Sdedit:Guided\nimagesynthesisandeditingwithstochasticdifferentialequations.In:ICLR(2022)\n30. Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., Qie, X.: T2i-adapter:\nLearning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453 (2023)\n31. Nichol, A.Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B.,\nSutskever,I.,Chen,M.:Glide:Towardsphotorealisticimagegenerationandediting\nwith text-guided diffusion models. In: ICML (2022)\n32. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,\nC., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow\ninstructions with human feedback. NeurIPS (2022)\n33. Parmar, G., Zhang, R., Zhu, J.Y.: On aliased resizing and surprising subtleties in\ngan evaluation. In: CVPR (2022)\n34. von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K.,\nDavaadorj, M., Wolf, T.: Diffusers: State-of-the-art diffusion models. https://\ngithub.com/huggingface/diffusers (2022)\n35. Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,T.,Müller,J.,Penna,\nJ.,Rombach,R.:Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimage\nsynthesis. arXiv preprint arXiv:2307.01952 (2023)\n36. Prabhudesai, M., Goyal, A., Pathak, D., Fragkiadaki, K.: Aligning text-to-image\ndiffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739\n(2023)\n37. Qin,C.,Zhang,S.,Yu,N.,Feng,Y.,Yang,X.,Zhou,Y.,Wang,H.,Niebles,J.C.,\nXiong,C.,Savarese,S.,etal.:Unicontrol:Aunifieddiffusionmodelforcontrollable\nvisual generation in the wild. NeurIPS (2023)\n38. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\nG., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models\nfrom natural language supervision. In: ICML (2021)\n39. Raffel,C.,Shazeer,N.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,Zhou,Y.,Li,\nW., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text\ntransformer. JMLR (2020)\n40. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125\n(2022)\n41. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,\nSutskever, I.: Zero-shot text-to-image generation. In: ICML (2021)\n42. Riba,E.,Mishkin,D.,Ponsa,D.,Rublee,E.,Bradski,G.:Kornia:anopensource\ndifferentiable computer vision library for pytorch. In: CVPR (2020)\nControlNet++ 27\n43. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: CVPR (2022)\n44. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-\nical image segmentation. In: MICCAI (2015)\n45. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation.\nIn: CVPR (2023)\n46. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,\nK., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-\nto-image diffusion models with deep language understanding. NeurIPS (2022)\n47. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-\nverted residuals and linear bottlenecks. In: CVPR (2018)\n48. Schuhmann,C.,Beaumont,R.,Vencu,R.,Gordon,C.,Wightman,R.,Cherti,M.,\nCoombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,\nS., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An open\nlarge-scale dataset for training next generation image-text models. ArXiv (2022)\n49. Schuhmann,C.,Vencu,R.,Beaumont,R.,Kaczmarczyk,R.,Mullis,C.,Katta,A.,\nCoombes,T.,Jitsev,J.,Komatsuzaki,A.:Laion-400m:Opendatasetofclip-filtered\n400 million image-text pairs. ArXiv (2021)\n50. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In: ICML (2015)\n51. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.In:ICLR(2021)\n52. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-\nbased generative modeling through stochastic differential equations. In: ICLR\n(2021)\n53. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-\nlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n54. Wang, X., Darrell, T., Rambhatla, S.S., Girdhar, R., Misra, I.: Instancediffusion:\nInstance-level control for image generation (2024)\n55. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., Li, H.: Human preference\nscore v2: A solid benchmark for evaluating human preferences of text-to-image\nsynthesis. arXiv preprint arXiv:2306.09341 (2023)\n56. Wu, X., Sun, K., Zhu, F., Zhao, R., Li, H.: Better aligning text-to-image models\nwith human preference. In: ICCV (2023)\n57. Xiao,T.,Liu,Y.,Zhou,B.,Jiang,Y.,Sun,J.:Unifiedperceptualparsingforscene\nunderstanding. In: ECCV (2018)\n58. Xie,J.,Li,Y.,Huang,Y.,Liu,H.,Zhang,W.,Zheng,Y.,Shou,M.Z.:Boxdiff:Text-\nto-image synthesis with training-free box-constrained diffusion. In: ICCV (2023)\n59. Xie, S., Tu, Z.: Holistically-nested edge detection. In: ICCV (2015)\n60. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., Dong, Y.: Imagere-\nward: Learning and evaluating human preferences for text-to-image generation.\nNeurIPS (2023)\n61. Yang,Z.,Wang,J.,Gan,Z.,Li,L.,Lin,K.,Wu,C.,Duan,N.,Liu,Z.,Liu,C.,Zeng,\nM., et al.: Reco: Region-controlled text-to-image generation. In: CVPR (2023)\n62. Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compati-\nble image prompt adapter for text-to-image diffusion models. arXiv preprint\narXiv:2308.06721 (2023)\n63. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image\ndiffusion models. In: ICCV (2023)\n28 Li et al.\n64. Zhang,T.,Zhang,Y.,Vineet,V.,Joshi,N.,Wang,X.:Controllabletext-to-image\ngeneration with gpt-4. arXiv preprint arXiv:2305.18583 (2023)\n65. Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-\ncontrolnet: All-in-one control to text-to-image diffusion models. NeurIPS (2023)\n66. Zhao, W., Bai, L., Rao, Y., Zhou, J., Lu, J.: Unipc: A unified predictor-corrector\nframework for fast sampling of diffusion models. arXiv preprint arXiv:2302.04867\n(2023)\n67. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing\nthrough ade20k dataset. In: CVPR (2017)\n68. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.:\nSemantic understanding of scenes through the ade20k dataset. IJCV (2019)\n69. Zhou, D., Li, Y., Ma, F., Yang, Z., Yang, Y.: Migc: Multi-instance generation\ncontroller for text-to-image synthesis. In: CVPR (2024)\n70. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-\nlanguage understanding with advanced large language models. arXiv preprint\narXiv:2304.10592 (2023)\n71. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In: ICCV (2017)",
    "pdf_filename": "ControlNet++_Improving_Conditional_Controls_with_Efficient_Consistency_Feedback.pdf"
}