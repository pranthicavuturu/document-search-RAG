{
    "title": "STREAM: A Universal State-Space Model for Sparse Geometric Data",
    "abstract": "datasets. It further achieves, for the first time, 100% test Handlingsparseandunstructuredgeometricdata,suchas accuracyonall11classesoftheDVS128Gesturesdataset. pointcloudsorevent-basedvision, isapressingchallenge in the field of machine vision. Recently, sequence models such as Transformers and state-space models entered the 1.Introduction domainofgeometricdata. Thesemethodsrequirespecial- ized preprocessing to create a sequential view of a set of Computer vision computes relationships between data points. Furthermore,priorworksinvolvingsequencemod- points in spatial coordinates X, Y (in R2) or X, Y, Z (in els iterate geometric data with either uniform or learned R3),orspatio-temporalcoordinates(R3 orR4),whereone step sizes, implicitly relying on the model to infer the un- ofthedimensionsdenotestimet. Convolutionalneuralnet- derlying geometric structure. In this work, we propose to worksbasedonstructured,uniformlyspacedandlocallin- encodegeometricstructureexplicitlyintotheparameteriza- earoperationssuccessfullyaddressthisproblemforclassi- tion of a state-space model. State-space models are based calcamerarecordingssuchasimagesorvideos,whichare on linear dynamics governed by a one-dimensional vari- themselvesstructuredanddiscrete. Manymodalitiesofre- able such as time or a spatial coordinate. We exploit this centinterest,however,areneitherstructurednoruniformly dynamic variable to inject relative differences of coordi- spaced. SensorssuchasLightDetectionandRanging(Li- nates into the step size of the state-space model. The re- DAR)orevent-basedcameras[21,28]samplesignalsbased sultinggeometricoperationcomputesinteractionsbetween onsparseprocesses,resultinginsparsegeometricdatawith allpairsofN pointsinO(N)steps.Ourmodeldeploysthe irregularly spaced coordinates. Point cloud analysis was Mambaselectivestate-spacemodelwithamodifiedCUDA the central research field for sparse geometric data over kernel to efficiently map sparse geometric data to modern the past decade [13]. While early works followed voxel- hardware. The resulting sequence model, which we call based approaches [25, 50], point-based methods dominate STREAM,achievescompetitiveresultsonarangeofbench- the research landscape today [4, 5, 46, 48]. Meanwhile, marksfrompoint-cloudclassificationtoevent-basedvision event-based cameras [21, 28] raised considerable interest and audio classification. STREAM demonstrates a pow- in the computer vision community. Although these cam- erful inductive bias for sparse geometric data by improv- erasrecordsparsegeometricdata,mostworkscollapsethe ingthePointMambabaselinewhentrainedfromscratchon sparsestreamofeventsintoframes[28,56,57],potentially losing unique properties of these cameras such as low la- *Theseauthorscontributedequallytothiswork. tencyandhighdynamicrange. Thestructuralsimilaritybe- 1 4202 voN 91 ]VC.sc[ 1v30621.1142:viXra",
    "body": "STREAM: A Universal State-Space Model for Sparse Geometric Data\nMarkScho¨ne1* YashBhisikar2* KaranBania2* KhaleelullaKhanNazeer1\nChristianMayr1,3,4 AnandSubramoney3\nDavidKappel4\n1TUDDresdenUniversityofTechnology 2BITSPilani\n3CenterforScalableDataAnalyticsandArtificialIntelligence(ScaDS.AI)\n4CentreforTactileInternet(CeTI)withHuman-in-the-Loop\n5RoyalHolloway,UniversityofLondon 6BielefeldUniversity\n{mark.schoene, khaleelulla.khan, christian.mayr}@tu-dresden.de\n{f20210483, f20212582}@goa.bits-pilani.ac.in\nanand.subramoney@rhul.ac.uk\ndavid.kappel@uni-bielefeld.de\nAbstract the ModelNet40 and ScanObjectNN point cloud analysis\ndatasets. It further achieves, for the first time, 100% test\nHandlingsparseandunstructuredgeometricdata,suchas accuracyonall11classesoftheDVS128Gesturesdataset.\npointcloudsorevent-basedvision, isapressingchallenge\nin the field of machine vision. Recently, sequence models\nsuch as Transformers and state-space models entered the\n1.Introduction\ndomainofgeometricdata. Thesemethodsrequirespecial-\nized preprocessing to create a sequential view of a set of Computer vision computes relationships between data\npoints. Furthermore,priorworksinvolvingsequencemod- points in spatial coordinates X, Y (in R2) or X, Y, Z (in\nels iterate geometric data with either uniform or learned R3),orspatio-temporalcoordinates(R3 orR4),whereone\nstep sizes, implicitly relying on the model to infer the un- ofthedimensionsdenotestimet. Convolutionalneuralnet-\nderlying geometric structure. In this work, we propose to worksbasedonstructured,uniformlyspacedandlocallin-\nencodegeometricstructureexplicitlyintotheparameteriza- earoperationssuccessfullyaddressthisproblemforclassi-\ntion of a state-space model. State-space models are based calcamerarecordingssuchasimagesorvideos,whichare\non linear dynamics governed by a one-dimensional vari- themselvesstructuredanddiscrete. Manymodalitiesofre-\nable such as time or a spatial coordinate. We exploit this centinterest,however,areneitherstructurednoruniformly\ndynamic variable to inject relative differences of coordi- spaced. SensorssuchasLightDetectionandRanging(Li-\nnates into the step size of the state-space model. The re- DAR)orevent-basedcameras[21,28]samplesignalsbased\nsultinggeometricoperationcomputesinteractionsbetween onsparseprocesses,resultinginsparsegeometricdatawith\nallpairsofN pointsinO(N)steps.Ourmodeldeploysthe irregularly spaced coordinates. Point cloud analysis was\nMambaselectivestate-spacemodelwithamodifiedCUDA the central research field for sparse geometric data over\nkernel to efficiently map sparse geometric data to modern the past decade [13]. While early works followed voxel-\nhardware. The resulting sequence model, which we call based approaches [25, 50], point-based methods dominate\nSTREAM,achievescompetitiveresultsonarangeofbench- the research landscape today [4, 5, 46, 48]. Meanwhile,\nmarksfrompoint-cloudclassificationtoevent-basedvision event-based cameras [21, 28] raised considerable interest\nand audio classification. STREAM demonstrates a pow- in the computer vision community. Although these cam-\nerful inductive bias for sparse geometric data by improv- erasrecordsparsegeometricdata,mostworkscollapsethe\ningthePointMambabaselinewhentrainedfromscratchon sparsestreamofeventsintoframes[28,56,57],potentially\nlosing unique properties of these cameras such as low la-\n*Theseauthorscontributedequallytothiswork. tencyandhighdynamicrange. Thestructuralsimilaritybe-\n1\n4202\nvoN\n91\n]VC.sc[\n1v30621.1142:viXra\nspaced step sizes. The resulting model, STREAM, han-\ndlessparsegeometricdatasuchaspointcloudsandevent-\nbasedvision.\n• Our state-space model formulation demonstrates a valu-\nable inductive bias for geometric structure by improv-\ningthePointMamba[20]baselinebyupto2.84%when\ntrainedfromscratchontheScanObjectNNdataset[43].\n• Weshowcompellingresultsforevent-basedvision, pro-\ncessedasastreamofeventswithourpurelyrecurrentneu-\nralnetworkwithouttheusageofframesorspatialconvo-\nlutions. Forthefirsttime,wedemonstrate100%classifi-\ncationaccuracyonall11classesoftheDVS128-Gestures\ndataset[1].\n• We provide an efficient CUDA implementation for in-\ntegrating irregular step sizes based on the selective-scan\nimplementationof[9].\n2.RelatedWork\n2.1.PointCloudAnalysis\nEarly point-based methods such as PointNets [4, 31] di-\nrectly pass the point cloud through a Multi-layer Percep-\ntron (MLP) and introduce a permutation invariance of the\nset of points via pooling operations, implicitly integrating\nspatial information. In contrast, methods for explicitly in-\ntegrating spatial information parameterize linear operators\nsuchasconvolutionsbasedonrelativedifferencesbetween\nFigure 1. A unified view on point cloud and event stream mod-\npointcoordinates.Theconvolutionalneuralnetworksbased\neling. A.AdjacencystructureofpointbasedmethodsB.Theco-\nonirregularlyspacedoperatorspresentedin[17,46,48,49]\nordinatesofsparsegeometricdataareirregularlyspacedC.Point\noutperform PointNets on a range of point cloud analysis\ncloudsandeventstreamsareorderedbyspatialandtemporalaxes,\nrespectively,beforethestate-spacemodel. tasks. These works parameterize the convolution kernel\nwith MLPs evaluated at the relative differences between\npointcoordinates.TobreaktheO(cid:0) N2(cid:1)\ncomplexityofcom-\ntween point clouds and event streams encourages method- puting pairwise interactions for all N points, locality con-\nologicaltransfer,especiallyfromthedomainofpointcloud strains are added to scale to larger point sets (see fig.1A).\nanalysis to event-based vision [35, 45]. Our work demon- As shown in fig.2A, our method also explicitly integrates\nstrates the reverse: The inherently temporal state-space spatialinformationinthissense,butwithakernelparame-\nmodel formulation presented in sec.3.2 improves event- terizedbyastate-spacemodel(SSM)insteadofaMLP(see\nbasedvisionontheDVS128-Gesturedataset[1]to100%, equation (3)). This parameterization enables the complete\nandatthesametimeprovidesavaluableinductivebiasfor computationtobeperformedinO(N)steps.\ngeometricdatasuchaspointcloudsasdemonstratedonthe Morerecently,sequencemodelsdemonstratedfavorable\nModelNet40andScanObjectNNdatasets[43,51]. results over convolutional methods [5, 20, 26, 52]. The\nWe show that sparse geometric data with irregularly pointcloudisflattenedintoasequenceandthenprocessed\nspacedcoordinatesasshowninfig.1Bcanbenaturallyin- bytransformerorstate-spacemodelbasedbackbones. This\ntegratedinthestate-spacemodelformalism. Incontrastto formulation inherits many strong scaling properties of se-\npriorsequencemodelssuchasPointMamba[20]orEvent- quence models including masked pre-training [52] or gen-\nMamba[36],weexplicitlyencodegeometricstructureinto\nerative pre-training [5] objectives. Transformers, however,\nthestate-spacemodelparameterizationbyinjectingrelative sufferfromO(cid:0) N2(cid:1) complexityinthenumberofpointsN,\ndifferencesofcoordinatesasstepsizes. limitingtheirapplicationtolargerpointclouds. State-space\nContributions. Thispapermakesthefollowingmaininno- models, in contrast, have sequential O(N) complexity for\nvationsoverthestate-of-the-art: inference and O(NlogN) complexity for parallel train-\n• We propose a unified framework for modeling sparse ing [10, 11]. At the same time, SSMs can learn long dis-\ngeometric data with state-space models with irregularly tant relationships between inputs including convolutional\n2\noperators with rational transfer functions [10, 30]. The analysisforward.\nrecently introduced Mamba state-space model [9] sparked Related from the perspective of discretizing continuous\nwidespreaddiscourse,andwasquicklyadaptedinmanydo- kernelsistheTENNsframework[27]. Theyconstructcon-\nmains, including point cloud analysis. Works like Point- volutionsoverfinitetimehorizonswithorthogonalpolyno-\nMamba [20] or Point Cloud Mamba (PCM) [55] deploy mials that can be discretized on irregularly spaced times-\nMamba in established point cloud frameworks such as the tamps. In practice, they choose regularly spaced frames,\nmasked autoencoder developed in [26]. By relying on however, and show compelling results on event-based vi-\nthe default Mamba architecture, spatial information is in- sionbenchmarks. Incomparison,STREAMparameterizes\ntegrated implicitly in these works similar to PointNets. In infinitetimehorizonsviathestate-spacemodel.\ncontrast, we show in sec.3.2 that spatial information can\nbeintegratedexplicitlyviatheSSMparameterization. This 3.STREAM\nformulationallowsustoremovespecializedpreprocessing\nIn this section, we present our method for efficiently\nmethods to represent point clouds as sequences [20, 55],\nmodeling sparse sets of N coordinates {x }N , where\nandsimplyorderbythespatialcoordinatesX,Y,orZ. i i=0\n(cid:16) (cid:17)\nx = x(1),...,x(d) ∈Rd. This general formulation in-\ni i i\n2.2.Event-basedVision\ncludesdatain3Dspace,withx=(X,Y,Z),aswellasthe\nstreamsrecordedbyevent-basedcameras. Inthelattercase\nMoststate-of-the-artevent-basedvisionmethodsconstruct\none of the dimensions (e.g. x(1)) is interpreted as event\nframes from the asynchronous event-stream by binning\ntime while others (x(2), x(3)) denote spatial locations (see\neventsinregularlyspacedtimeintervals[6,28,42,56,57].\nfig.1C). Furthermore, higher dimensions (d > 3) can be\nFew methods operate on the sparse unstructured event\nusedtodenotedatathatincludesotherdimensions,suchas\nstreams directly. AEGNN [37] computes spatio-temporal\ncolorintensity,etc. STREAMisillustratedinfig.2A-C.\nfeaturesbasedonsubsampledeventstreamswithgraphneu-\nral networks. Their method admits an event-based infer- Notation. Wedenotetensor-valuedvariablesandfunctions\nence method where only nodes that correspond to incom- suchasmulti-dimensionalcoordinatesx ∈ Rd ormatrices\ning events are updated asynchronously. EventMamba [36] A∈Rm×minboldcharacters. Scalarvariablessuchasthe\nis tightly related to the point cloud analysis models dis- coordinatesx(1),...,x(d)ofatensorxortheentriesA ij of\ncussed in sec.2.1. They process subsampled raw event amatrixAaredenotedinregularcharacters.\nstreams similar to [37, 39] with a combination of a point\n3.1.ProblemStatement\nfeature extractor and Mamba. Its high similarity with\nPointMamba [20] places EventMamba in the category of Consider a set {x }N of points sparsely distributed in\ni i=0\nmethodsimplicitlyintegratingspatio-temporalinformation. space. Weassignamulti-dimensionalrepresentationvector\nConceptuallyclosertoourmodelarefilteringmethodsthat u to each coordinate x as common practice in machine\ni i\nprocessthestreamofeventsrecursivelywhileexplicitlyin- learning. Suchinputsignalscanbeformulatedmathemati-\ntegrating spatio-temporal information. Early methods like callyassumsofDiracdeltapulsesevaluatedatthecoordi-\nHOTS [18] create a spatial representation of the (X,Y)- nates{x }N\ni i=0\nplaneofanevent-basedcamerabyintegratingexacttimes-\ntamps with exponentially moving averages. Similar to our N\n(cid:88)\nmethod,EventNet[39]recursivelyiteratestheeventstream. u(x)= δ(x−x i)u i, (1)\nIncontrasttoourstate-spacemodel,theirrecurrentnetwork i=0\nparameterization does not inherit stability guarantees from\nwhereu ∈Rn istherepresentationatx . Toreasonabout\nthe theory of linear systems and lacks an efficient paral- i i\nspatialrelationshipsbetweenthesepulses,wedefineanin-\nlelizationalongthetimedimensionduringtraining,limiting\nteractionkernelΦ(x,x′)thatmodelspairwiseinteractions.\nthescalabilitytolongerstreams. Event-SSM[38]proposed\nAcompleteviewofallpossibleinteractionswiththepoint\na discretization method for simplified state-space layers\nx isgivenby\n(S5)[40]tooperatedirectlyonasynchronouseventstreams. k\nTheirmodelexcelsatspikingaudioclassification,butfalls (cid:90)\nbehind the state-of-the-art in event-based vision. Concur- y(x k)= Φ(x k,x′)u(x′)dx′ (2)\nrentlytoourwork,S7[41]exploresimprovementsoftheS5\nN\narchitecture and Event-SSM through a more stable param- (cid:88)\n= Φ(x ,x )u . (3)\nk i i\neterization and input dependent state-space model param-\ni=0\neters. In contrast, STREAM can improve sequence-based\npoint cloud analysis (see sec.4.1), and therefore drive the Thisformulationcontainsthefamiliarconvolutionoperator\nconvergence between event-based vision and point cloud as a special case when Φ(x,x′) ≡ Φ(x−x′). Equation\n3\nFigure2. STREAMencodesgeometricinformationintotheSSMparameters. A.Pointcloudinput. B.TheSTREAMmoduleconverts\nrelativedifferencesincoordinatesintothe∆ scaleoftheSSM.C.ASTREAMmoduleintegratespairwisespatialrelationshipsbasedon\nk\nanexponentiallyoscillatingkernel.\n(3)highlightsthatthecomputationofy(x )atthecoordi- with u(t),y(t)∈R, states h(t)∈Rm and parameters\ni\nnates {x }N requires evaluating Φ(x ,x ) only at pairs A(t)∈Rm×m,B(t)∈Rm×1,C(t)∈R1×m. For the\ni i=0 i j\nofthesecoordinates. Inthegeneralcase,thisoperationre- sake of simplicity, we assume that the parameters are rep-\nquiresO(cid:0) N2(cid:1)\nstepstocompute. Asshowninfig.1A,this resentedbypiecewiseconstantfunctions,i.e. A =A(t ),\ni i\ncomplexitycanbereducedtoO(kN)byfocusingonlocal B = B(t ), C = C(t ) are constant on the intervals\ni i i i\nneighborhoodsofk points,atechniquebroadlyusedinthe (t ,t ]. We show in appendix6 that solving the linear\ni−1 i\nliterature of point convolutions [46, 48]. In the following systeminequations(4)and(5)fortheinputinequation(1)\nsections,wepresentamethodforrecursivelyintegratingN yieldsthekernelfunction\nsparsely sampled points in O(N) time without restricting\n \ntolocalneighborhoods. k\n(cid:89)\nΦ(t k,t i)=C k exp(A j∆ j)B i, (6)\n3.2. Integrating Sparse Coordinates with State-\nj=i+1\nspaceModels\nwhere∆ = t −t . Importantly,theoutputy(t)ofthe\nState-space models (SSMs) have demonstrated outstand- j j j−1\nstate-space model at t ,...,t can be computed with the\ningperformanceonlong-rangedependencymodelingtasks, 0 N\nrecursiveformula\nwhileremainingefficientfortrainingandinference[9,11].\nSSMsprojecttheinputthroughalinearrecursivekernelto h\nk\n=h(t k)=eAk∆kh k−1+B ku\nk\n(7)\nstates h(t). The independent variable t is usually inter-\ny =y(t )=C h . (8)\npretedas,andalignedwith,timeintheinputsignaldomain. k k k k\nThe linear nature of SSMs allows the exact integration of\nEquations (6) and (7) highlight how relative differences\nsparsesetsofpointswithirregularlyspacedcoordinates,as\n∆ = t −t inthecoordinatetexplicitlyparameterize\nj j j−1\nwewillshowbelow. Alineartime-varyingSSMactingon\nourpairwiseinteractionkernelΦ. Anexampleoftheinter-\na scalar input function u(t) and producing a scalar output\nactiondefinedbyequation(6)canbefoundinFig.2C.For\ny(t)isdefinedthrough complex A ∈ C, theexponentially oscillatingkernel eA∆\nh˙ (t)=A(t)h(t)+B(t)u(t) (4) integrates the history of signals u i occuring in irregularly\nspacedintervals∆ withi<k. Withthisparameterization,\ni\ny(t)=C(t)h(t) , (5)\nourmethoddiffersfromotherMambabasedworks,where\n4\n∆isafunctionofu thatdoesn’texplicitlytakethedynam- where δ ∈ Rn is a learnable parameter. We call\ni\nicsoftintoaccount. Acompletederivationcanbefoundin ourresultingstate-spacemodelparameterizationSTREAM\nappendix6. forSpatio-TemporalRecursiveEncodingofAsynchronous\nThe recursive formulation in equation (7) is a lin- Modalities.\near recurrent neural network, whose state-to-state tran-\n3.4.ModelingPointCloudswithSTREAM\nsition matrix eAk∆k is parameterized by the differences\n∆ k =t k−t k−1intheirregularlyspacedcoordinatest i.As The explicit coordinate t in the STREAM module is one-\nsuch, our method positions itself among the methods dis- dimensional and strictly ordered. For the case of event\ncussed in sec.2.1 that explicitly integrate spatial informa- streams, this ordering corresponds to temporal order from\ntion through the parameterization of a linear operator with past to future. Point clouds fit into this framework by or-\nthecoordinates. deringthesetofpointsw.r.t. oneofthespatialcoordinates\nTheoutputsequencey(t 0),...,y(t N)canbecomputed fromlefttoright. Asshowninfig.2A,B,weselectoneof\nsequentiallyinO(N)time. Thisallowsefficientinference thecoordinatestobeintegratedexplicitly. InsertingtheX\non streams of pulses recorded by sensors such as event- coordinatefortinequation(11)withoutlossofgenerality\nbasedcamerasorLiDAR.Atthesametime,linearrecursive andsettingu =Θ(X ,Y ,Z ),whereΘ:R3 −→Rnis\nk k k k\nequations admit an efficient parallelization via the Scan apointwiseencoder,yieldsourSTREAMformulationfor\nprimitiveavailableinCUDA[3].Onsufficientlymanypro- pointcloudanalysis. Toachievestate-of-the-artresults,we\ncessors,theoutputsequencecanbecomputedinO(logN) sortthepointsetbyallthreespatialcoordinatesrespectively\ntime,whichallowsmassiveparallelizationonverylongse- andfeedtheconcatenationofthethreeresultingsequences\nquences of up to 1 million inputs [9]. We show in ap- toSTREAM(seesec.4.1.1,fig.3).\npendix7howourformulationmapstothescanprimitive.\n4.Experiments\n3.3.Selectivestate-spacemodelsandSTREAM\nWe demonstrate empirically that the explicitly parameteri-\nWe develop our method based on the selective state-space\nzationofthestate-spacemodelwithcoordinatedifferences\nmodel also known as Mamba [9], which was designed for\n∆ presented in sections 3.2 and 3.3 is a strong abstrac-\nregularlyspacedmodalities,andlanguagemodelinginpar- i\ntion that exploits the sparse geometry of both point clouds\nticular.Inaccordancewiththispurpose,theintegrationtime\nand event-based vision. When trained from scratch, our\nsteps∆ andthematricesB ,C arefunctionsoftheinput\ni i i\nmethod improves the PointMamba [20] baseline by up to\nsignalu ,anddonotcarryexplicitinformationaboutspatial\ni\n2.8%ontheScanObjectNNdataset[43]. Atthesametime,\nrelationships. Formally,Mambaisparameterizedby\nourmethodsetsanewstate-of-the-artforevent-basedges-\nA ≡A ∀i ∆ =Ψ(Linear(u )) (9) turerecognitionontheDVS128-Gesturesdataset[1].\ni i i\nB i =∆ iLinear(u i) C i =Linear(u i) , (10) 4.1.STREAMforPointCloudAnalysis\nwhereAisalearneddiagonalmatrix,andΨ(u)=ln(1+ 4.1.1 PointCloudModel\neu) is the softplus function. This single-input u single-\ni\nOur goal is to evaluate the explicit SSM parameterization\noutputy SSMisthenrepeatedntimesandwrappedbylin-\ni\ndefined in sec.3.3. Therefore, we align our architecture\near transformations to create a multi-input u multi-output\ni\nclose to PointMamba [20], which uses the default Mamba\ny model as shown in fig.2B. Note that the total state size\ni\nmodule [9]. As visualized in fig.3A, we sample N points\nisnminthemulti-inputmulti-outputcase.\nand sort this point set by the X, Y, and Z coordinates re-\nWeproposetoexplicitlyrepresenttheirregularlyspaced\nspectively. The vector representations extracted from the\npulse timings t in the Mamba parameterization accord-\ni\npoint coordinates individually are scaled and shifted by\ning to equation (7). Therefore, we explicitly set ∆ to\ni\nlearnedparametersforthethreesortedsequencesseparately\n(t −t )Ψ(δ), whereδ isatrainabletimescaleparam-\ni i−1\ntoindicatethesortingdimension.Theresultingsequenceof\neter, in contrast to equation (9). We further decouple the\n3N pointsisconcatenatedalongthesequencedimensionto\ncoefficientofB inequation(10)fromthetimedifferences\ni\nform the input to our model. This preprocessing simpli-\n∆ toavoidzerocoefficientsincaseofoverlappingpulses\ni\nfiestheorderingofthepointsetbyavoidingthetwospace-\nt −t =0.Withtheseadjustments,ourmodelisdefined\ni i−1\nfillingHilbert-curvesappliedbyPointMambaatthecostof\nby\nincreasingtheinputsequencelengthfrom2N to3N.\nA ≡A∀i ∆ =(t −t )Ψ(δ) (11) Similar to [5, 20, 26], our model is composed of two\ni i i i−1\nstages. Theinputsequenceisgroupedintosetsof32points\nB =Γ Linear(u ) C =Linear(u ) (12)\ni i i i i thatareindependentlyprocessedbyaSTREAMmoduleto\nΓ =Ψ(Linear(u )) , (13)\ni i represent local information. A backbone of 12 STREAM\n5\nFigure3.PointcloudprocessingpipelineinspiredbyPointMamba[20].FPS:Farthestpointsampling,kNN:k-NearestNeighbors.\nlayersoperatesonthefeaturesextractedfromthesegroups Ourresultsshowthattheinductivebiasofexplicitlyen-\ntoagglomerateglobalinformation. codingspatialrelationshipsintheSSMparameterizationis\nparticularlyusefulwhentrainingmodelsfromscratch.\n4.1.2 PointCloudResults 4.2.STREAMforEventStreams\nWe evaluate our method on two standardized point cloud\nclassification benchmarks. The ModelNet40 dataset [51]\ncontains12311cleanpointcloudssampledfrom3DCAD\nobjects. A more realistic dataset of physically scanned\nindoor scenes is given by ScanObjectNN [43]. ScanOb-\njectNN contains about 15000 scans from 2902 unique\nobjects with varying difficulty. In both cases, we train\nour method with the same hyperparameters as Point-\nMamba[20].\nWereportthebestoverallclassificationaccuracy,i.e.the\naverageoverallsamples,obtainedfrom5differentrandom\nseeds. Tab.1 shows that replacing the Mamba module in Figure4. Hierarchicalsubsamplingarchitectureswithtwostages\nPointMambawithaSTREAMmoduleimprovestheperfor- tohandlelongeventstreams.\nmance of models trained from scratch across all instances\non ScanObjectNN. In particular, the hardest instance PB-\nT50-RSbenefitsfromtheexactintegrationofspatialinfor-\n4.2.1 EventStreamModel\nmationandimprovesPointMambafrom82.48%to85.32%\nforthebestoutof5randomseeds. Withameanandstan- In contrast to most prior works, our method directly op-\ndard deviation of (84.4 ± 0.7)%, this poses a significant erates on the stream of events as visualized in fig.4. The\nimprovement. stream is encoded as a sequence of tokens (t ,u ), where\ni i\nOn ModelNet40, STREAM improves the overall accu- each pixel of the event-based camera is uniquely encoded\nracy of PointMamba by 0.3% from 92.4% to 92.7% as by a separate token, similar to how text is represented as\nreported in tab.2. Despite the smaller gain compared to tokens in modern language modeling [33]. This sequence\nScanObjectNN,thelowvarianceonthisdatasetindicatesa is directly processed by a STREAM module. To accom-\nsignificantimprovementonModelNet40aswell. Themean modatelargestreamsofupto131072eventspersamplein\nandstandarddeviationofourmodelis(92.6±0.1)%. GPU memory, the output of the first STREAM module is\n6\nMethods Backbone Param. (M)↓ OBJ-BG↑ OBJ-ONLY↑ PB-T50-RS↑\nSupervisedLearningOnly\nPointMLP[24] - 13.2 85.4 - 85.4\nRepSurf-U[34] - 1.5 - - 84.3\nADS[15] - - - - 87.5\nPoint-MAE[26] Transformer 22.1 86.75 86.92 80.78\nPCM[55] Mamba 34.2 - - 88.1\nPointMamba[20] Mamba 12.3 88.30 87.78 82.48\nOurs STREAM 12.3 90.02(+1.72) 88.64(+0.86) 85.32(+2.84)\nTrainingfromPre-training(Single-Modal)\nPoint-BERT[52] Transformer 22.1 87.43 88.12 83.07\nMaskPoint[23] Transformer 22.1 89.30 88.10 84.30\nPoint-MAE[26] Transformer 22.1 92.77 91.22 89.04\nPointMamba[20] Mamba 12.3 94.32 92.60 89.31\nTable1. ObjectclassificationonScanObjectNN[43]. Wereportoverallaccuracy(OA)inpercentandindicatetheimprovementoverthe\nPointMambabaselinethroughusingourSTREAMmoduleinsteadofthedefaultMambamodule.\nMethods Param. (M)↓ OA(%)↑ Methods Param. (M)↓ Acc(%)↑\nSupervisedLearningOnly Frame-based\nPointNet[4] 3.5 89.2 ACE-BET[22] - 98.9\nPointNet++[31] 1.5 90.7 ExACT[56] - 98.9\nPointCNN[19] 0.6 92.2 SpikMamba[6] 0.2 99.0\nDGCNN[29] 1.8 92.9 EventMamba[36] 0.3 99.2\nPointNeXt[32] 1.4 92.9 TENNs-PLEIADES[27] 0.2 100.0∗\nPCT[12] 2.9 93.2\nEvent-based\nOctFormer[44] 4.0 92.7\nPCM-Tiny[55] 6.9 93.1 Event-SSM[38] 5.0 97.7\nPoint-MAE[26] 22.1 92.3 S7[41] 4.1 99.2\nPointMamba[20] 12.3 92.4 STREAM(ours) 0.2+1.0† 100.0\nSTREAM(ours) 12.3 92.7 (+0.3)\nTable 3. Comparison of STREAM to the state-of-the-art on the\nwithSelf-supervisedpre-training\nDVS128-Gesturesdataset[1]. ∗evaluatedon10outof11classes,\nPoint-BERT[52] 22.1 92.7 omittingthe”other”class. †0.2MparametersforSTREAM,and\nMaskPoint[23] 22.1 92.6 1.0Mparametersforencodingtheinputstreamastokens.\nPoint-M2AE[54] 12.8 93.4\nPoint-MAE[26] 22.1 93.2\nPointGPT-S[5] 29.2 93.3\nACT[8] 22.1 93.6\nPointMamba[20] 12.3 93.6\nsubsampledtherawstreambeforepresentingittoaneural\nTable2. ClassificationonModelNet40. Wereportoverallaccu- network[37,39]. Tocreateahierarchicalarchitecture, the\nracy(OA)inpercentandindicatetheimprovementoverthePoint-\nsequence is subsampled a second time after half the num-\nMamba baseline through using our STREAM module instead of\nber of layers. The state dimension is increased by a factor\nthedefaultMambamodule.\nof 2 upon the second subsampling. Our hierarchical sub-\nsamplingarchitectureisvisualizedinfig.4. Weapplyaver-\nagepoolingalongthesequencedimensionbeforecomput-\nsubsampled by a factor of 8 or 16 depending on the task. ing the class labels. To improve generalization, we imple-\nThis way, every event is processed by the neural network mentasetofgeometricdataaugmentationsandavariantof\nat least once directly, in contrast to previous methods that CutMix[53]thatdirectlymixeseventstreams[38].\n7\nMethods Param. (M)↓ Acc(%)↑ 4.3.AblationStudy\nFrame-based We compare our STREAM module against the Mamba\nmodule [9] in tab.5. The central difference is the explicit\nBittarandGarner[2] 0.1 71.7\nintegration of spatio-temporal information in the recurrent\nBittarandGarner[2] 3.9 77.4\nHammouamrietal.[14] 0.7 79.8\noperator eA∆k with ∆\nk\n= t\nk\n− t k−1. We additionally\nexperiment with all combinations of applying linear trans-\nHammouamrietal.[14] 2.5 80.7\nformations activated by Softplus functions to renormalize\nEvent-based the ∆ and Γ parameters in equations (11) and (12), re-\nk k\nspectively. WhilethefairlysmallDVS128Gesturesdataset\nEvent-SSM[38] 0.1 85.3\npossesses high variance, STREAM consistently improves\nEvent-SSM[38] 0.6 88.4\noverMamba. Thiseffectisstrongerexpressedonthelarger\nS7[41] 0.6 88.2\nSpiking Speech Commands dataset, which is less affected\nSTREAM(ours) 0.2 86.3\nbytrainingnoiseduetothelargernumberofsamples.Here,\nSTREAMcreatesasignificantmargintotheMambabase-\nTable 4. Comparison of STREAM to the state-of-the-art on the\nline.\nSpikingSpeechCommandsdataset[7].\n5.Discussion\n4.2.2 EventStreamResults WehaveintroducedSTREAM,asequencemodelforpoint\ncloud and event stream data, which achieves competitive\nWeevaluateSTREAMontwopopularevent-baseddatasets. resultsonarangeofbenchmarksfrompoint-cloudclassifi-\nTheDVS128Gesturesdataset[1]consistsof1342record- cationtoevent-basedvision. Prioreffortsofunifyingthese\ningsof10distinctclassesofhandgesturesandanadditional modalitiestransferredpointcloudmodelstoeventstreams,\nclassofarbitraryothergestures. Whilethisdatasetfeatures orignorethespatio-temporalstructureofeithermodalityby\nonlyarelativelysmallnumberofsamplesfor11handges- applyingthedefaultMambamodeltoasequentialviewof\nturecategoriesrecordedfrom29subjects,thetotalnumber the data. In contrast, our model exploits the dynamics of\nofeventsinthisdatasetaddsuptoabout390Mevents. Our state-spacemodels, atemporalprocessatfirstsight, toen-\nmodelhasaninitialmodeldimensionofn=32andastate- codespatialgeometricinformationintothemodelsparame-\nspacedimensionofm=32,whichisexpandedbyafactor terization.ThisinductivebiasprovedvaluableasSTREAM\nof 2 upon subsampling. We deploy a total of 6 STREAM improves our reference model, PointMamba, when trained\nblocks,whichbythenotationoffig.4correspondstoL=2. from scratch on point cloud datasets such as ModelNet40\nWe train with a batch size of 32 on sequences of 65536 andScanObjectNN.Bydesign,STREAMappliestospatio-\neventssuchthatthetotalinputsequencescreatedbyCutMix temporal modalities such as event-based vision. We oper-\nareupto131072eventslong,andsubsamplebyafactorof ate on the stream of events without collapsing events into\n16. Evaluation is conducted on the full sequences of up frames and without using 2D convolutions. This process-\nto 1.5M events. Remarkably, our fully event-based model ingparadigmachieves100%classificationaccuracyonall\nsetsanewstate-of-the-artontheDVS128-Gesturesdataset 11 classes of the event-based DVS128 Gestures dataset, a\nasreportedintab.3, improvingoverbothframe-basedand result that has so far only been achieved on the 10 prede-\nevent-based references. While [27] reported 100% accu- termined classes by [27]. A practically relevant property\nracyon10outofthe11classes,omittingthe‘others’class, of STREAM is that it allows asynchronous inference on\nweforthefirsttimepresentamodelthatcanreachamaxi- streamsofpointsoreventsrecordedfromLiDARsensorsor\nmumaccuracyof100%onall11classesofthedataset. event-basedcameras. We,therefore,expectSTREAMtobe\nwell suited for sensor fusion of these modalities. Previous\nInadditiontoevent-basedvision,weevaluateSTREAM\nwork has shown that SSMs can benefit significantly from\non an event-based audio classification task. The Spiking\nself-supervised pre-training on larger datasets [20], which\nSpeechCommandsdataset[7]containsmorethan100000\nwewillexploreforourmethodinfutureresearch.\nsamples converted from the original Speech Commands\ndataset[47]withamediannumberof8100eventspersam-\nAcknowledgments\nple. The model dimension is again n = 32 with a smaller\nstate-space dimension of m = 4 compared to the vision\nMSissupportedwithfundsfromBosch-Forschungsstiftung\nmodel. Wedeploy8STREAMblocks(L=3infig.4),and imStifterverband. YBandKBwerefundedbytheGerman\nsetthesubsamplingfactorto8. Withthisconfiguration,we Academic Exchange Service (DAAD) under the funding\nreport a classification accuracy of 86.3% in tab.4, which programme WISE (57698568). DK is funded by the\nsettlesbetweenthesmallandlargemodelsreportedin[38]. German Federal Ministry of Education and Research\n8\nDVS128Gestures SpikingSpeechCommands\nModel\nsoftplus+Linear Validation softplus+Linear Validation\nt −t t −t\nk k−1 ∆ Γ Accuracy k k−1 ∆ Γ Accuracy\nMamba ✗ ✓ ✓ 98.6± 0.7 ✗ ✓ ✓ 86.8± 0.1\n✓ ✗ ✗ 99.2± 0.3 ✓ ✗ ✗ 87.9± 0.1\nSTREAM ✓ ✗ ✓ 98.8± 0.2 ✓ ✗ ✓ 87.9± 0.3\n(variants) ✓ ✓ ✗ 98.8± 0.6 ✓ ✓ ✗ 87.8± 0.2\n✓ ✓ ✓ 98.6± 1.2 ✓ ✓ ✓ 87.9± 0.3\nTable5. STREAMversusMamba[9]ontheDVS128Gestures[1]andSpikingSpeechCommands[7]datasets. STREAMdirectlyfeeds\nthe time coordinate of irregularly spaced events as a parameter to the state-space model as described in sec.3.3, which is indicated by\ncolumnt −t .DifferentvariantsofSTREAMmightapplylineartransformationsandSoftplusactivationssimilartoMambatoadjust\nk k−1\nthetimescalesofeA∆k (columns∆)orthefactorofΓ kinequation(12)(columnsΓ).\n(BMBF) within the project EVENTS (16ME0733). KKN andSegmentation. In2017IEEEConferenceonComputer\nisfundedbytheGermanFederalMinistryofEducationand Vision and Pattern Recognition (CVPR), pages 77–85, Los\nResearch(BMBF)withintheKI-ASICproject(16ES0996). Alamitos,CA,USA,2017.IEEEComputerSociety. 1,2,7\nCM receives funding from the German Research Foun- [5] GuangyanChen,MeilingWang,YiYang,KaiYu,LiYuan,\ndation (DFG, Deutsche Forschungsgemeinschaft) as andYufengYue.Pointgpt:Auto-regressivelygenerativepre-\npart of Germany’s Excellence Strategy – EXC 2050/1 – trainingfrompointclouds. InAdvancesinNeuralInforma-\nProject ID 390696704 – Cluster of Excellence “Centre tionProcessingSystems,pages29667–29679.CurranAsso-\nfor Tactile Internet with Human-in-the-Loop” (CeTI) of ciates,Inc.,2023. 1,2,5,7\nTechnische Universita¨t Dresden. This work was partially\n[6] JiaqiChen,YanYang,ShizhuoDeng,DaTeng,andLiyuan\nfunded by the German Federal Ministry of Education and\nPan. Spikmamba: Whensnnmeetsmambainevent-based\nResearch (BMBF) and the free state of Saxony within\nhumanactionrecognition,2024. 3,7,1\nthe ScaDS.AI center of excellence for AI research. The\n[7] BenjaminCramer,YannikStradmann,JohannesSchemmel,\nauthors gratefully acknowledge the computing time made\nandFriedemannZenke. Theheidelbergspikingdatasetsfor\navailable to them on the high-performance computer at\nthesystematicevaluationofspikingneuralnetworks. IEEE\nthe NHR Center of TU Dresden. This center is jointly\nTransactionsonNeuralNetworksandLearningSystems,33\nsupported by the Federal Ministry of Education and\n(7):2744–2757,2022. 8,9\nResearch and the state governments participating in the\n[8] RunpeiDong,ZekunQi,LinfengZhang,JunboZhang,Jian-\nNHR (www.nhr-verein.de/unsere-partner). The authors\njianSun,ZhengGe,LiYi,andKaishengMa. Autoencoders\ngratefully acknowledge the Gauss Centre for Supercom-\nascross-modalteachers:Canpretrained2dimagetransform-\nputing e.V. (www.gauss-centre.eu) for funding this project\nershelp3drepresentationlearning? InTheEleventhInter-\nby providing computing time on the GCS Supercomputer\nnationalConferenceonLearningRepresentations,2023. 7\nJUWELS [16] at Ju¨lich Supercomputing Centre (JSC).\n[9] AlbertGuandTriDao.Mamba:Linear-timesequencemod-\nelingwithselectivestatespaces,2024. 2,3,4,5,8,9,1\nReferences [10] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri\nDao,AtriRudra,andChristopherRe´. Combiningrecurrent,\n[1] ArnonAmir,BrianTaba,DavidBerg,TimothyMelano,Jef- convolutional,andcontinuous-timemodelswithlinearstate\nfrey McKinstry, Carmelo Di Nolfo, Tapan Nayak, Alexan- spacelayers. InAdvancesinNeuralInformationProcessing\nder Andreopoulos, Guillaume Garreau, Marcela Mendoza, Systems34:AnnualConferenceonNeuralInformationPro-\nJeff Kusnitz, Michael Debole, Steve Esser, Tobi Delbruck, cessingSystems2021,NeurIPS2021,December6-14,2021,\nMyron Flickner, and Dharmendra Modha. A low power, virtual,pages572–585,2021. 2,3\nfullyevent-basedgesturerecognitionsystem. In2017IEEE [11] AlbertGu,KaranGoel,andChristopherRe.Efficientlymod-\nConference on Computer Vision and Pattern Recognition elinglongsequenceswithstructuredstatespaces. InInter-\n(CVPR),pages7388–7397,2017. 2,5,7,8,9 nationalConferenceonLearningRepresentations,2022. 2,\n[2] AlexandreBittarandPhilipN.Garner. Asurrogategradient 4,1\nspikingbaselineforspeechcommandrecognition. Frontiers [12] Meng-HaoGuo,Jun-XiongCai,Zheng-NingLiu,Tai-Jiang\ninNeuroscience,16,2022. 8 Mu, Ralph R. Martin, and Shi-Min Hu. Pct: Point cloud\n[3] GuyEBlelloch. Prefixsumsandtheirapplications,1990. 5, transformer. Computational Visual Media, 7(2):187–199,\n2 2021. 7\n[4] R.QiCharles,HaoSu,MoKaichun,andLeonidasJ.Guibas. [13] YulanGuo,HanyunWang,QingyongHu,HaoLiu,LiLiu,\nPointNet:DeepLearningonPointSetsfor3DClassification and Mohammed Bennamoun. Deep learning for 3d point\n9\nclouds: A survey. IEEE Transactions on Pattern Analysis 23–27,2022,Proceedings,PartII,pages604–621.Springer,\nandMachineIntelligence,43(12):4338–4364,2021. 1 2022. 2,3,5,7\n[14] Ilyass Hammouamri, Ismail Khalfaoui-Hassani, and Tim- [27] YanRuPeiandOlivierCoenen.TENNs-PLEIADES:Build-\nothe´e Masquelier. Learning delays in spiking neural net- ing Temporal Kernels with Orthogonal Polynomials, 2024.\nworksusingdilatedconvolutionswithlearnablespacings.In arXiv:2405.12179. 3,7,8\nTheTwelfthInternationalConferenceonLearningRepresen- [28] EtiennePerot,PierredeTournemire,DavideNitti,Jonathan\ntations,2024. 8 Masci,andAmosSironi. Learningtodetectobjectswitha1\n[15] Cheng-Yao Hong, Yu-Ying Chou, and Tyng-Luh Liu. At- megapixeleventcamera.InAdvancesinNeuralInformation\ntention discriminant sampling for point clouds. In 2023 ProcessingSystems,pages16639–16652.CurranAssociates,\nIEEE/CVF International Conference on Computer Vision Inc.,2020. 1,3\n(ICCV),pages14383–14394,2023. 7 [29] AnhVietPhan,MinhLeNguyen,YenLamHoangNguyen,\n[16] Ju¨lich Supercomputing Centre. JUWELS Cluster and andLamThuBui. Dgcnn: Aconvolutionalneuralnetwork\nBooster:ExascalePathfinderwithModularSupercomputing overlarge-scalelabeledgraphs. NeuralNetworks,108:533–\nArchitectureatJuelichSupercomputingCentre. Journalof 543,2018. 7\nlarge-scaleresearchfacilities,7(A138),2021. 9 [30] MichaelPoli,StefanoMassaroli,EricNguyen,DanielYFu,\nTri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,\n[17] Sanghyeon Kim and Eunbyung Park. SMPConv: Self-\nandChristopherRe. Hyenahierarchy: Towardslargercon-\nMovingPointRepresentationsforContinuousConvolution.\nvolutionallanguagemodels. InProceedingsofthe40thIn-\nIn2023IEEE/CVFConferenceonComputerVisionandPat-\nternationalConferenceonMachineLearning,pages28043–\nternRecognition(CVPR),pages10289–10299,2023. ISSN:\n28078.PMLR,2023. 3\n2575-7075. 2\n[31] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\n[18] Xavier Lagorce, Garrick Orchard, Francesco Galluppi,\nGuibas. Pointnet++: Deephierarchicalfeaturelearningon\nBertramE.Shi,andRyadB.Benosman. Hots: Ahierarchy\npointsetsinametricspace. InAdvancesinNeuralInforma-\nofevent-basedtime-surfacesforpatternrecognition. IEEE\ntionProcessingSystems.CurranAssociates, Inc., 2017. 2,\nTransactionsonPatternAnalysisandMachineIntelligence,\n7\n39(7):1346–1359,2017. 3\n[32] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,\n[19] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di,\nHasanAbedAlKaderHammoud,MohamedElhoseiny,and\nandBaoquanChen.Pointcnn:Convolutiononx-transformed\nBernardGhanem. Pointnext: revisitingpointnet++withim-\npoints. InAdvancesinNeuralInformationProcessingSys-\nprovedtrainingandscalingstrategies. InProceedingsofthe\ntems.CurranAssociates,Inc.,2018. 7\n36th International Conference on Neural Information Pro-\n[20] DingkangLiang,XinZhou,WeiXu,XingkuiZhu,Zhikang\ncessingSystems,RedHook,NY,USA,2024.CurranAsso-\nZou,XiaoqingYe,XiaoTan,andXiangBai. Pointmamba:\nciatesInc. 7\nA simple state space model for point cloud analysis. In\n[33] AlecRadford,JeffreyWu,RewonChild,DavidLuan,Dario\nTheThirty-eighthAnnualConferenceonNeuralInformation\nAmodei, Ilya Sutskever, et al. Language models are unsu-\nProcessingSystems,2024. 2,3,5,6,7,8,1\npervisedmultitasklearners. OpenAIblog,2019. 6\n[21] PatrickLichtsteiner,ChristophPosch,andTobiDelbruck. A\n[34] Haoxi Ran, Jun Liu, and Chengjie Wang. Surface Repre-\n128×128120db15µslatencyasynchronoustemporalcon-\nsentationforPointClouds. In2022IEEE/CVFConference\ntrastvisionsensor. IEEEJournalofSolid-StateCircuits,43\nonComputerVisionandPatternRecognition(CVPR),pages\n(2):566–576,2008. 1\n18920–18930, LosAlamitos, CA,USA,2022.IEEECom-\n[22] ChangLiu,XiaojuanQi,EdmundY.Lam,andNgaiWong.\nputerSociety. 7\nFast classification and action recognition with event-based\n[35] HongweiRen,YueZhou,HaotianFU,YulongHuang,Ren-\nimaging. IEEEAccess,10:55638–55649,2022. 7\njingXu,andBojunCheng.Ttpoint:Atensorizedpointcloud\n[23] HaotianLiu, MuCai, andYongJaeLee. Maskeddiscrim- networkforlightweightactionrecognitionwitheventcam-\nination for self-supervised learning on point clouds. Pro- eras. InProceedingsofthe31stACMInternationalConfer-\nceedings of the European Conference on Computer Vision enceonMultimedia,page8026–8034,NewYork,NY,USA,\n(ECCV),2022. 7 2023.AssociationforComputingMachinery. 2\n[24] XuMa,CanQin,HaoxuanYou,HaoxiRan,andYunFu.Re- [36] Hongwei Ren, Yue Zhou, Jiadong Zhu, Haotian Fu, Yu-\nthinkingnetworkdesignandlocalgeometryinpointcloud: longHuang,XiaopengLin,YuetongFang,FeiMa,HaoYu,\nAsimpleresidualMLPframework.InInternationalConfer- andBojunCheng. Rethinkingefficientandeffectivepoint-\nenceonLearningRepresentations,2022. 7 based networks for event camera classification and regres-\n[25] DanielMaturanaandSebastianScherer. Voxnet: A3dcon- sion:Eventmamba,2024. 2,3,7,1\nvolutional neural network for real-time object recognition. [37] Simon Schaefer, Daniel Gehrig, and Davide Scaramuzza.\nIn 2015 IEEE/RSJ International Conference on Intelligent Aegnn: Asynchronous event-based graph neural networks.\nRobotsandSystems(IROS),pages922–928,2015. 1 In Proceedings of the IEEE/CVF Conference on Com-\n[26] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, puterVisionandPatternRecognition(CVPR),pages12371–\nYonghongTian,andLiYuan.Maskedautoencodersforpoint 12381,2022. 3,7\ncloudself-supervisedlearning. InComputerVision–ECCV [38] MarkScho¨ne,NeerajMohanSushma,JingyueZhuge,Chris-\n2022: 17thEuropeanConference,TelAviv,Israel,October tianMayr,AnandSubramoney,andDavidKappel. Scalable\n10\nevent-by-eventprocessingofneuromorphicsensorysignals [52] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie\nwithdeepstate-spacemodels. InACM/IEEEInternational Zhou, and Jiwen Lu. Point-BERT: Pre-training 3D Point\nConferenceonNeuromorphicSystems.IEEE,2024. 3,7,8 Cloud Transformers with Masked Point Modeling . In\n[39] YusukeSekikawa,KosukeHara,andHideoSaito. Eventnet: 2022 IEEE/CVF Conference on Computer Vision and Pat-\nAsynchronousrecursiveeventprocessing.InProceedingsof ternRecognition(CVPR),pages19291–19300,LosAlami-\ntheIEEE/CVFConferenceonComputerVisionandPattern tos,CA,USA,2022.IEEEComputerSociety. 2,7\nRecognition(CVPR),2019. 3,7 [53] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\n[40] Jimmy T.H. Smith, Andrew Warrington, and Scott Linder- Chun,JunsukChoe,andYoungjoonYoo. Cutmix: Regular-\nman. Simplified state space layers for sequence modeling. izationstrategytotrainstrongclassifierswithlocalizablefea-\nInTheEleventhInternationalConferenceonLearningRep- tures. InProceedingsoftheIEEE/CVFInternationalCon-\nresentations,2023. 3,1,2 ferenceonComputerVision(ICCV),2019. 7\n[41] Taylan Soydan, Nikola Zubic´, Nico Messikommer, Sid- [54] Renrui Zhang, Ziyu Guo, Rongyao Fang, Bin Zhao, Dong\ndharthaMishra,andDavideScaramuzza. S7: Selectiveand Wang,YuQiao,HongshengLi,andPengGao. Point-m2ae:\nsimplifiedstatespacelayersforsequencemodeling,2024.3, multi-scalemaskedautoencodersforhierarchicalpointcloud\n7,8 pre-training. InProceedingsofthe36thInternationalCon-\n[42] Anand Subramoney, Khaleelulla Khan Nazeer, Mark ference on Neural Information Processing Systems, Red\nScho¨ne,ChristianMayr,andDavidKappel. Efficientrecur- Hook,NY,USA,2024.CurranAssociatesInc. 7\nrentarchitecturesthroughactivitysparsityandsparseback- [55] Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Ji, and\npropagation through time. In The Eleventh International Shuicheng Yan. Point cloud mamba: Point cloud learning\nConferenceonLearningRepresentations,2023. 3 viastatespacemodel. CoRR,abs/2403.00762,2024. 3,7,1\n[43] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, [56] JiazhouZhou,XuZheng,YuanhuiyiLyu,andLinWang.Ex-\nDuc Thanh Nguyen, and Sai-Kit Yeung. Revisiting point act: Language-guidedconceptualreasoninganduncertainty\ncloudclassification: Anewbenchmarkdatasetandclassifi- estimationforevent-basedactionrecognitionandmore. In\ncationmodelonreal-worlddata.InInternationalConference 2024 IEEE/CVF Conference on Computer Vision and Pat-\nonComputerVision(ICCV),2019. 2,5,6,7 ternRecognition(CVPR),pages18633–18643,2024. 1,3,\n[44] Peng-Shuai Wang. Octformer: Octree-based transformers 7\nfor3dpointclouds. ACMTrans.Graph.,42(4),2023. 7 [57] Nikola Zubic, Mathias Gehrig, and Davide Scaramuzza.\n[45] Qinyi Wang, Yexin Zhang, Junsong Yuan, and Yilong Lu. StateSpaceModelsforEventCameras. In2024IEEE/CVF\nSpace-time event clouds for gesture recognition: From rgb Conference on Computer Vision and Pattern Recognition\ncamerastoeventcameras. In2019IEEEWinterConference (CVPR),pages5819–5828,LosAlamitos,CA,USA,2024.\non Applications of Computer Vision (WACV), pages 1826– IEEEComputerSociety. 1,3\n1835,2019. 2\n[46] Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei\nPokrovsky, and Raquel Urtasun. Deep Parametric Contin-\nuous Convolutional Neural Networks. In 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages2589–2597,2018. ISSN:2575-7075. 1,2,4\n[47] Pete Warden. Speech commands: A dataset for limited-\nvocabularyspeechrecognition,2018. 8\n[48] Wenxuan Wu, Zhongang Qi, and Li Fuxin. PointConv:\nDeep Convolutional Networks on 3D Point Clouds. In\n2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 9613–9622, 2019. ISSN:\n2575-7075. 1,2,4\n[49] WenxuanWu,LiFuxin,andQiShan.PointConvFormer:Re-\nvengeofthePoint-basedConvolution. In2023IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR),pages21802–21813,2023. ISSN:2575-7075. 2\n[50] ZhirongWu, ShuranSong, AdityaKhosla, FisherYu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d\nshapenets: Adeeprepresentationforvolumetricshapes. In\nProceedings of the IEEE Conference on Computer Vision\nandPatternRecognition(CVPR),2015. 1\n[51] ZhirongWu, ShuranSong, AdityaKhosla, FisherYu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d\nshapenets: A deep representation for volumetric shapes.\nIn2015IEEEConferenceonComputerVisionandPattern\nRecognition(CVPR),pages1912–1920,2015. 2,6\n11\nSTREAM: A Universal State-Space Model for Sparse Geometric Data\nSupplementary Material\n6.Derivations Notethatequation(18)canbeevaluatedonallcoordinates\nt>t . Thetwocanonicaloptionsfordiscretizingthevari-\n0\nThissectionprovidescompletederivationsofequations(6)\nableshandyareequidistantstepsresultinginaregulargrid,\nand (7). We restrict to the case of single-input single-\norusingtheinputcoordinatest ,...,t .Whilemoststate-\n0 N\noutput (SISO) state-space models with multi-dimensional\nspace model works discretize on equidistant steps, [40]\nstate. Multi-input multi-output (MIMO) formulations like\nshows on a toy task that the SSM formulation is capable\nS4[11]orMamba[9]canbeobtainedbycreatingnparal-\nofsolvingtaskswithirregularlyspacedstepsaswell.\nlel instances of the SISO model and mixing the input and\nOurworkdiffersfromotherrecentMambabasedmodels\noutputcomponentswithlinearlayers.\nsuchasPointMamba[20],PointCloudMamba[55],Event-\nOurmotivationwastoreasonaboutspatialrelationships\nMamba[36],orSpikMamba[6]byintegratingthetruetim-\nbetweeninputpulses. Therefore,wedefinedaninteraction\ningsoftheinputsinthefollowingsense.\nkernelΦ(x,x′)thatmodelspairwiseinteractions. Acom-\nWewilluseh = 0fornotationalsimplicityinthefol-\n0\nplete view of all possible interactions with the point x is\nk lowing. Discretizingequation(18)ontheDiracdeltacoded\ngivenby\ninput(1)thenyields\n(cid:90)\ny(x k)= Φ(x k,x′)u(x′)dx′ (14) (cid:90) tk (cid:18)(cid:90) tk (cid:19) (cid:88)N\nh(t )= exp A(t′)dt′ B(t) δ(t−t )u dt\nk i i\nN\n(cid:88) t0 t i=0\n= Φ(x ,x )u . (15)\ni=0\nk i i =(cid:88)k exp(cid:18)(cid:90) tk A(t)dt(cid:19)\nB(t )u . (20)\ni i\nWe will now derive a parameterization of the interaction i=0 ti\nkernelΦwithastate-spacemodel.\nWe decompose the integral from t to t into the integrals\nA linear time-varying state-space model acting on a i k\nfromt tot andsumoverthem\nscalar input function u(t) and producing a scalar output j−1 j\ny(t)isdefinedthrough  \n(cid:88)k (cid:88)k (cid:90) tj\nh˙ (t)=A(t)h(t)+B(t)u(t) (16) h(t k)= exp A(t)dtB(t i)u i. (21)\ny(t)=C(t)h(t) , (17) i=0 j=i+1 tj−1\nwith u(t),y(t)∈R, states h(t)∈Rm and parameters WewillfurtherassumethatA(t),B(t),C(t)areconstant\nA(t)∈Rm×m,B(t)∈Rm×1,C(t)∈R1×m. on the intervals (t j−1,t j] for j = i+1,...,k. Denoting\nNote that we stick to our notation introduced in sec.3, A(t j)=A j,B(t j)=B j,C(t j)=C j,h(t k)=h k,and\ndenotingvectorsandmatriceswithboldfaceandscalarval- ∆ j =t j −t j−1weget\nueswithregularface.\n \nk k\n6.1.SolutionoftheLinearState-spaceModel (cid:88) (cid:88)\nh(t k)= exp A j∆ jB iu\ni\n(22)\nLinear ordinary differential equations have a well known i=0 j=i+1\n \nanalyticalsolution. Wereferthereadertostandardcalculus k k\n(cid:88) (cid:89)\ntextbooks. As such, the linear dynamics of equation (16) =  exp(A j∆ j)B iu i. (23)\nforinitialvalueh 0 =h(t 0)admittheanalyticalsolution i=0 j=i+1\n(cid:90) t (cid:18)(cid:90) t (cid:19)\nh(t)=h + exp A(t′′)dt′′ B(t′)u(t′)dt′, Comparingtoequation(15),wereadoffthediscretekernel\n0\nfunction\nt0 t′\n(18)\n \nk\n(cid:89)\nwhich can be checked by taking the derivative of h and Φ(t k,t i)=C k exp(A j∆ j)B i. (24)\ncomparing it to equation (16). Comparing equations (18),\nj=i+1\n(17)and(14),wereadoffthekernel\nProposition1. Thekernelsparameterizedbyequation(24)\n(cid:18)(cid:90) t (cid:19)\nΦ(t,t′)=C(t)exp A(t′′)dt′′ B(t′) (19) contain convolution operations with rational kernels as a\nt′ specialcase.\n1\nProof. Considerthelineartime-invariant(LTI)casewhere withthebinaryoperator•definedthrough\nA,B,Careconstantforallt. Then\nc •c =[a ·a ,a ·b +b ] . (28)\ni j i j j i j\nk\n(cid:89)\nexp(A ∆ )=exp(A(t −t )) (25) As[3]shows,theoperator•isassociative,i.e.\nj j k i\nj=i+1\n(c •c )•c =c •(c •c ) . (29)\ni j k i j k\nThe proposition follows from the fundamental result from\nlinear systems theory that LTI state-space models can rep- Associative operators can be parallelized to run in\nresenteveryrationaltransferfunction. O(logN)timeonsequenceoflengthN givensufficiently\nmany processors [3]. We use this primitive to parallelize\n6.2.RecursiveComputationoftheInteractionKer- ourmodel. Thepairscareinitializedas\nnel\nc =(cid:2) eA∆0,B u (cid:3) ... c =(cid:2) eA∆N,B u (cid:3) (30)\n0 0 0 N N N\nLet’sexpandequation(23)inarecursiveform\nFig.5 shows an example of how the Scan primitive com-\n \nk k putes the entire recurrence in equation (26) in roughly\n(cid:88) (cid:89)\nh k =h(x k)=  eAj∆jB iu i 2logN steps.\ni=0 j=i+1\n \nk−1 k\n(cid:88) (cid:89)\n=Bu k+  eAj∆jB iu i\ni=0 j=i+1\n \nk−1 k−1\n(cid:88) (cid:89)\n=Bu k+ eAk∆k eAj∆jB iu\ni\ni=0 j=i+1\n \nk−1 k−1\n(cid:88) (cid:89)\n=Bu k+eAk∆k  eAj∆jB iu i\ni=0 j=i+1\n=Bu k+eAk∆kh k−1.\nThisconcludesthederivationofourrecurrentoperator\nh\nk\n=Bu k+eAk∆kh k−1. (26)\nInlinewithmostrecentSSMworks,wechooseA ≡Aas\ni\nadiagonalmatrixforalli.\nRemark 1. Equation (26) allows asynchronous inference\nonstreamsofincomingcoordinatesinO(1)timepercoor-\ndinate,orO(N)timeforthefullstreamofcoordinates.\n7.Scan\nLineartime-varyingsystemssuchastheonegivenbyequa-\ntion (26) resemble an associative operation, with well-\nknowntimecomplexityofO(logN)[3]. Thegoalofthis\nsection is not to provide a proof, but to give the reader a\nclear idea of how irrgularly spaced sequences can be par-\nallelized in the same way that regular SSMs can be paral-\nlelized with the Scan primitive [9, 40]. The presentation\nfollows[3].\nConsiderthepair\nc =[a ,b ] (27)\ni i i\n2\nFigure5.Caption\n3",
    "pdf_filename": "STREAM_A_Universal_State-Space_Model_for_Sparse_Geometric_Data.pdf"
}