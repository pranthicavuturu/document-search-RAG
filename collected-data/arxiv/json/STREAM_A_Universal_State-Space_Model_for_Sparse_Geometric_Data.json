{
    "title": "STREAM A Universal State-Space Model for Sparse Geometric Data",
    "context": "Handling sparse and unstructured geometric data, such as point clouds or event-based vision, is a pressing challenge in the field of machine vision. Recently, sequence models such as Transformers and state-space models entered the domain of geometric data. These methods require special- ized preprocessing to create a sequential view of a set of points. Furthermore, prior works involving sequence mod- els iterate geometric data with either uniform or learned step sizes, implicitly relying on the model to infer the un- derlying geometric structure. In this work, we propose to encode geometric structure explicitly into the parameteriza- tion of a state-space model. State-space models are based on linear dynamics governed by a one-dimensional vari- able such as time or a spatial coordinate. We exploit this dynamic variable to inject relative differences of coordi- nates into the step size of the state-space model. The re- sulting geometric operation computes interactions between all pairs of N points in O (N) steps. Our model deploys the Mamba selective state-space model with a modified CUDA kernel to efficiently map sparse geometric data to modern hardware. The resulting sequence model, which we call STREAM, achieves competitive results on a range of bench- marks from point-cloud classification to event-based vision and audio classification. STREAM demonstrates a pow- erful inductive bias for sparse geometric data by improv- ing the PointMamba baseline when trained from scratch on *These authors contributed equally to this work. the ModelNet40 and ScanObjectNN point cloud analysis datasets. It further achieves, for the first time, 100 % test accuracy on all 11 classes of the DVS128 Gestures dataset. Computer vision computes relationships between data points in spatial coordinates X, Y (in R2) or X, Y , Z (in R3), or spatio-temporal coordinates (R3 or R4), where one of the dimensions denotes time t. Convolutional neural net- works based on structured, uniformly spaced and local lin- ear operations successfully address this problem for classi- cal camera recordings such as images or videos, which are themselves structured and discrete. Many modalities of re- cent interest, however, are neither structured nor uniformly spaced. Sensors such as Light Detection and Ranging (Li- DAR) or event-based cameras [21, 28] sample signals based on sparse processes, resulting in sparse geometric data with irregularly spaced coordinates. Point cloud analysis was the central research field for sparse geometric data over the past decade [13]. While early works followed voxel- based approaches [25, 50], point-based methods dominate the research landscape today [4, 5, 46, 48]. Meanwhile, event-based cameras [21, 28] raised considerable interest in the computer vision community. Although these cam- eras record sparse geometric data, most works collapse the sparse stream of events into frames [28, 56, 57], potentially losing unique properties of these cameras such as low la- tency and high dynamic range. The structural similarity be- 1 arXiv:2411.12603v1  [cs.CV]  19 Nov 2024",
    "body": "STREAM: A Universal State-Space Model for Sparse Geometric Data\nMark Sch¨one1*\nYash Bhisikar2*\nKaran Bania2*\nKhaleelulla Khan Nazeer1\nChristian Mayr1,3,4\nAnand Subramoney3\nDavid Kappel4\n1TUD Dresden University of Technology\n2BITS Pilani\n3Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI)\n4Centre for Tactile Internet (CeTI) with Human-in-the-Loop\n5Royal Holloway, University of London\n6Bielefeld University\n{mark.schoene, khaleelulla.khan, christian.mayr}@tu-dresden.de\n{f20210483, f20212582}@goa.bits-pilani.ac.in\nanand.subramoney@rhul.ac.uk\ndavid.kappel@uni-bielefeld.de\nAbstract\nHandling sparse and unstructured geometric data, such as\npoint clouds or event-based vision, is a pressing challenge\nin the field of machine vision. Recently, sequence models\nsuch as Transformers and state-space models entered the\ndomain of geometric data. These methods require special-\nized preprocessing to create a sequential view of a set of\npoints. Furthermore, prior works involving sequence mod-\nels iterate geometric data with either uniform or learned\nstep sizes, implicitly relying on the model to infer the un-\nderlying geometric structure. In this work, we propose to\nencode geometric structure explicitly into the parameteriza-\ntion of a state-space model. State-space models are based\non linear dynamics governed by a one-dimensional vari-\nable such as time or a spatial coordinate. We exploit this\ndynamic variable to inject relative differences of coordi-\nnates into the step size of the state-space model. The re-\nsulting geometric operation computes interactions between\nall pairs of N points in O (N) steps. Our model deploys the\nMamba selective state-space model with a modified CUDA\nkernel to efficiently map sparse geometric data to modern\nhardware. The resulting sequence model, which we call\nSTREAM, achieves competitive results on a range of bench-\nmarks from point-cloud classification to event-based vision\nand audio classification. STREAM demonstrates a pow-\nerful inductive bias for sparse geometric data by improv-\ning the PointMamba baseline when trained from scratch on\n*These authors contributed equally to this work.\nthe ModelNet40 and ScanObjectNN point cloud analysis\ndatasets. It further achieves, for the first time, 100 % test\naccuracy on all 11 classes of the DVS128 Gestures dataset.\n1. Introduction\nComputer vision computes relationships between data\npoints in spatial coordinates X, Y (in R2) or X, Y , Z (in\nR3), or spatio-temporal coordinates (R3 or R4), where one\nof the dimensions denotes time t. Convolutional neural net-\nworks based on structured, uniformly spaced and local lin-\near operations successfully address this problem for classi-\ncal camera recordings such as images or videos, which are\nthemselves structured and discrete. Many modalities of re-\ncent interest, however, are neither structured nor uniformly\nspaced. Sensors such as Light Detection and Ranging (Li-\nDAR) or event-based cameras [21, 28] sample signals based\non sparse processes, resulting in sparse geometric data with\nirregularly spaced coordinates. Point cloud analysis was\nthe central research field for sparse geometric data over\nthe past decade [13]. While early works followed voxel-\nbased approaches [25, 50], point-based methods dominate\nthe research landscape today [4, 5, 46, 48]. Meanwhile,\nevent-based cameras [21, 28] raised considerable interest\nin the computer vision community. Although these cam-\neras record sparse geometric data, most works collapse the\nsparse stream of events into frames [28, 56, 57], potentially\nlosing unique properties of these cameras such as low la-\ntency and high dynamic range. The structural similarity be-\n1\narXiv:2411.12603v1  [cs.CV]  19 Nov 2024\n\nFigure 1. A unified view on point cloud and event stream mod-\neling. A. Adjacency structure of point based methods B. The co-\nordinates of sparse geometric data are irregularly spaced C. Point\nclouds and event streams are ordered by spatial and temporal axes,\nrespectively, before the state-space model.\ntween point clouds and event streams encourages method-\nological transfer, especially from the domain of point cloud\nanalysis to event-based vision [35, 45]. Our work demon-\nstrates the reverse: The inherently temporal state-space\nmodel formulation presented in sec. 3.2 improves event-\nbased vision on the DVS128-Gesture dataset [1] to 100 %,\nand at the same time provides a valuable inductive bias for\ngeometric data such as point clouds as demonstrated on the\nModelNet40 and ScanObjectNN datasets [43, 51].\nWe show that sparse geometric data with irregularly\nspaced coordinates as shown in fig. 1B can be naturally in-\ntegrated in the state-space model formalism. In contrast to\nprior sequence models such as PointMamba [20] or Event-\nMamba [36], we explicitly encode geometric structure into\nthe state-space model parameterization by injecting relative\ndifferences of coordinates as step sizes.\nContributions. This paper makes the following main inno-\nvations over the state-of-the-art:\n• We propose a unified framework for modeling sparse\ngeometric data with state-space models with irregularly\nspaced step sizes. The resulting model, STREAM, han-\ndles sparse geometric data such as point clouds and event-\nbased vision.\n• Our state-space model formulation demonstrates a valu-\nable inductive bias for geometric structure by improv-\ning the PointMamba [20] baseline by up to 2.84 % when\ntrained from scratch on the ScanObjectNN dataset [43].\n• We show compelling results for event-based vision, pro-\ncessed as a stream of events with our purely recurrent neu-\nral network without the usage of frames or spatial convo-\nlutions. For the first time, we demonstrate 100 % classifi-\ncation accuracy on all 11 classes of the DVS128-Gestures\ndataset [1].\n• We provide an efficient CUDA implementation for in-\ntegrating irregular step sizes based on the selective-scan\nimplementation of [9].\n2. Related Work\n2.1. Point Cloud Analysis\nEarly point-based methods such as PointNets [4, 31] di-\nrectly pass the point cloud through a Multi-layer Percep-\ntron (MLP) and introduce a permutation invariance of the\nset of points via pooling operations, implicitly integrating\nspatial information. In contrast, methods for explicitly in-\ntegrating spatial information parameterize linear operators\nsuch as convolutions based on relative differences between\npoint coordinates. The convolutional neural networks based\non irregularly spaced operators presented in [17, 46, 48, 49]\noutperform PointNets on a range of point cloud analysis\ntasks.\nThese works parameterize the convolution kernel\nwith MLPs evaluated at the relative differences between\npoint coordinates. To break the O\n\u0000N 2\u0001\ncomplexity of com-\nputing pairwise interactions for all N points, locality con-\nstrains are added to scale to larger point sets (see fig. 1A).\nAs shown in fig. 2A, our method also explicitly integrates\nspatial information in this sense, but with a kernel parame-\nterized by a state-space model (SSM) instead of a MLP (see\nequation (3)). This parameterization enables the complete\ncomputation to be performed in O (N) steps.\nMore recently, sequence models demonstrated favorable\nresults over convolutional methods [5, 20, 26, 52].\nThe\npoint cloud is flattened into a sequence and then processed\nby transformer or state-space model based backbones. This\nformulation inherits many strong scaling properties of se-\nquence models including masked pre-training [52] or gen-\nerative pre-training [5] objectives. Transformers, however,\nsuffer from O\n\u0000N 2\u0001\ncomplexity in the number of points N,\nlimiting their application to larger point clouds. State-space\nmodels, in contrast, have sequential O (N) complexity for\ninference and O (N log N) complexity for parallel train-\ning [10, 11]. At the same time, SSMs can learn long dis-\ntant relationships between inputs including convolutional\n2\n\noperators with rational transfer functions [10, 30].\nThe\nrecently introduced Mamba state-space model [9] sparked\nwidespread discourse, and was quickly adapted in many do-\nmains, including point cloud analysis. Works like Point-\nMamba [20] or Point Cloud Mamba (PCM) [55] deploy\nMamba in established point cloud frameworks such as the\nmasked autoencoder developed in [26].\nBy relying on\nthe default Mamba architecture, spatial information is in-\ntegrated implicitly in these works similar to PointNets. In\ncontrast, we show in sec. 3.2 that spatial information can\nbe integrated explicitly via the SSM parameterization. This\nformulation allows us to remove specialized preprocessing\nmethods to represent point clouds as sequences [20, 55],\nand simply order by the spatial coordinates X, Y , or Z.\n2.2. Event-based Vision\nMost state-of-the-art event-based vision methods construct\nframes from the asynchronous event-stream by binning\nevents in regularly spaced time intervals [6, 28, 42, 56, 57].\nFew methods operate on the sparse unstructured event\nstreams directly. AEGNN [37] computes spatio-temporal\nfeatures based on subsampled event streams with graph neu-\nral networks. Their method admits an event-based infer-\nence method where only nodes that correspond to incom-\ning events are updated asynchronously. EventMamba [36]\nis tightly related to the point cloud analysis models dis-\ncussed in sec. 2.1.\nThey process subsampled raw event\nstreams similar to [37, 39] with a combination of a point\nfeature extractor and Mamba.\nIts high similarity with\nPointMamba [20] places EventMamba in the category of\nmethods implicitly integrating spatio-temporal information.\nConceptually closer to our model are filtering methods that\nprocess the stream of events recursively while explicitly in-\ntegrating spatio-temporal information. Early methods like\nHOTS [18] create a spatial representation of the (X, Y )-\nplane of an event-based camera by integrating exact times-\ntamps with exponentially moving averages. Similar to our\nmethod, EventNet [39] recursively iterates the event stream.\nIn contrast to our state-space model, their recurrent network\nparameterization does not inherit stability guarantees from\nthe theory of linear systems and lacks an efficient paral-\nlelization along the time dimension during training, limiting\nthe scalability to longer streams. Event-SSM [38] proposed\na discretization method for simplified state-space layers\n(S5) [40] to operate directly on asynchronous event streams.\nTheir model excels at spiking audio classification, but falls\nbehind the state-of-the-art in event-based vision. Concur-\nrently to our work, S7 [41] explores improvements of the S5\narchitecture and Event-SSM through a more stable param-\neterization and input dependent state-space model param-\neters. In contrast, STREAM can improve sequence-based\npoint cloud analysis (see sec. 4.1), and therefore drive the\nconvergence between event-based vision and point cloud\nanalysis forward.\nRelated from the perspective of discretizing continuous\nkernels is the TENNs framework [27]. They construct con-\nvolutions over finite time horizons with orthogonal polyno-\nmials that can be discretized on irregularly spaced times-\ntamps. In practice, they choose regularly spaced frames,\nhowever, and show compelling results on event-based vi-\nsion benchmarks. In comparison, STREAM parameterizes\ninfinite time horizons via the state-space model.\n3. STREAM\nIn this section, we present our method for efficiently\nmodeling sparse sets of N coordinates {xi}N\ni=0 , where\nxi =\n\u0010\nx(1)\ni , . . . , x(d)\ni\n\u0011\n∈Rd. This general formulation in-\ncludes data in 3D space, with x = (X, Y, Z), as well as the\nstreams recorded by event-based cameras. In the latter case\none of the dimensions (e.g. x(1)) is interpreted as event\ntime while others (x(2), x(3)) denote spatial locations (see\nfig. 1C). Furthermore, higher dimensions (d > 3) can be\nused to denote data that includes other dimensions, such as\ncolor intensity, etc. STREAM is illustrated in fig. 2A-C.\nNotation. We denote tensor-valued variables and functions\nsuch as multi-dimensional coordinates x ∈Rd or matrices\nA ∈Rm×m in bold characters. Scalar variables such as the\ncoordinates x(1), . . . , x(d) of a tensor x or the entries Aij of\na matrix A are denoted in regular characters.\n3.1. Problem Statement\nConsider a set {xi}N\ni=0 of points sparsely distributed in\nspace. We assign a multi-dimensional representation vector\nui to each coordinate xi as common practice in machine\nlearning. Such input signals can be formulated mathemati-\ncally as sums of Dirac delta pulses evaluated at the coordi-\nnates {xi}N\ni=0\nu (x) =\nN\nX\ni=0\nδ (x −xi) ui ,\n(1)\nwhere ui ∈Rn is the representation at xi. To reason about\nspatial relationships between these pulses, we define an in-\nteraction kernel Φ(x, x′) that models pairwise interactions.\nA complete view of all possible interactions with the point\nxk is given by\ny (xk) =\nZ\nΦ (xk, x′) u (x′) dx′\n(2)\n=\nN\nX\ni=0\nΦ (xk, xi) ui .\n(3)\nThis formulation contains the familiar convolution operator\nas a special case when Φ (x, x′) ≡Φ (x −x′). Equation\n3\n\nFigure 2. STREAM encodes geometric information into the SSM parameters. A. Point cloud input. B. The STREAM module converts\nrelative differences in coordinates into the ∆k scale of the SSM. C. A STREAM module integrates pairwise spatial relationships based on\nan exponentially oscillating kernel.\n(3) highlights that the computation of y (xi) at the coordi-\nnates {xi}N\ni=0 requires evaluating Φ (xi, xj) only at pairs\nof these coordinates. In the general case, this operation re-\nquires O\n\u0000N 2\u0001\nsteps to compute. As shown in fig. 1A, this\ncomplexity can be reduced to O (kN) by focusing on local\nneighborhoods of k points, a technique broadly used in the\nliterature of point convolutions [46, 48]. In the following\nsections, we present a method for recursively integrating N\nsparsely sampled points in O (N) time without restricting\nto local neighborhoods.\n3.2. Integrating Sparse Coordinates with State-\nspace Models\nState-space models (SSMs) have demonstrated outstand-\ning performance on long-range dependency modeling tasks,\nwhile remaining efficient for training and inference [9, 11].\nSSMs project the input through a linear recursive kernel to\nstates h (t). The independent variable t is usually inter-\npreted as, and aligned with, time in the input signal domain.\nThe linear nature of SSMs allows the exact integration of\nsparse sets of points with irregularly spaced coordinates, as\nwe will show below. A linear time-varying SSM acting on\na scalar input function u (t) and producing a scalar output\ny (t) is defined through\n˙h (t) = A (t) h (t) + B (t) u (t)\n(4)\ny (t) = C (t) h (t) ,\n(5)\nwith u(t), y(t) ∈R, states h (t) ∈Rm and parameters\nA (t) ∈Rm×m, B (t) ∈Rm×1, C (t) ∈R1×m.\nFor the\nsake of simplicity, we assume that the parameters are rep-\nresented by piecewise constant functions, i.e. Ai = A (ti),\nBi = B (ti), Ci = C (ti) are constant on the intervals\n(ti−1, ti]. We show in appendix 6 that solving the linear\nsystem in equations (4) and (5) for the input in equation (1)\nyields the kernel function\nΦ (tk, ti) = Ck\n\n\nk\nY\nj=i+1\nexp (Aj∆j)\n\nBi ,\n(6)\nwhere ∆j = tj −tj−1. Importantly, the output y(t) of the\nstate-space model at t0, . . . , tN can be computed with the\nrecursive formula\nhk = h (tk) = eAk∆khk−1 + Bkuk\n(7)\nyk = y(tk) = Ckhk .\n(8)\nEquations (6) and (7) highlight how relative differences\n∆j = tj −tj−1 in the coordinate t explicitly parameterize\nour pairwise interaction kernel Φ. An example of the inter-\naction defined by equation (6) can be found in Fig. 2C. For\ncomplex A ∈C, the exponentially oscillating kernel eA∆\nintegrates the history of signals ui occuring in irregularly\nspaced intervals ∆i with i < k. With this parameterization,\nour method differs from other Mamba based works, where\n4\n\n∆is a function of ui that doesn’t explicitly take the dynam-\nics of t into account. A complete derivation can be found in\nappendix 6.\nThe recursive formulation in equation (7) is a lin-\near recurrent neural network, whose state-to-state tran-\nsition matrix eAk∆k is parameterized by the differences\n∆k = tk −tk−1 in the irregularly spaced coordinates ti. As\nsuch, our method positions itself among the methods dis-\ncussed in sec. 2.1 that explicitly integrate spatial informa-\ntion through the parameterization of a linear operator with\nthe coordinates.\nThe output sequence y (t0) , . . . , y (tN) can be computed\nsequentially in O (N) time. This allows efficient inference\non streams of pulses recorded by sensors such as event-\nbased cameras or LiDAR. At the same time, linear recursive\nequations admit an efficient parallelization via the Scan\nprimitive available in CUDA [3]. On sufficiently many pro-\ncessors, the output sequence can be computed in O (log N)\ntime, which allows massive parallelization on very long se-\nquences of up to 1 million inputs [9].\nWe show in ap-\npendix 7 how our formulation maps to the scan primitive.\n3.3. Selective state-space models and STREAM\nWe develop our method based on the selective state-space\nmodel also known as Mamba [9], which was designed for\nregularly spaced modalities, and language modeling in par-\nticular. In accordance with this purpose, the integration time\nsteps ∆i and the matrices Bi, Ci are functions of the input\nsignal ui, and do not carry explicit information about spatial\nrelationships. Formally, Mamba is parameterized by\nAi ≡A\n∀i\n∆i = Ψ (Linear (ui))\n(9)\nBi = ∆iLinear (ui)\nCi = Linear (ui) ,\n(10)\nwhere A is a learned diagonal matrix, and Ψ (u) = ln(1 +\neu) is the softplus function. This single-input ui single-\noutput yi SSM is then repeated n times and wrapped by lin-\near transformations to create a multi-input ui multi-output\nyi model as shown in fig. 2B. Note that the total state size\nis nm in the multi-input multi-output case.\nWe propose to explicitly represent the irregularly spaced\npulse timings ti in the Mamba parameterization accord-\ning to equation (7).\nTherefore, we explicitly set ∆i to\n(ti −ti−1) Ψ (δ), where δ is a trainable time scale param-\neter, in contrast to equation (9). We further decouple the\ncoefficient of Bi in equation (10) from the time differences\n∆i to avoid zero coefficients in case of overlapping pulses\nti −ti−1 = 0. With these adjustments, our model is defined\nby\nAi ≡A ∀i\n∆i = (ti −ti−1) Ψ (δ)\n(11)\nBi = ΓiLinear (ui)\nCi = Linear (ui)\n(12)\nΓi = Ψ (Linear (ui)) ,\n(13)\nwhere δ\n∈\nRn is a learnable parameter.\nWe call\nour resulting state-space model parameterization STREAM\nfor Spatio-Temporal Recursive Encoding of Asynchronous\nModalities.\n3.4. Modeling Point Clouds with STREAM\nThe explicit coordinate t in the STREAM module is one-\ndimensional and strictly ordered.\nFor the case of event\nstreams, this ordering corresponds to temporal order from\npast to future. Point clouds fit into this framework by or-\ndering the set of points w.r.t. one of the spatial coordinates\nfrom left to right. As shown in fig. 2A,B, we select one of\nthe coordinates to be integrated explicitly. Inserting the X\ncoordinate for t in equation (11) without loss of generality\nand setting uk = Θ (Xk, Yk, Zk), where Θ : R3 −→Rn is\na point wise encoder, yields our STREAM formulation for\npoint cloud analysis. To achieve state-of-the-art results, we\nsort the point set by all three spatial coordinates respectively\nand feed the concatenation of the three resulting sequences\nto STREAM (see sec. 4.1.1, fig. 3).\n4. Experiments\nWe demonstrate empirically that the explicitly parameteri-\nzation of the state-space model with coordinate differences\n∆i presented in sections 3.2 and 3.3 is a strong abstrac-\ntion that exploits the sparse geometry of both point clouds\nand event-based vision. When trained from scratch, our\nmethod improves the PointMamba [20] baseline by up to\n2.8 % on the ScanObjectNN dataset [43]. At the same time,\nour method sets a new state-of-the-art for event-based ges-\nture recognition on the DVS128-Gestures dataset [1].\n4.1. STREAM for Point Cloud Analysis\n4.1.1\nPoint Cloud Model\nOur goal is to evaluate the explicit SSM parameterization\ndefined in sec. 3.3.\nTherefore, we align our architecture\nclose to PointMamba [20], which uses the default Mamba\nmodule [9]. As visualized in fig. 3A, we sample N points\nand sort this point set by the X, Y , and Z coordinates re-\nspectively. The vector representations extracted from the\npoint coordinates individually are scaled and shifted by\nlearned parameters for the three sorted sequences separately\nto indicate the sorting dimension. The resulting sequence of\n3N points is concatenated along the sequence dimension to\nform the input to our model. This preprocessing simpli-\nfies the ordering of the point set by avoiding the two space-\nfilling Hilbert-curves applied by PointMamba at the cost of\nincreasing the input sequence length from 2N to 3N.\nSimilar to [5, 20, 26], our model is composed of two\nstages. The input sequence is grouped into sets of 32 points\nthat are independently processed by a STREAM module to\nrepresent local information. A backbone of 12 STREAM\n5\n\nFigure 3. Point cloud processing pipeline inspired by PointMamba [20]. FPS: Farthest point sampling, kNN: k-Nearest Neighbors.\nlayers operates on the features extracted from these groups\nto agglomerate global information.\n4.1.2\nPoint Cloud Results\nWe evaluate our method on two standardized point cloud\nclassification benchmarks. The ModelNet40 dataset [51]\ncontains 12 311 clean point clouds sampled from 3D CAD\nobjects.\nA more realistic dataset of physically scanned\nindoor scenes is given by ScanObjectNN [43]. ScanOb-\njectNN contains about 15 000 scans from 2902 unique\nobjects with varying difficulty.\nIn both cases, we train\nour method with the same hyperparameters as Point-\nMamba [20].\nWe report the best overall classification accuracy, i.e. the\naverage over all samples, obtained from 5 different random\nseeds. Tab. 1 shows that replacing the Mamba module in\nPointMamba with a STREAM module improves the perfor-\nmance of models trained from scratch across all instances\non ScanObjectNN. In particular, the hardest instance PB-\nT50-RS benefits from the exact integration of spatial infor-\nmation and improves PointMamba from 82.48 % to 85.32 %\nfor the best out of 5 random seeds. With a mean and stan-\ndard deviation of (84.4 ± 0.7) %, this poses a significant\nimprovement.\nOn ModelNet40, STREAM improves the overall accu-\nracy of PointMamba by 0.3 % from 92.4 % to 92.7 % as\nreported in tab. 2. Despite the smaller gain compared to\nScanObjectNN, the low variance on this dataset indicates a\nsignificant improvement on ModelNet40 as well. The mean\nand standard deviation of our model is (92.6 ± 0.1) %.\nOur results show that the inductive bias of explicitly en-\ncoding spatial relationships in the SSM parameterization is\nparticularly useful when training models from scratch.\n4.2. STREAM for Event Streams\nFigure 4. Hierarchical subsampling architectures with two stages\nto handle long event streams.\n4.2.1\nEvent Stream Model\nIn contrast to most prior works, our method directly op-\nerates on the stream of events as visualized in fig. 4. The\nstream is encoded as a sequence of tokens (ti, ui), where\neach pixel of the event-based camera is uniquely encoded\nby a separate token, similar to how text is represented as\ntokens in modern language modeling [33]. This sequence\nis directly processed by a STREAM module. To accom-\nmodate large streams of up to 131 072 events per sample in\nGPU memory, the output of the first STREAM module is\n6\n\nMethods\nBackbone\nParam. (M) ↓\nOBJ-BG ↑\nOBJ-ONLY ↑\nPB-T50-RS ↑\nSupervised Learning Only\nPointMLP [24]\n-\n13.2\n85.4\n-\n85.4\nRepSurf-U [34]\n-\n1.5\n-\n-\n84.3\nADS [15]\n-\n-\n-\n-\n87.5\nPoint-MAE [26]\nTransformer\n22.1\n86.75\n86.92\n80.78\nPCM [55]\nMamba\n34.2\n-\n-\n88.1\nPointMamba [20]\nMamba\n12.3\n88.30\n87.78\n82.48\nOurs\nSTREAM\n12.3\n90.02 (+1.72)\n88.64 (+0.86)\n85.32 (+2.84)\nTraining from Pre-training (Single-Modal)\nPoint-BERT [52]\nTransformer\n22.1\n87.43\n88.12\n83.07\nMaskPoint [23]\nTransformer\n22.1\n89.30\n88.10\n84.30\nPoint-MAE [26]\nTransformer\n22.1\n92.77\n91.22\n89.04\nPointMamba [20]\nMamba\n12.3\n94.32\n92.60\n89.31\nTable 1. Object classification on ScanObjectNN [43]. We report overall accuracy (OA) in percent and indicate the improvement over the\nPointMamba baseline through using our STREAM module instead of the default Mamba module.\nMethods\nParam. (M) ↓\nOA (%) ↑\nSupervised Learning Only\nPointNet [4]\n3.5\n89.2\nPointNet++ [31]\n1.5\n90.7\nPointCNN [19]\n0.6\n92.2\nDGCNN [29]\n1.8\n92.9\nPointNeXt [32]\n1.4\n92.9\nPCT [12]\n2.9\n93.2\nOctFormer [44]\n4.0\n92.7\nPCM-Tiny [55]\n6.9\n93.1\nPoint-MAE [26]\n22.1\n92.3\nPointMamba [20]\n12.3\n92.4\nSTREAM (ours)\n12.3\n92.7 (+0.3)\nwith Self-supervised pre-training\nPoint-BERT [52]\n22.1\n92.7\nMaskPoint [23]\n22.1\n92.6\nPoint-M2AE [54]\n12.8\n93.4\nPoint-MAE [26]\n22.1\n93.2\nPointGPT-S [5]\n29.2\n93.3\nACT [8]\n22.1\n93.6\nPointMamba [20]\n12.3\n93.6\nTable 2. Classification on ModelNet40. We report overall accu-\nracy (OA) in percent and indicate the improvement over the Point-\nMamba baseline through using our STREAM module instead of\nthe default Mamba module.\nsubsampled by a factor of 8 or 16 depending on the task.\nThis way, every event is processed by the neural network\nat least once directly, in contrast to previous methods that\nMethods\nParam. (M) ↓\nAcc (%) ↑\nFrame-based\nACE-BET [22]\n-\n98.9\nExACT [56]\n-\n98.9\nSpikMamba [6]\n0.2\n99.0\nEventMamba [36]\n0.3\n99.2\nTENNs-PLEIADES [27]\n0.2\n100.0∗\nEvent-based\nEvent-SSM [38]\n5.0\n97.7\nS7 [41]\n4.1\n99.2\nSTREAM (ours)\n0.2 + 1.0†\n100.0\nTable 3. Comparison of STREAM to the state-of-the-art on the\nDVS128-Gestures dataset [1]. ∗evaluated on 10 out of 11 classes,\nomitting the ”other” class. †0.2 M parameters for STREAM, and\n1.0 M parameters for encoding the input stream as tokens.\nsubsampled the raw stream before presenting it to a neural\nnetwork [37, 39]. To create a hierarchical architecture, the\nsequence is subsampled a second time after half the num-\nber of layers. The state dimension is increased by a factor\nof 2 upon the second subsampling. Our hierarchical sub-\nsampling architecture is visualized in fig. 4. We apply aver-\nage pooling along the sequence dimension before comput-\ning the class labels. To improve generalization, we imple-\nment a set of geometric data augmentations and a variant of\nCutMix [53] that directly mixes event streams [38].\n7\n\nMethods\nParam. (M) ↓\nAcc (%) ↑\nFrame-based\nBittar and Garner [2]\n0.1\n71.7\nBittar and Garner [2]\n3.9\n77.4\nHammouamri et al. [14]\n0.7\n79.8\nHammouamri et al. [14]\n2.5\n80.7\nEvent-based\nEvent-SSM [38]\n0.1\n85.3\nEvent-SSM [38]\n0.6\n88.4\nS7 [41]\n0.6\n88.2\nSTREAM (ours)\n0.2\n86.3\nTable 4. Comparison of STREAM to the state-of-the-art on the\nSpiking Speech Commands dataset [7].\n4.2.2\nEvent Stream Results\nWe evaluate STREAM on two popular event-based datasets.\nThe DVS128 Gestures dataset [1] consists of 1342 record-\nings of 10 distinct classes of hand gestures and an additional\nclass of arbitrary other gestures. While this dataset features\nonly a relatively small number of samples for 11 hand ges-\nture categories recorded from 29 subjects, the total number\nof events in this dataset adds up to about 390 M events. Our\nmodel has an initial model dimension of n = 32 and a state-\nspace dimension of m = 32, which is expanded by a factor\nof 2 upon subsampling. We deploy a total of 6 STREAM\nblocks, which by the notation of fig. 4 corresponds to L = 2.\nWe train with a batch size of 32 on sequences of 65 536\nevents such that the total input sequences created by CutMix\nare up to 131 072 events long, and subsample by a factor of\n16. Evaluation is conducted on the full sequences of up\nto 1.5 M events. Remarkably, our fully event-based model\nsets a new state-of-the-art on the DVS128-Gestures dataset\nas reported in tab. 3, improving over both frame-based and\nevent-based references. While [27] reported 100 % accu-\nracy on 10 out of the 11 classes, omitting the ‘others’ class,\nwe for the first time present a model that can reach a maxi-\nmum accuracy of 100 % on all 11 classes of the dataset.\nIn addition to event-based vision, we evaluate STREAM\non an event-based audio classification task. The Spiking\nSpeech Commands dataset [7] contains more than 100 000\nsamples converted from the original Speech Commands\ndataset [47] with a median number of 8100 events per sam-\nple. The model dimension is again n = 32 with a smaller\nstate-space dimension of m = 4 compared to the vision\nmodel. We deploy 8 STREAM blocks (L = 3 in fig. 4), and\nset the subsampling factor to 8. With this configuration, we\nreport a classification accuracy of 86.3 % in tab. 4, which\nsettles between the small and large models reported in [38].\n4.3. Ablation Study\nWe compare our STREAM module against the Mamba\nmodule [9] in tab. 5. The central difference is the explicit\nintegration of spatio-temporal information in the recurrent\noperator eA∆k with ∆k = tk −tk−1.\nWe additionally\nexperiment with all combinations of applying linear trans-\nformations activated by Softplus functions to renormalize\nthe ∆k and Γk parameters in equations (11) and (12), re-\nspectively. While the fairly small DVS128 Gestures dataset\npossesses high variance, STREAM consistently improves\nover Mamba. This effect is stronger expressed on the larger\nSpiking Speech Commands dataset, which is less affected\nby training noise due to the larger number of samples. Here,\nSTREAM creates a significant margin to the Mamba base-\nline.\n5. Discussion\nWe have introduced STREAM, a sequence model for point\ncloud and event stream data, which achieves competitive\nresults on a range of benchmarks from point-cloud classifi-\ncation to event-based vision. Prior efforts of unifying these\nmodalities transferred point cloud models to event streams,\nor ignore the spatio-temporal structure of either modality by\napplying the default Mamba model to a sequential view of\nthe data. In contrast, our model exploits the dynamics of\nstate-space models, a temporal process at first sight, to en-\ncode spatial geometric information into the models parame-\nterization. This inductive bias proved valuable as STREAM\nimproves our reference model, PointMamba, when trained\nfrom scratch on point cloud datasets such as ModelNet40\nand ScanObjectNN. By design, STREAM applies to spatio-\ntemporal modalities such as event-based vision. We oper-\nate on the stream of events without collapsing events into\nframes and without using 2D convolutions. This process-\ning paradigm achieves 100 % classification accuracy on all\n11 classes of the event-based DVS128 Gestures dataset, a\nresult that has so far only been achieved on the 10 prede-\ntermined classes by [27]. A practically relevant property\nof STREAM is that it allows asynchronous inference on\nstreams of points or events recorded from LiDAR sensors or\nevent-based cameras. We, therefore, expect STREAM to be\nwell suited for sensor fusion of these modalities. Previous\nwork has shown that SSMs can benefit significantly from\nself-supervised pre-training on larger datasets [20], which\nwe will explore for our method in future research.\nAcknowledgments\nMS is supported with funds from Bosch-Forschungsstiftung\nim Stifterverband. YB and KB were funded by the German\nAcademic Exchange Service (DAAD) under the funding\nprogramme WISE (57698568).\nDK is funded by the\nGerman Federal Ministry of Education and Research\n8\n\nModel\nDVS128 Gestures\nSpiking Speech Commands\ntk −tk−1\nsoftplus + Linear\nValidation\ntk −tk−1\nsoftplus + Linear\nValidation\n∆\nΓ\nAccuracy\n∆\nΓ\nAccuracy\nMamba\n✗\n✓\n✓\n98.6± 0.7\n✗\n✓\n✓\n86.8± 0.1\n✓\n✗\n✗\n99.2± 0.3\n✓\n✗\n✗\n87.9± 0.1\nSTREAM\n✓\n✗\n✓\n98.8± 0.2\n✓\n✗\n✓\n87.9± 0.3\n(variants)\n✓\n✓\n✗\n98.8± 0.6\n✓\n✓\n✗\n87.8± 0.2\n✓\n✓\n✓\n98.6± 1.2\n✓\n✓\n✓\n87.9± 0.3\nTable 5. STREAM versus Mamba [9] on the DVS128 Gestures [1] and Spiking Speech Commands [7] datasets. STREAM directly feeds\nthe time coordinate of irregularly spaced events as a parameter to the state-space model as described in sec. 3.3, which is indicated by\ncolumn tk −tk−1. Different variants of STREAM might apply linear transformations and Softplus activations similar to Mamba to adjust\nthe time scales of eA∆k (columns ∆) or the factor of Γk in equation (12) (columns Γ).\n(BMBF) within the project EVENTS (16ME0733). KKN\nis funded by the German Federal Ministry of Education and\nResearch (BMBF) within the KI-ASIC project (16ES0996).\nCM receives funding from the German Research Foun-\ndation\n(DFG,\nDeutsche\nForschungsgemeinschaft)\nas\npart of Germany’s Excellence Strategy – EXC 2050/1 –\nProject ID 390696704 – Cluster of Excellence “Centre\nfor Tactile Internet with Human-in-the-Loop” (CeTI) of\nTechnische Universit¨at Dresden. This work was partially\nfunded by the German Federal Ministry of Education and\nResearch (BMBF) and the free state of Saxony within\nthe ScaDS.AI center of excellence for AI research. The\nauthors gratefully acknowledge the computing time made\navailable to them on the high-performance computer at\nthe NHR Center of TU Dresden.\nThis center is jointly\nsupported by the Federal Ministry of Education and\nResearch and the state governments participating in the\nNHR (www.nhr-verein.de/unsere-partner).\nThe authors\ngratefully acknowledge the Gauss Centre for Supercom-\nputing e.V. (www.gauss-centre.eu) for funding this project\nby providing computing time on the GCS Supercomputer\nJUWELS [16] at J¨ulich Supercomputing Centre (JSC).\nReferences\n[1] Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jef-\nfrey McKinstry, Carmelo Di Nolfo, Tapan Nayak, Alexan-\nder Andreopoulos, Guillaume Garreau, Marcela Mendoza,\nJeff Kusnitz, Michael Debole, Steve Esser, Tobi Delbruck,\nMyron Flickner, and Dharmendra Modha.\nA low power,\nfully event-based gesture recognition system. In 2017 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 7388–7397, 2017. 2, 5, 7, 8, 9\n[2] Alexandre Bittar and Philip N. Garner. A surrogate gradient\nspiking baseline for speech command recognition. Frontiers\nin Neuroscience, 16, 2022. 8\n[3] Guy E Blelloch. Prefix sums and their applications, 1990. 5,\n2\n[4] R. Qi Charles, Hao Su, Mo Kaichun, and Leonidas J. Guibas.\nPointNet: Deep Learning on Point Sets for 3D Classification\nand Segmentation . In 2017 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 77–85, Los\nAlamitos, CA, USA, 2017. IEEE Computer Society. 1, 2, 7\n[5] Guangyan Chen, Meiling Wang, Yi Yang, Kai Yu, Li Yuan,\nand Yufeng Yue. Pointgpt: Auto-regressively generative pre-\ntraining from point clouds. In Advances in Neural Informa-\ntion Processing Systems, pages 29667–29679. Curran Asso-\nciates, Inc., 2023. 1, 2, 5, 7\n[6] Jiaqi Chen, Yan Yang, Shizhuo Deng, Da Teng, and Liyuan\nPan. Spikmamba: When snn meets mamba in event-based\nhuman action recognition, 2024. 3, 7, 1\n[7] Benjamin Cramer, Yannik Stradmann, Johannes Schemmel,\nand Friedemann Zenke. The heidelberg spiking data sets for\nthe systematic evaluation of spiking neural networks. IEEE\nTransactions on Neural Networks and Learning Systems, 33\n(7):2744–2757, 2022. 8, 9\n[8] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jian-\njian Sun, Zheng Ge, Li Yi, and Kaisheng Ma. Autoencoders\nas cross-modal teachers: Can pretrained 2d image transform-\ners help 3d representation learning? In The Eleventh Inter-\nnational Conference on Learning Representations, 2023. 7\n[9] Albert Gu and Tri Dao. Mamba: Linear-time sequence mod-\neling with selective state spaces, 2024. 2, 3, 4, 5, 8, 9, 1\n[10] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri\nDao, Atri Rudra, and Christopher R´e. Combining recurrent,\nconvolutional, and continuous-time models with linear state\nspace layers. In Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Information Pro-\ncessing Systems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 572–585, 2021. 2, 3\n[11] Albert Gu, Karan Goel, and Christopher Re. Efficiently mod-\neling long sequences with structured state spaces. In Inter-\nnational Conference on Learning Representations, 2022. 2,\n4, 1\n[12] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R. Martin, and Shi-Min Hu. Pct: Point cloud\ntransformer.\nComputational Visual Media, 7(2):187–199,\n2021. 7\n[13] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu,\nand Mohammed Bennamoun.\nDeep learning for 3d point\n9\n\nclouds: A survey. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 43(12):4338–4364, 2021. 1\n[14] Ilyass Hammouamri, Ismail Khalfaoui-Hassani, and Tim-\noth´ee Masquelier.\nLearning delays in spiking neural net-\nworks using dilated convolutions with learnable spacings. In\nThe Twelfth International Conference on Learning Represen-\ntations, 2024. 8\n[15] Cheng-Yao Hong, Yu-Ying Chou, and Tyng-Luh Liu. At-\ntention discriminant sampling for point clouds.\nIn 2023\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 14383–14394, 2023. 7\n[16] J¨ulich Supercomputing Centre.\nJUWELS Cluster and\nBooster: Exascale Pathfinder with Modular Supercomputing\nArchitecture at Juelich Supercomputing Centre. Journal of\nlarge-scale research facilities, 7(A138), 2021. 9\n[17] Sanghyeon Kim and Eunbyung Park.\nSMPConv:\nSelf-\nMoving Point Representations for Continuous Convolution.\nIn 2023 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 10289–10299, 2023. ISSN:\n2575-7075. 2\n[18] Xavier Lagorce, Garrick Orchard, Francesco Galluppi,\nBertram E. Shi, and Ryad B. Benosman. Hots: A hierarchy\nof event-based time-surfaces for pattern recognition. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n39(7):1346–1359, 2017. 3\n[19] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di,\nand Baoquan Chen. Pointcnn: Convolution on x-transformed\npoints. In Advances in Neural Information Processing Sys-\ntems. Curran Associates, Inc., 2018. 7\n[20] Dingkang Liang, Xin Zhou, Wei Xu, Xingkui Zhu, Zhikang\nZou, Xiaoqing Ye, Xiao Tan, and Xiang Bai. Pointmamba:\nA simple state space model for point cloud analysis.\nIn\nThe Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. 2, 3, 5, 6, 7, 8, 1\n[21] Patrick Lichtsteiner, Christoph Posch, and Tobi Delbruck. A\n128× 128 120 db 15 µs latency asynchronous temporal con-\ntrast vision sensor. IEEE Journal of Solid-State Circuits, 43\n(2):566–576, 2008. 1\n[22] Chang Liu, Xiaojuan Qi, Edmund Y. Lam, and Ngai Wong.\nFast classification and action recognition with event-based\nimaging. IEEE Access, 10:55638–55649, 2022. 7\n[23] Haotian Liu, Mu Cai, and Yong Jae Lee. Masked discrim-\nination for self-supervised learning on point clouds.\nPro-\nceedings of the European Conference on Computer Vision\n(ECCV), 2022. 7\n[24] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Re-\nthinking network design and local geometry in point cloud:\nA simple residual MLP framework. In International Confer-\nence on Learning Representations, 2022. 7\n[25] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-\nvolutional neural network for real-time object recognition.\nIn 2015 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 922–928, 2015. 1\n[26] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,\nYonghong Tian, and Li Yuan. Masked autoencoders for point\ncloud self-supervised learning. In Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part II, pages 604–621. Springer,\n2022. 2, 3, 5, 7\n[27] Yan Ru Pei and Olivier Coenen. TENNs-PLEIADES: Build-\ning Temporal Kernels with Orthogonal Polynomials, 2024.\narXiv:2405.12179. 3, 7, 8\n[28] Etienne Perot, Pierre de Tournemire, Davide Nitti, Jonathan\nMasci, and Amos Sironi. Learning to detect objects with a 1\nmegapixel event camera. In Advances in Neural Information\nProcessing Systems, pages 16639–16652. Curran Associates,\nInc., 2020. 1, 3\n[29] Anh Viet Phan, Minh Le Nguyen, Yen Lam Hoang Nguyen,\nand Lam Thu Bui. Dgcnn: A convolutional neural network\nover large-scale labeled graphs. Neural Networks, 108:533–\n543, 2018. 7\n[30] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu,\nTri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,\nand Christopher Re. Hyena hierarchy: Towards larger con-\nvolutional language models. In Proceedings of the 40th In-\nternational Conference on Machine Learning, pages 28043–\n28078. PMLR, 2023. 3\n[31] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In Advances in Neural Informa-\ntion Processing Systems. Curran Associates, Inc., 2017. 2,\n7\n[32] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,\nHasan Abed Al Kader Hammoud, Mohamed Elhoseiny, and\nBernard Ghanem. Pointnext: revisiting pointnet++ with im-\nproved training and scaling strategies. In Proceedings of the\n36th International Conference on Neural Information Pro-\ncessing Systems, Red Hook, NY, USA, 2024. Curran Asso-\nciates Inc. 7\n[33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 2019. 6\n[34] Haoxi Ran, Jun Liu, and Chengjie Wang.\nSurface Repre-\nsentation for Point Clouds . In 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n18920–18930, Los Alamitos, CA, USA, 2022. IEEE Com-\nputer Society. 7\n[35] Hongwei Ren, Yue Zhou, Haotian FU, Yulong Huang, Ren-\njing Xu, and Bojun Cheng. Ttpoint: A tensorized point cloud\nnetwork for lightweight action recognition with event cam-\neras. In Proceedings of the 31st ACM International Confer-\nence on Multimedia, page 8026–8034, New York, NY, USA,\n2023. Association for Computing Machinery. 2\n[36] Hongwei Ren, Yue Zhou, Jiadong Zhu, Haotian Fu, Yu-\nlong Huang, Xiaopeng Lin, Yuetong Fang, Fei Ma, Hao Yu,\nand Bojun Cheng. Rethinking efficient and effective point-\nbased networks for event camera classification and regres-\nsion: Eventmamba, 2024. 2, 3, 7, 1\n[37] Simon Schaefer, Daniel Gehrig, and Davide Scaramuzza.\nAegnn: Asynchronous event-based graph neural networks.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 12371–\n12381, 2022. 3, 7\n[38] Mark Sch¨one, Neeraj Mohan Sushma, Jingyue Zhuge, Chris-\ntian Mayr, Anand Subramoney, and David Kappel. Scalable\n10\n\nevent-by-event processing of neuromorphic sensory signals\nwith deep state-space models. In ACM/IEEE International\nConference on Neuromorphic Systems. IEEE, 2024. 3, 7, 8\n[39] Yusuke Sekikawa, Kosuke Hara, and Hideo Saito. Eventnet:\nAsynchronous recursive event processing. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 3, 7\n[40] Jimmy T.H. Smith, Andrew Warrington, and Scott Linder-\nman. Simplified state space layers for sequence modeling.\nIn The Eleventh International Conference on Learning Rep-\nresentations, 2023. 3, 1, 2\n[41] Taylan Soydan, Nikola Zubi´c, Nico Messikommer, Sid-\ndhartha Mishra, and Davide Scaramuzza. S7: Selective and\nsimplified state space layers for sequence modeling, 2024. 3,\n7, 8\n[42] Anand Subramoney,\nKhaleelulla Khan Nazeer,\nMark\nSch¨one, Christian Mayr, and David Kappel. Efficient recur-\nrent architectures through activity sparsity and sparse back-\npropagation through time.\nIn The Eleventh International\nConference on Learning Representations, 2023. 3\n[43] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua,\nDuc Thanh Nguyen, and Sai-Kit Yeung.\nRevisiting point\ncloud classification: A new benchmark dataset and classifi-\ncation model on real-world data. In International Conference\non Computer Vision (ICCV), 2019. 2, 5, 6, 7\n[44] Peng-Shuai Wang.\nOctformer: Octree-based transformers\nfor 3d point clouds. ACM Trans. Graph., 42(4), 2023. 7\n[45] Qinyi Wang, Yexin Zhang, Junsong Yuan, and Yilong Lu.\nSpace-time event clouds for gesture recognition: From rgb\ncameras to event cameras. In 2019 IEEE Winter Conference\non Applications of Computer Vision (WACV), pages 1826–\n1835, 2019. 2\n[46] Shenlong Wang,\nSimon Suo,\nWei-Chiu Ma,\nAndrei\nPokrovsky, and Raquel Urtasun. Deep Parametric Contin-\nuous Convolutional Neural Networks. In 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2589–2597, 2018. ISSN: 2575-7075. 1, 2, 4\n[47] Pete Warden.\nSpeech commands: A dataset for limited-\nvocabulary speech recognition, 2018. 8\n[48] Wenxuan Wu, Zhongang Qi, and Li Fuxin.\nPointConv:\nDeep Convolutional Networks on 3D Point Clouds.\nIn\n2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 9613–9622, 2019. ISSN:\n2575-7075. 1, 2, 4\n[49] Wenxuan Wu, Li Fuxin, and Qi Shan. PointConvFormer: Re-\nvenge of the Point-based Convolution. In 2023 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 21802–21813, 2023. ISSN: 2575-7075. 2\n[50] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets: A deep representation for volumetric shapes. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2015. 1\n[51] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets:\nA deep representation for volumetric shapes.\nIn 2015 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1912–1920, 2015. 2, 6\n[52] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie\nZhou, and Jiwen Lu.\nPoint-BERT: Pre-training 3D Point\nCloud Transformers with Masked Point Modeling .\nIn\n2022 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 19291–19300, Los Alami-\ntos, CA, USA, 2022. IEEE Computer Society. 2, 7\n[53] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classifiers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), 2019. 7\n[54] Renrui Zhang, Ziyu Guo, Rongyao Fang, Bin Zhao, Dong\nWang, Yu Qiao, Hongsheng Li, and Peng Gao. Point-m2ae:\nmulti-scale masked autoencoders for hierarchical point cloud\npre-training. In Proceedings of the 36th International Con-\nference on Neural Information Processing Systems, Red\nHook, NY, USA, 2024. Curran Associates Inc. 7\n[55] Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Ji, and\nShuicheng Yan. Point cloud mamba: Point cloud learning\nvia state space model. CoRR, abs/2403.00762, 2024. 3, 7, 1\n[56] Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, and Lin Wang. Ex-\nact: Language-guided conceptual reasoning and uncertainty\nestimation for event-based action recognition and more. In\n2024 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 18633–18643, 2024. 1, 3,\n7\n[57] Nikola Zubic, Mathias Gehrig, and Davide Scaramuzza.\nState Space Models for Event Cameras . In 2024 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 5819–5828, Los Alamitos, CA, USA, 2024.\nIEEE Computer Society. 1, 3\n11\n\nSTREAM: A Universal State-Space Model for Sparse Geometric Data\nSupplementary Material\n6. Derivations\nThis section provides complete derivations of equations (6)\nand (7).\nWe restrict to the case of single-input single-\noutput (SISO) state-space models with multi-dimensional\nstate. Multi-input multi-output (MIMO) formulations like\nS4 [11] or Mamba [9] can be obtained by creating n paral-\nlel instances of the SISO model and mixing the input and\noutput components with linear layers.\nOur motivation was to reason about spatial relationships\nbetween input pulses. Therefore, we defined an interaction\nkernel Φ(x, x′) that models pairwise interactions. A com-\nplete view of all possible interactions with the point xk is\ngiven by\ny (xk) =\nZ\nΦ (xk, x′) u (x′) dx′\n(14)\n=\nN\nX\ni=0\nΦ (xk, xi) ui .\n(15)\nWe will now derive a parameterization of the interaction\nkernel Φ with a state-space model.\nA linear time-varying state-space model acting on a\nscalar input function u (t) and producing a scalar output\ny (t) is defined through\n˙h (t) = A (t) h (t) + B (t) u (t)\n(16)\ny (t) = C (t) h (t) ,\n(17)\nwith u(t), y(t) ∈R, states h (t) ∈Rm and parameters\nA (t) ∈Rm×m, B (t) ∈Rm×1, C (t) ∈R1×m.\nNote that we stick to our notation introduced in sec. 3,\ndenoting vectors and matrices with bold face and scalar val-\nues with regular face.\n6.1. Solution of the Linear State-space Model\nLinear ordinary differential equations have a well known\nanalytical solution. We refer the reader to standard calculus\ntextbooks. As such, the linear dynamics of equation (16)\nfor initial value h0 = h (t0) admit the analytical solution\nh (t) = h0 +\nZ t\nt0\nexp\n\u0012Z t\nt′ A (t′′) dt′′\n\u0013\nB (t′) u (t′) dt′ ,\n(18)\nwhich can be checked by taking the derivative of h and\ncomparing it to equation (16). Comparing equations (18),\n(17) and (14), we read off the kernel\nΦ (t, t′) = C (t) exp\n\u0012Z t\nt′ A (t′′) dt′′\n\u0013\nB (t′)\n(19)\nNote that equation (18) can be evaluated on all coordinates\nt > t0. The two canonical options for discretizing the vari-\nables h and y are equidistant steps resulting in a regular grid,\nor using the input coordinates t0, . . . , tN. While most state-\nspace model works discretize on equidistant steps, [40]\nshows on a toy task that the SSM formulation is capable\nof solving tasks with irregularly spaced steps as well.\nOur work differs from other recent Mamba based models\nsuch as PointMamba [20], Point Cloud Mamba [55], Event-\nMamba [36], or SpikMamba [6] by integrating the true tim-\nings of the inputs in the following sense.\nWe will use h0 = 0 for notational simplicity in the fol-\nlowing. Discretizing equation (18) on the Dirac delta coded\ninput (1) then yields\nh (tk) =\nZ tk\nt0\nexp\n\u0012Z tk\nt\nA (t′) dt′\n\u0013\nB (t)\nN\nX\ni=0\nδ (t −ti) uidt\n=\nk\nX\ni=0\nexp\n\u0012Z tk\nti\nA (t) dt\n\u0013\nB (ti) ui .\n(20)\nWe decompose the integral from ti to tk into the integrals\nfrom tj−1 to tj and sum over them\nh (tk) =\nk\nX\ni=0\nexp\n\n\nk\nX\nj=i+1\nZ tj\ntj−1\nA (t) dt\n\nB (ti) ui . (21)\nWe will further assume that A (t) , B (t) , C (t) are constant\non the intervals (tj−1, tj] for j = i + 1, . . . , k. Denoting\nA (tj) = Aj, B (tj) = Bj, C (tj) = Cj, h (tk) = hk, and\n∆j = tj −tj−1 we get\nh (tk) =\nk\nX\ni=0\nexp\n\n\nk\nX\nj=i+1\nAj∆j\n\nBiui\n(22)\n=\nk\nX\ni=0\n\n\nk\nY\nj=i+1\nexp (Aj∆j)\n\nBiui .\n(23)\nComparing to equation (15), we read off the discrete kernel\nfunction\nΦ (tk, ti) = Ck\n\n\nk\nY\nj=i+1\nexp (Aj∆j)\n\nBi .\n(24)\nProposition 1. The kernels parameterized by equation (24)\ncontain convolution operations with rational kernels as a\nspecial case.\n1\n\nProof. Consider the linear time-invariant (LTI) case where\nA, B, C are constant for all t. Then\nk\nY\nj=i+1\nexp (Aj∆j) = exp (A (tk −ti))\n(25)\nThe proposition follows from the fundamental result from\nlinear systems theory that LTI state-space models can rep-\nresent every rational transfer function.\n6.2. Recursive Computation of the Interaction Ker-\nnel\nLet’s expand equation (23) in a recursive form\nhk = h (xk) =\nk\nX\ni=0\n\n\nk\nY\nj=i+1\neAj∆j\n\nBiui\n= Buk +\nk−1\nX\ni=0\n\n\nk\nY\nj=i+1\neAj∆j\n\nBiui\n= Buk +\nk−1\nX\ni=0\neAk∆k\n\n\nk−1\nY\nj=i+1\neAj∆j\n\nBiui\n= Buk + eAk∆k\nk−1\nX\ni=0\n\n\nk−1\nY\nj=i+1\neAj∆j\n\nBiui\n= Buk + eAk∆khk−1 .\nThis concludes the derivation of our recurrent operator\nhk = Buk + eAk∆khk−1 .\n(26)\nIn line with most recent SSM works, we choose Ai ≡A as\na diagonal matrix for all i.\nRemark 1. Equation (26) allows asynchronous inference\non streams of incoming coordinates in O (1) time per coor-\ndinate, or O (N) time for the full stream of coordinates.\n7. Scan\nLinear time-varying systems such as the one given by equa-\ntion (26) resemble an associative operation, with well-\nknown time complexity of O (log N) [3]. The goal of this\nsection is not to provide a proof, but to give the reader a\nclear idea of how irrgularly spaced sequences can be par-\nallelized in the same way that regular SSMs can be paral-\nlelized with the Scan primitive [9, 40]. The presentation\nfollows [3].\nConsider the pair\nci = [ai, bi]\n(27)\nwith the binary operator • defined through\nci • cj = [ai · aj, aj · bi + bj] .\n(28)\nAs [3] shows, the operator • is associative, i.e.\n(ci • cj) • ck = ci • (cj • ck) .\n(29)\nAssociative operators can be parallelized to run in\nO (log N) time on sequence of length N given sufficiently\nmany processors [3]. We use this primitive to parallelize\nour model. The pairs c are initialized as\nc0 =\n\u0002\neA∆0, B0u0\n\u0003\n. . .\ncN =\n\u0002\neA∆N , BNuN\n\u0003\n(30)\nFig. 5 shows an example of how the Scan primitive com-\nputes the entire recurrence in equation (26) in roughly\n2 log N steps.\n2\n\nFigure 5. Caption\n3",
    "pdf_filename": "STREAM_A_Universal_State-Space_Model_for_Sparse_Geometric_Data.pdf"
}