{
    "title": "Enhancing Reasoning Capabilities of LLMs",
    "abstract": "Largelanguagemodels(LLMs)arecapableofsolvingawiderangeoftasks,yet theyhavestruggledwithreasoning. Toaddressthis,weproposeAdditionalLogic Training(ALT),whichaimstoenhanceLLMs’reasoningcapabilitiesbyprogram- generatedlogicalreasoningsamples. Wefirstestablishprinciplesfordesigning high-qualitysamplesbyintegratingsymboliclogictheoryandpreviousempirical insights. Then,basedontheseprinciples,weconstructasyntheticcorpusnamed FormalLogicDeductionDiverse (FLD×2),comprisingnumeroussamplesof multi-stepdeductionwithunknownfacts,diversereasoningrules,diverselinguistic expressions,andchallengingdistractors. Finally,weempiricallyshowthatALTon FLD×2substantiallyenhancesthereasoningcapabilitiesofstate-of-the-artLLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logicalreasoningbenchmarks,upto10pointsonmathandcodingbenchmarks, and5pointsonthebenchmarksuiteBBH. 1 Introduction Knowledge and reasoning have long been considered essential elements for achieving artificial intelligence(McCarthy,1959;Weizenbaum,1966;Winograd,1971;ColmerauerandRoussel,1973; Shortliffe,1976;ElkanandGreiner,1993). Knowledgereferstofactsabouttheworld,e.g.,“objects withmassgenerateagravitationalfield”and“theEarthhasmass.” Reasoninginvolvescombining multiplefactsaccordingtospecificrulestoobtainnewknowledge. Forexample,thenewknowledge that“theEarthgeneratesagravitationalfield”canbederivedfromtheaforementionedtwofacts. RecentobservationssuggestthatLLMscansolveproblemsusingmemorizedknowledgeofsimilar samples seen during pre-training, but they cannot solve novel, unknown problems that require reasoning(HodelandWest,2023;Dasguptaetal.,2023;Zhangetal.,2024). Forinstance,LLMscan solvefamousarithmeticproblemsasisbutnotwhenthenumbersornamesarechanged(Razeghi et al., 2022; Mirzadeh et al., 2024), and they can solve coding tests from past years before the “knowledgecutoff”butnotfromthepresentyear(Mitchell,2023). Thisbiastowardsknowledgehas beenobservedeveninstate-of-the-artLLMssuchasGPT-4(Liuetal.,2023b;Wuetal.,2023;Dziri etal.,2023). LLMs’poorreasoningcapabilitiescanstemfromthelackofhigh-qualityreasoningsamplesinthe pre-trainingcorpus,whichprimarilyconsistsofhuman-writtentexts(Betzetal.,2021;Morishita etal.,2023). Indeed,reasoningsamplesinhuman-writtentextsoftenexhibitlowquality,asevidenced by fallacies and biases commonly found in online debates (Hansson, 2004; Guias¸u and Tindale, 2018;Chengetal.,2017). Thisisunsurprisinggiventhathumansusuallythinkreflexivelyrather thanthroughrigidreasoning(Kahneman,2011;SunsteinandHastie,2015;Paglieri,2017). Thus,a *EqualContribution †WorkdoneatHitachi 38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024). 4202 voN 91 ]GL.sc[ 1v89421.1142:viXra",
    "body": "Enhancing Reasoning Capabilities of LLMs\nvia Principled Synthetic Logic Corpus\nTerufumiMorishita1 GakuMorio1∗ AtsukiYamaguchi2∗† YasuhiroSogawa1\n1AdvancedAIInnovationCenter,Hitachi 2TheUniversityofSheffield\nAbstract\nLargelanguagemodels(LLMs)arecapableofsolvingawiderangeoftasks,yet\ntheyhavestruggledwithreasoning. Toaddressthis,weproposeAdditionalLogic\nTraining(ALT),whichaimstoenhanceLLMs’reasoningcapabilitiesbyprogram-\ngeneratedlogicalreasoningsamples. Wefirstestablishprinciplesfordesigning\nhigh-qualitysamplesbyintegratingsymboliclogictheoryandpreviousempirical\ninsights. Then,basedontheseprinciples,weconstructasyntheticcorpusnamed\nFormalLogicDeductionDiverse (FLD×2),comprisingnumeroussamplesof\nmulti-stepdeductionwithunknownfacts,diversereasoningrules,diverselinguistic\nexpressions,andchallengingdistractors. Finally,weempiricallyshowthatALTon\nFLD×2substantiallyenhancesthereasoningcapabilitiesofstate-of-the-artLLMs,\nincluding LLaMA-3.1-70B. Improvements include gains of up to 30 points on\nlogicalreasoningbenchmarks,upto10pointsonmathandcodingbenchmarks,\nand5pointsonthebenchmarksuiteBBH.\n1 Introduction\nKnowledge and reasoning have long been considered essential elements for achieving artificial\nintelligence(McCarthy,1959;Weizenbaum,1966;Winograd,1971;ColmerauerandRoussel,1973;\nShortliffe,1976;ElkanandGreiner,1993). Knowledgereferstofactsabouttheworld,e.g.,“objects\nwithmassgenerateagravitationalfield”and“theEarthhasmass.” Reasoninginvolvescombining\nmultiplefactsaccordingtospecificrulestoobtainnewknowledge. Forexample,thenewknowledge\nthat“theEarthgeneratesagravitationalfield”canbederivedfromtheaforementionedtwofacts.\nRecentobservationssuggestthatLLMscansolveproblemsusingmemorizedknowledgeofsimilar\nsamples seen during pre-training, but they cannot solve novel, unknown problems that require\nreasoning(HodelandWest,2023;Dasguptaetal.,2023;Zhangetal.,2024). Forinstance,LLMscan\nsolvefamousarithmeticproblemsasisbutnotwhenthenumbersornamesarechanged(Razeghi\net al., 2022; Mirzadeh et al., 2024), and they can solve coding tests from past years before the\n“knowledgecutoff”butnotfromthepresentyear(Mitchell,2023). Thisbiastowardsknowledgehas\nbeenobservedeveninstate-of-the-artLLMssuchasGPT-4(Liuetal.,2023b;Wuetal.,2023;Dziri\netal.,2023).\nLLMs’poorreasoningcapabilitiescanstemfromthelackofhigh-qualityreasoningsamplesinthe\npre-trainingcorpus,whichprimarilyconsistsofhuman-writtentexts(Betzetal.,2021;Morishita\netal.,2023). Indeed,reasoningsamplesinhuman-writtentextsoftenexhibitlowquality,asevidenced\nby fallacies and biases commonly found in online debates (Hansson, 2004; Guias¸u and Tindale,\n2018;Chengetal.,2017). Thisisunsurprisinggiventhathumansusuallythinkreflexivelyrather\nthanthroughrigidreasoning(Kahneman,2011;SunsteinandHastie,2015;Paglieri,2017). Thus,a\n*EqualContribution\n†WorkdoneatHitachi\n38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).\n4202\nvoN\n91\n]GL.sc[\n1v89421.1142:viXra\n75 LLaMA-3.1-70B 85 60 80\n70 + ALT 80 50 75\n65 75 40 70\n60 65\n70 30\n55 60\n65 20\n50 55\n45 60 10 50\nLogic Math Code NLI Others CoT CoT Pro\nBenchmark sets (5-shot) BBH (3-shot) BBH (0-shot) MMLU (5-shot)\nFigure1: TheperformancegainstoLLaMA-3.1-70BbyAdditionalLogicTraining(ALT)onthe\nproposedsyntheticcorpus,FLD×2(FormalLogicDeductionDiverse). Eachbenchmarkset,suchas\n“Logic”and“Math”,comprisesvariousbenchmarksinthatdomain. Tables2,4showsthedetails.\nstraightforwardstrategytoimproveLLMs’reasoningcapabilitiesistopreparemanyhigh-quality\nreasoningsamplesandtrainLLMsonthem.\nWe propose one such approach, Additional Logic Training (ALT), which utilizes high-quality\nsamplesoflogicalreasoning,themostfundamentalformofreasoning. Topreparesuchsamples,we\nutilizesyntheticgeneration(Clarketal.,2021;Betzetal.,2021;Tafjordetal.,2021;Morishitaetal.,\n2023),wherecomputerprogramsgeneratedeductivereasoningsamplesinwhichagivenhypothesis\nisprovenordisprovenbycombininggivenfactsfollowingrigidreasoningrules. WeoverviewALT\ninFigure2.\nInsyntheticgeneration,computerprogramsgeneratesamplesaccordingtopre-designedpatterns,so\nthisdesignlargelydeterminesthequalityofthesesamplesbynature. Thus,westartbydiscussing\nwhat is the ideal design for synthetic logic samples, incorporating symbolic logic theory and\nempiricalfindings(Section2). Theessenceoflogicalreasoningliesinitsabilitytohandleunknown\nfacts, unlike knowledge, which deals solely with established facts, such as commonsense facts;\ntherefore,samplesmustcoverreasoningwithunknownfacts. Samplesmustincludebothillogical\nandlogicalreasoningtoenableLLMstodistinguishbetweenthem. Thesamplesmustcovervarious\npatternsregardingacomprehensivesetofreasoningaspects,suchasreasoningrulesandlinguistic\nexpressionsoflogicalstatements. Wesummarizethesediscussionsintodesignprinciples,which\nguide the design of synthetic logic samples. Finally, based on these principles, we construct a\nsyntheticcorpusnamedFormalLogicDeductionDiverse(FLD×2),comprisingnumeroussamples\nofmulti-stepdeductionwithunknownfacts,diversereasoningrules,diverselinguisticexpressions,\nandchallengingdistractors(Section3).\nWe then empirically verify that ALT can enhance LLMs’ reasoning capabilities (Sections 4, 5).\nUsing31benchmarkscoveringdiversetasks,weobservedthatALTonFLD×2substantiallyboosts\nstate-of-the-artLLMs’reasoningcapabilities. EvenLLaMA-3.1-70B,thelargestLLMpre-trained\nonover15trilliontokens,showssubstantialimprovementswithALT(Figure1). Amongsynthetic\nlogiccorporawithdifferentsampledesigns,FLD×2yieldedthelargestperformancegains,validating\nourproposeddesignprinciples. Moreover,wediscoveredthatemployingaknowledge-forgetting\npreventionmethodduringALTiscriticallyimportant,asitlikelypreventstheLLM’sknowledgeof\nestablishedfactsfrombeingdisplacedbytheunknownfactsincludedinsyntheticlogiccorpora.\nFinally,weanalyzewhichtask-solvingcapabilitiesALTcanenhanceandwhy(Section6). Weob-\nservedasubstantialimprovementofupto30pointsonlogicalreasoningtasks(Table4a).Surprisingly,\nwealsoobservedimprovementsinabductivereasoningtasks,whichgobeyondthesyntheticlogic\ncorpora’soriginaldeductivereasoningtasks. Caseanalysesindicatethattheseimprovementsresult\nfromLLMshavingacquiredthefundamentalsofthelogicreflectedinthedesignprinciples. Wealso\nobservedimprovementsofupto10pointsonmathandcodingtasks,indicatingthegeneralizabilityof\ntheobtainedreasoningcapabilities(Tables4b,4c). Wealsoobservedimprovementsofupto6points\nonnaturallanguageinference(NLI)tasks(Table4d). CaseanalysessuggestthatLLMssuccessfully\nintegratedthecommonsenseknowledgetheyhadoriginallyacquiredduringpre-trainingwiththe\nlogicalreasoningcapabilitiesnewlyacquiredfromALT.Improvementsacrossvariousothertasks\n(Table4e)demonstratethebroadbenefitsoftheobtainedreasoningcapabilitiesbeyondstandard\nreasoning tasks, though the modest improvements of up to 2 points indicate the need for future\nresearchonmoreeffectiveapplicationofthesecapabilities.\n2\nSample Generator Logical Reasoning Sample\nFacts(w/negatives = fact3, fact4):\nMulti-step Deductive Reasoning 1.If that if a Foo star exists a Haz star also exists, Input\nthen a Jaz star exists.\nDesign Principles elimination 2.If a Foo star exist a Gaz star exists, ℱ→ℋ →ℐ\n1. Include unknown facts. ∧ and if a Gaz star exists a Haz star also exists.\n2 3. . I In nc cl lu ud de e n vae rg ioa uti sve facts. 𝓕𝓕→𝓗𝓗Mo→dus𝓘𝓘 ponens𝓕𝓕→𝓖𝓖 ∧(𝓖𝓖→𝓗𝓗) 3.If a Foo star exists and a Kax star existℱs,→𝒢𝒢∧𝒢𝒢→ℋ\nreasoning rules. then a Jaz star exists.\n4. I lin nc glu ud ise ti cv a er xio pu res ssions. 𝓕𝓕 𝓕𝓕→M𝓖𝓖odus po(n𝓖𝓖ens→𝓗𝓗) Hy4 p. oIf ta h H ea sz i sst :ar exists, then a Gaz star existsℱ.∧𝒦𝒦 →𝒥𝒥\nℋ→𝒢𝒢\nA Jaz star exists.\nintroduction 𝓖𝓖 (𝓖𝓖→𝓗𝓗) Logical steps: 𝒥𝒥\ndC eo a dm M x uip cou tl ml ie t oit ss ne t c n e ra upe n l s e ds e se x ,T d p sh u ure ec co st hi sr o e aan sm n b sy yy o l lt t oh h ge e isr m. 𝓕𝓕→𝓗𝓗→ →𝓘𝓘 (𝓕𝓕→𝓗𝓗 𝓗𝓗 𝓙𝓙) aA ddss au sm s (u re m\ne\nmt ph t oia o vt n ea\nℱ\naF so so\nu\nmst pa tr\ni\noFF e Faa nx acc i cs ℱtt t t22 s\n1)\n.    aIf A AA\nI\na\nfGa HG\na\nJH\naF\na\naaz aFo\nz\nzz\no\nzso sot ss\ns\ntas tt\nt\naaar st\na\na rtrr re\na\nr e eex\na\nre i xx\nx\nls\ne\nsx ii it ss\nox\nsi s ts t\ni\nt\n. ss st\ne\ns..\nt\n.xsℱ 𝒢𝒢\niℋ\nsℱt→ s→.𝒢𝒢\nℋ\nOutput\nFigure2: OurproposedAdditionalLogicTraining(ALT)aimstoenhanceLLMs’reasoningcapa-\nbilitiesthroughtrainingonmanysyntheticallygeneratedlogicalreasoningsamples. Oursample\ngenerator(left)firstgeneratesasampleofmulti-stepdeductivereasoningandthenconvertsitinto\nadeductionsamplewritteninEnglish(right). LLMsmustgeneratelogicalstepstoderiveagiven\nhypothesis from provided facts. The sample generator adheres to theoretically and empirically\ngroundeddesignprinciplesdiscussedinSection2. RefertoFigureD.3forarealsample.\nOurcontributionsaresummarizedasfollows:\n• We propose Additional Logic Training (ALT) and empirically verify that it can enhance the\nreasoningcapabilityofstate-of-the-artLLMsacrossvarioussizes,from7Bto70B.\n• We establish systematic design principles for synthetic logic samples; then, we construct a\nsynthetic corpus named Formal Logic Deduction Diverse (FLD×2), comprising numerous\nsamplesofmulti-stepdeductionwithunknownfacts,diversereasoningrules,diverselinguistic\nexpressions, and challenging distractors. We empirically verify that Formal Logic Deduction\nDiverseindeedleadstothelargestimprovementsamongcorporawithdifferentsampledesigns.\n• WedemonstratethatLLMsenhancedbyALTcansolvenotonlytheoriginallogicalreasoning\ntaskspresentinsyntheticlogiccorporabutalsoothertasks,suchasmathandcodingtasks,and\nnotablyNLItasks,whichrequireintegratingknowledgeandreasoning. Thisfindingunderscores\nthepotentialforadvancingtrulyversatileAIpossessingbothknowledgeandreasoningcapabilities.\nWereleasethecorpus,code,andthetrainedmodelunderapermissivelicense1.\n2 HowShouldSyntheticLogicSamplesBeDesigned?\nInsyntheticgeneration,computerprogramsgeneratesamplesaccordingtopre-designedpatterns,\nsothisdesignlargelydeterminesthequalityofthesamples. WhilePreviousstudieshaveexamined\nseveraldesigns(Clarketal.,2021;Betzetal.,2021;Tafjordetal.,2021;Morishitaetal.,2023),these\ndesignswerenotsystematicallydiscussed,sotheymaynotbethemosteffectiveones.\nThus,westartbydiscussinghowtooptimallydesignsyntheticlogicsamples.Tothisend,weconsider\nsymboliclogictheoryassuggestedbyMorishitaetal.(2023)andintegrateempiricalfindingsfrom\npreviousstudies. First,weobservethattheessenceoflogicalreasoning,basedsolelyonthelogical\nrelationshipsbetweenfacts,liesinitsabilitytohandleunknownfacts,unlikeknowledge,whichby\ndefinitiondealssolelywithestablishedfacts(Section2.1). Therefore,wearguethatsamplesshould\ncoverreasoningwithunknownfactstorepresentthisessentialaspectoflogicalreasoning. Wealso\nobservethatlogicalreasoninginvolvesvariousotheraspects,suchasillogicalreasoning,reasoning\nrules,andlinguisticexpressionsthatrepresentlogicalstatements(sections2.2to2.4). Thesamples\nshouldcovervariouspatternsregardingtheseaspectstoenableLLMstosolvevariousreasoning\nproblems. Wesummarizethesediscussionsintothefollowingdesignprinciples,whichguidethe\ndesignofsyntheticlogicsamples.\n1https://github.com/hitachi-nlp/FLD\n3\n2.1 TeachingReasoningwithUnknownFacts\nWefirstexploretheessenceoflogicalreasoningthatdifferentiatesitselffromknowledge. Consider\nthefollowinglogicalstep:\nTheEarthorbitstheSun. IftheEarthorbitsthesun,theEarthhasfourseasons.\n(1)\nTheEarthhasfourseasons.\nThisstepisvalidbecausetheconclusionislogicallyderivedfromthetwopremises. Next,consider\nanotherlogicalstep:\nTheEarthorbitstheSun. IftheEarthorbitsthesun,theEarthdoesnothavefourseasons.\n(2)\nTheEarthdoesnothavefourseasons.\nThesecondpremiseandconsequently,theconclusion,isfactuallywrong. Nevertheless,ifthepremise\nwashypotheticallycorrect, theconclusioncouldbelogicallyderived. Therefore, step(2)isalso\nlogicallyvalid. Finally:\n1.AFoostarexists. 2.IfaFoostarexists,aBarstaralsoexists.\n(3)\nABarstarexists.\n“Foostar”and“Barstar”areunknowns;nonetheless,wecanstilldeterminethatstep(3)islogically\nvalid. Steps (1) to (3) above can be abstracted into a deduction rule, i.e., modus ponens, using\nsymbols:\nF F →G\nmodusponens (4)\nG\nAswehaveseen,thelogicalvalidityofadeductionruledependssolelyonwhethertheconclusion\nislogicallyderivedfromthepremises, notonthefactualcorrectnessofthecontentsofF andG.\nTherefore,thecontentsofF andG canbearbitrary.\nNow,weconsiderwhatkindofsampleswouldbeneededtoteachthedeductionrule(4)toLLMs.\nWeassumeatasktogeneratetheconclusiongiventhepremisesaspromptinputs. Ifthelearnerwere\nhuman,theywouldbeabletoinfertheunderlyingdeductionrule(4)byobservingsamplessuchas\n(1)to(2). Asaresult,theywouldbecomeabletosolvetheunknownproblem(3).\nHowever,fromapurelyinductiveperspective,samples(1)to(2)cannotsimplybegeneralizedtothe\ndeductionrule(4). Thisisbecausethesamples(1)to(2)themselvesdonotcontaintheinformation\nthatthecontentsofF andG arearbitrary. Infact,onecouldgeneralizesamples(1)to(2)toother\nrules;forexample,theconclusionG canbederivedifF andF →G aregivenaspremisesandF\nandG include’Earth’astheircontents. Innumerablesuchdeductionrulescanbeinductivelyinferred\nfromthegivensamples. Inotherwords,inductionhasarbitrariness(Hume,1748;Goodman,1954;\nQuine,1969).\nHumansprefersimplerrules(Bertrand;Wittgenstein,1922),sotheyboldlyinduceuptothededuction\nrule(4). However,itisunclearhowpurelyinductivelearnerssuchasLLMs,whichextractonlywhat\ncanbeinferredfromsampleswithoutpriorpreferences,induceupto(4).Forexample,ifonlyspecific\ncontentssuchas“Aliceiskind”and“Bobissmart”areassignedtoF andG intrainingsamples,an\nLLMcoulddevelopintoamachinethatgeneratestheconclusionG onlywhentheinputcontainsthe\nspecificcontents. InorderforLLMstoaccuratelyinducethatF andG areindeedarbitrary:\nDesignPrinciple1(ReasoningwithUnknownFacts). Preparemanysamplesassigningarbitrary\ncontentstoF andG.TheywillmakeLLMsaccuratelyinduceF andGareindeedarbitrary,ultimately\nenablingthemtoreasonwithunknownfacts.\n2.2 TeachingIllogicalReasoning\nSupposewehaveLLMstrainedonalargenumberofsamplesasfollows:\nF∧G (F∧G)→H\n(5)\nH\nwhere∧denoteslogicalconjunction,andarbitrarycontentsareassignedtoF,G,H. Supposethat\nwegivethisLLMaproblemsuchas:\nF (F∧G)→H\n(6)\n??\nSincethepremisesareinsufficientforlogicallydeductingtheconclusion,outputtingnothingisthe\ncorrectanswer.\n4\nUnfortunately,anLLMcouldoutputH,whichwasindeedoftenobservedinourpreliminaryexperi-\nments. ThisisbecausewhiletheLLMscaninducefromsample(5)thatitcangeneratetheconclusion\nHwhenthetwopremisesof(5)aregiven,theLLMscannotinducefromthesamplethatitisnot\nallowedtogeneratetheconclusionHwhenthepremisesof(6)aregiven,assuchinformationisnot\nincludedinthesample(5)itself. Therefore,\nDesignPrinciple2(IllogicalReasoning). Includenegativesamplessuchas(6). Thesesampleswill\nmakeLLMsinducethatconclusionscannotbederivedfrominsufficientpremises.\n2.3 TeachingDiverseReasoningRules\nDeductionrulesotherthan(4)exist:\n(F∧G) (F∧G) (F →G)∧(G →H)\n∧elimination syllogism\nF G F→H\nF→G ¬(F∨G) ¬(F∧G)\ncontraposition DeMorgan’slaws (7)\n¬G→¬F ¬F∧¬G ¬F∨¬G\nwhere∨denoteslogicaldisjunctionand¬negation. Sincethereareinfinitelymanypossiblelogical\nformulas that can appear as premises and conclusions, there are infinitely many deduction rules.\nProvidingLLMswiththeseinfinitedeductionrulesisobviouslyintractable.\nInsteadofdirectlyprovidingtheseinfinitedeductionrules,wecantakeanotherapproach. Consider\nmulti-stepdeductivereasoning(Figure2left),wheremultipledeductionrulesderiveaconclusion.\nNotice that the syllogism in (7) can be expressed by multi-step deductive reasoning using more\n“atomic”deductionrules. Indeed,thereexistsasetofatomicdeductionrulescalledtheaxiomsthat\nsatisfiesthefollowing:\nTheorem2.1(CompletenessofFirst-OrderPredicateLogicGödel(1930)). Anyvaliddeductionrule\ncanbeexpressedbymultistepdeductivereasoningconstructedfromtheaxioms.\nIncontrasttotheaxioms,the‘compound’deductionrules,suchassyllogism,contraposition,and\nDe Morgan’s laws, are called theorems. According to the completeness Theorem 2.1, if we can\nhandletheaxioms,wecaneffectivelyhandleotherdeductionrulesaswell. Indeed,Morishitaetal.\n(2023)empiricallyverifiedthatalanguagemodeltrainedontheaxiomsgeneralizestohandleother\ndeductionrulesmoreeffectivelythanthosetrainedonnon-axiomdeductionrules. Therefore,\nDesignPrinciple3(DiverseReasoningRules). Samplesshouldexpressmulti-stepdeductioncon-\nstructedfromtheaxioms. TheywilleffectivelyteachLLMsdiversedeductionrules(Morishitaetal.,\n2023)\nInmulti-stepdeductivereasoning,thenumberoflogicalstepssfrompremisestoaconclusioncan\nvarylargelydependingontheproblem. Therefore:\nDesignPrinciple3’(DiverseReasoningRules). Samplesshouldincludediversenumbersoflogical\nstepss.\nIdeally,thiswouldbesufficient,butempiricalevidencehasshownthatLLMsstrugglewithconstruct-\ningmulti-stepdeductivereasoningwithlargestepss(Gontieretal.,2020;Morishitaetal.,2023).\nConsequently,LLMswouldnotexcelathandlingtheoremsthatrequirealargenumberofstepss\nwhenexpressedbytheaxioms. Therefore,asanadditionalcountermeasure:\nDesignPrinciple3”(DiverseReasoningRules).Samplesshouldalsoincluderepresentativetheorems,\nsuchassyllogism,contraposition,andDeMorgan’slaws.\n2.4 TeachingDiverseLinguisticExpressionsthatRepresentLogicalStatements\nTherearevariouslinguisticstructuresforexpressingthelogicalrelationshipF →G,suchas“IfF\nthenG”,“F leadstoG”,and“F resultsinG”. Ifweonlyincludespecificexpressionsinthecorpora,\nLLMsmayonlylearntoreacttothesespecificexpressions,whichhasbeenobservedinprevious\nexperiments(Zhangetal.,2022;Yuanetal.,2023). Topreventthis,\nDesign Principle 4 (Diverse Linguistic Expressions). Samples should include diverse linguistic\nexpressionsthatrepresentlogicalstatements.\nInthischapter, wehaveestablishedtheprinciplestoguidethedesignofsyntheticlogicsamples.\nNext,weconstructasyntheticlogiccorpusbasedontheseprinciples.\n5\nTable1: Syntheticlogiccorporacomparedinthisstudy,withtheirfeaturescategorizedaccordingto\nourproposeddesignprinciples(DP).Notethatthelastrowoftheablationcorporalistsvariationsof\nFLD×2,eachofwhichdiffersfromtheoriginalregardingoneofthedesignprinciples.\nDP1 DP2 DP3 DP4\nvocabularysize distractors deductionrules logicalsteps expressionsperformula\nRuleTaker(Clarketal.,2021) ≤100 random 2\n1–5 O(1)\n(RT) (hand-selected) formula (implication)\nPARARULE-Plus(Baoetal.,2022) ≤100 random 2\n1–5 O(1)\n(PRP) (hand-selected) formula (implication)\n≃15k random 13\nFLD(Morishitaetal.,2023)\n(WordNet,subset) formula (axioms)\n1–8 10∼100\nFLD×2 ≃100k adversarial ≃50 1–8 10∼100\n(WordNet,full) formula (axiomsandtheorems) (moreextensivethanFLD)\nFLD×2 100 notused 2(implication) 1 1\nablationcorpora→ →w/oDP1 →w/oDP2 →w/oDP3.rules →w/oDP3.steps →w/oDP4\n3 CreatingaSyntheticCorpusbasedonDesignPrinciples\nTopreparediversesamplesreflectingthedesignprinciples1to4(DP1-4),webuiltanovelsample\ngeneratorbyextendingthepreviousonebyMorishitaetal.(2023)andthengeneratedthesynthetic\nlogiccorpusnamedFLD×2(FormalLogicDeductionDiverse). Figure2showsaschematicofour\ngenerator and a deduction sample. Table 1 compares FLD×2 with existing corpora. Figure D.3\nprovidesanactualdeductionsampleincludedinFLD×2.\nMorespecifically,ourgeneratorgeneratesdeductionsamplesthroughthefollowingsteps. First,the\ngeneratorrandomlygeneratesasampleofmulti-stepdeductivereasoningwritteninlogicalformulas,\nasshownontheleftsideofFigure2,whereaconclusionisderivedfrompremisesusingmultiple\ndeductionrules(SeeAppendixD.3formoredetailsofthisgenerationprocedure). Atthistime,the\ngeneratoralsogenerates‘distractor’logicalformulas,whichexpressnegativepremisesofDP2. Next,\nthegeneratorconvertseachlogicalformulaintoEnglishexpressions. Toachievethis,thegenerator\nfirstrandomlyselectsatemplatefrompre-definedoptions,suchas“IfF,thenG,”“F leadstoG,”\nor “F results in G,” for the logical formula “F → G.” It then assigns English content randomly\nconstructedfromavocabulary, suchas“(that)aFoostarexists”and“(that)aBarstarexists,” to\neachsymbol,suchasF andG. Finally,itconvertsthemulti-stepdeductionintoadeductionsample\n(rightsideofFigure2)byusingthepremisesas‘facts’, theconclusionas‘hypothesis’, andthe\nintermediatelogicalstepsas‘logicalsteps’. ThedeductionsamplerequiresLLMstogeneratelogical\nstepsthatderiveagivenhypothesisbasedonthegivenfacts.\nTable1outlinesthecomparisonofFLD×2withotherexistingcorpora(Clarketal.,2021;Baoetal.,\n2022;Morishitaetal.,2023)intermsofDP1-4,whichisdetailedasfollows:\n• DP1: WeassignF andG contentrandomlyconstructedfromavocabulary. Whiletheexisting\ncorporausedsmall-sizedvocabularyofupto15k,weusealargevocabularyofaround100kwords\nbuiltfromWordNet(Miller,1995).ThiswillteachLLMsthatF andGaretrulyarbitrary,ultimately\nenablingthemtoreasonwithunknownfacts.\n• DP2: Theexistingcorporausedrandomlygeneratedlogicalformulasasdistractors. Incontrast,\nweimplementadversarialdistractors. Forexample,forapremiseF ∧G,weuseF withmissing\ninformation(seeEquations(5),(6)),andforapremiseF →H,weuseF ∧G →Hwithmissing\ninformation as distractors. These distractors teach LLMsprecisely when a conclusioncan and\ncannotbederived. Aswithpreviouscorpora,weincludeavariablenumberofdistractorsineach\nsample,randomlychosenfromarangeof0to20.\n• DP3-3”: Whiletheexistingcorporausedasmallnumberofdeductionrulesofupto13(referto\nFigureB.4ofMorishitaetal.(2023)),weincludediversedeductionrules,encompassingtheaxioms\nandrepresentativetheorems,suchasmodusponens,syllogisms,andcontraposition,totalingabout\n50rules. Weincludesampleswithuptos=8logicalsteps,following(Morishitaetal.,2023).\n• DP4: We manually craft several more English templates per logical formulas than those used\nin FLD. Since the templates have a nested structure, they yield combinatorially more diverse\nEnglishexpressions. Whilecountingtheexactnumberoftheresultingexpressionsisintractable,\nweobservedatleastdozensofexpressionsperlogicalformula,includingminorvariations. See\nAppendixD.4fordetails.\n6\n4 ExperimentalSetup\nWebrieflyexplaintheexperimentalsettings. RefertoAppendixEforthedetails.\nSyntheticLogicCorpora: WeexaminetheproposedFLD×2andpreviouscorpora(Table1).\nLLMs: Weusedthestate-of-the-artLLM,LLaMA-3.1(8Band70B)(AI@Meta,2024).\nTrainingSettings: WetrainedtheLLMsbyamethodsimilartosupervisedfine-tuning;asillustrated\ninFigure2,weusedthefactsandhypothesisasinputsandlogicalstepsandadditionalanswerlabel\n(seeAppendixD.1)asoutputs. WeexcludedlosscomputationfortheinputstopreventLLMsfrom\nlearningtogenerateunknownfacts. WetrainedtheLLMsfor1epochon100ksamples(∼0.1B\ntokens)fromthetrainingsplitofeachcorpus,withabatchsizeof256,resultingin390steps,witha\nlinearwarmupfor200steps. Weusedthelearningrateof2e-05forthe8Bmodeland3e-06forthe\n70Bmodel. WeusedHuggingface(Wolfetal.,2020)forimplementation.\nPreventionofKnowledgeForgettingbyRecallAdamOptimizer: Syntheticlogiccorporainclude\nmanysampleswithunknownfacts,sotrainingonthemshouldcauseLLMstoforgettheirknowledge\nof existing facts. To prevent this, we employed the Recall Adam optimizer (Chen et al., 2020),\nwhich regularizes parameter updates to avoid deviating too far from the pre-training parameters.\nRecall Adam stands out for LLM training for several reasons (see Appendix E.0.1 for details).\nWe used our re-implemented version 2. The hyperparameters were: β = 0.9,β = 0.999,ϵ =\n1 2\n10−6,fishercoefficient=4000forthe8Bmodeland2000forthe70Bmodel.\nBenchmarks: WeevaluatedthetrainedLLMson31benchmarksshowninTableE.7using5-shot\nin-contextlearning,exceptforBBHandAbuductionRules,whichused3-shotin-contextlearning.\nThesebenchmarkscoverawiderangeoftasksandareprominentinLLMevaluation. Notethatwe\nexcludedthesyntheticlogiccorporausedfortraining,astrainingonthemoftenleadstooverfitting\nto their superficial and statistical cues (Zhang et al., 2022; Yuan et al., 2023), failing to measure\ntrulygeneralizablereasoningcapabilities. Weusedlm-evaluation-harness(Gaoetal.,2023)and\nbigcode-evaluation-harness(BenAllaletal.,2022)fortheimplementation.\n5 CanAdditionalLogicTrainingEnhanceLLMs’Capabilities?\nTable 2 show the performance of LLMs before and after ALT. Most LLMs trained with ALT\noutperformedtheircounterpartswithoutALT.Notably,ALTyieldedsubstantialgainsofupto10\npointsevenforLLaMA-3.1-70B,thelargestLLMpre-trainedonover15trilliontokens. Theseresults\nverifythatALTcanenhancethecapabilitiesofstate-of-the-artLLMs.\nAmongtheLLMstrainedwithALT,theonetrainedonFLD×2(i.e.,⊕ALT-FLD×2)achievedthe\nhighestgeneralizationperformanceacrossthebenchmarks. Table3showstheperformanceofthe\nLLMs trained on ablated FLD×2 corpora, each of which lacks one of the design principles. As\nseen, ablatinganydesignprinciplealmostalwaysledtoperformancedegradation. Theseresults\ndemonstratethattheproposeddesignprinciplesarecriticaltoobtainingthemaximumpossiblegain\nfromALT,andeachprincipleisindispensable.\nTableF.8showsthattheLLMstrainedwithALTwithoutpreventingknowledgeforgettingbyRecall\nAdamoptimizerunderperformedcomparedtotheircounterpartstrainedwithknowledgeforgetting\npreventionandeventheLLMwithoutALT.Thisbehaviorpresumablyoccurredbecausetheunknown\nfactsincludedinsyntheticlogiccorporadisplacedtheLLM’sknowledgeofexistingfacts. Therefore,\nknowledge-forgettingpreventioniscriticallyimportantforthesuccessofALT.\n6 WhatCapabilitiesCanAdditionalLogicTrainingEnhanceandWhy?\nWeanalyzetheresultsoneachbenchmarkoreachcaseanddiscusswhetherandwhytheLLM’s\ncapabilitiestosolvethetaskscanorcannotbeenhancedbyALT.\n6.1 LogicalReasoningTasks\nTable4ashowsthatALTsubstantiallyboostedLLaMA-3.1-70B’sperformancebyupto30points\non various benchmarks dealing with logical reasoning tasks. Surprisingly, we also observed im-\nprovementsonabductivereasoningtasks,whichgobeyondtheoriginaldeductivereasoningtasks\n2https://github.com/hitachi-nlp/rec-adam\n7\nTable2: 5-shotperformanceofLLMsbeforeandafterALT.⊕ALT-xdenotestheLLMtrainedwith\nALTonthesyntheticlogiccorpusxfromTable1. Thecolorshowstherankineachcolumn(darker\nisbetter). Eachbenchmarkset,suchas“Logic”and“Math”,comprisesvariousbenchmarksinthat\ndomain(seeTableE.7). “Avg.” representsthemicro-averageofallthebenchmarks.\n(a)LLaMA-3.1-8B.\nAvg. Logic Math Code NLI Others BBH(3-shot) BBH(0-shot) MMLU\nCoT CoT Pro\nLLaMA-3.1-8B 47.9 42.8 39.6 35.4 65.4 60.7 44.9 61.9 8.2 36.5 65.3 35.8\n±0.4 ±0.5 ±0.3 ±0.3 ±0.4 ±0.4 ±0.2 ±0.4 ±0.4 ±0.4\n⊕ALT-PRP 48.1 43.7 39.2 35.7 65.6 60.8 44.9 61.8 8.2 36.4 65.3 35.3\n±0.2 ±0.3 ±0.2 ±0.2 ±0.2 ±0.2 ±0.1 ±0.2 ±0.2 ±0.2\n⊕ALT-RT 50.1 46.8 42.4 36.5 68.6 61.3 46.9 63.5 13.7 38.4 65.3 35.7\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\n⊕ALT-FLD 51.9 51.6 43.4 38.1 70.1 61.5 46.7 64.9 11.9 39.6 65.4 36.2\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\n⊕ALT-FLD×2 52.0 52.2 43.2 38.0 70.7 61.5 46.5 65.3 11.3 38.7 65.5 36.4\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\n(b)LLaMA-3.1-70B.\nAvg. Logic Math Code NLI Others BBH(3-shot) BBH(0-shot) MMLU\nCoT CoT Pro\nLLaMA-3.1-70B 60.0 57.4 60.0 46.2 73.7 67.7 60.4 82.1 6.5 50.1 78.7 50.7\n±0.4 ±0.5 ±0.3 ±0.3 ±0.3 ±0.2 ±0.1 ±0.3 ±0.3 ±0.4\n⊕ALT-PRP 60.4 57.7 59.8 49.2 73.5 67.6 60.4 82.2 6.0 50.1 78.7 50.9\n±0.4 ±0.5 ±0.3 ±0.3 ±0.4 ±0.3 ±0.2 ±0.4 ±0.3 ±0.4\n⊕ALT-RT 62.7 61.4 62.1 50.8 75.4 68.4 64.1 82.5 11.5 59.2 79.0 52.4\n±0.2 ±0.3 ±0.2 ±0.2 ±0.3 ±0.2 ±0.2 ±0.3 ±0.2 ±0.3\n⊕ALT-FLD 64.2 65.7 63.6 52.0 75.3 68.5 65.0 83.6 12.1 59.9 79.3 54.4\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.1 ±0.2\n⊕ALT-FLD×2 64.4 66.1 63.3 52.4 76.1 68.5 65.4 83.6 11.4 60.8 79.5 54.4\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\nTable3: LLaMA-3.1-8Btrainedontheablationcorpora.\nAvg. Logic Math Code NLI Others BBH(3-shot) BBH(0-shot) MMLU\nCoT CoT Pro\n⊕ALT-FLD×2 52.0 52.2 43.2 38.0 70.7 61.5 46.5 65.3 11.3 38.7 65.5 36.4\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\nw/oDP1 51.4 52.2 43.1 39.2 70.0 59.4 46.7 64.7 11.5 38.9 65.4 36.1\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\nw/oDP2 50.6 49.9 43.1 38.1 71.1 59.3 46.1 64.6 10.4 37.4 65.4 35.7\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\nw/oDP3.rules 50.7 50.4 42.8 38.3 69.5 59.4 46.4 64.0 11.8 38.3 65.6 36.2\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\nw/oDP3.steps 51.1 51.5 43.1 38.7 69.6 59.5 46.8 65.0 12.3 38.8 65.6 36.3\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\nw/oDP4 51.3 52.2 42.8 38.4 70.3 59.5 46.1 64.8 12.8 39.3 65.5 36.3\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\ninsyntheticlogiccorpora. Abductivereasoninginvolvesguessingthemissingpremisesthatcaused\ntheobservedconclusionratherthanderivingaconclusionfromthepremises. Forexample,from\ntheobservedconclusion,“thewindowglassathomewasbrokenandtheroomwasransacked,”we\nguessthepremise“aburglarbrokein.” Theimprovementswouldbeduetothefactthat,whilethe\nsurfaceformofabductivereasoningproblemsdiffersfromthatofdeductivereasoning,theysharethe\nfundamentalsoflogicreflectedinthedesignprinciples.\nNext,weconductcaseanalysestoseewhethertheLLMenhancedbyALTacquiredtheabilities\nintendedbytheproposeddesignprinciples(DP1-4). Table5showsproblemswhereLLaMA-3.1-\n70B’serrorshavebeencorrectedbyALT.Thefirstproblemisverysimple,soitissurprisingthat\nLLaMA-3.1-70Bfailedtosolveit,indicatingtheinherentdifficultyoflearninglogicalreasoning\nsolely from pre-training. In contrast, ⊕ALT-FLD×2, which was additionally trained on FLD×2,\nsolved the problem correctly. The premises of the problem are randomly constructed to express\nunknownfacts. Therefore,theresultsuggeststhat⊕ALT-FLD×2acquiredgenuinelogicalreasoning\nability,whichcanhandleunknownfacts(DP1).\nInthesecondproblem,⊕ALT-FLD×2correctlyanswered“neutral”,indicatingthatitsuccessfully\nlearnedthatconclusionscannotbederivedfrominsufficientfacts(DP2).\nThe third problem comes from the FOLIO benchmark. To solve this problem, LLMs must use\nsyllogismatthefirststepasfollows: “Alleelsarefish,andnofishareplants. Therefore,allellsare\nnotplants.” ⊕ALT-FLD×2answeredthisproblemcorrectly,suggestingthatitsuccessfullylearned\ndiversedeductionrules(DP3).\nFOLIOproblemsarecreatedbasedonWikipediatopics,describingtheminmorenaturalandrealistic\nlinguistic expressions than in other benchmarks. As seen in the fourth problem, ⊕ALT-FLD×2\nunderstands such expressions, suggesting the effect of diverse expressions from DP4 and/or that\nLLMscanintegratetheiroriginallinguisticabilitywiththenewlyacquiredlogicalreasoningability.\n8\nTable4: Benchmark-wise5-shotperformanceofLLaMA-3.1-70BbeforeandafterALTonFLD×2.\nRefertoTableF.9 forLLaMA-3.1-8Bresults. TableE.7detailseachbenchmark.\n(a)Logic.\nbAbiD FOLIO LogicNLI RobustLR AR-LSAT LogiQA ReClor AbductionR ART\nLLaMA-3.1-70B 83.8 58.9 34.9 49.6 21.5 64.3 33.7 84.0 85.4\n±1.2 ±1.6 ±1.1 ±0.9 ±1.0 ±1.2 ±0.7 ±0.7 ±0.9\n⊕ALT-FLD×2 83.5 66.7 50.9 81.6 25.0 69.4 36.3 95.7 85.5\n±0.5 ±0.6 ±0.5 ±0.3 ±0.4 ±0.5 ±0.3 ±0.2 ±0.4\n(b)Math.\nGSM8k MATH MathQA\nCoT CoT(0-shot) - -\nLLaMA-3.1-70B 80.9 75.2 65.4 23.7 55.0\n±1.1 ±1.2 ±1.3 ±0.6 ±0.9\n⊕ALT-FLD×2 83.3 80.4 73.0 24.4 55.4\n±0.4 ±0.4 ±0.5 ±0.2 ±0.4\n(c)Code.\nHumanEval MBPP MBPP+ MultiPL-E(cpp) MultiPL-E(go)\nLLaMA-3.1-70B 32.3 43.4 48.7 29.8 76.6\n⊕ALT-FLD×2 42.6 49.5 52.5 38.7 78.6\n(d)Naturallanguageinference(NLI).\nHELP MNLI RTE SNLI\nLLaMA-3.1-70B 45.8 82.2 84.0 82.6\n±0.5 ±0.4 ±0.7 ±0.4\n⊕ALT-FLD×2 51.3 83.7 87.2 82.3\n±0.2 ±0.2 ±0.3 ±0.2\n(e)Others.\nCommonsenseQA HellaSwag SQuAD WinoGrande ARCe ARCc GPQA OpenBookQA SciQ\nLLaMA-3.1-70B 81.2 69.2 38.5 85.6 89.1 65.3 40.7 41.4 98.5\n±1.1 ±0.5 ±0.0 ±1.0 ±0.6 ±1.4 ±1.4 ±0.7 ±0.4\n⊕ALT-FLD×2 82.5 69.6 40.1 86.1 89.4 66.7 40.6 42.8 98.5\n±0.4 ±0.2 ±0.0 ±0.4 ±0.3 ±0.6 ±0.6 ±0.3 ±0.2\n6.2 MathandCodingTasks\nTables4b,4cshowsthatALTsubstantiallyboostedtheLLaMA-3.1-70B’sperformancebyupto7\nand10pointsonmathandcodingtasks,respectively. Themathimprovementsarereasonable,as\nunderstandingpredicatelogicisaprerequisiteforsolvingmathematicalproblems. Forcoding,some\nrecentstudieshaveverifiedtheoppositedirection,namely,thattrainingoncodingdataimproves\nlogicalreasoningabilities(Jiangetal.,2024b;MAetal.,2024;Uchiyamaetal.,2024).\n6.3 NLITasks\nTable4dshowsthatALTsubstantiallyboostedtheLLaMA-3.1-70B’sperformancebyupto6points\nonvariousnaturallanguageinference(NLI)benchmarks. NLIissimilartodeductivereasoningin\nassessingwhetherapremisesupportsorcontradictsahypothesis. However,themaindifferenceis\nthatthisjudgmentrequiresarichsetofcommonsenseknowledgebeyondthegivenpremise.\nConsiderthefifthprobleminTable5: bysupplementingthegivenfact“AnIndianwomanisdancing\nwithherpartner”withthecommonsenseknowledge“Ifsomeoneisdancing,thenhe/sheismoving.”,\nwecanderivethehypothesis“Awomanismoving.” Thesixthproblemismorechallengingaswe\nhavetotracemultiplelogicalstepswhilesupplementingwithsufficientcommonsenseknowledgeas\nfollows: “achurchchoirsingsatachurch,”“baseballisoftenplayedatabaseballfield,”“aperson\ncannotbeintwoormoreplacesatthesametime,”“therefore,achurchchoircannotsingforbaseball.”\nSincesyntheticlogiccorporaonlycontainunknownfacts,LLMscannotacquirenewknowledge\nfromthem. Therefore,thecommonsenseknowledgeusedtosolvetheaboveproblemsmusthave\nbeenacquiredbytheLLMsfrompre-training. ThissuggeststhatLLMscanintegratetheiroriginal\nknowledgewiththelogicalreasoningcapabilitiesnewlyacquiredfromALTtosolveproblems.\n9\nTable5: ProblemswhereLLaMA-3.1-70Binitiallyansweredincorrectlyandthencorrectlyafter\ntrainingwithALTonFLD×2. Redhighlightsthepremisesrelatedtothehypothesis.\nanswer required\nbenchmarkpremises hypothesis\n(LLaMA-3.1-70B/gold) ability\nMiceareafraidofwolves.Catsareafraidofsheep.\nJessicais neutral/\nJessicaisacat.Wolvesareafraidofcats. DP1\nLogicNLI afraidofsheep. entailment\nWinonaisawolf.Sheepareafraidofcats.\nRhettisnotmodest.Vivianisconfused. Rhettis entailment/\nDP2\nRhettislazy.Ifsomeoneismodestornotconfused,thenheisnoteager. confused. neutral\nAlleelsarefish.Nofishareplants.\nEverythingdisplayedinthecollectioniseitheraplantorananimal. Theseaeel\nneutral/\nAllanimalsdisplayedinthecollectionaremulticellular. ismulticellular DP3\nFOLIO entailment\nAseaeelisdisplayedinthecollection. orisbacteria.\nTheseaeelisaneelorananimalornotaplant.\nCommonutilitiesincludewater,electricity,gas,heating,sewer,trash,andrecycling.\nManyapartmentrentscoverthecostofwaterandelectricity. NoahandAvaboth\nneutral/\nSusanlivesinanapartmentwheretherentcoversallutilities. needtopay DP4\nentailment\nTherentoftheapartmentwhereAvalivesdoesnotcoveranyutilityexpenses. theheatingbill.\nNoahlivesinanapartmentwheretherentdoesnotcoverheating.\nneutral/\nAnIndianwomanisdancingwithherpartner. Awomanismoving. reasoning\nSNLI entailment\nwith\nThischurchchoirsingstothemasses Achoirissinging entailment/ commonsense\nastheysingjoyoussongsfromthebookatachurch. atabaseballgame. contradiction knowledge\nTable6: ProblemsthatLLaMA-3.1-70BtrainedwithALTonFLD×2stillcannotsolve.\nbenchmark question answer\nARC Theendresultintheprocessofphotosynthesisistheproductionof Chlorophyllinthe\n(challenge) sugarandoxygen.Whichstepsignalsthebeginningofphotosynthesis? leafcaptureslightenergy.\nAspin-halfparticleisinalinearsuperposition0.8|↑⟩+0.6|↓⟩ofitsspin-up\nGPQA andspin-downstates.If|↑⟩and|↓⟩aretheeigenstatesofσz,thenwhat −0.7\nistheexpectationvalueuptoonedecimalplace,oftheoperator10σz+5σx?\nARC Beaversbuildtheirhomesinpondsandstreams.Whichcharacteristic (A)waterprooffur(B)webbedhindfeet\n(challenge) isleastcriticaltobuildinghomesinanaquaticenvironment? (C)arge,sharpteeth(D)flat,widetail\n6.4 OtherTasks\nImprovementsacrossvariousothertasks(Table4e)demonstratethebroadbenefitsoftheobtained\nreasoningcapabilitiesbeyondstandardreasoningtasks;thoughtheimprovementsweremodestatup\nto2percentagepoints,whichmaybeduetothefollowingreasons. First,thesebenchmarksinclude\nproblemsthatpurelytestknowledge,suchasthefirstoneinTable6. SinceALTdoesnotaimto\nprovidenewknowledge,theabilitytosolvesuchproblemsdoesnotimprovebynature. Next,some\nproblemsmayrequireknowledgethatistooadvancedforLLMs,sopotentialimprovementsbythe\nenhancedreasoningcapabilitiesmaybebottlenecked. Forexample,thesecondproblemdoesinvolve\nreasoningbutrequiressufficientquantummechanicsknowledgeasaprerequisite. However,these\nknowledge-relatedissuesshouldbesolvedbyimprovingthequantityandqualityofpre-training.\nFinally,LLMsmaynotbeabletofullyutilizethepotentialofenhancedreasoningcapabilitiesfor\nproblems that require complex procedures. To solve the third problem, LLMs first must attempt\nreasoningrelatedtoeachchoiceasfollows: “Tobuildhomesinanaquaticenvironment,oneneedsto\nmaintainbodyheatandinsulationdespitebeingfrequentlysubmergedincoldwater. Therefore,the\nwaterprooffurof(A)isessential”,and“Tobuild...,onemustgatherandprocessnaturalmaterials\nlikewood. Large,sharpteethof(C)arecriticalastheyallowbeaverstocutdowntreesandshape\nbranches.” Next,whilereasoningtraceson(A)to(D)allseemreasonable,LLMsmustchoosethe\nsinglebestanswer, consideringthesubtlenuanceofthequestioncontext, asfollows: “Sincethe\nquestionemphasizestheaquaticenvironment,theleastrelatedreasoningtraceshouldbe(C).”This\ncomplexprocedurecontrastswithlogicalreasoningandNLIproblems,whereLLMscandirectly\nobtainananswerfromasinglereasoningtrace. Previousstudiesalsoobservedthatsuchprocedureon\nmultiple-choiceQAproblemsarechallengingforLLMs(RobinsonandWingate,2023;Zhengetal.,\n2024;Wangetal.,2024a). SinceALTalonedoesnotteachLLMssuchtask-specificprocedures,\nadditionaltrainingontheseproceduresshouldbenecessarytosolvetheseproblems.\n7 Conclusion\nTowardsversatileartificialintelligencewithreasoningcapabilities,weproposedAdditionalLogic\nTrainingonsyntheticlogicsamples. Weestablishedsystematicdesignprincipleswell-groundedon\nsymboliclogictheoryandpreviousempiricalfindings. WeconstructedacorpusnamedFormalLogic\nDeductionDiverse(FLD×2)basedonthedesignprinciples. WeempiricallyshowedthatALTon\nFLD×2substantiallyenhancesthecapabilitiesofstate-of-the-artLLMs.\n10\nAcknowledgement\nComputational resources of AI Bridging Cloud Infrastructure (ABCI) provided by the National\nInstituteofAdvancedIndustrialScienceandTechnology(AIST)wereused. WethankDr. Masaaki\nShimizuatHitachifortheconvenienceofadditionalcomputationalresources. WethankDr. Naoaki\nOkazaki,aprofessorattheTokyoInstituteofTechnology,forthekeencomments.\nReferences\nAI@Meta.2024. Llama3modelcard.\nAidaAmini, SaadiaGabriel, ShanchuanLin, RikKoncel-Kedziorski, YejinChoi, andHannaneh\nHajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-\nbased formalisms. In Proceedings of the 2019 Conference of the North American Chapter of\ntheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(Long\nandShortPapers),pages2357–2367,Minneapolis,Minnesota.AssociationforComputational\nLinguistics.\nRisako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, and Mitsuhiro Okada. 2023.\nEvaluatinglargelanguagemodelswithNeuBAROCO:Syllogisticreasoningabilityandhuman-like\nbiases. InProceedingsofthe4thNaturalLogicMeetsMachineLearningWorkshop,pages1–11,\nNancy,France.AssociationforComputationalLinguistics.\nYoichiAoki,KeitoKudo,TatsukiKuribayashi,ShusakuSone,MasayaTaniguchi,KeisukeSakaguchi,\nandKentaroInui.2024. Firstheuristicthenrational: Dynamicuseofheuristicsinlanguagemodel\nreasoning.\nAmandaAskell.2020. Gpt-3: Towardsrenaissancemodels. DailyNousBlog: PhilosophersOn\nGPT-3.\nJacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,DavidDohan,\nEllenJiang,CarrieCai,MichaelTerry,QuocLe,etal.2021.Programsynthesiswithlargelanguage\nmodels. arXivpreprintarXiv:2108.07732.\nQimingBao,AlexYuxuanPeng,TimHartill,NesetTan,ZhenyunDeng,MichaelWitbrock,and\nJiamouLiu.2022. Multi-stepdeductivereasoningovernaturallanguage: Anempiricalstudyon\nout-of-distributiongeneralisation. InProceedingsofthe16thInternationalWorkshoponNeural-\nSymbolicLearningandReasoningaspartofthe2ndInternationalJointConferenceonLearning\n& Reasoning (IJCLR 2022), pages 202–217, Cumberland Lodge, Windsor Great Park, United\nKingdom.\nLoubnaBenAllal,AntonLozhkov,GuilhermePenedo,ThomasWolf,andLeandrovonWerra.2024.\nCosmopedia.\nLoubnaBenAllal,NiklasMuennighoff,LogeshKumarUmapathi,BenLipkin,andLeandrovon\nWerra.2022. Aframeworkfortheevaluationofcodegenerationmodels. https://github.\ncom/bigcode-project/bigcode-evaluation-harness.\nLuisaBentivogli,IdoDagan,HoaTrangDang,DaniloGiampiccolo,andBernardoMagnini.2009.\nThefifthpascalrecognizingtextualentailmentchallenge. InTextAnalysisConference.\nLeonardo Bertolazzi, Albert Gatt, and Raffaella Bernardi. 2024. A systematic analysis of large\nlanguagemodelsassoftreasoners: Thecaseofsyllogisticinferences.\nRussellBertrand. Ahistoryofwesternphilosophy.\nGregorBetz,ChristianVoigt,andKyleRichardson.2021. Criticalthinkingforlanguagemodels.\nInProceedingsofthe14thInternationalConferenceonComputationalSemantics(IWCS),pages\n63–75,Groningen,TheNetherlands(online).AssociationforComputationalLinguistics.\nChandraBhagavatula,RonanLeBras,ChaitanyaMalaviya,KeisukeSakaguchi,AriHoltzman,Han-\nnahRashkin,DougDowney,ScottWen-tauYih,andYejinChoi.2019. Abductivecommonsense\nreasoning. arXivpreprintarXiv:1908.05739.\n11\nNeeladri Bhuiya, Viktor Schlegel, and Stefan Winkler. 2024. Seemingly plausible distractors in\nmulti-hopreasoning: Arelargelanguagemodelsattentivereaders?\nKajBostrom,XinyuZhao,SwaratChaudhuri,andGregDurrett.2021. Flexiblegenerationofnatural\nlanguagedeductions. InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,pages6266–6278,OnlineandPuntaCana,DominicanRepublic.Association\nforComputationalLinguistics.\nSamuelRBowman,GaborAngeli,ChristopherPotts,andChristopherDManning. Alargeannotated\ncorpusforlearningnaturallanguageinference.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney,Ming-HoYee,YangtianZi,CarolynJaneAnderson,MollyQFeldman,ArjunGuha,\nMichaelGreenberg,andAbhinavJangda.2023. Multipl-e: Ascalableandpolyglotapproachto\nbenchmarkingneuralcodegeneration. IEEETransactionsonSoftwareEngineering,49(7):3675–\n3691.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchenKrueger,MichaelPetrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,BrookeChan,\nScottGray,NickRyder,MikhailPavlov,AletheaPower,LukaszKaiser,MohammadBavarian,\nClemensWinter,PhilippeTillet,FelipePetroskiSuch,DaveCummings,MatthiasPlappert,Fotios\nChantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamHebgenGuss,AlexNichol,AlexPaino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,\nBobMcGrew, DarioAmodei, SamMcCandlish, IlyaSutskever, andWojciechZaremba.2021.\nEvaluatinglargelanguagemodelstrainedoncode.\nSanyuanChen,YutaiHou,YimingCui,WanxiangChe,TingLiu,andXiangzhanYu.2020. Recall\nandlearn: Fine-tuningdeeppretrainedlanguagemodelswithlessforgetting. InProceedingsof\nthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages\n7870–7881,Online.AssociationforComputationalLinguistics.\nXinyunChen,RyanAndrewChi,XuezhiWang,andDennyZhou.2024. Premiseordermattersin\nreasoningwithlargelanguagemodels. InProceedingsofthe41stInternationalConferenceon\nMachineLearning,volume235ofProceedingsofMachineLearningResearch,pages6596–6620.\nPMLR.\nJ.Cheng,M.Bernstein,C.Danescu-Niculescu-Mizil,andJ.Leskovec.2017. Anyonecanbecomea\ntroll: Causesoftrollingbehaviorinonlinediscussions. CSCW:ProceedingsoftheConferenceon\nComputer-SupportedCooperativeWork.ConferenceonComputer-SupportedCooperativeWork,\n2017.\nPeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,and\nOyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXivpreprintarXiv:1803.05457.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over\nlanguage. InProceedingsoftheTwenty-NinthInternationalConferenceonInternationalJoint\nConferencesonArtificialIntelligence,pages3882–3890.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,ChristopherHesse,andJohn\nSchulman.2021.Trainingverifierstosolvemathwordproblems.arXivpreprintarXiv:2110.14168.\nA.ColmerauerandPRoussel.1973. Thebirthofprolog. TheALPNewsletter.\nIdoDagan,OrenGlickman,andBernardoMagnini.2005. ThePASCALrecognisingtextualentail-\nmentchallenge. pages177–190.\n12\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pi-\npatanangkura,andPeterClark.2021. Explaininganswerswithentailmenttrees. InProceedingsof\nthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages7358–7370,\nOnlineandPuntaCana,DominicanRepublic.AssociationforComputationalLinguistics.\nIshitaDasgupta,AndrewK.Lampinen,StephanieC.Y.Chan,HannahR.Sheahan,AntoniaCreswell,\nDharshanKumaran,JamesL.McClelland,andFelixHill.2023.Languagemodelsshowhuman-like\ncontenteffectsonreasoningtasks.\nJohnDougrez-Lewis,MahmudElahiAkhter,YulanHe,andMariaLiakata.2024. Assessingthe\nreasoningabilitiesofchatgptinthecontextofclaimverification.\nNouhaDziri,XimingLu,MelanieSclar,XiangLorraineLi,LiweiJiang,BillYuchenLin,PeterWest,\nChandraBhagavatula,RonanLeBras,JenaD.Hwang,SoumyaSanyal,SeanWelleck,XiangRen,\nAllysonEttinger,ZaidHarchaoui,andYejinChoi.2023. Faithandfate: Limitsoftransformerson\ncompositionality.\nTiwalayoEisape,MichaelTessler,IshitaDasgupta,FeiSha,SjoerdSteenkiste,andTalLinzen.2024.\nAsystematiccomparisonofsyllogisticreasoninginhumansandlanguagemodels. InProceedings\nof the 2024 Conference of the North American Chapter of the Association for Computational\nLinguistics: HumanLanguageTechnologies(Volume1: LongPapers),pages8425–8444,Mexico\nCity,Mexico.AssociationforComputationalLinguistics.\nCharlesElkanandRussellGreiner.1993. Buildinglargeknowledge-basedsystems: Representation\nandinferenceinthecycproject: Dblenatandrvguha.\nLeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFoster,\nLaurenceGolding,JeffreyHsu,AlainLeNoac’h,HaonanLi,KyleMcDonell,NiklasMuennighoff,\nChrisOciepa,JasonPhang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,LintangSutawika,\nEricTang,AnishThite,BenWang,KevinWang,andAndyZou.2023. Aframeworkforfew-shot\nlanguagemodelevaluation.\nDaniloGiampiccolo,BernardoMagnini,IdoDagan,andWilliamBDolan.2007. ThethirdPASCAL\nrecognizingtextualentailmentchallenge. InACL-PASCALWorkshoponTextualEntailmentand\nParaphrasing,pages1–9.\nKurt Gödel. 1930. Uber die vollständigkeit des logikkalküls. Ph.D. thesis, Ph. D. dissertation,\nUniversityofVienna.\nNicolasGontier,KoustuvSinha,SivaReddy,andChrisPal.2020. Measuringsystematicgeneraliza-\ntioninneuralproofgenerationwithtransformers. AdvancesinNeuralInformationProcessing\nSystems,33:22231–22242.\nNelsonGoodman.1954. Fact,fiction,andforecast.london: Universityoflondon.\nRadu Cornel Guias¸u and Christopher W Tindale. 2018. Logical fallacies and invasion biology.\nBiology&philosophy,33(5-6):34.\nIvanHabernal,HenningWachsmuth,IrynaGurevych,andBennoStein.2018.Theargumentreasoning\ncomprehension task: Identification and reconstruction of implicit warrants. In Proceedings\nof the 2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: HumanLanguageTechnologies,Volume1(LongPapers),pages1930–1940,New\nOrleans,Louisiana.AssociationforComputationalLinguistics.\nPengruiHan,PeiyangSong,HaofeiYu,andJiaxuanYou.2024. In-contextlearningmaynotelicit\ntrustworthyreasoning: A-not-berrorsinpretrainedlanguagemodels.\nSimengHan,HaileySchoelkopf,YilunZhao,ZhentingQi,MartinRiddell,LukeBenson,LucySun,\nEkaterinaZubova,YujieQiao,MatthewBurtell,etal.2022. Folio: Naturallanguagereasoning\nwithfirst-orderlogic. arXive-prints,pagesarXiv–2209.\nSvenOveHansson.2004. Fallaciesofrisk. JournalofRiskResearch,7(3):353–360.\n13\nDanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,andJacob\nSteinhardt. 2021a. Measuring massive multitask language understanding. Proceedings of the\nInternationalConferenceonLearningRepresentations(ICLR).\nDanHendrycks,CollinBurns,SauravKadavath,AkulArora,StevenBasart,EricTang,DawnSong,\nandJacobSteinhardt.2021b. Measuringmathematicalproblemsolvingwiththemathdataset.\nNeurIPS.\nNamgyuHo,LauraSchmid,andSe-YoungYun.2023. Largelanguagemodelsarereasoningteachers.\nDamianHodelandJevinWest.2023. Response: Emergentanalogicalreasoninginlargelanguage\nmodels.\nRuixinHong,HongmingZhang,XinyuPang,DongYu,andChangshuiZhang.2024. Acloserlook\nattheself-verificationabilitiesoflargelanguagemodelsinlogicalreasoning. InProceedingsofthe\n2024ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:\nHumanLanguageTechnologies(Volume1: LongPapers),pages900–925,MexicoCity,Mexico.\nAssociationforComputationalLinguistics.\nPeng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, and Shujian Huang. 2024. Large language\nmodelsarelimitedinout-of-contextknowledgereasoning.\nJieHuang,XinyunChen,SwaroopMishra,HuaixiuStevenZheng,AdamsWeiYu,XinyingSong,\nandDennyZhou.2024. Largelanguagemodelscannotself-correctreasoningyet. InTheTwelfth\nInternationalConferenceonLearningRepresentations.\nDavid Hume. 1748. An enquiry concerning human understanding (section iv). Recuperado de\nhttp://www.clorenzano.com.ar.\nBowenJiang,YangxinyuXie,ZhuoqunHao,XiaomengWang,TanwiMallick,WeijieJ.Su,CamilloJ.\nTaylor,andDanRoth.2024a. Apeekintotokenbias: Largelanguagemodelsarenotyetgenuine\nreasoners.\nJinJiang,YuchenYan,YangLiu,YonggangJin,ShuaiPeng,MengdiZhang,XunliangCai,Yixin\nCao,LiangcaiGao,andZhiTang.2024b. Logicpro: Improvingcomplexlogicalreasoningvia\nprogram-guidedlearning.\nDanielKahneman.2011. Thinking,fastandslow. Macmillan.\nTameraLanham,AnnaChen,AnshRadhakrishnan,BenoitSteiner,CarsonDenison,DannyHer-\nnandez, DustinLi, EsinDurmus, EvanHubinger, JacksonKernion, Kamile˙ Lukošiu¯te˙, Karina\nNguyen,NewtonCheng,NicholasJoseph,NicholasSchiefer,OliverRausch,RobinLarson,Sam\nMcCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy\nMaxwell,TimothyTelleen-Lawton,TristanHume,ZacHatfield-Dodds,JaredKaplan,JanBrauner,\nSamuelR.Bowman,andEthanPerez.2023. Measuringfaithfulnessinchain-of-thoughtreasoning.\nLiunianHaroldLi,JackHessel,YoungjaeYu,XiangRen,Kai-WeiChang,andYejinChoi.2023.\nSymbolicchain-of-thoughtdistillation:Smallmodelscanalso“think”step-by-step.InProceedings\nofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long\nPapers),pages2665–2679,Toronto,Canada.AssociationforComputationalLinguistics.\nShiyangLi,JianshuChen,YelongShen,ZhiyuChen,XinluZhang,ZekunLi,HongWang,JingQian,\nBaolinPeng,YiMao,WenhuChen,andXifengYan.2022. Explanationsfromlargelanguage\nmodelsmakesmallreasonersbetter.\nHanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang.\n2023a. Logiqa2.0—animproveddatasetforlogicalreasoninginnaturallanguageunderstanding.\nIEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,31:2947–2962.\nHanmengLiu,RuoxiNing,ZhiyangTeng,JianLiu,QijiZhou,andYueZhang.2023b. Evaluating\nthelogicalreasoningabilityofchatgptandgpt-4.\nHanmengLiu,ZhiyangTeng,LeyangCui,ChaoliZhang,QijiZhou,andYueZhang.2023c.LogiCoT:\nLogicalchain-of-thoughtinstructiontuning. InFindingsoftheAssociationforComputationalLin-\nguistics: EMNLP2023,pages2908–2921,Singapore.AssociationforComputationalLinguistics.\n14\nJianLiu,LeyangCui,HanmengLiu,DandanHuang,YileWang,andYueZhang.2020. Logiqa:\nAchallengedatasetformachinereadingcomprehensionwithlogicalreasoning. InProceedings\nof the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages\n3622–3628.InternationalJointConferencesonArtificialIntelligenceOrganization. Maintrack.\nJiaweiLiu,ChunqiuStevenXia,YuyaoWang,andLingmingZhang.2023d. Isyourcodegenerated\nbychatGPTreallycorrect? rigorousevaluationoflargelanguagemodelsforcodegeneration. In\nThirty-seventhConferenceonNeuralInformationProcessingSystems.\nYinhanLiu, MyleOtt, Naman Goyal, JingfeiDu, MandarJoshi, DanqiChen, OmerLevy, Mike\nLewis,LukeZettlemoyer,andVeselinStoyanov.2019. RoBERTa: ArobustlyoptimizedBERT\npretrainingapproach. arXivpreprintarXiv:1907.11692.\nZiyiLiu, IsabelleLee, YongkangDu, SoumyaSanyal, andJieyuZhao.2024. Self-contradictory\nreasoningevaluationanddetection.\nZimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and\nHongshengLi.2024. Mathgenie: Generatingsyntheticdatawithquestionback-translationfor\nenhancingmathematicalreasoningofllms.\nYINGWEIMA,YueLiu,YueYu,YuanliangZhang,YuJiang,ChangjianWang,andShanshanLi.\n2024. AtwhichtrainingstagedoescodedatahelpLLMsreasoning? InTheTwelfthInternational\nConferenceonLearningRepresentations.\nLucieCharlotteMagister,JonathanMallinson,JakubAdamek,EricMalmi,andAliakseiSeveryn.\n2023. Teachingsmalllanguagemodelstoreason. InProceedingsofthe61stAnnualMeetingofthe\nAssociationforComputationalLinguistics(Volume2: ShortPapers),pages1773–1781,Toronto,\nCanada.AssociationforComputationalLinguistics.\nJohnW.McCarthy.1959.Programswithcommonsense.InProc.TeddingConf.ontheMechanization\nofThoughtProcesses,pages75–91.\nTodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal.2018. Canasuitofarmorconduct\nelectricity? anewdatasetforopenbookquestionanswering. InEMNLP.\nGeorge A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM,\n38(11):39–41.\nImanMirzadeh,KeivanAlizadeh,HoomanShahrokhi,OncelTuzel,SamyBengio,andMehrdad\nFarajtabar.2024. Gsm-symbolic: Understandingthelimitationsofmathematicalreasoninginlarge\nlanguagemodels.\nMelanie Mitchell. 2023. Can large language models reason? blog, pages\nhttps://aiguide.substack.com/p/can–large–language–models–reason.\nArindamMitra,LucianoDelCorro,ShwetiMahajan,AndresCodas,ClarisseSimoes,SahajAgarwal,\nXuxiChen,AnastasiaRazdaibiedina,ErikJones,KritiAggarwal,HamidPalangi,GuoqingZheng,\nCorbyRosset,HamedKhanpour,andAhmedAwadallah.2023. Orca2: Teachingsmalllanguage\nmodelshowtoreason.\nPhilippMondorfandBarbaraPlank.2024. Liar,liar,logicalmire: Abenchmarkforsuppositional\nreasoninginlargelanguagemodels.\nTerufumiMorishita,GakuMorio,AtsukiYamaguchi,andYasuhiroSogawa.2023.Learningdeductive\nreasoningfromsyntheticcorpusbasedonformallogic. InProceedingsofthe40thInternational\nConferenceonMachineLearning,volume202ofProceedingsofMachineLearningResearch,\npages25254–25274.PMLR.\nTerufumiMorishita,AtsukiYamaguchi,GakuMorio,HikaruTomonari,OsamuImaichi,andYasuhiro\nSogawa.2024. JFLD:AJapanesebenchmarkfordeductivereasoningbasedonformallogic. In\nProceedingsofthe2024JointInternationalConferenceonComputationalLinguistics,Language\nResourcesandEvaluation(LREC-COLING2024),pages9526–9535,Torino,Italia.ELRAand\nICCL.\n15\nAliakbarNafar,K.BrentVenable,andParisaKordjamshidi.2024. Teachingprobabilisticlogical\nreasoningtotransformers. InFindingsoftheAssociationforComputationalLinguistics: EACL\n2024,pages1615–1632,St.Julian’s,Malta.AssociationforComputationalLinguistics.\nKentaroOzeki,RisakoAndo,TakanobuMorishita,HirohikoAbe,KojiMineshima,andMitsuhiro\nOkada.2024. Exploringreasoningbiasesinlargelanguagemodelsthroughsyllogism: Insights\nfromtheNeuBAROCOdataset. InFindingsoftheAssociationforComputationalLinguisticsACL\n2024,pages16063–16077,Bangkok,Thailandandvirtualmeeting.AssociationforComputational\nLinguistics.\nFabio Paglieri. 2017. A plea for ecological argument technologies. Philosophy & Technology,\n30(2):209–238.\nMihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty,\nArindamMitra,andChittaBaral.2024. LogicBench: Towardssystematicevaluationoflogical\nreasoningabilityoflargelanguagemodels. InProceedingsofthe62ndAnnualMeetingofthe\nAssociationforComputationalLinguistics(Volume1:LongPapers),pages13679–13707,Bangkok,\nThailand.AssociationforComputationalLinguistics.\nNisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj\nVarshney,andChittaBaral.2024. Multi-logieval: Towardsevaluatingmulti-steplogicalreasoning\nabilityoflargelanguagemodels.\nXinyuPi,QianLiu,BeiChen,MortezaZiyadi,ZeqiLin,QiangFu,YanGao,Jian-GuangLou,and\nWeizhuChen.2022. Reasoninglikeprogramexecutors. InProceedingsofthe2022Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessing,pages761–779,AbuDhabi,UnitedArab\nEmirates.AssociationforComputationalLinguistics.\nWillardVanOrmanQuine.1969. Epistemologynaturalized.ontologicalrelativityandotheressays.\nNewYork: ColumbiaUP.\nAlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever.2019.Language\nmodelsareunsupervisedmultitasklearners.\nJackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,John\nAslanides,SarahHenderson,RomanRing,SusannahYoung,etal.2021. Scalinglanguagemodels:\nMethods,analysis&insightsfromtraininggopher. arXivpreprintarXiv:2112.11446.\nPranavRajpurkar,RobinJia,andPercyLiang.2018. Knowwhatyoudon’tknow: Unanswerable\nquestionsforSQuAD. InProceedingsofthe56thAnnualMeetingoftheAssociationforComputa-\ntionalLinguistics(Volume2: ShortPapers),pages784–789,Melbourne,Australia.Association\nforComputationalLinguistics.\nYasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pre-\ntrainingtermfrequenciesonfew-shotnumericalreasoning. InFindingsoftheAssociationfor\nComputationalLinguistics: EMNLP2022,pages840–854.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani,JulianMichael,andSamuelRBowman.2023. Gpqa: Agraduate-levelgoogle-proofq&a\nbenchmark. arXivpreprintarXiv:2311.12022.\nJoshuaRobinsonandDavidWingate.2023. Leveraginglargelanguagemodelsformultiplechoice\nquestionanswering. InTheEleventhInternationalConferenceonLearningRepresentations.\nMohammedSaeed,NaserAhmadi,PreslavNakov,andPaoloPapotti.2021. RuleBERT:Teaching\nsoftrulestopre-trainedlanguagemodels. InProceedingsofthe2021ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing,pages1460–1476,OnlineandPuntaCana,Dominican\nRepublic.AssociationforComputationalLinguistics.\nSwarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. 2020. PRover: Proof\ngeneration for interpretable reasoning over rules. In Proceedings of the 2020 Conference on\nEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages122–136,Online.Association\nforComputationalLinguistics.\n16\nKeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi.2021. Winogrande: An\nadversarialwinogradschemachallengeatscale. CommunicationsoftheACM,64(9):99–106.\nSoumya Sanyal, Zeyi Liao, and Xiang Ren. 2022a. Robustlr: Evaluating robustness to logical\nperturbationindeductivereasoning. arXivpreprintarXiv:2205.12598.\nSoumya Sanyal, Harman Singh, and Xiang Ren. 2022b. Fairr: Faithful and robust deductive\nreasoningovernaturallanguage. InProceedingsofthe60thAnnualMeetingoftheAssociationfor\nComputationalLinguistics(Volume1: LongPapers),pages1075–1093.\nehShortliffe.1976. Computerbasedmedicalconsultations: Mycin. Elsevier.\nKumarShridhar,AlessandroStolfo,andMrinmayaSachan.2023. Distillingreasoningcapabilities\nintosmallerlanguagemodels. InFindingsoftheAssociationforComputationalLinguistics: ACL\n2023,pages7059–7073,Toronto,Canada.AssociationforComputationalLinguistics.\nDamienSileo.2024. Scalingsyntheticlogicalreasoningdatasetswithcontext-sensitivedeclarative\ngrammars.\nZayne Rea Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. 2024. MuSR:\nTestingthelimitsofchain-of-thoughtwithmultistepsoftreasoning. InTheTwelfthInternational\nConferenceonLearningRepresentations.\nCassRSunsteinandReidHastie.2015. Wiser: gettingbeyondgroupthinktomakegroupssmarter.\nHarvardBusinessReviewPress,Boston.\nMiracSuzgun,NathanScales,NathanaelSchärli,SebastianGehrmann,YiTay,HyungWonChung,\nAakankshaChowdhery,QuocVLe,EdHChi,DennyZhou,,andJasonWei.2022. Challenging\nbig-benchtasksandwhetherchain-of-thoughtcansolvethem. arXivpreprintarXiv:2210.09261.\nOyvindTafjord,BhavanaDalvi,andPeterClark.2021. ProofWriter: Generatingimplications,proofs,\nand abductive statements over natural language. In Findings of the Association for Computa-\ntionalLinguistics: ACL-IJCNLP2021,pages3621–3634,Online.AssociationforComputational\nLinguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Common-\nsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint\narXiv:1811.00937.\nJidongTian,YitianLi,WenqingChen,LiqiangXiao,HaoHe,andYaohuiJin.2021. Diagnosingthe\nfirst-orderlogicalreasoningabilitythroughlogicnli. InProceedingsofthe2021Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessing,pages3738–3747.\nTrieuHTrinh,YuhuaiWu,QuocVLe,HeHe,andThangLuong.2024. Solvingolympiadgeometry\nwithouthumandemonstrations. Nature,625(7995):476–482.\nMilesTurpin,JulianMichael,EthanPerez,andSamuelR.Bowman.2023. Languagemodelsdon’t\nalwayssaywhattheythink: Unfaithfulexplanationsinchain-of-thoughtprompting.\nFumiyaUchiyama,TakeshiKojima,AndrewGambardella,QiCao,YusukeIwasawa,andYutaka\nMatsuo. 2024. Which programming language and what features at pre-training stage affect\ndownstreamlogicalinferenceperformance?\nYuxuanWan,WenxuanWang,YiliuYang,YouliangYuan,JentseHuang,PinjiaHe,WenxiangJiao,\nandMichaelR.Lyu.2024. Logicasker: Evaluatingandimprovingthelogicalreasoningabilityof\nlargelanguagemodels.\nHaochunWang,SendongZhao,ZewenQiang,BingQin,andTingLiu.2024a. Beyondtheanswers:\nReviewingtherationalityofmultiplechoicequestionansweringfortheevaluationoflargelanguage\nmodels.\nPeifengWang,ZhengyangWang,ZhengLi,YifanGao,BingYin,andXiangRen.2023. SCOTT:\nSelf-consistentchain-of-thoughtdistillation. InProceedingsofthe61stAnnualMeetingofthe\nAssociationforComputationalLinguistics(Volume1: LongPapers),pages5546–5558,Toronto,\nCanada.AssociationforComputationalLinguistics.\n17\nSiyuanWang,ZhongyuWei,YejinChoi,andXiangRen.2024b. CanLLMsreasonwithrules? logic\nscaffoldingforstress-testingandimprovingLLMs. InProceedingsofthe62ndAnnualMeeting\noftheAssociationforComputationalLinguistics(Volume1: LongPapers), pages7523–7543,\nBangkok,Thailand.AssociationforComputationalLinguistics.\nYuboWang,XueguangMa,GeZhang,YuanshengNi,AbhranilChandra,ShiguangGuo,Weiming\nRen,AaranArulraj,XuanHe,ZiyanJiang,TianleLi,MaxKu,KaiWang,AlexZhuang,Rongqi\nFan,XiangYue,andWenhuChen.2024c. Mmlu-pro: Amorerobustandchallengingmulti-task\nlanguageunderstandingbenchmark(publishedatneurips2024trackdatasetsandbenchmarks).\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\nQuocVLe,andDennyZhou.2022. Chainofthoughtpromptingelicitsreasoninginlargelanguage\nmodels. InAdvancesinNeuralInformationProcessingSystems.\nJosephWeizenbaum.1966. Eliza—acomputerprogramforthestudyofnaturallanguagecommuni-\ncationbetweenmanandmachine. CommunicationsoftheACM,9(1):36–45.\nJohannesWelbl,NelsonF.Liu,andMattGardner.2017. Crowdsourcingmultiplechoicescience\nquestions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 94–106,\nCopenhagen,Denmark.AssociationforComputationalLinguistics.\nJasonWeston,AntoineBordes,SumitChopra,AlexanderMRush,BartVanMerriënboer,Armand\nJoulin,andTomasMikolov.2015. Towardsai-completequestionanswering: Asetofprerequisite\ntoytasks. arXivpreprintarXiv:1502.05698.\nAdinaWilliams,NikitaNangia,andSamuelRBowman.2018. Abroad-coveragechallengecorpus\nforsentenceunderstandingthroughinference. InProceedingsofNAACL-HLT,pages1112–1122.\nTWinograd.1971. Proceduresasarepresentationfordatainacomputerprogramforunderstanding\nnaturallanguage,mitaitechnicalreport235.\nLudwigWittgenstein.1922. TractatusLogicoPhilosophicus: Logical-PhilosophicalTreatise. Really\nSimpleMedia.\nThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvonPlaten,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,SylvainGugger,\nMariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art\nnatural language processing. In Empirical Methods in Natural Language Processing: System\nDemonstrations,pages38–45.\nZhaofengWu,LinluQiu,AlexisRoss,EkinAkyürek,BoyuanChen,BailinWang,NajoungKim,\nJacob Andreas, and Yoon Kim. 2023. Reasoning or reciting? exploring the capabilities and\nlimitationsoflanguagemodelsthroughcounterfactualtasks.\nHitomiYanaka,KojiMineshima,DaisukeBekki,KentaroInui,SatoshiSekine,LashaAbzianidze,and\nJohanBos.2019. Help: Adatasetforidentifyingshortcomingsofneuralmodelsinmonotonicity\nreasoning. arXivpreprintarXiv:1904.12166.\nNathanYoung,QimingBao,JoshuaBensemann,andMichaelJWitbrock.2022. Abductionrules:\nTrainingtransformerstoexplainunexpectedinputs. InFindingsoftheAssociationforComputa-\ntionalLinguistics: ACL2022,pages218–227.\nWeihaoYu,ZihangJiang,YanfeiDong,andJiashiFeng.2020. Reclor: Areadingcomprehension\ndataset requiring logical reasoning. In International Conference on Learning Representations\n(ICLR).\nZhangdieYuan,SongboHu,IvanVulic´,AnnaKorhonen,andZaiqiaoMeng.2023. Canpretrained\nlanguagemodels(yet)reasondeductively? InProceedingsofthe17thConferenceoftheEuropean\nChapteroftheAssociationforComputationalLinguistics,pages1439–1454.\nRowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi.2019. Hellaswag: Cana\nmachinereallyfinishyoursentence? InProceedingsofthe57thAnnualMeetingoftheAssociation\nforComputationalLinguistics,pages4791–4800.\n18\nHonghuaZhang,LiunianHaroldLi,TaoMeng,Kai-WeiChang,andGuyVandenBroeck.2022. On\ntheparadoxoflearningtoreasonfromdata.\nHughZhang,JeffDa,DeanLee,VaughnRobinson,CatherineWu,WillSong,TiffanyZhao,Pranav\nRaja,DylanSlack,QinLyu,SeanHendryx,RussellKaplan,MicheleLunati,andSummerYue.\n2024. Acarefulexaminationoflargelanguagemodelperformanceongradeschoolarithmetic.\nJunZhao,JingqiTong,YurongMou,MingZhang,QiZhang,andXuanjingHuang.2024a. Exploring\nthecompositionaldeficiencyoflargelanguagemodelsinmathematicalreasoning.\nWentingZhao,JustinChiu,JenaHwang,FaezeBrahman,JackHessel,SanjibanChoudhury,Yejin\nChoi,XiangLi,andAlaneSuhr.2024b. UNcommonsensereasoning: Abductivereasoningabout\nuncommonsituations. InProceedingsofthe2024ConferenceoftheNorthAmericanChapterof\ntheAssociationforComputationalLinguistics: HumanLanguageTechnologies(Volume1: Long\nPapers),pages8487–8505,MexicoCity,Mexico.AssociationforComputationalLinguistics.\nChujieZheng,HaoZhou,FandongMeng,JieZhou,andMinlieHuang.2024. Largelanguagemodels\narenotrobustmultiplechoiceselectors. InTheTwelfthInternationalConferenceonLearning\nRepresentations.\nWanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming\nZhou,andNanDuan.2021. Ar-lsat: Investigatinganalyticalreasoningoftext. arXivpreprint\narXiv:2104.06598.\nJinPengZhou,CharlesEStaats,WendaLi,ChristianSzegedy,KilianQWeinberger,andYuhuaiWu.\n2024a. Don’ttrust: Verify–groundingLLMquantitativereasoningwithautoformalization. InThe\nTwelfthInternationalConferenceonLearningRepresentations.\nYueZhou,YadaZhu,DiegoAntognini,YoonKim,andYangZhang.2024b. Paraphraseandsolve:\nExploringandexploitingtheimpactofsurfaceformonmathematicalreasoninginlargelanguage\nmodels. InProceedingsofthe2024ConferenceoftheNorthAmericanChapteroftheAssociation\nforComputationalLinguistics: HumanLanguageTechnologies(Volume1: LongPapers),pages\n2793–2804,MexicoCity,Mexico.AssociationforComputationalLinguistics.\nKaijieZhu,JiaaoChen,JindongWang,NeilZhenqiangGong,DiyiYang,andXingXie.2024. Dyval:\nDynamicevaluationoflargelanguagemodelsforreasoningtasks. InTheTwelfthInternational\nConferenceonLearningRepresentations.\n19\nA RelatedWork\nA.1 InvestigationofReasoningCapabilitiesofLLMs\nManystudiesexamineLLMs’reasoningcapabilities(Askell,2020;Raeetal.,2021;Razeghietal.,\n2022;Liuetal.,2023b;Turpinetal.,2023;Lanhametal.,2023;Wuetal.,2023;HodelandWest,\n2023;Dzirietal.,2023;Dasguptaetal.,2023). Pateletal.(2024)observedLLMs’performance\nsignificantly declines as reasoning steps increase in multi-step logical reasoning tasks. Dougrez-\nLewisetal.(2024)revealedChatGPTstruggleswithabductivereasoningwhenverifyingclaimsby\ndecomposingtheirevidenceintoatomicreasoningsteps. Wangetal.(2024b)foundthatGPT-series\nmodelsshowedsignificantgapscomparedtohumansindealingwithinferencerules. Parmaretal.\n(2024)introducedLogicBenchandshowedthatexistingLLMsstrugglewithinstancesinvolving\ncomplexreasoningandnegations. Wanetal.(2024)introducedLogicAsker,whichassesseswhether\nLLMscanemployasetofatomicreasoningskillsgroundedinpropositionalandpredicatelogicand\nfoundsignificantgapsinLLMs’learningoflogicalrules. Bhuiyaetal.(2024)proposedachallenging\nmulti-hopreasoningbenchmarkwithseeminglyplausiblebutincorrectmulti-hopreasoningchains\nandfoundthatstate-of-the-artLLMs’capabilitiestoperformmulti-hopreasoningisaffectedbysuch\nchains. MondorfandPlank(2024)introducedTruthQuest,whichassessesLLMs’capabilitiesto\nconductsuppositionalreasoning,i.e.,reasoningwhereeachstatementcanbefalse,andfoundthat\nLLMsexhibitsignificantdifficultiessolvingthesetasks. Spragueetal.(2024)introducedacomplex\nmulti-stepreasoningbenchmark,MuSR,andcharacterizedthegapsthatremainfortechniqueslike\nchain-of-thoughttoperformrobustreasoning.\nBiasesandErrors Andoetal.(2023);Ozekietal.(2024);Bertolazzietal.(2024);Eisapeetal.\n(2024)foundthatLLMsexhibithuman-likereasoningbiasesinsyllogisticarguments. Jiangetal.\n(2024a)foundthatLLMsexibit“token-biases”insolvinglogicalreasoningproblems. Aokietal.\n(2024)revealedthatLMsrelyheavilyonheuristics,suchaslexicaloverlap,intheearlierstagesof\nreasoning. Zhaoetal.(2024a)constructedaMATHTRAPwithcarefullydesignedlogicaltrapsinto\ntheproblemdescriptionsofMATHandGSM8kandfoundthatwhileLLMspossesstheknowledge\nrequired to solve these traps, they do not spontaneously use such knowledge them to handle the\nproblems. Hanetal.(2024)foundthatLLMsexhibitA-Not-Berrorssimilartohumaninfants,failing\ntosuppressthepreviouslyestablishedresponsepatternduringICL.Liuetal.(2024)foundthatLLMs\noftencontradictthemselvesinreasoningtasksinvolvingcontextualinformationunderstandingor\ncommonsense. Zhouetal.(2024b)foundthatsubtlealterationsinthesurfaceformcansignificantly\nimpacttheanswerdistribution,suggestingthatLLMssolvereasoningproblemsusingsurfacecues.\nChenetal.(2024)foundthatthereasoningperformanceofLLMsisaffectedbytheorderofthe\npremises. Hongetal.(2024);Huangetal.(2024)foundthatLLMsstruggletoidentifyfallacious\nreasoningstepsaccurately,suggestingchallengesinself-verificationmethods.\nReasoninginUnknownSituation Zhaoetal.(2024b)foundthatLLMsstrugglewithreasoningin\nuncommonsituations. Zhuetal.(2024)introducedaframeworktodynamicallygeneratereasoning\nsamples,andLLMsperformworseinthosesamples. Huetal.(2024)foundthatwhileLLMscan\nconductreasoningwhenrelevantknowledgeisgivenincontext,theyarenotproficientatreasoning\nwithknowledgeembeddedinthetrainingdata.\nA.2 SyntheticLogicCorpusforTrainingLLMs\nLaterstudies(Sahaetal.,2020;Dalvietal.,2021;Tafjordetal.,2021;Sanyaletal.,2022b)showed\nthatT5cangenerateeventheintermediatelogicalstepsaswellasthefinalanswer.\nPARARULE-Plus(Baoetal.,2022)istheenhancedversionofPARARULE(Clarketal.,2021),a\nvariationofRuleTaker,thatincludesmoresamplesandmorelogicalsteps. RoBERTa(Liuetal.,\n2019)trainedonPARARULE-PlusoutperformedthemodelstrainedonRuleTaker.\nArtificial Argument Corpus (Betz et al., 2021) includes single-step deductive reasoning samples\nconstructedfromhand-selecteddeductionrulesusefulforcriticalthinking. Theyshowedthatthe\nGPT-2(Radfordetal.,2019)trainedonthiscorpuscangeneralizetosolveNLItasks. However,at\nthesametime,theyfoundthattheLMdoesnotgeneralizewelltosolvemorechallengingreasoning\ntaskssuchasARC(Habernaletal.,2018)andLogiQA(Liuetal.,2020).\n20\nFLDbyMorishitaetal.(2023,2024)isthefirstsyntheticlogiccorpusbasedonformallogictheory.\nItincludesmultistepdeductivereasoningsamplesconstructedfromtheaxiomsoffirst-orderpredicate\nlogic,whichcanexpressanydeductionruleduetothecompletenesstheorem. Duetothisnature,T5\ntrainedonFLDgeneralizesmosteffectivelytoothersyntheticlogiccorpora,comparedtomodels\ntrainedonothercorpora.\nGontieretal.(2020)investigatedthedeductivereasoningcapabilitiesofLMsonacorpuscomposed\nofaspecifictypeofmultistepinference,kinshiprelationshipsonsynthetickinshipgraphs. They\nfoundthatLMscansolvethistaskwhentherearerelativelyfewproofsteps,butitisdifficultfor\nthemtogeneralizetosolveproofstepslongerthanthoseshownintrainingdata. Bostrometal.(2021)\nstudiedhowtocreaterealisticnaturallanguageexpressionsthatrepresentdeductionrules. Tothis\nend,theyscrapedsentencesfromWikipediausingatemplate-basedmethodandparaphrasedthem.\nTheyshowedthattrainingonthiscorpushelpssolvereal-worlddeductivereasoningproblemssuch\nasEntailmentBank(Dalvietal.,2021). Pietal.(2022)usedsyntheticdatafromprogramexecutors,\nmostnotablySQLprograms. Theyverifiedthatthisdatacanenhancenumericalreasoning,logical\nreasoning,andmulti-hopreasoningabilities. Trinhetal.(2024)generated100milliongeometry\nproblemsandverifiedthatthecapabilityofartificialintelligencecanbeenhancedtotopassthebronze\nmedalthresholdoftheInternationalMathematicsOlympiad. Saeedetal.(2021);Nafaretal.(2024)\ncreatedsoftreasoningrulesinvolvingwithprobabilisticlogic,insteadofhard-logicexaminedbythe\naformentionedstudies. Sileo(2024)introducedasimplerandmoregeneraldeclarativeframework\nforsyntheticgeneration,andverifieditseffectiveness. Zhouetal.(2024a)syntheticallygenerateda\nlargedatasetofmathematics,andgainedover12pointsonGSM8k.\nWhilethesestudiespartlyexaminedtheeffectofsyntheticlogiccorpora,whetherthisapproachis\npromisingremainsanopenquestion. Ithasbeenunexploredwhetherthecapabilitiesobtainedfrom\nsyntheticlogiccorporageneralizestosolvevarioustasksbeyondtheoriginaltasksinthesecorpora.\nAdditionally, theeffectofthesecorporahasonlybeenexaminedforsmallLMstrainedonsmall\npre-trainingcorporasuchasT5andRoBERTa; ithasbeenhighlyquestionablewhethertheycan\nstillbenefitstate-of-the-artLLMstrainedonahugepre-trainingcorpus. Furthermore,eveniftheir\nbenefitswereverified,itremainsunclearwhichdesignofsyntheticlogicsamplesyieldsthelargest\nbenefitsduetothelackofsystematicdiscussionsonsampledesignsandempiricalverificationof\nthesedesigns. Weaimedtoanswerthesequestionsinthispaperanddemonstratethepotentialof\nsyntheticlogiccorpora.\nA.3 DistillingReasoningTracesfromVeryLargeLLMs\nRecent approaches (Ho et al., 2023; Magister et al., 2023; Li et al., 2022, 2023; Shridhar et al.,\n2023;Wangetal.,2023;Mitraetal.,2023;Liuetal.,2023c;BenAllaletal.,2024;Luetal.,2024)\nutilize very large LLMs, such as GPT-4, to prepare synthetic reasoning datasets to train smaller\nLLMs. Atypicalprocedureisasfollows: (i)prepareexistingreasoningproblems,(ii)promptlarge\nLLMstogeneratereasoningtracestosolvetheseproblemsusingtechniquessuchaschain-of-thought\nprompting(Weietal.,2022),and(iii)trainsmallerLLMsonthesereasoningtraces.\nThe distillation approach and the synthetic logic corpora approach examined in this paper have\nspecificadvantagesanddisadvantages,asfollows.\nThe advantage of the distillation approach is its immediate practical effect, as it directly teaches\nLLMssolutionstovariousexistingproblems. Thedisadvantagescouldbethat(i)itisnon-trivialfor\nspecificsolutionstospecificproblemstogeneralizetootherproblems,(ii)thenumberoftraining\nsamplesislimitedtoexistingproblemsinnature,(iii)thecorrectnessandfaithfulnessofthereasoning\ntracesarenotguaranteed;indeed,somestudies(Turpinetal.,2023;Lanhametal.,2023)suggestthat\nlargeLLMsdonotalwaysfaithfullyfollowthe“reasoningtraces”theythemselvesgenerate,and(iv)\nitcannotenhancetheverylargeLLMsthemselvesbynature.\nTheadvantagesofsyntheticlogiccorpusapproachesarethat(i)sincetheyteachthefundamentalsof\nreasoning,suchasdeductivereasoning,theyhavethepotentialtogeneralizetovariousproblems,\n(ii)theycangenerateanunlimitednumberofnewsamples,and(iii)thecorrectnessofthereasoning\ntracesisguaranteedbynature. Thedisadvantageofthisapproachisthat,asitonlyteachesthebasics\nof reasoning, additional training may be needed to solve more complex real-world problems, as\nsuggestedinSection6.4.\n21\nWehypothesizethatintegratingbothapproachescouldbepromising. Thatis,wefirsttrainLLMs\nusingALTtomakethemunderstandthefundamentalsofreasoningthroughhigh-qualitysamplesand\nthentrainthemusingmorerealisticreasoningtracestosolvecomplexreal-worldproblems.\nB Limitations\n• We only used deductive reasoning samples for ALT. Future work should examine other\nreasoningsamples,e.g.,abductiveandinductivereasoning.\n• Weonlyexaminedthefirst-orderpredicatelogicsystem. Futureworkshouldexamineother\nlogicsystems,suchasmodalandlinearlogic.\nC EthicsandSocialImpacts\nTheultimategoalofthedirectionofthisstudyistodevelopanAIcapableofreasoninglogically\nstepbystep. IfAIcanmakeadecisiononelogicalstepatatime,itwouldbehighlyexplainableand\ntransparenttousers. Furthermore,theuserwouldbeabletotracetheAI’serrors. Webelievethatour\nstudyisasteptowardssuchAIthatwillpositivelyimpactsociety.\nD DetailsofFormalLogicDeductionDiverse\nFigureD.3showsarealsamplefromFLD×2. Below,Webrieflyexplainoursamplegenerator. Please\nrefertoMorishitaetal.(2023)forthedetails.\nD.1 AnswerLabels\nInadditiontothelogicalsteps,thesamplesofFLD×2andpreviouscorporaincludeanswerlabels\n(FigureD.3): “proved”indicatingthatthehypothesiscanbeprovedbythelogicalsteps,“disproved”\nindicatingthatthehypothesiscanbedisproved,and“unknown”indicatingthatthegivenfactsare\ninsufficientforeitherprovingordisprovingthehypothesis. Forsampleswith“unknown”labels,the\nlogicalstepsare“None.”. FLD×2haveauniformdistributionoverthelabels.\nD.2 Splits\nFLD×2includes100k/5k/5ksamplesfortrain/valid/testsplits.\nD.3 GenerationofMultistepDeduction\nOursamplegeneratorfirstrandomlygeneratesexamplesofmultistepdeductionbyforward-and\nbackwardrandomdeduction,usingthedeductionrulesspecifiedbyauser.\nThe forward random deduction is done as follows. The generator first chooses a deduction rule\nrandomlyandformstheinitialtreewheretherootnodeistheconclusionofthechosendeduction\nrulesandthechildnodesarethepremisesofthechosendeductionrule. Thegeneratornextrandomly\nchoosesanotherdeductionrulethatcanbe“jointed”totherootnoteofthetree. Adeductionrulecan\nbejointedtotherootnodeofatreeifoneofthepremisesofthatdeductionrulecanbeidentified\nwiththerootnode. Then,thegeneratorupdatesthetreebyjointingthischosendeductionrule. The\ngeneratorcontinuesthisstepmultipletimesuntilthetreeachievestherequireddepth.\nThebackwardrandomdeductionisdoneasfollows. Foreachstep,thegeneratorrandomlychoosesa\nleafnodeofthetree. Then,thegeneratorrandomlychoosesadeductionrulethatcanbejointedto\ntheleafnode. Here,adeductionrulecanbejointedtotheleafnodeifthedeductionrule’sconclusion\ncanbeidentifiedwiththeleafnode. Then, thegeneratorupdatesthetreebyjointingthischosen\ndeductionrule. Thegeneratorcontinuesthisstepmultipletimesuntilthecomplexityofbranches\nachievestherequiredlevel.\n22\nFigure D.3: A real deduction sample included in Formal Logic Deduction Diverse. Facts and\nhypothesisaregiventoLLMs,thentheLLMsarerequiredtogeneratelogicalstepsto(dis-)prove\nthehypothesisbasedonthefacts,andananswerlabel(seeAppendixD.2).\nD.4 LinguisticExpressions\nWepreparedlinguistictemplatesforeachlogicalformula,exemplifiedasfollows:\n⟨(A∧B)→C⟩:If⟨(A∧B).predicate_phrase⟩, then⟨C.predicate_phrase⟩.\n:⟨(A∧B).noun_pharse⟩⟨cause_synonyms⟩⟨C.noun_phrase⟩.\n:(...)\n⟨(A∧B).predicate_phrase⟩:A⟨occur_synonyms⟩andalsoB⟨occur_synonyms⟩.\n:AandalsoB⟨occur_synonyms⟩.\n:BothAandB⟨occur_synonyms⟩.\n:(...)\n⟨C.predicate_phrase⟩:C⟨occur_synonyms⟩.\n:(...)\n⟨occur_synonyms⟩:occur\n:happen\n:takeplace\n:(...)\n⟨(A∧B).noun_pharse⟩:AandB\n:AandalsoB\n:BothAandB\n:ThatAandB⟨occur_synonyms⟩\n:(...)\n⟨cause_synonyms⟩:cause\n:resultin\n:leadto\n:bringabout\n:(...)\n(...) (D.1)\nAs can be seen, the templates can be nested deeply, yielding combinatorially diverse linguistic\nexpressions.\nExpandingthesetemplatesbeforehandisintractableduetothecombinatorialexplosion,soweexpand\nthesetemplatesontheflytorandomlysampleasingleexpressionatatime. Estimatingtheexact\nnumberofexpressionsisintractableforthesamereason.\nWe manually crafted several additional English templates per logical formula (i.e., the left-hand\nsidesof(D.1))comparedtothoseusedinFLD,whichyieldcombinatoriallymorediverseEnglish\n23\nexpressions. Weobservedthatatleastdozensofexpressions,includingminorvariations,areyielded\nforeachformula.\nE DetailsofExperimentalSetup\nE.0.1 PreventionofKnowledgeForgettingbyRecallAdamOptimizer\nWeemployedtheRecallAdam(RecAdam)optimizer(Chenetal.,2020),whichregularizesparameter\nupdatestopreventthemfrombeingtoofarfromthepre-trainingparameters. RecallAdamstandsout\nforLLMtrainingasitdoesnotrequireaccesstothepre-trainingcorpus,whichisofteninaccessible\nortoohugetohandle,nordoesitrequirechangestothemodelarchitecture,andithasaproventrack\nrecordofusageinlanguagemodelssuchasBERT.\nE.1 Benchmarks\nTableE.7detailsthebenchmarksusedintheexperiments.\nE.2 ExperimentalRuns\nWeshowtheaverageandstandarddeviationsoverfiveseeds.\nE.3 ComputationalResources\nTheentireexperiment,includingpreliminaryones,tookabout1weekx128NVIDIAH100GPUsof\nourown.\nF ResultswithoutusingRecallAdam\nTableF.8showstheresultsofLLMstrainedwithoutusingRecallAdam.\n24\nTableE.7:31benchmarksusedintheexperiments.Thesebenchmarkscoverawiderangeoftasksand\nareprominentforLLMevaluation. Wealsoshowtheformofreasoningandthetypeofknowledge\nrequiredtosolvetheproblemsineachbenchmark.\nReasoning Required\nSet Benchmarks\nform knowledge\nbAbideduction(Westonetal.,2015),\nFOLIO(Hanetal.,2022) -\nLogicNLI(Tianetal.,2021) (notrequired)\nRobustLR(Sanyaletal.,2022a) deduction\nLogic\nAR-LSAT(Zhongetal.,2021)\nLogiQA2(Liuetal.,2023a) commonsense\nReClor(Yuetal.,2020)\nAbductionRules(Youngetal.,2022)\nabduction\nART(Bhagavatulaetal.,2019) commonsense\nHELP(Yanakaetal.,2019) validate\nNLI\nMultiNLI(Williamsetal.,2018) aconclusion\ncommonsense\nRTE(Daganetal.,2005;Giampiccoloetal.,2007;Bentivoglietal.,2009) basedon\nSNLI(Bowmanetal.) givenpremises\nGSM8k(Cobbeetal.,2021)\nMath MATH(Hendrycksetal.,2021b) Math Math\nMathQA(Aminietal.,2019)\nHumanEval(Chenetal.,2021)\nCoding\nMBPP(Austinetal.,2021)\nCoding Coding\nMBPP+(Liuetal.,2023d)\nMultiPL-E(cpp/go)(Cassanoetal.,2023)\nCommonsenseQA(Talmoretal.,2018)\nHellaSWAG(Zellersetal.,2019)\ncommonsense\nSQuAD2(Rajpurkaretal.,2018)\nOthers\nWinoGrande(Sakaguchietal.,2021) complicated\nprocedures\nARC(easy/challenge)(Clarketal.,2018)\nGPQA(Reinetal.,2023)\nscience\nOpenBookQA(Mihaylovetal.,2018)\nSciQ(Welbletal.,2017)\nMMLU(Hendrycksetal.,2021a)\naggregated MMLU-Pro(Wangetal.,2024c) various various\nBBH(Suzgunetal.,2022)\n25\nTableF.8:5-shotperformanceofLLMsbeforeandafterALT.⊕ALT-xdenotestheLLMtrainedwith\nALTonthesyntheticlogiccorpusxfromTable1. Colorshowstherankineachcolumn(darkeris\nbetter). “Logic”,“Math”,“Code”,and“Others”eachcomprisesvariousbenchmarks(seeTableE.7).\n“Avg.” representsthemicro-averageofallthebenchmarks. “w/oRecAdam”denotesthatLLMwas\ntrainedwithoutknowledgeforgettingpreventionbyRecallAdamoptimizer.\n(a)LLaMA-3.1-8B.\nAvg. Logic Math Code NLI Others BBH(3-shot) BBH(0-shot) MMLU\nCoT CoT Pro\nLLaMA-3.1-8B 47.9 42.8 39.6 35.4 65.4 60.7 44.9 61.9 8.2 36.5 65.3 35.8\n±0.4 ±0.5 ±0.3 ±0.3 ±0.4 ±0.4 ±0.2 ±0.4 ±0.4 ±0.4\n⊕ALT-PRPw/oRecAdam 43.5 39.5\n±0.2\n29.1\n±0.3\n35.3 57.8\n±0.2\n61.0\n±0.2\n40.5\n±0.2\n47.0\n±0.2\n3.9\n±0.1\n6.3\n±0.1\n64.9\n±0.2\n34.0\n±0.2\n⊕ALT-PRP 48.1 43.7 39.2 35.7 65.6 60.8 44.9 61.8 8.2 36.4 65.3 35.3\n±0.2 ±0.3 ±0.2 ±0.2 ±0.2 ±0.2 ±0.1 ±0.2 ±0.2 ±0.2\n⊕ALT-RT 50.1 46.8 42.4 36.5 68.6 61.3 46.9 63.5 13.7 38.4 65.3 35.7\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\n⊕ALT-FLD 51.9 51.6 43.4 38.1 70.1 61.5 46.7 64.9 11.9 39.6 65.4 36.2\n⊕ALT-FLD×2 52.0\n52.2±0.1 43.2±0.2\n38.0\n70.7±0.1 61.5±0.1 46.5±0.2 65.3±0.2 11.3±0.1 38.7±0.2 65.5±0.1 36.4±0.2\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\n(b)LLaMA-3.1-70B.\nAvg. Logic Math Code NLI Others BBH(3-shot) BBH(0-shot) MMLU\nCoT CoT Pro\nLLaMA-3.1-70B 60.0 57.4 60.0 46.2 73.7 67.7 60.4 82.1 6.5 50.1 78.7 50.7\n±0.4 ±0.5 ±0.3 ±0.3 ±0.3 ±0.2 ±0.1 ±0.3 ±0.3 ±0.4\n⊕ALT-PRPw/oRecAdam 58.8 54.3\n±0.4\n59.2\n±0.5\n48.2 72.7\n±0.3\n65.9\n±0.3\n60.4\n±0.4\n81.5\n±0.3\n6.1\n±0.2\n48.3\n±0.4\n78.5\n±0.3\n50.7\n±0.4\n⊕ALT-PRP 60.4 57.7 59.8 49.2 73.5 67.6 60.4 82.2 6.0 50.1 78.7 50.9\n±0.4 ±0.5 ±0.3 ±0.3 ±0.4 ±0.3 ±0.2 ±0.4 ±0.3 ±0.4\n⊕ALT-RT 62.7 61.4 62.1 50.8 75.4 68.4 64.1 82.5 11.5 59.2 79.0 52.4\n±0.2 ±0.3 ±0.2 ±0.2 ±0.3 ±0.2 ±0.2 ±0.3 ±0.2 ±0.3\n⊕ALT-FLD 64.2 65.7 63.6 52.0 75.3 68.5 65.0 83.6 12.1 59.9 79.3 54.4\n⊕ALT-FLD×2 64.4\n66.1±0.1 63.3±0.2\n52.4\n76.1±0.1 68.5±0.1 65.4±0.2 83.6±0.1 11.4±0.1 60.8±0.2 79.5±0.1 54.4±0.2\n±0.1 ±0.2 ±0.1 ±0.1 ±0.2 ±0.2 ±0.1 ±0.2 ±0.1 ±0.2\nTableF.9: Benchmark-wise5-shotperformanceofLLaMA-3.1-8BbeforeandafterALTonFLD×2.\n(a)Logic.\nbAbiD FOLIO LogicNLI RobustLR AR-LSAT LogiQA ReClor AbductionR ART\nLLaMA-3.1-8B 48.7 50.0 28.5 43.2 20.7 39.6 28.7 52.4 73.4\n±1.6 ±1.6 ±1.0 ±0.9 ±1.0 ±1.2 ±0.7 ±0.9 ±1.1\n⊕ALT-FLD×2 55.8 54.5 42.0 62.6 21.1 42.8 29.4 85.5 76.1\n±0.6 ±0.6 ±0.4 ±0.3 ±0.4 ±0.4 ±0.2 ±0.2 ±0.4\n(b)Math.\nGSM8k MATH MathQA\nCoT CoT(0-shot) - -\nLLaMA-3.1-8B 50.2 51.5 39.5 14.1 42.8\n±1.4 ±1.4 ±1.3 ±0.5 ±0.9\n⊕ALT-FLD×2 53.6 56.4 48.4 14.3 43.3\n±0.5 ±0.5 ±0.5 ±0.2 ±0.3\n(c)Coding.\nHumanEval MBPP MBPP+ MultiPL-E(cpp) MultiPL-E(go)\nLLaMA-3.1-8B 22.6 31.6 38.1 21.7 63.0\n⊕ALT-FLD×2 25.9 34.0 39.9 23.0 67.1\n(d)Naturallanguageinference(NLI).\nHELP MNLI RTE SNLI\nLLaMA-3.1-8B 46.4 68.1 74.6 72.6\n±0.5 ±0.5 ±0.9 ±0.4\n⊕ALT-FLD×2 47.9 75.3 83.1 76.5\n±0.2 ±0.2 ±0.3 ±0.1\n(e)Others.\nCommonsenseQA HellaSwag SQuAD WinoGrande ARCe ARCc GPQA OpenBookQA SciQ\nLLaMA-3.1-8B 73.9 61.2 30.8 77.4 84.2 54.7 31.1 35.3 97.7\n±1.3 ±0.5 ±0.0 ±1.2 ±0.7 ±1.5 ±1.3 ±0.7 ±0.5\n⊕ALT-FLD×2 74.8 61.5 33.5 78.1 85.0 55.6 31.1 36.3 97.6\n±0.4 ±0.2 ±0.0 ±0.5 ±0.3 ±0.5 ±0.5 ±0.2 ±0.2\n26\nNeurIPSPaperChecklist\nThe checklist is designed to encourage best practices for responsible machine learning research,\naddressingissuesofreproducibility,transparency,researchethics,andsocietalimpact.Donotremove\nthechecklist: Thepapersnotincludingthechecklistwillbedeskrejected. Thechecklistshould\nfollowthereferencesandprecedethe(optional)supplementalmaterial. ThechecklistdoesNOT\ncounttowardsthepagelimit.\nPleasereadthechecklistprinciplescarefullyforinformationonhowtoanswerthesequestions. For\neachquestioninthechecklist:\n• Youshouldanswer[Yes],[No],or[NA].\n• [NA] means either that the question is Not Applicable for that particular paper or the\nrelevantinformationisNotAvailable.\n• Pleaseprovideashort(1–2sentence)justificationrightafteryouranswer(evenforNA).\nThechecklistanswersareanintegralpartofyourpapersubmission. Theyarevisibletothe\nreviewers,areachairs,seniorareachairs,andethicsreviewers. Youwillbeaskedtoalsoincludeit\n(aftereventualrevisions)withthefinalversionofyourpaper,anditsfinalversionwillbepublished\nwiththepaper.\nThereviewersofyourpaperwillbeaskedtousethechecklistasoneofthefactorsintheirevaluation.\nWhile\"[Yes]\"isgenerallypreferableto\"[No]\",itisperfectlyacceptabletoanswer\"[No]\"provideda\nproperjustificationisgiven(e.g.,\"errorbarsarenotreportedbecauseitwouldbetoocomputationally\nexpensive\"or\"wewereunabletofindthelicenseforthedatasetweused\"). Ingeneral,answering\n\"[No]\"or\"[NA]\"isnotgroundsforrejection. Whilethequestionsarephrasedinabinaryway,we\nacknowledgethatthetrueanswerisoftenmorenuanced,sopleasejustuseyourbestjudgmentand\nwriteajustificationtoelaborate. Allsupportingevidencecanappeareitherinthemainpaperorthe\nsupplementalmaterial,providedinappendix. Ifyouanswer[Yes] toaquestion,inthejustification\npleasepointtothesection(s)whererelatedmaterialforthequestioncanbefound.\nIMPORTANT,please:\n• Deletethisinstructionblock,butkeepthesectionheading“NeurIPSpaperchecklist\",\n• Keepthechecklistsubsectionheadings,questions/answersandprinciplesbelow.\n• Donotmodifythequestionsandonlyusetheprovidedmacrosforyouranswers.\n1. Claims\nQuestion: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe\npaper’scontributionsandscope?\nAnswer: [Yes]\nJustification:ClaimsstatedinSection1issupportedbytheexperimentalresultsinSections5,\n6.\nprinciples:\n• The answer NA means that the abstract and introduction do not include the claims\nmadeinthepaper.\n• Theabstractand/orintroductionshouldclearlystatetheclaimsmade,includingthe\ncontributionsmadeinthepaperandimportantassumptionsandlimitations. ANoor\nNAanswertothisquestionwillnotbeperceivedwellbythereviewers.\n• Theclaimsmadeshouldmatchtheoreticalandexperimentalresults,andreflecthow\nmuchtheresultscanbeexpectedtogeneralizetoothersettings.\n• Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals\narenotattainedbythepaper.\n2. Limitations\nQuestion: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?\nAnswer: [Yes]\n27\nJustification: AppendixB\nprinciples:\n• TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat\nthepaperhaslimitations,butthosearenotdiscussedinthepaper.\n• Theauthorsareencouragedtocreateaseparate\"Limitations\"sectionintheirpaper.\n• Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto\nviolationsoftheseassumptions(e.g.,independenceassumptions,noiselesssettings,\nmodelwell-specification,asymptoticapproximationsonlyholdinglocally).Theauthors\nshouldreflectonhowtheseassumptionsmightbeviolatedinpracticeandwhatthe\nimplicationswouldbe.\n• Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwas\nonlytestedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoften\ndependonimplicitassumptions,whichshouldbearticulated.\n• Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.\nForexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution\nisloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbe\nusedreliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandle\ntechnicaljargon.\n• Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithms\nandhowtheyscalewithdatasetsize.\n• If applicable, the authors should discuss possible limitations of their approach to\naddressproblemsofprivacyandfairness.\n• Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby\nreviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover\nlimitationsthataren’tacknowledgedinthepaper. Theauthorsshouldusetheirbest\njudgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimpor-\ntantroleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewers\nwillbespecificallyinstructedtonotpenalizehonestyconcerninglimitations.\n3. TheoryAssumptionsandProofs\nQuestion: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand\nacomplete(andcorrect)proof?\nAnswer: [NA]\nJustification: Ourpaperdoesnotincludetheoreticalresults.\nprinciples:\n• TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.\n• Allthetheorems, formulas, andproofsinthepapershouldbenumberedandcross-\nreferenced.\n• Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.\n• Theproofscaneitherappearinthemainpaperorthesupplementalmaterial, butif\ntheyappearinthesupplementalmaterial,theauthorsareencouragedtoprovideashort\nproofsketchtoprovideintuition.\n• Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented\nbyformalproofsprovidedinappendixorsupplementalmaterial.\n• TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.\n4. ExperimentalResultReproducibility\nQuestion: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-\nperimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions\nofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?\nAnswer: [Yes]\nJustification: Section4,appendixE.Further,wereleasealltheresources,including(i)the\ncorpus,(ii)thetrainedmodel,and(iii)codeforcorpusgeneration,LLMtraining,andLLM\nevaluation3.\n3https://anonymous.4open.science/r/ALT/README.md\n28\nprinciples:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhetherthecodeanddataareprovidedornot.\n• Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken\ntomaketheirresultsreproducibleorverifiable.\n• Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.\nForexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully\nmightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay\nbenecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame\ndataset,orprovideaccesstothemodel. Ingeneral. releasingcodeanddataisoften\nonegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed\ninstructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase\nofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare\nappropriatetotheresearchperformed.\n• WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-\nsionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe\nnatureofthecontribution. Forexample\n(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow\ntoreproducethatalgorithm.\n(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe\nthearchitectureclearlyandfully.\n(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould\neitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce\nthemodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstruct\nthedataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthorsarewelcometodescribetheparticularwaytheyprovideforreproducibility.\nInthecaseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedin\nsomeway(e.g.,toregisteredusers),butitshouldbepossibleforotherresearchers\ntohavesomepathtoreproducingorverifyingtheresults.\n5. Openaccesstodataandcode\nQuestion: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-\ntionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental\nmaterial?\nAnswer: [Yes]\nJustification: wereleasethecode,data,andmodel.\nprinciples:\n• TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.\n• PleaseseetheNeurIPScodeanddatasubmissionprinciples(https://nips.cc/\npublic/guides/CodeSubmissionPolicy)formoredetails.\n• Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe\npossible,so“No”isanacceptableanswer. Paperscannotberejectedsimplyfornot\nincludingcode,unlessthisiscentraltothecontribution(e.g.,foranewopen-source\nbenchmark).\n• Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto\nreproducetheresults. SeetheNeurIPScodeanddatasubmissionprinciples(https:\n//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.\n• Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow\ntoaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.\n• Theauthorsshouldprovidescriptstoreproduceallexperimentalresultsforthenew\nproposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they\nshouldstatewhichonesareomittedfromthescriptandwhy.\n29\n• Atsubmissiontime, topreserveanonymity, theauthorsshouldreleaseanonymized\nversions(ifapplicable).\n• Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe\npaper)isrecommended,butincludingURLstodataandcodeispermitted.\n6. ExperimentalSetting/Details\nQuestion: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Section4,appendixE.\nprinciples:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail\nthatisnecessarytoappreciatetheresultsandmakesenseofthem.\n• Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental\nmaterial.\n7. ExperimentStatisticalSignificance\nQuestion:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate\ninformationaboutthestatisticalsignificanceoftheexperiments?\nAnswer: [Yes]\nJustification: AsstatedinAppendixE.\nprinciples:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n• Theauthorsshouldanswer\"Yes\"iftheresultsareaccompaniedbyerrorbars,confi-\ndenceintervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupport\nthemainclaimsofthepaper.\n• Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for\nexample,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall\nrunwithgivenexperimentalconditions).\n• Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,\ncalltoalibraryfunction,bootstrap,etc.)\n• Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).\n• Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderror\nofthemean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96 % CI, if the\nhypothesisofNormalityoferrorsisnotverified.\n• Forasymmetricdistributions,theauthorsshouldbecarefulnottoshowintablesor\nfiguressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative\nerrorrates).\n• Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow\ntheywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.\n8. ExperimentsComputeResources\nQuestion: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-\nputerresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce\ntheexperiments?\nAnswer: [Yes]\nJustification: AppendixE.3.\nprinciples:\n• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.\n30\n• ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,\norcloudprovider,includingrelevantmemoryandstorage.\n• Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual\nexperimentalrunsaswellasestimatethetotalcompute.\n• Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecompute\nthantheexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthat\ndidn’tmakeitintothepaper).\n9. CodeOfEthics\nQuestion: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe\nNeurIPSCodeofEthicshttps://neurips.cc/public/Ethicsprinciples?\nAnswer: [Yes]\nJustification:\nprinciples:\n• TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.\n• IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea\ndeviationfromtheCodeofEthics.\n• Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsid-\nerationduetolawsorregulationsintheirjurisdiction).\n10. BroaderImpacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietalimpactsoftheworkperformed?\nAnswer: [Yes]\nJustification: AppendixC\nprinciples:\n• TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.\n• IftheauthorsanswerNAorNo,theyshouldexplainwhytheirworkhasnosocietal\nimpactorwhythepaperdoesnotaddresssocietalimpact.\n• Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses\n(e.g.,disinformation,generatingfakeprofiles,surveillance),fairnessconsiderations\n(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific\ngroups),privacyconsiderations,andsecurityconsiderations.\n• Theconferenceexpectsthatmanypaperswillbefoundationalresearchandnottied\ntoparticularapplications,letalonedeployments. However,ifthereisadirectpathto\nanynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate\ntopointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto\ngeneratedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout\nthatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain\nmodelsthatgenerateDeepfakesfaster.\n• Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyis\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnologyisbeingusedasintendedbutgivesincorrectresults,andharmsfollowing\nfrom(intentionalorunintentional)misuseofthetechnology.\n• Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom\nfeedbackovertime,improvingtheefficiencyandaccessibilityofML).\n11. Safeguards\nQuestion: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible\nreleaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,\nimagegenerators,orscrapeddatasets)?\nAnswer: [No]\nJustification:\n31\nprinciples:\n• TheanswerNAmeansthatthepaperposesnosuchrisks.\n• Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith\nnecessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring\nthatusersadheretousageprinciplesorrestrictionstoaccessthemodelorimplementing\nsafetyfilters.\n• DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors\nshoulddescribehowtheyavoidedreleasingunsafeimages.\n• Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdo\nnotrequirethis,butweencourageauthorstotakethisintoaccountandmakeabest\nfaitheffort.\n12. Licensesforexistingassets\nQuestion: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin\nthepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand\nproperlyrespected?\nAnswer: [Yes]\nJustification: Allthecorporaandbenchmarksusedintheexperimentsproperlystatetheir\nlicenses.\nprinciples:\n• TheanswerNAmeansthatthepaperdoesnotuseexistingassets.\n• Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.\n• Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea\nURL.\n• Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.\n• Forscrapeddatafromaparticularsource(e.g.,website),thecopyrightandtermsof\nserviceofthatsourceshouldbeprovided.\n• Ifassetsarereleased,thelicense,copyrightinformation,andtermsofuseinthepackage\nshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasetshas\ncuratedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethelicense\nofadataset.\n• Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseof\nthederivedasset(ifithaschanged)shouldbeprovided.\n• Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachoutto\ntheasset’screators.\n13. NewAssets\nQuestion:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation\nprovidedalongsidetheassets?\nAnswer: [Yes]\nJustification: Wewillreleaseourcorpus.\nprinciples:\n• TheanswerNAmeansthatthepaperdoesnotreleasenewassets.\n• Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir\nsubmissions via structured templates. This includes details about training, license,\nlimitations,etc.\n• Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose\nassetisused.\n• Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither\ncreateananonymizedURLorincludeananonymizedzipfile.\n14. CrowdsourcingandResearchwithHumanSubjects\nQuestion: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper\nincludethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as\nwellasdetailsaboutcompensation(ifany)?\n32\nAnswer: [NA]\nJustification: Thispaperdoesnotinvolvecrowdsourcingnorresearchwithhumanobjects.\nprinciples:\n• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n• Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontribu-\ntionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe\nincludedinthemainpaper.\n• AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,\norotherlaborshouldbepaidatleasttheminimumwageinthecountryofthedata\ncollector.\n15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman\nSubjects\nQuestion: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether\nsuchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)\napprovals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor\ninstitution)wereobtained?\nAnswer: [NA]\nJustification: THispaperdoesnotinvolvecrowdsourcingnorresearchwithhumanobjects.\nprinciples:\n• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith\nhumansubjects.\n• Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)\nmayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you\nshouldclearlystatethisinthepaper.\n• Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions\nandlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe\nprinciplesfortheirinstitution.\n• Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if\napplicable),suchastheinstitutionconductingthereview.\n33",
    "pdf_filename": "Enhancing_Reasoning_Capabilities_of_LLMs_via_Principled_Synthetic_Logic_Corpus.pdf"
}