{
    "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
    "context": "Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning. To address this, we propose Additional Logic Training (ALT), which aims to enhance LLMs’ reasoning capabilities by program- generated logical reasoning samples. We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights. Then, based on these principles, we construct a synthetic corpus named Formal Logic Deduction Diverse (FLD×2), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. Finally, we empirically show that ALT on FLD×2 substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH. 1 Knowledge and reasoning have long been considered essential elements for achieving artificial intelligence (McCarthy, 1959; Weizenbaum, 1966; Winograd, 1971; Colmerauer and Roussel, 1973; Shortliffe, 1976; Elkan and Greiner, 1993). Knowledge refers to facts about the world, e.g., “objects with mass generate a gravitational field” and “the Earth has mass.” Reasoning involves combining multiple facts according to specific rules to obtain new knowledge. For example, the new knowledge that “the Earth generates a gravitational field” can be derived from the aforementioned two facts. Recent observations suggest that LLMs can solve problems using memorized knowledge of similar samples seen during pre-training, but they cannot solve novel, unknown problems that require reasoning (Hodel and West, 2023; Dasgupta et al., 2023; Zhang et al., 2024). For instance, LLMs can solve famous arithmetic problems as is but not when the numbers or names are changed (Razeghi et al., 2022; Mirzadeh et al., 2024), and they can solve coding tests from past years before the “knowledge cutoff” but not from the present year (Mitchell, 2023). This bias towards knowledge has been observed even in state-of-the-art LLMs such as GPT-4 (Liu et al., 2023b; Wu et al., 2023; Dziri et al., 2023). LLMs’ poor reasoning capabilities can stem from the lack of high-quality reasoning samples in the pre-training corpus, which primarily consists of human-written texts (Betz et al., 2021; Morishita et al., 2023). Indeed, reasoning samples in human-written texts often exhibit low quality, as evidenced by fallacies and biases commonly found in online debates (Hansson, 2004; Guia¸su and Tindale, 2018; Cheng et al., 2017). This is unsurprising given that humans usually think reflexively rather than through rigid reasoning (Kahneman, 2011; Sunstein and Hastie, 2015; Paglieri, 2017). Thus, a *Equal Contribution †Work done at Hitachi 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2411.12498v1  [cs.LG]  19 Nov 2024",
    "body": "Enhancing Reasoning Capabilities of LLMs\nvia Principled Synthetic Logic Corpus\nTerufumi Morishita1\nGaku Morio1∗\nAtsuki Yamaguchi2∗†\nYasuhiro Sogawa1\n1Advanced AI Innovation Center, Hitachi\n2The University of Sheffield\nAbstract\nLarge language models (LLMs) are capable of solving a wide range of tasks, yet\nthey have struggled with reasoning. To address this, we propose Additional Logic\nTraining (ALT), which aims to enhance LLMs’ reasoning capabilities by program-\ngenerated logical reasoning samples. We first establish principles for designing\nhigh-quality samples by integrating symbolic logic theory and previous empirical\ninsights. Then, based on these principles, we construct a synthetic corpus named\nFormal Logic Deduction Diverse (FLD×2), comprising numerous samples of\nmulti-step deduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. Finally, we empirically show that ALT on\nFLD×2 substantially enhances the reasoning capabilities of state-of-the-art LLMs,\nincluding LLaMA-3.1-70B. Improvements include gains of up to 30 points on\nlogical reasoning benchmarks, up to 10 points on math and coding benchmarks,\nand 5 points on the benchmark suite BBH.\n1\nIntroduction\nKnowledge and reasoning have long been considered essential elements for achieving artificial\nintelligence (McCarthy, 1959; Weizenbaum, 1966; Winograd, 1971; Colmerauer and Roussel, 1973;\nShortliffe, 1976; Elkan and Greiner, 1993). Knowledge refers to facts about the world, e.g., “objects\nwith mass generate a gravitational field” and “the Earth has mass.” Reasoning involves combining\nmultiple facts according to specific rules to obtain new knowledge. For example, the new knowledge\nthat “the Earth generates a gravitational field” can be derived from the aforementioned two facts.\nRecent observations suggest that LLMs can solve problems using memorized knowledge of similar\nsamples seen during pre-training, but they cannot solve novel, unknown problems that require\nreasoning (Hodel and West, 2023; Dasgupta et al., 2023; Zhang et al., 2024). For instance, LLMs can\nsolve famous arithmetic problems as is but not when the numbers or names are changed (Razeghi\net al., 2022; Mirzadeh et al., 2024), and they can solve coding tests from past years before the\n“knowledge cutoff” but not from the present year (Mitchell, 2023). This bias towards knowledge has\nbeen observed even in state-of-the-art LLMs such as GPT-4 (Liu et al., 2023b; Wu et al., 2023; Dziri\net al., 2023).\nLLMs’ poor reasoning capabilities can stem from the lack of high-quality reasoning samples in the\npre-training corpus, which primarily consists of human-written texts (Betz et al., 2021; Morishita\net al., 2023). Indeed, reasoning samples in human-written texts often exhibit low quality, as evidenced\nby fallacies and biases commonly found in online debates (Hansson, 2004; Guia¸su and Tindale,\n2018; Cheng et al., 2017). This is unsurprising given that humans usually think reflexively rather\nthan through rigid reasoning (Kahneman, 2011; Sunstein and Hastie, 2015; Paglieri, 2017). Thus, a\n*Equal Contribution\n†Work done at Hitachi\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\narXiv:2411.12498v1  [cs.LG]  19 Nov 2024\n\nLogic\nMath\nCode\nNLI\nOthers\nBenchmark sets (5-shot)\n45\n50\n55\n60\n65\n70\n75\nLLaMA-3.1-70B\n+ ALT\n \nCoT\nBBH (3-shot)\n60\n65\n70\n75\n80\n85\n \nCoT\nBBH (0-shot)\n10\n20\n30\n40\n50\n60\n \nPro\nMMLU (5-shot)\n50\n55\n60\n65\n70\n75\n80\nFigure 1: The performance gains to LLaMA-3.1-70B by Additional Logic Training (ALT) on the\nproposed synthetic corpus, FLD×2 (Formal Logic Deduction Diverse). Each benchmark set, such as\n“Logic” and “Math”, comprises various benchmarks in that domain. Tables 2, 4 shows the details.\nstraightforward strategy to improve LLMs’ reasoning capabilities is to prepare many high-quality\nreasoning samples and train LLMs on them.\nWe propose one such approach, Additional Logic Training (ALT), which utilizes high-quality\nsamples of logical reasoning, the most fundamental form of reasoning. To prepare such samples, we\nutilize synthetic generation (Clark et al., 2021; Betz et al., 2021; Tafjord et al., 2021; Morishita et al.,\n2023), where computer programs generate deductive reasoning samples in which a given hypothesis\nis proven or disproven by combining given facts following rigid reasoning rules. We overview ALT\nin Figure 2.\nIn synthetic generation, computer programs generate samples according to pre-designed patterns, so\nthis design largely determines the quality of these samples by nature. Thus, we start by discussing\nwhat is the ideal design for synthetic logic samples, incorporating symbolic logic theory and\nempirical findings (Section 2). The essence of logical reasoning lies in its ability to handle unknown\nfacts, unlike knowledge, which deals solely with established facts, such as commonsense facts;\ntherefore, samples must cover reasoning with unknown facts. Samples must include both illogical\nand logical reasoning to enable LLMs to distinguish between them. The samples must cover various\npatterns regarding a comprehensive set of reasoning aspects, such as reasoning rules and linguistic\nexpressions of logical statements. We summarize these discussions into design principles, which\nguide the design of synthetic logic samples. Finally, based on these principles, we construct a\nsynthetic corpus named Formal Logic Deduction Diverse (FLD×2), comprising numerous samples\nof multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions,\nand challenging distractors (Section 3).\nWe then empirically verify that ALT can enhance LLMs’ reasoning capabilities (Sections 4, 5).\nUsing 31 benchmarks covering diverse tasks, we observed that ALT on FLD×2 substantially boosts\nstate-of-the-art LLMs’ reasoning capabilities. Even LLaMA-3.1-70B, the largest LLM pre-trained\non over 15 trillion tokens, shows substantial improvements with ALT (Figure 1). Among synthetic\nlogic corpora with different sample designs, FLD×2 yielded the largest performance gains, validating\nour proposed design principles. Moreover, we discovered that employing a knowledge-forgetting\nprevention method during ALT is critically important, as it likely prevents the LLM’s knowledge of\nestablished facts from being displaced by the unknown facts included in synthetic logic corpora.\nFinally, we analyze which task-solving capabilities ALT can enhance and why (Section 6). We ob-\nserved a substantial improvement of up to 30 points on logical reasoning tasks (Table 4a). Surprisingly,\nwe also observed improvements in abductive reasoning tasks, which go beyond the synthetic logic\ncorpora’s original deductive reasoning tasks. Case analyses indicate that these improvements result\nfrom LLMs having acquired the fundamentals of the logic reflected in the design principles. We also\nobserved improvements of up to 10 points on math and coding tasks, indicating the generalizability of\nthe obtained reasoning capabilities (Tables 4b, 4c). We also observed improvements of up to 6 points\non natural language inference (NLI) tasks (Table 4d). Case analyses suggest that LLMs successfully\nintegrated the commonsense knowledge they had originally acquired during pre-training with the\nlogical reasoning capabilities newly acquired from ALT. Improvements across various other tasks\n(Table 4e) demonstrate the broad benefits of the obtained reasoning capabilities beyond standard\nreasoning tasks, though the modest improvements of up to 2 points indicate the need for future\nresearch on more effective application of these capabilities.\n2\n\nFacts (w/negatives = fact3, fact4):\n1. If that if a Foo star exists a Haz star also exists,\nthen a Jaz star exists.\n2. If a Foo star exist a Gaz star exists,\nand if a Gaz star exists a Haz star also exists.\n3. If a Foo star exists and a Kax star exists,\nthen a Jaz star exists.\n4. If a Haz star exists, then a Gaz star exists.\nHypothesis:\nA Jaz star exists.\nLogical steps:\nFact2 If a Foo star exist\na Gaz star exists.\nAssume that a Foo star exists. A Gaz star exists.\nFact2  A Haz star exists.\n(remove assumption ℱ)  If a Foo star exists\na Haz star also exists.\nFact1 A Jaz star exists.\nMultistep deduction by the\naxioms can express any other \ndeduction rules, such as syllogism.\n𝓖𝓖\n𝓗𝓗\nModus ponens\n𝓕𝓕→𝓖𝓖∧(𝓖𝓖→𝓗𝓗)\n∧ elimination\n𝓕𝓕→𝓖𝓖\n(𝓖𝓖→𝓗𝓗)\n𝓕𝓕\n(𝓖𝓖→𝓗𝓗)\n(𝓕𝓕→𝓗𝓗)\n→introduction\nModus ponens\n𝓕𝓕→𝓗𝓗→𝓘𝓘\n𝓕𝓕→𝓗𝓗→𝓘𝓘\n𝓙𝓙\nLogical Reasoning Sample\nSample Generator\nℱ→ℋ→ℐ\nℱ→𝒢𝒢∧𝒢𝒢→ℋ\nℱ∧𝒦𝒦→𝒥𝒥\nℋ→𝒢𝒢\n𝒥𝒥\nℱ→𝒢𝒢\nadd assumption ℱ\n𝒢𝒢\nCompleteness Theorem\nMulti-step Deductive Reasoning\nDesign Principles\n1.\nInclude unknown facts.\n2.\nInclude negative facts.\n3.\nInclude various\nreasoning rules.\n4.\nInclude various\nlinguistic expressions.\nInput\nOutput\nℋ\nℱ→ℋ\nFigure 2: Our proposed Additional Logic Training (ALT) aims to enhance LLMs’ reasoning capa-\nbilities through training on many synthetically generated logical reasoning samples. Our sample\ngenerator (left) first generates a sample of multi-step deductive reasoning and then converts it into\na deduction sample written in English (right). LLMs must generate logical steps to derive a given\nhypothesis from provided facts. The sample generator adheres to theoretically and empirically\ngrounded design principles discussed in Section 2. Refer to Figure D.3 for a real sample.\nOur contributions are summarized as follows:\n• We propose Additional Logic Training (ALT) and empirically verify that it can enhance the\nreasoning capability of state-of-the-art LLMs across various sizes, from 7B to 70B.\n• We establish systematic design principles for synthetic logic samples; then, we construct a\nsynthetic corpus named Formal Logic Deduction Diverse (FLD×2), comprising numerous\nsamples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic\nexpressions, and challenging distractors. We empirically verify that Formal Logic Deduction\nDiverse indeed leads to the largest improvements among corpora with different sample designs.\n• We demonstrate that LLMs enhanced by ALT can solve not only the original logical reasoning\ntasks present in synthetic logic corpora but also other tasks, such as math and coding tasks, and\nnotably NLI tasks, which require integrating knowledge and reasoning. This finding underscores\nthe potential for advancing truly versatile AI possessing both knowledge and reasoning capabilities.\nWe release the corpus, code, and the trained model under a permissive license 1.\n2\nHow Should Synthetic Logic Samples Be Designed?\nIn synthetic generation, computer programs generate samples according to pre-designed patterns,\nso this design largely determines the quality of the samples. While Previous studies have examined\nseveral designs (Clark et al., 2021; Betz et al., 2021; Tafjord et al., 2021; Morishita et al., 2023), these\ndesigns were not systematically discussed, so they may not be the most effective ones.\nThus, we start by discussing how to optimally design synthetic logic samples. To this end, we consider\nsymbolic logic theory as suggested by Morishita et al. (2023) and integrate empirical findings from\nprevious studies. First, we observe that the essence of logical reasoning, based solely on the logical\nrelationships between facts, lies in its ability to handle unknown facts, unlike knowledge, which by\ndefinition deals solely with established facts (Section 2.1). Therefore, we argue that samples should\ncover reasoning with unknown facts to represent this essential aspect of logical reasoning. We also\nobserve that logical reasoning involves various other aspects, such as illogical reasoning, reasoning\nrules, and linguistic expressions that represent logical statements (sections 2.2 to 2.4). The samples\nshould cover various patterns regarding these aspects to enable LLMs to solve various reasoning\nproblems. We summarize these discussions into the following design principles, which guide the\ndesign of synthetic logic samples.\n1https://github.com/hitachi-nlp/FLD\n3\n\n2.1\nTeaching Reasoning with Unknown Facts\nWe first explore the essence of logical reasoning that differentiates itself from knowledge. Consider\nthe following logical step:\nThe Earth orbits the Sun.\nIf the Earth orbits the sun, the Earth has four seasons.\nThe Earth has four seasons.\n(1)\nThis step is valid because the conclusion is logically derived from the two premises. Next, consider\nanother logical step:\nThe Earth orbits the Sun.\nIf the Earth orbits the sun, the Earth does not have four seasons.\nThe Earth does not have four seasons.\n(2)\nThe second premise and consequently, the conclusion, is factually wrong. Nevertheless, if the premise\nwas hypothetically correct, the conclusion could be logically derived. Therefore, step (2) is also\nlogically valid. Finally:\n1. A Foo star exists.\n2. If a Foo star exists, a Bar star also exists.\nA Bar star exists.\n(3)\n“Foo star” and “Bar star” are unknowns; nonetheless, we can still determine that step (3) is logically\nvalid. Steps (1) to (3) above can be abstracted into a deduction rule, i.e., modus ponens, using\nsymbols:\nF\nF →G\nmodus ponens\nG\n(4)\nAs we have seen, the logical validity of a deduction rule depends solely on whether the conclusion\nis logically derived from the premises, not on the factual correctness of the contents of F and G.\nTherefore, the contents of F and G can be arbitrary.\nNow, we consider what kind of samples would be needed to teach the deduction rule (4) to LLMs.\nWe assume a task to generate the conclusion given the premises as prompt inputs. If the learner were\nhuman, they would be able to infer the underlying deduction rule (4) by observing samples such as\n(1) to (2). As a result, they would become able to solve the unknown problem (3).\nHowever, from a purely inductive perspective, samples (1) to (2) cannot simply be generalized to the\ndeduction rule (4). This is because the samples (1) to (2) themselves do not contain the information\nthat the contents of F and G are arbitrary. In fact, one could generalize samples (1) to (2) to other\nrules; for example, the conclusion G can be derived if F and F →G are given as premises and F\nand G include ’Earth’ as their contents. Innumerable such deduction rules can be inductively inferred\nfrom the given samples. In other words, induction has arbitrariness (Hume, 1748; Goodman, 1954;\nQuine, 1969).\nHumans prefer simpler rules (Bertrand; Wittgenstein, 1922), so they boldly induce up to the deduction\nrule (4). However, it is unclear how purely inductive learners such as LLMs, which extract only what\ncan be inferred from samples without prior preferences, induce up to (4). For example, if only specific\ncontents such as “Alice is kind” and “Bob is smart” are assigned to F and G in training samples, an\nLLM could develop into a machine that generates the conclusion G only when the input contains the\nspecific contents. In order for LLMs to accurately induce that F and G are indeed arbitrary:\nDesign Principle 1 (Reasoning with Unknown Facts). Prepare many samples assigning arbitrary\ncontents to F and G. They will make LLMs accurately induce F and G are indeed arbitrary, ultimately\nenabling them to reason with unknown facts.\n2.2\nTeaching Illogical Reasoning\nSuppose we have LLMs trained on a large number of samples as follows:\nF ∧G\n(F ∧G) →H\nH\n(5)\nwhere ∧denotes logical conjunction, and arbitrary contents are assigned to F, G, H. Suppose that\nwe give this LLM a problem such as:\nF\n(F ∧G) →H\n??\n(6)\nSince the premises are insufficient for logically deducting the conclusion, outputting nothing is the\ncorrect answer.\n4\n\nUnfortunately, an LLM could output H, which was indeed often observed in our preliminary experi-\nments. This is because while the LLMs can induce from sample (5) that it can generate the conclusion\nH when the two premises of (5) are given, the LLMs cannot induce from the sample that it is not\nallowed to generate the conclusion H when the premises of (6) are given, as such information is not\nincluded in the sample (5) itself. Therefore,\nDesign Principle 2 (Illogical Reasoning). Include negative samples such as (6). These samples will\nmake LLMs induce that conclusions cannot be derived from insufficient premises.\n2.3\nTeaching Diverse Reasoning Rules\nDeduction rules other than (4) exist:\n(F∧G)\nF\n(F∧G) ∧elimination\nG\n(F →G)∧(G →H) syllogism\nF→H\nF→G\ncontraposition\n¬G→¬F\n¬(F∨G)\n¬F∧¬G\n¬(F∧G)\nDe Morgan’s laws\n¬F∨¬G\n(7)\nwhere ∨denotes logical disjunction and ¬ negation. Since there are infinitely many possible logical\nformulas that can appear as premises and conclusions, there are infinitely many deduction rules.\nProviding LLMs with these infinite deduction rules is obviously intractable.\nInstead of directly providing these infinite deduction rules, we can take another approach. Consider\nmulti-step deductive reasoning (Figure 2 left), where multiple deduction rules derive a conclusion.\nNotice that the syllogism in (7) can be expressed by multi-step deductive reasoning using more\n“atomic” deduction rules. Indeed, there exists a set of atomic deduction rules called the axioms that\nsatisfies the following:\nTheorem 2.1 (Completeness of First-Order Predicate Logic Gödel (1930)). Any valid deduction rule\ncan be expressed by multistep deductive reasoning constructed from the axioms.\nIn contrast to the axioms, the ‘compound’ deduction rules, such as syllogism, contraposition, and\nDe Morgan’s laws, are called theorems. According to the completeness Theorem 2.1, if we can\nhandle the axioms, we can effectively handle other deduction rules as well. Indeed, Morishita et al.\n(2023) empirically verified that a language model trained on the axioms generalizes to handle other\ndeduction rules more effectively than those trained on non-axiom deduction rules. Therefore,\nDesign Principle 3 (Diverse Reasoning Rules). Samples should express multi-step deduction con-\nstructed from the axioms. They will effectively teach LLMs diverse deduction rules (Morishita et al.,\n2023)\nIn multi-step deductive reasoning, the number of logical steps s from premises to a conclusion can\nvary largely depending on the problem. Therefore:\nDesign Principle 3’ (Diverse Reasoning Rules). Samples should include diverse numbers of logical\nsteps s.\nIdeally, this would be sufficient, but empirical evidence has shown that LLMs struggle with construct-\ning multi-step deductive reasoning with large steps s (Gontier et al., 2020; Morishita et al., 2023).\nConsequently, LLMs would not excel at handling theorems that require a large number of steps s\nwhen expressed by the axioms. Therefore, as an additional countermeasure:\nDesign Principle 3” (Diverse Reasoning Rules). Samples should also include representative theorems,\nsuch as syllogism, contraposition, and De Morgan’s laws.\n2.4\nTeaching Diverse Linguistic Expressions that Represent Logical Statements\nThere are various linguistic structures for expressing the logical relationship F →G, such as “If F\nthen G”, “F leads to G”, and “F results in G”. If we only include specific expressions in the corpora,\nLLMs may only learn to react to these specific expressions, which has been observed in previous\nexperiments (Zhang et al., 2022; Yuan et al., 2023). To prevent this,\nDesign Principle 4 (Diverse Linguistic Expressions). Samples should include diverse linguistic\nexpressions that represent logical statements.\nIn this chapter, we have established the principles to guide the design of synthetic logic samples.\nNext, we construct a synthetic logic corpus based on these principles.\n5\n\nTable 1: Synthetic logic corpora compared in this study, with their features categorized according to\nour proposed design principles (DP). Note that the last row of the ablation corpora lists variations of\nFLD×2, each of which differs from the original regarding one of the design principles.\nDP1\nDP2\nDP3\nDP4\nvocabulary size\ndistractors\ndeduction rules\nlogical steps\nexpressions per formula\nRuleTaker (Clark et al., 2021)\n(RT)\n≤100\n(hand-selected)\nrandom\nformula\n2\n(implication)\n1–5\nO(1)\nPARARULE-Plus (Bao et al., 2022)\n(PRP)\n≤100\n(hand-selected)\nrandom\nformula\n2\n(implication)\n1–5\nO(1)\nFLD (Morishita et al., 2023)\n≃15k\n(WordNet, subset)\nrandom\nformula\n13\n(axioms)\n1–8\n10∼100\nFLD×2\n≃100k\n(WordNet, full)\nadversarial\nformula\n≃50\n(axioms and theorems)\n1–8\n10∼100\n(more extensive than FLD)\nFLD×2\nablation corpora →\n100\n→w/o DP1\nnot used\n→w/o DP2\n2 (implication)\n→w/o DP3.rules\n1\n→w/o DP3.steps\n1\n→w/o DP4\n3\nCreating a Synthetic Corpus based on Design Principles\nTo prepare diverse samples reflecting the design principles 1 to 4 (DP1-4), we built a novel sample\ngenerator by extending the previous one by Morishita et al. (2023) and then generated the synthetic\nlogic corpus named FLD×2 (Formal Logic Deduction Diverse). Figure 2 shows a schematic of our\ngenerator and a deduction sample. Table 1 compares FLD×2 with existing corpora. Figure D.3\nprovides an actual deduction sample included in FLD×2.\nMore specifically, our generator generates deduction samples through the following steps. First, the\ngenerator randomly generates a sample of multi-step deductive reasoning written in logical formulas,\nas shown on the left side of Figure 2, where a conclusion is derived from premises using multiple\ndeduction rules (See Appendix D.3 for more details of this generation procedure). At this time, the\ngenerator also generates ‘distractor’ logical formulas, which express negative premises of DP2. Next,\nthe generator converts each logical formula into English expressions. To achieve this, the generator\nfirst randomly selects a template from pre-defined options, such as “If F, then G,” “F leads to G,”\nor “F results in G,” for the logical formula “F →G.” It then assigns English content randomly\nconstructed from a vocabulary, such as “(that) a Foo star exists” and “(that) a Bar star exists,” to\neach symbol, such as F and G. Finally, it converts the multi-step deduction into a deduction sample\n(right side of Figure 2) by using the premises as ‘facts’, the conclusion as ‘hypothesis’, and the\nintermediate logical steps as ‘logical steps’. The deduction sample requires LLMs to generate logical\nsteps that derive a given hypothesis based on the given facts.\nTable 1 outlines the comparison of FLD×2 with other existing corpora (Clark et al., 2021; Bao et al.,\n2022; Morishita et al., 2023) in terms of DP1-4, which is detailed as follows:\n• DP1: We assign F and G content randomly constructed from a vocabulary. While the existing\ncorpora used small-sized vocabulary of up to 15k, we use a large vocabulary of around 100k words\nbuilt from WordNet (Miller, 1995). This will teach LLMs that F and G are truly arbitrary, ultimately\nenabling them to reason with unknown facts.\n• DP2: The existing corpora used randomly generated logical formulas as distractors. In contrast,\nwe implement adversarial distractors. For example, for a premise F ∧G, we use F with missing\ninformation (see Equations (5), (6)), and for a premise F →H, we use F ∧G →H with missing\ninformation as distractors. These distractors teach LLMs precisely when a conclusion can and\ncannot be derived. As with previous corpora, we include a variable number of distractors in each\nsample, randomly chosen from a range of 0 to 20.\n• DP3-3”: While the existing corpora used a small number of deduction rules of up to 13 (refer to\nFigure B.4 of Morishita et al. (2023)), we include diverse deduction rules, encompassing the axioms\nand representative theorems, such as modus ponens, syllogisms, and contraposition, totaling about\n50 rules. We include samples with up to s = 8 logical steps, following (Morishita et al., 2023).\n• DP4: We manually craft several more English templates per logical formulas than those used\nin FLD. Since the templates have a nested structure, they yield combinatorially more diverse\nEnglish expressions. While counting the exact number of the resulting expressions is intractable,\nwe observed at least dozens of expressions per logical formula, including minor variations. See\nAppendix D.4 for details.\n6\n\n4\nExperimental Setup\nWe briefly explain the experimental settings. Refer to Appendix E for the details.\nSynthetic Logic Corpora: We examine the proposed FLD×2 and previous corpora (Table 1).\nLLMs: We used the state-of-the-art LLM, LLaMA-3.1 (8B and 70B) (AI@Meta, 2024).\nTraining Settings: We trained the LLMs by a method similar to supervised fine-tuning; as illustrated\nin Figure 2, we used the facts and hypothesis as inputs and logical steps and additional answer label\n(see Appendix D.1) as outputs. We excluded loss computation for the inputs to prevent LLMs from\nlearning to generate unknown facts. We trained the LLMs for 1 epoch on 100k samples (∼0.1B\ntokens) from the training split of each corpus, with a batch size of 256, resulting in 390 steps, with a\nlinear warmup for 200 steps. We used the learning rate of 2e-05 for the 8B model and 3e-06 for the\n70B model. We used Huggingface (Wolf et al., 2020) for implementation.\nPrevention of Knowledge Forgetting by Recall Adam Optimizer: Synthetic logic corpora include\nmany samples with unknown facts, so training on them should cause LLMs to forget their knowledge\nof existing facts. To prevent this, we employed the Recall Adam optimizer (Chen et al., 2020),\nwhich regularizes parameter updates to avoid deviating too far from the pre-training parameters.\nRecall Adam stands out for LLM training for several reasons (see Appendix E.0.1 for details).\nWe used our re-implemented version 2. The hyperparameters were: β1 = 0.9, β2 = 0.999, ϵ =\n10−6, fisher coefficient = 4000 for the 8B model and 2000 for the 70B model.\nBenchmarks: We evaluated the trained LLMs on 31 benchmarks shown in Table E.7 using 5-shot\nin-context learning, except for BBH and AbuductionRules, which used 3-shot in-context learning.\nThese benchmarks cover a wide range of tasks and are prominent in LLM evaluation. Note that we\nexcluded the synthetic logic corpora used for training, as training on them often leads to overfitting\nto their superficial and statistical cues (Zhang et al., 2022; Yuan et al., 2023), failing to measure\ntruly generalizable reasoning capabilities. We used lm-evaluation-harness (Gao et al., 2023) and\nbigcode-evaluation-harness (Ben Allal et al., 2022) for the implementation.\n5\nCan Additional Logic Training Enhance LLMs’ Capabilities?\nTable 2 show the performance of LLMs before and after ALT. Most LLMs trained with ALT\noutperformed their counterparts without ALT. Notably, ALT yielded substantial gains of up to 10\npoints even for LLaMA-3.1-70B, the largest LLM pre-trained on over 15 trillion tokens. These results\nverify that ALT can enhance the capabilities of state-of-the-art LLMs.\nAmong the LLMs trained with ALT, the one trained on FLD×2 (i.e., ⊕ALT-FLD×2) achieved the\nhighest generalization performance across the benchmarks. Table 3 shows the performance of the\nLLMs trained on ablated FLD×2 corpora, each of which lacks one of the design principles. As\nseen, ablating any design principle almost always led to performance degradation. These results\ndemonstrate that the proposed design principles are critical to obtaining the maximum possible gain\nfrom ALT, and each principle is indispensable.\nTable F.8 shows that the LLMs trained with ALT without preventing knowledge forgetting by Recall\nAdam optimizer underperformed compared to their counterparts trained with knowledge forgetting\nprevention and even the LLM without ALT. This behavior presumably occurred because the unknown\nfacts included in synthetic logic corpora displaced the LLM’s knowledge of existing facts. Therefore,\nknowledge-forgetting prevention is critically important for the success of ALT.\n6\nWhat Capabilities Can Additional Logic Training Enhance and Why?\nWe analyze the results on each benchmark or each case and discuss whether and why the LLM’s\ncapabilities to solve the tasks can or cannot be enhanced by ALT.\n6.1\nLogical Reasoning Tasks\nTable 4a shows that ALT substantially boosted LLaMA-3.1-70B’s performance by up to 30 points\non various benchmarks dealing with logical reasoning tasks. Surprisingly, we also observed im-\nprovements on abductive reasoning tasks, which go beyond the original deductive reasoning tasks\n2https://github.com/hitachi-nlp/rec-adam\n7\n\nTable 2: 5-shot performance of LLMs before and after ALT. ⊕ALT-x denotes the LLM trained with\nALT on the synthetic logic corpus x from Table 1. The color shows the rank in each column (darker\nis better). Each benchmark set, such as “Logic” and “Math”, comprises various benchmarks in that\ndomain (see Table E.7). “Avg.” represents the micro-average of all the benchmarks.\n(a) LLaMA-3.1-8B.\nAvg.\nLogic\nMath\nCode\nNLI\nOthers\nBBH (3-shot)\nBBH (0-shot)\nMMLU\nCoT\nCoT\nPro\nLLaMA-3.1-8B\n47.9\n42.8±0.4\n39.6±0.5\n35.4\n65.4±0.3\n60.7±0.3\n44.9±0.4\n61.9±0.4\n8.2±0.2\n36.5±0.4\n65.3±0.4\n35.8±0.4\n⊕ALT-PRP\n48.1\n43.7±0.2\n39.2±0.3\n35.7\n65.6±0.2\n60.8±0.2\n44.9±0.2\n61.8±0.2\n8.2±0.1\n36.4±0.2\n65.3±0.2\n35.3±0.2\n⊕ALT-RT\n50.1\n46.8±0.1\n42.4±0.2\n36.5\n68.6±0.1\n61.3±0.1\n46.9±0.2\n63.5±0.2\n13.7±0.1\n38.4±0.2\n65.3±0.1\n35.7±0.2\n⊕ALT-FLD\n51.9\n51.6±0.1\n43.4±0.2\n38.1\n70.1±0.1\n61.5±0.1\n46.7±0.2\n64.9±0.2\n11.9±0.1\n39.6±0.2\n65.4±0.1\n36.2±0.2\n⊕ALT-FLD×2\n52.0\n52.2±0.1\n43.2±0.2\n38.0\n70.7±0.1\n61.5±0.1\n46.5±0.2\n65.3±0.2\n11.3±0.1\n38.7±0.2\n65.5±0.1\n36.4±0.2\n(b) LLaMA-3.1-70B.\nAvg.\nLogic\nMath\nCode\nNLI\nOthers\nBBH (3-shot)\nBBH (0-shot)\nMMLU\nCoT\nCoT\nPro\nLLaMA-3.1-70B\n60.0\n57.4±0.4\n60.0±0.5\n46.2\n73.7±0.3\n67.7±0.3\n60.4±0.3\n82.1±0.2\n6.5±0.1\n50.1±0.3\n78.7±0.3\n50.7±0.4\n⊕ALT-PRP\n60.4\n57.7±0.4\n59.8±0.5\n49.2\n73.5±0.3\n67.6±0.3\n60.4±0.4\n82.2±0.3\n6.0±0.2\n50.1±0.4\n78.7±0.3\n50.9±0.4\n⊕ALT-RT\n62.7\n61.4±0.2\n62.1±0.3\n50.8\n75.4±0.2\n68.4±0.2\n64.1±0.3\n82.5±0.2\n11.5±0.2\n59.2±0.3\n79.0±0.2\n52.4±0.3\n⊕ALT-FLD\n64.2\n65.7±0.1\n63.6±0.2\n52.0\n75.3±0.1\n68.5±0.1\n65.0±0.2\n83.6±0.1\n12.1±0.1\n59.9±0.2\n79.3±0.1\n54.4±0.2\n⊕ALT-FLD×2\n64.4\n66.1±0.1\n63.3±0.2\n52.4\n76.1±0.1\n68.5±0.1\n65.4±0.2\n83.6±0.2\n11.4±0.1\n60.8±0.2\n79.5±0.1\n54.4±0.2\nTable 3: LLaMA-3.1-8B trained on the ablation corpora.\nAvg.\nLogic\nMath\nCode\nNLI\nOthers\nBBH (3-shot)\nBBH (0-shot)\nMMLU\nCoT\nCoT\nPro\n⊕ALT-FLD×2\n52.0\n52.2±0.1\n43.2±0.2\n38.0\n70.7±0.1\n61.5±0.1\n46.5±0.2\n65.3±0.2\n11.3±0.1\n38.7±0.2\n65.5±0.1\n36.4±0.2\nw/o DP1\n51.4\n52.2±0.1\n43.1±0.2\n39.2\n70.0±0.1\n59.4±0.1\n46.7±0.2\n64.7±0.2\n11.5±0.1\n38.9±0.2\n65.4±0.1\n36.1±0.2\nw/o DP2\n50.6\n49.9±0.1\n43.1±0.2\n38.1\n71.1±0.1\n59.3±0.1\n46.1±0.2\n64.6±0.2\n10.4±0.1\n37.4±0.2\n65.4±0.1\n35.7±0.2\nw/o DP3.rules\n50.7\n50.4±0.1\n42.8±0.2\n38.3\n69.5±0.1\n59.4±0.1\n46.4±0.2\n64.0±0.2\n11.8±0.1\n38.3±0.2\n65.6±0.1\n36.2±0.2\nw/o DP3.steps\n51.1\n51.5±0.1\n43.1±0.2\n38.7\n69.6±0.1\n59.5±0.1\n46.8±0.2\n65.0±0.2\n12.3±0.1\n38.8±0.2\n65.6±0.1\n36.3±0.2\nw/o DP4\n51.3\n52.2±0.1\n42.8±0.2\n38.4\n70.3±0.1\n59.5±0.1\n46.1±0.2\n64.8±0.2\n12.8±0.1\n39.3±0.2\n65.5±0.1\n36.3±0.2\nin synthetic logic corpora. Abductive reasoning involves guessing the missing premises that caused\nthe observed conclusion rather than deriving a conclusion from the premises. For example, from\nthe observed conclusion, “the window glass at home was broken and the room was ransacked,” we\nguess the premise “a burglar broke in.” The improvements would be due to the fact that, while the\nsurface form of abductive reasoning problems differs from that of deductive reasoning, they share the\nfundamentals of logic reflected in the design principles.\nNext, we conduct case analyses to see whether the LLM enhanced by ALT acquired the abilities\nintended by the proposed design principles (DP1-4). Table 5 shows problems where LLaMA-3.1-\n70B’s errors have been corrected by ALT. The first problem is very simple, so it is surprising that\nLLaMA-3.1-70B failed to solve it, indicating the inherent difficulty of learning logical reasoning\nsolely from pre-training. In contrast, ⊕ALT-FLD×2, which was additionally trained on FLD×2,\nsolved the problem correctly. The premises of the problem are randomly constructed to express\nunknown facts. Therefore, the result suggests that ⊕ALT-FLD×2 acquired genuine logical reasoning\nability, which can handle unknown facts (DP1).\nIn the second problem, ⊕ALT-FLD×2 correctly answered “neutral”, indicating that it successfully\nlearned that conclusions cannot be derived from insufficient facts (DP2).\nThe third problem comes from the FOLIO benchmark. To solve this problem, LLMs must use\nsyllogism at the first step as follows: “All eels are fish, and no fish are plants. Therefore, all ells are\nnot plants.” ⊕ALT-FLD×2 answered this problem correctly, suggesting that it successfully learned\ndiverse deduction rules (DP3).\nFOLIO problems are created based on Wikipedia topics, describing them in more natural and realistic\nlinguistic expressions than in other benchmarks. As seen in the fourth problem, ⊕ALT-FLD×2\nunderstands such expressions, suggesting the effect of diverse expressions from DP4 and/or that\nLLMs can integrate their original linguistic ability with the newly acquired logical reasoning ability.\n8\n\nTable 4: Benchmark-wise 5-shot performance of LLaMA-3.1-70B before and after ALT on FLD×2.\nRefer to Table F.9 for LLaMA-3.1-8B results. Table E.7 details each benchmark.\n(a) Logic.\nbAbiD FOLIO LogicNLI RobustLR AR-LSAT LogiQA ReClor AbductionR\nART\nLLaMA-3.1-70B 83.8±1.2 58.9±1.6\n34.9±1.1\n49.6±0.9\n21.5±1.0\n64.3±1.2 33.7±0.7\n84.0±0.7\n85.4±0.9\n⊕ALT-FLD×2\n83.5±0.5 66.7±0.6\n50.9±0.5\n81.6±0.3\n25.0±0.4\n69.4±0.5 36.3±0.3\n95.7±0.2\n85.5±0.4\n(b) Math.\nGSM8k\nMATH MathQA\nCoT\nCoT (0-shot)\n-\n-\nLLaMA-3.1-70B 80.9±1.1 75.2±1.2\n65.4±1.3\n23.7±0.6\n55.0±0.9\n⊕ALT-FLD×2\n83.3±0.4 80.4±0.4\n73.0±0.5\n24.4±0.2\n55.4±0.4\n(c) Code.\nHumanEval\nMBPP\nMBPP+\nMultiPL-E (cpp)\nMultiPL-E (go)\nLLaMA-3.1-70B\n32.3\n43.4\n48.7\n29.8\n76.6\n⊕ALT-FLD×2\n42.6\n49.5\n52.5\n38.7\n78.6\n(d) Natural language inference (NLI).\nHELP\nMNLI\nRTE\nSNLI\nLLaMA-3.1-70B\n45.8±0.5\n82.2±0.4\n84.0±0.7\n82.6±0.4\n⊕ALT-FLD×2\n51.3±0.2\n83.7±0.2\n87.2±0.3\n82.3±0.2\n(e) Others.\nCommonsenseQA HellaSwag SQuAD WinoGrande ARCe\nARCc\nGPQA OpenBookQA\nSciQ\nLLaMA-3.1-70B\n81.2±1.1\n69.2±0.5\n38.5±0.0\n85.6±1.0\n89.1±0.6 65.3±1.4 40.7±1.4\n41.4±0.7\n98.5±0.4\n⊕ALT-FLD×2\n82.5±0.4\n69.6±0.2\n40.1±0.0\n86.1±0.4\n89.4±0.3 66.7±0.6 40.6±0.6\n42.8±0.3\n98.5±0.2\n6.2\nMath and Coding Tasks\nTables 4b, 4c shows that ALT substantially boosted the LLaMA-3.1-70B’s performance by up to 7\nand 10 points on math and coding tasks, respectively. The math improvements are reasonable, as\nunderstanding predicate logic is a prerequisite for solving mathematical problems. For coding, some\nrecent studies have verified the opposite direction, namely, that training on coding data improves\nlogical reasoning abilities (Jiang et al., 2024b; MA et al., 2024; Uchiyama et al., 2024).\n6.3\nNLI Tasks\nTable 4d shows that ALT substantially boosted the LLaMA-3.1-70B’s performance by up to 6 points\non various natural language inference (NLI) benchmarks. NLI is similar to deductive reasoning in\nassessing whether a premise supports or contradicts a hypothesis. However, the main difference is\nthat this judgment requires a rich set of commonsense knowledge beyond the given premise.\nConsider the fifth problem in Table 5: by supplementing the given fact “An Indian woman is dancing\nwith her partner” with the commonsense knowledge “If someone is dancing, then he/she is moving.”,\nwe can derive the hypothesis “A woman is moving.” The sixth problem is more challenging as we\nhave to trace multiple logical steps while supplementing with sufficient commonsense knowledge as\nfollows: “a church choir sings at a church,” “baseball is often played at a baseball field,” “a person\ncannot be in two or more places at the same time,” “therefore, a church choir cannot sing for baseball.”\nSince synthetic logic corpora only contain unknown facts, LLMs cannot acquire new knowledge\nfrom them. Therefore, the commonsense knowledge used to solve the above problems must have\nbeen acquired by the LLMs from pre-training. This suggests that LLMs can integrate their original\nknowledge with the logical reasoning capabilities newly acquired from ALT to solve problems.\n9\n\nTable 5: Problems where LLaMA-3.1-70B initially answered incorrectly and then correctly after\ntraining with ALT on FLD×2. Red highlights the premises related to the hypothesis.\nbenchmark premises\nhypothesis\nanswer\n(LLaMA-3.1-70B/gold)\nrequired\nability\nLogicNLI\nMice are afraid of wolves. Cats are afraid of sheep.\nJessica is a cat. Wolves are afraid of cats.\nWinona is a wolf. Sheep are afraid of cats.\nJessica is\nafraid of sheep.\nneutral/\nentailment\nDP1\nRhett is not modest. Vivian is confused.\nRhett is lazy. If someone is modest or not confused, then he is not eager.\nRhett is\nconfused.\nentailment/\nneutral\nDP2\nFOLIO\nAll eels are fish. No fish are plants.\nEverything displayed in the collection is either a plant or an animal.\nAll animals displayed in the collection are multicellular.\nA sea eel is displayed in the collection.\nThe sea eel is an eel or an animal or not a plant.\nThe sea eel\nis multicellular\nor is bacteria.\nneutral/\nentailment\nDP3\nCommon utilities include water, electricity, gas, heating, sewer, trash, and recycling.\nMany apartment rents cover the cost of water and electricity.\nSusan lives in an apartment where the rent covers all utilities.\nThe rent of the apartment where Ava lives does not cover any utility expenses.\nNoah lives in an apartment where the rent does not cover heating.\nNoah and Ava both\nneed to pay\nthe heating bill.\nneutral/\nentailment\nDP4\nSNLI\nAn Indian woman is dancing with her partner.\nA woman is moving.\nneutral/\nentailment\nreasoning\nwith\ncommonsense\nknowledge\nThis church choir sings to the masses\nas they sing joyous songs from the book at a church.\nA choir is singing\nat a baseball game.\nentailment/\ncontradiction\nTable 6: Problems that LLaMA-3.1-70B trained with ALT on FLD×2 still cannot solve.\nbenchmark question\nanswer\nARC\n(challenge)\nThe end result in the process of photosynthesis is the production of\nsugar and oxygen. Which step signals the beginning of photosynthesis?\nChlorophyll in the\nleaf captures light energy.\nGPQA\nA spin-half particle is in a linear superposition 0.8| ↑⟩+ 0.6| ↓⟩of its spin-up\nand spin-down states. If | ↑⟩and | ↓⟩are the eigenstates of σz, then what\nis the expectation value up to one decimal place, of the operator 10σz + 5σx?\n−0.7\nARC\n(challenge)\nBeavers build their homes in ponds and streams. Which characteristic\nis least critical to building homes in an aquatic environment?\n(A) waterproof fur (B) webbed hind feet\n(C) arge, sharp teeth (D) flat, wide tail\n6.4\nOther Tasks\nImprovements across various other tasks (Table 4e) demonstrate the broad benefits of the obtained\nreasoning capabilities beyond standard reasoning tasks; though the improvements were modest at up\nto 2 percentage points, which may be due to the following reasons. First, these benchmarks include\nproblems that purely test knowledge, such as the first one in Table 6. Since ALT does not aim to\nprovide new knowledge, the ability to solve such problems does not improve by nature. Next, some\nproblems may require knowledge that is too advanced for LLMs, so potential improvements by the\nenhanced reasoning capabilities may be bottlenecked. For example, the second problem does involve\nreasoning but requires sufficient quantum mechanics knowledge as a prerequisite. However, these\nknowledge-related issues should be solved by improving the quantity and quality of pre-training.\nFinally, LLMs may not be able to fully utilize the potential of enhanced reasoning capabilities for\nproblems that require complex procedures. To solve the third problem, LLMs first must attempt\nreasoning related to each choice as follows: “To build homes in an aquatic environment, one needs to\nmaintain body heat and insulation despite being frequently submerged in cold water. Therefore, the\nwaterproof fur of (A) is essential”, and “To build . . . , one must gather and process natural materials\nlike wood. Large, sharp teeth of (C) are critical as they allow beavers to cut down trees and shape\nbranches.” Next, while reasoning traces on (A) to (D) all seem reasonable, LLMs must choose the\nsingle best answer, considering the subtle nuance of the question context, as follows: “Since the\nquestion emphasizes the aquatic environment, the least related reasoning trace should be (C).” This\ncomplex procedure contrasts with logical reasoning and NLI problems, where LLMs can directly\nobtain an answer from a single reasoning trace. Previous studies also observed that such procedure on\nmultiple-choice QA problems are challenging for LLMs (Robinson and Wingate, 2023; Zheng et al.,\n2024; Wang et al., 2024a). Since ALT alone does not teach LLMs such task-specific procedures,\nadditional training on these procedures should be necessary to solve these problems.\n7\nConclusion\nTowards versatile artificial intelligence with reasoning capabilities, we proposed Additional Logic\nTraining on synthetic logic samples. We established systematic design principles well-grounded on\nsymbolic logic theory and previous empirical findings. We constructed a corpus named Formal Logic\nDeduction Diverse (FLD×2) based on the design principles. We empirically showed that ALT on\nFLD×2 substantially enhances the capabilities of state-of-the-art LLMs.\n10\n\nAcknowledgement\nComputational resources of AI Bridging Cloud Infrastructure (ABCI) provided by the National\nInstitute of Advanced Industrial Science and Technology (AIST) were used. We thank Dr. Masaaki\nShimizu at Hitachi for the convenience of additional computational resources. We thank Dr. Naoaki\nOkazaki, a professor at the Tokyo Institute of Technology, for the keen comments.\nReferences\nAI@Meta. 2024. Llama 3 model card.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh\nHajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-\nbased formalisms. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 2357–2367, Minneapolis, Minnesota. Association for Computational\nLinguistics.\nRisako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, and Mitsuhiro Okada. 2023.\nEvaluating large language models with NeuBAROCO: Syllogistic reasoning ability and human-like\nbiases. In Proceedings of the 4th Natural Logic Meets Machine Learning Workshop, pages 1–11,\nNancy, France. Association for Computational Linguistics.\nYoichi Aoki, Keito Kudo, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Keisuke Sakaguchi,\nand Kentaro Inui. 2024. First heuristic then rational: Dynamic use of heuristics in language model\nreasoning.\nAmanda Askell. 2020. Gpt-3: Towards renaissance models. Daily Nous Blog: Philosophers On\nGPT-3.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732.\nQiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, and\nJiamou Liu. 2022. Multi-step deductive reasoning over natural language: An empirical study on\nout-of-distribution generalisation. In Proceedings of the 16th International Workshop on Neural-\nSymbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning\n& Reasoning (IJCLR 2022), pages 202–217, Cumberland Lodge, Windsor Great Park, United\nKingdom.\nLoubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024.\nCosmopedia.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro von\nWerra. 2022. A framework for the evaluation of code generation models. https://github.\ncom/bigcode-project/bigcode-evaluation-harness.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009.\nThe fifth pascal recognizing textual entailment challenge. In Text Analysis Conference.\nLeonardo Bertolazzi, Albert Gatt, and Raffaella Bernardi. 2024. A systematic analysis of large\nlanguage models as soft reasoners: The case of syllogistic inferences.\nRussell Bertrand. A history of western philosophy.\nGregor Betz, Christian Voigt, and Kyle Richardson. 2021. Critical thinking for language models.\nIn Proceedings of the 14th International Conference on Computational Semantics (IWCS), pages\n63–75, Groningen, The Netherlands (online). Association for Computational Linguistics.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. 2019. Abductive commonsense\nreasoning. arXiv preprint arXiv:1908.05739.\n11\n\nNeeladri Bhuiya, Viktor Schlegel, and Stefan Winkler. 2024. Seemingly plausible distractors in\nmulti-hop reasoning: Are large language models attentive readers?\nKaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural\nlanguage deductions. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6266–6278, Online and Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated\ncorpus for learning natural language inference.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha,\nMichael Greenberg, and Abhinav Jangda. 2023. Multipl-e: A scalable and polyglot approach to\nbenchmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):3675–\n3691.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.\nEvaluating large language models trained on code.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. 2020. Recall\nand learn: Fine-tuning deep pretrained language models with less forgetting. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n7870–7881, Online. Association for Computational Linguistics.\nXinyun Chen, Ryan Andrew Chi, Xuezhi Wang, and Denny Zhou. 2024. Premise order matters in\nreasoning with large language models. In Proceedings of the 41st International Conference on\nMachine Learning, volume 235 of Proceedings of Machine Learning Research, pages 6596–6620.\nPMLR.\nJ. Cheng, M. Bernstein, C. Danescu-Niculescu-Mizil, and J. Leskovec. 2017. Anyone can become a\ntroll: Causes of trolling behavior in online discussions. CSCW: Proceedings of the Conference on\nComputer-Supported Cooperative Work. Conference on Computer-Supported Cooperative Work,\n2017.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv preprint arXiv:1803.05457.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over\nlanguage. In Proceedings of the Twenty-Ninth International Conference on International Joint\nConferences on Artificial Intelligence, pages 3882–3890.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\nA. Colmerauer and P Roussel. 1973. The birth of prolog. The ALP Newsletter.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entail-\nment challenge. pages 177–190.\n12\n\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pi-\npatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358–7370,\nOnline and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nIshita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Hannah R. Sheahan, Antonia Creswell,\nDharshan Kumaran, James L. McClelland, and Felix Hill. 2023. Language models show human-like\ncontent effects on reasoning tasks.\nJohn Dougrez-Lewis, Mahmud Elahi Akhter, Yulan He, and Maria Liakata. 2024. Assessing the\nreasoning abilities of chatgpt in the context of claim verification.\nNouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West,\nChandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren,\nAllyson Ettinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith and fate: Limits of transformers on\ncompositionality.\nTiwalayo Eisape, Michael Tessler, Ishita Dasgupta, Fei Sha, Sjoerd Steenkiste, and Tal Linzen. 2024.\nA systematic comparison of syllogistic reasoning in humans and language models. In Proceedings\nof the 2024 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Volume 1: Long Papers), pages 8425–8444, Mexico\nCity, Mexico. Association for Computational Linguistics.\nCharles Elkan and Russell Greiner. 1993. Building large knowledge-based systems: Representation\nand inference in the cyc project: Db lenat and rv guha.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff,\nChris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,\nEric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot\nlanguage model evaluation.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B Dolan. 2007. The third PASCAL\nrecognizing textual entailment challenge. In ACL-PASCAL Workshop on Textual Entailment and\nParaphrasing, pages 1–9.\nKurt Gödel. 1930. Uber die vollständigkeit des logikkalküls. Ph.D. thesis, Ph. D. dissertation,\nUniversity of Vienna.\nNicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal. 2020. Measuring systematic generaliza-\ntion in neural proof generation with transformers. Advances in Neural Information Processing\nSystems, 33:22231–22242.\nNelson Goodman. 1954. Fact, fiction, and forecast. london: University of london.\nRadu Cornel Guia¸su and Christopher W Tindale. 2018. Logical fallacies and invasion biology.\nBiology & philosophy, 33(5-6):34.\nIvan Habernal, Henning Wachsmuth, Iryna Gurevych, and Benno Stein. 2018. The argument reasoning\ncomprehension task: Identification and reconstruction of implicit warrants.\nIn Proceedings\nof the 2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1930–1940, New\nOrleans, Louisiana. Association for Computational Linguistics.\nPengrui Han, Peiyang Song, Haofei Yu, and Jiaxuan You. 2024. In-context learning may not elicit\ntrustworthy reasoning: A-not-b errors in pretrained language models.\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun,\nEkaterina Zubova, Yujie Qiao, Matthew Burtell, et al. 2022. Folio: Natural language reasoning\nwith first-order logic. arXiv e-prints, pages arXiv–2209.\nSven Ove Hansson. 2004. Fallacies of risk. Journal of Risk Research, 7(3):353–360.\n13\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. 2021a. Measuring massive multitask language understanding. Proceedings of the\nInternational Conference on Learning Representations (ICLR).\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset.\nNeurIPS.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2023. Large language models are reasoning teachers.\nDamian Hodel and Jevin West. 2023. Response: Emergent analogical reasoning in large language\nmodels.\nRuixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, and Changshui Zhang. 2024. A closer look\nat the self-verification abilities of large language models in logical reasoning. In Proceedings of the\n2024 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Papers), pages 900–925, Mexico City, Mexico.\nAssociation for Computational Linguistics.\nPeng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, and Shujian Huang. 2024. Large language\nmodels are limited in out-of-context knowledge reasoning.\nJie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song,\nand Denny Zhou. 2024. Large language models cannot self-correct reasoning yet. In The Twelfth\nInternational Conference on Learning Representations.\nDavid Hume. 1748. An enquiry concerning human understanding (section iv). Recuperado de\nhttp://www. clorenzano. com. ar.\nBowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J.\nTaylor, and Dan Roth. 2024a. A peek into token bias: Large language models are not yet genuine\nreasoners.\nJin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, Mengdi Zhang, Xunliang Cai, Yixin\nCao, Liangcai Gao, and Zhi Tang. 2024b. Logicpro: Improving complex logical reasoning via\nprogram-guided learning.\nDaniel Kahneman. 2011. Thinking, fast and slow. Macmillan.\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Her-\nnandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamil˙e Lukoši¯ut˙e, Karina\nNguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam\nMcCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy\nMaxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan, Jan Brauner,\nSamuel R. Bowman, and Ethan Perez. 2023. Measuring faithfulness in chain-of-thought reasoning.\nLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023.\nSymbolic chain-of-thought distillation: Small models can also “think” step-by-step. In Proceedings\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 2665–2679, Toronto, Canada. Association for Computational Linguistics.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\nBaolin Peng, Yi Mao, Wenhu Chen, and Xifeng Yan. 2022. Explanations from large language\nmodels make small reasoners better.\nHanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang.\n2023a. Logiqa 2.0—an improved dataset for logical reasoning in natural language understanding.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:2947–2962.\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023b. Evaluating\nthe logical reasoning ability of chatgpt and gpt-4.\nHanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang. 2023c. LogiCoT:\nLogical chain-of-thought instruction tuning. In Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 2908–2921, Singapore. Association for Computational Linguistics.\n14\n\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa:\nA challenge dataset for machine reading comprehension with logical reasoning. In Proceedings\nof the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages\n3622–3628. International Joint Conferences on Artificial Intelligence Organization. Main track.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023d. Is your code generated\nby chatGPT really correct? rigorous evaluation of large language models for code generation. In\nThirty-seventh Conference on Neural Information Processing Systems.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT\npretraining approach. arXiv preprint arXiv:1907.11692.\nZiyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, and Jieyu Zhao. 2024. Self-contradictory\nreasoning evaluation and detection.\nZimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and\nHongsheng Li. 2024. Mathgenie: Generating synthetic data with question back-translation for\nenhancing mathematical reasoning of llms.\nYINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li.\n2024. At which training stage does code data help LLMs reasoning? In The Twelfth International\nConference on Learning Representations.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.\n2023. Teaching small language models to reason. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), pages 1773–1781, Toronto,\nCanada. Association for Computational Linguistics.\nJohn W. McCarthy. 1959. Programs with common sense. In Proc. Tedding Conf. on the Mechanization\nof Thought Processes, pages 75–91.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In EMNLP.\nGeorge A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM,\n38(11):39–41.\nIman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad\nFarajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large\nlanguage models.\nMelanie\nMitchell.\n2023.\nCan\nlarge\nlanguage\nmodels\nreason?\nblog,\npages\nhttps://aiguide.substack.com/p/can–large–language–models–reason.\nArindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal,\nXuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng,\nCorby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023. Orca 2: Teaching small language\nmodels how to reason.\nPhilipp Mondorf and Barbara Plank. 2024. Liar, liar, logical mire: A benchmark for suppositional\nreasoning in large language models.\nTerufumi Morishita, Gaku Morio, Atsuki Yamaguchi, and Yasuhiro Sogawa. 2023. Learning deductive\nreasoning from synthetic corpus based on formal logic. In Proceedings of the 40th International\nConference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,\npages 25254–25274. PMLR.\nTerufumi Morishita, Atsuki Yamaguchi, Gaku Morio, Hikaru Tomonari, Osamu Imaichi, and Yasuhiro\nSogawa. 2024. JFLD: A Japanese benchmark for deductive reasoning based on formal logic. In\nProceedings of the 2024 Joint International Conference on Computational Linguistics, Language\nResources and Evaluation (LREC-COLING 2024), pages 9526–9535, Torino, Italia. ELRA and\nICCL.\n15\n\nAliakbar Nafar, K. Brent Venable, and Parisa Kordjamshidi. 2024. Teaching probabilistic logical\nreasoning to transformers. In Findings of the Association for Computational Linguistics: EACL\n2024, pages 1615–1632, St. Julian’s, Malta. Association for Computational Linguistics.\nKentaro Ozeki, Risako Ando, Takanobu Morishita, Hirohiko Abe, Koji Mineshima, and Mitsuhiro\nOkada. 2024. Exploring reasoning biases in large language models through syllogism: Insights\nfrom the NeuBAROCO dataset. In Findings of the Association for Computational Linguistics ACL\n2024, pages 16063–16077, Bangkok, Thailand and virtual meeting. Association for Computational\nLinguistics.\nFabio Paglieri. 2017. A plea for ecological argument technologies. Philosophy & Technology,\n30(2):209–238.\nMihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty,\nArindam Mitra, and Chitta Baral. 2024. LogicBench: Towards systematic evaluation of logical\nreasoning ability of large language models. In Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 13679–13707, Bangkok,\nThailand. Association for Computational Linguistics.\nNisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj\nVarshney, and Chitta Baral. 2024. Multi-logieval: Towards evaluating multi-step logical reasoning\nability of large language models.\nXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, Jian-Guang Lou, and\nWeizhu Chen. 2022. Reasoning like program executors. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pages 761–779, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nWillard Van Orman Quine. 1969. Epistemology naturalized. ontological relativity and other essays.\nNew York: Columbia UP.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable\nquestions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 2: Short Papers), pages 784–789, Melbourne, Australia. Association\nfor Computational Linguistics.\nYasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. 2022. Impact of pre-\ntraining term frequencies on few-shot numerical reasoning. In Findings of the Association for\nComputational Linguistics: EMNLP 2022, pages 840–854.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R Bowman. 2023. Gpqa: A graduate-level google-proof q&a\nbenchmark. arXiv preprint arXiv:2311.12022.\nJoshua Robinson and David Wingate. 2023. Leveraging large language models for multiple choice\nquestion answering. In The Eleventh International Conference on Learning Representations.\nMohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching\nsoft rules to pre-trained language models. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 1460–1476, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nSwarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. 2020. PRover: Proof\ngeneration for interpretable reasoning over rules. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 122–136, Online. Association\nfor Computational Linguistics.\n16\n\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106.\nSoumya Sanyal, Zeyi Liao, and Xiang Ren. 2022a. Robustlr: Evaluating robustness to logical\nperturbation in deductive reasoning. arXiv preprint arXiv:2205.12598.\nSoumya Sanyal, Harman Singh, and Xiang Ren. 2022b.\nFairr: Faithful and robust deductive\nreasoning over natural language. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1075–1093.\neh Shortliffe. 1976. Computer based medical consultations: Mycin. Elsevier.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2023. Distilling reasoning capabilities\ninto smaller language models. In Findings of the Association for Computational Linguistics: ACL\n2023, pages 7059–7073, Toronto, Canada. Association for Computational Linguistics.\nDamien Sileo. 2024. Scaling synthetic logical reasoning datasets with context-sensitive declarative\ngrammars.\nZayne Rea Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. 2024. MuSR:\nTesting the limits of chain-of-thought with multistep soft reasoning. In The Twelfth International\nConference on Learning Representations.\nCass R Sunstein and Reid Hastie. 2015. Wiser: getting beyond groupthink to make groups smarter.\nHarvard Business Review Press, Boston.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. 2022. Challenging\nbig-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs,\nand abductive statements over natural language. In Findings of the Association for Computa-\ntional Linguistics: ACL-IJCNLP 2021, pages 3621–3634, Online. Association for Computational\nLinguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018.\nCommon-\nsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint\narXiv:1811.00937.\nJidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin. 2021. Diagnosing the\nfirst-order logical reasoning ability through logicnli. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pages 3738–3747.\nTrieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. 2024. Solving olympiad geometry\nwithout human demonstrations. Nature, 625(7995):476–482.\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language models don’t\nalways say what they think: Unfaithful explanations in chain-of-thought prompting.\nFumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, and Yutaka\nMatsuo. 2024. Which programming language and what features at pre-training stage affect\ndownstream logical inference performance?\nYuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen tse Huang, Pinjia He, Wenxiang Jiao,\nand Michael R. Lyu. 2024. Logicasker: Evaluating and improving the logical reasoning ability of\nlarge language models.\nHaochun Wang, Sendong Zhao, Zewen Qiang, Bing Qin, and Ting Liu. 2024a. Beyond the answers:\nReviewing the rationality of multiple choice question answering for the evaluation of large language\nmodels.\nPeifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT:\nSelf-consistent chain-of-thought distillation. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 5546–5558, Toronto,\nCanada. Association for Computational Linguistics.\n17\n\nSiyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. 2024b. Can LLMs reason with rules? logic\nscaffolding for stress-testing and improving LLMs. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 7523–7543,\nBangkok, Thailand. Association for Computational Linguistics.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi\nFan, Xiang Yue, and Wenhu Chen. 2024c. Mmlu-pro: A more robust and challenging multi-task\nlanguage understanding benchmark (published at neurips 2024 track datasets and benchmarks).\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\nQuoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language\nmodels. In Advances in Neural Information Processing Systems.\nJoseph Weizenbaum. 1966. Eliza—a computer program for the study of natural language communi-\ncation between man and machine. Communications of the ACM, 9(1):36–45.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science\nquestions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 94–106,\nCopenhagen, Denmark. Association for Computational Linguistics.\nJason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand\nJoulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite\ntoy tasks. arXiv preprint arXiv:1502.05698.\nAdina Williams, Nikita Nangia, and Samuel R Bowman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In Proceedings of NAACL-HLT, pages 1112–1122.\nT Winograd. 1971. Procedures as a representation for data in a computer program for understanding\nnatural language, mit ai technical report 235.\nLudwig Wittgenstein. 1922. Tractatus Logico Philosophicus: Logical-Philosophical Treatise. Really\nSimple Media.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art\nnatural language processing. In Empirical Methods in Natural Language Processing: System\nDemonstrations, pages 38–45.\nZhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim,\nJacob Andreas, and Yoon Kim. 2023. Reasoning or reciting? exploring the capabilities and\nlimitations of language models through counterfactual tasks.\nHitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, and\nJohan Bos. 2019. Help: A dataset for identifying shortcomings of neural models in monotonicity\nreasoning. arXiv preprint arXiv:1904.12166.\nNathan Young, Qiming Bao, Joshua Bensemann, and Michael J Witbrock. 2022. Abductionrules:\nTraining transformers to explain unexpected inputs. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022, pages 218–227.\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension\ndataset requiring logical reasoning. In International Conference on Learning Representations\n(ICLR).\nZhangdie Yuan, Songbo Hu, Ivan Vuli´c, Anna Korhonen, and Zaiqiao Meng. 2023. Can pretrained\nlanguage models (yet) reason deductively? In Proceedings of the 17th Conference of the European\nChapter of the Association for Computational Linguistics, pages 1439–1454.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4791–4800.\n18\n\nHonghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. 2022. On\nthe paradox of learning to reason from data.\nHugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav\nRaja, Dylan Slack, Qin Lyu, Sean Hendryx, Russell Kaplan, Michele Lunati, and Summer Yue.\n2024. A careful examination of large language model performance on grade school arithmetic.\nJun Zhao, Jingqi Tong, Yurong Mou, Ming Zhang, Qi Zhang, and Xuanjing Huang. 2024a. Exploring\nthe compositional deficiency of large language models in mathematical reasoning.\nWenting Zhao, Justin Chiu, Jena Hwang, Faeze Brahman, Jack Hessel, Sanjiban Choudhury, Yejin\nChoi, Xiang Li, and Alane Suhr. 2024b. UNcommonsense reasoning: Abductive reasoning about\nuncommon situations. In Proceedings of the 2024 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (Volume 1: Long\nPapers), pages 8487–8505, Mexico City, Mexico. Association for Computational Linguistics.\nChujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024. Large language models\nare not robust multiple choice selectors. In The Twelfth International Conference on Learning\nRepresentations.\nWanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming\nZhou, and Nan Duan. 2021. Ar-lsat: Investigating analytical reasoning of text. arXiv preprint\narXiv:2104.06598.\nJin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy, Kilian Q Weinberger, and Yuhuai Wu.\n2024a. Don’t trust: Verify – grounding LLM quantitative reasoning with autoformalization. In The\nTwelfth International Conference on Learning Representations.\nYue Zhou, Yada Zhu, Diego Antognini, Yoon Kim, and Yang Zhang. 2024b. Paraphrase and solve:\nExploring and exploiting the impact of surface form on mathematical reasoning in large language\nmodels. In Proceedings of the 2024 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages\n2793–2804, Mexico City, Mexico. Association for Computational Linguistics.\nKaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. 2024. Dyval:\nDynamic evaluation of large language models for reasoning tasks. In The Twelfth International\nConference on Learning Representations.\n19\n\nA\nRelated Work\nA.1\nInvestigation of Reasoning Capabilities of LLMs\nMany studies examine LLMs’ reasoning capabilities (Askell, 2020; Rae et al., 2021; Razeghi et al.,\n2022; Liu et al., 2023b; Turpin et al., 2023; Lanham et al., 2023; Wu et al., 2023; Hodel and West,\n2023; Dziri et al., 2023; Dasgupta et al., 2023). Patel et al. (2024) observed LLMs’ performance\nsignificantly declines as reasoning steps increase in multi-step logical reasoning tasks. Dougrez-\nLewis et al. (2024) revealed ChatGPT struggles with abductive reasoning when verifying claims by\ndecomposing their evidence into atomic reasoning steps. Wang et al. (2024b) found that GPT-series\nmodels showed significant gaps compared to humans in dealing with inference rules. Parmar et al.\n(2024) introduced LogicBench and showed that existing LLMs struggle with instances involving\ncomplex reasoning and negations. Wan et al. (2024) introduced LogicAsker, which assesses whether\nLLMs can employ a set of atomic reasoning skills grounded in propositional and predicate logic and\nfound significant gaps in LLMs’ learning of logical rules. Bhuiya et al. (2024) proposed a challenging\nmulti-hop reasoning benchmark with seemingly plausible but incorrect multi-hop reasoning chains\nand found that state-of-the-art LLMs’ capabilities to perform multi-hop reasoning is affected by such\nchains. Mondorf and Plank (2024) introduced TruthQuest, which assesses LLMs’ capabilities to\nconduct suppositional reasoning, i.e., reasoning where each statement can be false, and found that\nLLMs exhibit significant difficulties solving these tasks. Sprague et al. (2024) introduced a complex\nmulti-step reasoning benchmark, MuSR, and characterized the gaps that remain for techniques like\nchain-of-thought to perform robust reasoning.\nBiases and Errors\nAndo et al. (2023); Ozeki et al. (2024); Bertolazzi et al. (2024); Eisape et al.\n(2024) found that LLMs exhibit human-like reasoning biases in syllogistic arguments. Jiang et al.\n(2024a) found that LLMs exibit “token-biases” in solving logical reasoning problems. Aoki et al.\n(2024) revealed that LMs rely heavily on heuristics, such as lexical overlap, in the earlier stages of\nreasoning. Zhao et al. (2024a) constructed a MATHTRAP with carefully designed logical traps into\nthe problem descriptions of MATH and GSM8k and found that while LLMs possess the knowledge\nrequired to solve these traps, they do not spontaneously use such knowledge them to handle the\nproblems. Han et al. (2024) found that LLMs exhibit A-Not-B errors similar to human infants, failing\nto suppress the previously established response pattern during ICL. Liu et al. (2024) found that LLMs\noften contradict themselves in reasoning tasks involving contextual information understanding or\ncommonsense. Zhou et al. (2024b) found that subtle alterations in the surface form can significantly\nimpact the answer distribution, suggesting that LLMs solve reasoning problems using surface cues.\nChen et al. (2024) found that the reasoning performance of LLMs is affected by the order of the\npremises. Hong et al. (2024); Huang et al. (2024) found that LLMs struggle to identify fallacious\nreasoning steps accurately, suggesting challenges in self-verification methods.\nReasoning in Unknown Situation\nZhao et al. (2024b) found that LLMs struggle with reasoning in\nuncommon situations. Zhu et al. (2024) introduced a framework to dynamically generate reasoning\nsamples, and LLMs perform worse in those samples. Hu et al. (2024) found that while LLMs can\nconduct reasoning when relevant knowledge is given in context, they are not proficient at reasoning\nwith knowledge embedded in the training data.\nA.2\nSynthetic Logic Corpus for Training LLMs\nLater studies (Saha et al., 2020; Dalvi et al., 2021; Tafjord et al., 2021; Sanyal et al., 2022b) showed\nthat T5 can generate even the intermediate logical steps as well as the final answer.\nPARARULE-Plus (Bao et al., 2022) is the enhanced version of PARARULE (Clark et al., 2021), a\nvariation of RuleTaker, that includes more samples and more logical steps. RoBERTa (Liu et al.,\n2019) trained on PARARULE-Plus outperformed the models trained on RuleTaker.\nArtificial Argument Corpus (Betz et al., 2021) includes single-step deductive reasoning samples\nconstructed from hand-selected deduction rules useful for critical thinking. They showed that the\nGPT-2 (Radford et al., 2019) trained on this corpus can generalize to solve NLI tasks. However, at\nthe same time, they found that the LM does not generalize well to solve more challenging reasoning\ntasks such as ARC (Habernal et al., 2018) and LogiQA (Liu et al., 2020).\n20\n\nFLD by Morishita et al. (2023, 2024) is the first synthetic logic corpus based on formal logic theory.\nIt includes multistep deductive reasoning samples constructed from the axioms of first-order predicate\nlogic, which can express any deduction rule due to the completeness theorem. Due to this nature, T5\ntrained on FLD generalizes most effectively to other synthetic logic corpora, compared to models\ntrained on other corpora.\nGontier et al. (2020) investigated the deductive reasoning capabilities of LMs on a corpus composed\nof a specific type of multistep inference, kinship relationships on synthetic kinship graphs. They\nfound that LMs can solve this task when there are relatively few proof steps, but it is difficult for\nthem to generalize to solve proof steps longer than those shown in training data. Bostrom et al. (2021)\nstudied how to create realistic natural language expressions that represent deduction rules. To this\nend, they scraped sentences from Wikipedia using a template-based method and paraphrased them.\nThey showed that training on this corpus helps solve real-world deductive reasoning problems such\nas EntailmentBank (Dalvi et al., 2021). Pi et al. (2022) used synthetic data from program executors,\nmost notably SQL programs. They verified that this data can enhance numerical reasoning, logical\nreasoning, and multi-hop reasoning abilities. Trinh et al. (2024) generated 100 million geometry\nproblems and verified that the capability of artificial intelligence can be enhanced to to pass the bronze\nmedal threshold of the International Mathematics Olympiad. Saeed et al. (2021); Nafar et al. (2024)\ncreated soft reasoning rules involving with probabilistic logic, instead of hard-logic examined by the\naformentioned studies. Sileo (2024) introduced a simpler and more general declarative framework\nfor synthetic generation, and verified its effectiveness. Zhou et al. (2024a) synthetically generated a\nlarge dataset of mathematics, and gained over 12 points on GSM8k.\nWhile these studies partly examined the effect of synthetic logic corpora, whether this approach is\npromising remains an open question. It has been unexplored whether the capabilities obtained from\nsynthetic logic corpora generalizes to solve various tasks beyond the original tasks in these corpora.\nAdditionally, the effect of these corpora has only been examined for small LMs trained on small\npre-training corpora such as T5 and RoBERTa; it has been highly questionable whether they can\nstill benefit state-of-the-art LLMs trained on a huge pre-training corpus. Furthermore, even if their\nbenefits were verified, it remains unclear which design of synthetic logic samples yields the largest\nbenefits due to the lack of systematic discussions on sample designs and empirical verification of\nthese designs. We aimed to answer these questions in this paper and demonstrate the potential of\nsynthetic logic corpora.\nA.3\nDistilling Reasoning Traces from Very Large LLMs\nRecent approaches (Ho et al., 2023; Magister et al., 2023; Li et al., 2022, 2023; Shridhar et al.,\n2023; Wang et al., 2023; Mitra et al., 2023; Liu et al., 2023c; Ben Allal et al., 2024; Lu et al., 2024)\nutilize very large LLMs, such as GPT-4, to prepare synthetic reasoning datasets to train smaller\nLLMs. A typical procedure is as follows: (i) prepare existing reasoning problems, (ii) prompt large\nLLMs to generate reasoning traces to solve these problems using techniques such as chain-of-thought\nprompting (Wei et al., 2022), and (iii) train smaller LLMs on these reasoning traces.\nThe distillation approach and the synthetic logic corpora approach examined in this paper have\nspecific advantages and disadvantages, as follows.\nThe advantage of the distillation approach is its immediate practical effect, as it directly teaches\nLLMs solutions to various existing problems. The disadvantages could be that (i) it is non-trivial for\nspecific solutions to specific problems to generalize to other problems, (ii) the number of training\nsamples is limited to existing problems in nature, (iii) the correctness and faithfulness of the reasoning\ntraces are not guaranteed; indeed, some studies (Turpin et al., 2023; Lanham et al., 2023) suggest that\nlarge LLMs do not always faithfully follow the “reasoning traces” they themselves generate, and (iv)\nit cannot enhance the very large LLMs themselves by nature.\nThe advantages of synthetic logic corpus approaches are that (i) since they teach the fundamentals of\nreasoning, such as deductive reasoning, they have the potential to generalize to various problems,\n(ii) they can generate an unlimited number of new samples, and (iii) the correctness of the reasoning\ntraces is guaranteed by nature. The disadvantage of this approach is that, as it only teaches the basics\nof reasoning, additional training may be needed to solve more complex real-world problems, as\nsuggested in Section 6.4.\n21\n\nWe hypothesize that integrating both approaches could be promising. That is, we first train LLMs\nusing ALT to make them understand the fundamentals of reasoning through high-quality samples and\nthen train them using more realistic reasoning traces to solve complex real-world problems.\nB\nLimitations\n• We only used deductive reasoning samples for ALT. Future work should examine other\nreasoning samples, e.g., abductive and inductive reasoning.\n• We only examined the first-order predicate logic system. Future work should examine other\nlogic systems, such as modal and linear logic.\nC\nEthics and Social Impacts\nThe ultimate goal of the direction of this study is to develop an AI capable of reasoning logically\nstep by step. If AI can make a decision one logical step at a time, it would be highly explainable and\ntransparent to users. Furthermore, the user would be able to trace the AI’s errors. We believe that our\nstudy is a step towards such AI that will positively impact society.\nD\nDetails of Formal Logic Deduction Diverse\nFigure D.3 shows a real sample from FLD×2. Below, We briefly explain our sample generator. Please\nrefer to Morishita et al. (2023) for the details.\nD.1\nAnswer Labels\nIn addition to the logical steps, the samples of FLD×2 and previous corpora include answer labels\n(Figure D.3): “proved” indicating that the hypothesis can be proved by the logical steps, “disproved”\nindicating that the hypothesis can be disproved, and “unknown” indicating that the given facts are\ninsufficient for either proving or disproving the hypothesis. For samples with “unknown” labels, the\nlogical steps are “None.”. FLD×2 have a uniform distribution over the labels.\nD.2\nSplits\nFLD×2 includes 100k/5k/5k samples for train/valid/test splits.\nD.3\nGeneration of Multistep Deduction\nOur sample generator first randomly generates examples of multistep deduction by forward- and\nbackward random deduction, using the deduction rules specified by a user.\nThe forward random deduction is done as follows. The generator first chooses a deduction rule\nrandomly and forms the initial tree where the root node is the conclusion of the chosen deduction\nrules and the child nodes are the premises of the chosen deduction rule. The generator next randomly\nchooses another deduction rule that can be “jointed” to the root note of the tree. A deduction rule can\nbe jointed to the root node of a tree if one of the premises of that deduction rule can be identified\nwith the root node. Then, the generator updates the tree by jointing this chosen deduction rule. The\ngenerator continues this step multiple times until the tree achieves the required depth.\nThe backward random deduction is done as follows. For each step, the generator randomly chooses a\nleaf node of the tree. Then, the generator randomly chooses a deduction rule that can be jointed to\nthe leaf node. Here, a deduction rule can be jointed to the leaf node if the deduction rule’s conclusion\ncan be identified with the leaf node. Then, the generator updates the tree by jointing this chosen\ndeduction rule. The generator continues this step multiple times until the complexity of branches\nachieves the required level.\n22\n\nFigure D.3: A real deduction sample included in Formal Logic Deduction Diverse. Facts and\nhypothesis are given to LLMs, then the LLMs are required to generate logical steps to (dis-)prove\nthe hypothesis based on the facts, and an answer label (see Appendix D.2).\nD.4\nLinguistic Expressions\nWe prepared linguistic templates for each logical formula, exemplified as follows:\n⟨(A ∧B) →C⟩: If ⟨(A ∧B).predicate_phrase⟩, then ⟨C.predicate_phrase⟩.\n: ⟨(A ∧B).noun_pharse⟩⟨cause_synonyms⟩⟨C.noun_phrase⟩.\n: (. . . )\n⟨(A ∧B).predicate_phrase⟩: A ⟨occur_synonyms⟩and also B ⟨occur_synonyms⟩.\n: A and also B ⟨occur_synonyms⟩.\n: Both A and B ⟨occur_synonyms⟩.\n: (. . . )\n⟨C.predicate_phrase⟩: C ⟨occur_synonyms⟩.\n: (. . . )\n⟨occur_synonyms⟩: occur\n: happen\n: take place\n: (. . . )\n⟨(A ∧B).noun_pharse⟩: A and B\n: A and also B\n: Both A and B\n: That A and B ⟨occur_synonyms⟩\n: (. . . )\n⟨cause_synonyms⟩: cause\n: result in\n: lead to\n: bring about\n: (. . . )\n( . . . )\n(D.1)\nAs can be seen, the templates can be nested deeply, yielding combinatorially diverse linguistic\nexpressions.\nExpanding these templates beforehand is intractable due to the combinatorial explosion, so we expand\nthese templates on the fly to randomly sample a single expression at a time. Estimating the exact\nnumber of expressions is intractable for the same reason.\nWe manually crafted several additional English templates per logical formula (i.e., the left-hand\nsides of (D.1)) compared to those used in FLD, which yield combinatorially more diverse English\n23\n\nexpressions. We observed that at least dozens of expressions, including minor variations, are yielded\nfor each formula.\nE\nDetails of Experimental Setup\nE.0.1\nPrevention of Knowledge Forgetting by Recall Adam Optimizer\nWe employed the Recall Adam (RecAdam) optimizer (Chen et al., 2020), which regularizes parameter\nupdates to prevent them from being too far from the pre-training parameters. Recall Adam stands out\nfor LLM training as it does not require access to the pre-training corpus, which is often inaccessible\nor too huge to handle, nor does it require changes to the model architecture, and it has a proven track\nrecord of usage in language models such as BERT.\nE.1\nBenchmarks\nTable E.7 details the benchmarks used in the experiments.\nE.2\nExperimental Runs\nWe show the average and standard deviations over five seeds.\nE.3\nComputational Resources\nThe entire experiment, including preliminary ones, took about 1 week x 128 NVIDIA H100 GPUs of\nour own.\nF\nResults without using Recall Adam\nTable F.8 shows the results of LLMs trained without using Recall Adam.\n24\n\nTable E.7: 31 benchmarks used in the experiments. These benchmarks cover a wide range of tasks and\nare prominent for LLM evaluation. We also show the form of reasoning and the type of knowledge\nrequired to solve the problems in each benchmark.\nSet\nBenchmarks\nReasoning\nform\nRequired\nknowledge\nLogic\nbAbi deduction (Weston et al., 2015),\ndeduction\n-\n(not required)\nFOLIO (Han et al., 2022)\nLogicNLI (Tian et al., 2021)\nRobustLR (Sanyal et al., 2022a)\nAR-LSAT (Zhong et al., 2021)\ncommonsense\nLogiQA2 (Liu et al., 2023a)\nReClor (Yu et al., 2020)\nAbductionRules (Young et al., 2022)\nabduction\nART (Bhagavatula et al., 2019)\ncommonsense\nNLI\nHELP (Yanaka et al., 2019)\nvalidate\na conclusion\nbased on\ngiven premises\ncommonsense\nMultiNLI (Williams et al., 2018)\nRTE (Dagan et al., 2005; Giampiccolo et al., 2007; Bentivogli et al., 2009)\nSNLI (Bowman et al.)\nMath\nGSM8k (Cobbe et al., 2021)\nMath\nMath\nMATH (Hendrycks et al., 2021b)\nMathQA (Amini et al., 2019)\nCoding\nHumanEval (Chen et al., 2021)\nCoding\nCoding\nMBPP (Austin et al., 2021)\nMBPP+ (Liu et al., 2023d)\nMultiPL-E (cpp/go) (Cassano et al., 2023)\nOthers\nCommonsenseQA (Talmor et al., 2018)\ncomplicated\nprocedures\ncommonsense\nHellaSWAG (Zellers et al., 2019)\nSQuAD2 (Rajpurkar et al., 2018)\nWinoGrande (Sakaguchi et al., 2021)\nARC (easy/challenge) (Clark et al., 2018)\nscience\nGPQA (Rein et al., 2023)\nOpenBookQA (Mihaylov et al., 2018)\nSciQ (Welbl et al., 2017)\naggregated\nMMLU (Hendrycks et al., 2021a)\nvarious\nvarious\nMMLU-Pro (Wang et al., 2024c)\nBBH (Suzgun et al., 2022)\n25\n\nTable F.8: 5-shot performance of LLMs before and after ALT. ⊕ALT-x denotes the LLM trained with\nALT on the synthetic logic corpus x from Table 1. Color shows the rank in each column (darker is\nbetter). “Logic”, “Math”, “Code”, and “Others” each comprises various benchmarks (see Table E.7).\n“Avg.” represents the micro-average of all the benchmarks. “w/o RecAdam” denotes that LLM was\ntrained without knowledge forgetting prevention by Recall Adam optimizer.\n(a) LLaMA-3.1-8B.\nAvg.\nLogic\nMath\nCode\nNLI\nOthers\nBBH (3-shot)\nBBH (0-shot)\nMMLU\nCoT\nCoT\nPro\nLLaMA-3.1-8B\n47.9\n42.8±0.4\n39.6±0.5\n35.4\n65.4±0.3\n60.7±0.3\n44.9±0.4\n61.9±0.4\n8.2±0.2\n36.5±0.4\n65.3±0.4\n35.8±0.4\n⊕ALT-PRP w/o RecAdam\n43.5\n39.5±0.2\n29.1±0.3\n35.3\n57.8±0.2\n61.0±0.2\n40.5±0.2\n47.0±0.2\n3.9±0.1\n6.3±0.1\n64.9±0.2\n34.0±0.2\n⊕ALT-PRP\n48.1\n43.7±0.2\n39.2±0.3\n35.7\n65.6±0.2\n60.8±0.2\n44.9±0.2\n61.8±0.2\n8.2±0.1\n36.4±0.2\n65.3±0.2\n35.3±0.2\n⊕ALT-RT\n50.1\n46.8±0.1\n42.4±0.2\n36.5\n68.6±0.1\n61.3±0.1\n46.9±0.2\n63.5±0.2\n13.7±0.1\n38.4±0.2\n65.3±0.1\n35.7±0.2\n⊕ALT-FLD\n51.9\n51.6±0.1\n43.4±0.2\n38.1\n70.1±0.1\n61.5±0.1\n46.7±0.2\n64.9±0.2\n11.9±0.1\n39.6±0.2\n65.4±0.1\n36.2±0.2\n⊕ALT-FLD×2\n52.0\n52.2±0.1\n43.2±0.2\n38.0\n70.7±0.1\n61.5±0.1\n46.5±0.2\n65.3±0.2\n11.3±0.1\n38.7±0.2\n65.5±0.1\n36.4±0.2\n(b) LLaMA-3.1-70B.\nAvg.\nLogic\nMath\nCode\nNLI\nOthers\nBBH (3-shot)\nBBH (0-shot)\nMMLU\nCoT\nCoT\nPro\nLLaMA-3.1-70B\n60.0\n57.4±0.4\n60.0±0.5\n46.2\n73.7±0.3\n67.7±0.3\n60.4±0.3\n82.1±0.2\n6.5±0.1\n50.1±0.3\n78.7±0.3\n50.7±0.4\n⊕ALT-PRPw/o RecAdam\n58.8\n54.3±0.4\n59.2±0.5\n48.2\n72.7±0.3\n65.9±0.3\n60.4±0.4\n81.5±0.3\n6.1±0.2\n48.3±0.4\n78.5±0.3\n50.7±0.4\n⊕ALT-PRP\n60.4\n57.7±0.4\n59.8±0.5\n49.2\n73.5±0.3\n67.6±0.3\n60.4±0.4\n82.2±0.3\n6.0±0.2\n50.1±0.4\n78.7±0.3\n50.9±0.4\n⊕ALT-RT\n62.7\n61.4±0.2\n62.1±0.3\n50.8\n75.4±0.2\n68.4±0.2\n64.1±0.3\n82.5±0.2\n11.5±0.2\n59.2±0.3\n79.0±0.2\n52.4±0.3\n⊕ALT-FLD\n64.2\n65.7±0.1\n63.6±0.2\n52.0\n75.3±0.1\n68.5±0.1\n65.0±0.2\n83.6±0.1\n12.1±0.1\n59.9±0.2\n79.3±0.1\n54.4±0.2\n⊕ALT-FLD×2\n64.4\n66.1±0.1\n63.3±0.2\n52.4\n76.1±0.1\n68.5±0.1\n65.4±0.2\n83.6±0.2\n11.4±0.1\n60.8±0.2\n79.5±0.1\n54.4±0.2\nTable F.9: Benchmark-wise 5-shot performance of LLaMA-3.1-8B before and after ALT on FLD×2.\n(a) Logic.\nbAbiD FOLIO LogicNLI RobustLR AR-LSAT LogiQA ReClor AbductionR\nART\nLLaMA-3.1-8B 48.7±1.6 50.0±1.6\n28.5±1.0\n43.2±0.9\n20.7±1.0\n39.6±1.2 28.7±0.7\n52.4±0.9\n73.4±1.1\n⊕ALT-FLD×2 55.8±0.6 54.5±0.6\n42.0±0.4\n62.6±0.3\n21.1±0.4\n42.8±0.4 29.4±0.2\n85.5±0.2\n76.1±0.4\n(b) Math.\nGSM8k\nMATH MathQA\nCoT\nCoT (0-shot)\n-\n-\nLLaMA-3.1-8B 50.2±1.4 51.5±1.4\n39.5±1.3\n14.1±0.5\n42.8±0.9\n⊕ALT-FLD×2\n53.6±0.5 56.4±0.5\n48.4±0.5\n14.3±0.2\n43.3±0.3\n(c) Coding.\nHumanEval\nMBPP\nMBPP+\nMultiPL-E (cpp)\nMultiPL-E (go)\nLLaMA-3.1-8B\n22.6\n31.6\n38.1\n21.7\n63.0\n⊕ALT-FLD×2\n25.9\n34.0\n39.9\n23.0\n67.1\n(d) Natural language inference (NLI).\nHELP\nMNLI\nRTE\nSNLI\nLLaMA-3.1-8B\n46.4±0.5\n68.1±0.5\n74.6±0.9\n72.6±0.4\n⊕ALT-FLD×2\n47.9±0.2\n75.3±0.2\n83.1±0.3\n76.5±0.1\n(e) Others.\nCommonsenseQA HellaSwag SQuAD WinoGrande ARCe\nARCc\nGPQA OpenBookQA\nSciQ\nLLaMA-3.1-8B\n73.9±1.3\n61.2±0.5\n30.8±0.0\n77.4±1.2\n84.2±0.7 54.7±1.5 31.1±1.3\n35.3±0.7\n97.7±0.5\n⊕ALT-FLD×2\n74.8±0.4\n61.5±0.2\n33.5±0.0\n78.1±0.5\n85.0±0.3 55.6±0.5 31.1±0.5\n36.3±0.2\n97.6±0.2\n26\n\nNeurIPS Paper Checklist\nThe checklist is designed to encourage best practices for responsible machine learning research,\naddressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove\nthe checklist: The papers not including the checklist will be desk rejected. The checklist should\nfollow the references and precede the (optional) supplemental material. The checklist does NOT\ncount towards the page limit.\nPlease read the checklist principles carefully for information on how to answer these questions. For\neach question in the checklist:\n• You should answer [Yes] , [No] , or [NA] .\n• [NA] means either that the question is Not Applicable for that particular paper or the\nrelevant information is Not Available.\n• Please provide a short (1–2 sentence) justification right after your answer (even for NA).\nThe checklist answers are an integral part of your paper submission. They are visible to the\nreviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it\n(after eventual revisions) with the final version of your paper, and its final version will be published\nwith the paper.\nThe reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.\nWhile \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a\nproper justification is given (e.g., \"error bars are not reported because it would be too computationally\nexpensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering\n\"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we\nacknowledge that the true answer is often more nuanced, so please just use your best judgment and\nwrite a justification to elaborate. All supporting evidence can appear either in the main paper or the\nsupplemental material, provided in appendix. If you answer [Yes] to a question, in the justification\nplease point to the section(s) where related material for the question can be found.\nIMPORTANT, please:\n• Delete this instruction block, but keep the section heading “NeurIPS paper checklist\",\n• Keep the checklist subsection headings, questions/answers and principles below.\n• Do not modify the questions and only use the provided macros for your answers.\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper’s contributions and scope?\nAnswer: [Yes]\nJustification: Claims stated in Section 1 is supported by the experimental results in Sections 5,\n6.\nprinciples:\n• The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n• The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n• The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n• It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\n27\n\nJustification: Appendix B\nprinciples:\n• The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n• The authors are encouraged to create a separate \"Limitations\" section in their paper.\n• The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n• The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n• The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n• The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n• If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n• While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren’t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\nJustification: Our paper does not include theoretical results.\nprinciples:\n• The answer NA means that the paper does not include theoretical results.\n• All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n• All assumptions should be clearly stated or referenced in the statement of any theorems.\n• The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n• Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n• Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: Section 4, appendix E. Further, we release all the resources, including (i) the\ncorpus, (ii) the trained model, and (iii) code for corpus generation, LLM training, and LLM\nevaluation 3.\n3https://anonymous.4open.science/r/ALT/README.md\n28\n\nprinciples:\n• The answer NA means that the paper does not include experiments.\n• If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n• If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n• Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n• While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: we release the code, data, and model.\nprinciples:\n• The answer NA means that paper does not include experiments requiring code.\n• Please see the NeurIPS code and data submission principles (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n• While we encourage the release of code and data, we understand that this might not be\npossible, so “No” is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n• The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission principles (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n• The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n• The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n29\n\n• At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n• Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Section 4, appendix E.\nprinciples:\n• The answer NA means that the paper does not include experiments.\n• The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n• The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: As stated in Appendix E.\nprinciples:\n• The answer NA means that the paper does not include experiments.\n• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n• The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n• The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n• The assumptions made should be given (e.g., Normally distributed errors).\n• It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n• It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96 % CI, if the\nhypothesis of Normality of errors is not verified.\n• For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n• If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: Appendix E.3.\nprinciples:\n• The answer NA means that the paper does not include experiments.\n30\n\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n• The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n• The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn’t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/Ethicsprinciples?\nAnswer: [Yes]\nJustification:\nprinciples:\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n• If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n• The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: Appendix C\nprinciples:\n• The answer NA means that there is no societal impact of the work performed.\n• If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n• Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n• The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n• The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n• If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [No]\nJustification:\n31\n\nprinciples:\n• The answer NA means that the paper poses no such risks.\n• Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage principles or restrictions to access the model or implementing\nsafety filters.\n• Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n• We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: All the corpora and benchmarks used in the experiments properly state their\nlicenses.\nprinciples:\n• The answer NA means that the paper does not use existing assets.\n• The authors should cite the original paper that produced the code package or dataset.\n• The authors should state which version of the asset is used and, if possible, include a\nURL.\n• The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n• For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n• If assets are released, the license, copyright information, and terms of use in the package\nshould be provided. For popular datasets, paperswithcode.com/datasets has\ncurated licenses for some datasets. Their licensing guide can help determine the license\nof a dataset.\n• For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n• If this information is not available online, the authors are encouraged to reach out to\nthe asset’s creators.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: We will release our corpus.\nprinciples:\n• The answer NA means that the paper does not release new assets.\n• Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n• The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n• At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\n32\n\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing nor research with human objects.\nprinciples:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: THis paper does not involve crowdsourcing nor research with human objects.\nprinciples:\n• The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n• Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n• We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nprinciples for their institution.\n• For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n33",
    "pdf_filename": "Enhancing_Reasoning_Capabilities_of_LLMs_via_Principled_Synthetic_Logic_Corpus.pdf"
}