{
    "title": "Fast Convergence of Softmax Policy Mirror Ascent",
    "context": "Natural policy gradient (NPG) is a common policy optimization algorithm and can be viewed as mirror ascent in the space of prob- abilities. Recently, Vaswani et al. [2021] in- troduced a policy gradient method that cor- responds to mirror ascent in the dual space of logits. We refine this algorithm, removing its need for a normalization across actions and analyze the resulting method (referred to as SPMA). For tabular MDPs, we prove that SPMA with a constant step-size matches the linear convergence of NPG and achieves a faster convergence than constant step-size (ac- celerated) softmax policy gradient. To handle large state-action spaces, we extend SPMA to use a log-linear policy parameterization. Un- like that for NPG, generalizing SPMA to the lin- ear function approximation (FA) setting does not require compatible function approxima- tion. Unlike MDPO, a practical generalization of NPG, SPMA with linear FA only requires solv- ing convex softmax classification problems. We prove that SPMA achieves linear conver- gence to the neighbourhood of the optimal value function. We extend SPMA to handle non-linear FA and evaluate its empirical per- formance on the MuJoCo and Atari bench- marks. Our results demonstrate that SPMA consistently achieves similar or better perfor- mance compared to MDPO, PPO and TRPO. 1 Policy gradient (PG) methods [Williams, 1992; Sut- ton et al., 1999; Konda and Tsitsiklis, 2000; Kakade, 2001] have been critical to the achievements of rein- forcement learning (RL). Although the PG objective is non-concave, recent theoretical research [Agarwal et al., 2021; Mei et al., 2020, 2021; Bhandari and Russo, 2021; Lan, 2023; Shani et al., 2020; Liu et al., 2024; Lu et al., 2024; Alfano and Rebeschini, 2022; Yuan et al., 2023] has analyzed PG methods in simplified settings and demonstrated their global convergence to an optimal policy. While such simplified analyses are helpful in understanding the underlying optimization issues, the resulting methods are rarely used in practice. On the other hand, while methods such as TRPO [Schulman, 2015], PPO [Schulman et al., 2017], MDPO [Tomar et al., 2020] are commonly used in deep RL, their theoreti- cal analysis in the function approximation setting is quite limited. In particular, existing work either (i) analyzes these methods only in the impractical tab- ular setting [Tomar et al., 2020; Shani et al., 2020] or (ii) modifies these algorithms to make them more amenable to theoretical analysis [Liu et al., 1906; Zhong and Zhang, 2024]. Unfortunately, these modified al- gorithms are quite different from the original variants and are not systematically benchmarked on standard environments. Consequently, there exists a large gap between PG methods that have theoretical guarantees in realistic settings versus those which are implemented in practice. To make matters worse, it has been demon- strated that code-level implementation details impact the empirical performance more than the underlying algorithm [Engstrom et al., 2019]. Designing theoretically principled PG algorithms that simultaneously have good empirical performance on the standard set of benchmarks is the main motivation be- hind this work. To that end, we leverage an algorithm first proposed by Vaswani et al. [2021], which we modify to remove the need for normalization. We coin this re- finement Softmax Policy Mirror Ascent (referred to as SPMA). We show that SPMA has comparable convergence guarantees as existing theoretical techniques [Lu et al., 2024; Yuan et al., 2023] in the tabular and function approximation settings, while achieving comparable practical performance as PPO, TRPO and MDPO, without additional algorithmic modifications. In particular, we make the following contributions. Contribution 1: In Section 3, we focus on the multi- armed bandit and tabular MDP settings, where the number of parameters scales with the number of states and actions. We develop the SPMA algorithm, which parameterizes the policy using the softmax function arXiv:2411.12042v1  [cs.LG]  18 Nov 2024",
    "body": "Fast Convergence of Softmax Policy Mirror Ascent\nReza Asad1\nReza Babanezhad2\nIssam Laradji3\nNicolas Le Roux4\nSharan Vaswani1\n1Simon Fraser University\n2 Samsung AI\n3 ServiceNow\n4Mila, Université de Montréal, McGill\nAbstract\nNatural policy gradient (NPG) is a common\npolicy optimization algorithm and can be\nviewed as mirror ascent in the space of prob-\nabilities. Recently, Vaswani et al. [2021] in-\ntroduced a policy gradient method that cor-\nresponds to mirror ascent in the dual space\nof logits. We refine this algorithm, removing\nits need for a normalization across actions\nand analyze the resulting method (referred\nto as SPMA). For tabular MDPs, we prove\nthat SPMA with a constant step-size matches\nthe linear convergence of NPG and achieves a\nfaster convergence than constant step-size (ac-\ncelerated) softmax policy gradient. To handle\nlarge state-action spaces, we extend SPMA to\nuse a log-linear policy parameterization. Un-\nlike that for NPG, generalizing SPMA to the lin-\near function approximation (FA) setting does\nnot require compatible function approxima-\ntion. Unlike MDPO, a practical generalization\nof NPG, SPMA with linear FA only requires solv-\ning convex softmax classification problems.\nWe prove that SPMA achieves linear conver-\ngence to the neighbourhood of the optimal\nvalue function. We extend SPMA to handle\nnon-linear FA and evaluate its empirical per-\nformance on the MuJoCo and Atari bench-\nmarks. Our results demonstrate that SPMA\nconsistently achieves similar or better perfor-\nmance compared to MDPO, PPO and TRPO.\n1\nIntroduction\nPolicy gradient (PG) methods [Williams, 1992; Sut-\nton et al., 1999; Konda and Tsitsiklis, 2000; Kakade,\n2001] have been critical to the achievements of rein-\nforcement learning (RL). Although the PG objective is\nnon-concave, recent theoretical research [Agarwal et al.,\n2021; Mei et al., 2020, 2021; Bhandari and Russo, 2021;\nLan, 2023; Shani et al., 2020; Liu et al., 2024; Lu et al.,\n2024; Alfano and Rebeschini, 2022; Yuan et al., 2023]\nhas analyzed PG methods in simplified settings and\ndemonstrated their global convergence to an optimal\npolicy. While such simplified analyses are helpful in\nunderstanding the underlying optimization issues, the\nresulting methods are rarely used in practice. On the\nother hand, while methods such as TRPO [Schulman,\n2015], PPO [Schulman et al., 2017], MDPO [Tomar et al.,\n2020] are commonly used in deep RL, their theoreti-\ncal analysis in the function approximation setting is\nquite limited. In particular, existing work either (i)\nanalyzes these methods only in the impractical tab-\nular setting [Tomar et al., 2020; Shani et al., 2020]\nor (ii) modifies these algorithms to make them more\namenable to theoretical analysis [Liu et al., 1906; Zhong\nand Zhang, 2024]. Unfortunately, these modified al-\ngorithms are quite different from the original variants\nand are not systematically benchmarked on standard\nenvironments. Consequently, there exists a large gap\nbetween PG methods that have theoretical guarantees\nin realistic settings versus those which are implemented\nin practice. To make matters worse, it has been demon-\nstrated that code-level implementation details impact\nthe empirical performance more than the underlying\nalgorithm [Engstrom et al., 2019].\nDesigning theoretically principled PG algorithms that\nsimultaneously have good empirical performance on the\nstandard set of benchmarks is the main motivation be-\nhind this work. To that end, we leverage an algorithm\nfirst proposed by Vaswani et al. [2021], which we modify\nto remove the need for normalization. We coin this re-\nfinement Softmax Policy Mirror Ascent (referred to as\nSPMA). We show that SPMA has comparable convergence\nguarantees as existing theoretical techniques [Lu et al.,\n2024; Yuan et al., 2023] in the tabular and function\napproximation settings, while achieving comparable\npractical performance as PPO, TRPO and MDPO, without\nadditional algorithmic modifications. In particular, we\nmake the following contributions.\nContribution 1: In Section 3, we focus on the multi-\narmed bandit and tabular MDP settings, where the\nnumber of parameters scales with the number of states\nand actions. We develop the SPMA algorithm, which\nparameterizes the policy using the softmax function\narXiv:2411.12042v1  [cs.LG]  18 Nov 2024\n\nFast Convergence of Softmax Policy Mirror Ascent\nand uses a mirror ascent (with the log-sum-exp mirror\nmap) update. Compared to NPG that can be viewed\nas mirror ascent in the space of probabilities, SPMA\ncorresponds to mirror ascent in the dual space of logits\nand does not require a normalization across actions.\nGiven access to the exact policy gradients, we prove\nthat SPMA with a constant step-size converges to the\noptimal policy at a linear rate and thus matches the rate\nof NPG [Khodadadian et al., 2021; Liu et al., 2024]. In\ncomparison, constant step-size softmax policy gradient\n(SPG) [Agarwal et al., 2021; Mei et al., 2020] can only\nachieve sublinear convergence rates even with Nesterov\nacceleration [Chen et al., 2023]. Hence, by changing\nthe mirror map (from Euclidean to log-sum-exp) while\nusing the same policy parameterization, SPMA can result\nin an exponential improvement over SPG.\nContribution 2: In order to handle MDPs with large\nstate-action spaces, we use function approximation (e.g.\nlinear models or neural networks) to parameterize the\npolicies resulting in the class of log-linear or energy-\nbased policies [Haarnoja et al., 2017; Agarwal et al.,\n2021; Yuan et al., 2023] respectively. By interpreting\nthe policy parameterization as a constraint on the\ncorresponding logits, we use projected mirror ascent to\nextend SPMA to the FA setting and design Algorithm 1.\nUnlike that for NPG, generalizing SPMA does not require\ncompatible function approximation, and thus results in\na more practical algorithm. Unlike MDPO [Tomar et al.,\n2020] which results in non-convex surrogates for linear\nFA, SPMA requires solving convex softmax classification\nproblems in each iteration.\nContribution 3: In Section 4.2, we state the condi-\ntions under which Algorithm 1 converges to the neigh-\nbourhood of the optimal value function, and charac-\nterize the resulting linear convergence rate. Hence, for\nlog-linear policies, Algorithm 1 matches the theoretical\nconvergence of NPG with compatible function approxi-\nmation [Agarwal et al., 2021; Alfano and Rebeschini,\n2022; Yuan et al., 2023]. Our theoretical results are\nbetter than those in Vaswani et al. [2021] and Schulman\n[2015] which prove sublinear convergence to a stationary\npoint for idealized variants of SPMA and TRPO respec-\ntively. In contrast to Kuba et al. [2022] which prove\nthat the idealized variants of PPO and TRPO converge\nto the optimal policy asymptotically, we characterize\nthe non-asymptotic convergence rate for Algorithm 1.\nContribution 4: We empirically evaluate SPMA across\nsimple MDPs with tabular and linear parameterization,\nAtari games with a discrete action space and a neural\npolicy parameterization with CNNs, and continuous\ncontrol MuJoCo tasks with a continuous action space\nand a neural policy parameterization with MLPs. We\ndemonstrate that SPMA has consistently good perfor-\nmance – on Atari games SPMA achieves better results\nthan both TRPO and PPO while matching or outperform-\ning MDPO, whereas on MuJoCo tasks, SPMA outperforms\nPPO and achieves similar or better results than MDPO.\n2\nProblem Formulation\nWe consider an infinite-horizon discounted Markov de-\ncision process (MDP) [Puterman, 2014] defined by\nM = ⟨S, A, P, r, ρ, γ⟩, where S and A represent the\nstates and actions, P : S × A →∆S is the transition\nprobability function, r : S × A →[0, 1] is the reward\nfunction, ρ ∈∆S is the initial state distribution, and\nγ ∈[0, 1) represents the discount factor. In this paper,\nwe exclusively consider the setting where the number\nof states and actions is finite, but potentially large.\nGiven s ∈S, the policy π induces a probability dis-\ntribution π(.|s) over the actions.\nThe action-value\nfunction Qπ : S × A →R induced by π is defined\nas Qπ(s, a) := E[P∞\nt=0 γtr(st, at)|s0 = s, a0 = a]\nwhere st ∼p(.|st−1, at−1), and at ∼π(.|st) for t ≥1.\nThe value function corresponding to Qπ starting from\nstate s is defined as V π(s) := Ea∼π(.|s)[Qπ(s, a)] with\nJ(π) := V π(ρ) := Es∼ρ[V π(s)] representing the ex-\npected discounted cumulative reward. Furthermore,\nthe advantage function Aπ : S × A →R is de-\nfined as Aπ(s, a) := Qπ(s, a) −V π(s).\nThe policy\nalso induces a discounted state-occupancy measure\ndπ(s) := (1 −γ) P∞\nt=0 γtPrπ[st = s|s0 ∼ρ] over\nthe states. The objective is to find an optimal pol-\nicy π∗that maximizes the expected reward J(π), i.e.\nπ∗= arg maxπ J(π). As a special case, in the bandit\nsetting, |S| = 1, |A| = K, γ = 0, and J(π) = ⟨π, r⟩,\nwith K representing the number of arms.\n3\nSoftmax Policy Mirror Ascent:\nTabular Parameterization\nSoftmax policy mirror ascent (referred to as SPMA) rep-\nresents the policy using the softmax function h : RA →\n∆A i.e. π(·|s) = h(z(s, ·)) s.t. for all (s, a) ∈S × A,\nπ(a|s) =\nexp(z(s,a))\nP\na′ exp(z(s,a′)), where the logits z are SA-\ndimensional vectors and ∆A is the A-dimensional\nsimplex.\nWe first focus on the tabular parameteri-\nzation where the number of parameters scales with\nthe number of states and actions, and aim to learn\nthe logits corresponding to an optimal policy. With\nsome abuse of notation, we use J(z) to refer to J(π)\nwhere π(·|s) = h(z(s, ·)) and state the objective as:\nmaxz∈RSA J(z).\nAs the name suggests, SPMA uses mirror ascent (MA)\nto maximize J(z). For a differentiable, strictly convex\nmirror map Φ, MA [Beck and Teboulle, 2003; Bubeck\net al., 2015] is an iterative algorithm whose update at\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\niteration t ∈[T] can be stated in two equivalent ways:\n∇Φ(zt+1) = ∇Φ(zt) + η ∇zJ(zt)\n(1)\nzt+1 = arg max\nz∈RSA\n\u0014\n⟨z −zt, ∇zJ(zt)⟩−1\nη DΦ(z, zt)\n\u0015\nwhere zt is the logit at iteration t, η is the step-size\nand DΦ(z, z′) := Φ(z) −Φ(z′) −⟨∇Φ(z′), z −z′⟩is the\nBregman divergence between logits z and z′ induced by\nthe mirror map Φ. Hence, the MA update at iteration\nt can be interpreted as moving in the gradient direction\n∇zJ(zt) while staying “close” to the logit zt, where the\nproximity between logits is measured according to the\nBregman divergence and weighted by 1\nη.\n3.1\nBandit Setting\nIt is instructive to first instantiate the SPMA update for\nthe bandit setting where J(π) = ⟨π, r⟩. In this setting,\n∇zJ(z) ∈RA s.t. [∇zJ(z)](a) = π(a) [r(a) −⟨π, r⟩].\nFollowing Vaswani et al. [2021], we use the log-sum-\nexp mirror map i.e. ϕ(z) = ln(P\na exp(z(a)). Since\n[∇ϕ(z)](a) =\nexp(z(a))\nP\na′ exp(z(a′)) = [h(z)](a) = π(a), the\nlogit and probability spaces are dual to each other, and\nthe ∇ϕ map can be used to move between these spaces.\nGiven this, the SPMA update can be written as:\nπt+1(a) = πt(a) (1 + η [r(a) −⟨π, r⟩])\n= πt(a) [1 + η\nX\na′̸=a\nπt(a′) ∆(a, a′)] ,\n(2)\nwhere ∆(a, a′) := r(a) −r(a′) represents the reward\ngap between arms a and a′. We first ensure that πt+1\nis a valid probability distribution. Since r(a) ∈[0, 1]\nfor all a, η ≤1 is sufficient to guarantee that πt+1(a)\nis non-negative for every a. Moreover, P\na πt+1(a) =\nP\na πt(a) + η P\na πt(a)[r(a) −⟨π, r⟩] = P\na πt(a) = 1.\nHence, for η ≤1, Eq. (2) results in a valid update to\nthe policy. The above update is related to the PROD al-\ngorithm [Cesa-Bianchi et al., 2007] used for the experts\nproblem in the online learning literature. In contrast\nto SPMA which is derived from mirror ascent, PROD is\nderived using a linearization of the Hedge [Freund and\nSchapire, 1997] algorithm and requires explicit normal-\nization to obtain probabilities.\n3.2\nMDP Setting\nIn order to extend SPMA to the MDP setting, we use a\n(state-wise) weighted log-sum-exp mirror map, i.e. for\na logit z ∈RSA, we define Φ(z) := P\ns w(s) ϕ(z(s, ·)) =\nP\ns w(s) ln(P\na exp(z(s, a)) where w(s) are the per-\nstate weights. Following the proof of Vaswani et al.\n[2024, Lemma 11], the resulting Bregman divergence\nis given as: DΦ(z, z′) = P\ns w(s) KL(π′(·|s)||π(·|s))\nwhere π and π′ are the policies corresponding to logits z\nand z′. At iteration t of SPMA, we choose w(s) = dπt(s)\nand use the policy gradient theorem [Sutton et al., 1999]\nto calculate [∇J(zt)](s, a) = dπt(s) πt(a|s) Aπt(s, a).\nThe resulting SPMA update is given as:\nzt+1 = arg max\nz∈RSA\nX\ns\ndπt(s)\n\u0014\n⟨πt(·|s) Aπt(s, ·), z(s, ·)⟩\n−1\nη KL(πt(·|s) || h(z(s, ·))\n\u0015\n.\nSince the above maximization decomposes over the\nstates, we can write the per-state update for each s ∈S\nin terms of πt+1(·|s) = h(zt+1(s, ·)) as follows:\nπt+1(a|s) = πt(a|s) (1 + ηAπt(s, a)) .\n(3)\nSimilar to the bandit case, since r(s, a) ∈[0, 1],\nπt+1(a|s) is non-negative for η ≤1 −γ.\nSince\nP\na πt(a|s)Aπt(s, a)\n=\n0, P πt+1(a|s)\n=\n1, and\nhence Eq. (3) results in a valid policy update.\nIn order to compare the SPMA update to existing\nmethods, note that for the tabular parameteriza-\ntion, natural policy gradient (NPG) update [Kakade,\n2001] is the same as policy mirror ascent [Lan, 2023;\nJohnson et al., 2023; Xiao, 2022] and is given by:\nπt+1(a|s) ∝πt(a|s) exp(η Aπt(s, a)).\nIn contrast to\nNPG, the SPMA update in Eq. (3) is linear in both η\nand Aπt(s, a) and does not require an explicit nor-\nmalization across actions to ensure valid probability\ndistributions. On the other hand, softmax policy gra-\ndient (SPG) [Agarwal et al., 2021; Mei et al., 2020]\ncorresponds to choosing the mirror map ϕ in Eq. (1) to\nbe the Euclidean norm and has the following update:\nzt+1(s, a) = zt(s, a) + η πt(a|s)Aπt(s, a). Compared to\nSPG that uses the softmax policy gradient to update\nthe logits, SPMA uses the softmax policy gradient to\ndirectly update the probabilities. As we demonstrate in\nthe next section, this desirable property enables SPMA\nto achieve faster rates than SPG.\n3.3\nTheoretical Results\nIn this section, we prove convergence guarantees for\nSPMA in the multi-armed bandit and tabular MDP\nsettings. We first establish linear convergence for SPMA\nfor multi-armed bandits for any constant η ≤1.\nTheorem 1. The SPMA update in Eq. (2) with (i) a\nconstant step-size η ≤1, and (ii) uniform initialization\ni.e. π0(a) = 1\nK for all a converges as:\nr(a∗) −⟨πT , r⟩≤\n\u0012\n1 −1\nK\n\u0013\nexp\n\u0012−η ∆min T\nK\n\u0013\n,\nwhere T is the number of iterations, a∗is the op-\ntimal arm i.e.\na∗= arg maxa r(a) and ∆min :=\nmina̸=a∗∆(a∗, a) = r(a∗) −r(a) is the gap.\nThe above theorem (proved in Appendix A) shows that\nfor multi-armed bandit problems, SPMA can achieve\n\nFast Convergence of Softmax Policy Mirror Ascent\nlinear convergence to the optimal arm, and the resulting\nrate depends on both the gap and the number of arms.\nIn Appendix A.1, we prove that SPMA with specific gap-\ndependent step-sizes can achieve a global super-linear\nconvergence rate for multi-armed bandits. To the best\nof our knowledge, these are the first global super-linear\nrates for PG methods on multi-armed bandit problems.\nIn the next theorem, we extend the linear convergence\nresult to tabular MDPs and prove that when given\naccess to the exact policy gradients, SPMA results in\nlinear convergence to the optimal value function for\nany sufficiently small constant step-size.\nTheorem 2. Using the SPMA update in Eq. (3) with a\nstep-size η < min\nn\n1 −γ,\n1\nCt(1−γ)\no\nconverges as:\n\r\r\rV π∗−V πT\n\r\r\r\n∞≤\n T −1\nY\nt=0\nαt\n! \r\r\rV π∗−V π0\n\r\r\r\n∞,\nwhere\nαt\n:=\n(1 −η Ct (1 −γ)),\nCt\n:=\nmins{πt(˜at(s)|s) ∆t(s)}, ˜at(s) := arg maxa Qπt(s, a)\nand ∆t(s) := maxa Qπt(s, a) −maxa̸=˜a Qπt(s, a).\nFor ease of exposition, the above theorem considers\n˜at(s) to be the unique action maximizing Qπt(s, ·) for\nevery state s and policy πt. In Appendix B, we extend\nthis to include multiple optimal actions with a mi-\nnor change in the definition of the gap. For rewards in\n(0, 1), Ct (1−γ) is in (0, 1) and depends on the initializa-\ntion. If C := mint∈[T ] Ct, then the above implies that\nwhen T ∈O\n\u0010\n1\nη C(1−γ) ln(1/ϵ)\n\u0011\n, SPMA guarantees that\nV πT (s) ≥V ∗(s) −ϵ for all s ∈S. In order to put the\nabove convergence result in context, note that SPG with\na constant step-size results in a Θ (1/ϵ) convergence [Mei\net al., 2020]. Recently, Chen et al. [2023] proved that\nconstant step-size SPG with Nesterov acceleration can\nobtain an O(1/√ϵ) convergence. In contrast, the above\ntheorem demonstrates that by choosing the appropri-\nate mirror map, constant step-size SPMA can achieve a\nfaster O(log(1/ϵ)) rate of convergence. On the other\nhand, Liu et al. [2024]; Lu et al. [2024] prove that SPG\nwith adaptive step-sizes can also result in linear con-\nvergence. However, the resulting rate depends on the\ndistribution mismatch ratio\n\r\r\r dπ∗\nρ\n\r\r\r\n∞that can be expo-\nnentially large in the size of the state space [Li et al.,\n2021]. In contrast, the convergence result in Theorem 2\nhas no such dependence. The linear convergence rate\nin Theorem 2 matches that of NPG with a constant\nstep-size [Liu et al., 2024] and compared to Liu et al.\n[2024, Theorem 5.4], it results in a better dependence\n(exponential vs polynomial) on the gap ∆t(s). Finally,\nwe note that for the tabular parameterization, a variant\nof TRPO has been shown to achieve O(1/ϵ2) convergence\nto the optimal policy [Shani et al., 2020].\nIn the next section, we extend SPMA to exploit function\napproximation to handle large state-action spaces.\n4\nHandling Function Approximation\nHandling large MDPs requires function approximation\n(FA) techniques to share information between states\nand actions. For example, given a set of state-action\nfeatures X ∈RSA×d where d << SA, the log-linear\npolicy parameterization [Agarwal et al., 2021; Alfano\nand Rebeschini, 2022; Yuan et al., 2023] considers poli-\ncies of the form: π(a|s) =\nexp(⟨X(s,a),θ⟩)\nP\na′ exp(⟨X(s,a′),θ⟩) where\nθ ∈Rd is the parameter to be learned. Hence, the\nlog-linear policy parameterization can handle large\nstate-action spaces while learning a compressed d-\ndimensional representation. We interpret the log-linear\nAlgorithm 1: SPMA with function approximation\nInput: θ0 (parameters for the initial policy π0), fθ\n(function approximation), T (number of\nouter-loop), m (number of inner-loops), η\n(outer-loop step-size), ζ (inner-loop step-size)\nfor t ←0 to T −1 do\n1. Interact with the environment using πt and\nform the surrogate function ℓt(θ) in Eq. (5)\n2. Initialize inner-loop: ω0 = θt\nfor k ←0 to m −1 do\nωk+1 = ωk −ζ ∇ωℓt(ωk)\nend\n3. θt+1 = ωm\n4. Update πt+1(·|s) = h(fθt+1(s, .))\nend\nReturn θT\npolicy parameterization as a constraint in the space\nof logits.\nSpecifically, the logits z are constrained\nto lie in the set Z = {z ∈RSA|∃θ s.t. z = Xθ\n}, meaning that the logits are required to be realiz-\nable by the linear model with features X.\nWe de-\nfine Π as the corresponding set of feasible policies, i.e.\nΠ = {π|∀s ∈S , π(·|s) = h(z(s, ·)) s.t. z ∈Z}. Hence,\nthe policies in Π are constrained to be log-linear. Note\nthat, as in the case of log-linear policies, Π can be a\nnon-convex set, even when Z is convex. For general\nenergy-based or neural policies [Haarnoja et al., 2017;\nAgarwal et al., 2021], π(a|s) ∝exp(fθ(s, a)) where\nfθ : RSA →R is a complex, non-linear model.\nIn\nthis case, the logits are constrained to lie in the set:\nZ = {z ∈RSA|∃θ s.t. z(s, a) = fθ(s, a)}.\nThe above interpretation allows us to extend SPMA to\nthe FA setting. Specifically, we use the same mirror\nascent update as in Eq. (1) with an additional pro-\njection step onto the feasible set Z. Specifically, we\ndefine zt+1/2 s.t. ∇Φ(zt+1/2) = ∇Φ(zt)+η ∇zJ(zt) and\ncompute zt+1 = arg minz∈Z DΦ(z, zt+1/2). This step\ndenotes the Bregman projection of zt+1/2 onto Z, i.e.\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nwe seek to find the closest (according to the Bregman\ndivergence) realizable point (in Z) to the “ideal” point\nzt+1/2 which corresponds to using the tabular parame-\nterization. Following Vaswani et al. [2021]; Lavington\net al. [2023], we convert the above projection prob-\nlem into an unconstrained minimization problem where\n∀(s, a), zt+1(s, a) = fθt+1(s, a), zθ(s, a) := fθ(s, a) ∈Z,\nθt+1 = arg minθ DΦ(zθ, zt+1/2) i.e. we aim to find the\nparameter θ that realizes the point zθ ∈Z which\nis closest to zt+1/2. Following Section 3.2, using the\nlog-sum-exp mirror map weighted by dπt(s) at itera-\ntion t results in the following optimization problem\nθt+1 = arg minθ ˜ℓt(θ) where,\n˜ℓt(θ) :=\nX\ns\ndπt(s) KL(πt+1/2(·|s) || πθ(·|s))\n(4)\n=\nE\ns∼dπt H (h(fθt(s, ·))(1 + ηAπt(s, ·)), h(fθ(s, ·))) + Ct .\nHere, H(p, q) := −Ep[ln(q)] = −P\na p(a) ln(q(a)) is\nthe cross-entropy between distributions p and q and\nCt is a constant with respect to θ. We refer to ˜ℓt(θ) as\nthe ideal surrogate. Minimizing this surrogate requires\ncalculating the expectation over the states sampled\naccording to πt. In order to have a practical algorithm,\nwe can run trajectories τ starting from the initial state\ndistribution ρ, following the policy πt and thus sampling\nfrom the dπt distribution (see Agarwal et al. [2021,\nAlgorithm 3] for the detailed procedure). Given these\nsamples, we form the surrogate ℓt(θ) defined as:\nX\ns∼τ\nKL (h(fθt(s, ·))(1 + ηAπt(s, ·)) || h(fθ(s, ·))) . (5)\nNote that E[ℓt(θ)] = ˜ℓt(θ) where the expectation is\nw.r.t. to dπt. We use m steps of (stochastic) gradient\ndescent to approximately minimize ℓt(θ). Putting ev-\nerything together, the algorithm incorporating general\nFA is presented in Algorithm 1.\nLog-linear Policy Parameterization: For this spe-\ncial case, the problem in Eq. (4) is equivalent to a\nweighted (according to dπt(s)) multi-class classifica-\ntion for each state. The per-state problem corresponds\nto a softmax classification into A classes using a lin-\near model with features X and soft labels equal to\nπt+1/2(·|s). Computing θt+1 thus involves minimizing\na smooth, convex function.\nIn the next section, we compare Algorithm 1 to existing\napproaches that incorporate FA.\n4.1\nComparison to Existing Approaches\nComparison to NPG:\nA principled extension of\nNPG to handle FA is via the compatible function\napproximation [Kakade, 2001; Agarwal et al., 2021].\nAn example of such an algorithm, Q-NPG involves\nsolving a quadratic surrogate at each iteration t: ˆωt =\nminω\nP\ns dπt(s) P\na πt(a|s) (fω(s, a) −Qπt(s, a))2.\nThe policy parameters are updated using ˆωt which\ncorresponds to the natural gradient direction. While\nthis\napproach\nresults\nin\ntheoretical\nguarantees\n(see Section 4.2 for details); for a general parame-\nterization, the resulting algorithm involves changing\nthe representation of the critic at every iteration.\nConsequently, solving the surrogate is expensive,\nlimiting the practicality of the method.\nComparison to MDPO: A more practical extension\nof NPG is mirror descent policy optimization [Tomar\net al., 2020] (MDPO).\nSimilar to SPMA, MDPO can be\ninterpreted as projected (onto the feasible set of\npolicies) mirror ascent in the space of probabili-\nties [Vaswani et al., 2021]. The resulting surrogate\n(as a function of the policy parameters) is given by:\nP\ns dπt(s) KL (πθ(·|s) || h (fθt(s, ·) exp(η Qπt(s, ·)))).\nUnlike the surrogate in Eq. (4), the MDPO surrogate\nis non-convex even when using a tabular softmax\nparameterization for the policy, and consequently does\nnot have any theoretical guarantees. However, MDPO\nresults in good empirical performance, and we compare\nto it in Section 5.\nComparison to TRPO: As explained in Vaswani\net al. [2021], the surrogate in Eq. (4) is closely\nrelated to TRPO.\nIn particular, the TRPO update\nconsists of solving the following optimization prob-\nlem: P\ns dπt(s) P\na πt(a|s)Aπt(s, a) πθ(a|s)\nπθt(a|s), such that\nEs∼dπt [KL(πt(·|s) || πθ(·|s))] ≤δ. SPMA (i) uses instead\nP\ns dπt(s) P\na πt(a|s)Aπt(s, a) log πθ(a|s)\nπθt(a|s), i.e. the log-\narithm of the importance sampling ratio, making the\nresulting update more stable [Vaswani et al., 2021]\nand (ii) enforces the proximity between policies via\na regularization rather than a constraint. Enforcing\nthe trust-region constraint in TRPO requires additional\nhyper-parameters, code-level optimizations and com-\nputation [Engstrom et al., 2019]. In contrast, SPMA is\nmore computationally efficient and simpler to imple-\nment in practice. In the next section, we study the\ntheoretical properties of Algorithm 1.\n4.2\nTheoretical Guarantee\nFor rewards in [0, 1] and for a general policy pa-\nrameterization, Vaswani et al. [2021] prove that, for\nη ≤1 −γ, Algorithm 1 results in monotonic improve-\nment, i.e. J(πt+1) ≥J(πt) and hence converges to a\nstationary point at an O(1/ϵ) rate. Since J is non-\nconvex and can have multiple stationary points, the\nresult in Vaswani et al. [2021] does not provide suf-\nficient evidence for the good empirical performance\nof Algorithm 1. In this section, we prove that, under\nreasonable assumptions similar to existing works, Al-\ngorithm 1 can converge to the neighbourhood of the\noptimal value function at a linear rate. The size of\n\nFast Convergence of Softmax Policy Mirror Ascent\nthe neighbourhood is determined by various practical\nfactors such as sampling, inexact optimization, and\nbias due to the FA. In order to state our result, we first\nstate and justify our assumptions.\nRecall that in order to have a practical algorithm, we\nminimize ℓt(θ) obtained by sampling from dπt.\nAssumption 1. Excess Risk: For all iterations t of Al-\ngorithm 1, |˜ℓt(θt+1) −min ˜ℓt(θ)| ≤ϵstat.\nThe above assumption quantifies the excess risk in-\ncurred by minimizing a finite sampled “dataset” of\nstates as compared to minimizing over the population\nloss ˜ℓt(θ). This is a standard assumption in the lit-\nerature analyzing the convergence of policy gradient\nmethods with FA [Agarwal et al., 2021; Alfano and\nRebeschini, 2022; Yuan et al., 2023]. If n is the num-\nber of samples and the surrogate is minimized using\n(stochastic) gradient descent, using the standard gener-\nalization results [Lei and Ying, 2021; Nikolakakis et al.,\n2022], we expect ϵstat = O(1/n) for the log-linear pa-\nrameterization. For example, using m iterations of\nSGD would result in ϵstat = O(1/n + 1/m) [Lei and\nYing, 2021, Theorem 6]. For a general parameteriza-\ntion, where the surrogate might be non-convex, the\nexcess risk can be bounded up to the optimization er-\nror [Nikolakakis et al., 2022]. Under the appropriate\ntechnical assumptions, ℓt(θ) can been shown to satisfy\nthe Polyak-Lojasiewicz condition\n[Liu et al., 2022]\nimplying that the optimization error for (stochastic)\ngradient descent can be made arbitrarily small. The\nnext assumption quantifies the bias incurred because\nof a policy parameterization with limited expressive\npower compared to using the tabular parameterization.\nAssumption 2. Bias: For all iterations t of Algo-\nrithm 1, minθ ˜ℓt(θ) ≤ϵbias.\nThe above assumption captures the flexibility of the\nmodel class being used in the policy parameterization.\nFor a tabular parameterization where the number of\nparameters scales as SA, ϵbias = 0, whereas for the\nlog-linear parameterization, ϵbias depends on the ex-\npressivity of the features.\nThe final assumption is\nconcerned with exploration and indicates that the ini-\ntial state distribution has full support implying that\nthe method does not require explicit exploration.\nAssumption 3. Exploration: ∀s ∈S, ρ(s) ≥ρmin > 0.\nThe above assumption is standard in the litera-\nture [Agarwal et al., 2021; Xiao, 2022] and helps isolate\nand study the optimization properties of PG methods.\nWe prove the following theorem in Appendix B.\nTheorem 3. Under assumption 1-3, Algorithm 1 with\nη < min\nn\n1 −γ,\n1\nCt(1−γ)\no\nconverges as,\nJ(π∗) −J(πT )\n≤\n T −1\nY\nt=0\nαt\n!\n(J(π∗) −J(π0)) + β\nT −1\nX\nt=0\nT −1\nY\ni=t+1\nαi ,\nwhere β =\n√\n2\n(1−γ)2ρmin\n√ϵstat + ϵbias and αt has the same\ndefinition as in Theorem 2.\nThe above theorem shows that Algorithm 1 converges\nlinearly to the neighbourhood of the optimal value func-\ntion. Furthermore, for the log-linear parameterization,\nthe size of the neighbourhood can be bounded explic-\nitly. For example, if the logits zt+1/2 for every t lie in\nthe span of the features, ϵbias = 0 (this is similar to\nthe linear Bellman completeness condition used in the\nanalysis of value-based methods [Munos, 2005]) and\nϵstat = O(1/n + 1/m). By using more samples and\nwith more (S)GD iterations, the size of the neighbour-\nhood can be made arbitrarily small. Except for the\nneighbourhood term, the above convergence result is\nsimilar to that for the tabular setting in Theorem 2.\nThe other difference is that the result in the tabular\nsetting holds in the ℓ∞norm and thus holds for all\nstates, whereas the result in Theorem 3 only holds for\na fixed starting state distribution ρ. In practice, Aπt\nis typically estimated via a critic. To account for this,\nwe generalize the proof of Theorem 3 in Appendix B,\nand prove that Algorithm 1 converges linearly to a\nneighbourhood that depends on an additional term\nproportional to the critic error.\nWe now compare to the existing theoretical results for\nPG methods with FA. For the log-linear policy param-\neterization, Q-NPG and its variants have been shown to\nachieve linear convergence to the neighbourhood of the\noptimal value function [Agarwal et al., 2021; Alfano\nand Rebeschini, 2022; Yuan et al., 2023]. The size of the\nneighbourhood depends on similar quantities as Theo-\nrem 3. Finally, we note that while an ideal, impractical\nvariant of TRPO has a monotonic improvement guaran-\ntee similar to Algorithm 1 [Schulman, 2015], it does not\nhave convergence guarantees comparable to Theorem 3.\n5\nEmpirical Evaluation\nWe evaluate SPMA on three types of problems: (i) tab-\nular MDPs with access to exact policy gradients, (ii)\nMDPs with continuous states but discrete actions, using\ninexact policy gradients, and (iii) MDPs with contin-\nuous state-actions spaces and inexact gradients. For\ntabular MDPs, we use the tabular parameterization\nand compare SPMA against NPG and constant step-size\nSPG [Mei et al., 2020]. For these environments, we\nalso consider log-linear policies and compare SPMA to\nMDPO and SPG. For non-tabular environments, we con-\nsider PPO, TRPO and MDPO as baselines. We consider\ntwo variants of TRPO – TRPO-constrained, the stan-\ndard optimized variant in Raffin et al. [2021] and\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nFigure 1: On Atari games, where a CNN-based actor network is employed, SPMA matches or surpasses MDPO and\noutperforms PPO, as well as both the constrained and regularized versions of TRPO.\nTRPO-regularized, the original regularized variant.\nTRPO-constrained is able to effectively enforce the\ntrust region constraint using conjugate gradient, but\nintroduces additional hyper-parameters, requires code-\nlevel optimization techniques and is computationally\nexpensive.\nOn the other hand, TRPO-regularized\nis significantly more efficient and theoretically prin-\ncipled [Lazić et al., 2021], and is similar to SPMA’s\nobjective (see Section 4.1). For details regarding the\nhyper-parameters of all methods for each environment,\nrefer to Appendices C and D.\nTabular MDP Results: We present the results in Ap-\npendix C, and summarize the key findings here. We\nobserve that SPMA and NPG achieve comparable per-\nformance, both consistently outperforming SPG (Fig-\nure 3). However, in the linear FA setting, SPMA demon-\nstrates superior performance compared to MDPO in\nthe CliffWorld environment [Sutton, 2018] (Figure 5),\nwhile performing similarly in the Frozen Lake environ-\nment [Brockman, 2016] (Figure 6). In both environ-\nments, SPMA and MDPO consistently outperform SPG.\nIn the remainder of this section, we focus on the non-\ntabular settings with inexact policy gradients.\nFor\nthese experiments, we follow the protocol of Tomar\net al. [2020], using 5 seeds and reporting the aver-\nage results along with their 95% confidence intervals.\nAdditionally, we employ the actor-critic architecture,\npolicy parameterization, and GAE [Schulman et al.,\n2015] (to estimate the advantage function) from stable\nbaselines [Raffin et al., 2021]. We emphasize that, in\ncontrast to prior work, we do not make ad-hoc adjust-\nments to SPMA (i.e., the actor). To set the step-size\nη in Algorithm 1, we perform a grid search over five\nvalues (fixed across all experiments) and set the inner\nloop step-size ζ using Armijo line search [Armijo, 1966].\nAtari and Mujoco Results: We evaluate the per-\nformance of SPMA compared to the baselines across\nvarious Atari 2600 games [Bellemare et al., 2013] and\nMuJoCo [Todorov et al., 2012] control tasks from Ope-\nnAI Gym [Brockman, 2016]. The observation space for\nAtari games consists of a 210×160×3 image, represent-\ning the current state of the game. The action space in\n\nFast Convergence of Softmax Policy Mirror Ascent\nFigure 2: On MuJoCo control tasks, where a two-layer MLP actor network is used, SPMA matches or outperforms\nMDPO while consistently outperforming PPO and regularized TRPO. In contrast to the results on Atari games, with\na shallow MLP, TRPO-constrained outperforms all methods.\nthese environments is discrete, whereas in MuJoCo, it\nis continuous and by default represented by a diagonal\nGaussian distribution in Raffin et al. [2021]. Addition-\nally, the actor-critic network for Atari uses a CNN as\na feature extractor, while MuJoCo employs an MLP.\nComparing the results in Fig. 1 and 2, our key observa-\ntions are as follows: i) SPMA consistently outperforms\nor matches MDPO and PPO across all environments; ii)\nalthough TRPO-constrained achieves superior perfor-\nmance on MuJoCo, its performance degrades consid-\nerably on Atari games. We conjecture that the con-\njugate gradient algorithm in TRPO-constrained per-\nforms poorly when the actor network is a CNN rather\nthan a two-layer MLP; iii) TRPO-regularized, which\nhas a similar objective as SPMA (see Section 4.1 for a\ncomparison) does not perform as well on MuJoCo and\nhas considerably worse performance on Atari. Hence,\nwe observe that replacing the sampling ratio by its\nlog can result in substantial empirical gains. This be-\nhaviour has also been observed for PPO Vaswani et al.\n[2021].\nOverall, our experiments demonstrate that,\ndespite being theoretically grounded, SPMA exhibits\nstrong empirical performance across various environ-\nments without relying on ad-hoc adjustments.\n6\nDiscussion\nWe developed SPMA, a PG method that corresponds to\nmirror ascent in the dual space of logits. We believe\nthat our paper bridges the gap between theoretical\nPG methods and practical objectives by presenting\na method that offers strong theoretical convergence\nguarantees while delivering competitive practical per-\nformance (compared to PPO, TRPO, MDPO), without re-\nlying on additional heuristics or algorithmic modifica-\ntions. In the future, we aim to develop techniques for\nadaptively tuning the step-size and avoiding expensive\ngrid-searches. We also plan to develop and analyze an\noff-policy variant of SPMA.\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nReferences\nAgarwal, A., Kakade, S. M., Lee, J. D., and Mahajan,\nG. (2021). On the theory of policy gradient methods:\nOptimality, approximation, and distribution shift. J.\nMach. Learn. Res., 22(98):1–76.\nAlfano, C. and Rebeschini, P. (2022).\nLinear con-\nvergence for natural policy gradient with log-\nlinear policy parametrization.\narXiv preprint\narXiv:2209.15382.\nArmijo, L. (1966). Minimization of functions having\nlipschitz continuous first partial derivatives. Pacific\nJournal of mathematics, 16(1):1–3.\nBeck, A. and Teboulle, M. (2003). Mirror descent and\nnonlinear projected subgradient methods for convex\noptimization. Operations Research Letters, 31(3):167–\n175.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling,\nM. (2013). The arcade learning environment: An\nevaluation platform for general agents. Journal of\nArtificial Intelligence Research, 47:253–279.\nBhandari, J. and Russo, D. (2021). On the linear con-\nvergence of policy gradient methods for finite mdps.\nIn International Conference on Artificial Intelligence\nand Statistics, pages 2386–2394. PMLR.\nBrockman, G. (2016). Openai gym. arXiv preprint\narXiv:1606.01540.\nBubeck, S. et al. (2015). Convex optimization: Algo-\nrithms and complexity. Foundations and Trends®\nin Machine Learning, 8(3-4):231–357.\nCesa-Bianchi, N., Mansour, Y., and Stoltz, G. (2007).\nImproved second-order bounds for prediction with\nexpert advice. Machine Learning, 66:321–352.\nChen, Y.-J., Huang, N.-C., Lee, C.-p., and Hsieh, P.-C.\n(2023). Accelerated policy gradient: On the con-\nvergence rates of the nesterov momentum for re-\ninforcement learning. In Forty-first International\nConference on Machine Learning.\nEngstrom, L., Ilyas, A., Santurkar, S., Tsipras, D.,\nJanoos, F., Rudolph, L., and Madry, A. (2019). Im-\nplementation matters in deep rl: A case study on ppo\nand trpo. In International conference on learning\nrepresentations.\nFreund, Y. and Schapire, R. E. (1997). A decision-\ntheoretic generalization of on-line learning and an\napplication to boosting. Journal of computer and\nsystem sciences, 55(1):119–139.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S.\n(2017). Reinforcement learning with deep energy-\nbased policies. In International conference on ma-\nchine learning, pages 1352–1361. PMLR.\nJohnson, E., Pike-Burke, C., and Rebeschini, P. (2023).\nOptimal convergence rate for exact policy mirror de-\nscent in discounted markov decision processes. arXiv\npreprint arXiv:2302.11381.\nKakade, S. M. (2001). A natural policy gradient. Ad-\nvances in neural information processing systems, 14.\nKhodadadian, S., Jhunjhunwala, P. R., Varma, S. M.,\nand Maguluri, S. T. (2021).\nOn the linear con-\nvergence of natural policy gradient algorithm. In\n2021 60th IEEE Conference on Decision and Control\n(CDC), pages 3794–3799. IEEE.\nKonda, V. R. and Tsitsiklis, J. N. (2000). Actor-critic\nalgorithms. In Advances in neural information pro-\ncessing systems, pages 1008–1014.\nKuba, J. G., de Witt, C. S., and Foerster, J. (2022).\nMirror learning: A unifying framework of policy\noptimisation. arXiv preprint arXiv:2201.02373.\nLan, G. (2023). Policy mirror descent for reinforcement\nlearning: Linear convergence, new sampling complex-\nity, and generalized problem classes. Mathematical\nprogramming, 198(1):1059–1106.\nLavington, J. W., Vaswani, S., Babanezhad, R.,\nSchmidt, M., and Roux, N. L. (2023). Target-based\nsurrogates for stochastic optimization. arXiv preprint\narXiv:2302.02607.\nLazić, N., Hao, B., Abbasi-Yadkori, Y., Schuurmans,\nD., and Szepesvári, C. (2021). Optimization issues in\nkl-constrained approximate policy iteration. arXiv\npreprint arXiv:2102.06234.\nLei, Y. and Ying, Y. (2021). Sharper generalization\nbounds for learning with gradient-dominated objec-\ntive functions. In International Conference on Learn-\ning Representations.\nLi, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2021).\nSoftmax policy gradient methods can take exponen-\ntial time to converge. In Conference on Learning\nTheory, pages 3107–3110. PMLR.\nLiu, B., Cai, Q., Yang, Z., and Wang, Z. (1906).\nNeural proximal/trust region policy optimization at-\ntains globally optimal policy (2019). arXiv preprint\narXiv:1906.10306.\nLiu, C., Zhu, L., and Belkin, M. (2022). Loss landscapes\nand optimization in over-parameterized non-linear\nsystems and neural networks. Applied and Compu-\ntational Harmonic Analysis, 59:85–116.\nLiu, J., Li, W., and Wei, K. (2024).\nElementary\nanalysis of policy gradient methods. arXiv preprint\narXiv:2404.03372.\nLu, M., Aghaei, M., Raj, A., and Vaswani, S.\n(2024). Towards principled, practical policy gradi-\nent for bandits and tabular mdps. arXiv preprint\narXiv:2405.13136.\n\nFast Convergence of Softmax Policy Mirror Ascent\nMei, J., Gao, Y., Dai, B., Szepesvari, C., and Schu-\nurmans, D. (2021). Leveraging non-uniformity in\nfirst-order non-convex optimization. In International\nConference on Machine Learning, pages 7555–7564.\nPMLR.\nMei, J., Xiao, C., Szepesvari, C., and Schuurmans, D.\n(2020). On the global convergence rates of softmax\npolicy gradient methods. In International Conference\non Machine Learning, pages 6820–6829. PMLR.\nMunos, R. (2005). Error bounds for approximate value\niteration. In Proceedings of the National Confer-\nence on Artificial Intelligence, volume 20, page 1006.\nMenlo Park, CA; Cambridge, MA; London; AAAI\nPress; MIT Press; 1999.\nNikolakakis, K. E., Haddadpour, F., Karbasi, A., and\nKalogerias, D. S. (2022). Beyond lipschitz: Sharp\ngeneralization and excess risk bounds for full-batch\ngd. arXiv preprint arXiv:2204.12446.\nPuterman, M. L. (2014). Markov decision processes:\ndiscrete stochastic dynamic programming. John Wiley\n& Sons.\nRaffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernes-\ntus, M., and Dormann, N. (2021). Stable-baselines3:\nReliable reinforcement learning implementations.\nJournal of Machine Learning Research, 22(268):1–\n8.\nSchulman, J. (2015). Trust region policy optimization.\narXiv preprint arXiv:1502.05477.\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and\nAbbeel, P. (2015). High-dimensional continuous con-\ntrol using generalized advantage estimation. arXiv\npreprint arXiv:1506.02438.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A.,\nand Klimov, O. (2017). Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347.\nShani, L., Efroni, Y., and Mannor, S. (2020). Adaptive\ntrust region policy optimization: Global convergence\nand faster rates for regularized mdps. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 34, pages 5668–5675.\nSutton, R. S. (2018).\nReinforcement learning: An\nintroduction. A Bradford Book.\nSutton, R. S., McAllester, D., Singh, S., and Mansour,\nY. (1999). Policy gradient methods for reinforcement\nlearning with function approximation. Advances in\nneural information processing systems, 12.\nTodorov, E., Erez, T., and Tassa, Y. (2012). Mujoco:\nA physics engine for model-based control. In 2012\nIEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 5026–5033. IEEE.\nTomar, M., Shani, L., Efroni, Y., and Ghavamzadeh,\nM. (2020). Mirror descent policy optimization. arXiv\npreprint arXiv:2005.09814.\nVaswani, S., Bachem, O., Totaro, S., Müller, R., Garg,\nS., Geist, M., Machado, M. C., Castro, P. S., and\nRoux, N. L. (2021). A general class of surrogate func-\ntions for stable and efficient reinforcement learning.\narXiv preprint arXiv:2108.05828.\nVaswani, S., Kazemi, A., Babanezhad Harikandeh, R.,\nand Le Roux, N. (2024). Decision-aware actor-critic\nwith function approximation and theoretical guar-\nantees. Advances in Neural Information Processing\nSystems, 36.\nWilliams, R. J. (1992).\nSimple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229–256.\nXiao, L. (2022). On the convergence rates of policy\ngradient methods.\nJournal of Machine Learning\nResearch, 23(282):1–36.\nYuan, R., Du, S. S., Gower, R. M., Lazaric, A., and\nXiao, L. (2023). Linear convergence of natural policy\ngradient methods with log-linear policies. In Inter-\nnational Conference on Learning Representations.\nZhong, H. and Zhang, T. (2024). A theoretical analysis\nof optimistic proximal policy optimization in linear\nmarkov decision processes. Advances in Neural In-\nformation Processing Systems, 36.\n\nSupplementary Material\nOrganization of the Appendix\nA Multi-armed Bandit Proofs\nB MDP Proofs\nC Tabular MDP Experiments\nD Additional Details for Stable Baselines Experiments\nA\nMulti-armed Bandit Proofs\nTheorem 1. The SPMA update in Eq. (2) with (i) a constant step-size η ≤1, and (ii) uniform initialization i.e.\nπ0(a) = 1\nK for all a converges as:\nr(a∗) −⟨πT , r⟩≤\n\u0012\n1 −1\nK\n\u0013\nexp\n\u0012−η ∆min T\nK\n\u0013\n,\nwhere T is the number of iterations, a∗is the optimal arm i.e. a∗= arg maxa r(a) and ∆min := mina̸=a∗∆(a∗, a) =\nr(a∗) −r(a) is the gap.\nProof. . As in equation (2), we can write the update for arm a as following where ∆(a, a′) = r(a) −r(a′),\nπt+1(a) = πt(a)\n\n1 + η\nX\na′̸=a\nπt(a′)∆(a, a′)\n\n\n1 −πt+1(a∗) = 1 −πt(a∗) −η πt(a∗)\n\nX\na′̸=a∗\nπt(a′)∆(a∗, a′)\n\n\n(6)\nWe first find a lower-bound for P\na′̸=a∗πt(a′)∆(a∗, a′):\nX\na′̸=a∗\nπt(a′)∆(a∗, a′) ≥∆min\nX\na′̸=a∗\nπt(a′)\n= ∆min(1 −πt(a∗))\n(7)\nNext, we observe that P\na′̸=a∗πt(a′)∆(a∗, a′) ≥0. Using this information and starting with a uniform initialization\nfor selecting an arm implies a monotonic improvement on the probability of selecting the optimal arm:\nπt+1(a∗) > πt(a∗) > ... > π0(a∗) = 1\nK\n(8)\n\nFast Convergence of Softmax Policy Mirror Ascent\nLet ϵt = 1 −πt(a∗).\nϵt+1 = ϵt −η πt(a∗)\n\nX\na′̸=a∗\nπt(a′)∆(a∗, a′)\n\n\n≤ϵt −η\nK\n\nX\na′̸=a∗\nπt(a′)∆(a∗, a′)\n\n\n(using (8))\n≤ϵt −η∆min\nK\nϵt\n(using (7))\n= ϵt\n\u0012\n1 −η∆min\nK\n\u0013\nRecursing from t = 0 to t = T −1 we have:\nϵT ≤ϵ0\n\u0012\n1 −η∆min\nK\n\u0013T\n≤ϵ0 exp\n\u0012−η∆minT\nK\n\u0013\n(using 1 −x ≤exp(−x))\n=\n\u0012\n1 −1\nK\n\u0013\nexp\n\u0012−η∆minT\nK\n\u0013\nFinally, we define the sub-optimality gap, δT := r(a∗) −⟨πT , r⟩:\nδT =\nX\na′\nπT (a′) [r(a∗) −r(a′)]\n=\nX\na′̸=a∗\nπT (a)∆(a∗, a)\n≤max\na′ ∆(a∗, a′)\nX\na′̸=a∗\nπT (a)\n= max\na′ ∆(a∗, a′)(1 −πT (a∗))\n≤1 −πT (a∗)\n(using the fact 0 ≤r ≤1)\n= ϵT\n≤\n\u0012\n1 −1\nK\n\u0013\nexp\n\u0012−η∆minT\nK\n\u0013\nA.1\nSuper-linear Rate for Bandits\nIn order to achieve the desired fast rate of convergence, we modify the update in Eq. (2) to use a set of\n\u0000K\n2\n\u0001\nconstant gap-dependent step-sizes {ηa,a′}a,a′∈[K]. The new update can be written as:\nπt+1(a) = πt(a) [1 +\nX\na′̸=a\nπt(a′) ηa,a′ ∆(a, a′)]\n(9)\nThe following theorem shows that the above update results in super-linear convergence.\nTheorem 4. Using the SPMA update in Eq. (9) with (i) ηa,a′ =\n1\n|∆(a,a′)| and a (ii) uniform initialization similar\nto Theorem 1 results in valid probability distributions and converges as:\nr(a∗) −⟨πT , r⟩≤\n\u0014\u0012\n1 −1\nK\n\u0013\u00152T\nwhere T is the number of iterations, a∗is the optimal arm and ∆(a, a′) := r(a) −r(a′) represents the reward gap\nbetween arms a and a′.\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nProof. We define ∆(a, a′) := r(a) −r(a′),\nAπt = r(a) −⟨πt, r⟩\n=\nX\na′\nπt(a′)[r(a) −r(a′)]\n=\nX\na′\nπt(a′)∆(a, a′)\nChoosing different step sizes for every pair of arms, depending on their corresponding gap, ηa,a′ =\n1\n|∆(a,a′)| we get\nthe following update for πt+1(a):\nπt+1(a) = πt(a)\n\n1 +\nX\na′̸=a\nηa,a′πt(a′)∆(a, a′)\n\n\n= πt(a)\n\n1 +\nX\na′̸=a\nπt(a′) sign (∆(a, a′))\n\n\n(i)\nNow we check if πt+1 is a probability distribution with this choice of η. Note that ∆(a, a′) = −∆(a′, a).\nX\na\nπt+1(a) =\nX\na\nπt(a) +\nX\na\nπt(a)\nX\na′̸=a\nπt(a′) sign (∆(a, a′))\n= 1 +\nX\n(a,a′),a̸=a′\nπt(a)πt(a′)(sign (∆(a, a′)) + sign (∆(a′, a)))\n= 1 +\nX\n(a,a′),a̸=a′\nπt(a)πt(a′)(sign (∆(a, a′)) −sign (∆(a, a′)))\n( since ∆(a, a′) = −∆(a′, a))\n= 1\nFurthermore, it is clear that πt(a) ∈[0, 1]. Based on this we just need to show that the probability of the optimal\narm a∗converges to 1.\nComputing the probability of pulling the optimal arm using update (i):\nπt+1(a∗) = πt(a∗)\n\n1 +\nX\na′̸=a∗\nπt(a′) sign (∆(a∗, a′))\n\n\n= πt(a∗)\n\n1 +\nX\na′̸=a∗\nπt(a′)\n\n\n(∆(a∗, a′) > 0 ∀a′)\n= πt(a∗) [2 −πt(a∗)]\n(ii)\nWe use induction to show πt(a∗) = 1 −\n\u0002\n(1 −1\nK )\n\u00032t\nsolves the recurrence relation (ii). We consider the uniform\ndistribution over the arms at the initialization i.e. π0(a) =\n1\nK , ∀a ∈A. For the base case, we show the suggested\nsolution satisfies recursion (ii):\nπ1(a∗) = 1\nK\n\u0012\n2 −1\nK\n\u0013\n(using the recursion in (ii))\n=\n\u0012\n1 −1 + 1\nK\n\u0013 \u0012\n1 + 1 −1\nK\n\u0013\n= 1 −\n\u0014\u0012\n1 −1\nK\n\u0013\u00152\n\nFast Convergence of Softmax Policy Mirror Ascent\nAssuming the suggested solution is true for t, we show it is also true for t + 1:\nπt+1(a∗) =\n\"\n1 −\n\u0012\n1 −1\nK\n\u00132t# \"\n2 −1 +\n\u0012\n1 −1\nK\n\u00132t#\n= 1 −\n\"\u0012\n1 −1\nK\n\u00132t+1#\nLet δT := r(a∗) −⟨πT , r⟩represent the sub-optimality gap.\nδT =\nX\na′\nπT (a′) [r(a∗) −r(a′)]\n=\nX\na′̸=a∗\nπT (a)∆(a∗, a)\n≤max\na′ ∆(a∗, a′)\nX\na′̸=a∗\nπT (a)\n≤1 −πT (a∗)\n(using the fact 0 ≤r ≤1)\n=\n\u0014\u0012\n1 −1\nK\n\u0013\u00152T\n(using the formula for πT (a∗))\nB\nMDP Proofs\nB.1\nTabular Setting\nLemma 1. For any policy πt we have\nX\na\nπt(a|s)[Aπt(s, a)]2 ≥Ct max\na\nAπt(s, a)\nwhere Ct := mins{πt( ˜\nAt(s)|s) ∆t(s)}, ˜\nAt(s) := arg maxa∈A Qπt(s, a), πt( ˜\nAt(s)|s) = P\na∈˜\nAt(s) πt(at(s)|s) and\n∆t(s) := maxa∈A Qπt(s, a) −maxa/∈˜\nA Qπt(s, a).\nProof. Recall ˜\nAt(s) := arg maxa∈A Aπt(s, a) i.e. ˜\nAt(s) is a set containing actions with maximum advantage for\nstate s. Let’s define πt( ˜\nAt(s)|s) = P\na∈˜\nAt(s) πt(˜at(s)|s). We can split the sum on the LHS of the above over\n˜\nAt(s):\nX\na\nπt(a|s)[Aπt(s, a)]2 =\nX\na∈˜\nAt(s)\nπt(˜at(s)|s)[max\na\nAπt(s, a)][max\na\nAπt(s, a)]\n+\nX\na/∈˜\nAt(s)\nπt(a|s)[Aπt(s, a)]2\n= πt( ˜\nAt(s)|s)[max\na\nAπt(s, a)][max\na\nAπt(s, a)]\n+\nX\na/∈˜\nAt(s)\nπt(a|s)[Aπt(s, a)]2\n(10)\nLet ˜πt be the following distribution over the actions.\n˜πt(a|s) =\n(\n0\nif a ∈˜\nAt(s)\nπt(a|s)\n1−πt( ˜\nAt(s)|s)\notherwise\nRe-writing P\na πt(a|s)Aπt(s, a) = 0 using the above distribution we obtain:\n(1 −πt( ˜\nAt(s)|s)) Ea∼˜\nπt[Aπt(s, a)] + πt( ˜\nAt(s)|s)[max\na\nAπt(s, a)] = 0\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\n(1 −πt( ˜\nAt(s)|s)) Ea∼˜\nπt[Aπt(s, a)] = −πt( ˜\nAt(s)|s)[max\na\nAπt(s, a)]\n(11)\nExpanding the second term in Eq. 10 using ˜πt we obtain:\nX\na/∈˜\nAt(s)\nπt(a|s)[Aπt(s, a)]2 = (1 −πt( ˜\nAt(s)|s)) Ea∼˜\nπt[Aπt(s, a)]2\n≥(1 −πt( ˜\nAt(s)|s)) (Ea∼˜\nπt[Aπt(s, a)])2\n(using E[x2] ≥(E[x])2)\n= (1 −πt( ˜\nAt(s)|s)) (Ea∼˜\nπt[Aπt(s, a)]) (Ea∼˜\nπt[Aπt(s, a)])\n= −πt( ˜\nAt(s)|s)[max\na\nAπt(s, a)] (Ea∼˜\nπt[Aπt(s, a)])\n(using Eq. 11)\nPlugging in the result above into Eq. 10 we obtain:\nX\na\nπt(a|s)[Aπt(s, a)]2 ≥πt( ˜\nAt(s)|s)[max\na\nAπt(s, a)][max\na\nAπt(s, a)]\n−πt( ˜\nAt(s)|s)[max\na\nAπt(s, a)] (Ea∼˜\nπt[Aπt(s, a)])\n≥πt( ˜\nAt(s)|s)[max\na\nAπt(s, a)]\nh\nmax\na\nAπt(s, a) −Ea∼˜\nπt[Aπt(s, a)]\ni\n≥πt( ˜\nAt(s)|s)[max\na\nAπt(s, a)]\n\nmax\na\nAπt(s, a) −max\na/∈˜\nA\nAπt(s, a)\n|\n{z\n}\n:=∆t(s)\n\n\n= πt( ˜\nAt(s)|s)[max\na\nAπt(s, a)]∆t(s)\n≥Ct max\na\nAπt(s, a)\nLemma 2. Using the update πt+1(a|s) = πt(a|s) (1 + ηAπt(s, a)) with a step-size η < min\nn\n1 −γ,\n1\nCt(1−γ)\no\n, at\nany iteration t and state s ∈S, we have\nV ∗(s) −V πt+1(s) ≤[1 −η Ct(1 −γ)] [V ∗(s) −V πt(s)]\nwhere Ct := mins{πt( ˜\nAt(s)|s) ∆t(s)}, ˜\nAt(s) := arg maxa Qπt(s, a), πt( ˜\nAt(s)|s) = P\na∈˜\nAt(s) πt(at(s)|s), ∆t(s) :=\nmaxa Qπt(s, a) −maxa/∈˜\nA Qπt(s, a), and V ∗(s) is the value function corresponding to the optimal policy π∗at\ns ∈S.\nProof. First, we use the value difference Lemma to show the SPMA update in Eq. (3) leads to a monotonic\nimprovement in the value function.\nV πt+1(s) −V πt(s) =\n1\n1 −γ Es∼dπt+1\n\"X\na\nπt+1(a|s)Aπt(s, a)\n#\n(12)\nPlugging update Eq. (3) into the term within the brackets, we obtain the following:\nX\na\nπt+1(a|s)Aπt(s, a) =\nX\na\nπt(a|s)Aπt(s, a)[1 + ηAπt(s, a)]\n=\nX\na\nπt(a|s)Aπt(s, a) + η\nX\na\nπt(a|s)[Aπt(s, a)]2\n= η\nX\na\nπt(a|s)[Aπt(s, a)]2\n> 0\n\nFast Convergence of Softmax Policy Mirror Ascent\nHence, V πt+1(s) ≥V πt(s). Using Lemma 1, we have:\nη\nX\na\nπt(a|s)[Aπt(s, a)]2 ≥η Ct max\na\nAπt(s, a)\n(13)\nCombining the above with the result from the value difference Lemma we have:\nX\na\nπt+1(a|s)Aπt(s, a) = η\nX\na\nπt(a|s)[Aπt(s, a)]2\n≥η Ct max\na\nAπt(s, a)\n(14)\nWe now show a linear convergence when using the update in Eq. (3). Let T be the Bellman optimality operator\ndefined as:\n(Tv)(s) = max\na {r(s, a) + γ\nX\ns′\nPr[s′|s, a]v(s′)}\nApplying the operator at iteration t we obtain:\nTV πt(s) −V πt(s) = max\na\nQπt(s, a) −V πt(s) = max\na\nAπt(s, a)\n(15)\nLet T π be an operator w.r.t π defined as:\nT π(v) =\nX\na\nπ(a|s)r(s, a) + γ\nX\na\nπ(a|s)\nX\ns′\nPr[s′|s, a]v(s′)\nApplying T π to V π′(s) results in:\nT πV π′(s) =\nX\na\nπ(a|s)r(s, a) + γ\nX\na\nπ(a|s)\nX\ns′\nPr[s′|s, a]V π′(s)\n=\nX\na\nπ(a|s)Qπ′(s, a)\nUsing the above we obtain:\nT πt+1V πt(s) −V πt(s) =\nX\na\nπt+1(a|s)Aπt(s, a)\n≥η Ct max\na\nAπt(s, a)\n(using Ineq. 14)\n= η Ct [TV πt(s) −V πt(s)]\n(using Eq. 15)\nAssuming π∗is the optimal policy we have:\nV ∗(s) −V πt+1(s) = V ∗(s) −T πt+1V πt+1(s)\n(since T πV π(s) = V π(s))\n≤V ∗(s) −T πt+1V πt(s)\n(since V πt+1(s) ≥V πt(s) ∀s)\n= V ∗(s) −V πt(s) −[T πt+1V πt(s) −V πt(s)]\n(add and subtract V πt(s))\n≤V ∗(s) −V πt(s) −η Ct [TV πt(s) −V πt(s)]\n= η Ct[V ∗(s) −V πt(s)] + (1 −η Ct)[V ∗(s) −V πt(s)] −η Ct [TV πt(s) −V πt(s)]\n= η Ct [TV ∗(s) −V πt(s) −TV πt(s) + V πt(s)] + (1 −η Ct) [V ∗(s) −V πt(s)]\n= η Ct [TV ∗(s) −TV πt(s)] + (1 −η Ct) [V ∗(s) −V πt(s)]\n≤γ η Ct [V ∗(s) −V πt(s)] + (1 −η Ct) [V ∗(s) −V πt(s)]\n(T is a γ contraction map)\n= [1 −η Ct(1 −γ)] [V ∗(s) −V πt(s)]\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nTheorem 2. Using the SPMA update in Eq. (3) with a step-size η < min\nn\n1 −γ,\n1\nCt(1−γ)\no\nconverges as:\n\r\r\rV π∗−V πT\n\r\r\r\n∞≤\n T −1\nY\nt=0\nαt\n! \r\r\rV π∗−V π0\n\r\r\r\n∞,\nwhere αt := (1 −η Ct (1 −γ)), Ct := mins{πt(˜at(s)|s) ∆t(s)}, ˜at(s) := arg maxa Qπt(s, a) and ∆t(s) :=\nmaxa Qπt(s, a) −maxa̸=˜a Qπt(s, a).\nProof. Using Lemma 2 we have\nV ∗(s) −V πt+1(s) ≤[1 −η Ct(1 −γ)] [V ∗(s) −V πt(s)]\nIf η <\n1\nCt(1−γ), both sides of the inequality above are positive leading to |V ∗(s) −V πt+1(s)| ≤(1 −η Ct(1 −\nγ))|V ∗(s) −V πt(s)|. This is true for all s ∈S, hence we have:\n\r\r\rV π∗−V πt+1\n\r\r\r\n∞≤(1 −η Ct(1 −γ))\n\r\r\rV π∗−V πt\n\r\r\r\n∞\n= αt\n\r\r\rV π∗−V πt\n\r\r\r\n∞\nRecursing from t = 0 to t = T −1 we obtain a linear convergence:\n\r\r\rV π∗−V πT\n\r\r\r\n∞≤\n T −1\nY\nt=0\nαt\n! \r\r\rV π∗−V π0\n\r\r\r\n∞\nB.2\nFunction Approximation With Exact Advantage\nRecall the definitions of ˜ℓt and ℓt\n˜ℓt(θ) =\nX\ns\ndπt(s) KL(πt+1/2(·|s) || πθ(·|s))\nℓt(θ) =\nX\ns∼τ\nKL (h(fθt(s, ·))(1 + ηAπt(s, ·)) || h(fθ(s, ·)))\nTheorem 3. Under assumption 1-3, Algorithm 1 with η < min\nn\n1 −γ,\n1\nCt(1−γ)\no\nconverges as,\nJ(π∗) −J(πT )\n≤\n T −1\nY\nt=0\nαt\n!\n(J(π∗) −J(π0)) + β\nT −1\nX\nt=0\nT −1\nY\ni=t+1\nαi ,\nwhere β =\n√\n2\n(1−γ)2ρmin\n√ϵstat + ϵbias and αt has the same definition as in Theorem 2.\nProof. We assumed that zθ(s, a) := fθ(s, a) ∀(s, a) and zt(s, a) = fθt+1(s, a) where fθ : RSA →R is a complex,\nnon-linear model. We remind the following updates:\nzt+1/2 = arg max\n¯z∈R|S||A| {⟨∇zJ(zt), z⟩−1/ηDΦ(z, zt)}\n∇Φ(zt+1/2) = ∇Φ(zt) + η∇zJ(zt)\n(Mirror Ascent update without projection)\nπt+1/2 = h(zt+1/2)\n(h is softmax)\nπt+1/2(a|s) = πt(a|s)(1 + ηAπt(s, a))\nθt+1 = (S)GD(ℓt(θ))\n(using (Stochastic)Gradient Descent for m iteration to minimize ℓt)\nπt+1 = h(zt+1)\n\nFast Convergence of Softmax Policy Mirror Ascent\nzt+1/2 is an unprojected update for the tabular setting and therefore using Lemma 2 we have:\nV ∗(s) −V πt+1/2(s) ≤[1 −η Ct(1 −γ)] [V ∗(s) −V πt(s)]\nBy adding and removing V πt+1(s) to both sides and rearranging we have\nV ∗(s) −V πt+1(s) ≤[1 −η Ct(1 −γ)] [V ∗(s) −V πt(s)] + V πt+1/2(s) −V πt+1(s).\nTaking the expectation w.r.t. ρ, we obtain:\nJ(π∗) −J(πt+1) ≤[1 −η Ct(1 −γ)] [J(π∗) −J(πt)] + J(πt+1/2) −J(πt+1)\n|\n{z\n}\n:=E1\nThe term E1 can be bounded as follows:\nE1 =\nX\ns\ndπt+1/2(s)⟨Qπt+1(s, .), πt+1/2(.|s) −πt+1(.|s)⟩\n≤\nX\ns\ndπt+1/2(s) ∥Qπt+1(s, .)∥∞\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n(Holder inequality)\n≤\n1\n1 −γ\nX\ns\ndπt+1/2(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n(since ∥Qπt+1(s, .)∥∞≤\n1\n1−γ )\n=\n1\n1 −γ\nX\ns\ndπt+1/2(s)\nρ(s)\nρ(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n≤\n1\n1 −γ\n\r\r\r\r\ndπt+1/2\nρ\n\r\r\r\r\n∞\nX\ns\nρ(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n≤\n1\n(1 −γ)ρmin\nX\ns\nρ(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n(since dπt+1/2(s) ≤1 and using assumption 3)\n≤\n1\n(1 −γ)2ρmin\nX\ns\ndπt(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n(sicne dπt ≥(1 −γ)ρ)\n≤\n√\n2\n(1 −γ)2ρmin\nX\ns\ndπt(s)\nq\nKL(πt+1/2(.|s) || πt+1(.|s))\n(using strong convexity of KL divergence or Pinsker’s inequality)\n≤\n√\n2\n(1 −γ)2ρmin\nv\nu\nu\nu\nt\nX\ns\ndπt(s)KL(πt+1/2(.|s) || πt+1(.|s))\n|\n{z\n}\n:=E2\n(due to concavity of √and Jensen’s inequality)\nwhere E2 can be bounded as follows.\nE2 = ˜ℓt(θt+1)\n= ˜ℓt(θt+1) −min\nθ\n˜ℓt(θt+1) + min\nθ\n˜ℓt(θt+1)\n≤ϵstat + min\nθ\n˜ℓt(θt+1)\n(using assumption 1)\n≤ϵstat + ϵbias\n(using assumption 2)\nPutting everything together we have:\nE1 ≤\n√\n2\n(1 −γ)2ρmin\n√ϵstat + ϵbias\n|\n{z\n}\n:=β\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nTherefore we have\nJ(π∗) −J(πt+1) ≤[1 −η Ct(1 −γ)]\n|\n{z\n}\nαt\n[J(π∗) −J(πt)] + β\nUnrolling the above recursion for T iterations,\nJ(π∗) −J(πT ) ≤\n T −1\nY\nt=0\nαt\n!\n(J(π∗) −J(π0)) + β\nT −1\nX\nt=0\nT −1\nY\ni=t+1\nαt\nB.3\nFunction Approximation With Inexact Advantage\nIn practice computing the Aπt at every iteration is costly. In this section, we assume that we access an oracle\nsuch that at each iteration t, it gives us ˆAπt an approximation of Aπt.\nAssumption 4. Valid Approximation. For all iterations t and (s, a) ∈S × A, | ˆAπt(s, a)| ≤\n1\n1−γ .\nAssumption 5. Approximation Error. For all iterations t and s ∈S,\n\r\r\rAπt(s, .) −ˆAπt(s, .)\n\r\r\r\n∞≤ϵapprox.\nUsing this inexact advantage function, we define the following update and functions\nπt+1/2(a|s) = πt(a|s)(1 + η ˆAπt(s, a))\n(replacing Aπt with ˆAπt)\n˜ℓt(θ) =\nX\ns\ndπt(s) KL(πt+1/2(·|s) || πθ(·|s))\nℓt(θ) =\nX\ns∼τ\nKL\n\u0010\nh(fθt(s, ·))(1 + η ˆAπt(s, ·)) || h(fθ(s, ·))\n\u0011\nSince we use the inexact advantage, we cannot reuse the result of Lemma 2. So we provide a variant of that\nlemma with an inexact advantage.\nLemma 3. Using the update πt+1(a|s) = πt(a|s) (1 + η ˆAπt(s, a)) with (i) a step-size η < min\nn\n1 −γ,\n1\nCt(1−γ)\no\nand (ii) ˆAπt satisfying assumptions 4 and 5, at any iteration t and s ∈S we have\nV ∗(s) −V π(s) ≤[1 −η Ct(1 −γ)] [V ∗(s) −V πt(s)] + ϵapprox\n1 −γ\nwhere Ct := mins{πt( ˜\nAt(s)|s) ∆t(s)},\n˜\nAt(s) := arg maxa Qπt(s, a), πt( ˜\nAt(s)|s) = P\na∈˜\nAt(s) πt(at(s)|s) and\n∆t(s) := maxa Qπt(s, a) −maxa/∈˜\nA Qπt(s, a), and V ∗(s) is the value function corresponding to the optimal policy\nπ∗at s ∈S.\n\nFast Convergence of Softmax Policy Mirror Ascent\nProof. First, we use the value difference Lemma for a state s ∈S\nV π(s) −V πt(s) =\n1\n1 −γ Es′∼dπ\n\"X\na\nπ(a|s′)Aπt(s′, a)\n#\n=\n1\n1 −γ Es′∼dπ\n\"X\na\nπt(a|s′) (1 + η ˆAπt(s′, a))Aπt(s′, a)\n#\n=\n1\n1 −γ Es′∼dπ\n\"X\na\nηπt(a|s′) ˆAπt(s′, a)Aπt(s′, a)\n#\n=\n1\n1 −γ Es′∼dπ\n\"X\na\nηπt(a|s′) ( ˆAπt(s′, a) −Aπt(s′, a) + Aπt(s′, a))Aπt(s′, a)\n#\n=\n1\n1 −γ Es′∼dπ\n\"X\na\nηπt(a|s′) (Aπt(s′, a))2\n#\n|\n{z\n}\n:=T1\n+\n1\n1 −γ Es′∼dπ\n\"X\na\nηπt(a|s′) ( ˆAπt(s′, a) −Aπt(s′, a))Aπt(s′, a)\n#\n|\n{z\n}\n:=T2\nT1 can be bounded using Lemma 1,\nT1 ≥ηCt max\na\nAπt(s, a)\nTo bound T2, we use assumption 5,\nT2 ≥−η\n\"X\na\nπt(a|s′) |( ˆAπt(s′, a) −Aπt(s′, a))||Aπt(s′, a)|\n#\n≥−η\n\"X\na\nπt(a|s′) |( ˆAπt(s′, a) −Aπt(s′, a))|\n1\n1 −γ\n#\n(since Aπ ≤1/(1 −γ))\n≥−η\n\"X\na\nπt(a|s′) ϵapprox\n1 −γ\n#\n(using assumption 5)\n= −ηϵapprox\n1 −γ\nUsing the lower-bound for T1 and T2 we have\nX\na\nπ(a|s)Aπt(s, a) ≥ηCt max\na\nAπt(s, a) −ηϵapprox\n1 −γ\n(16)\nPutting everything together we have,\nV π(s) ≥V πt(s) −ηϵapprox\n(1 −γ)2\n(17)\n≥V πt(s) −ϵapprox\n(1 −γ)\n(since η ≤1 −γ)\n=⇒V πt(s) −V π(s) ≤ϵapprox\n(1 −γ)\n(18)\n(19)\nLet T be the Bellman optimality operator defined as:\n(Tv)(s) = max\na {r(s, a) + γ\nX\ns′\nPr[s′|s, a]v(s′)}\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nApplying the operator at iteration t we obtain:\nTV πt(s) −V πt(s) = max\na\nQπt(s, a) −V πt(s) = max\na\nAπt(s, a)\n(20)\nLet T π be an operator w.r.t π defined as:\nT π(v) =\nX\na\nπ(a|s)r(s, a) + γ\nX\na\nπ(a|s)\nX\ns′\nPr[s′|s, a]v(s′)\nApplying T π to V π′(s) results in:\nT πV π′(s) =\nX\na\nπ(a|s)r(s, a) + γ\nX\na\nπ(a|s)\nX\ns′\nPr[s′|s, a]V π′(s)\n=\nX\na\nπ(a|s)Qπ′(s, a)\nUsing the above we obtain:\nT πV πt(s) −V πt(s) =\nX\na\nπ(a|s)Aπt(s, a)\n≥η Ct max\na\nAπt(s, a) −ηϵapprox\n1 −γ\n(using Eq. (16))\n= η Ct [TV πt(s) −V πt(s)] −ηϵapprox\n1 −γ\n(using Eq. 20)\n≥η Ct [TV πt(s) −V πt(s)] −ϵapprox\n(since η ≤1 −γ)\nUsing Eq. (18) we have\nT πV πt(s) −T πV π(s) = γ\nX\na\nπ(a|s)\nX\ns′\nPr[s′|s, a](V πt(s′) −V π(s′))\n(21)\n≤γ\nX\na\nπ(a|s)\nX\ns′\nPr[s′|s, a]ϵapprox\n1 −γ\n(using Eq. (18))\n= γϵapprox\n1 −γ\n(22)\nAssuming π∗is the optimal policy we have:\nV ∗(s) −V π(s) = V ∗(s) −T πV π(s)\n(since T πV π(s) = V π(s))\n= V ∗(s) −T πV πt(s) + T πV πt(s) −T πV π(s)\n≤V ∗(s) −T πV πt(s) + γϵapprox\n1 −γ\n(using Eq. (22))\n= V ∗(s) −V πt(s) −[T πV πt(s) −V πt(s)] + γϵapprox\n1 −γ\n(add and subtract V πt(s))\n≤V ∗(s) −V πt(s) −η Ct [TV πt(s) −V πt(s)] + ϵapprox + γϵapprox\n1 −γ\n= η Ct[V ∗(s) −V πt(s)] + (1 −η Ct)[V ∗(s) −V πt(s)] −η Ct [TV πt(s) −V πt(s)] + ϵapprox\n1 −γ\n= η Ct [TV ∗(s) −V πt(s) −TV πt(s) + V πt(s)] + (1 −η Ct) [V ∗(s) −V πt(s)] + ϵapprox\n1 −γ\n= η Ct [TV ∗(s) −TV πt(s)] + (1 −η Ct) [V ∗(s) −V πt(s)] + ϵapprox\n1 −γ\n≤γ η Ct [V ∗(s) −V πt(s)] + (1 −η Ct) [V ∗(s) −V πt(s)] + ϵapprox\n1 −γ\n(T is a γ contraction map)\n= [1 −η Ct(1 −γ)] [V ∗(s) −V πt(s)] + ϵapprox\n1 −γ\n\nFast Convergence of Softmax Policy Mirror Ascent\nTheorem 5. Under assumption 1-5, Algorithm 1 with η < min\nn\n1 −γ,\n1\nCt(1−γ)\no\nconverges as,\nJ(π∗) −J(πT )\n≤\n T −1\nY\nt=0\nαt\n!\n(J(π∗) −J(π0)) + β\nT −1\nX\nt=0\nT −1\nY\ni=t+1\nαt ,\nwhere β =\n√\n2\n(1−γ)2ρmin\n√ϵstat + ϵbias + ϵapprox\n1−γ , αt = [1 −η Ct(1 −γ)], Ct := mins{πt( ˜\nAt(s)|s) ∆t(s)}, ˜\nAt(s) :=\narg maxa Qπt(s, a), πt( ˜\nAt(s)|s) = P\na∈˜\nAt(s) πt(at(s)|s) and ∆t(s) := maxa Qπt(s, a) −maxa/∈˜\nA Qπt(s, a).\nProof. Using Lemma 3 with π = πt+1/2,\nV ∗(s) −V πt+1/2(s) ≤[1 −η Ct(1 −γ)] [V ∗(s) −V πt(s)] + ϵapprox\n1 −γ\nThe rest of the proof is similar to the proof of Theorem 3. For completeness, we repeat it here. By adding and\nremoving V πt+1(s) to both sides and rearranging we have\nV ∗(s) −V πt+1(s) ≤[1 −η Ct(1 −γ)] [V ∗(s) −V πt(s)] + V πt+1/2(s) −V πt+1(s) + ϵapprox\n1 −γ .\nTaking the expectation w.r.t. ρ we obtain:\nJ(π∗) −J(πt+1) ≤[1 −η Ct(1 −γ)] [J(π∗) −J(πt)] + J(πt+1/2) −J(πt+1)\n|\n{z\n}\n:=E1\n+ϵapprox\n1 −γ .\nThe term E1 can be bounded as follows:\nE1 =\nX\ns\ndπt+1/2(s)⟨Qπt+1(s, .), πt+1/2(.|s) −πt+1(.|s)⟩\n≤\nX\ns\ndπt+1/2(s) ∥Qπt+1(s, .)∥∞\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n(Holder inequality)\n≤\n1\n1 −γ\nX\ns\ndπt+1/2(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n(since ∥Qπt+1(s, .)∥∞≤\n1\n1−γ )\n=\n1\n1 −γ\nX\ns\ndπt+1/2(s)\nρ(s)\nρ(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n≤\n1\n1 −γ\n\r\r\r\r\ndπt+1/2\nρ\n\r\r\r\r\n∞\nX\ns\nρ(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n≤\n1\n(1 −γ)ρmin\nX\ns\nρ(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n(since dπt+1/2(s) ≤1 and using assumption 3)\n≤\n1\n(1 −γ)2ρmin\nX\ns\ndπt(s)\n\r\rπt+1/2(.|s) −πt+1(.|s)\n\r\r\n1\n(sicne dπt ≥(1 −γ)ρ)\n≤\n√\n2\n(1 −γ)2ρmin\nX\ns\ndπt(s)\nq\nKL(πt+1/2(.|s) || πt+1(.|s))\n(using strong convexity of KL divergence or Pinsker’s inequality)\n≤\n√\n2\n(1 −γ)2ρmin\nv\nu\nu\nu\nt\nX\ns\ndπt(s)KL(πt+1/2(.|s) || πt+1(.|s))\n|\n{z\n}\n:=E2\n(due to concavity of √)\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nwhere E2 can be bounded as follows:\nE2 = ˜ℓt(θt+1)\n= ˜ℓt(θt+1) −min\nθ\n˜ℓt(θt+1) + min\nθ\n˜ℓt(θt+1)\n≤ϵstat + min\nθ\n˜ℓt(θt+1)\n(using assumption 1)\n≤ϵstat + ϵbias\n(using assumption 2)\nPutting everything together we have:\nE1 ≤\n√\n2\n(1 −γ)2ρmin\n√ϵstat + ϵbias\n|\n{z\n}\n:=β′\nTherefore we have\nJ(π∗) −J(πt+1) ≤[1 −η Ct(1 −γ)]\n|\n{z\n}\nαt\n[J(π∗) −J(πt)] + β′ + ϵapprox\n1 −γ\n|\n{z\n}\n:=β\n.\nUnrolling the above recursion for T iterations,\nJ(π∗) −J(πT ) ≤\n T −1\nY\nt=0\nαt\n!\n(J(π∗) −J(π0)) + β\nT −1\nX\nt=0\nT −1\nY\ni=t+1\nαi\n\nFast Convergence of Softmax Policy Mirror Ascent\nC\nTabular MDP Experiments\nIn this section, we empirically evaluate SPMA on tabular MDP environments. For these experiments, we use\nCliff World [Sutton, 2018] and Frozen Lake [Brockman, 2016] following the setup in Vaswani et al. [2024]. In\nsubsection, C.1 we examine the case where the policy is parametrized using softmax tabular representation. In\nsubsection, C.2 we investigate the function approximation setting as described in Section 4, where the policy is\nparametrized using a linear model.\nC.1\nSoftmax Tabular Representation\nFor this parametrization we initialize z ∈RS×A uniformly , i.e., π0(a|s) =\n1\n|A| for each a and s. Furthermore, for\neach algorithm, we set η using a grid search and pick the step-sizes that result in the best area under the curve\n(AUC). The tabular MDP results suggest SPMA and NPG achieve similar performance and they both outperform\nSPG [Sutton et al., 1999; Schulman et al., 2017] (see Fig. 3). To analyze the sensitivity of each algorithm to the\nchoice of η, we examine each optimizer across different values of η. The results in Fig. 4 suggest that overall SPG\n(in green) is more sensitive to different values of η compared to SPMA (blue) and NPG (red).\nFigure 3: SPMA matches the performance of NPG and they both outperform SPG.\nFigure 4: SPG (green) is more sensitive to η compared to SPMA and NPG (blue and red).\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nC.2\nLinear Functional Approximation (Linear FA)\nFor the Linear FA setting, we use a log-linear policy parametrization: πt(a|s) =\nexp(X(s,a)θ)\nP\na′ exp(X(s,a′)θ), with X ∈RSA×d\nand θ ∈Rd representing the features and parameters. We use constant initialization for θ and following Vaswani\net al. [2024], use tile-coded features for X. As in the previous section, we set η for SPMA and MDPO via grid search\nand report results based on the best AUC. For the inner loop optimization (e.g., minimizing Eq. (5)), we use\nArmijo line search [Armijo, 1966], avoiding an additional grid search for the step-size. For SPG we use the update\nfrom Mei et al. [2020] where Armijo line search is used to set η.\nWe make the following observations from the results: (i) SPG performs poorly in the linear FA setting, while both\nSPMA and MDPO perform well when the parameter dimension d and the number of inner loop optimizations m\nare sufficiently large. (ii) In the CW environment, for smaller d, SPMA converges faster than MDPO (Fig. 5, top\nrow). Increasing m from 25 to 50 narrows the gap between SPMA and MDPO (top vs. bottom row). (iii) In the FL\nenvironment, SPMA and MDPO perform similarly, both outperforming SPG (Fig. 6).\nFigure 5: CW environment: The top row (m = 25) shows that SPMA converges faster than MDPO as d decreases,\nwhile the bottom row (m = 50) shows the gap decreases as the number of inner loop optimizations increases.\nFigure 6: FL environment: The top row (m = 25) and bottom row (m = 50) show that SPMA and MDPO have\nsimilar convergence and both outperform SPG. The performance of both SPMA and MDPO improves as d increases\n(i.e., the bias decreases) and m increases (i.e., the optimization error decreases).\n\nFast Convergence of Softmax Policy Mirror Ascent\nD\nAdditional Details for Stable Baselines Experiments\nIn subsection D.1, we provide additional details on the hyper-parameters used for the results in Section 5. Next,\nwe present an ablation study on the number of inner loop optimization steps (m) in subsection D.2.\nD.1\nAtari and Mujoco Details\nIn the Atari experiments, we use the default hyper-parameters for each method from stable baselines [Raffin\net al., 2021]. This choice is motivated by two factors: i) following the work of Tomar et al. [2020], we aim to\nevaluate the effectiveness of different surrogate losses without conducting an exhaustive search over numerous\nhyper-parameters; ii) the CNN-based actor and critic networks make grid searching over many hyper-parameters\n(e.g., framestack, GAE λ, horizon length, discount factor) computationally infeasible. For a complete list of\nhyper-parameters used in the Atari experiments, see Table 1.\nIn the MuJoCo experiments, we use the default hyper-parameters from stable baselines for each method,\nbut perform a grid search on the Adam inner loop step size for PPO and TRPO-constrained (best among\n[3 × 10−3, 3 × 10−4, 3 × 10−5]) and the probability ratio clipping parameter in PPO (best from [0.1, 0.2, 0.3]). For\nthe regularized surrogates (i.e., the remaining methods: SPMA, MDPO, and TRPO-regularized), we avoid a grid\nsearch for the inner loop step size by using a full batch (i.e., the horizon length) along with the Armijo line\nsearch [Armijo, 1966]. See Table 2 for the full list of hyper-parameters used in the MuJoCo experiments.\nTo set η for the regularized surrogates, we perform a grid search over five fixed values ([0.3, 0.5, 0.7, 0.9, 1.0]).\nAlthough Tomar et al. [2020] anneals η from 1 to 0 during training, we observe that using a constant step size\nresults in better performance. Our grid search strategy for all stable baselines experiments is consistent: we run\nthe experiments for 2 million iterations, select the hyper-parameters that yield the best AUC, and then use these\nhyper-parameters for an additional 8 million iterations.\nHyperparameter\nSPMA\nMDPO\nTRPO_regularized\nTRPO_constrained\nPPO\nReward normalization\n✗\n✗\n✗\n✗\n✗\nObservation normalization\n✗\n✗\n✗\n✗\n✗\nOrthogonal weight initialization\n✓\n✓\n✓\n✓\n✓\nValue function clipping\n✗\n✗\n✗\n✗\n✗\nGradient clipping\n✗\n✗\n✗\n✗\n✓\nProbability ratio clipping\n✗\n✗\n✗\n✗\n✓\nAdam step-size\n3 × 10−4\nMinibatch size\n256\nFramestack\n4\nNumber of environment copies\n8\nGAE λ\n0.95\nHorizon (T)\n128\nNumber of inner loop updates (m)\n5\nEntropy coefficient\n0\nDiscount factor\n0.99\nTotal number of timesteps\n107\nNumber of runs for plot averages\n5\nConfidence interval for plot runs\n∼95%\nTable 1: Hyper-parameters for Atari experiments.\nHyperparameter\nSPMA\nMDPO\nTRPO_regularized\nTRPO_constrained\nPPO\nMinibatch size\n2048\n2048\n2048\n64\n64\nReward normalization\n✗\n✗\n✗\n✗\n✗\nObservation normalization\n✗\n✗\n✗\n✗\n✗\nOrthogonal weight initialization\n✓\n✓\n✓\n✓\n✓\nValue function clipping\n✗\n✗\n✗\n✗\n✗\nGradient clipping\n✗\n✗\n✗\n✗\n✓\nProbability ratio clipping\n✗\n✗\n✗\n✗\n✓\nAdam step-size\n✗\n✗\n✗\n✓\n✓\nGAE λ\n0.95\nHorizon (T)\n2048\nNumber of inner loop updates (m)\n5\nEntropy coefficient\n0\nDiscount factor\n0.99\nTotal number of timesteps\n107\nNumber of runs for plot averages\n5\nConfidence interval for plot runs\n∼95%\nTable 2: Hyper-parameters for MuJoCo experiments.\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nD.2\nAblation Study on the Number of Inner Loop Optimization Steps\nIn this subsection, we investigate the effect of varying the number of inner loop optimization steps (m) in the\nstable baselines experiments. Consistent with Tomar et al. [2020], we observe that using m = 1 results in poor\nperformance, so we focus on larger values of m. In the MuJoCo experiments, increasing m from 5 to 10 and 15\nconsistently improves the performance of SPMA (see Figure 9). Specifically, for larger m, SPMA becomes comparable\nto TRPO-constrained on Hopper and Ant, while outperforming it on HalfCheetah (see Figure 7).\nFor the Atari experiments, we observe that increasing m does not necessarily improve the results across methods\n(see Figure 10). We conjecture that this is a side-effect of using a constant tuned step-size (for m = 5) in the\ninner-loop. In the future, we plan to run the full grid-search for the inner-loop step-size for each value of m.\nAlternatively, we plan to investigate an adaptive way of setting the inner-loop step-size.\n\nFast Convergence of Softmax Policy Mirror Ascent\n(a)\n(b)\nFigure 7: MuJoCo results for m = 10 (a) and m = 15 (b). As m increases from 5 ( Figure 2) to 10 and 15, SPMA\nshows performance comparable to the fine-tuned TRPO-constrained.\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\n(a)\n(b)\nFigure 8: Atari results for m = 10 (top) and m = 15 (bottom). Increasing m does not necessarily lead to\nperformance improvements.\n\nFast Convergence of Softmax Policy Mirror Ascent\nFigure 9: MuJoCo ablation on m: The rows correspond to the Hopper-v4, Walker2d-v4, HalfCheetah-v4, and\nAnt-v4 environments, respectively. As the number of inner loop optimization steps m increases, SPMA shows\nimprovements in expected reward and becomes comparable to the fine-tuned TRPO-constrained.\n\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nFigure 10: Atari ablation on m: The rows correspond to the BeamRider-v4, DemonAttack-v4, Alien-v4, and\nAmidar-v4 games. We observe that increasing m does not necessarily improve results across methods.",
    "pdf_filename": "Fast_Convergence_of_Softmax_Policy_Mirror_Ascent.pdf"
}