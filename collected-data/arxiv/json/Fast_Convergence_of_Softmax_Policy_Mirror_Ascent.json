{
    "title": "Fast Convergence of Softmax Policy Mirror Ascent",
    "abstract": "demonstrated their global convergence to an optimal policy. While such simplified analyses are helpful in Natural policy gradient (NPG) is a common understanding the underlying optimization issues, the policy optimization algorithm and can be resulting methods are rarely used in practice. On the viewed as mirror ascent in the space of prob- other hand, while methods such as TRPO [Schulman, abilities. Recently, Vaswani et al. [2021] in- 2015], PPO [Schulman et al., 2017], MDPO [Tomar et al., troduced a policy gradient method that cor- 2020] are commonly used in deep RL, their theoreti- responds to mirror ascent in the dual space cal analysis in the function approximation setting is of logits. We refine this algorithm, removing quite limited. In particular, existing work either (i) its need for a normalization across actions analyzes these methods only in the impractical tab- and analyze the resulting method (referred ular setting [Tomar et al., 2020; Shani et al., 2020] to as SPMA). For tabular MDPs, we prove or (ii) modifies these algorithms to make them more that SPMA with a constant step-size matches amenabletotheoreticalanalysis[Liuetal.,1906;Zhong the linear convergence of NPG and achieves a and Zhang, 2024]. Unfortunately, these modified al- fasterconvergencethanconstantstep-size(ac- gorithms are quite different from the original variants celerated)softmaxpolicygradient. Tohandle and are not systematically benchmarked on standard large state-action spaces, we extend SPMA to environments. Consequently, there exists a large gap use a log-linear policy parameterization. Un- between PG methods that have theoretical guarantees like that for NPG, generalizing SPMA to the lin- inrealisticsettingsversusthosewhichareimplemented ear function approximation (FA) setting does inpractice. Tomakemattersworse,ithasbeendemon- not require compatible function approxima- strated that code-level implementation details impact tion. Unlike MDPO, a practical generalization the empirical performance more than the underlying ofNPG,SPMAwithlinearFAonlyrequiressolv- algorithm [Engstrom et al., 2019]. ing convex softmax classification problems. We prove that SPMA achieves linear conver- Designing theoretically principled PG algorithms that gence to the neighbourhood of the optimal simultaneouslyhavegoodempiricalperformanceonthe value function. We extend SPMA to handle standard set of benchmarks is the main motivation be- non-linear FA and evaluate its empirical per- hind this work. To that end, we leverage an algorithm formance on the MuJoCo and Atari bench- firstproposedbyVaswanietal.[2021],whichwemodify marks. Our results demonstrate that SPMA to remove the need for normalization. We coin this re- consistently achieves similar or better perfor- finementSoftmaxPolicyMirrorAscent(referredtoas mance compared to MDPO, PPO and TRPO. SPMA). WeshowthatSPMAhascomparableconvergence guarantees as existing theoretical techniques [Lu et al., 1 Introduction 2024; Yuan et al., 2023] in the tabular and function Policy gradient (PG) methods [Williams, 1992; Sut- approximation settings, while achieving comparable ton et al., 1999; Konda and Tsitsiklis, 2000; Kakade, practical performance as PPO, TRPO and MDPO, without 2001] have been critical to the achievements of rein- additional algorithmic modifications. In particular, we forcement learning (RL). Although the PG objective is make the following contributions. non-concave,recenttheoreticalresearch[Agarwaletal., Contribution 1: In Section 3, we focus on the multi- 2021;Meietal.,2020,2021;BhandariandRusso,2021; armed bandit and tabular MDP settings, where the Lan, 2023; Shani et al., 2020; Liu et al., 2024; Lu et al., number of parameters scales with the number of states 2024; Alfano and Rebeschini, 2022; Yuan et al., 2023] and actions. We develop the SPMA algorithm, which parameterizes the policy using the softmax function 4202 voN 81 ]GL.sc[ 1v24021.1142:viXra",
    "body": "Fast Convergence of Softmax Policy Mirror Ascent\nReza Asad1 Reza Babanezhad2 Issam Laradji3 Nicolas Le Roux4 Sharan Vaswani1\n1Simon Fraser University 2 Samsung AI 3 ServiceNow 4Mila, Université de Montréal, McGill\nAbstract has analyzed PG methods in simplified settings and\ndemonstrated their global convergence to an optimal\npolicy. While such simplified analyses are helpful in\nNatural policy gradient (NPG) is a common\nunderstanding the underlying optimization issues, the\npolicy optimization algorithm and can be\nresulting methods are rarely used in practice. On the\nviewed as mirror ascent in the space of prob-\nother hand, while methods such as TRPO [Schulman,\nabilities. Recently, Vaswani et al. [2021] in-\n2015], PPO [Schulman et al., 2017], MDPO [Tomar et al.,\ntroduced a policy gradient method that cor-\n2020] are commonly used in deep RL, their theoreti- responds to mirror ascent in the dual space\ncal analysis in the function approximation setting is\nof logits. We refine this algorithm, removing\nquite limited. In particular, existing work either (i)\nits need for a normalization across actions\nanalyzes these methods only in the impractical tab-\nand analyze the resulting method (referred\nular setting [Tomar et al., 2020; Shani et al., 2020]\nto as SPMA). For tabular MDPs, we prove\nor (ii) modifies these algorithms to make them more\nthat SPMA with a constant step-size matches\namenabletotheoreticalanalysis[Liuetal.,1906;Zhong\nthe linear convergence of NPG and achieves a\nand Zhang, 2024]. Unfortunately, these modified al-\nfasterconvergencethanconstantstep-size(ac-\ngorithms are quite different from the original variants\ncelerated)softmaxpolicygradient. Tohandle\nand are not systematically benchmarked on standard\nlarge state-action spaces, we extend SPMA to\nenvironments. Consequently, there exists a large gap\nuse a log-linear policy parameterization. Un-\nbetween PG methods that have theoretical guarantees\nlike that for NPG, generalizing SPMA to the lin-\ninrealisticsettingsversusthosewhichareimplemented\near function approximation (FA) setting does\ninpractice. Tomakemattersworse,ithasbeendemon-\nnot require compatible function approxima-\nstrated that code-level implementation details impact\ntion. Unlike MDPO, a practical generalization\nthe empirical performance more than the underlying\nofNPG,SPMAwithlinearFAonlyrequiressolv-\nalgorithm [Engstrom et al., 2019].\ning convex softmax classification problems.\nWe prove that SPMA achieves linear conver- Designing theoretically principled PG algorithms that\ngence to the neighbourhood of the optimal simultaneouslyhavegoodempiricalperformanceonthe\nvalue function. We extend SPMA to handle standard set of benchmarks is the main motivation be-\nnon-linear FA and evaluate its empirical per- hind this work. To that end, we leverage an algorithm\nformance on the MuJoCo and Atari bench- firstproposedbyVaswanietal.[2021],whichwemodify\nmarks. Our results demonstrate that SPMA to remove the need for normalization. We coin this re-\nconsistently achieves similar or better perfor- finementSoftmaxPolicyMirrorAscent(referredtoas\nmance compared to MDPO, PPO and TRPO. SPMA). WeshowthatSPMAhascomparableconvergence\nguarantees as existing theoretical techniques [Lu et al.,\n1 Introduction\n2024; Yuan et al., 2023] in the tabular and function\nPolicy gradient (PG) methods [Williams, 1992; Sut- approximation settings, while achieving comparable\nton et al., 1999; Konda and Tsitsiklis, 2000; Kakade, practical performance as PPO, TRPO and MDPO, without\n2001] have been critical to the achievements of rein- additional algorithmic modifications. In particular, we\nforcement learning (RL). Although the PG objective is make the following contributions.\nnon-concave,recenttheoreticalresearch[Agarwaletal.,\nContribution 1: In Section 3, we focus on the multi-\n2021;Meietal.,2020,2021;BhandariandRusso,2021;\narmed bandit and tabular MDP settings, where the\nLan, 2023; Shani et al., 2020; Liu et al., 2024; Lu et al.,\nnumber of parameters scales with the number of states\n2024; Alfano and Rebeschini, 2022; Yuan et al., 2023]\nand actions. We develop the SPMA algorithm, which\nparameterizes the policy using the softmax function\n4202\nvoN\n81\n]GL.sc[\n1v24021.1142:viXra\nFast Convergence of Softmax Policy Mirror Ascent\nand uses a mirror ascent (with the log-sum-exp mirror thanbothTRPOandPPOwhilematchingoroutperform-\nmap) update. Compared to NPG that can be viewed ingMDPO,whereasonMuJoCotasks,SPMAoutperforms\nas mirror ascent in the space of probabilities, SPMA PPO and achieves similar or better results than MDPO.\ncorresponds to mirror ascent in the dual space of logits\nand does not require a normalization across actions.\n2 Problem Formulation\nGiven access to the exact policy gradients, we prove\nthat SPMA with a constant step-size converges to the\nWe consider an infinite-horizon discounted Markov de-\noptimalpolicyatalinearrateandthusmatchestherate\ncision process (MDP) [Puterman, 2014] defined by\nof NPG [Khodadadian et al., 2021; Liu et al., 2024]. In\nM = ⟨S,A,P,r,ρ,γ⟩, where S and A represent the\ncomparison, constant step-size softmax policy gradient\nstates and actions, P :S ×A→∆ is the transition\n(SPG) [Agarwal et al., 2021; Mei et al., 2020] can only S\nprobability function, r : S ×A → [0,1] is the reward\nachieve sublinear convergence rates even with Nesterov\nfunction, ρ∈∆ is the initial state distribution, and\nacceleration [Chen et al., 2023]. Hence, by changing S\nγ ∈[0,1) represents the discount factor. In this paper,\nthe mirror map (from Euclidean to log-sum-exp) while\nwe exclusively consider the setting where the number\nusingthesamepolicyparameterization,SPMAcanresult\nof states and actions is finite, but potentially large.\nin an exponential improvement over SPG.\nGiven s ∈ S, the policy π induces a probability dis-\nContribution 2: In order to handle MDPs with large\ntribution π(.|s) over the actions. The action-value\nstate-actionspaces,weusefunctionapproximation(e.g.\nfunction Qπ : S × A → R induced by π is defined\nlinear models or neural networks) to parameterize the\nas Qπ(s,a) := E[(cid:80)∞ γtr(s ,a )|s = s,a = a]\npolicies resulting in the class of log-linear or energy- t=0 t t 0 0\nwhere s ∼ p(.|s ,a ), and a ∼ π(.|s ) for t ≥ 1.\nbased policies [Haarnoja et al., 2017; Agarwal et al., t t−1 t−1 t t\nThe value function corresponding to Qπ starting from\n2021; Yuan et al., 2023] respectively. By interpreting\nstate s is defined as Vπ(s):=E [Qπ(s,a)] with\nthe policy parameterization as a constraint on the a∼π(.|s)\nJ(π) := Vπ(ρ) := E [Vπ(s)] representing the ex-\ncorresponding logits, we use projected mirror ascent to s∼ρ\npected discounted cumulative reward. Furthermore,\nextend SPMA to the FA setting and design Algorithm 1.\nthe advantage function Aπ : S × A → R is de-\nUnlike that for NPG, generalizing SPMA does not require\nfined as Aπ(s,a) := Qπ(s,a) − Vπ(s). The policy\ncompatiblefunctionapproximation, andthusresultsin\nalso induces a discounted state-occupancy measure\na more practical algorithm. Unlike MDPO [Tomar et al.,\ndπ(s) := (1 − γ)(cid:80)∞ γtPrπ[s = s|s ∼ ρ] over\n2020] which results in non-convex surrogates for linear t=0 t 0\nthe states. The objective is to find an optimal pol-\nFA, SPMA requires solving convex softmax classification\nicy π∗ that maximizes the expected reward J(π), i.e.\nproblems in each iteration.\nπ∗ = argmax J(π). As a special case, in the bandit\nπ\nContribution 3: In Section 4.2, we state the condi- setting, |S| = 1, |A| = K, γ = 0, and J(π) = ⟨π,r⟩,\ntions under which Algorithm 1 converges to the neigh- with K representing the number of arms.\nbourhood of the optimal value function, and charac-\nterize the resulting linear convergence rate. Hence, for\nlog-linear policies, Algorithm 1 matches the theoretical 3 Softmax Policy Mirror Ascent:\nconvergence of NPG with compatible function approxi- Tabular Parameterization\nmation [Agarwal et al., 2021; Alfano and Rebeschini,\n2022; Yuan et al., 2023]. Our theoretical results are Softmax policy mirror ascent (referred to as SPMA) rep-\nbetterthanthoseinVaswanietal.[2021]andSchulman resentsthepolicyusingthesoftmaxfunctionh:RA →\n[2015]whichprovesublinearconvergencetoastationary ∆ i.e. π(·|s) = h(z(s,·)) s.t. for all (s,a) ∈ S ×A,\nA\npoint for idealized variants of SPMA and TRPO respec- π(a|s) = exp(z(s,a)) , where the logits z are SA-\ntively. In contrast to Kuba et al. [2022] which prove\n(cid:80) a′exp(z(s,a′))\ndimensional vectors and ∆ is the A-dimensional\nA\nthat the idealized variants of PPO and TRPO converge simplex. We first focus on the tabular parameteri-\nto the optimal policy asymptotically, we characterize zation where the number of parameters scales with\nthe non-asymptotic convergence rate for Algorithm 1.\nthe number of states and actions, and aim to learn\nContribution 4: We empirically evaluate SPMA across the logits corresponding to an optimal policy. With\nsimpleMDPswithtabularandlinearparameterization, some abuse of notation, we use J(z) to refer to J(π)\nAtari games with a discrete action space and a neural where π(·|s) = h(z(s,·)) and state the objective as:\npolicy parameterization with CNNs, and continuous max z∈RSAJ(z).\ncontrol MuJoCo tasks with a continuous action space\nAs the name suggests, SPMA uses mirror ascent (MA)\nand a neural policy parameterization with MLPs. We\nto maximize J(z). For a differentiable, strictly convex\ndemonstrate that SPMA has consistently good perfor-\nmirror map Φ, MA [Beck and Teboulle, 2003; Bubeck\nmance – on Atari games SPMA achieves better results\net al., 2015] is an iterative algorithm whose update at\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\niteration t∈[T] can be stated in two equivalent ways: to calculate [∇J(z t)](s,a) = dπt(s)π t(a|s)Aπt(s,a).\nThe resulting SPMA update is given as:\n∇Φ(z )=∇Φ(z )+η∇ J(z ) (1)\nt+1 t z t (cid:20)\n(cid:20) (cid:21) (cid:88)\n1 z =argmax dπt(s) ⟨π (·|s)Aπt(s,·), z(s,·)⟩\nz =argmax ⟨z−z ,∇ J(z )⟩− D (z,z ) t+1 t\nt+1 z∈RSA t z t η Φ t z∈RSA s\n(cid:21)\n1\n− KL(π (·|s)||h(z(s,·)) .\nwhere z t is the logit at iteration t, η is the step-size η t\nand D (z,z′):=Φ(z)−Φ(z′)−⟨∇Φ(z′),z−z′⟩ is the\nΦ\nBregmandivergencebetweenlogitsz andz′ inducedby Since the above maximization decomposes over the\nthe mirror map Φ. Hence, the MA update at iteration states,wecanwritetheper-stateupdateforeachs∈S\ntcanbeinterpretedasmovinginthegradientdirection in terms of π (·|s)=h(z (s,·)) as follows:\nt+1 t+1\n∇ J(z ) while staying “close” to the logit z , where the\nz t t\nproximity between logits is measured according to the π t+1(a|s)=π t(a|s)(1+ηAπt(s,a)). (3)\nBregman divergence and weighted by 1.\nη\nSimilar to the bandit case, since r(s,a) ∈ [0,1],\n3.1 Bandit Setting π (a|s) is non-negative for η ≤ 1 − γ. Since\nt+1\nIt is instructive to first instantiate the SPMA update for (cid:80) aπ t(a|s)Aπt(s,a) = 0, (cid:80) π t+1(a|s) = 1, and\nthe bandit setting where J(π)=⟨π,r⟩. In this setting, hence Eq. (3) results in a valid policy update.\n∇ J(z) ∈ RA s.t. [∇ J(z)](a) = π(a)[r(a)−⟨π,r⟩].\nz z\nIn order to compare the SPMA update to existing\nFollowing Vaswani et al. [2021], we use the log-sum-\n(cid:80) methods, note that for the tabular parameteriza-\nexp mirror map i.e. ϕ(z) = ln( exp(z(a)). Since\na tion, natural policy gradient (NPG) update [Kakade,\n[∇ϕ(z)](a) = exp(z(a)) = [h(z)](a) = π(a), the\n(cid:80) a′exp(z(a′)) 2001] is the same as policy mirror ascent [Lan, 2023;\nlogitandprobabilityspacesaredualtoeachother, and\nJohnson et al., 2023; Xiao, 2022] and is given by:\nthe∇ϕmapcanbeusedtomovebetweenthesespaces. π t+1(a|s) ∝ π t(a|s) exp(ηAπt(s,a)). In contrast to\nGiven this, the SPMA update can be written as:\nNPG, the SPMA update in Eq. (3) is linear in both η\nand Aπt(s,a) and does not require an explicit nor-\nπ (a)=π (a)(1+η[r(a)−⟨π,r⟩])\nt+1 t malization across actions to ensure valid probability\n(cid:88)\n=π (a)[1+η π (a′)∆(a,a′)], (2) distributions. On the other hand, softmax policy gra-\nt t\na′̸=a dient (SPG) [Agarwal et al., 2021; Mei et al., 2020]\ncorrespondstochoosingthemirrormapϕinEq.(1)to\nwhere ∆(a,a′) := r(a)−r(a′) represents the reward be the Euclidean norm and has the following update:\ngap between arms a and a′. We first ensure that π t+1 z t+1(s,a)=z t(s,a)+ηπ t(a|s)Aπt(s,a). Compared to\nis a valid probability distribution. Since r(a) ∈ [0,1] SPG that uses the softmax policy gradient to update\nfor all a, η ≤1 is sufficient to guarantee that π t+1(a) the logits, SPMA uses the softmax policy gradient to\n(cid:80)\nis non-negative for every a. Moreover, aπ t+1(a) = directlyupdatetheprobabilities. Aswedemonstratein\n(cid:80) (cid:80) (cid:80)\naπ t(a)+η aπ t(a)[r(a)−⟨π,r⟩] = aπ t(a) = 1. the next section, this desirable property enables SPMA\nHence, for η ≤1, Eq. (2) results in a valid update to to achieve faster rates than SPG.\nthe policy. The above update is related to the PROD al-\n3.3 Theoretical Results\ngorithm[Cesa-Bianchietal.,2007]usedfortheexperts\nIn this section, we prove convergence guarantees for\nproblem in the online learning literature. In contrast\nSPMA in the multi-armed bandit and tabular MDP\nto SPMA which is derived from mirror ascent, PROD is\nsettings. We first establish linear convergence for SPMA\nderived using a linearization of the Hedge [Freund and\nfor multi-armed bandits for any constant η ≤1.\nSchapire, 1997] algorithm and requires explicit normal-\nization to obtain probabilities. Theorem 1. The SPMA update in Eq. (2) with (i) a\nconstant step-size η ≤1, and (ii) uniform initialization\n3.2 MDP Setting\ni.e. π (a)= 1 for all a converges as:\nIn order to extend SPMA to the MDP setting, we use a 0 K\n(state-wise) weighted log-sum-exp mirror map, i.e. for (cid:18) (cid:19) (cid:18) (cid:19)\n1 −η∆ T\nalogitz ∈RSA,wedefineΦ(z):=(cid:80) sw(s)ϕ(z(s,·))= r(a∗)−⟨π T,r⟩≤ 1−\nK\nexp Kmin ,\n(cid:80) (cid:80)\nw(s)ln( exp(z(s,a)) where w(s) are the per-\ns a\nstate weights. Following the proof of Vaswani et al. where T is the number of iterations, a∗ is the op-\n[2024, Lemma 11], the resulting Bregman divergence timal arm i.e. a∗ = argmax r(a) and ∆ :=\nis given as: D Φ(z,z′) = (cid:80) sw(s)KL(π′(·|s)||π(·|s)) min a̸=a∗∆(a∗,a)=r(a∗)−r(a) a is the gap. min\nwhereπandπ′arethepoliciescorrespondingtologitsz\nand z′. At iteration t of SPMA, we choose w(s)=dπt(s) Theabovetheorem(provedinAppendixA)showsthat\nandusethepolicygradienttheorem[Suttonetal.,1999] for multi-armed bandit problems, SPMA can achieve\nFast Convergence of Softmax Policy Mirror Ascent\nlinearconvergencetotheoptimalarm,andtheresulting In the next section, we extend SPMA to exploit function\nrate depends on both the gap and the number of arms. approximation to handle large state-action spaces.\nInAppendixA.1, weprovethatSPMAwithspecificgap-\ndependent step-sizes can achieve a global super-linear 4 Handling Function Approximation\nconvergence rate for multi-armed bandits. To the best\nof our knowledge, these are the first global super-linear Handling large MDPs requires function approximation\nrates for PG methods on multi-armed bandit problems. (FA) techniques to share information between states\nand actions. For example, given a set of state-action\nIn the next theorem, we extend the linear convergence features X ∈ RSA×d where d << SA, the log-linear\nresult to tabular MDPs and prove that when given policy parameterization [Agarwal et al., 2021; Alfano\naccess to the exact policy gradients, SPMA results in and Rebeschini, 2022; Yuan et al., 2023] considers poli-\nlinear convergence to the optimal value function for cies of the form: π(a|s) = exp(⟨X(s,a),θ⟩) where\nany sufficiently small constant step-size. (cid:80) a′exp(⟨X(s,a′),θ⟩)\nθ ∈ Rd is the parameter to be learned. Hence, the\nTheorem 2. Using the SPMA update in Eq. (3) with a\nlog-linear policy parameterization can handle large\n(cid:110) (cid:111)\nstep-size η <min 1−γ, 1 converges as: state-action spaces while learning a compressed d-\nCt(1−γ)\ndimensional representation. We interpret the log-linear\n(cid:13)\n(cid:13)Vπ∗\n−VπT(cid:13)\n(cid:13)\n≤(cid:32)T (cid:89)−1\nα\n(cid:33) (cid:13)\n(cid:13)Vπ∗\n−Vπ0(cid:13)\n(cid:13) ,\nAlgorithm 1: SPMA with function approximation\n(cid:13) (cid:13) ∞ t=0 t (cid:13) (cid:13) ∞ Input: θ 0 (parameters for the initial policy π 0), f θ\n(function approximation), T (number of\nwhere α := (1 − ηC (1 − γ)), C := outer-loop), m (number of inner-loops), η\nt t t\nmin s{π t(a˜ t(s)|s)∆t(s)}, a˜ t(s) := argmax aQπt(s,a) (outer-loop step-size), ζ (inner-loop step-size)\nand ∆t(s):=max aQπt(s,a)−max a̸=a˜Qπt(s,a). for t←0 to T −1 do\n1. Interact with the environment using π and\nt\nFor ease of exposition, the above theorem considers form the surrogate function ℓ t(θ) in Eq. (5)\na˜ t(s) to be the unique action maximizing Qπt(s,·) for 2. Initialize inner-loop: ω 0 =θ t\nevery state s and policy π . In Appendix B, we extend for k ←0 to m−1 do\nt\nthis to include multiple optimal actions with a mi- ω k+1 =ω k−ζ∇ ωℓ t(ω k)\nnor change in the definition of the gap. For rewards in end\n(0,1),C t(1−γ)isin(0,1)anddependsontheinitializa- 3. θ t+1 =ω m\ntion. If C :=min t∈[T]C t, then the above implies that 4. Update π t+1(·|s)=h(f θt+1(s,.))\n(cid:16) (cid:17) end\nwhen T ∈ O 1 ln(1/ϵ) , SPMA guarantees that\nηC(1−γ) Return θ\nT\nVπT(s)≥V∗(s)−ϵ for all s∈S. In order to put the\naboveconvergenceresultincontext,notethatSPGwith policy parameterization as a constraint in the space\naconstantstep-sizeresultsinaΘ(1/ϵ)convergence[Mei of logits. Specifically, the logits z are constrained\net al., 2020]. Recently, Chen et al. [2023] proved that to lie in the set Z = {z ∈ RSA|∃θ s.t. z = Xθ\nconstant step-size SPG with Nesterov acceleration can }, meaning that the logits are required to be realiz-\n√\nobtainanO(1/ ϵ)convergence. Incontrast,theabove able by the linear model with features X. We de-\ntheorem demonstrates that by choosing the appropri- fine Π as the corresponding set of feasible policies, i.e.\nate mirror map, constant step-size SPMA can achieve a Π={π|∀s∈S,π(·|s)=h(z(s,·)) s.t. z ∈Z}. Hence,\nfaster O(log(1/ϵ)) rate of convergence. On the other the policies in Π are constrained to be log-linear. Note\nhand, Liu et al. [2024]; Lu et al. [2024] prove that SPG that, as in the case of log-linear policies, Π can be a\nwith adaptive step-sizes can also result in linear con- non-convex set, even when Z is convex. For general\nvergence. However, the resulting rate depends on the energy-based or neural policies [Haarnoja et al., 2017;\ndistribution mismatch ratio\n(cid:13) (cid:13)dπ∗(cid:13)\n(cid:13) that can be expo- Agarwal et al., 2021], π(a|s) ∝ exp(f θ(s,a)) where\n(cid:13) ρ (cid:13) ∞ f : RSA → R is a complex, non-linear model. In\nnentially large in the size of the state space [Li et al., θ\nthis case, the logits are constrained to lie in the set:\n2021]. Incontrast,theconvergenceresultinTheorem2\nZ ={z ∈RSA|∃θ s.t. z(s,a)=f (s,a)}.\nhas no such dependence. The linear convergence rate θ\nin Theorem 2 matches that of NPG with a constant The above interpretation allows us to extend SPMA to\nstep-size [Liu et al., 2024] and compared to Liu et al. the FA setting. Specifically, we use the same mirror\n[2024, Theorem 5.4], it results in a better dependence ascent update as in Eq. (1) with an additional pro-\n(exponential vs polynomial) on the gap ∆t(s). Finally, jection step onto the feasible set Z. Specifically, we\nwenotethatforthetabularparameterization,avariant definez s.t. ∇Φ(z )=∇Φ(z )+η∇ J(z )and\nt+1/2 t+1/2 t z t\nofTRPOhasbeenshowntoachieveO(1/ϵ2)convergence compute z = argmin D (z,z ). This step\nt+1 z∈Z Φ t+1/2\nto the optimal policy [Shani et al., 2020]. denotes the Bregman projection of z onto Z, i.e.\nt+1/2\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nwe seek to find the closest (according to the Bregman min ω(cid:80) sdπt(s)(cid:80) aπ t(a|s)(f ω(s,a)−Qπt(s,a))2.\ndivergence) realizable point (in Z) to the “ideal” point The policy parameters are updated using ωˆ which\nt\nz which corresponds to using the tabular parame- corresponds to the natural gradient direction. While\nt+1/2\nterization. Following Vaswani et al. [2021]; Lavington this approach results in theoretical guarantees\net al. [2023], we convert the above projection prob- (see Section 4.2 for details); for a general parame-\nlemintoanunconstrainedminimizationproblemwhere terization, the resulting algorithm involves changing\n∀(s,a),z (s,a)=f (s,a),z (s,a):=f (s,a)∈Z, the representation of the critic at every iteration.\nt+1 θt+1 θ θ\nθ = argmin D (z ,z ) i.e. we aim to find the Consequently, solving the surrogate is expensive,\nt+1 θ Φ θ t+1/2\nparameter θ that realizes the point z ∈ Z which limiting the practicality of the method.\nθ\nis closest to z . Following Section 3.2, using the\nt+1/2 Comparison to MDPO: A more practical extension\nlog-sum-exp mirror map weighted by dπt(s) at itera-\nof NPG is mirror descent policy optimization [Tomar\ntion t results in the following optimization problem\net al., 2020] (MDPO). Similar to SPMA, MDPO can be\nθ =argmin ℓ˜(θ) where,\nt+1 θ t interpreted as projected (onto the feasible set of\nℓ˜(θ):=(cid:88)\ndπt(s)KL(π (·|s)||π (·|s)) (4)\npolicies) mirror ascent in the space of probabili-\nt t+1/2 θ ties [Vaswani et al., 2021]. The resulting surrogate\ns\n(as a function of the policy parameters) is given by:\n= s∼E dπtH(h(f θt(s,·))(1+ηAπt(s,·)),h(f θ(s,·)))+C t. (cid:80) sdπt(s)KL(π θ(·|s)||h(f θt(s,·) exp(ηQπt(s,·)))).\nUnlike the surrogate in Eq. (4), the MDPO surrogate\nHere, H(p,q) := −E p[ln(q)] = −(cid:80) ap(a)ln(q(a)) is is non-convex even when using a tabular softmax\nthe cross-entropy between distributions p and q and parameterization for the policy, and consequently does\nC t is a constant with respect to θ. We refer to ℓ˜ t(θ) as not have any theoretical guarantees. However, MDPO\nthe ideal surrogate. Minimizing this surrogate requires resultsingoodempiricalperformance,andwecompare\ncalculating the expectation over the states sampled to it in Section 5.\naccording to π . In order to have a practical algorithm,\nt\nwe can run trajectories τ starting from the initial state\nComparison to TRPO: As explained in Vaswani\net al. [2021], the surrogate in Eq. (4) is closely\ndistributionρ,followingthepolicyπ andthussampling\nt\nfrom the dπt distribution (see Agarwal et al. [2021, related to TRPO. In particular, the TRPO update\nconsists of solving the following optimization prob-\nAlgorithm 3] for the detailed procedure). Given these\nsamples, we form the surrogate ℓ t(θ) defined as: l Eem: (cid:80) [Ksd Lπ (t π(s () ·|(cid:80)\ns)a\n||π πt( (a ·| |s s) )A )]π ≤t(s δ, .a S) Pππ Mθθ At( (a (a| i|s )s)\n)\nu, ss eu sc ih nstt eh aa dt\n(cid:88) s∼dπt t θ\ns∼τKL(h(f θt(s,·))(1+ηAπt(s,·))||h(f θ(s,·))) . (5) (cid:80)\naris\nthd mπt( os f)(cid:80) theaπ imt( pa o|s r) tA anπ ct( es s, aa m) plo lg\ninπ\ngπ θθ\nt\nr(\n(\naa a| t|s\ns\ni) o),\n,\ni m.e a. kt ih ne gl to hg e-\nNote that E[ℓ (θ)] = ℓ˜(θ) where the expectation is resulting update more stable [Vaswani et al., 2021]\nt t and (ii) enforces the proximity between policies via\nw.r.t. to dπt. We use m steps of (stochastic) gradient\na regularization rather than a constraint. Enforcing\ndescent to approximately minimize ℓ (θ). Putting ev-\nt the trust-region constraint in TRPO requires additional\nerything together, the algorithm incorporating general\nhyper-parameters, code-level optimizations and com-\nFA is presented in Algorithm 1.\nputation [Engstrom et al., 2019]. In contrast, SPMA is\nLog-linear Policy Parameterization: For this spe- more computationally efficient and simpler to imple-\ncial case, the problem in Eq. (4) is equivalent to a ment in practice. In the next section, we study the\nweighted (according to dπt(s)) multi-class classifica- theoretical properties of Algorithm 1.\ntion for each state. The per-state problem corresponds\nto a softmax classification into A classes using a lin- 4.2 Theoretical Guarantee\near model with features X and soft labels equal to\nFor rewards in [0,1] and for a general policy pa-\nπ (·|s). Computing θ thus involves minimizing\nt+1/2 t+1 rameterization, Vaswani et al. [2021] prove that, for\na smooth, convex function.\nη ≤1−γ, Algorithm 1 results in monotonic improve-\nInthenextsection,wecompareAlgorithm1toexisting ment, i.e. J(π t+1) ≥ J(π t) and hence converges to a\napproaches that incorporate FA. stationary point at an O(1/ϵ) rate. Since J is non-\nconvex and can have multiple stationary points, the\n4.1 Comparison to Existing Approaches\nresult in Vaswani et al. [2021] does not provide suf-\nComparison to NPG: A principled extension of ficient evidence for the good empirical performance\nNPG to handle FA is via the compatible function of Algorithm 1. In this section, we prove that, under\napproximation [Kakade, 2001; Agarwal et al., 2021]. reasonable assumptions similar to existing works, Al-\nAn example of such an algorithm, Q-NPG involves gorithm 1 can converge to the neighbourhood of the\nsolving a quadratic surrogate at each iteration t: ωˆ = optimal value function at a linear rate. The size of\nt\nFast Convergence of Softmax Policy Mirror Ascent\nthe neighbourhood is determined by various practical J(π∗)−J(π )\nT\nfactors such as sampling, inexact optimization, and (cid:32)T−1 (cid:33) T−1 T−1\n(cid:89) (cid:88) (cid:89)\nbiasduetotheFA.Inordertostateourresult, wefirst ≤ α (J(π∗)−J(π ))+β α ,\nt 0 i\nstate and justify our assumptions.\nt=0 t=0 i=t+1\nRecall that in order to have a practical algorithm, we √ √\nwhereβ = 2 ϵ +ϵ andα hasthesame\nminimize ℓ t(θ) obtained by sampling from dπt.\ndefinition\na( s1− inγ)2 Tρ hmeinoremsta 2t\n.\nbias t\nAssumption 1. ExcessRisk: ForalliterationstofAl- The above theorem shows that Algorithm 1 converges\ngorithm 1, |ℓ˜ t(θ t+1)−minℓ˜ t(θ)|≤ϵ stat. linearlytotheneighbourhoodoftheoptimalvaluefunc-\ntion. Furthermore, for the log-linear parameterization,\nThe above assumption quantifies the excess risk in-\nthe size of the neighbourhood can be bounded explic-\ncurred by minimizing a finite sampled “dataset” of\nitly. For example, if the logits z for every t lie in\nstates as compared to minimizing over the population t+1/2\nthe span of the features, ϵ = 0 (this is similar to\nloss ℓ˜(θ). This is a standard assumption in the lit- bias\nt the linear Bellman completeness condition used in the\nerature analyzing the convergence of policy gradient\nanalysis of value-based methods [Munos, 2005]) and\nmethods with FA [Agarwal et al., 2021; Alfano and\nϵ = O(1/n+1/m). By using more samples and\nRebeschini, 2022; Yuan et al., 2023]. If n is the num- stat\nwith more (S)GD iterations, the size of the neighbour-\nber of samples and the surrogate is minimized using\nhood can be made arbitrarily small. Except for the\n(stochastic) gradient descent, using the standard gener-\nneighbourhood term, the above convergence result is\nalization results [Lei and Ying, 2021; Nikolakakis et al.,\nsimilar to that for the tabular setting in Theorem 2.\n2022], we expect ϵ = O(1/n) for the log-linear pa-\nstat The other difference is that the result in the tabular\nrameterization. For example, using m iterations of\nsetting holds in the ℓ norm and thus holds for all\nSGD would result in ϵ = O(1/n+1/m) [Lei and ∞\nstat states, whereas the result in Theorem 3 only holds for\nYing, 2021, Theorem 6]. For a general parameteriza-\na fixed starting state distribution ρ. In practice, Aπt\ntion, where the surrogate might be non-convex, the\nis typically estimated via a critic. To account for this,\nexcess risk can be bounded up to the optimization er-\nwe generalize the proof of Theorem 3 in Appendix B,\nror [Nikolakakis et al., 2022]. Under the appropriate\nand prove that Algorithm 1 converges linearly to a\ntechnical assumptions, ℓ (θ) can been shown to satisfy\nt neighbourhood that depends on an additional term\nthe Polyak-Lojasiewicz condition [Liu et al., 2022]\nproportional to the critic error.\nimplying that the optimization error for (stochastic)\ngradient descent can be made arbitrarily small. The We now compare to the existing theoretical results for\nnext assumption quantifies the bias incurred because PG methods with FA. For the log-linear policy param-\nof a policy parameterization with limited expressive eterization, Q-NPG and its variants have been shown to\npower compared to using the tabular parameterization. achieve linear convergence to the neighbourhood of the\noptimal value function [Agarwal et al., 2021; Alfano\nAssumption 2. Bias: For all iterations t of Algo- andRebeschini,2022;Yuanetal.,2023]. Thesizeofthe\nrithm 1, min ℓ˜(θ)≤ϵ . neighbourhood depends on similar quantities as Theo-\nθ t bias\nrem3. Finally,wenotethatwhileanideal,impractical\nThe above assumption captures the flexibility of the variant of TRPO has a monotonic improvement guaran-\nmodel class being used in the policy parameterization. teesimilartoAlgorithm1[Schulman,2015],itdoesnot\nFor a tabular parameterization where the number of haveconvergenceguaranteescomparabletoTheorem3.\nparameters scales as SA, ϵ = 0, whereas for the\nbias\nlog-linear parameterization, ϵ depends on the ex- 5 Empirical Evaluation\nbias\npressivity of the features. The final assumption is\nWe evaluate SPMA on three types of problems: (i) tab-\nconcerned with exploration and indicates that the ini-\nular MDPs with access to exact policy gradients, (ii)\ntial state distribution has full support implying that\nMDPswithcontinuousstatesbutdiscreteactions,using\nthe method does not require explicit exploration.\ninexact policy gradients, and (iii) MDPs with contin-\nAssumption3. Exploration: ∀s∈S,ρ(s)≥ρ >0. uous state-actions spaces and inexact gradients. For\nmin\ntabular MDPs, we use the tabular parameterization\nThe above assumption is standard in the litera- and compare SPMA against NPG and constant step-size\nture[Agarwaletal.,2021;Xiao,2022]andhelpsisolate SPG [Mei et al., 2020]. For these environments, we\nand study the optimization properties of PG methods. also consider log-linear policies and compare SPMA to\nWe prove the following theorem in Appendix B. MDPO and SPG. For non-tabular environments, we con-\nsider PPO, TRPO and MDPO as baselines. We consider\nTheorem 3. Under assumption 1-3, Algorithm 1 with\ntwo variants of TRPO – TRPO-constrained, the stan-\n(cid:110) (cid:111)\nη <min 1−γ, 1 converges as, dard optimized variant in Raffin et al. [2021] and\nCt(1−γ)\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nFigure 1: On Atari games, where a CNN-based actor network is employed, SPMA matches or surpasses MDPO and\noutperforms PPO, as well as both the constrained and regularized versions of TRPO.\nTRPO-regularized, the original regularized variant. In the remainder of this section, we focus on the non-\nTRPO-constrained is able to effectively enforce the tabular settings with inexact policy gradients. For\ntrust region constraint using conjugate gradient, but these experiments, we follow the protocol of Tomar\nintroduces additional hyper-parameters, requires code- et al. [2020], using 5 seeds and reporting the aver-\nlevel optimization techniques and is computationally age results along with their 95% confidence intervals.\nexpensive. On the other hand, TRPO-regularized Additionally, we employ the actor-critic architecture,\nis significantly more efficient and theoretically prin- policy parameterization, and GAE [Schulman et al.,\ncipled [Lazić et al., 2021], and is similar to SPMA’s 2015] (to estimate the advantage function) from stable\nobjective (see Section 4.1). For details regarding the baselines [Raffin et al., 2021]. We emphasize that, in\nhyper-parameters of all methods for each environment, contrast to prior work, we do not make ad-hoc adjust-\nrefer to Appendices C and D. ments to SPMA (i.e., the actor). To set the step-size\nη in Algorithm 1, we perform a grid search over five\nTabularMDPResults: WepresenttheresultsinAp-\nvalues (fixed across all experiments) and set the inner\npendix C, and summarize the key findings here. We\nloopstep-sizeζ usingArmijolinesearch[Armijo,1966].\nobserve that SPMA and NPG achieve comparable per-\nformance, both consistently outperforming SPG (Fig- Atari and Mujoco Results: We evaluate the per-\nure 3). However, in the linear FA setting, SPMA demon- formance of SPMA compared to the baselines across\nstrates superior performance compared to MDPO in various Atari 2600 games [Bellemare et al., 2013] and\nthe CliffWorld environment [Sutton, 2018] (Figure 5), MuJoCo [Todorov et al., 2012] control tasks from Ope-\nwhile performing similarly in the Frozen Lake environ- nAI Gym [Brockman, 2016]. The observation space for\nment [Brockman, 2016] (Figure 6). In both environ- Atari games consists of a 210×160×3 image, represent-\nments, SPMA and MDPO consistently outperform SPG. ing the current state of the game. The action space in\nFast Convergence of Softmax Policy Mirror Ascent\nFigure 2: On MuJoCo control tasks, where a two-layer MLP actor network is used, SPMA matches or outperforms\nMDPO while consistently outperforming PPO and regularized TRPO. In contrast to the results on Atari games, with\na shallow MLP, TRPO-constrained outperforms all methods.\nthese environments is discrete, whereas in MuJoCo, it [2021]. Overall, our experiments demonstrate that,\nis continuous and by default represented by a diagonal despite being theoretically grounded, SPMA exhibits\nGaussian distribution in Raffin et al. [2021]. Addition- strong empirical performance across various environ-\nally, the actor-critic network for Atari uses a CNN as ments without relying on ad-hoc adjustments.\na feature extractor, while MuJoCo employs an MLP.\n6 Discussion\nComparing the results in Fig. 1 and 2, our key observa-\nWe developed SPMA, a PG method that corresponds to\ntions are as follows: i) SPMA consistently outperforms\nmirror ascent in the dual space of logits. We believe\nor matches MDPO and PPO across all environments; ii)\nthat our paper bridges the gap between theoretical\nalthough TRPO-constrained achieves superior perfor-\nPG methods and practical objectives by presenting\nmance on MuJoCo, its performance degrades consid-\na method that offers strong theoretical convergence\nerably on Atari games. We conjecture that the con-\nguarantees while delivering competitive practical per-\njugate gradient algorithm in TRPO-constrained per-\nformance (compared to PPO, TRPO, MDPO), without re-\nforms poorly when the actor network is a CNN rather\nlying on additional heuristics or algorithmic modifica-\nthan a two-layer MLP; iii) TRPO-regularized, which\ntions. In the future, we aim to develop techniques for\nhas a similar objective as SPMA (see Section 4.1 for a\nadaptively tuning the step-size and avoiding expensive\ncomparison) does not perform as well on MuJoCo and\ngrid-searches. We also plan to develop and analyze an\nhas considerably worse performance on Atari. Hence,\noff-policy variant of SPMA.\nwe observe that replacing the sampling ratio by its\nlog can result in substantial empirical gains. This be-\nhaviour has also been observed for PPO Vaswani et al.\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nReferences Johnson, E., Pike-Burke, C., and Rebeschini, P. (2023).\nOptimal convergence rate for exact policy mirror de-\nAgarwal, A., Kakade, S. M., Lee, J. D., and Mahajan,\nscentindiscountedmarkovdecisionprocesses. arXiv\nG.(2021). Onthetheoryofpolicygradientmethods:\npreprint arXiv:2302.11381.\nOptimality, approximation, and distribution shift. J.\nMach. Learn. Res., 22(98):1–76. Kakade, S. M. (2001). A natural policy gradient. Ad-\nvances in neural information processing systems, 14.\nAlfano, C. and Rebeschini, P. (2022). Linear con-\nKhodadadian, S., Jhunjhunwala, P. R., Varma, S. M.,\nvergence for natural policy gradient with log-\nlinear policy parametrization. arXiv preprint and Maguluri, S. T. (2021). On the linear con-\narXiv:2209.15382. vergence of natural policy gradient algorithm. In\n2021 60th IEEE Conference on Decision and Control\nArmijo, L. (1966). Minimization of functions having (CDC), pages 3794–3799. IEEE.\nlipschitz continuous first partial derivatives. Pacific\nKonda, V. R. and Tsitsiklis, J. N. (2000). Actor-critic\nJournal of mathematics, 16(1):1–3.\nalgorithms. In Advances in neural information pro-\nBeck, A. and Teboulle, M. (2003). Mirror descent and cessing systems, pages 1008–1014.\nnonlinear projected subgradient methods for convex\nKuba, J. G., de Witt, C. S., and Foerster, J. (2022).\noptimization.OperationsResearchLetters,31(3):167–\nMirror learning: A unifying framework of policy\n175. optimisation. arXiv preprint arXiv:2201.02373.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, Lan,G.(2023). Policymirrordescentforreinforcement\nM. (2013). The arcade learning environment: An learning: Linearconvergence,newsamplingcomplex-\nevaluation platform for general agents. Journal of ity, and generalized problem classes. Mathematical\nArtificial Intelligence Research, 47:253–279. programming, 198(1):1059–1106.\nBhandari, J. and Russo, D. (2021). On the linear con- Lavington, J. W., Vaswani, S., Babanezhad, R.,\nvergence of policy gradient methods for finite mdps. Schmidt, M., and Roux, N. L. (2023). Target-based\nInInternational Conference on Artificial Intelligence surrogatesforstochasticoptimization.arXivpreprint\nand Statistics, pages 2386–2394. PMLR. arXiv:2302.02607.\nBrockman, G. (2016). Openai gym. arXiv preprint Lazić, N., Hao, B., Abbasi-Yadkori, Y., Schuurmans,\narXiv:1606.01540. D.,andSzepesvári,C.(2021). Optimizationissuesin\nkl-constrained approximate policy iteration. arXiv\nBubeck, S. et al. (2015). Convex optimization: Algo-\npreprint arXiv:2102.06234.\nrithms and complexity. Foundations and Trends®\nin Machine Learning, 8(3-4):231–357. Lei, Y. and Ying, Y. (2021). Sharper generalization\nbounds for learning with gradient-dominated objec-\nCesa-Bianchi, N., Mansour, Y., and Stoltz, G. (2007). tivefunctions. InInternational Conference on Learn-\nImproved second-order bounds for prediction with ing Representations.\nexpert advice. Machine Learning, 66:321–352.\nLi, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2021).\nChen, Y.-J., Huang, N.-C., Lee, C.-p., and Hsieh, P.-C. Softmax policy gradient methods can take exponen-\n(2023). Accelerated policy gradient: On the con- tial time to converge. In Conference on Learning\nvergence rates of the nesterov momentum for re- Theory, pages 3107–3110. PMLR.\ninforcement learning. In Forty-first International\nLiu, B., Cai, Q., Yang, Z., and Wang, Z. (1906).\nConference on Machine Learning.\nNeural proximal/trust region policy optimization at-\nEngstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., tains globally optimal policy (2019). arXiv preprint\nJanoos, F., Rudolph, L., and Madry, A. (2019). Im- arXiv:1906.10306.\nplementationmattersindeeprl: Acasestudyonppo\nLiu,C.,Zhu,L.,andBelkin,M.(2022).Losslandscapes\nand trpo. In International conference on learning\nand optimization in over-parameterized non-linear\nrepresentations.\nsystems and neural networks. Applied and Compu-\nFreund, Y. and Schapire, R. E. (1997). A decision- tational Harmonic Analysis, 59:85–116.\ntheoretic generalization of on-line learning and an Liu, J., Li, W., and Wei, K. (2024). Elementary\napplication to boosting. Journal of computer and analysis of policy gradient methods. arXiv preprint\nsystem sciences, 55(1):119–139. arXiv:2404.03372.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. Lu, M., Aghaei, M., Raj, A., and Vaswani, S.\n(2017). Reinforcement learning with deep energy- (2024). Towards principled, practical policy gradi-\nbased policies. In International conference on ma- ent for bandits and tabular mdps. arXiv preprint\nchine learning, pages 1352–1361. PMLR. arXiv:2405.13136.\nFast Convergence of Softmax Policy Mirror Ascent\nMei, J., Gao, Y., Dai, B., Szepesvari, C., and Schu- Tomar, M., Shani, L., Efroni, Y., and Ghavamzadeh,\nurmans, D. (2021). Leveraging non-uniformity in M.(2020). Mirrordescentpolicyoptimization. arXiv\nfirst-ordernon-convexoptimization. InInternational preprint arXiv:2005.09814.\nConference on Machine Learning, pages 7555–7564.\nVaswani, S., Bachem, O., Totaro, S., Müller, R., Garg,\nPMLR.\nS., Geist, M., Machado, M. C., Castro, P. S., and\nMei, J., Xiao, C., Szepesvari, C., and Schuurmans, D. Roux,N.L.(2021). Ageneralclassofsurrogatefunc-\n(2020). On the global convergence rates of softmax tions for stable and efficient reinforcement learning.\npolicygradientmethods.InInternationalConference arXiv preprint arXiv:2108.05828.\non Machine Learning, pages 6820–6829. PMLR. Vaswani, S., Kazemi, A., Babanezhad Harikandeh, R.,\nand Le Roux, N. (2024). Decision-aware actor-critic\nMunos, R. (2005). Error bounds for approximate value\niteration. In Proceedings of the National Confer- with function approximation and theoretical guar-\nence on Artificial Intelligence, volume 20, page 1006. antees. Advances in Neural Information Processing\nSystems, 36.\nMenlo Park, CA; Cambridge, MA; London; AAAI\nPress; MIT Press; 1999. Williams, R. J. (1992). Simple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nNikolakakis, K. E., Haddadpour, F., Karbasi, A., and\nlearning. Machine learning, 8(3-4):229–256.\nKalogerias, D. S. (2022). Beyond lipschitz: Sharp\ngeneralization and excess risk bounds for full-batch Xiao, L. (2022). On the convergence rates of policy\ngd. arXiv preprint arXiv:2204.12446. gradient methods. Journal of Machine Learning\nResearch, 23(282):1–36.\nPuterman, M. L. (2014). Markov decision processes:\ndiscretestochasticdynamicprogramming.JohnWiley Yuan, R., Du, S. S., Gower, R. M., Lazaric, A., and\nXiao,L.(2023). Linearconvergenceofnaturalpolicy\n& Sons.\ngradient methods with log-linear policies. In Inter-\nRaffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernes- national Conference on Learning Representations.\ntus, M., and Dormann, N. (2021). Stable-baselines3:\nZhong,H.andZhang,T.(2024). Atheoreticalanalysis\nReliable reinforcement learning implementations.\nof optimistic proximal policy optimization in linear\nJournal of Machine Learning Research, 22(268):1–\nmarkov decision processes. Advances in Neural In-\n8.\nformation Processing Systems, 36.\nSchulman, J. (2015). Trust region policy optimization.\narXiv preprint arXiv:1502.05477.\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and\nAbbeel, P. (2015). High-dimensional continuous con-\ntrol using generalized advantage estimation. arXiv\npreprint arXiv:1506.02438.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A.,\nandKlimov,O.(2017). Proximalpolicyoptimization\nalgorithms. arXiv preprint arXiv:1707.06347.\nShani, L., Efroni, Y., and Mannor, S. (2020). Adaptive\ntrust region policy optimization: Global convergence\nand faster rates for regularized mdps. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 34, pages 5668–5675.\nSutton, R. S. (2018). Reinforcement learning: An\nintroduction. A Bradford Book.\nSutton, R. S., McAllester, D., Singh, S., and Mansour,\nY.(1999). Policygradientmethodsforreinforcement\nlearning with function approximation. Advances in\nneural information processing systems, 12.\nTodorov, E., Erez, T., and Tassa, Y. (2012). Mujoco:\nA physics engine for model-based control. In 2012\nIEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 5026–5033. IEEE.\nSupplementary Material\nOrganization of the Appendix\nA Multi-armed Bandit Proofs\nB MDP Proofs\nC Tabular MDP Experiments\nD Additional Details for Stable Baselines Experiments\nA Multi-armed Bandit Proofs\nTheorem 1. The SPMA update in Eq. (2) with (i) a constant step-size η ≤1, and (ii) uniform initialization i.e.\nπ (a)= 1 for all a converges as:\n0 K\n(cid:18) (cid:19) (cid:18) (cid:19)\n1 −η∆ T\nr(a∗)−⟨π ,r⟩≤ 1− exp min ,\nT K K\nwhereT isthenumberofiterations,a∗ istheoptimalarmi.e. a∗ =argmax r(a)and∆ :=min ∆(a∗,a)=\na min a̸=a∗\nr(a∗)−r(a) is the gap.\nProof. . As in equation (2), we can write the update for arm a as following where ∆(a,a′)=r(a)−r(a′),\n \n(cid:88)\nπ t+1(a)=π t(a)1+η π t(a′)∆(a,a′)\na′̸=a\n \n(cid:88)\n1−π t+1(a∗)=1−π t(a∗)−ηπ t(a∗) π t(a′)∆(a∗,a′) (6)\na′̸=a∗\nWe first find a lower-bound for (cid:80) π (a′)∆(a∗,a′):\na′̸=a∗ t\n(cid:88) (cid:88)\nπ (a′)∆(a∗,a′)≥∆ π (a′)\nt min t\na′̸=a∗ a′̸=a∗ (7)\n=∆ (1−π (a∗))\nmin t\nNext,weobservethat(cid:80) π (a′)∆(a∗,a′)≥0. Usingthisinformationandstartingwithauniforminitialization\na′̸=a∗ t\nfor selecting an arm implies a monotonic improvement on the probability of selecting the optimal arm:\n1\nπ (a∗)>π (a∗)>...>π (a∗)= (8)\nt+1 t 0 K\nFast Convergence of Softmax Policy Mirror Ascent\nLet ϵ =1−π (a∗).\nt t\n \n(cid:88)\nϵ\nt+1\n=ϵ t−ηπ t(a∗) π t(a′)∆(a∗,a′)\na′̸=a∗\n \nη (cid:88)\n≤ϵ t−\nK\n π t(a′)∆(a∗,a′) (using (8))\na′̸=a∗\nη∆\n≤ϵ − minϵ (using (7))\nt K t\n(cid:18) (cid:19)\nη∆\n=ϵ 1− min\nt K\nRecursing from t=0 to t=T −1 we have:\n(cid:18)\nη∆\n(cid:19)T\nϵ ≤ϵ 1− min\nT 0 K\n(cid:18) (cid:19)\n−η∆ T\n≤ϵ exp min (using 1−x≤exp(−x))\n0 K\n(cid:18) (cid:19) (cid:18) (cid:19)\n1 −η∆ T\n= 1− exp min\nK K\nFinally, we define the sub-optimality gap, δ :=r(a∗)−⟨π ,r⟩:\nT T\n(cid:88)\nδ = π (a′)[r(a∗)−r(a′)]\nT T\na′\n(cid:88)\n= π (a)∆(a∗,a)\nT\na′̸=a∗\n(cid:88)\n≤max∆(a∗,a′) π (a)\nT\na′\na′̸=a∗\n=max∆(a∗,a′)(1−π (a∗))\nT\na′\n≤1−π (a∗) (using the fact 0≤r ≤1)\nT\n=ϵ\nT\n(cid:18) (cid:19) (cid:18) (cid:19)\n1 −η∆ T\n≤ 1− exp min\nK K\nA.1 Super-linear Rate for Bandits\nIn order to achieve the desired fast rate of convergence, we modify the update in Eq. (2) to use a set of\n(cid:0)K(cid:1)\n2\nconstant gap-dependent step-sizes {η } . The new update can be written as:\na,a′ a,a′∈[K]\n(cid:88)\nπ (a)=π (a)[1+ π (a′)η ∆(a,a′)] (9)\nt+1 t t a,a′\na′̸=a\nThe following theorem shows that the above update results in super-linear convergence.\nTheorem 4. Using the SPMA update in Eq. (9) with (i) η = 1 and a (ii) uniform initialization similar\na,a′ |∆(a,a′)|\nto Theorem 1 results in valid probability distributions and converges as:\n(cid:20)(cid:18)\n1\n(cid:19)(cid:21)2T\nr(a∗)−⟨π ,r⟩≤ 1−\nT K\nwhere T is the number of iterations, a∗ is the optimal arm and ∆(a,a′):=r(a)−r(a′) represents the reward gap\nbetween arms a and a′.\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nProof. We define ∆(a,a′):=r(a)−r(a′),\nAπt =r(a)−⟨π ,r⟩\nt\n(cid:88)\n= π (a′)[r(a)−r(a′)]\nt\na′\n(cid:88)\n= π (a′)∆(a,a′)\nt\na′\nChoosing different step sizes for every pair of arms, depending on their corresponding gap, η = 1 we get\na,a′ |∆(a,a′)|\nthe following update for π (a):\nt+1\n \n(cid:88)\nπ t+1(a)=π t(a)1+ η a,a′π t(a′)∆(a,a′)\na′̸=a\n \n(cid:88)\n=π t(a)1+ π t(a′)sign(∆(a,a′)) (i)\na′̸=a\nNow we check if π is a probability distribution with this choice of η. Note that ∆(a,a′)=−∆(a′,a).\nt+1\n(cid:88) (cid:88) (cid:88) (cid:88)\nπ (a)= π (a)+ π (a) π (a′)sign(∆(a,a′))\nt+1 t t t\na a a a′̸=a\n(cid:88)\n=1+ π (a)π (a′)(sign(∆(a,a′))+sign(∆(a′,a)))\nt t\n(a,a′),a̸=a′\n(cid:88)\n=1+ π (a)π (a′)(sign(∆(a,a′))−sign(∆(a,a′))) ( since ∆(a,a′)=−∆(a′,a))\nt t\n(a,a′),a̸=a′\n=1\nFurthermore, it is clear that π (a)∈[0,1]. Based on this we just need to show that the probability of the optimal\nt\narm a∗ converges to 1.\nComputing the probability of pulling the optimal arm using update (i):\n \n(cid:88)\nπ t+1(a∗)=π t(a∗)1+ π t(a′)sign(∆(a∗,a′))\na′̸=a∗\n \n(cid:88)\n=π t(a∗)1+ π t(a′) (∆(a∗,a′)>0 ∀a′)\na′̸=a∗\n=π (a∗)[2−π (a∗)] (ii)\nt t\nWe use induction to show π\n(a∗)=1−(cid:2)\n(1−\n1)(cid:3)2t\nsolves the recurrence relation (ii). We consider the uniform\nt K\ndistribution over the arms at the initialization i.e. π (a)= 1, ∀a∈A. For the base case, we show the suggested\n0 K\nsolution satisfies recursion (ii):\n(cid:18) (cid:19)\n1 1\nπ (a∗)= 2− (using the recursion in (ii))\n1 K K\n(cid:18) (cid:19)(cid:18) (cid:19)\n1 1\n= 1−1+ 1+1−\nK K\n(cid:20)(cid:18)\n1\n(cid:19)(cid:21)2\n=1− 1−\nK\nFast Convergence of Softmax Policy Mirror Ascent\nAssuming the suggested solution is true for t, we show it is also true for t+1:\n(cid:34) (cid:18)\n1\n(cid:19)2t(cid:35) (cid:34) (cid:18)\n1\n(cid:19)2t(cid:35)\nπ (a∗)= 1− 1− 2−1+ 1−\nt+1 K K\n(cid:34)(cid:18)\n1\n(cid:19)2t+1(cid:35)\n=1− 1−\nK\nLet δ :=r(a∗)−⟨π ,r⟩ represent the sub-optimality gap.\nT T\n(cid:88)\nδ = π (a′)[r(a∗)−r(a′)]\nT T\na′\n(cid:88)\n= π (a)∆(a∗,a)\nT\na′̸=a∗\n(cid:88)\n≤max∆(a∗,a′) π (a)\nT\na′\na′̸=a∗\n≤1−π (a∗) (using the fact 0≤r ≤1)\nT\n(cid:20)(cid:18)\n1\n(cid:19)(cid:21)2T\n= 1− (using the formula for π (a∗))\nK T\nB MDP Proofs\nB.1 Tabular Setting\nLemma 1. For any policy π we have\nt\n(cid:88)\nπ (a|s)[Aπt(s,a)]2 ≥ C maxAπt(s,a)\nt t\na\na\nwhere C\nt\n:= min s{π t(A˜ t(s)|s)∆t(s)}, A˜ t(s) := argmax a∈AQπt(s,a), π t(A˜ t(s)|s) = (cid:80)\na∈A˜\nt(s)π t(a t(s)|s) and\n∆t(s):=max a∈AQπt(s,a)−max a∈/A˜Qπt(s,a).\nProof. Recall A˜ t(s):=argmax a∈AAπt(s,a) i.e. A˜ t(s) is a set containing actions with maximum advantage for\nstate s. Let’s define π (A˜ (s)|s) = (cid:80) π (a˜ (s)|s). We can split the sum on the LHS of the above over\nt t a∈A˜ t(s) t t\nA˜ (s):\nt\n(cid:88) (cid:88)\nπ (a|s)[Aπt(s,a)]2 = π (a˜ (s)|s)[maxAπt(s,a)][maxAπt(s,a)]\nt t t\na a\na a∈A˜ t(s)\n(cid:88)\n+ π (a|s)[Aπt(s,a)]2\nt\na∈/A˜ t(s) (10)\n=π (A˜ (s)|s)[maxAπt(s,a)][maxAπt(s,a)]\nt t\na a\n(cid:88)\n+ π (a|s)[Aπt(s,a)]2\nt\na∈/A˜ t(s)\nLet π˜ be the following distribution over the actions.\nt\n(cid:40) 0 if a∈A˜ (s)\nt\nπ˜(a|s)=\nt πt(a|s) otherwise\n1−πt(A˜ t(s)|s)\nRe-writing (cid:80) aπ t(a|s)Aπt(s,a)=0 using the above distribution we obtain:\n(1−π (A˜ (s)|s)) E [Aπt(s,a)]+π (A˜ (s)|s)[maxAπt(s,a)]=0\nt t a∼π˜t t t\na\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\n(1−π (A˜ (s)|s)) E [Aπt(s,a)]=−π (A˜ (s)|s)[maxAπt(s,a)] (11)\nt t a∼π˜t t t\na\nExpanding the second term in Eq. 10 using π˜ we obtain:\nt\n(cid:88) π (a|s)[Aπt(s,a)]2 = (1−π (A˜ (s)|s)) E [Aπt(s,a)]2\nt t t a∼π˜t\na∈/A˜ t(s)\n≥ (1−π (A˜ (s)|s)) (E [Aπt(s,a)])2 (using E[x2]≥(E[x])2)\nt t a∼π˜t\n= (1−π (A˜ (s)|s)) (E [Aπt(s,a)])(E [Aπt(s,a)])\nt t a∼π˜t a∼π˜t\n=− π (A˜ (s)|s)[maxAπt(s,a)](E [Aπt(s,a)]) (using Eq. 11)\nt t\na\na∼π˜t\nPlugging in the result above into Eq. 10 we obtain:\n(cid:88) π (a|s)[Aπt(s,a)]2 ≥ π (A˜ (s)|s)[maxAπt(s,a)][maxAπt(s,a)]\nt t t\na a\na\n− π (A˜ (s)|s)[maxAπt(s,a)](E [Aπt(s,a)])\nt t\na\na∼π˜t\n(cid:104) (cid:105)\n≥ π (A˜ (s)|s)[maxAπt(s,a)] maxAπt(s,a)−E [Aπt(s,a)]\nt t\na a\na∼π˜t\n \n \n≥ π (A˜ (s)|s)[maxAπt(s,a)]maxAπt(s,a)−maxAπt(s,a)\nt t  \na  a a∈/A˜ \n(cid:124) (cid:123)(cid:122) (cid:125)\n:=∆t(s)\n= π (A˜ (s)|s)[maxAπt(s,a)]∆t(s)\nt t\na\n≥ C maxAπt(s,a)\nt\na\n(cid:110) (cid:111)\nLemma 2. Using the update π t+1(a|s)=π t(a|s)(1+ηAπt(s,a)) with a step-size η <min 1−γ, Ct(11\n−γ)\n, at\nany iteration t and state s∈S, we have\nV∗(s)−Vπt+1(s)≤[1−η C (1−γ)][V∗(s)−Vπt(s)]\nt\nwhere C\nt\n:=min s{π t(A˜ t(s)|s)∆t(s)}, A˜ t(s):=argmax aQπt(s,a), π t(A˜ t(s)|s)=(cid:80)\na∈A˜\nt(s)π t(a t(s)|s), ∆t(s):=\nmax aQπt(s,a)−max a∈/A˜Qπt(s,a), and V∗(s) is the value function corresponding to the optimal policy π∗ at\ns∈S.\nProof. First, we use the value difference Lemma to show the SPMA update in Eq. (3) leads to a monotonic\nimprovement in the value function.\n(cid:34) (cid:35)\n1 (cid:88)\nVπt+1(s)−Vπt(s)= 1−γE s∼dπt+1 π t+1(a|s)Aπt(s,a) (12)\na\nPlugging update Eq. (3) into the term within the brackets, we obtain the following:\n(cid:88) (cid:88)\nπ (a|s)Aπt(s,a)= π (a|s)Aπt(s,a)[1+ηAπt(s,a)]\nt+1 t\na a\n(cid:88) (cid:88)\n= π (a|s)Aπt(s,a)+η π (a|s)[Aπt(s,a)]2\nt t\na a\n(cid:88)\n=η π (a|s)[Aπt(s,a)]2\nt\na\n>0\nFast Convergence of Softmax Policy Mirror Ascent\nHence, Vπt+1(s)≥Vπt(s). Using Lemma 1, we have:\n(cid:88)\nη π (a|s)[Aπt(s,a)]2 ≥η C maxAπt(s,a) (13)\nt t\na\na\nCombining the above with the result from the value difference Lemma we have:\n(cid:88) (cid:88)\nπ (a|s)Aπt(s,a)=η π (a|s)[Aπt(s,a)]2\nt+1 t\na a (14)\n≥η C maxAπt(s,a)\nt\na\nWe now show a linear convergence when using the update in Eq. (3). Let T be the Bellman optimality operator\ndefined as:\n(cid:88)\n(Tv)(s)=max{r(s,a)+γ Pr[s′|s,a]v(s′)}\na\ns′\nApplying the operator at iteration t we obtain:\nTVπt(s)−Vπt(s)=maxQπt(s,a)−Vπt(s)=maxAπt(s,a)\n(15)\na a\nLet Tπ be an operator w.r.t π defined as:\n(cid:88) (cid:88) (cid:88)\nTπ(v)= π(a|s)r(s,a)+γ π(a|s) Pr[s′|s,a]v(s′)\na a s′\nApplying Tπ to Vπ′(s) results in:\nTπVπ′ (s)=(cid:88) π(a|s)r(s,a)+γ(cid:88) π(a|s)(cid:88) Pr[s′|s,a]Vπ′\n(s)\na a s′\n=(cid:88) π(a|s)Qπ′\n(s,a)\na\nUsing the above we obtain:\n(cid:88)\nTπt+1Vπt(s)−Vπt(s)= π (a|s)Aπt(s,a)\nt+1\na\n≥η C maxAπt(s,a) (using Ineq. 14)\nt\na\n=η C [TVπt(s)−Vπt(s)] (using Eq. 15)\nt\nAssuming π∗ is the optimal policy we have:\nV∗(s)−Vπt+1(s)=V∗(s)−Tπt+1Vπt+1(s) (since TπVπ(s)=Vπ(s))\n≤V∗(s)−Tπt+1Vπt(s) (since Vπt+1(s)≥Vπt(s) ∀s)\n=V∗(s)−Vπt(s)−[Tπt+1Vπt(s)−Vπt(s)] (add and subtract Vπt(s))\n≤V∗(s)−Vπt(s)−η C [TVπt(s)−Vπt(s)]\nt\n=η C [V∗(s)−Vπt(s)]+(1−η C )[V∗(s)−Vπt(s)]−η C [TVπt(s)−Vπt(s)]\nt t t\n=η C [TV∗(s)−Vπt(s)−TVπt(s)+Vπt(s)]+(1−η C )[V∗(s)−Vπt(s)]\nt t\n=η C [TV∗(s)−TVπt(s)]+(1−η C )[V∗(s)−Vπt(s)]\nt t\n≤γ η C [V∗(s)−Vπt(s)]+(1−η C )[V∗(s)−Vπt(s)] (T is a γ contraction map)\nt t\n=[1−η C (1−γ)][V∗(s)−Vπt(s)]\nt\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\n(cid:110) (cid:111)\nTheorem 2. Using the SPMA update in Eq. (3) with a step-size η <min 1−γ, 1 converges as:\nCt(1−γ)\n(cid:13)\n(cid:13)Vπ∗\n−VπT(cid:13)\n(cid:13)\n≤(cid:32)T (cid:89)−1\nα\n(cid:33) (cid:13)\n(cid:13)Vπ∗\n−Vπ0(cid:13)\n(cid:13) ,\n(cid:13) (cid:13) t (cid:13) (cid:13)\n∞ ∞\nt=0\nwhere α\nt\n:= (1 − ηC t(1 − γ)), C\nt\n:= min s{π t(a˜ t(s)|s)∆t(s)}, a˜ t(s) := argmax aQπt(s,a) and ∆t(s) :=\nmax aQπt(s,a)−max a̸=a˜Qπt(s,a).\nProof. Using Lemma 2 we have\nV∗(s)−Vπt+1(s)≤[1−η C (1−γ)][V∗(s)−Vπt(s)]\nt\nIf η < Ct(11 −γ), both sides of the inequality above are positive leading to |V∗(s)−Vπt+1(s)| ≤ (1−η C t(1−\nγ))|V∗(s)−Vπt(s)|. This is true for all s∈S, hence we have:\n(cid:13) (cid:13) (cid:13) (cid:13)\n(cid:13)Vπ∗ −Vπt+1(cid:13) ≤(1−η C (1−γ))(cid:13)Vπ∗ −Vπt(cid:13)\n(cid:13) (cid:13) t (cid:13) (cid:13)\n∞ ∞\n(cid:13) (cid:13)\n=α (cid:13)Vπ∗ −Vπt(cid:13)\nt(cid:13) (cid:13)\n∞\nRecursing from t=0 to t=T −1 we obtain a linear convergence:\n(cid:13)\n(cid:13)Vπ∗\n−VπT(cid:13)\n(cid:13)\n≤(cid:32)T (cid:89)−1\nα\n(cid:33) (cid:13)\n(cid:13)Vπ∗\n−Vπ0(cid:13)\n(cid:13)\n(cid:13) (cid:13) t (cid:13) (cid:13)\n∞ ∞\nt=0\nB.2 Function Approximation With Exact Advantage\nRecall the definitions of ℓ˜ and ℓ\nt t\nℓ˜(θ)=(cid:88)\ndπt(s)KL(π (·|s)||π (·|s))\nt t+1/2 θ\ns\n(cid:88)\nℓ (θ)= KL(h(f (s,·))(1+ηAπt(s,·))||h(f (s,·)))\nt θt θ\ns∼τ\n(cid:110) (cid:111)\nTheorem 3. Under assumption 1-3, Algorithm 1 with η <min 1−γ, 1 converges as,\nCt(1−γ)\nJ(π∗)−J(π )\nT\n(cid:32)T−1 (cid:33) T−1 T−1\n(cid:89) (cid:88) (cid:89)\n≤ α (J(π∗)−J(π ))+β α ,\nt 0 i\nt=0 t=0 i=t+1\n√ √\nwhere β = 2 ϵ +ϵ and α has the same definition as in Theorem 2.\n(1−γ)2ρmin stat bias t\nProof. We assumed that z (s,a):=f (s,a) ∀(s,a) and z (s,a)=f (s,a) where f :RSA →R is a complex,\nθ θ t θt+1 θ\nnon-linear model. We remind the following updates:\nz =argmax{⟨∇ J(z ),z⟩−1/ηD (z,z )}\nt+1/2 z t Φ t\nz¯∈R|S||A|\n∇Φ(z )=∇Φ(z )+η∇ J(z ) (Mirror Ascent update without projection)\nt+1/2 t z t\nπ =h(z ) (h is softmax)\nt+1/2 t+1/2\nπ (a|s)=π (a|s)(1+ηAπt(s,a))\nt+1/2 t\nθ =(S)GD(ℓ (θ)) (using (Stochastic)Gradient Descent for m iteration to minimize ℓ )\nt+1 t t\nπ =h(z )\nt+1 t+1\nFast Convergence of Softmax Policy Mirror Ascent\nz is an unprojected update for the tabular setting and therefore using Lemma 2 we have:\nt+1/2\nV∗(s)−Vπt+1/2(s)≤[1−η C t(1−γ)][V∗(s)−Vπt(s)]\nBy adding and removing Vπt+1(s) to both sides and rearranging we have\nV∗(s)−Vπt+1(s)≤[1−η C t(1−γ)][V∗(s)−Vπt(s)]+Vπt+1/2(s)−Vπt+1(s).\nTaking the expectation w.r.t. ρ, we obtain:\nJ(π∗)−J(π )≤[1−η C (1−γ)][J(π∗)−J(π )]+J(π )−J(π )\nt+1 t t t+1/2 t+1\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=E1\nThe term E can be bounded as follows:\n1\n(cid:88)\nE\n1\n= dπt+1/2(s)⟨Qπt+1(s,.),π t+1/2(.|s)−π t+1(.|s)⟩\ns\n≤(cid:88) dπt+1/2(s)∥Qπt+1(s,.)∥ ∞(cid:13) (cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13) (cid:13)\n1\n(Holder inequality)\ns\n≤ 1−1\nγ\n(cid:88) dπt+1/2(s)(cid:13) (cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13) (cid:13)\n1\n(since ∥Qπt+1(s,.)∥\n∞\n≤ 1−1 γ)\ns\n1 (cid:88)dπt+1/2(s) (cid:13) (cid:13)\n=\n1−γ ρ(s)\nρ(s)(cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13)\n1\ns\n1\n(cid:13) (cid:13)dπt+1/2(cid:13)\n(cid:13) (cid:88) (cid:13) (cid:13)\n≤\n1−γ\n(cid:13)\n(cid:13) ρ\n(cid:13)\n(cid:13)\nρ(s)(cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13)\n1\n∞ s\n1 (cid:88) (cid:13) (cid:13)\n≤\n(1−γ)ρ\nρ(s)(cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13)\n1\n(since dπt+1/2(s)≤1 and using assumption 3)\nmin\ns\n≤\n(1−γ1\n)2ρ\n(cid:88) dπt(s)(cid:13)\n(cid:13)π t+1/2(.|s)−π\nt+1(.|s)(cid:13)\n(cid:13)\n1\n(sicne dπt ≥(1−γ)ρ)\nmin\ns\n√\n2 (cid:88) (cid:113)\n≤ dπt(s) KL(π (.|s)||π (.|s))\n(1−γ)2ρ t+1/2 t+1\nmin\ns\n(using strong convexity of KL divergence or Pinsker’s inequality)\n√\n≤ (1−γ)2\n2ρ\nmin(cid:118) (cid:117) (cid:117) (cid:117)(cid:88) dπt(s)KL(π t+1/2(.|s)||π t+1(.|s)) (due to concavity of √ and Jensen’s inequality)\n(cid:116) s\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=E2\nwhere E can be bounded as follows.\n2\nE =ℓ˜(θ )\n2 t t+1\n=ℓ˜(θ )−minℓ˜(θ )+minℓ˜(θ )\nt t+1 t t+1 t t+1\nθ θ\n≤ϵ +minℓ˜(θ ) (using assumption 1)\nstat t t+1\nθ\n≤ϵ +ϵ (using assumption 2)\nstat bias\nPutting everything together we have:\n√\n2 √\nE ≤ ϵ +ϵ\n1 (1−γ)2ρ stat bias\nmin\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=β\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nTherefore we have\nJ(π∗)−J(π )≤[1−η C (1−γ)][J(π∗)−J(π )]+β\nt+1 t t\n(cid:124) (cid:123)(cid:122) (cid:125)\nαt\nUnrolling the above recursion for T iterations,\n(cid:32)T−1 (cid:33) T−1 T−1\n(cid:89) (cid:88) (cid:89)\nJ(π∗)−J(π )≤ α (J(π∗)−J(π ))+β α\nT t 0 t\nt=0 t=0 i=t+1\nB.3 Function Approximation With Inexact Advantage\nIn practice computing the Aπt at every iteration is costly. In this section, we assume that we access an oracle\nsuch that at each iteration t, it gives us Aˆπt an approximation of Aπt.\nAssumption 4. Valid Approximation. For all iterations t and (s,a)∈S×A, |Aˆπt(s,a)|≤ 1 .\n1−γ\n(cid:13) (cid:13)\nAssumption 5. Approximation Error. For all iterations t and s∈S, (cid:13) (cid:13)Aπt(s,.)−Aˆπt(s,.)(cid:13)\n(cid:13)\n≤ϵ approx.\n∞\nUsing this inexact advantage function, we define the following update and functions\nπ t+1/2(a|s)=π t(a|s)(1+ηAˆπt(s,a)) (replacing Aπt with Aˆπt)\nℓ˜(θ)=(cid:88)\ndπt(s)KL(π (·|s)||π (·|s))\nt t+1/2 θ\ns\nℓ (θ)=(cid:88) KL(cid:16) h(f (s,·))(1+ηAˆπt(s,·))||h(f (s,·))(cid:17)\nt θt θ\ns∼τ\nSince we use the inexact advantage, we cannot reuse the result of Lemma 2. So we provide a variant of that\nlemma with an inexact advantage.\n(cid:110) (cid:111)\nLemma 3. Using the update π t+1(a|s)=π t(a|s)(1+ηAˆπt(s,a)) with (i) a step-size η <min 1−γ, Ct(11\n−γ)\nand (ii) Aˆπt satisfying assumptions 4 and 5, at any iteration t and s∈S we have\nϵ\nV∗(s)−Vπ(s)≤[1−η C (1−γ)][V∗(s)−Vπt(s)]+ approx\nt 1−γ\nwhere C\nt\n:= min s{π t(A˜ t(s)|s)∆t(s)}, A˜ t(s) := argmax aQπt(s,a), π t(A˜ t(s)|s) = (cid:80)\na∈A˜\nt(s)π t(a t(s)|s) and\n∆t(s):=max aQπt(s,a)−max a∈/A˜Qπt(s,a), and V∗(s) is the value function corresponding to the optimal policy\nπ∗ at s∈S.\nFast Convergence of Softmax Policy Mirror Ascent\nProof. First, we use the value difference Lemma for a state s∈S\n(cid:34) (cid:35)\n1 (cid:88)\nVπ(s)−Vπt(s)= 1−γE\ns′∼dπ\nπ(a|s′)Aπt(s′,a)\na\n(cid:34) (cid:35)\n= 1−1 γE\ns′∼dπ\n(cid:88) π t(a|s′)(1+ηAˆπt(s′,a))Aπt(s′,a)\na\n(cid:34) (cid:35)\n= 1−1 γE\ns′∼dπ\n(cid:88) ηπ t(a|s′)Aˆπt(s′,a)Aπt(s′,a)\na\n(cid:34) (cid:35)\n= 1−1 γE\ns′∼dπ\n(cid:88) ηπ t(a|s′)(Aˆπt(s′,a)−Aπt(s′,a)+Aπt(s′,a))Aπt(s′,a)\na\n(cid:34) (cid:35)\n1 (cid:88)\n= 1−γE\ns′∼dπ\nηπ t(a|s′)(Aπt(s′,a))2\na\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=T1\n(cid:34) (cid:35)\n+ 1−1 γE\ns′∼dπ\n(cid:88) ηπ t(a|s′)(Aˆπt(s′,a)−Aπt(s′,a))Aπt(s′,a)\na\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=T2\nT can be bounded using Lemma 1,\n1\nT ≥ ηC maxAπt(s,a)\n1 t\na\nTo bound T , we use assumption 5,\n2\n(cid:34) (cid:35)\nT ≥−η (cid:88) π (a|s′)|(Aˆπt(s′,a)−Aπt(s′,a))||Aπt(s′,a)|\n2 t\na\n(cid:34) (cid:35)\n≥−η (cid:88) π (a|s′)|(Aˆπt(s′,a)−Aπt(s′,a))| 1 (since Aπ ≤1/(1−γ))\nt 1−γ\na\n(cid:34) (cid:35)\n≥−η (cid:88) π (a|s′)ϵ approx (using assumption 5)\nt 1−γ\na\nηϵ\n=− approx\n1−γ\nUsing the lower-bound for T and T we have\n1 2\n(cid:88) π(a|s)Aπt(s,a)≥ηC maxAπt(s,a)− ηϵ approx (16)\nt a 1−γ\na\nPutting everything together we have,\nηϵ\nVπ(s)≥Vπt(s)− approx (17)\n(1−γ)2\nϵ\n≥Vπt(s)− approx (since η ≤1−γ)\n(1−γ)\nϵ\n=⇒ Vπt(s)−Vπ(s)≤ approx (18)\n(1−γ)\n(19)\nLet T be the Bellman optimality operator defined as:\n(cid:88)\n(Tv)(s)=max{r(s,a)+γ Pr[s′|s,a]v(s′)}\na\ns′\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nApplying the operator at iteration t we obtain:\nTVπt(s)−Vπt(s)=maxQπt(s,a)−Vπt(s)=maxAπt(s,a)\n(20)\na a\nLet Tπ be an operator w.r.t π defined as:\n(cid:88) (cid:88) (cid:88)\nTπ(v)= π(a|s)r(s,a)+γ π(a|s) Pr[s′|s,a]v(s′)\na a s′\nApplying Tπ to Vπ′(s) results in:\nTπVπ′ (s)=(cid:88) π(a|s)r(s,a)+γ(cid:88) π(a|s)(cid:88) Pr[s′|s,a]Vπ′\n(s)\na a s′\n=(cid:88) π(a|s)Qπ′\n(s,a)\na\nUsing the above we obtain:\n(cid:88)\nTπVπt(s)−Vπt(s)= π(a|s)Aπt(s,a)\na\nηϵ\n≥η C maxAπt(s,a)− approx (using Eq. (16))\nt a 1−γ\nηϵ\n=η C [TVπt(s)−Vπt(s)]− approx (using Eq. 20)\nt 1−γ\n≥η C [TVπt(s)−Vπt(s)]−ϵ (since η ≤1−γ)\nt approx\nUsing Eq. (18) we have\n(cid:88) (cid:88)\nTπVπt(s)−TπVπ(s)=γ π(a|s) Pr[s′|s,a](Vπt(s′)−Vπ(s′)) (21)\na s′\n≤γ(cid:88) π(a|s)(cid:88) Pr[s′|s,a]ϵ approx (using Eq. (18))\n1−γ\na s′\nγϵ\n= approx (22)\n1−γ\nAssuming π∗ is the optimal policy we have:\nV∗(s)−Vπ(s)=V∗(s)−TπVπ(s) (since TπVπ(s)=Vπ(s))\n=V∗(s)−TπVπt(s)+TπVπt(s)−TπVπ(s)\nγϵ\n≤V∗(s)−TπVπt(s)+ approx (using Eq. (22))\n1−γ\nγϵ\n=V∗(s)−Vπt(s)−[TπVπt(s)−Vπt(s)]+ approx (add and subtract Vπt(s))\n1−γ\nγϵ\n≤V∗(s)−Vπt(s)−η C [TVπt(s)−Vπt(s)]+ϵ + approx\nt approx 1−γ\nϵ\n=η C [V∗(s)−Vπt(s)]+(1−η C )[V∗(s)−Vπt(s)]−η C [TVπt(s)−Vπt(s)]+ approx\nt t t 1−γ\nϵ\n=η C [TV∗(s)−Vπt(s)−TVπt(s)+Vπt(s)]+(1−η C )[V∗(s)−Vπt(s)]+ approx\nt t 1−γ\nϵ\n=η C [TV∗(s)−TVπt(s)]+(1−η C )[V∗(s)−Vπt(s)]+ approx\nt t 1−γ\nϵ\n≤γ η C [V∗(s)−Vπt(s)]+(1−η C )[V∗(s)−Vπt(s)]+ approx (T is a γ contraction map)\nt t 1−γ\nϵ\n=[1−η C (1−γ)][V∗(s)−Vπt(s)]+ approx\nt 1−γ\nFast Convergence of Softmax Policy Mirror Ascent\n(cid:110) (cid:111)\nTheorem 5. Under assumption 1-5, Algorithm 1 with η <min 1−γ, 1 converges as,\nCt(1−γ)\nJ(π∗)−J(π )\nT\n(cid:32)T−1 (cid:33) T−1 T−1\n(cid:89) (cid:88) (cid:89)\n≤ α (J(π∗)−J(π ))+β α ,\nt 0 t\nt=0 t=0 i=t+1\n√ √\nwhere β = 2 ϵ +ϵ + ϵapprox, α = [1−η C (1−γ)], C := min {π (A˜ (s)|s)∆t(s)}, A˜ (s) :=\n(1−γ)2ρmin stat bias 1−γ t t t s t t t\nargmax aQπt(s,a), π t(A˜ t(s)|s)=(cid:80)\na∈A˜\nt(s)π t(a t(s)|s) and ∆t(s):=max aQπt(s,a)−max a∈/A˜Qπt(s,a).\nProof. Using Lemma 3 with π =π ,\nt+1/2\nϵ\nV∗(s)−Vπt+1/2(s)≤[1−η C t(1−γ)][V∗(s)−Vπt(s)]+ 1ap −pr γox\nThe rest of the proof is similar to the proof of Theorem 3. For completeness, we repeat it here. By adding and\nremoving Vπt+1(s) to both sides and rearranging we have\nϵ\nV∗(s)−Vπt+1(s)≤[1−η C t(1−γ)][V∗(s)−Vπt(s)]+Vπt+1/2(s)−Vπt+1(s)+ 1ap −pr γox.\nTaking the expectation w.r.t. ρ we obtain:\nϵ\nJ(π∗)−J(π )≤[1−η C (1−γ)][J(π∗)−J(π )]+J(π )−J(π )+ approx.\nt+1 t t t+1/2 t+1 1−γ\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=E1\nThe term E can be bounded as follows:\n1\n(cid:88)\nE\n1\n= dπt+1/2(s)⟨Qπt+1(s,.),π t+1/2(.|s)−π t+1(.|s)⟩\ns\n≤(cid:88) dπt+1/2(s)∥Qπt+1(s,.)∥ ∞(cid:13) (cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13) (cid:13)\n1\n(Holder inequality)\ns\n≤ 1−1\nγ\n(cid:88) dπt+1/2(s)(cid:13) (cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13) (cid:13)\n1\n(since ∥Qπt+1(s,.)∥\n∞\n≤ 1−1 γ)\ns\n1 (cid:88)dπt+1/2(s) (cid:13) (cid:13)\n=\n1−γ ρ(s)\nρ(s)(cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13)\n1\ns\n1\n(cid:13) (cid:13)dπt+1/2(cid:13)\n(cid:13) (cid:88) (cid:13) (cid:13)\n≤\n1−γ\n(cid:13)\n(cid:13) ρ\n(cid:13)\n(cid:13)\nρ(s)(cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13)\n1\n∞ s\n1 (cid:88) (cid:13) (cid:13)\n≤\n(1−γ)ρ\nρ(s)(cid:13)π t+1/2(.|s)−π t+1(.|s)(cid:13)\n1\n(since dπt+1/2(s)≤1 and using assumption 3)\nmin\ns\n≤\n(1−γ1\n)2ρ\n(cid:88) dπt(s)(cid:13)\n(cid:13)π t+1/2(.|s)−π\nt+1(.|s)(cid:13)\n(cid:13)\n1\n(sicne dπt ≥(1−γ)ρ)\nmin\ns\n√\n2 (cid:88) (cid:113)\n≤ dπt(s) KL(π (.|s)||π (.|s))\n(1−γ)2ρ t+1/2 t+1\nmin\ns\n(using strong convexity of KL divergence or Pinsker’s inequality)\n√\n≤ (1−γ)2\n2ρ\nmin(cid:118) (cid:117) (cid:117) (cid:117)(cid:88) dπt(s)KL(π t+1/2(.|s)||π t+1(.|s)) (due to concavity of √ )\n(cid:116) s\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=E2\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nwhere E can be bounded as follows:\n2\nE =ℓ˜(θ )\n2 t t+1\n=ℓ˜(θ )−minℓ˜(θ )+minℓ˜(θ )\nt t+1 t t+1 t t+1\nθ θ\n≤ϵ +minℓ˜(θ ) (using assumption 1)\nstat t t+1\nθ\n≤ϵ +ϵ (using assumption 2)\nstat bias\nPutting everything together we have:\n√\n2 √\nE ≤ ϵ +ϵ\n1 (1−γ)2ρ stat bias\nmin\n(cid:124) (cid:123)(cid:122) (cid:125)\n:=β′\nTherefore we have\nϵ\nJ(π∗)−J(π )≤[1−η C (1−γ)][J(π∗)−J(π )]+β′+ approx .\nt+1 t t 1−γ\n(cid:124) (cid:123)(cid:122) (cid:125)\nαt (cid:124) (cid:123)(cid:122) (cid:125)\n:=β\nUnrolling the above recursion for T iterations,\n(cid:32)T−1 (cid:33) T−1 T−1\n(cid:89) (cid:88) (cid:89)\nJ(π∗)−J(π )≤ α (J(π∗)−J(π ))+β α\nT t 0 i\nt=0 t=0 i=t+1\nFast Convergence of Softmax Policy Mirror Ascent\nC Tabular MDP Experiments\nIn this section, we empirically evaluate SPMA on tabular MDP environments. For these experiments, we use\nCliff World [Sutton, 2018] and Frozen Lake [Brockman, 2016] following the setup in Vaswani et al. [2024]. In\nsubsection, C.1 we examine the case where the policy is parametrized using softmax tabular representation. In\nsubsection, C.2 we investigate the function approximation setting as described in Section 4, where the policy is\nparametrized using a linear model.\nC.1 Softmax Tabular Representation\nFor this parametrization we initialize z ∈RS×A uniformly , i.e., π (a|s)= 1 for each a and s. Furthermore, for\n0 |A|\neach algorithm, we set η using a grid search and pick the step-sizes that result in the best area under the curve\n(AUC). The tabular MDP results suggest SPMA and NPG achieve similar performance and they both outperform\nSPG [Sutton et al., 1999; Schulman et al., 2017] (see Fig. 3). To analyze the sensitivity of each algorithm to the\nchoice of η, we examine each optimizer across different values of η. The results in Fig. 4 suggest that overall SPG\n(in green) is more sensitive to different values of η compared to SPMA (blue) and NPG (red).\nFigure 3: SPMA matches the performance of NPG and they both outperform SPG.\nFigure 4: SPG (green) is more sensitive to η compared to SPMA and NPG (blue and red).\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nC.2 Linear Functional Approximation (Linear FA)\nFortheLinearFAsetting,weusealog-linearpolicyparametrization: π (a|s)= exp(X(s,a)θ) ,withX∈RSA×d\nt (cid:80) a′exp(X(s,a′)θ)\nand θ ∈Rd representing the features and parameters. We use constant initialization for θ and following Vaswani\net al. [2024], use tile-coded features for X. As in the previous section, we set η for SPMA and MDPO via grid search\nand report results based on the best AUC. For the inner loop optimization (e.g., minimizing Eq. (5)), we use\nArmijo line search [Armijo, 1966], avoiding an additional grid search for the step-size. For SPG we use the update\nfrom Mei et al. [2020] where Armijo line search is used to set η.\nWe make the following observations from the results: (i) SPG performs poorly in the linear FA setting, while both\nSPMA and MDPO perform well when the parameter dimension d and the number of inner loop optimizations m\nare sufficiently large. (ii) In the CW environment, for smaller d, SPMA converges faster than MDPO (Fig. 5, top\nrow). Increasing m from 25 to 50 narrows the gap between SPMA and MDPO (top vs. bottom row). (iii) In the FL\nenvironment, SPMA and MDPO perform similarly, both outperforming SPG (Fig. 6).\nFigure 5: CW environment: The top row (m=25) shows that SPMA converges faster than MDPO as d decreases,\nwhile the bottom row (m=50) shows the gap decreases as the number of inner loop optimizations increases.\nFigure 6: FL environment: The top row (m = 25) and bottom row (m = 50) show that SPMA and MDPO have\nsimilar convergence and both outperform SPG. The performance of both SPMA and MDPO improves as d increases\n(i.e., the bias decreases) and m increases (i.e., the optimization error decreases).\nFast Convergence of Softmax Policy Mirror Ascent\nD Additional Details for Stable Baselines Experiments\nIn subsection D.1, we provide additional details on the hyper-parameters used for the results in Section 5. Next,\nwe present an ablation study on the number of inner loop optimization steps (m) in subsection D.2.\nD.1 Atari and Mujoco Details\nIn the Atari experiments, we use the default hyper-parameters for each method from stable baselines [Raffin\net al., 2021]. This choice is motivated by two factors: i) following the work of Tomar et al. [2020], we aim to\nevaluate the effectiveness of different surrogate losses without conducting an exhaustive search over numerous\nhyper-parameters; ii) the CNN-based actor and critic networks make grid searching over many hyper-parameters\n(e.g., framestack, GAE λ, horizon length, discount factor) computationally infeasible. For a complete list of\nhyper-parameters used in the Atari experiments, see Table 1.\nIn the MuJoCo experiments, we use the default hyper-parameters from stable baselines for each method,\nbut perform a grid search on the Adam inner loop step size for PPO and TRPO-constrained (best among\n[3×10−3,3×10−4,3×10−5]) and the probability ratio clipping parameter in PPO (best from [0.1,0.2,0.3]). For\nthe regularized surrogates (i.e., the remaining methods: SPMA, MDPO, and TRPO-regularized), we avoid a grid\nsearch for the inner loop step size by using a full batch (i.e., the horizon length) along with the Armijo line\nsearch [Armijo, 1966]. See Table 2 for the full list of hyper-parameters used in the MuJoCo experiments.\nTo set η for the regularized surrogates, we perform a grid search over five fixed values ([0.3,0.5,0.7,0.9,1.0]).\nAlthough Tomar et al. [2020] anneals η from 1 to 0 during training, we observe that using a constant step size\nresults in better performance. Our grid search strategy for all stable baselines experiments is consistent: we run\nthe experiments for 2 million iterations, select the hyper-parameters that yield the best AUC, and then use these\nhyper-parameters for an additional 8 million iterations.\nHyperparameter SPMA MDPO TRPO_regularized TRPO_constrained PPO\nRewardnormalization ✗ ✗ ✗ ✗ ✗\nObservationnormalization ✗ ✗ ✗ ✗ ✗\nOrthogonalweightinitialization ✓ ✓ ✓ ✓ ✓\nValuefunctionclipping ✗ ✗ ✗ ✗ ✗\nGradientclipping ✗ ✗ ✗ ✗ ✓\nProbabilityratioclipping ✗ ✗ ✗ ✗ ✓\nAdamstep-size 3×10−4\nMinibatchsize 256\nFramestack 4\nNumberofenvironmentcopies 8\nGAEλ 0.95\nHorizon(T) 128\nNumberofinnerloopupdates(m) 5\nEntropycoefficient 0\nDiscountfactor 0.99\nTotalnumberoftimesteps 107\nNumberofrunsforplotaverages 5\nConfidenceintervalforplotruns ∼95%\nTable 1: Hyper-parameters for Atari experiments.\nHyperparameter SPMA MDPO TRPO_regularized TRPO_constrained PPO\nMinibatchsize 2048 2048 2048 64 64\nRewardnormalization ✗ ✗ ✗ ✗ ✗\nObservationnormalization ✗ ✗ ✗ ✗ ✗\nOrthogonalweightinitialization ✓ ✓ ✓ ✓ ✓\nValuefunctionclipping ✗ ✗ ✗ ✗ ✗\nGradientclipping ✗ ✗ ✗ ✗ ✓\nProbabilityratioclipping ✗ ✗ ✗ ✗ ✓\nAdamstep-size ✗ ✗ ✗ ✓ ✓\nGAEλ 0.95\nHorizon(T) 2048\nNumberofinnerloopupdates(m) 5\nEntropycoefficient 0\nDiscountfactor 0.99\nTotalnumberoftimesteps 107\nNumberofrunsforplotaverages 5\nConfidenceintervalforplotruns ∼95%\nTable 2: Hyper-parameters for MuJoCo experiments.\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nD.2 Ablation Study on the Number of Inner Loop Optimization Steps\nIn this subsection, we investigate the effect of varying the number of inner loop optimization steps (m) in the\nstable baselines experiments. Consistent with Tomar et al. [2020], we observe that using m=1 results in poor\nperformance, so we focus on larger values of m. In the MuJoCo experiments, increasing m from 5 to 10 and 15\nconsistentlyimprovestheperformanceofSPMA(seeFigure9). Specifically, forlargerm, SPMAbecomescomparable\nto TRPO-constrained on Hopper and Ant, while outperforming it on HalfCheetah (see Figure 7).\nFor the Atari experiments, we observe that increasing m does not necessarily improve the results across methods\n(see Figure 10). We conjecture that this is a side-effect of using a constant tuned step-size (for m=5) in the\ninner-loop. In the future, we plan to run the full grid-search for the inner-loop step-size for each value of m.\nAlternatively, we plan to investigate an adaptive way of setting the inner-loop step-size.\nFast Convergence of Softmax Policy Mirror Ascent\n(a)\n(b)\nFigure 7: MuJoCo results for m=10 (a) and m=15 (b). As m increases from 5 ( Figure 2) to 10 and 15, SPMA\nshows performance comparable to the fine-tuned TRPO-constrained.\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\n(a)\n(b)\nFigure 8: Atari results for m = 10 (top) and m = 15 (bottom). Increasing m does not necessarily lead to\nperformance improvements.\nFast Convergence of Softmax Policy Mirror Ascent\nFigure 9: MuJoCo ablation on m: The rows correspond to the Hopper-v4, Walker2d-v4, HalfCheetah-v4, and\nAnt-v4 environments, respectively. As the number of inner loop optimization steps m increases, SPMA shows\nimprovements in expected reward and becomes comparable to the fine-tuned TRPO-constrained.\nReza Asad1, Reza Babanezhad2, Issam Laradji3, Nicolas Le Roux4, Sharan Vaswani1\nFigure 10: Atari ablation on m: The rows correspond to the BeamRider-v4, DemonAttack-v4, Alien-v4, and\nAmidar-v4 games. We observe that increasing m does not necessarily improve results across methods.",
    "pdf_filename": "Fast_Convergence_of_Softmax_Policy_Mirror_Ascent.pdf"
}