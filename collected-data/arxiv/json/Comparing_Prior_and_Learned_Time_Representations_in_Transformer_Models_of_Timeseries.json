{
    "title": "Comparing Prior and Learned Time Representations in Transformer Models of Timeseries",
    "abstract": "What sets timeseries analysis apart from other machine learning exercises is that time representation becomes a primary aspect of the experiment setup, as it must adequately represent the temporal relations that are relevant for the application at hand. In the work described here we study wo different variations of the Transformer architecture: one where we use the fixed time representation pro- posed in the literature and one where the time representation is learned from the data. Our experiments use data from predicting the energy output of solar panels, a task that exhibits known peri- odicities (daily and seasonal) that is straight-forward to encode in the fixed time representation. Our results indicate that even in an experiment where the phenomenon is well-understood, it is diffi- cult to encode prior knowledge due to side-effects that are difficult to mitigate. We conclude that research work is needed to work the human into the learning loop in ways that improve the robustness and trust-worthiness of the network. CCS CONCEPTS • Computing methodologies →Neural networks; Artificial intelligence; Supervised learning; • Mathematics of computing →Time series analysis. ACM Reference Format: Natalia Koliou, Tatiana Boura, Stasinos Konstantopoulos, George Meramve- liotakis, and George Kosmadakis. 2024. Comparing Prior and Learned Time Representations in Transformer Models of Timeseries. In 13th Conference on Artificial Intelligence (SETN 2024), September 11–13, 2024, Piraeus, Greece. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3688671.3688747 1 INTRODUCTION What sets apart timeseries analysis from other machine learning exercises is taking into account the sequence as well as, in most cases, the temporal distance between observations. This makes the representation of time a primary aspect of the experiment setup, ∗Both authors contributed equally to this research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SETN 2024, September 11–13, 2024, Piraeus, Greece © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0982-1/24/09 https://doi.org/10.1145/3688671.3688747 as it must be adequate for representing the temporal relations that are relevant for the application at hand. To elaborate on the various considerations that need to be ad- dressed, first consider that one cannot assume fully observed, uni- formly sampled inputs as there might be gaps in the data, vary- ing sampling rates, and (for multivariate timeseries) misalignment between the time steps of the different variables. This dictates a representation that allows time differences to be computed, so that (for example) September 2023 is ‘closer’ to January 2024 than it is to September 2022. Simple timestamps allow this but do not capture periodicity: Consider, for instance, an application with seasonal periodicity where September 2023 is ‘closer’ to September 2022 than to January 2024. There is a rich relevant literature in both signal processing and in non-parametric statistics, as well as in adapting AI/ML approaches to timeseries processing when facing irregularly sampled and/or sparse data. In particular, deep learning approaches that utilize re- current networks based on Gated Recurrent Units (GRUs) [1], Long Short-Term Memory networks (LSTMs) [2, 3], and ODE-RNNs [4] have shown promising results. In the work described we focus on deep learning methods as well, but specifically on adapting Trans- former models to timeseries analysis. We will first present how the relevant literature handles the representation of time when applying Transformer models to timeseries (Section 2) and then proceed to propose an alternative representation that is expected to out-perform the original representation for our specific application on predicting the energy output of solar panels (Section 3). We close with giving and discussing comparative experimental results (Section 4) and conclusions and future work (Section 5). 2 BACKGROUND Unlike recurrent and differential equation-based architectures which process inputs sequentially, Transformers [6] expect the complete time-series as input and use the attention mechanism to look for relationships between all inputs simultaneously. This has the side- effect that the temporal order is no longer implied by the order in which the inputs are presented to the network, so that input vec- tors must be augmented with features that represent time. But this also creates the opportunity to use time embeddings that represent temporal information in a way that encodes prior knowledge about the data. The most characteristic example is periodicity. When the data is known or suspected to exhibit periodicity, absolute positional encoding [7] encodes time as two features: the sine and the cosine arXiv:2411.12476v1  [cs.LG]  19 Nov 2024",
    "body": "Comparing Prior and Learned Time Representations in\nTransformer Models of Timeseries\nNatalia Koliou∗\nTatiana Boura∗\nStasinos Konstantopoulos\n{nataliakoliou,tatianabou,konstant}@iit.demokritos.gr\nInstitute of Informatics and Telecommunications,\nNCSR ‘Demokritos’\nAg. Paraskevi, Greece\nGeorge Meramveliotakis\nGeorge Kosmadakis\n{gmera,gkosmad}@ipta.demokritos.gr\nInstitute of Nuclear & Radiological Sciences and\nTechnology, Energy & Safety,\nNCSR ‘Demokritos’\nAg. Paraskevi, Greece\nABSTRACT\nWhat sets timeseries analysis apart from other machine learning\nexercises is that time representation becomes a primary aspect of\nthe experiment setup, as it must adequately represent the temporal\nrelations that are relevant for the application at hand. In the work\ndescribed here we study wo different variations of the Transformer\narchitecture: one where we use the fixed time representation pro-\nposed in the literature and one where the time representation is\nlearned from the data. Our experiments use data from predicting\nthe energy output of solar panels, a task that exhibits known peri-\nodicities (daily and seasonal) that is straight-forward to encode in\nthe fixed time representation. Our results indicate that even in an\nexperiment where the phenomenon is well-understood, it is diffi-\ncult to encode prior knowledge due to side-effects that are difficult\nto mitigate. We conclude that research work is needed to work the\nhuman into the learning loop in ways that improve the robustness\nand trust-worthiness of the network.\nCCS CONCEPTS\n• Computing methodologies →Neural networks; Artificial\nintelligence; Supervised learning; • Mathematics of computing\n→Time series analysis.\nACM Reference Format:\nNatalia Koliou, Tatiana Boura, Stasinos Konstantopoulos, George Meramve-\nliotakis, and George Kosmadakis. 2024. Comparing Prior and Learned Time\nRepresentations in Transformer Models of Timeseries. In 13th Conference\non Artificial Intelligence (SETN 2024), September 11–13, 2024, Piraeus, Greece.\nACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3688671.3688747\n1\nINTRODUCTION\nWhat sets apart timeseries analysis from other machine learning\nexercises is taking into account the sequence as well as, in most\ncases, the temporal distance between observations. This makes the\nrepresentation of time a primary aspect of the experiment setup,\n∗Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nSETN 2024, September 11–13, 2024, Piraeus, Greece\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0982-1/24/09\nhttps://doi.org/10.1145/3688671.3688747\nas it must be adequate for representing the temporal relations that\nare relevant for the application at hand.\nTo elaborate on the various considerations that need to be ad-\ndressed, first consider that one cannot assume fully observed, uni-\nformly sampled inputs as there might be gaps in the data, vary-\ning sampling rates, and (for multivariate timeseries) misalignment\nbetween the time steps of the different variables. This dictates a\nrepresentation that allows time differences to be computed, so that\n(for example) September 2023 is ‘closer’ to January 2024 than it is to\nSeptember 2022. Simple timestamps allow this but do not capture\nperiodicity: Consider, for instance, an application with seasonal\nperiodicity where September 2023 is ‘closer’ to September 2022\nthan to January 2024.\nThere is a rich relevant literature in both signal processing and in\nnon-parametric statistics, as well as in adapting AI/ML approaches\nto timeseries processing when facing irregularly sampled and/or\nsparse data. In particular, deep learning approaches that utilize re-\ncurrent networks based on Gated Recurrent Units (GRUs) [1], Long\nShort-Term Memory networks (LSTMs) [2, 3], and ODE-RNNs [4]\nhave shown promising results. In the work described we focus on\ndeep learning methods as well, but specifically on adapting Trans-\nformer models to timeseries analysis. We will first present how\nthe relevant literature handles the representation of time when\napplying Transformer models to timeseries (Section 2) and then\nproceed to propose an alternative representation that is expected to\nout-perform the original representation for our specific application\non predicting the energy output of solar panels (Section 3). We\nclose with giving and discussing comparative experimental results\n(Section 4) and conclusions and future work (Section 5).\n2\nBACKGROUND\nUnlike recurrent and differential equation-based architectures which\nprocess inputs sequentially, Transformers [6] expect the complete\ntime-series as input and use the attention mechanism to look for\nrelationships between all inputs simultaneously. This has the side-\neffect that the temporal order is no longer implied by the order in\nwhich the inputs are presented to the network, so that input vec-\ntors must be augmented with features that represent time. But this\nalso creates the opportunity to use time embeddings that represent\ntemporal information in a way that encodes prior knowledge about\nthe data.\nThe most characteristic example is periodicity. When the data\nis known or suspected to exhibit periodicity, absolute positional\nencoding [7] encodes time as two features: the sine and the cosine\narXiv:2411.12476v1  [cs.LG]  19 Nov 2024\n\nSETN 2024, September 11–13, 2024, Piraeus, Greece\nNatalia Koliou, Tatiana Boura, Stasinos Konstantopoulos, George Meramveliotakis, and George Kosmadakis\nof the raw timestamp. This representation allows the Transformer\nto refer to both absolute times (since the combination of sine and\ncosine maps to a single position on the trigonometric circle) and\nto a representation where all (for instance) peak positions have\nthe same feature value. Referring back to our previous example,\nremember how we want September 2022 and September 2023 to\nbe on a similar or identical time from the periodic perspective, but\nnot from the linear-time perspective. The Transformer will then\nhave the flexibility to weigh these features in accordance with the\nphenomenon being modeled.\nThe meaningful and successful usage of pre-computed time fea-\ntures requires expert knowledge of the application domain and the\nperiodicities that make sense for the phenomenon being modeled.\nIn other words, it adds upon the inductive bias of the algorithm.\nAs expected, a line of research emerged that aims to automati-\ncally acquire the time representation from the data. The multi-time\nattention network (mTAN) [5] is a Transformer model enhanced\nwith a time-attention module. This module learns temporal patterns\nby leveraging a trainable embedding function 𝜙(𝑡) that maps time\n𝑡(scaled to 0..1) to a vector of length 𝑑.\nThis vector has one linear element that captures the linear pro-\ngression of time and 𝑑−1 sinusoidal elements that capture different\nperiodicities as frequency/phase pairs. Formally, the embedding\nfunction is:\n𝜙𝑖(𝑡) =\n(\n𝜔0𝑡+ 𝛼0,\nif 𝑖= 0,\nsin(𝜔𝑖𝑡+ 𝛼𝑖),\nif 0 < 𝑖< 𝑑\n(1)\nwhere 𝜔𝑖and 𝛼𝑖are the learnable parameters.\nIn concrete neural network terms, this is implemented as one\nlinear layer for 𝜔0, 𝛼0 and one linear layer with a sine activation\nfunction for all other 𝜔𝑖, 𝛼𝑖. More precisely, it is implemented with\nmultiple such structures, one for each attention head. It should be\nnoted at this point that for the matrix multiplications to work out,\nthis modeling has the side-effect that 𝑑must be the same as the\ndimensionality of the output vector.\nThis modeling gives the Transformer the flexibility to search for\nthe period/phase pairs that best capture the periodicities implied\nby the data. Only minimal bias is introduced, namely that time is\nstructured as a sine/linear pair.\n3\nREPRESENTING TIME\nIn order to experiment with time representations and the effect they\nhave on the quality of the learned model, we have assumed an appli-\ncation where we need to predict the energy output of solar thermal\ncollectors based on external conditions. More detailed information\nabout the application and the data is provided in Section 4, and for\nthe purposes of the current discussion it suffices to mention that\nthe input variables are solar radiation and external temperature,\nwhich exhibit the obvious daily and annual periodicities.\nWe have noted however, that solar radiation behaves in a way\nthat is not captured by sinusoidal functions. While these functions\nrepresent well the similarity between the same time on a different\nday, as well as the similarity between early morning and late after-\nnoon, they fail to represent the fact that the whole of the night-time\nis the same and (as far as solar radiation is concerned) it makes no\ndifference if the time is 10pm or 24am. We have, therefore, thought\nFigure 1: Hourly mean pyranometer values (red) and their\nfunction approximations, triangular pulse (blue) and sinu-\nsoidal function (green).\nthat a triangular spike will be a better representation than a sine.\nThis is further corroborated by the data shown in Figure 1, showing\npyranometer (solar radiation) values aggregated to 1h intervals and\naveraged through the year, and the best approximation that can be\nachieved by the sine function and by the triangular spike.\nBased on the argument above, we have defined the four alterna-\ntive time embeddings presented below.\n3.1\nTriangular Pulse & Linear\nThe first embedding is a season-modulated triangular pulse/linear\npair. The base and peak of the triangular pulse is not the same for\neach day, but is calculated so that the pulse will start at sunrise,\npeak at noon, and end at sunset. This represents that solar radia-\ntion at noon is distinct from all other times during day, one hour\nbefore noon is similar to one hour after noon, and so on until sun-\nrise/sunset. Outside the base of the pulse, all times are represented\nby the same value of 0.01 to denote that the distinction between\nthem is not important.1\nAs for the linear function, it provides a straightforward method\nfor ensuring the uniqueness of timestamps within the representa-\ntion. By mapping each timestamp to a unique value, we establish\nclear distinctions between different time points. Unlike the period-\nicity of the triangular pulse, the linear function spans the entire\nrange of timestamps, from the earliest to the latest, using both date\nand time components. This continuous representation does not\nreset daily but maps the entire period linearly, ensuring a unique\nvalue for each timestamp based on its position in the overall time\nspan.\nThe second embedding is similar to the one above, but the pulse\nis fixed to start at 7am, peak at 1pm, and end at 9pm regardless of\nthe date. This simpler embedding is included in order to be able to\nsee if the more complex approach above has added value or there\nis no significant loss in accuracy when using fixed parameters.\n1This value is set to 0.01 instead of 0 to satisfy a technical requirement of the\nimplementation.\n\nComparing Prior and Learned Time Representations in Transformer Models of Timeseries\nSETN 2024, September 11–13, 2024, Piraeus, Greece\nFigure 2: Examples of learned time representation features\nwith a learnable triangular pulse. The five illustrated time\nfeatures come from the same testing sample and highlight\nthe capability of the model to learn (non-)isosceles pulses\nwith different bases.\n3.2\nSinusoidal\nThe third embedding follows the absolute positional encoding lit-\nerature and consists of two sine/cosine pairs, one pair computed\nfrom the month of the year and one pair computed from the hour\nof the day. Sine and cosine capture cyclical patterns effectively, and\nby combining the hour and the month embedding we guarantee\nthat the Transformer has the means to model daily and annual\nperiodicities and also to refer to absolute timepoints if the data\nprove this useful.\nTo find the appropriate phase shift and period, we argue as\nfollows: To represent the daily cycle we need our sine wave to\npeak at noon. We aim for noon to be distinct, while the remaining\ntimestamps should exhibit symmetrical correlation. To achieve this,\nobserve how a period of 12 for hours and 24 for months, with a\nzero shift, will work as expected. Naturally, the cosine must be\nparameterized with identical phase and period to maintain the\nproperty that the sine/cosine pair uniquely refers to a point in the\ntrigonometric circle.\n3.3\nSine & Sawtooth\nThe fourth and final embedding consists of a sine and a sawtooth\nfunction. Just like in the sinusoidal embedding, we also use the sine\nwave with the same parameters (shift, period) to express correlation\namong timestamps. However, when it comes to expressing unique-\nness, we considered using a sawtooth wave instead of a cosine\nwave, to observe whether any noticeable changes might take place.\nUnlike the linear function used in the triangular pulse embeddings\nwhich spans the entire range of timestamps, the sawtooth wave\nresets to 0 output after each day. The sawtooth wave parameters\n(shift, period) are set to (6, 6) for hours and (12, 12) for months to\nscale the output values for both cases within [-1, 1].\n3.4\nLearned Time Representations\nAs mentioned in the Background, the idea behind the mTAN is to\nlearn periodic features with a sinusoidal representation. We have\nargued that, for our application, a triangular pulse for representing\nthe non-linear features may be a better fit. Thus, we altered the\nactivation function of the presented model to convert the linear\nlayer to a triangular pulse.\nOur first attempt at creating such a function involved using a\ntriangular function with a fixed base. In this case, the start and end\nof the pulse corresponded to the hours when difference of the solar\noutput peaked (7 AM and 9 PM respectively). This approach was\nnot fruitful, as the model was not given the opportunity to learn\nthe periodic representations.\nNaturally, we then focused on learning the base of the pulse.\nChoosing 1 PM as the peak of the pulse, the two most straight-\nforward approaches we implemented were: (a) splitting the time\nvector and then using the hours where the absolute difference of\nthe elements in the learned time vector is the largest as the start and\nend of the pulse; and (b) splitting the time vector and then using\nthe hours where the elements themselves in the learned time vector\nhave the absolute largest values as the start and end of the pulse.\nHowever, both implementations failed to learn any meaningful\nform of pulse and remained fixed on the initial time parameters.\nThe approach that successfully achieved the task of learning\ndifferent triangular pulses was an engineering one. It emerged from\nthe idea that the non-linear function itself should be simple in terms\nof numerical computation and traceability, since the previous, more\ncomplex approaches were not suitable for the task. The idea is\ndescribed as follows: we first calculate the absolute difference of\neach value in the time vector of each representation of size 𝑑and\nthe value of the 13th element of this representation (corresponding\nto 1 PM). Then we compute the 25th percentile of these differences\nand replace each difference dist𝑖𝑗with zero if it is less than the\npercentile value 𝑣𝑖of the corresponding representation 𝑖, and with\n1 −dist𝑖𝑗\n𝑣𝑖\notherwise. Figure 2 presents a few example triangular\npulse time representations learned from employing this approach.\n4\nEXPERIMENTS AND RESULTS\n4.1\nExperimental setup\nOur data was collected from a pilot building at NCSR ‘Demokri-\ntos’. The pilot building features solar thermal collectors used to\nexperiment with how to most efficiently control heating systems\n(solar, heat pump, etc.) to achieve satisfactory space heating, sat-\nisfy hot water demand, and minimize electric power consumption.\nThe machine learning task associated with this application is to\npredict solar power production from variables reflecting external\nconditions, namely solar radiation and external temperature. The\noutput of the model is not specific power in terms of kWh, but a\nlabel that characterizes power production as being in one of five\nclasses, defined based on expected demand.\nBased on the above, the machine learning task is to transform a\nmulti-variate timeseries of external conditions aggregated into one-\nhour intervals, into a timeseries of power production levels (the five-\nclass labeling schema mentioned above). The dataset has a moderate\namount of gaps due to the sensors occasionally giving erroneous,\n\nSETN 2024, September 11–13, 2024, Piraeus, Greece\nNatalia Koliou, Tatiana Boura, Stasinos Konstantopoulos, George Meramveliotakis, and George Kosmadakis\nTable 1: Evaluation results for the classification task using prior and learned time representations. Each model was trained\nusing six different initialization seeds, and the final metrics are composed from the aggregation (mean and standard deviation)\nof the results from these trained models.\nPrior Time Representations\nLearned Time Representations\nMetric\ntr. pulse & linear †\nfixed tr. pulse & linear †\nsine & cosine ‡\nsine & sawtooth ‡\nsine & linear †\ntr. pulse & linear †\nPrecision (Micro)\n0.835 ± 0.017\n0.823 ± 0.013\n0.866 ± 0.010\n0.860 ± 0.004\n0.842 ± 0.009\n0.778 ± 0.010\nRecall (Micro)\n0.835 ± 0.017\n0.823 ± 0.013\n0.866 ± 0.010\n0.860 ± 0.004\n0.842 ± 0.009\n0.778 ± 0.010\nF-score (Micro)\n0.835 ± 0.017\n0.823 ± 0.013\n0.866 ± 0.010\n0.860 ± 0.004\n0.842 ± 0.009\n0.778 ± 0.010\nPrecision (Macro)\n0.650 ± 0.027\n0.629 ± 0.012\n0.679 ± 0.025\n0.666 ± 0.009\n0.647 ± 0.012\n0.582 ± 0.015\nRecall (Macro)\n0.715 ± 0.033\n0.673 ± 0.015\n0.722 ± 0.024\n0.721 ± 0.026\n0.694 ± 0.023\n0.621 ± 0.016\nF-score (Macro)\n0.663 ± 0.030\n0.632 ± 0.013\n0.689 ± 0.019\n0.676 ± 0.016\n0.646 ± 0.019\n0.571 ± 0.013\nPrecision (Weighted)\n0.884 ± 0.010\n0.876 ± 0.006\n0.894 ± 0.009\n0.895 ± 0.004\n0.892 ± 0.005\n0.867 ± 0.007\nRecall (Weighted)\n0.835 ± 0.017\n0.823 ± 0.013\n0.866 ± 0.010\n0.860 ± 0.004\n0.842 ± 0.009\n0.778 ± 0.010\nF-score (Weighted)\n0.853 ± 0.013\n0.842 ± 0.008\n0.876 ± 0.009\n0.872 ± 0.004\n0.860 ± 0.007\n0.812 ± 0.009\n†: input being sec/min/hour/day/month/year\n‡: input being month/hour\nFigure 3: Normalized training loss progression for each\nmodel. The models presented are the best-performing mod-\nels across different time representations (prior and learned).\nout-of-range readings which are removed. The dataset exhibits the\nobvious daily periodicity, and also exhibits a non-periodic trend.\nThe non-periodic trend is in reality seasonal periodicity, but since\nwe have used the data from 10 months it appears to the machine\nlearning task as a non-periodic trend.2\nTo apply the prior time representation we implemented a Trans-\nformer with an encoder, a decoder, and a classifier, in that order.\nWe refer to this architecture as PriorTime in the results presented\nbelow. The encoder maps the input into a latent representation, the\ndecoder reconstructs it preserving the original dimensionality of\nits features, and the classifier (a single linear layer) outputs a prob-\nability distribution over the five labels. To apply the learned time\nrepresentation, we used the mTAN architecture, where the decoder\nis the same single-layer linear classifier. Figure 3 give the training\n2Available at https://zenodo.org/records/12818885\nloss from training these models. The complete experimental setup\nis also published.3\n4.2\nClassification results and discussion\nTo evaluate the performance of our models on the classification\ntask, we compared both the PriorTime and mTAN models across\ndifferent time representations. Our evaluation criteria included\nvarious performance metrics such as precision, recall, and F1-score,\nas shown in Table 1.\nThe PriorTime model was trained and tested using four differ-\nent time representations: triangular pulse & linear, fixed triangular\npulse & linear, sine & cosine, and sine & sawtooth. Among these rep-\nresentations, the sine & cosine approach consistently demonstrated\nthe strongest performance. The sine & sawtooth representation\napproached the results achieved by sine & cosine; however, it only\noutperformed once, and the difference was not significant. Concern-\ning the two triangular pulse/linear representations, they exhibited\nthe weakest performance overall, with the fixed variation perform-\ning slightly worse than the season-modulated one. This was not\nsurprising, as we expected the more informed time representation\nto give better results.\nThe second point is that the sine representations consistently\noutperform the triangular ones in both settings (prior and learned),\ndespite the fact that the triangular pulse is more precise when it\ncomes to making similar times have similar inputs (solar radiation\nand temperature), as already discussed in the beginning of Section 3.\nWe believe that this is due to the fact that the identical close-to-zero\nvalues outside the base of the pulse hinder back-propagating loss to\nthe linear layer that feeds the sine/triangular activation function.\nWhen these two points are put together, they imply that (a) the\nfirst linear layer is capable of managing both seasonal variation\nand the daily periodicity by properly weighting the inputs of the\nactivation functions; (b) (almost) zeroing-out inputs hinders back-\npropagation; and (c) having an activation function that dynamically\nchanges to model the unique characteristics of each day individually\nenhances convergence.\n3See guide under time_representations/installation at the AINST-2024 branch\nof repository https://github.com/data-eng/navgreen and also directly accessible at\nhttps://github.com/data-eng/navgreen/releases/tag/AINST-2024\n\nComparing Prior and Learned Time Representations in Transformer Models of Timeseries\nSETN 2024, September 11–13, 2024, Piraeus, Greece\n(a) Initial feature representation\n(b) Final feature representation\nFigure 4: Example of the learning progress of a sine time feature. Both figures illustrate the time representation of the test\nexamples by aggregating them per time-feature using the mean and standard deviation. The left figure shows the feature before\nthe learning process starts, while the right one shows the final learned feature. The learned time representation is meaningful\nsince it distinguishes midday and has similar values for hours with similar accumulated daylight.\n4.3\nDiscussion of the Learned Time\nRepresentations\nAs noted in Section 2, we are bound by the model’s architecture\nto learn 𝑛−1 periodic and 1 non-periodic feature representations\nfrom our data, with 𝑛being the number of the final output points\nof the time series (in our case, 24).\nWhen subjecting the periodic time representation to be a trian-\ngular pulse, the model starts from very narrow pulses and then\nproceeds to learn a combination of those, as presented previously\nin Figure 2. On the other hand, when we define the periodicity\nas a sine function, the initial weight initialization (which sets the\nweights to very small values) makes the initial representations a\nvery short part of the sinusoidal curve (so short that visually ap-\npears as linear in our graphs). The model then learns to scale the\ntime vectors to model more meaningful time representations. An\nexample of this behavior is presented in Figure 4.\nHowever, one might wonder: are all of the 23 learned time rep-\nresentations meaningful? Since it is hard to track the influence of\nevery representation on the classification task, we can only spec-\nulate. In Figure 5 are demonstrated all the periodic sine features,\ngrouped by their similarity. Most time features fall into the first\nthree groups. The last three groups, which only include one rep-\nresentation each, seem to be the most explainable regarding our\nknowledge of the domain. They capture larger periods that lead to\nthe numerical correlation of different hours in a day. We believe\nthat these are the most important features on which the model’s\nprediction relies and also that when it started to learn these features,\nit neglected the others and assigned smaller weights to them. To\nsubstantiate this assertion we illustrate in Figure 6 the correspond-\ning groups at the start of the learning process. It can be claimed\nthat the first three groups exhibit minimal change, whereas the last\nthree change visibly.\nTo model the time points for each example, we normalized each\ntimestamp using the Unix timestamps from our data’s date range.\nThe resulting time points range from 0 to 1, but within a daily\nsample, the hourly time points vary slightly. The Transformer’s\narchitecture utilizes the key/value/query formulation [6]. The query\nis the information that is being looked for, the key is the context\nor reference, and the value is the content that is being searched. In\nthe context of mTAN, the initial keys are the time points and the\ninitial queries are vectors with values equally distributed from 0 to\n1. Both of these terms are then passed through the same learnable\ntime layers. Since, initially, the keys and the queries have a different\norder of magnitude it is expected that either the model will increase\nthe magnitude of the keys or decrease the magnitude of the queries.\nThrough our experiments we discovered that the former occurs.\nAs presented in Figure 7 the smaller key values correspond to a\nsmaller part of the sine wave, whereas the larger query values cover\na larger part of the sine wave (Figure 8).\nTo further evaluate our claim, we modified the time point cre-\nation so that both queries and keys have the same values. To do\nso, we assigned each time point referring to an hour in a day its\nnormalized index by dividing it with the total number of hours in a\nday. As expected, the learned representation then was the same for\nboth keys and queries, as illustrated in Figure 9. Comparing them\nto the previously learned representations in Figures 7 and 8, the\nevaluation of the keys has indeed scaled up, while the evaluation\nof the queries has not changed drastically.\nThe main reason we opted for normalizing the time steps using\nUNIX time was that it slightly performed better for the classification\ntask. We believe that this is the case because when normalizing the\ntime points considering only the hour, we lose any other characteris-\ntic that relates to the progression of time. This further corroborates\nthat (as we extensively stressed earlier) the non-periodical features\nare crucial for our application.\n\nSETN 2024, September 11–13, 2024, Piraeus, Greece\nNatalia Koliou, Tatiana Boura, Stasinos Konstantopoulos, George Meramveliotakis, and George Kosmadakis\n(a) Group 1 (7 time representations)\n(b) Group 2 (9 time representations)\n(c) Group 3 (4 time representations)\n(d) Group 4 (1 time representation)\n(e) Group 5 (1 time representation)\n(f) Group 6 (1 time representation)\nFigure 5: Learned sine time representations grouped by their similarity. Each figure is comprised of the aggregated represen-\ntations computed on the testing data (mean and standard deviation of each time-feature). The top three figures gather the\nmajority of the features, whereas the bottom three include only one feature each.\n(a) Group 1\n(b) Group 2\n(c) Group 3\n(d) Group 4\n(e) Group 5\n(f) Group 6\nFigure 6: Initial values of the groups of the sine time representations.\n\nComparing Prior and Learned Time Representations in Transformer Models of Timeseries\nSETN 2024, September 11–13, 2024, Piraeus, Greece\n(a) Group 1\n(b) Group 2\n(c) Group 3\n(d) Group 4\n(e) Group 5\n(f) Group 6\nFigure 7: Learned sine time representations on the keys of the test data. The illustrated representations occurred when the keys\nand queries had different orders of magnitude.\n(a) Group 1\n(b) Group 2\n(c) Group 3\n(d) Group 4\n(e) Group 5\n(f) Group 6\nFigure 8: Learned sine time representations on the queries of the test data. The illustrated representations occurred when the\nkeys and queries had different orders of magnitude.\n\nSETN 2024, September 11–13, 2024, Piraeus, Greece\nNatalia Koliou, Tatiana Boura, Stasinos Konstantopoulos, George Meramveliotakis, and George Kosmadakis\n(a) Group 1\n(b) Group 2\n(c) Group 3\n(d) Group 4\n(e) Group 5\n(f) Group 6\nFigure 9: Learned sine time representations on the keys and queries of the test data. The illustrated representations occurred\nwhen the keys and queries had the same order of magnitude.\n5\nCONCLUSIONS\nWe studied the impact of time embeddings when applying Trans-\nformer models to timeseries analysis, and specifically the impact of\nusing a very informed, application-specific time representation, a\ngeneric sinusoidal representation known to capture periodic phe-\nnomena well, and a method for learning the parameters that best\nfit either of these two to the specific dataset at hand.\nOur experiments concluded that Transformers (and DNNs in\ngeneral we could argue) are not very amenable to over-engineering\nthe time representation due to side-effects that are difficult to miti-\ngate. This can be seen as a negative quality in applications such as\nours where there is extensive prior knowledge on a well-studied\nphenomenon, but can also be construed as a positive quality as the\nnetwork was able to ‘discover’ the knowledge we were trying to\nconvey. Especially the comparison between the sinusoidal prior\nand learned representations is very promising in this respect, at\nthe maximally-flexible mTAN network recovered almost all the\naccuracy of the prior-time sinusoidal network.\nA second level of analysis delved into the nature of the features\nthat mTAN learned. This analysis has demonstrated the ability of\nthe linear layer - sinusoidal activation function architecture to very\nclosely approximate the clearly non-smooth behavior discussed in\nSection 3 and shown in Figure 1.\nIt should, however, be noted that our analysis was restricted\nto what we could indirectly observe by trying out different pa-\nrameterizations and speculating based on our understanding of\nhow the network is trained. Besides any general advancements in\nmethodologies for explaining neural networks, this also showed us\na path for future work specifically targeting mTAN. Since one of\nthe hindrances was that we were unable to observe the effect of\neach feature due the distributed nature of the classifier, we would\nhave liked to systematically explore the effect of incrementally in-\ncreasing the number of features and observing which features gets\nlearned (interpreted as, is the most impactful for reducing loss),\nwhich is learned second and so on. However, the linear algebra\nbehind mTAN ties the dimensionality of the feature representa-\ntion to the dimensionality of the input, as we cannot de-couple the\nencoding of periodicity and the encoding of linear time. Since (as\nargued above) linear time is critical for performance, we are stuck\nwith a pre-defined dimensionality for the sine features as well.\nOur envisaged future research is to re-work the linear algebra of\nthe mTAN so that we can de-couple the encoding of the periodic\nand linear time representation, allowing to have only the latter\nbe constrained by the dimensionality of the input. In the specific\nexperiment presented here, this would translate to a better under-\nstanding of what happens when we inform the Transformer of the\nbias we want to apply regarding periodicities. Which, in its turn,\nis expected to lead to methods for affording the human operator\nintuitive and effective control of the network.\n\nComparing Prior and Learned Time Representations in Transformer Models of Timeseries\nSETN 2024, September 11–13, 2024, Piraeus, Greece\nACKNOWLEDGMENTS\nThis research has been co-financed by the European Union and\nGreek national funds through the program ‘Flagship actions in in-\nterdisciplinary scientific areas with a special interest in the connec-\ntion with the production network’ — GREEN SHIPPING — TAEDR-\n0534767 (Acronym: NAVGREEN). For more information please visit\nhttps://navgreen.gr\nThis research was co-funded by the European Union under GA\nno. 101135782 (MANOLO project). Views and opinions expressed\nare however those of the authors only and do not necessarily reflect\nthose of the European Union or CNECT. Neither the European\nUnion nor CNECT can be held responsible for them.\nAWS resources were provided by the National Infrastructures for\nResearch and Technology GRNET and funded by the EU Recovery\nand Resiliency Facility.\nREFERENCES\n[1] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan\nLiu. 2018. Recurrent Neural Networks for Multivariate Time Series with Missing\nValues. Scientific Reports 8 (04 2018). Issue 1.\n[2] Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. 2016. Phased LSTM: Accelerating\nRecurrent Network Training for Long or Event-based Sequences. In Advances In\nNeural Information Processing Systems. 3882–3890.\n[3] Trang Pham, Truyen Tran, Dinh Phung, and Svetha Venkatesh. 2017. Predicting\nhealthcare trajectories from medical records: A deep learning approach. Journal\nof Biomedical Informatics 69 (2017), 218–229.\n[4] Mona Schirmer, Mazin Eltayeb, Stefan Lessmann, and Maja Rudolph. 2022. Model-\ning Irregular Time Series with Continuous Recurrent Units. In Proceedings of the\n39th International Conference on Machine Learning, 17-23 Jul 2022, Vol. 162.\n[5] Satya Narayan Shukla and Benjamin Marlin. 2021. Multi-Time Attention Net-\nworks for Irregularly Sampled Time Series. In International Conference on Learning\nRepresentations.\n[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need.\nIn Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30.\nCurran Associates, Inc.\n[7] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,\nand Liang Sun. 2023. Transformers in Time Series: A Survey. In Proceedings of the\nThirty-Second International Joint Conference on Artificial Intelligence, Survey Track.\nMacao, 19–25 August 2023. https://doi.org/10.24963/ijcai.2023/759",
    "pdf_filename": "Comparing_Prior_and_Learned_Time_Representations_in_Transformer_Models_of_Timeseries.pdf"
}