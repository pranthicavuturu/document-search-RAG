{
    "title": "Comparing Prior and Learned Time Representations in",
    "abstract": "Whatsetstimeseriesanalysisapartfromothermachinelearning arerelevantfortheapplicationathand. exercisesisthattimerepresentationbecomesaprimaryaspectof Toelaborateonthevariousconsiderationsthatneedtobead- theexperimentsetup,asitmustadequatelyrepresentthetemporal dressed,firstconsiderthatonecannotassumefullyobserved,uni- relationsthatarerelevantfortheapplicationathand.Inthework formlysampledinputsastheremightbegapsinthedata,vary- describedherewestudywodifferentvariationsoftheTransformer ingsamplingrates,and(formultivariatetimeseries)misalignment architecture:onewhereweusethefixedtimerepresentationpro- betweenthetimestepsofthedifferentvariables.Thisdictatesa posedintheliteratureandonewherethetimerepresentationis representationthatallowstimedifferencestobecomputed,sothat learnedfromthedata.Ourexperimentsusedatafrompredicting (forexample)September2023is‘closer’toJanuary2024thanitisto theenergyoutputofsolarpanels,ataskthatexhibitsknownperi- September2022.Simpletimestampsallowthisbutdonotcapture odicities(dailyandseasonal)thatisstraight-forwardtoencodein periodicity:Consider,forinstance,anapplicationwithseasonal thefixedtimerepresentation.Ourresultsindicatethateveninan periodicitywhereSeptember2023is‘closer’toSeptember2022 experimentwherethephenomenoniswell-understood,itisdiffi- thantoJanuary2024. culttoencodepriorknowledgeduetoside-effectsthataredifficult Thereisarichrelevantliteratureinbothsignalprocessingandin tomitigate.Weconcludethatresearchworkisneededtoworkthe non-parametricstatistics,aswellasinadaptingAI/MLapproaches humanintothelearningloopinwaysthatimprovetherobustness totimeseriesprocessingwhenfacingirregularlysampledand/or andtrust-worthinessofthenetwork. sparsedata.Inparticular,deeplearningapproachesthatutilizere- currentnetworksbasedonGatedRecurrentUnits(GRUs)[1],Long CCSCONCEPTS Short-TermMemorynetworks(LSTMs)[2,3],andODE-RNNs[4] haveshownpromisingresults.Intheworkdescribedwefocuson •Computingmethodologies→Neuralnetworks;Artificial deeplearningmethodsaswell,butspecificallyonadaptingTrans- intelligence;Supervisedlearning;•Mathematicsofcomputing former modelstotimeseriesanalysis.Wewillfirstpresenthow →Timeseriesanalysis. therelevantliteraturehandlestherepresentationoftimewhen ACMReferenceFormat: applyingTransformermodelstotimeseries(Section2)andthen NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramve- proceedtoproposeanalternativerepresentationthatisexpectedto liotakis,andGeorgeKosmadakis.2024.ComparingPriorandLearnedTime out-performtheoriginalrepresentationforourspecificapplication RepresentationsinTransformerModelsofTimeseries.In13thConference onpredictingtheenergyoutputofsolarpanels(Section3).We onArtificialIntelligence(SETN2024),September11–13,2024,Piraeus,Greece. closewithgivinganddiscussingcomparativeexperimentalresults ACM,NewYork,NY,USA,9pages.https://doi.org/10.1145/3688671.3688747 (Section4)andconclusionsandfuturework(Section5). 1 INTRODUCTION 2 BACKGROUND Whatsetsaparttimeseriesanalysisfromothermachinelearning Unlikerecurrentanddifferentialequation-basedarchitectureswhich exercisesistakingintoaccountthesequenceaswellas,inmost processinputssequentially,Transformers[6]expectthecomplete cases,thetemporaldistancebetweenobservations.Thismakesthe time-seriesasinputandusetheattentionmechanismtolookfor representationoftimeaprimaryaspectoftheexperimentsetup, relationshipsbetweenallinputssimultaneously.Thishastheside- ∗Bothauthorscontributedequallytothisresearch. effectthatthetemporalorderisnolongerimpliedbytheorderin whichtheinputsarepresentedtothenetwork,sothatinputvec- Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor torsmustbeaugmentedwithfeaturesthatrepresenttime.Butthis classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed alsocreatestheopportunitytousetimeembeddingsthatrepresent forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation temporalinformationinawaythatencodespriorknowledgeabout onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored. Forallotheruses,contacttheowner/author(s). thedata. SETN2024,September11–13,2024,Piraeus,Greece Themostcharacteristicexampleisperiodicity.Whenthedata ©2024Copyrightheldbytheowner/author(s). isknownorsuspectedtoexhibitperiodicity,absolutepositional ACMISBN979-8-4007-0982-1/24/09 https://doi.org/10.1145/3688671.3688747 encoding[7]encodestimeastwofeatures:thesineandthecosine 4202 voN 91 ]GL.sc[ 1v67421.1142:viXra",
    "body": "Comparing Prior and Learned Time Representations in\nTransformer Models of Timeseries\nNataliaKoliou∗ GeorgeMeramveliotakis\nTatianaBoura∗ GeorgeKosmadakis\nStasinosKonstantopoulos {gmera,gkosmad}@ipta.demokritos.gr\n{nataliakoliou,tatianabou,konstant}@iit.demokritos.gr InstituteofNuclear&RadiologicalSciencesand\nInstituteofInformaticsandTelecommunications, Technology,Energy&Safety,\nNCSR‘Demokritos’ NCSR‘Demokritos’\nAg.Paraskevi,Greece Ag.Paraskevi,Greece\nABSTRACT asitmustbeadequateforrepresentingthetemporalrelationsthat\nWhatsetstimeseriesanalysisapartfromothermachinelearning arerelevantfortheapplicationathand.\nexercisesisthattimerepresentationbecomesaprimaryaspectof Toelaborateonthevariousconsiderationsthatneedtobead-\ntheexperimentsetup,asitmustadequatelyrepresentthetemporal dressed,firstconsiderthatonecannotassumefullyobserved,uni-\nrelationsthatarerelevantfortheapplicationathand.Inthework formlysampledinputsastheremightbegapsinthedata,vary-\ndescribedherewestudywodifferentvariationsoftheTransformer ingsamplingrates,and(formultivariatetimeseries)misalignment\narchitecture:onewhereweusethefixedtimerepresentationpro- betweenthetimestepsofthedifferentvariables.Thisdictatesa\nposedintheliteratureandonewherethetimerepresentationis representationthatallowstimedifferencestobecomputed,sothat\nlearnedfromthedata.Ourexperimentsusedatafrompredicting (forexample)September2023is‘closer’toJanuary2024thanitisto\ntheenergyoutputofsolarpanels,ataskthatexhibitsknownperi- September2022.Simpletimestampsallowthisbutdonotcapture\nodicities(dailyandseasonal)thatisstraight-forwardtoencodein periodicity:Consider,forinstance,anapplicationwithseasonal\nthefixedtimerepresentation.Ourresultsindicatethateveninan periodicitywhereSeptember2023is‘closer’toSeptember2022\nexperimentwherethephenomenoniswell-understood,itisdiffi- thantoJanuary2024.\nculttoencodepriorknowledgeduetoside-effectsthataredifficult Thereisarichrelevantliteratureinbothsignalprocessingandin\ntomitigate.Weconcludethatresearchworkisneededtoworkthe non-parametricstatistics,aswellasinadaptingAI/MLapproaches\nhumanintothelearningloopinwaysthatimprovetherobustness totimeseriesprocessingwhenfacingirregularlysampledand/or\nandtrust-worthinessofthenetwork. sparsedata.Inparticular,deeplearningapproachesthatutilizere-\ncurrentnetworksbasedonGatedRecurrentUnits(GRUs)[1],Long\nCCSCONCEPTS Short-TermMemorynetworks(LSTMs)[2,3],andODE-RNNs[4]\nhaveshownpromisingresults.Intheworkdescribedwefocuson\n•Computingmethodologies→Neuralnetworks;Artificial\ndeeplearningmethodsaswell,butspecificallyonadaptingTrans-\nintelligence;Supervisedlearning;•Mathematicsofcomputing\nformer modelstotimeseriesanalysis.Wewillfirstpresenthow\n→Timeseriesanalysis.\ntherelevantliteraturehandlestherepresentationoftimewhen\nACMReferenceFormat: applyingTransformermodelstotimeseries(Section2)andthen\nNataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramve- proceedtoproposeanalternativerepresentationthatisexpectedto\nliotakis,andGeorgeKosmadakis.2024.ComparingPriorandLearnedTime out-performtheoriginalrepresentationforourspecificapplication\nRepresentationsinTransformerModelsofTimeseries.In13thConference\nonpredictingtheenergyoutputofsolarpanels(Section3).We\nonArtificialIntelligence(SETN2024),September11–13,2024,Piraeus,Greece.\nclosewithgivinganddiscussingcomparativeexperimentalresults\nACM,NewYork,NY,USA,9pages.https://doi.org/10.1145/3688671.3688747\n(Section4)andconclusionsandfuturework(Section5).\n1 INTRODUCTION\n2 BACKGROUND\nWhatsetsaparttimeseriesanalysisfromothermachinelearning\nUnlikerecurrentanddifferentialequation-basedarchitectureswhich\nexercisesistakingintoaccountthesequenceaswellas,inmost\nprocessinputssequentially,Transformers[6]expectthecomplete\ncases,thetemporaldistancebetweenobservations.Thismakesthe\ntime-seriesasinputandusetheattentionmechanismtolookfor\nrepresentationoftimeaprimaryaspectoftheexperimentsetup,\nrelationshipsbetweenallinputssimultaneously.Thishastheside-\n∗Bothauthorscontributedequallytothisresearch. effectthatthetemporalorderisnolongerimpliedbytheorderin\nwhichtheinputsarepresentedtothenetwork,sothatinputvec-\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor torsmustbeaugmentedwithfeaturesthatrepresenttime.Butthis\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed alsocreatestheopportunitytousetimeembeddingsthatrepresent\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation\ntemporalinformationinawaythatencodespriorknowledgeabout\nonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.\nForallotheruses,contacttheowner/author(s). thedata.\nSETN2024,September11–13,2024,Piraeus,Greece Themostcharacteristicexampleisperiodicity.Whenthedata\n©2024Copyrightheldbytheowner/author(s).\nisknownorsuspectedtoexhibitperiodicity,absolutepositional\nACMISBN979-8-4007-0982-1/24/09\nhttps://doi.org/10.1145/3688671.3688747 encoding[7]encodestimeastwofeatures:thesineandthecosine\n4202\nvoN\n91\n]GL.sc[\n1v67421.1142:viXra\nSETN2024,September11–13,2024,Piraeus,Greece NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramveliotakis,andGeorgeKosmadakis\noftherawtimestamp.ThisrepresentationallowstheTransformer\ntorefertobothabsolutetimes(sincethecombinationofsineand\ncosinemapstoasinglepositiononthetrigonometriccircle)and\ntoarepresentationwhereall(forinstance)peakpositionshave\nthesamefeaturevalue.Referringbacktoourpreviousexample,\nrememberhowwewantSeptember2022andSeptember2023to\nbeonasimilaroridenticaltimefromtheperiodicperspective,but\nnotfromthelinear-timeperspective.TheTransformerwillthen\nhavetheflexibilitytoweighthesefeaturesinaccordancewiththe\nphenomenonbeingmodeled.\nThemeaningfulandsuccessfulusageofpre-computedtimefea-\nturesrequiresexpertknowledgeoftheapplicationdomainandthe\nperiodicitiesthatmakesenseforthephenomenonbeingmodeled.\nInotherwords,itaddsupontheinductivebiasofthealgorithm.\nAsexpected,alineofresearchemergedthataimstoautomati-\ncallyacquirethetimerepresentationfromthedata.Themulti-time\nattentionnetwork(mTAN)[5]isaTransformermodelenhanced\nwithatime-attentionmodule.Thismodulelearnstemporalpatterns Figure1:Hourlymeanpyranometervalues(red)andtheir\nbyleveragingatrainableembeddingfunction𝜙(𝑡)thatmapstime\nfunctionapproximations,triangularpulse(blue)andsinu-\n𝑡 (scaledto0..1)toavectoroflength𝑑.\nsoidalfunction(green).\nThisvectorhasonelinearelementthatcapturesthelinearpro-\ngressionoftimeand𝑑−1sinusoidalelementsthatcapturedifferent\nthatatriangularspikewillbeabetterrepresentationthanasine.\nperiodicitiesasfrequency/phasepairs.Formally,theembedding\nThisisfurthercorroboratedbythedatashowninFigure1,showing\nfunctionis:\npyranometer(solarradiation)valuesaggregatedto1hintervalsand\n(cid:40)\n𝜙 𝑖(𝑡)=\n𝜔 0𝑡+𝛼 0, if𝑖 =0,\n(1) averagedthroughtheyear,andthebestapproximationthatcanbe\nsin(𝜔 𝑖𝑡+𝛼 𝑖), if0<𝑖 <𝑑 achievedbythesinefunctionandbythetriangularspike.\nBasedontheargumentabove,wehavedefinedthefouralterna-\nwhere𝜔 𝑖 and𝛼 𝑖 arethelearnableparameters. tivetimeembeddingspresentedbelow.\nInconcreteneuralnetworkterms,thisisimplementedasone\nlinearlayerfor𝜔 0,𝛼 0andonelinearlayerwithasineactivation 3.1 TriangularPulse&Linear\nfunctionforallother𝜔 𝑖,𝛼 𝑖.Moreprecisely,itisimplementedwith Thefirstembeddingisaseason-modulatedtriangularpulse/linear\nmultiplesuchstructures,oneforeachattentionhead.Itshouldbe pair.Thebaseandpeakofthetriangularpulseisnotthesamefor\nnotedatthispointthatforthematrixmultiplicationstoworkout, eachday,butiscalculatedsothatthepulsewillstartatsunrise,\nthismodelinghastheside-effectthat𝑑 mustbethesameasthe peakatnoon,andendatsunset.Thisrepresentsthatsolarradia-\ndimensionalityoftheoutputvector. tionatnoonisdistinctfromallothertimesduringday,onehour\nThismodelinggivestheTransformertheflexibilitytosearchfor beforenoonissimilartoonehourafternoon,andsoonuntilsun-\ntheperiod/phasepairsthatbestcapturetheperiodicitiesimplied rise/sunset.Outsidethebaseofthepulse,alltimesarerepresented\nbythedata.Onlyminimalbiasisintroduced,namelythattimeis bythesamevalueof0.01todenotethatthedistinctionbetween\nstructuredasasine/linearpair. themisnotimportant.1\nAsforthelinearfunction,itprovidesastraightforwardmethod\n3 REPRESENTINGTIME forensuringtheuniquenessoftimestampswithintherepresenta-\nInordertoexperimentwithtimerepresentationsandtheeffectthey tion.Bymappingeachtimestamptoauniquevalue,weestablish\nhaveonthequalityofthelearnedmodel,wehaveassumedanappli- cleardistinctionsbetweendifferenttimepoints.Unliketheperiod-\ncationwhereweneedtopredicttheenergyoutputofsolarthermal icityofthetriangularpulse,thelinearfunctionspanstheentire\ncollectorsbasedonexternalconditions.Moredetailedinformation rangeoftimestamps,fromtheearliesttothelatest,usingbothdate\nabouttheapplicationandthedataisprovidedinSection4,andfor andtimecomponents.Thiscontinuousrepresentationdoesnot\nthepurposesofthecurrentdiscussionitsufficestomentionthat resetdailybutmapstheentireperiodlinearly,ensuringaunique\ntheinputvariablesaresolarradiationandexternaltemperature, valueforeachtimestampbasedonitspositionintheoveralltime\nwhichexhibittheobviousdailyandannualperiodicities. span.\nWehavenotedhowever,thatsolarradiationbehavesinaway Thesecondembeddingissimilartotheoneabove,butthepulse\nthatisnotcapturedbysinusoidalfunctions.Whilethesefunctions isfixedtostartat7am,peakat1pm,andendat9pmregardlessof\nrepresentwellthesimilaritybetweenthesametimeonadifferent thedate.Thissimplerembeddingisincludedinordertobeableto\nday,aswellasthesimilaritybetweenearlymorningandlateafter- seeifthemorecomplexapproachabovehasaddedvalueorthere\nnoon,theyfailtorepresentthefactthatthewholeofthenight-time isnosignificantlossinaccuracywhenusingfixedparameters.\nisthesameand(asfarassolarradiationisconcerned)itmakesno 1Thisvalueissetto0.01insteadof0tosatisfyatechnicalrequirementofthe\ndifferenceifthetimeis10pmor24am.Wehave,therefore,thought implementation.\nComparingPriorandLearnedTimeRepresentationsinTransformerModelsofTimeseries SETN2024,September11–13,2024,Piraeus,Greece\n3.4 LearnedTimeRepresentations\nAsmentionedintheBackground,theideabehindthemTAN isto\nlearnperiodicfeatureswithasinusoidalrepresentation.Wehave\narguedthat,forourapplication,atriangularpulseforrepresenting\nthenon-linearfeaturesmaybeabetterfit.Thus,wealteredthe\nactivationfunctionofthepresentedmodeltoconvertthelinear\nlayertoatriangularpulse.\nOurfirstattemptatcreatingsuchafunctioninvolvedusinga\ntriangularfunctionwithafixedbase.Inthiscase,thestartandend\nofthepulsecorrespondedtothehourswhendifferenceofthesolar\noutputpeaked(7AMand9PMrespectively).Thisapproachwas\nnotfruitful,asthemodelwasnotgiventheopportunitytolearn\ntheperiodicrepresentations.\nNaturally,wethenfocusedonlearningthebaseofthepulse.\nChoosing1PMasthepeakofthepulse,thetwomoststraight-\nforwardapproachesweimplementedwere:(a)splittingthetime\nvectorandthenusingthehourswheretheabsolutedifferenceof\ntheelementsinthelearnedtimevectoristhelargestasthestartand\nFigure2:Examplesoflearnedtimerepresentationfeatures\nendofthepulse;and(b)splittingthetimevectorandthenusing\nwithalearnabletriangularpulse.Thefiveillustratedtime\nthehourswheretheelementsthemselvesinthelearnedtimevector\nfeaturescomefromthesametestingsampleandhighlight\nhavetheabsolutelargestvaluesasthestartandendofthepulse.\nthecapabilityofthemodeltolearn(non-)isoscelespulses\nHowever, both implementations failed to learn any meaningful\nwithdifferentbases.\nformofpulseandremainedfixedontheinitialtimeparameters.\nTheapproachthatsuccessfullyachievedthetaskoflearning\ndifferenttriangularpulseswasanengineeringone.Itemergedfrom\n3.2 Sinusoidal theideathatthenon-linearfunctionitselfshouldbesimpleinterms\nofnumericalcomputationandtraceability,sincetheprevious,more\nThethirdembeddingfollowstheabsolutepositionalencodinglit-\ncomplex approaches were not suitable for the task. The idea is\neratureandconsistsoftwosine/cosinepairs,onepaircomputed\ndescribedasfollows:wefirstcalculatetheabsolutedifferenceof\nfromthemonthoftheyearandonepaircomputedfromthehour\neachvalueinthetimevectorofeachrepresentationofsize𝑑and\noftheday.Sineandcosinecapturecyclicalpatternseffectively,and\nthevalueofthe13thelementofthisrepresentation(corresponding\nbycombiningthehourandthemonthembeddingweguarantee\nto1PM).Thenwecomputethe25thpercentileofthesedifferences\nthattheTransformerhasthemeanstomodeldailyandannual\nperiodicitiesandalsotorefertoabsolutetimepointsifthedata\nandreplaceeachdifferencedist𝑖𝑗 withzeroifitislessthanthe\nprovethisuseful.\npercentilevalue𝑣\n𝑖\nofthecorrespondingrepresentation𝑖,andwith\nTo find the appropriate phase shift and period, we argue as 1−\ndi 𝑣s 𝑖t𝑖𝑗\notherwise.Figure2presentsafewexampletriangular\nfollows: To represent the daily cycle we need our sine wave to pulsetimerepresentationslearnedfromemployingthisapproach.\npeakatnoon.Weaimfornoontobedistinct,whiletheremaining\ntimestampsshouldexhibitsymmetricalcorrelation.Toachievethis,\n4 EXPERIMENTSANDRESULTS\nobservehowaperiodof12forhoursand24formonths,witha\nzeroshift,willworkasexpected.Naturally,thecosinemustbe 4.1 Experimentalsetup\nparameterized with identical phase and period to maintain the\nOurdatawascollectedfromapilotbuildingatNCSR‘Demokri-\npropertythatthesine/cosinepairuniquelyreferstoapointinthe\ntos’.Thepilotbuildingfeaturessolarthermalcollectorsusedto\ntrigonometriccircle.\nexperimentwithhowtomostefficientlycontrolheatingsystems\n(solar,heatpump,etc.)toachievesatisfactoryspaceheating,sat-\n3.3 Sine&Sawtooth\nisfyhotwaterdemand,andminimizeelectricpowerconsumption.\nThefourthandfinalembeddingconsistsofasineandasawtooth Themachinelearningtaskassociatedwiththisapplicationisto\nfunction.Justlikeinthesinusoidalembedding,wealsousethesine predictsolarpowerproductionfromvariablesreflectingexternal\nwavewiththesameparameters(shift,period)toexpresscorrelation conditions,namelysolarradiationandexternaltemperature.The\namongtimestamps.However,whenitcomestoexpressingunique- outputofthemodelisnotspecificpowerintermsofkWh,buta\nness, we considered using a sawtooth wave instead of a cosine labelthatcharacterizespowerproductionasbeinginoneoffive\nwave,toobservewhetheranynoticeablechangesmighttakeplace. classes,definedbasedonexpecteddemand.\nUnlikethelinearfunctionusedinthetriangularpulseembeddings Basedontheabove,themachinelearningtaskistotransforma\nwhichspanstheentirerangeoftimestamps,thesawtoothwave multi-variatetimeseriesofexternalconditionsaggregatedintoone-\nresetsto0outputaftereachday.Thesawtoothwaveparameters hourintervals,intoatimeseriesofpowerproductionlevels(thefive-\n(shift,period)aresetto(6,6)forhoursand(12,12)formonthsto classlabelingschemamentionedabove).Thedatasethasamoderate\nscaletheoutputvaluesforbothcaseswithin[-1,1]. amountofgapsduetothesensorsoccasionallygivingerroneous,\nSETN2024,September11–13,2024,Piraeus,Greece NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramveliotakis,andGeorgeKosmadakis\nTable1:Evaluationresultsfortheclassificationtaskusingpriorandlearnedtimerepresentations.Eachmodelwastrained\nusingsixdifferentinitializationseeds,andthefinalmetricsarecomposedfromtheaggregation(meanandstandarddeviation)\noftheresultsfromthesetrainedmodels.\nPriorTimeRepresentations LearnedTimeRepresentations\nMetric tr.pulse&linear† fixedtr.pulse&linear† sine&cosine‡ sine&sawtooth‡ sine&linear† tr.pulse&linear†\nPrecision(Micro) 0.835±0.017 0.823±0.013 0.866±0.010 0.860±0.004 0.842±0.009 0.778±0.010\nRecall(Micro) 0.835±0.017 0.823±0.013 0.866±0.010 0.860±0.004 0.842±0.009 0.778±0.010\nF-score(Micro) 0.835±0.017 0.823±0.013 0.866±0.010 0.860±0.004 0.842±0.009 0.778±0.010\nPrecision(Macro) 0.650±0.027 0.629±0.012 0.679±0.025 0.666±0.009 0.647±0.012 0.582±0.015\nRecall(Macro) 0.715±0.033 0.673±0.015 0.722±0.024 0.721±0.026 0.694±0.023 0.621±0.016\nF-score(Macro) 0.663±0.030 0.632±0.013 0.689±0.019 0.676±0.016 0.646±0.019 0.571±0.013\nPrecision(Weighted) 0.884±0.010 0.876±0.006 0.894±0.009 0.895±0.004 0.892±0.005 0.867±0.007\nRecall(Weighted) 0.835±0.017 0.823±0.013 0.866±0.010 0.860±0.004 0.842±0.009 0.778±0.010\nF-score(Weighted) 0.853±0.013 0.842±0.008 0.876±0.009 0.872±0.004 0.860±0.007 0.812±0.009\n†:inputbeingsec/min/hour/day/month/year ‡:inputbeingmonth/hour\nlossfromtrainingthesemodels.Thecompleteexperimentalsetup\nisalsopublished.3\n4.2 Classificationresultsanddiscussion\nToevaluatetheperformanceofourmodelsontheclassification\ntask,wecomparedboththePriorTimeandmTAN modelsacross\ndifferent time representations. Our evaluation criteria included\nvariousperformancemetricssuchasprecision,recall,andF1-score,\nasshowninTable1.\nThePriorTimemodelwastrainedandtestedusingfourdiffer-\nenttimerepresentations:triangularpulse&linear,fixedtriangular\npulse&linear,sine&cosine,andsine&sawtooth.Amongtheserep-\nresentations,thesine&cosineapproachconsistentlydemonstrated\nthestrongestperformance.Thesine&sawtoothrepresentation\napproachedtheresultsachievedbysine&cosine;however,itonly\noutperformedonce,andthedifferencewasnotsignificant.Concern-\ningthetwotriangularpulse/linearrepresentations,theyexhibited\nFigure 3: Normalized training loss progression for each\ntheweakestperformanceoverall,withthefixedvariationperform-\nmodel.Themodelspresentedarethebest-performingmod-\ningslightlyworsethantheseason-modulatedone.Thiswasnot\nelsacrossdifferenttimerepresentations(priorandlearned).\nsurprising,asweexpectedthemoreinformedtimerepresentation\ntogivebetterresults.\nThesecondpointisthatthesinerepresentationsconsistently\nout-of-rangereadingswhichareremoved.Thedatasetexhibitsthe outperformthetriangularonesinbothsettings(priorandlearned),\nobviousdailyperiodicity,andalsoexhibitsanon-periodictrend. despitethefactthatthetriangularpulseismoreprecisewhenit\nThenon-periodictrendisinrealityseasonalperiodicity,butsince comestomakingsimilartimeshavesimilarinputs(solarradiation\nwehaveusedthedatafrom10monthsitappearstothemachine andtemperature),asalreadydiscussedinthebeginningofSection3.\nlearningtaskasanon-periodictrend.2 Webelievethatthisisduetothefactthattheidenticalclose-to-zero\nToapplythepriortimerepresentationweimplementedaTrans- valuesoutsidethebaseofthepulsehinderback-propagatinglossto\nformerwithanencoder,adecoder,andaclassifier,inthatorder. thelinearlayerthatfeedsthesine/triangularactivationfunction.\nWerefertothisarchitectureasPriorTimeintheresultspresented Whenthesetwopointsareputtogether,theyimplythat(a)the\nbelow.Theencodermapstheinputintoalatentrepresentation,the firstlinearlayeriscapableofmanagingbothseasonalvariation\ndecoderreconstructsitpreservingtheoriginaldimensionalityof andthedailyperiodicitybyproperlyweightingtheinputsofthe\nitsfeatures,andtheclassifier(asinglelinearlayer)outputsaprob- activationfunctions;(b)(almost)zeroing-outinputshindersback-\nabilitydistributionoverthefivelabels.Toapplythelearnedtime propagation;and(c)havinganactivationfunctionthatdynamically\nrepresentation,weusedthemTAN architecture,wherethedecoder changestomodeltheuniquecharacteristicsofeachdayindividually\nisthesamesingle-layerlinearclassifier.Figure3givethetraining enhancesconvergence.\n3Seeguideundertime_representations/installationattheAINST-2024branch\nofrepositoryhttps://github.com/data-eng/navgreenandalsodirectlyaccessibleat\n2Availableathttps://zenodo.org/records/12818885 https://github.com/data-eng/navgreen/releases/tag/AINST-2024\nComparingPriorandLearnedTimeRepresentationsinTransformerModelsofTimeseries SETN2024,September11–13,2024,Piraeus,Greece\n(a)Initialfeaturerepresentation (b)Finalfeaturerepresentation\nFigure4:Exampleofthelearningprogressofasinetimefeature.Bothfiguresillustratethetimerepresentationofthetest\nexamplesbyaggregatingthempertime-featureusingthemeanandstandarddeviation.Theleftfigureshowsthefeaturebefore\nthelearningprocessstarts,whiletherightoneshowsthefinallearnedfeature.Thelearnedtimerepresentationismeaningful\nsinceitdistinguishesmiddayandhassimilarvaluesforhourswithsimilaraccumulateddaylight.\n4.3 DiscussionoftheLearnedTime Tomodelthetimepointsforeachexample,wenormalizedeach\nRepresentations timestampusingtheUnixtimestampsfromourdata’sdaterange.\nThe resulting time points range from 0 to 1, but within a daily\nAsnotedinSection2,weareboundbythemodel’sarchitecture\ntolearn𝑛−1periodicand1non-periodicfeaturerepresentations sample,thehourlytimepointsvaryslightly.TheTransformer’s\nfromourdata,with𝑛beingthenumberofthefinaloutputpoints architectureutilizesthekey/value/queryformulation[6].Thequery\nistheinformationthatisbeinglookedfor,thekeyisthecontext\nofthetimeseries(inourcase,24).\norreference,andthevalueisthecontentthatisbeingsearched.In\nWhensubjectingtheperiodictimerepresentationtobeatrian-\nthecontextofmTAN,theinitialkeysarethetimepointsandthe\ngularpulse,themodelstartsfromverynarrowpulsesandthen\ninitialqueriesarevectorswithvaluesequallydistributedfrom0to\nproceedstolearnacombinationofthose,aspresentedpreviously\n1.Bothofthesetermsarethenpassedthroughthesamelearnable\ninFigure2.Ontheotherhand,whenwedefinetheperiodicity\ntimelayers.Since,initially,thekeysandthequerieshaveadifferent\nasasinefunction,theinitialweightinitialization(whichsetsthe\norderofmagnitudeitisexpectedthateitherthemodelwillincrease\nweightstoverysmallvalues)makestheinitialrepresentationsa\nthemagnitudeofthekeysordecreasethemagnitudeofthequeries.\nveryshortpartofthesinusoidalcurve(soshortthatvisuallyap-\nThroughourexperimentswediscoveredthattheformeroccurs.\npearsaslinearinourgraphs).Themodelthenlearnstoscalethe\nAspresentedinFigure7thesmallerkeyvaluescorrespondtoa\ntimevectorstomodelmoremeaningfultimerepresentations.An\nsmallerpartofthesinewave,whereasthelargerqueryvaluescover\nexampleofthisbehaviorispresentedinFigure4.\nalargerpartofthesinewave(Figure8).\nHowever,onemightwonder:areallofthe23learnedtimerep-\nTofurtherevaluateourclaim,wemodifiedthetimepointcre-\nresentationsmeaningful?Sinceitishardtotracktheinfluenceof\nationsothatbothqueriesandkeyshavethesamevalues.Todo\neveryrepresentationontheclassificationtask,wecanonlyspec-\nso,weassignedeachtimepointreferringtoanhourinadayits\nulate.InFigure5aredemonstratedalltheperiodicsinefeatures,\nnormalizedindexbydividingitwiththetotalnumberofhoursina\ngroupedbytheirsimilarity.Mosttimefeaturesfallintothefirst\nday.Asexpected,thelearnedrepresentationthenwasthesamefor\nthreegroups.Thelastthreegroups,whichonlyincludeonerep-\nbothkeysandqueries,asillustratedinFigure9.Comparingthem\nresentationeach,seemtobethemostexplainableregardingour\ntothepreviouslylearnedrepresentationsinFigures7and8,the\nknowledgeofthedomain.Theycapturelargerperiodsthatleadto\nevaluationofthekeyshasindeedscaledup,whiletheevaluation\nthenumericalcorrelationofdifferenthoursinaday.Webelieve\nofthequerieshasnotchangeddrastically.\nthatthesearethemostimportantfeaturesonwhichthemodel’s\nThemainreasonweoptedfornormalizingthetimestepsusing\npredictionreliesandalsothatwhenitstartedtolearnthesefeatures,\nUNIXtimewasthatitslightlyperformedbetterfortheclassification\nitneglectedtheothersandassignedsmallerweightstothem.To\ntask.Webelievethatthisisthecasebecausewhennormalizingthe\nsubstantiatethisassertionweillustrateinFigure6thecorrespond-\ntimepointsconsideringonlythehour,weloseanyothercharacteris-\ninggroupsatthestartofthelearningprocess.Itcanbeclaimed\nticthatrelatestotheprogressionoftime.Thisfurthercorroborates\nthatthefirstthreegroupsexhibitminimalchange,whereasthelast\nthat(asweextensivelystressedearlier)thenon-periodicalfeatures\nthreechangevisibly.\narecrucialforourapplication.\nSETN2024,September11–13,2024,Piraeus,Greece NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramveliotakis,andGeorgeKosmadakis\n(a)Group1(7timerepresentations) (b)Group2(9timerepresentations) (c)Group3(4timerepresentations)\n(d)Group4(1timerepresentation) (e)Group5(1timerepresentation) (f)Group6(1timerepresentation)\nFigure5:Learnedsinetimerepresentationsgroupedbytheirsimilarity.Eachfigureiscomprisedoftheaggregatedrepresen-\ntationscomputedonthetestingdata(meanandstandarddeviationofeachtime-feature).Thetopthreefiguresgatherthe\nmajorityofthefeatures,whereasthebottomthreeincludeonlyonefeatureeach.\n(a)Group1 (b)Group2 (c)Group3\n(d)Group4 (e)Group5 (f)Group6\nFigure6:Initialvaluesofthegroupsofthesinetimerepresentations.\nComparingPriorandLearnedTimeRepresentationsinTransformerModelsofTimeseries SETN2024,September11–13,2024,Piraeus,Greece\n(a)Group1 (b)Group2 (c)Group3\n(d)Group4 (e)Group5 (f)Group6\nFigure7:Learnedsinetimerepresentationsonthekeysofthetestdata.Theillustratedrepresentationsoccurredwhenthekeys\nandquerieshaddifferentordersofmagnitude.\n(a)Group1 (b)Group2 (c)Group3\n(d)Group4 (e)Group5 (f)Group6\nFigure8:Learnedsinetimerepresentationsonthequeriesofthetestdata.Theillustratedrepresentationsoccurredwhenthe\nkeysandquerieshaddifferentordersofmagnitude.\nSETN2024,September11–13,2024,Piraeus,Greece NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramveliotakis,andGeorgeKosmadakis\n(a)Group1 (b)Group2 (c)Group3\n(d)Group4 (e)Group5 (f)Group6\nFigure9:Learnedsinetimerepresentationsonthekeysandqueriesofthetestdata.Theillustratedrepresentationsoccurred\nwhenthekeysandquerieshadthesameorderofmagnitude.\n5 CONCLUSIONS methodologiesforexplainingneuralnetworks,thisalsoshowedus\nWestudiedtheimpactoftimeembeddingswhenapplyingTrans- apathforfutureworkspecificallytargetingmTAN.Sinceoneof\nformermodelstotimeseriesanalysis,andspecificallytheimpactof thehindranceswasthatwewereunabletoobservetheeffectof\nusingaveryinformed,application-specifictimerepresentation,a eachfeatureduethedistributednatureoftheclassifier,wewould\ngenericsinusoidalrepresentationknowntocaptureperiodicphe- havelikedtosystematicallyexploretheeffectofincrementallyin-\nnomenawell,andamethodforlearningtheparametersthatbest creasingthenumberoffeaturesandobservingwhichfeaturesgets\nfiteitherofthesetwotothespecificdatasetathand. learned(interpretedas,isthemostimpactfulforreducingloss),\nOurexperimentsconcludedthatTransformers(andDNNsin whichislearnedsecondandsoon.However,thelinearalgebra\ngeneralwecouldargue)arenotveryamenabletoover-engineering behindmTAN tiesthedimensionalityofthefeaturerepresenta-\nthetimerepresentationduetoside-effectsthataredifficulttomiti- tiontothedimensionalityoftheinput,aswecannotde-couplethe\ngate.Thiscanbeseenasanegativequalityinapplicationssuchas encodingofperiodicityandtheencodingoflineartime.Since(as\nourswherethereisextensivepriorknowledgeonawell-studied arguedabove)lineartimeiscriticalforperformance,wearestuck\nphenomenon,butcanalsobeconstruedasapositivequalityasthe withapre-defineddimensionalityforthesinefeaturesaswell.\nnetworkwasableto‘discover’theknowledgeweweretryingto Ourenvisagedfutureresearchistore-workthelinearalgebraof\nconvey.Especiallythecomparisonbetweenthesinusoidalprior themTAN sothatwecande-coupletheencodingoftheperiodic\nandlearnedrepresentationsisverypromisinginthisrespect,at andlineartimerepresentation,allowingtohaveonlythelatter\nthemaximally-flexiblemTAN networkrecoveredalmostallthe beconstrainedbythedimensionalityoftheinput.Inthespecific\naccuracyoftheprior-timesinusoidalnetwork. experimentpresentedhere,thiswouldtranslatetoabetterunder-\nAsecondlevelofanalysisdelvedintothenatureofthefeatures standingofwhathappenswhenweinformtheTransformerofthe\nthatmTAN learned.Thisanalysishasdemonstratedtheabilityof biaswewanttoapplyregardingperiodicities.Which,initsturn,\nthelinearlayer-sinusoidalactivationfunctionarchitecturetovery isexpectedtoleadtomethodsforaffordingthehumanoperator\ncloselyapproximatetheclearlynon-smoothbehaviordiscussedin intuitiveandeffectivecontrolofthenetwork.\nSection3andshowninFigure1.\nItshould,however,benotedthatouranalysiswasrestricted\nto what we could indirectly observe by trying out different pa-\nrameterizationsandspeculatingbasedonourunderstandingof\nhowthenetworkistrained.Besidesanygeneraladvancementsin\nComparingPriorandLearnedTimeRepresentationsinTransformerModelsofTimeseries SETN2024,September11–13,2024,Piraeus,Greece\nACKNOWLEDGMENTS REFERENCES\nThisresearchhasbeenco-financedbytheEuropeanUnionand [1] ZhengpingChe,SanjayPurushotham,KyunghyunCho,DavidSontag,andYan\nLiu.2018.RecurrentNeuralNetworksforMultivariateTimeSerieswithMissing\nGreeknationalfundsthroughtheprogram‘Flagshipactionsinin-\nValues.ScientificReports8(042018).Issue1.\nterdisciplinaryscientificareaswithaspecialinterestintheconnec- [2] DanielNeil,MichaelPfeiffer,andShih-ChiiLiu.2016.PhasedLSTM:Accelerating\ntionwiththeproductionnetwork’—GREENSHIPPING—TAEDR- RecurrentNetworkTrainingforLongorEvent-basedSequences.InAdvancesIn\nNeuralInformationProcessingSystems.3882–3890.\n0534767(Acronym:NAVGREEN).Formoreinformationpleasevisit [3] TrangPham,TruyenTran,DinhPhung,andSvethaVenkatesh.2017.Predicting\nhttps://navgreen.gr healthcaretrajectoriesfrommedicalrecords:Adeeplearningapproach.Journal\nofBiomedicalInformatics69(2017),218–229.\nThisresearchwasco-fundedbytheEuropeanUnionunderGA\n[4] MonaSchirmer,MazinEltayeb,StefanLessmann,andMajaRudolph.2022.Model-\nno.101135782(MANOLOproject).Viewsandopinionsexpressed ingIrregularTimeSerieswithContinuousRecurrentUnits.InProceedingsofthe\narehoweverthoseoftheauthorsonlyanddonotnecessarilyreflect 39thInternationalConferenceonMachineLearning,17-23Jul2022,Vol.162.\n[5] SatyaNarayanShuklaandBenjaminMarlin.2021. Multi-TimeAttentionNet-\nthose of the European Union or CNECT. Neither the European\nworksforIrregularlySampledTimeSeries.InInternationalConferenceonLearning\nUnionnorCNECTcanbeheldresponsibleforthem. Representations.\nAWSresourceswereprovidedbytheNationalInfrastructuresfor [6] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN\nGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. AttentionisAllyouNeed.\nResearchandTechnologyGRNETandfundedbytheEURecovery InAdvancesinNeuralInformationProcessingSystems,I.Guyon,U.VonLuxburg,\nandResiliencyFacility. S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(Eds.),Vol.30.\nCurranAssociates,Inc.\n[7] QingsongWen,TianZhou,ChaoliZhang,WeiqiChen,ZiqingMa,JunchiYan,\nandLiangSun.2023.TransformersinTimeSeries:ASurvey.InProceedingsofthe\nThirty-SecondInternationalJointConferenceonArtificialIntelligence,SurveyTrack.\nMacao,19–25August2023. https://doi.org/10.24963/ijcai.2023/759",
    "pdf_filename": "Comparing_Prior_and_Learned_Time_Representations_in_Transformer_Models_of_Timeseries.pdf"
}