{
    "title": "Comparing Prior and Learned Time Representations in",
    "abstract": "Whatsetstimeseriesanalysisapartfromothermachinelearning arerelevantfortheapplicationathand. exercisesisthattimerepresentationbecomesaprimaryaspectof Toelaborateonthevariousconsiderationsthatneedtobead- theexperimentsetup,asitmustadequatelyrepresentthetemporal dressed,firstconsiderthatonecannotassumefullyobserved,uni- relationsthatarerelevantfortheapplicationathand.Inthework formlysampledinputsastheremightbegapsinthedata,vary- describedherewestudywodifferentvariationsoftheTransformer ingsamplingrates,and(formultivariatetimeseries)misalignment architecture:onewhereweusethefixedtimerepresentationpro- betweenthetimestepsofthedifferentvariables.Thisdictatesa posedintheliteratureandonewherethetimerepresentationis representationthatallowstimedifferencestobecomputed,sothat learnedfromthedata.Ourexperimentsusedatafrompredicting (forexample)September2023is‚Äòcloser‚ÄôtoJanuary2024thanitisto theenergyoutputofsolarpanels,ataskthatexhibitsknownperi- September2022.Simpletimestampsallowthisbutdonotcapture odicities(dailyandseasonal)thatisstraight-forwardtoencodein periodicity:Consider,forinstance,anapplicationwithseasonal thefixedtimerepresentation.Ourresultsindicatethateveninan periodicitywhereSeptember2023is‚Äòcloser‚ÄôtoSeptember2022 experimentwherethephenomenoniswell-understood,itisdiffi- thantoJanuary2024. culttoencodepriorknowledgeduetoside-effectsthataredifficult Thereisarichrelevantliteratureinbothsignalprocessingandin tomitigate.Weconcludethatresearchworkisneededtoworkthe non-parametricstatistics,aswellasinadaptingAI/MLapproaches humanintothelearningloopinwaysthatimprovetherobustness totimeseriesprocessingwhenfacingirregularlysampledand/or andtrust-worthinessofthenetwork. sparsedata.Inparticular,deeplearningapproachesthatutilizere- currentnetworksbasedonGatedRecurrentUnits(GRUs)[1],Long CCSCONCEPTS Short-TermMemorynetworks(LSTMs)[2,3],andODE-RNNs[4] haveshownpromisingresults.Intheworkdescribedwefocuson ‚Ä¢Computingmethodologies‚ÜíNeuralnetworks;Artificial deeplearningmethodsaswell,butspecificallyonadaptingTrans- intelligence;Supervisedlearning;‚Ä¢Mathematicsofcomputing former modelstotimeseriesanalysis.Wewillfirstpresenthow ‚ÜíTimeseriesanalysis. therelevantliteraturehandlestherepresentationoftimewhen ACMReferenceFormat: applyingTransformermodelstotimeseries(Section2)andthen NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramve- proceedtoproposeanalternativerepresentationthatisexpectedto liotakis,andGeorgeKosmadakis.2024.ComparingPriorandLearnedTime out-performtheoriginalrepresentationforourspecificapplication RepresentationsinTransformerModelsofTimeseries.In13thConference onpredictingtheenergyoutputofsolarpanels(Section3).We onArtificialIntelligence(SETN2024),September11‚Äì13,2024,Piraeus,Greece. closewithgivinganddiscussingcomparativeexperimentalresults ACM,NewYork,NY,USA,9pages.https://doi.org/10.1145/3688671.3688747 (Section4)andconclusionsandfuturework(Section5). 1 INTRODUCTION 2 BACKGROUND Whatsetsaparttimeseriesanalysisfromothermachinelearning Unlikerecurrentanddifferentialequation-basedarchitectureswhich exercisesistakingintoaccountthesequenceaswellas,inmost processinputssequentially,Transformers[6]expectthecomplete cases,thetemporaldistancebetweenobservations.Thismakesthe time-seriesasinputandusetheattentionmechanismtolookfor representationoftimeaprimaryaspectoftheexperimentsetup, relationshipsbetweenallinputssimultaneously.Thishastheside- ‚àóBothauthorscontributedequallytothisresearch. effectthatthetemporalorderisnolongerimpliedbytheorderin whichtheinputsarepresentedtothenetwork,sothatinputvec- Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor torsmustbeaugmentedwithfeaturesthatrepresenttime.Butthis classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed alsocreatestheopportunitytousetimeembeddingsthatrepresent forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation temporalinformationinawaythatencodespriorknowledgeabout onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored. Forallotheruses,contacttheowner/author(s). thedata. SETN2024,September11‚Äì13,2024,Piraeus,Greece Themostcharacteristicexampleisperiodicity.Whenthedata ¬©2024Copyrightheldbytheowner/author(s). isknownorsuspectedtoexhibitperiodicity,absolutepositional ACMISBN979-8-4007-0982-1/24/09 https://doi.org/10.1145/3688671.3688747 encoding[7]encodestimeastwofeatures:thesineandthecosine 4202 voN 91 ]GL.sc[ 1v67421.1142:viXra",
    "body": "Comparing Prior and Learned Time Representations in\nTransformer Models of Timeseries\nNataliaKoliou‚àó GeorgeMeramveliotakis\nTatianaBoura‚àó GeorgeKosmadakis\nStasinosKonstantopoulos {gmera,gkosmad}@ipta.demokritos.gr\n{nataliakoliou,tatianabou,konstant}@iit.demokritos.gr InstituteofNuclear&RadiologicalSciencesand\nInstituteofInformaticsandTelecommunications, Technology,Energy&Safety,\nNCSR‚ÄòDemokritos‚Äô NCSR‚ÄòDemokritos‚Äô\nAg.Paraskevi,Greece Ag.Paraskevi,Greece\nABSTRACT asitmustbeadequateforrepresentingthetemporalrelationsthat\nWhatsetstimeseriesanalysisapartfromothermachinelearning arerelevantfortheapplicationathand.\nexercisesisthattimerepresentationbecomesaprimaryaspectof Toelaborateonthevariousconsiderationsthatneedtobead-\ntheexperimentsetup,asitmustadequatelyrepresentthetemporal dressed,firstconsiderthatonecannotassumefullyobserved,uni-\nrelationsthatarerelevantfortheapplicationathand.Inthework formlysampledinputsastheremightbegapsinthedata,vary-\ndescribedherewestudywodifferentvariationsoftheTransformer ingsamplingrates,and(formultivariatetimeseries)misalignment\narchitecture:onewhereweusethefixedtimerepresentationpro- betweenthetimestepsofthedifferentvariables.Thisdictatesa\nposedintheliteratureandonewherethetimerepresentationis representationthatallowstimedifferencestobecomputed,sothat\nlearnedfromthedata.Ourexperimentsusedatafrompredicting (forexample)September2023is‚Äòcloser‚ÄôtoJanuary2024thanitisto\ntheenergyoutputofsolarpanels,ataskthatexhibitsknownperi- September2022.Simpletimestampsallowthisbutdonotcapture\nodicities(dailyandseasonal)thatisstraight-forwardtoencodein periodicity:Consider,forinstance,anapplicationwithseasonal\nthefixedtimerepresentation.Ourresultsindicatethateveninan periodicitywhereSeptember2023is‚Äòcloser‚ÄôtoSeptember2022\nexperimentwherethephenomenoniswell-understood,itisdiffi- thantoJanuary2024.\nculttoencodepriorknowledgeduetoside-effectsthataredifficult Thereisarichrelevantliteratureinbothsignalprocessingandin\ntomitigate.Weconcludethatresearchworkisneededtoworkthe non-parametricstatistics,aswellasinadaptingAI/MLapproaches\nhumanintothelearningloopinwaysthatimprovetherobustness totimeseriesprocessingwhenfacingirregularlysampledand/or\nandtrust-worthinessofthenetwork. sparsedata.Inparticular,deeplearningapproachesthatutilizere-\ncurrentnetworksbasedonGatedRecurrentUnits(GRUs)[1],Long\nCCSCONCEPTS Short-TermMemorynetworks(LSTMs)[2,3],andODE-RNNs[4]\nhaveshownpromisingresults.Intheworkdescribedwefocuson\n‚Ä¢Computingmethodologies‚ÜíNeuralnetworks;Artificial\ndeeplearningmethodsaswell,butspecificallyonadaptingTrans-\nintelligence;Supervisedlearning;‚Ä¢Mathematicsofcomputing\nformer modelstotimeseriesanalysis.Wewillfirstpresenthow\n‚ÜíTimeseriesanalysis.\ntherelevantliteraturehandlestherepresentationoftimewhen\nACMReferenceFormat: applyingTransformermodelstotimeseries(Section2)andthen\nNataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramve- proceedtoproposeanalternativerepresentationthatisexpectedto\nliotakis,andGeorgeKosmadakis.2024.ComparingPriorandLearnedTime out-performtheoriginalrepresentationforourspecificapplication\nRepresentationsinTransformerModelsofTimeseries.In13thConference\nonpredictingtheenergyoutputofsolarpanels(Section3).We\nonArtificialIntelligence(SETN2024),September11‚Äì13,2024,Piraeus,Greece.\nclosewithgivinganddiscussingcomparativeexperimentalresults\nACM,NewYork,NY,USA,9pages.https://doi.org/10.1145/3688671.3688747\n(Section4)andconclusionsandfuturework(Section5).\n1 INTRODUCTION\n2 BACKGROUND\nWhatsetsaparttimeseriesanalysisfromothermachinelearning\nUnlikerecurrentanddifferentialequation-basedarchitectureswhich\nexercisesistakingintoaccountthesequenceaswellas,inmost\nprocessinputssequentially,Transformers[6]expectthecomplete\ncases,thetemporaldistancebetweenobservations.Thismakesthe\ntime-seriesasinputandusetheattentionmechanismtolookfor\nrepresentationoftimeaprimaryaspectoftheexperimentsetup,\nrelationshipsbetweenallinputssimultaneously.Thishastheside-\n‚àóBothauthorscontributedequallytothisresearch. effectthatthetemporalorderisnolongerimpliedbytheorderin\nwhichtheinputsarepresentedtothenetwork,sothatinputvec-\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor torsmustbeaugmentedwithfeaturesthatrepresenttime.Butthis\nclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed alsocreatestheopportunitytousetimeembeddingsthatrepresent\nforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation\ntemporalinformationinawaythatencodespriorknowledgeabout\nonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.\nForallotheruses,contacttheowner/author(s). thedata.\nSETN2024,September11‚Äì13,2024,Piraeus,Greece Themostcharacteristicexampleisperiodicity.Whenthedata\n¬©2024Copyrightheldbytheowner/author(s).\nisknownorsuspectedtoexhibitperiodicity,absolutepositional\nACMISBN979-8-4007-0982-1/24/09\nhttps://doi.org/10.1145/3688671.3688747 encoding[7]encodestimeastwofeatures:thesineandthecosine\n4202\nvoN\n91\n]GL.sc[\n1v67421.1142:viXra\nSETN2024,September11‚Äì13,2024,Piraeus,Greece NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramveliotakis,andGeorgeKosmadakis\noftherawtimestamp.ThisrepresentationallowstheTransformer\ntorefertobothabsolutetimes(sincethecombinationofsineand\ncosinemapstoasinglepositiononthetrigonometriccircle)and\ntoarepresentationwhereall(forinstance)peakpositionshave\nthesamefeaturevalue.Referringbacktoourpreviousexample,\nrememberhowwewantSeptember2022andSeptember2023to\nbeonasimilaroridenticaltimefromtheperiodicperspective,but\nnotfromthelinear-timeperspective.TheTransformerwillthen\nhavetheflexibilitytoweighthesefeaturesinaccordancewiththe\nphenomenonbeingmodeled.\nThemeaningfulandsuccessfulusageofpre-computedtimefea-\nturesrequiresexpertknowledgeoftheapplicationdomainandthe\nperiodicitiesthatmakesenseforthephenomenonbeingmodeled.\nInotherwords,itaddsupontheinductivebiasofthealgorithm.\nAsexpected,alineofresearchemergedthataimstoautomati-\ncallyacquirethetimerepresentationfromthedata.Themulti-time\nattentionnetwork(mTAN)[5]isaTransformermodelenhanced\nwithatime-attentionmodule.Thismodulelearnstemporalpatterns Figure1:Hourlymeanpyranometervalues(red)andtheir\nbyleveragingatrainableembeddingfunctionùúô(ùë°)thatmapstime\nfunctionapproximations,triangularpulse(blue)andsinu-\nùë° (scaledto0..1)toavectoroflengthùëë.\nsoidalfunction(green).\nThisvectorhasonelinearelementthatcapturesthelinearpro-\ngressionoftimeandùëë‚àí1sinusoidalelementsthatcapturedifferent\nthatatriangularspikewillbeabetterrepresentationthanasine.\nperiodicitiesasfrequency/phasepairs.Formally,theembedding\nThisisfurthercorroboratedbythedatashowninFigure1,showing\nfunctionis:\npyranometer(solarradiation)valuesaggregatedto1hintervalsand\n(cid:40)\nùúô ùëñ(ùë°)=\nùúî 0ùë°+ùõº 0, ifùëñ =0,\n(1) averagedthroughtheyear,andthebestapproximationthatcanbe\nsin(ùúî ùëñùë°+ùõº ùëñ), if0<ùëñ <ùëë achievedbythesinefunctionandbythetriangularspike.\nBasedontheargumentabove,wehavedefinedthefouralterna-\nwhereùúî ùëñ andùõº ùëñ arethelearnableparameters. tivetimeembeddingspresentedbelow.\nInconcreteneuralnetworkterms,thisisimplementedasone\nlinearlayerforùúî 0,ùõº 0andonelinearlayerwithasineactivation 3.1 TriangularPulse&Linear\nfunctionforallotherùúî ùëñ,ùõº ùëñ.Moreprecisely,itisimplementedwith Thefirstembeddingisaseason-modulatedtriangularpulse/linear\nmultiplesuchstructures,oneforeachattentionhead.Itshouldbe pair.Thebaseandpeakofthetriangularpulseisnotthesamefor\nnotedatthispointthatforthematrixmultiplicationstoworkout, eachday,butiscalculatedsothatthepulsewillstartatsunrise,\nthismodelinghastheside-effectthatùëë mustbethesameasthe peakatnoon,andendatsunset.Thisrepresentsthatsolarradia-\ndimensionalityoftheoutputvector. tionatnoonisdistinctfromallothertimesduringday,onehour\nThismodelinggivestheTransformertheflexibilitytosearchfor beforenoonissimilartoonehourafternoon,andsoonuntilsun-\ntheperiod/phasepairsthatbestcapturetheperiodicitiesimplied rise/sunset.Outsidethebaseofthepulse,alltimesarerepresented\nbythedata.Onlyminimalbiasisintroduced,namelythattimeis bythesamevalueof0.01todenotethatthedistinctionbetween\nstructuredasasine/linearpair. themisnotimportant.1\nAsforthelinearfunction,itprovidesastraightforwardmethod\n3 REPRESENTINGTIME forensuringtheuniquenessoftimestampswithintherepresenta-\nInordertoexperimentwithtimerepresentationsandtheeffectthey tion.Bymappingeachtimestamptoauniquevalue,weestablish\nhaveonthequalityofthelearnedmodel,wehaveassumedanappli- cleardistinctionsbetweendifferenttimepoints.Unliketheperiod-\ncationwhereweneedtopredicttheenergyoutputofsolarthermal icityofthetriangularpulse,thelinearfunctionspanstheentire\ncollectorsbasedonexternalconditions.Moredetailedinformation rangeoftimestamps,fromtheearliesttothelatest,usingbothdate\nabouttheapplicationandthedataisprovidedinSection4,andfor andtimecomponents.Thiscontinuousrepresentationdoesnot\nthepurposesofthecurrentdiscussionitsufficestomentionthat resetdailybutmapstheentireperiodlinearly,ensuringaunique\ntheinputvariablesaresolarradiationandexternaltemperature, valueforeachtimestampbasedonitspositionintheoveralltime\nwhichexhibittheobviousdailyandannualperiodicities. span.\nWehavenotedhowever,thatsolarradiationbehavesinaway Thesecondembeddingissimilartotheoneabove,butthepulse\nthatisnotcapturedbysinusoidalfunctions.Whilethesefunctions isfixedtostartat7am,peakat1pm,andendat9pmregardlessof\nrepresentwellthesimilaritybetweenthesametimeonadifferent thedate.Thissimplerembeddingisincludedinordertobeableto\nday,aswellasthesimilaritybetweenearlymorningandlateafter- seeifthemorecomplexapproachabovehasaddedvalueorthere\nnoon,theyfailtorepresentthefactthatthewholeofthenight-time isnosignificantlossinaccuracywhenusingfixedparameters.\nisthesameand(asfarassolarradiationisconcerned)itmakesno 1Thisvalueissetto0.01insteadof0tosatisfyatechnicalrequirementofthe\ndifferenceifthetimeis10pmor24am.Wehave,therefore,thought implementation.\nComparingPriorandLearnedTimeRepresentationsinTransformerModelsofTimeseries SETN2024,September11‚Äì13,2024,Piraeus,Greece\n3.4 LearnedTimeRepresentations\nAsmentionedintheBackground,theideabehindthemTAN isto\nlearnperiodicfeatureswithasinusoidalrepresentation.Wehave\narguedthat,forourapplication,atriangularpulseforrepresenting\nthenon-linearfeaturesmaybeabetterfit.Thus,wealteredthe\nactivationfunctionofthepresentedmodeltoconvertthelinear\nlayertoatriangularpulse.\nOurfirstattemptatcreatingsuchafunctioninvolvedusinga\ntriangularfunctionwithafixedbase.Inthiscase,thestartandend\nofthepulsecorrespondedtothehourswhendifferenceofthesolar\noutputpeaked(7AMand9PMrespectively).Thisapproachwas\nnotfruitful,asthemodelwasnotgiventheopportunitytolearn\ntheperiodicrepresentations.\nNaturally,wethenfocusedonlearningthebaseofthepulse.\nChoosing1PMasthepeakofthepulse,thetwomoststraight-\nforwardapproachesweimplementedwere:(a)splittingthetime\nvectorandthenusingthehourswheretheabsolutedifferenceof\ntheelementsinthelearnedtimevectoristhelargestasthestartand\nFigure2:Examplesoflearnedtimerepresentationfeatures\nendofthepulse;and(b)splittingthetimevectorandthenusing\nwithalearnabletriangularpulse.Thefiveillustratedtime\nthehourswheretheelementsthemselvesinthelearnedtimevector\nfeaturescomefromthesametestingsampleandhighlight\nhavetheabsolutelargestvaluesasthestartandendofthepulse.\nthecapabilityofthemodeltolearn(non-)isoscelespulses\nHowever, both implementations failed to learn any meaningful\nwithdifferentbases.\nformofpulseandremainedfixedontheinitialtimeparameters.\nTheapproachthatsuccessfullyachievedthetaskoflearning\ndifferenttriangularpulseswasanengineeringone.Itemergedfrom\n3.2 Sinusoidal theideathatthenon-linearfunctionitselfshouldbesimpleinterms\nofnumericalcomputationandtraceability,sincetheprevious,more\nThethirdembeddingfollowstheabsolutepositionalencodinglit-\ncomplex approaches were not suitable for the task. The idea is\neratureandconsistsoftwosine/cosinepairs,onepaircomputed\ndescribedasfollows:wefirstcalculatetheabsolutedifferenceof\nfromthemonthoftheyearandonepaircomputedfromthehour\neachvalueinthetimevectorofeachrepresentationofsizeùëëand\noftheday.Sineandcosinecapturecyclicalpatternseffectively,and\nthevalueofthe13thelementofthisrepresentation(corresponding\nbycombiningthehourandthemonthembeddingweguarantee\nto1PM).Thenwecomputethe25thpercentileofthesedifferences\nthattheTransformerhasthemeanstomodeldailyandannual\nperiodicitiesandalsotorefertoabsolutetimepointsifthedata\nandreplaceeachdifferencedistùëñùëó withzeroifitislessthanthe\nprovethisuseful.\npercentilevalueùë£\nùëñ\nofthecorrespondingrepresentationùëñ,andwith\nTo find the appropriate phase shift and period, we argue as 1‚àí\ndi ùë£s ùëñtùëñùëó\notherwise.Figure2presentsafewexampletriangular\nfollows: To represent the daily cycle we need our sine wave to pulsetimerepresentationslearnedfromemployingthisapproach.\npeakatnoon.Weaimfornoontobedistinct,whiletheremaining\ntimestampsshouldexhibitsymmetricalcorrelation.Toachievethis,\n4 EXPERIMENTSANDRESULTS\nobservehowaperiodof12forhoursand24formonths,witha\nzeroshift,willworkasexpected.Naturally,thecosinemustbe 4.1 Experimentalsetup\nparameterized with identical phase and period to maintain the\nOurdatawascollectedfromapilotbuildingatNCSR‚ÄòDemokri-\npropertythatthesine/cosinepairuniquelyreferstoapointinthe\ntos‚Äô.Thepilotbuildingfeaturessolarthermalcollectorsusedto\ntrigonometriccircle.\nexperimentwithhowtomostefficientlycontrolheatingsystems\n(solar,heatpump,etc.)toachievesatisfactoryspaceheating,sat-\n3.3 Sine&Sawtooth\nisfyhotwaterdemand,andminimizeelectricpowerconsumption.\nThefourthandfinalembeddingconsistsofasineandasawtooth Themachinelearningtaskassociatedwiththisapplicationisto\nfunction.Justlikeinthesinusoidalembedding,wealsousethesine predictsolarpowerproductionfromvariablesreflectingexternal\nwavewiththesameparameters(shift,period)toexpresscorrelation conditions,namelysolarradiationandexternaltemperature.The\namongtimestamps.However,whenitcomestoexpressingunique- outputofthemodelisnotspecificpowerintermsofkWh,buta\nness, we considered using a sawtooth wave instead of a cosine labelthatcharacterizespowerproductionasbeinginoneoffive\nwave,toobservewhetheranynoticeablechangesmighttakeplace. classes,definedbasedonexpecteddemand.\nUnlikethelinearfunctionusedinthetriangularpulseembeddings Basedontheabove,themachinelearningtaskistotransforma\nwhichspanstheentirerangeoftimestamps,thesawtoothwave multi-variatetimeseriesofexternalconditionsaggregatedintoone-\nresetsto0outputaftereachday.Thesawtoothwaveparameters hourintervals,intoatimeseriesofpowerproductionlevels(thefive-\n(shift,period)aresetto(6,6)forhoursand(12,12)formonthsto classlabelingschemamentionedabove).Thedatasethasamoderate\nscaletheoutputvaluesforbothcaseswithin[-1,1]. amountofgapsduetothesensorsoccasionallygivingerroneous,\nSETN2024,September11‚Äì13,2024,Piraeus,Greece NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramveliotakis,andGeorgeKosmadakis\nTable1:Evaluationresultsfortheclassificationtaskusingpriorandlearnedtimerepresentations.Eachmodelwastrained\nusingsixdifferentinitializationseeds,andthefinalmetricsarecomposedfromtheaggregation(meanandstandarddeviation)\noftheresultsfromthesetrainedmodels.\nPriorTimeRepresentations LearnedTimeRepresentations\nMetric tr.pulse&linear‚Ä† fixedtr.pulse&linear‚Ä† sine&cosine‚Ä° sine&sawtooth‚Ä° sine&linear‚Ä† tr.pulse&linear‚Ä†\nPrecision(Micro) 0.835¬±0.017 0.823¬±0.013 0.866¬±0.010 0.860¬±0.004 0.842¬±0.009 0.778¬±0.010\nRecall(Micro) 0.835¬±0.017 0.823¬±0.013 0.866¬±0.010 0.860¬±0.004 0.842¬±0.009 0.778¬±0.010\nF-score(Micro) 0.835¬±0.017 0.823¬±0.013 0.866¬±0.010 0.860¬±0.004 0.842¬±0.009 0.778¬±0.010\nPrecision(Macro) 0.650¬±0.027 0.629¬±0.012 0.679¬±0.025 0.666¬±0.009 0.647¬±0.012 0.582¬±0.015\nRecall(Macro) 0.715¬±0.033 0.673¬±0.015 0.722¬±0.024 0.721¬±0.026 0.694¬±0.023 0.621¬±0.016\nF-score(Macro) 0.663¬±0.030 0.632¬±0.013 0.689¬±0.019 0.676¬±0.016 0.646¬±0.019 0.571¬±0.013\nPrecision(Weighted) 0.884¬±0.010 0.876¬±0.006 0.894¬±0.009 0.895¬±0.004 0.892¬±0.005 0.867¬±0.007\nRecall(Weighted) 0.835¬±0.017 0.823¬±0.013 0.866¬±0.010 0.860¬±0.004 0.842¬±0.009 0.778¬±0.010\nF-score(Weighted) 0.853¬±0.013 0.842¬±0.008 0.876¬±0.009 0.872¬±0.004 0.860¬±0.007 0.812¬±0.009\n‚Ä†:inputbeingsec/min/hour/day/month/year ‚Ä°:inputbeingmonth/hour\nlossfromtrainingthesemodels.Thecompleteexperimentalsetup\nisalsopublished.3\n4.2 Classificationresultsanddiscussion\nToevaluatetheperformanceofourmodelsontheclassification\ntask,wecomparedboththePriorTimeandmTAN modelsacross\ndifferent time representations. Our evaluation criteria included\nvariousperformancemetricssuchasprecision,recall,andF1-score,\nasshowninTable1.\nThePriorTimemodelwastrainedandtestedusingfourdiffer-\nenttimerepresentations:triangularpulse&linear,fixedtriangular\npulse&linear,sine&cosine,andsine&sawtooth.Amongtheserep-\nresentations,thesine&cosineapproachconsistentlydemonstrated\nthestrongestperformance.Thesine&sawtoothrepresentation\napproachedtheresultsachievedbysine&cosine;however,itonly\noutperformedonce,andthedifferencewasnotsignificant.Concern-\ningthetwotriangularpulse/linearrepresentations,theyexhibited\nFigure 3: Normalized training loss progression for each\ntheweakestperformanceoverall,withthefixedvariationperform-\nmodel.Themodelspresentedarethebest-performingmod-\ningslightlyworsethantheseason-modulatedone.Thiswasnot\nelsacrossdifferenttimerepresentations(priorandlearned).\nsurprising,asweexpectedthemoreinformedtimerepresentation\ntogivebetterresults.\nThesecondpointisthatthesinerepresentationsconsistently\nout-of-rangereadingswhichareremoved.Thedatasetexhibitsthe outperformthetriangularonesinbothsettings(priorandlearned),\nobviousdailyperiodicity,andalsoexhibitsanon-periodictrend. despitethefactthatthetriangularpulseismoreprecisewhenit\nThenon-periodictrendisinrealityseasonalperiodicity,butsince comestomakingsimilartimeshavesimilarinputs(solarradiation\nwehaveusedthedatafrom10monthsitappearstothemachine andtemperature),asalreadydiscussedinthebeginningofSection3.\nlearningtaskasanon-periodictrend.2 Webelievethatthisisduetothefactthattheidenticalclose-to-zero\nToapplythepriortimerepresentationweimplementedaTrans- valuesoutsidethebaseofthepulsehinderback-propagatinglossto\nformerwithanencoder,adecoder,andaclassifier,inthatorder. thelinearlayerthatfeedsthesine/triangularactivationfunction.\nWerefertothisarchitectureasPriorTimeintheresultspresented Whenthesetwopointsareputtogether,theyimplythat(a)the\nbelow.Theencodermapstheinputintoalatentrepresentation,the firstlinearlayeriscapableofmanagingbothseasonalvariation\ndecoderreconstructsitpreservingtheoriginaldimensionalityof andthedailyperiodicitybyproperlyweightingtheinputsofthe\nitsfeatures,andtheclassifier(asinglelinearlayer)outputsaprob- activationfunctions;(b)(almost)zeroing-outinputshindersback-\nabilitydistributionoverthefivelabels.Toapplythelearnedtime propagation;and(c)havinganactivationfunctionthatdynamically\nrepresentation,weusedthemTAN architecture,wherethedecoder changestomodeltheuniquecharacteristicsofeachdayindividually\nisthesamesingle-layerlinearclassifier.Figure3givethetraining enhancesconvergence.\n3Seeguideundertime_representations/installationattheAINST-2024branch\nofrepositoryhttps://github.com/data-eng/navgreenandalsodirectlyaccessibleat\n2Availableathttps://zenodo.org/records/12818885 https://github.com/data-eng/navgreen/releases/tag/AINST-2024\nComparingPriorandLearnedTimeRepresentationsinTransformerModelsofTimeseries SETN2024,September11‚Äì13,2024,Piraeus,Greece\n(a)Initialfeaturerepresentation (b)Finalfeaturerepresentation\nFigure4:Exampleofthelearningprogressofasinetimefeature.Bothfiguresillustratethetimerepresentationofthetest\nexamplesbyaggregatingthempertime-featureusingthemeanandstandarddeviation.Theleftfigureshowsthefeaturebefore\nthelearningprocessstarts,whiletherightoneshowsthefinallearnedfeature.Thelearnedtimerepresentationismeaningful\nsinceitdistinguishesmiddayandhassimilarvaluesforhourswithsimilaraccumulateddaylight.\n4.3 DiscussionoftheLearnedTime Tomodelthetimepointsforeachexample,wenormalizedeach\nRepresentations timestampusingtheUnixtimestampsfromourdata‚Äôsdaterange.\nThe resulting time points range from 0 to 1, but within a daily\nAsnotedinSection2,weareboundbythemodel‚Äôsarchitecture\ntolearnùëõ‚àí1periodicand1non-periodicfeaturerepresentations sample,thehourlytimepointsvaryslightly.TheTransformer‚Äôs\nfromourdata,withùëõbeingthenumberofthefinaloutputpoints architectureutilizesthekey/value/queryformulation[6].Thequery\nistheinformationthatisbeinglookedfor,thekeyisthecontext\nofthetimeseries(inourcase,24).\norreference,andthevalueisthecontentthatisbeingsearched.In\nWhensubjectingtheperiodictimerepresentationtobeatrian-\nthecontextofmTAN,theinitialkeysarethetimepointsandthe\ngularpulse,themodelstartsfromverynarrowpulsesandthen\ninitialqueriesarevectorswithvaluesequallydistributedfrom0to\nproceedstolearnacombinationofthose,aspresentedpreviously\n1.Bothofthesetermsarethenpassedthroughthesamelearnable\ninFigure2.Ontheotherhand,whenwedefinetheperiodicity\ntimelayers.Since,initially,thekeysandthequerieshaveadifferent\nasasinefunction,theinitialweightinitialization(whichsetsthe\norderofmagnitudeitisexpectedthateitherthemodelwillincrease\nweightstoverysmallvalues)makestheinitialrepresentationsa\nthemagnitudeofthekeysordecreasethemagnitudeofthequeries.\nveryshortpartofthesinusoidalcurve(soshortthatvisuallyap-\nThroughourexperimentswediscoveredthattheformeroccurs.\npearsaslinearinourgraphs).Themodelthenlearnstoscalethe\nAspresentedinFigure7thesmallerkeyvaluescorrespondtoa\ntimevectorstomodelmoremeaningfultimerepresentations.An\nsmallerpartofthesinewave,whereasthelargerqueryvaluescover\nexampleofthisbehaviorispresentedinFigure4.\nalargerpartofthesinewave(Figure8).\nHowever,onemightwonder:areallofthe23learnedtimerep-\nTofurtherevaluateourclaim,wemodifiedthetimepointcre-\nresentationsmeaningful?Sinceitishardtotracktheinfluenceof\nationsothatbothqueriesandkeyshavethesamevalues.Todo\neveryrepresentationontheclassificationtask,wecanonlyspec-\nso,weassignedeachtimepointreferringtoanhourinadayits\nulate.InFigure5aredemonstratedalltheperiodicsinefeatures,\nnormalizedindexbydividingitwiththetotalnumberofhoursina\ngroupedbytheirsimilarity.Mosttimefeaturesfallintothefirst\nday.Asexpected,thelearnedrepresentationthenwasthesamefor\nthreegroups.Thelastthreegroups,whichonlyincludeonerep-\nbothkeysandqueries,asillustratedinFigure9.Comparingthem\nresentationeach,seemtobethemostexplainableregardingour\ntothepreviouslylearnedrepresentationsinFigures7and8,the\nknowledgeofthedomain.Theycapturelargerperiodsthatleadto\nevaluationofthekeyshasindeedscaledup,whiletheevaluation\nthenumericalcorrelationofdifferenthoursinaday.Webelieve\nofthequerieshasnotchangeddrastically.\nthatthesearethemostimportantfeaturesonwhichthemodel‚Äôs\nThemainreasonweoptedfornormalizingthetimestepsusing\npredictionreliesandalsothatwhenitstartedtolearnthesefeatures,\nUNIXtimewasthatitslightlyperformedbetterfortheclassification\nitneglectedtheothersandassignedsmallerweightstothem.To\ntask.Webelievethatthisisthecasebecausewhennormalizingthe\nsubstantiatethisassertionweillustrateinFigure6thecorrespond-\ntimepointsconsideringonlythehour,weloseanyothercharacteris-\ninggroupsatthestartofthelearningprocess.Itcanbeclaimed\nticthatrelatestotheprogressionoftime.Thisfurthercorroborates\nthatthefirstthreegroupsexhibitminimalchange,whereasthelast\nthat(asweextensivelystressedearlier)thenon-periodicalfeatures\nthreechangevisibly.\narecrucialforourapplication.\nSETN2024,September11‚Äì13,2024,Piraeus,Greece NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramveliotakis,andGeorgeKosmadakis\n(a)Group1(7timerepresentations) (b)Group2(9timerepresentations) (c)Group3(4timerepresentations)\n(d)Group4(1timerepresentation) (e)Group5(1timerepresentation) (f)Group6(1timerepresentation)\nFigure5:Learnedsinetimerepresentationsgroupedbytheirsimilarity.Eachfigureiscomprisedoftheaggregatedrepresen-\ntationscomputedonthetestingdata(meanandstandarddeviationofeachtime-feature).Thetopthreefiguresgatherthe\nmajorityofthefeatures,whereasthebottomthreeincludeonlyonefeatureeach.\n(a)Group1 (b)Group2 (c)Group3\n(d)Group4 (e)Group5 (f)Group6\nFigure6:Initialvaluesofthegroupsofthesinetimerepresentations.\nComparingPriorandLearnedTimeRepresentationsinTransformerModelsofTimeseries SETN2024,September11‚Äì13,2024,Piraeus,Greece\n(a)Group1 (b)Group2 (c)Group3\n(d)Group4 (e)Group5 (f)Group6\nFigure7:Learnedsinetimerepresentationsonthekeysofthetestdata.Theillustratedrepresentationsoccurredwhenthekeys\nandquerieshaddifferentordersofmagnitude.\n(a)Group1 (b)Group2 (c)Group3\n(d)Group4 (e)Group5 (f)Group6\nFigure8:Learnedsinetimerepresentationsonthequeriesofthetestdata.Theillustratedrepresentationsoccurredwhenthe\nkeysandquerieshaddifferentordersofmagnitude.\nSETN2024,September11‚Äì13,2024,Piraeus,Greece NataliaKoliou,TatianaBoura,StasinosKonstantopoulos,GeorgeMeramveliotakis,andGeorgeKosmadakis\n(a)Group1 (b)Group2 (c)Group3\n(d)Group4 (e)Group5 (f)Group6\nFigure9:Learnedsinetimerepresentationsonthekeysandqueriesofthetestdata.Theillustratedrepresentationsoccurred\nwhenthekeysandquerieshadthesameorderofmagnitude.\n5 CONCLUSIONS methodologiesforexplainingneuralnetworks,thisalsoshowedus\nWestudiedtheimpactoftimeembeddingswhenapplyingTrans- apathforfutureworkspecificallytargetingmTAN.Sinceoneof\nformermodelstotimeseriesanalysis,andspecificallytheimpactof thehindranceswasthatwewereunabletoobservetheeffectof\nusingaveryinformed,application-specifictimerepresentation,a eachfeatureduethedistributednatureoftheclassifier,wewould\ngenericsinusoidalrepresentationknowntocaptureperiodicphe- havelikedtosystematicallyexploretheeffectofincrementallyin-\nnomenawell,andamethodforlearningtheparametersthatbest creasingthenumberoffeaturesandobservingwhichfeaturesgets\nfiteitherofthesetwotothespecificdatasetathand. learned(interpretedas,isthemostimpactfulforreducingloss),\nOurexperimentsconcludedthatTransformers(andDNNsin whichislearnedsecondandsoon.However,thelinearalgebra\ngeneralwecouldargue)arenotveryamenabletoover-engineering behindmTAN tiesthedimensionalityofthefeaturerepresenta-\nthetimerepresentationduetoside-effectsthataredifficulttomiti- tiontothedimensionalityoftheinput,aswecannotde-couplethe\ngate.Thiscanbeseenasanegativequalityinapplicationssuchas encodingofperiodicityandtheencodingoflineartime.Since(as\nourswherethereisextensivepriorknowledgeonawell-studied arguedabove)lineartimeiscriticalforperformance,wearestuck\nphenomenon,butcanalsobeconstruedasapositivequalityasthe withapre-defineddimensionalityforthesinefeaturesaswell.\nnetworkwasableto‚Äòdiscover‚Äôtheknowledgeweweretryingto Ourenvisagedfutureresearchistore-workthelinearalgebraof\nconvey.Especiallythecomparisonbetweenthesinusoidalprior themTAN sothatwecande-coupletheencodingoftheperiodic\nandlearnedrepresentationsisverypromisinginthisrespect,at andlineartimerepresentation,allowingtohaveonlythelatter\nthemaximally-flexiblemTAN networkrecoveredalmostallthe beconstrainedbythedimensionalityoftheinput.Inthespecific\naccuracyoftheprior-timesinusoidalnetwork. experimentpresentedhere,thiswouldtranslatetoabetterunder-\nAsecondlevelofanalysisdelvedintothenatureofthefeatures standingofwhathappenswhenweinformtheTransformerofthe\nthatmTAN learned.Thisanalysishasdemonstratedtheabilityof biaswewanttoapplyregardingperiodicities.Which,initsturn,\nthelinearlayer-sinusoidalactivationfunctionarchitecturetovery isexpectedtoleadtomethodsforaffordingthehumanoperator\ncloselyapproximatetheclearlynon-smoothbehaviordiscussedin intuitiveandeffectivecontrolofthenetwork.\nSection3andshowninFigure1.\nItshould,however,benotedthatouranalysiswasrestricted\nto what we could indirectly observe by trying out different pa-\nrameterizationsandspeculatingbasedonourunderstandingof\nhowthenetworkistrained.Besidesanygeneraladvancementsin\nComparingPriorandLearnedTimeRepresentationsinTransformerModelsofTimeseries SETN2024,September11‚Äì13,2024,Piraeus,Greece\nACKNOWLEDGMENTS REFERENCES\nThisresearchhasbeenco-financedbytheEuropeanUnionand [1] ZhengpingChe,SanjayPurushotham,KyunghyunCho,DavidSontag,andYan\nLiu.2018.RecurrentNeuralNetworksforMultivariateTimeSerieswithMissing\nGreeknationalfundsthroughtheprogram‚ÄòFlagshipactionsinin-\nValues.ScientificReports8(042018).Issue1.\nterdisciplinaryscientificareaswithaspecialinterestintheconnec- [2] DanielNeil,MichaelPfeiffer,andShih-ChiiLiu.2016.PhasedLSTM:Accelerating\ntionwiththeproductionnetwork‚Äô‚ÄîGREENSHIPPING‚ÄîTAEDR- RecurrentNetworkTrainingforLongorEvent-basedSequences.InAdvancesIn\nNeuralInformationProcessingSystems.3882‚Äì3890.\n0534767(Acronym:NAVGREEN).Formoreinformationpleasevisit [3] TrangPham,TruyenTran,DinhPhung,andSvethaVenkatesh.2017.Predicting\nhttps://navgreen.gr healthcaretrajectoriesfrommedicalrecords:Adeeplearningapproach.Journal\nofBiomedicalInformatics69(2017),218‚Äì229.\nThisresearchwasco-fundedbytheEuropeanUnionunderGA\n[4] MonaSchirmer,MazinEltayeb,StefanLessmann,andMajaRudolph.2022.Model-\nno.101135782(MANOLOproject).Viewsandopinionsexpressed ingIrregularTimeSerieswithContinuousRecurrentUnits.InProceedingsofthe\narehoweverthoseoftheauthorsonlyanddonotnecessarilyreflect 39thInternationalConferenceonMachineLearning,17-23Jul2022,Vol.162.\n[5] SatyaNarayanShuklaandBenjaminMarlin.2021. Multi-TimeAttentionNet-\nthose of the European Union or CNECT. Neither the European\nworksforIrregularlySampledTimeSeries.InInternationalConferenceonLearning\nUnionnorCNECTcanbeheldresponsibleforthem. Representations.\nAWSresourceswereprovidedbytheNationalInfrastructuresfor [6] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN\nGomez,≈ÅukaszKaiser,andIlliaPolosukhin.2017. AttentionisAllyouNeed.\nResearchandTechnologyGRNETandfundedbytheEURecovery InAdvancesinNeuralInformationProcessingSystems,I.Guyon,U.VonLuxburg,\nandResiliencyFacility. S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(Eds.),Vol.30.\nCurranAssociates,Inc.\n[7] QingsongWen,TianZhou,ChaoliZhang,WeiqiChen,ZiqingMa,JunchiYan,\nandLiangSun.2023.TransformersinTimeSeries:ASurvey.InProceedingsofthe\nThirty-SecondInternationalJointConferenceonArtificialIntelligence,SurveyTrack.\nMacao,19‚Äì25August2023. https://doi.org/10.24963/ijcai.2023/759",
    "pdf_filename": "Comparing_Prior_and_Learned_Time_Representations_in_Transformer_Models_of_Timeseries.pdf"
}