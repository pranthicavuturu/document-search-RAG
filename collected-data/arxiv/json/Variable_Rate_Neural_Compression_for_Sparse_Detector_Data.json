{
    "title": "Variable Rate Neural Compression for Sparse Detector Data",
    "abstract": "High-energy large-scale particle colliders generate data at extraordinary rates, reaching up to one terabyte per second in nuclear physics and several petabytes per second in high-energy physics. Developing real-time high-throughput data compression algorithms to reduce data volume and meet the bandwidth requirement for storage has become increasingly critical. Deep learning is a promising technology that can address this challenging topic. At the newly constructed sPHENIX experiment at the Relativistic Heavy Ion Collider, a Time Projection Chamber (TPC) serves as the main tracking detector, which records three-dimensional particle trajectories in a volume of a gas- filled cylinder. In terms of occupancy, the resulting data flow can be very sparse reaching 10−3 for proton-proton collisions. Such sparsity presents a challenge to conventional learning-free lossy compression algorithms, such as SZ, ZFP, and MGARD. In contrast, emerging deep learning-based models, particularly those utilizing convolutional neural networks for compression, arXiv:2411.11942v1  [physics.ins-det]  18 Nov 2024",
    "body": "Variable Rate Neural Compression for Sparse Detector\nData\nYi Huanga, Yeonju Goa, Jin Huanga, Shuhang Lic, Xihaier Luoa, Thomas\nMarshalle, Joseph D. Osborna, Christopher Pinkenburga, Yihui Rena,\nEvgeny Shulgaa,d, Shinjae Yooa, Byung-Jun Yoona,b\naBrookhaven National Laboratory, PO Box 5000, Upton, NY, 11973, USA\nbTexas A&M University, 3128 TAMU, College Station, TX, 77843, USA\ncColumbia University, 538 West 120th Street, New York, NY, 10027, USA\ndStony Brook University, 100 Nichols Road, Stony Brook, NY, 11794, USA\neUniversity of California, Los Angeles, 475 Portola Plaza, Los Angeles, CA, 90095, USA\nAbstract\nHigh-energy large-scale particle colliders generate data at extraordinary rates,\nreaching up to one terabyte per second in nuclear physics and several petabytes\nper second in high-energy physics.\nDeveloping real-time high-throughput\ndata compression algorithms to reduce data volume and meet the bandwidth\nrequirement for storage has become increasingly critical. Deep learning is a\npromising technology that can address this challenging topic. At the newly\nconstructed sPHENIX experiment at the Relativistic Heavy Ion Collider,\na Time Projection Chamber (TPC) serves as the main tracking detector,\nwhich records three-dimensional particle trajectories in a volume of a gas-\nfilled cylinder. In terms of occupancy, the resulting data flow can be very\nsparse reaching 10−3 for proton-proton collisions. Such sparsity presents a\nchallenge to conventional learning-free lossy compression algorithms, such as\nSZ, ZFP, and MGARD. In contrast, emerging deep learning-based models,\nparticularly those utilizing convolutional neural networks for compression,\narXiv:2411.11942v1  [physics.ins-det]  18 Nov 2024\n\nhave outperformed these conventional methods in terms of compression ra-\ntios and reconstruction accuracy. However, research on the efficacy of these\ndeep learning models in handling sparse datasets, like those produced in par-\nticle colliders, remains limited. Furthermore, most deep learning models do\nnot adapt their processing speeds to data sparsity, which affects efficiency. To\naddress this issue, we propose a novel approach for TPC data compression via\nkey-point identification facilitated by sparse convolution. Our proposed algo-\nrithm, BCAE-VS, achieves a 75% improvement in reconstruction accuracy with\na 10% increase in compression ratio over the previous state-of-the-art model.\nAdditionally, BCAE-VS manages to achieve these results with a model size over\ntwo orders of magnitude smaller. Lastly, we have experimentally verified that\nas sparsity increases, so does the model’s throughput. Our code along with\nthe pretrained models and dataset used for model development are avail-\nable at https://github.com/BNL-DAQ-LDRD/NeuralCompression_v3 and\nhttps://zenodo.org/records/10028587, respectively.\nKeywords:\nDeep Learning, Autoencoder, High-throughput Inference, Data\nCompression, Sparse Data, Sparse Neural Network, High Energy and\nNuclear Physics\n1. Introduction\n1.1. Background\nHigh-energy particle colliders, such as the Large Hadron Collider (LHC) [1]\nand the Relativistic Heavy Ion Collider (RHIC) [2], are critical tools for prob-\ning the fundamental building blocks of matter. In these experiments, pro-\ntons and heavy ions are accelerated to velocities approaching the speed of\n2\n\nsilicon tracker\nTPC\ncalorimeters\nFigure 1: sPHENIX detector assembly. Time projection chamber (TPC) con-\ntributes the majority of data.\nlight and providing collisions at extremely high energies (for RHIC nucleon-\nnucleon center of mass energy is 200 GeV in heavy ion collisions up to and\n510 GeV in proton-proton collisions). These complex interactions generate a\nmultitude of subatomic particles (thousands of particles in the central heavy\nion collisions), enabling scientists to explore the deepest questions about the\nnature of the universe. To capture the products of collisions, the interac-\ntion point is surrounded by the state of art particle detectors.\nRecently\nconstructed sPHENIX experiment [2] at RHIC focuses on studying the mi-\ncroscopic properties of strongly interacting matter, such as the quark-gluon\nplasma, a state of matter that existed in the early universe.\nTo achieve this, sPHENIX employs an array of advanced particle detectors\n(Figure 1). The Time Projection Chamber (TPC) [3] is the primary tracking\ndetector. The TPC acts as a three-dimensional (3D) camcorder, continu-\n3\n\nously recording the trajectories of charged particles as they pass through its\ndetection volume. As particles move through the TPC, they ionize the gas\nmolecules, creating clusters of charge. Clusters drift in the electric field to\nthe readout pads and induce electrical signals. These signals (hits) are reg-\nistered by 160 thousand detector channels. Both the position and time of a\nhit are captured by one of the 40 million voxels in a 3D grid of the active de-\ntection volume. The system then digitizes these signals, transforming them\ninto analog-to-digital conversion (ADC) values. The TPC readout digitizes\n3.2 trillion voxels per second and generates terabits per second of data after\nzero suppression.\nTraditionally, collider experiments have relied on a level-1 trigger system\nto manage the massive data output by selecting only the most significant\ncollision events for storage, while discarding less valuable data. However,\nnext-generation collider experiments, including sPHENIX, are moving to-\nwards streaming data acquisition (DAQ) systems [4, 5, 6], which aim to con-\ntinuously record a large fraction to all collision events. While this approach\nmaximizes the physics output, it presents a critical challenge: handling the\nimmense data volume in real time.\nTo reduce the TPC data volume while preserving key information, ef-\nficient data compression techniques are essential. This paper introduces a\ndeep neural network-based lossy compression algorithm designed to meet\nthe high-throughput demands of streaming DAQ systems. Moreover, unlike\nspecialized algorithms tailored specifically for TPC data [7], our approach\nis data-driven and makes no assumptions about the underlying physics, al-\nlowing it to generalize to other datasets with similar characteristics, such as\n4\n\nthose for the future electron-ion collider [6, 5].\n1.2. Related Work and Research Gaps\nConventional scientific lossy compression methods are motivated in fields\nsuch as climate science, fluid dynamics, cosmology, and molecular dynamics.\nThese methods have been driven by the necessity to handle large datasets\nproduced by distributed and high-fidelity simulations in a high-performance\ncomputing environment. For example, the error-bounded SZ compression\nalgorithm [8, 9, 10] has proven effective for compressing data from climate sci-\nence and cosmology simulations. Other examples include the ZFP method [11],\nwhich was developed specifically for hydrodynamics simulations, and the\nMGARD method [12, 13], which was designed to compress turbulent chan-\nnel flow and climate simulation data.\nWhile there is extensive research on general-purpose lossy compression,\nfew existing methods are optimized for highly sparse data. Although the\naforementioned compression algorithms have demonstrated reasonable per-\nformance with TPC data, none of them are specifically tailored for this type of\ninput. In this context, a specialized neural network-based compression model,\nthe Bicephalous Convolutional Auto-Encoder (BCAE), introduced in [14] and\nlater refined in [15], has been shown to outperform traditional methods in\nterms of both compression ratio and reconstruction accuracy for sparse TPC\ndata.\nHowever, despite the BCAE’s superior performance, its design treats the\nsparsity of TPC data as a challenge to overcome rather than as an oppor-\ntunity to improve compression efficiency. The BCAE model compresses data\nby mapping the input array to a fixed-size code, regardless of the input’s\n5\n\noccupancy (i.e., the fraction of non-zero elements). This results in ineffi-\nciencies: when the code size is large enough to accommodate inputs with\nhigh signal occupancy, there is wasted space for sparser inputs; conversely,\nwhen the code size is optimized for sparser inputs, denser inputs suffer from\ninformation loss during compression.\n1.3. Challenges and Solutions\nChallenge 1. Occupancy-based variable compression ratio. While\nTPC data on average exhibits low occupancy (around 10%), the actual oc-\ncupancy can vary significantly. For instance, in the dataset used for model\ndevelopment, occupancy ranges from less than 5% to over 25%. Therefore,\nthere is a need for compression algorithms capable of producing larger codes\nfor denser inputs and smaller codes for sparser ones.\nSolution. We propose BCAE-VS, a bicephalous autoencoder that provides\na variable compression ratio based on TPC data occupancy. The design of\nBCAE-VS is motivated by the hypothesis that a trajectory can be reliably\nreconstructed from a subset of signals. Consequently, data compression can\nbe achieved by selectively down-sampling signals rather than resizing the input\narray. To implement this, BCAE-VS’s encoder predicts the importance of each\nsignal in the input array for trajectory reconstruction. Signals with higher\nimportance are treated as analogous to the key points that anchor shapes\nin computer vision tasks such as action and gesture recognition [16, 17, 18,\n19].\nCompression is achieved by saving only the coordinates and neural\nrepresentations of these key points.\nChallenge 2. Leveraging sparsity for throughput.\nDespite the im-\npressive reconstruction accuracy of BCAE-VS, implementing its encoder with\n6\n\nconventional (dense) convolution operations will lead to constant computa-\ntion time disregarding input data with changing sparsity and complexity.\nSolution. We use sparse convolution for implementing BCAE-VS’s encoder.\nWith a manageable runtime overhead, the sparse convolution-powered en-\ncoder of BCAE-VS generates output only for the signals with input exclusively\nfrom the signals and skips all matrix multiplications with all-zero operands.\nAs the overhead and the number of non-zero values vary with occupancy,\nBCAE-VS achieves a variable throughput that increases significantly as occu-\npancy decreases.\nMain Contributions:\n• Introduction of BCAE-VS: We propose BCAE-VS, an improved BCAE\nmodel enabling Variable compression ratio for Sparse data. This model\naims to enhance both compression efficiency and reconstruction accu-\nracy by selectively down-sampling signals rather than reducing the size\nof the input array.\n• Improved reconstruction performance: BCAE-VS achieves a 75%\nimprovement in reconstruction accuracy and a 10% higher compression\nratio on average compared to the most accurate BCAE model.\n• Use of sparse convolution for high throughput: To address the\ncomputational inefficiencies of conventional convolution with high spar-\nsity data, BCAE-VS employs sparse convolution. This approach pro-\ncesses only the relevant signals, significantly reducing the computa-\ntional overhead associated with matrix multiplications involving all-\nzero operands.\n7\n\n2. TPC Data\n2.1. Overview\nFigure 2 shows a schematic view of sPHENIX TPC. sPHENIX TPC is\na cylindrical gas-filled drift chamber.\nWorking gas mixture is 75% argon\n(Ar), 20% carbon tetrafluoride (CF4), and 5% isobutane (C4H10). Collisions\nhappen in the center of the TPC. Charged particles produced in the collisions\ntraverse the volume of the TPC at a speed close to the speed of light. As\ncharged particles travel, they ionize the gas atoms along trajectories and\ncreate trails of electrons and ions.\nExtremely uniform electric field E =\n400 V cm−1 provides uniform drift velocity for electron clouds drift towards\nthe readout planes.\nThe positions and arrival times of the electrons are\nrecorded by the sensors arranged in concentric layers on the readout planes.\nGas ionization by a particle depends only on its velocity and the square of the\ncharge, thus the density of the electron cloud allows to distinguish different\nparticles, e.g. protons, kaons, and pions. The chamber is placed within the\nmagnetic field of a superconducting magnet, which provides a magnetic field\nB = 1.4 T collinear with the electric field. Trajectory of the particle is bent\nby the magnetic field along the axial direction. This bending allows for the\ndetermination of the momenta and charges of the particles based on the\ncurvature of their trajectory.\nThe sPHENIX TPC readout plane has three groups of layers: inner,\nmiddle, and outer.\nEach layer group produces a readout of a 3D (axial,\nradial, and azimuthal) array of ADC values. Each array has 16 layers along\nthe radial direction and 249 temporal sampling points (for one TPC drift\ntime window) along the axial direction on each side of the TPC, representing\n8\n\nB\nE\nE\nreadout\nplane\nreadout\nplane\ncentral\nmembrane\ninteraction\npoint\nionized\nelectrons\ntrajectory\nFigure 2: A schematic view of sPHENIX TPC.\ntime evolution. The number of readout pads along the azimuthal direction\nincreases from inner to outer layers. The number is constant within a layer\ngroup and it is 1152, 1536, and 2304, respectively. In this study, we will focus\non the 16 layers of the outer group of the TPC with 2304 readout pads along\nthe azimuthal direction. Each element in the (layer, pad, temporal) array is\nmapped to a voxel in the 3D space. We demonstrate a full azimuthal outer\nlayer group readout in Figure 3A (experiment 3060, event 8 in the test split\nof the dataset).\nTo match the subdivision of the readout electronics in the TPC readout\nchain, the data from a layer group has the same division. It has 24 equal-size\nnon-overlapping sections: 12 along the azimuthal direction (30 degrees per\nsection) and 2 along the axial direction (separated by the transverse plane\npassing the collision point). We call these sections TPC wedges. Figure 3B\nshows a zoom-in view of one of the wedges from the outer layer group. Each\n9\n\nA. full TPC outer layer group\nB. a TPC wedge\nC. zoom-in of a trajectory\nFigure 3: Visualization of the outer layer group TPC data of an Au+Au collision\nevent with 0-10% centrality and 170 kHz pileup.\nouter-layer wedge is an array of dimensions 16, 192, and 249 in radial, az-\nimuthal, and axial directions, respectively. All ADC data from the same\nwedge will be transmitted to the same group of front-end electronics, after\nwhich a real-time lossy compression algorithm could be deployed. There-\nfore, TPC wedges are used as the direct input to the deep neural network\ncompression algorithms.\n2.2. Dataset and Preprocessing\nTo train a compression algorithm, we used a dataset containing 1310 sim-\nulated events for central √sNN = 200 GeV Au+Au collisions with 170 kHz\npileup collisions1. The data were generated with the HIJING event genera-\n1Zenodo linked: https://zenodo.org/records/10028587\n10\n\ntor [20] and Geant4 Monte Carlo detector simulation package [21] integrated\nwith the sPHENIX software framework [22].\nThe simulated TPC readout (ADC values) from these events are repre-\nsented as 10-bit unsigned integers ∈[0, 1023]. To reduce unnecessary data\ntransmission, a zero-suppression algorithm has been applied. All ADC values\nbelow 64 are set to be zero as large fraction of them represent noise. This\nzero-compression makes the TPC data sparse at about 10.8% occupancy of\nnon-zero values. A voxel that has a non-zero ADC value is a signal voxel. A\nvoxel with a 0 ADC value is a non-signal voxel.\nWe divide the 1310 total events into 1048 events for training and 262\nfor testing. Each event contains 24 outer-layer wedges. Thus, the training\npartition contains 25152 TPC outer-layer wedges, while the testing portion\nhas 6288.\nFinally, as trajectory coordinates must be interpolated from neighboring\nsensors using the ADC values, it is important to preserve the relative ADC\nratio between the sensors. Hence, for this study, we work with log ADC values\n(defined as log2(ADC+1)) instead of the raw ADC values. A log ADC value\nis a float number ∈[0., 10.]. As zero-suppression is implemented at 64 for this\ndataset, and all nonzero log-ADC values exceed 6. Zero-suppression makes\nthe distribution of ADC bimodal and extremely skewed.\nRemark: This distribution presents a significant challenge for neural\nnetwork-based algorithms since neural network tends to work well when input\ndata is distributed approximately normally [23, 24]. In Section 3.1 we will\nshow one approach to handle the discontinuity in the data distribution with\na compression autoencoder equipped with two collaborating decoders. We\n11\n\nwill discuss the limitation of this approach and provide a better solution in\nSection 3.2 with sparse convolution. As a result, the sparsity of the data\nis no longer an obstacle to overcome but rather an advantage that we can\nleverage.\n3. Method\n3.1. Bicephalous Convolutional Autoencoder\nTo address the problem caused by the difficult distribution of log ADC\nvalue, Bicephalous Convolutional Autoencoder (BCAE) was proposed in [14]\nand later optimized in [15]. A BCAE is an autoencoder with two decoders – one\nfor segmentation and the other for regression (See Figure 4). During training,\na code significantly smaller than the input is generated by the encoder and\nfed to the decoders. The segmentation decoder, Dseg, will output a score\ns ∈(0, 1) for each voxel in the input. The weights of Dseg are updated by\ncomparing s to the true label of the input – 1 for a signal voxel and 0 for\na non-signal voxel. The regression decoder, Dreg, will output a value r for\neach voxel. Given a threshold s0 ∈(0, 1), the predicted value (ˆr) at each\nvoxel is set to r if s > s0 and to 0 if otherwise. The weights of Dreg are then\nupdated by comparing ˆr to the true input value of the voxel. The weights\nof the encoder E are updated with the combined gradient information from\nboth decoders. During inference, we only run the encoder part and save the\ncode to storage for later use.\nThe design of BCAE is motivated by two considerations: reconstruction\naccuracy and efficiency. First, the segmentation decoder is tasked to tackle\nthe discontinuity in the input data distribution so that the regression decoder\n12\n\ncan focus solely on approximating the ADC values.\nSecond, having two\ndecoders does not affect the compression (inference) throughput as only the\nencoder will be executed during compression. From the first consideration,\nwe can see that a better solution with respect to reconstruction accuracy\nmight be to have two single-tasking encoder-decoder pairs. However, since\nthe encoding will run in real-time in deployment, it is crucial to keep the\nencoder simple and lightweight as much as possible for the high TPC data\nrate. Additionally, as demonstrated in [15], the possible adverse effect of a\nsimple encoder could be compensated to some extent by a more complicated\ndecoder.\nencoder E\ncode\nsegmentation\ndecoder Ds\nregression\ndecoder Dr\nsegmentation\noutput\nregression\noutput\nrecon-\nstruction\ninput x\nlabel\nregression loss\nsegmentation loss\nfeed-forward\nback-propagation\nhard thresholding\nelement-wise\nmultiplication\nFigure 4: Bicephalous convolutional autoencoder.\nAlthough BCAE has been shown to outperform existing non-neural network-\nbased compression algorithms in reconstruction accuracy at equal or higher\ncompression ratio [14], the approach does not leverage the sparsity of the\ninput. First, its dense convolution incurs static computation costs despite\nvarying input sparsity. Second, to produce a significantly smaller code, the\ndown-sizing in the encoder breaks the sparsity of the input. This makes it\n13\n\nhard for the decoders to reconstruct a trajectory at its correct location, espe-\ncially in higher occupancy areas (see Figure 10). Finally, for BCAE, the same\ncode length is used regardless of data sparsity. This means that if a chosen\ncode length is suitable for sparse and relatively simple data, it will become\ninsufficient for denser and more complex ones.\n3.2. BCAE-VS for Key-Point Identification\nFigure 3C shows a close-up view of a typical trajectory formed by a streak\nof signal patches. The localized and seemingly redundant signal voxels of the\nsame patch suggest the possibility of reconstructing the trajectory with only\na fraction of them. As a starter, we performed a sanity check by randomly\nmasking out 50% of signal voxels and used a neural network to reconstruct\nthe original input.\nThis compression by random sampling does not even\nrequire a learnable encoder network! The reconstruction mean squared error\n(MSE) (in raw ADC) is 95, which is much smaller than the 218 reported\nin [14].\nThese results suggest that if randomly selected signal voxels can perform\neffectively in reconstruction, a carefully selected subset of signal voxels should\nyield even better performance. Such a subset is analogous to the key points\nin computer vision, to which location a geometric shape is anchored. This\nconcept led to the development of BCAE-VS whose encoder compresses by\ndown-selecting signal voxels rather than down-sizing the whole input array.\nTo achieve compression by down-selecting, an encoder assigns an impor-\ntance score to each signal voxel, retaining only those with higher importance\nand discarding the rest. However, implementing such an encoder with stan-\ndard (dense) convolutions requires running 3D convolutions that maintain\n14\n\nthe input dimension, which can be relatively slow [15]. This is where sparse\nconvolution becomes a potentially better alternative, as it can compute solely\nfor the signal voxels and use input exclusively from them.\nThis section is structured as follows: Section 3.2.1 explains the operation\nof BCAE-VS. Section 3.2.2 contrasts sparse convolution with dense convolu-\ntion. In Section 3.2.3, we explore using random thresholding for identifying\nkey signal voxels. Finally, Section 3.2.4 addresses the balance between com-\npression and reconstruction in BCAE-VS.\n3.2.1. The Working of BCAE-VS\nsparse\nencoder E\nprobability\nvalue\ncode\nsegmentation\ndecoder Ds\nregression\ndecoder Dr\nsegmentation\noutput\nregression\noutput\nrecon-\nstruction\ninput xs\nlabel\nregression error\nsegmentation loss\nA. BCAE-VS training\nsparse\nencoder E\nprobability\nvalue\ncode\nstorage\ninput\nB. BCAE-VS inference\nfeed-forward\nback-propagation\nhard thresholding\nsoft thresholding\nelement-wise multiplication\nsparse-encoded tensor\nFigure 5: Bicephalous autoencoder with sparse encoder for key-points identifi-\ncation.\nTo describe BCAE-VS in more detail, let us first fix some notations. We\nwill use x to denote a n-dimensional array and xs to denote the sparse form of\nx with only the location and value of the non-zero entries specified. Although\n15\n\nx and xs represent the same input, the different notations emphasize the fact\nthat zeros (non-signal voxels) do not occupy any space in xs while x contains\nall the zeros.\nAs demonstrated in Figure 5, for each entry in a TPC wedge xs, the en-\ncoder E of BCAE-VS produces two outputs, importance score p and value v.\nThe importance score output helps us determine the contribution of signal\nvoxel in accurate reconstruction and the value is what we will save to the\nstorage should the voxel be selected. There are two reasons for letting E pro-\nduce v instead of using the original (raw or log) ADC value for reconstruction\n(as we did in the “encoder-free” compression). First, since we will have to\nsave a value for a selected voxel anyway, an adjusted value produced by a\ntrained neural network may be more helpful than the original (raw or log)\nADC value. Second, the gain in inference speed from having one less output\nchannel is marginal according to our experiment (at least for the specific\nsparse convolution implementation we chose for this work).\nDuring the training of BCAE-VS, we pass importance score p through\na soft mask function with an element-wise random threshold to get a soft\nmask m. The element-wise multiplication of v and m form the input to the\ndecoders. For inference (compression only), the random soft thresholding\nwill be replaced by a random hard thresholding. More details on the soft\nmask and random thresholding will be discussed in Section 3.2.\n3.2.2. Sparse Convolution\nFor symbolic simplicity, we explain sparse convolution with input with\ntwo spatial dimensions, height and width. Generalization to a higher dimen-\nsional input is straightforward. We use s and d to denote the stride and\n16\n\ndilation of a convolution, and Kh and Kw to denote the kernel window for\nthe height and width direction, respectively. For example, for convolution\nwith kernel size 3, we have Kh = Kw = [−1, 0, 1]. We use i, o, h, w, kh, and\nkw to iterate through the input and output channels, height and width, and\nthe kernel, respectively. The output Y can be calculated from the weight W,\nbias b, and input X as follows:\nYo,h,w =\nX\ni,kh,kw\nWo,i,kh,kwXi, sh+dkh, sw+dkw + bo\n=\nX\nkh,kw\nX\ni\nWo,i,kh,kwXi, sh+dkh, sw+dkw\n|\n{z\n}\nmatrix multiplication\n+bo\n(1)\nFor a dense convolution, the summation P\nkh,kw is taken over K = Kh × Kw\nand hence we cannot avoid the matrix multiplication in Equation 1 even if\nthe input vector Xi, sh+dkh, sw+dkw is an all-zero vector. However, for a sparse\nconvolution, the summation is taken over\nKh,w = {(kh, kw) |(sh + dkh, sw + dkw) ∈S } ⊂K\nwhere S is the collection of coordinates in a sparse input. The map (h, w) 7→\nKh,w is commonly referred to as the kernel map in the sparse convolution\nliterature.\nThere are two types of sparse convolution: normal and submanifold sparse\nconvolution. Let Nh,w = {(sh + dkh, sw + dkw) |(kh, kw) ∈K} be the neigh-\nborhood of (h, w) with respect to the kernel, normal sparse convolution com-\nputes for a coordinate (h, w) if Nh,w ∩S is not empty, while submanifold\n17\n\nsparse convolution only computes for (h, w) ∈S. Since normal sparse con-\nvolution will introduce new entries to S every time the convolution with\nkernel size > 1 is applied, and breaks the sparsity, we use submanifold sparse\nconvolution for the BCAE-VS encoder.\nAs we can see from the discussion above, while sparse convolution can re-\nduce runtime by avoiding computations on zero input, it incurs overhead from\ncomputing kernel maps. Consequently, sparse convolution is more efficient\nat lower sparsity but loses its advantage as sparsity decreases. Additionally,\nthe efficiency of sparse convolution varies with the specific implementation.\nFor this work, we use the MinkowskiEngine library from NVIDIA2, as it pro-\nvides the highest throughput on the hardware used in this research. Details\non throughput are available in Section 4.3.\n3.2.3. Random Thresholding\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\np\nφ(p, θ; α = 4)\nφ with α = 4\nand varying value of θ\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nθ\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\np\nφ(p, θ = .5; α)\nφ with θ = .5\nand varying value of α\n1\n2\n3\n4\n5\n6\n7\n8\n9\nα\nFigure 6: The soft mask function.\n2https://github.com/NVIDIA/MinkowskiEngine\n18\n\nprobability p\nrandom threshold θ\nsoft gate φ (p, θ)\nFigure 7: Random thresholding. For each signal voxel, the encoder will calculate an\nimportance score p (left). A threshold θ is then generated following Uniform(0, 1) for each\nvoxel (middle). A soft mask is obtained by φ(p, θ) (right).\nThe problem of key-point identification in TPC data compression can be\nthought of as a “self-supervised classification” problem. It is a classification\nproblem since we want to predict a label 1 for a signal voxel crucial for\naccurate reconstruction and a 0 for a non-essential one. It is a self-supervised\nproblem since we do not have a ground truth label to match but have to infer\nthe label by observing the effect of keeping or discarding a signal voxel on\nreconstruction.\nFor this goal, we designed the random thresholding approach that works\nas follows. For a signal voxel with predicted importance score p, we generate\na random number θ ∼Uniform(0, 1), and calculate a soft mask with the\nfunction defined in Equation 2\nφ(p, θ; α, ε) = sigmoid (α [logit(p, ε) −logit(θ, ε)])\n(2)\nThe ε in the logit function is used to improve numerical stability (p or 1−p\nless than ε will be replaced with ε). We set ε = 10−8 for this study. Plots of\nφ with varying threshold θ and α are shown in Figure 6. We can see that φ\n19\n\nis a differentiable approximation to the step function with a step at θ. For a\nfixed θ, α increases the steepness of φ when making the transition from 0 to\n1. For this study, we set α = 4.\n3.2.4. Balancing Reconstruction with Compression\nTo train BCAE-VS, we need to balance the reconstruction accuracy with\ncompression.\nWe do this by adjusting the reconstruction loss Lrecon and\ncompression loss Lcomp.\nThe reconstruction loss Lrecon is the weighted sum of the segmentation\nloss Lseg and the regression loss Lreg. We use focal loss [25] for segmentation\nsince it is designed to deal with datasets with unbalanced classes (TPC data\nis sparse and hence has a high percentage of voxels in the class 0).\nWe\nuse mean absolute error (L1) for regression following the convention in [15].\nThe reconstruction loss Lrecon(x) is defined as λsegLseg(x) + Lreg(x) with λseg\nadjusted by the end of each epoch with the same method as discussed in [15,\nsection 2.5]\nSince BCAE-VS compresses by identifying key points, we want the average\nof p, denoted as µ, to be small, so that only the valuable signal voxels can\nget a high importance score. However, since there is no need for the average\nimportance score to go arbitrarily small, we also set a lower bound lprob so\nthat the average importance will be penalized more lightly when it is close\nto lprob, and will no longer be penalized if it goes below lprob. Hence, the\ncompression loss function of BCAE-VS is defined as\nLcomp (x, lprob) =\n\n\n\nµ(xs) (µ(xs) −lprob)\nµ(xs) > lprob\n0\nµ(xs) ≤lprob\n20\n\nThe overall loss L(x) of BCAE-VS is defined as λcompLcomp (xs, lprob) +\nLrecon(x). In this study, we set lprob = .1 and λcomp = 30.\n4. Results\n4.1. Reconstruction Accuracy\nTable 1: Comparing compression algorithm with dense and sparse convolutions.\nreconstruction performance\nefficiency\nmodel\ncomp.\nratio ↑\nL1 ↓\nL2 ↓\nPSNR ↑\nrecall ↑\nprecision ↑\nencoder\nsize\nthroughput ↑\nBCAE-2D\n31\n.152\n.862\n20.6\n.907\n.906\n169k\n9.6k\nBCAE-HT (3D)\n31\n.138\n.781\n20.8\n.916\n.915\n9.8k\n9.6k\nBCAE++ (3D)\n31\n.112\n.617\n21.4\n.936\n.934\n226k\n3.2k\nBCAE-VS\n34\n.028\n.089\n26.0\n.988\n.996\n382\n5.6k\n0.05\n0.1\n0.15\n0.2\n0.25\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\noccupancy\nL1\nBCAE-2D\nBCAE-HT\nBCAE++\nBCAE-VS\n0\n100\n200\ncount\navg. occupancy=.108\n0.05\n0.1\n0.15\n0.2\n0.25\n18\n20\n22\n24\n26\n28\n30\noccupancy\nPRNR\nBCAE-2D\nBCAE-HT\nBCAE++\nBCAE-VS\n0\n100\n200\ncount\navg. occupancy=.108\nFigure 8: Reconstruction L1 error (left) and peak signal-to-noise ratio (PSNR,\nright) as a function of occupancy for dense BCAE models.\n21\n\n0.05\n0.1\n0.15\n0.2\n0.25\n0.8\n0.85\n0.9\n0.95\n1\noccupancy\nrecall\nBCAE-2D\nBCAE-HT\nBCAE++\nBCAE-VS\n0\n100\n200\ncount\navg. occupancy=.108\n0.05\n0.1\n0.15\n0.2\n0.25\n0.8\n0.85\n0.9\n0.95\n1\noccupancy\nprecision\nBCAE-2D\nBCAE-HT\nBCAE++\nBCAE-VS\n0\n100\n200\ncount\navg. occupancy=.108\nFigure 9: Reconstruction recall (left) and precision (right) as a function of\noccupancy for dense BCAE models.\nSince there is no value below 6 in the input log ADC, we apply a zero-\nsuppression at 6 to the reconstruction produced by BCAE-VS. The reason\nthat we did not use the regression output transform approach (mapping all\nregression output to above 6) introduced in dense BCAEs [15] is because it\nmakes the training of BCAE-VS unstable.\nWe measured the reconstruction accuracy in L1, L2, peak signal-to-noise\nratio (PSNR), recall, and precision and listed the result in Table 1. We com-\nputed reconstruction from compressed data saved in half-precision (float16).\nHere, recall is defined as the fraction of signals that get a positive value in\nthe prediction, and precision is defined as the fraction of true signals over\nall voxels that has a positive prediction. To compare the compression ratio\nof the BCAE-VS with those of the dense BCAE models, we use the convention\n22\n\nBCAE-2D\nBCAE-HT\nBCAE++\nBCAE-VS\nreconstruction\ndifference\n6\n6.5\n7\n7.5\n8\n-8\n-4\n0\n4\n8\nground truth\nBCAE-2D\nBCAE-HT\nBCAE++\nBCAE-VS\nreconstruction\ndifference\n6\n6.5\n7\n7.5\n8\n-8\n-4\n0\n4\n8\nground truth\nFigure 10: Visualization of reconstruction performance for dense BCAE models.\ndetailed in Section 4.2. The encoder size is measured by trainable parame-\nters. The throughput is measured by the number of TPC wedges processed\nper second.\nWe also plot the dependence of reconstruction accuracy on occupancy\nin Figure 8 for L1 and PSNR and in Figure 9 for recall and precision. We\ncan observe that the reconstruction accuracy metrics (L1, recall, and preci-\nsion) of the dense BCAE models depend linearly on occupancy with a strong\ncorrelation. On the contrary, in a wide range of occupancy where the TPC\nwedges are the most populated, the sparse model BCAE-VS’s performance\nis correlated weakly with occupancy and maintains a superior performance.\nHowever, towards the two ends of the occupancy distribution, the BCAE-VS\nstarts to fail. From Figure 9, we can observe that the failure of BCAE-VS is\n23\n\ndriven by a drastic decline in recall for extremely high-occupancy input and\nby precision for low-occupancy ones.\nIn Figure 10, we compare BCAEs’ reconstruction performance qualitatively\non two TPC wedges. The difference is calculated by subtracting reconstruc-\ntion from the ground truth. From the comparison, we can make two ob-\nservations. First, dense BCAEs (BCAE-2D, BCAE-HT, BCAE++) have difficulty\nreconstructing a track at its correct location while BCAE-VS conserves the\ntrack location even in densely packed regions. This is evident with the cloud\nof blue and red dots and the lack thereof in the plot of difference.\nSec-\nond, dense BCAEs under-perform in input region with higher occupancy while\nBCAE-VS maintains accurate reconstruction over a wider range of occupancy.\n4.2. Compression Ratio\nThe compression ratio of the algorithm depends on data precision and\nthe format of storage of the input and the code.\nOn the input side, raw ADC values from TPC are collected as 10-bit\nintegers. However, since most commonly used computing platforms cannot\nhandle 10-bit integers, we consider input ADC values as 16-bits floats. For\nthe input size, we follow the convention in the dense BCAEs works [15, 14]\nand define it as the size of the zero-padded regularly shaped tensors.\nWith respect to the precision of the code, although we train all models\nin full precision (float32), it is shown in [15] that the difference in accuracy\nbetween reconstruction from full-precision compressed data and those down-\ncast to float16 is negligible. Similar observation can also be made for BCAE-VS\naccording to Table 2. Hence, compressed data is saved in float16 format for\nall models.\n24\n\nTable 2: Reconstruction accuracy with full (float32) and half (float16) precision code.\nprecision\nL1\nL2\nPSNR\nrecall\nprecision\nhalf\n.028394\n.088652\n26.000\n.988447\n.995573\nfull\n.028415\n.088818\n25.996\n.988419\n.995569\nSince a dense encoder output a code as a regularly shaped tensor, the\ncompression ratio of a dense BCAE can be calculated directly as the ratio\nof the input size to the code size. However, since the code produced by the\nsparse encoder of BCAE-VS is a sparse tensor, to compute a compression ratio\ncomparable to those of the dense models, we need the following conversion.\nAssume that the Coordinate List (COO) format is used for saving the output\nfrom the sparse encoder of BCAE-VS. We need to save the location and value\nof all entries in v whose corresponding value in p is larger than a (random)\nthreshold. Since a TPC wedge has shape (16, 192, 249), we need 4 bit to\nspecify the index of in the first dimension, and 8 bits each for the second\nand last dimensions. Hence we need (4 + 8 + 8) + 16 = 36 bits to save one\nentry in v. Suppose the occupancy of the wedge is o and a fraction k of the\nentries in v will be retained, the compression ratio C for this wedge can be\ncalculated using the following formula\nC =\n16bits × input size\n36bits × input size × o × k =\n4\n9ok\n(3)\nOver the test split of the dataset, on average a fraction of .133 of the\nsignal voxels is retained for each wedge. The averaged compression ratio is\n33.9 with wedge-wise compression ratio calculated using Equation 3. The left\npanel of Figure 11 shows compression ratio and retention ratio as a function\n25\n\n0.05\n0.1\n0.15\n0.2\n0.25\n20\n40\n60\n80\n100\noccupancy\ncompression ratio\ncompression ratio, mean=33.9\n0.05\n0.1\n0.15\n0.2\n0.25\n0.11\n0.12\n0.13\n0.14\n0.15\nretention fraction\nretention fraction, mean=.133\nFigure 11: Compression ratio, retention fraction (left), and importance assign-\nment to the signal voxels (right).\nof occupancy. While the dependency of retention on occupancy is relatively\nweak, BCAE-VS tends to retain more signal voxels at higher occupancy. This\nbehavior is desirable, as more voxels need to be preserved for accurate re-\nconstruction when trajectories are closer together. The compression ratio,\non the other hand, exhibits a clear inverse relationship to occupancy. These\nobservations demonstrated BCAE-VS’s ability to dynamically adjust key-point\nidentification and achieve a variable compression ratio.\nWe also visualize the importance score assignment by the encoder of\nBCAE-VS on isolated tracks in the right panel of Figure 11.\nWe can see\nthat the assignment is polarized – most of the probabilities are close to zero\nand only a small fraction of signal voxels along the boundary of a track are\nassigned with a high importance score.\n4.3. Throughput\nUnlike dense convolutions, the inference speed of sparse convolution de-\npends on occupancy. As discussed in Section 3.2.1, lower occupancy leads to\n26\n\n0.05\n0.1\n0.15\n0.2\n0.25\n2,000\n4,000\n6,000\n8,000\n10,000\n12,000\n14,000\n16,000\n18,000\noccupancy\nthroughput\nBCAE-2D, (9594, 16)\nBCAE-HT, (9603, 16)\nBCAE++,\n(3176, 4)\nBCAE-VS, (5594, 64)\n0\n50\n100\n150\ncount\navg. occupancy=.108\n0 100 200\ncount\nFigure 12: Throughput comparison. The lines of BCAE-HT and BCAE-2D overlap. The\nnumbers in the parenthesis are the TPC wedges compressed per second and the batch size\nused to obtain the throughput.\nfewer matrix multiplications. Therefore, in an occupancy regime where the\nspeed gain reduced matrix multiplications outweigh the overhead of com-\nputing the kernel map, BCAE-VS may achieve lower compression latency.\nTo test this hypothesis, we conducted a throughput study using a NVIDIA\nRTXTM 6000 ADA graphic card with driver 555.42.02 and PyTorch 2.4.0\ncompiled with CUDA 12.23 and demonstrate the result in Figure 12. Here,\nthroughput is measured as the number of TPC wedges compressed per sec-\nond.\n3A fix build of MinkowskiEngine for CUDA 12.2 can be found at https://github.\ncom/NVIDIA/MinkowskiEngine/pull/567\n27\n\nThe throughput for each dense BCAE is averaged from 1000 batches with\na warm-up of 10 batches. We evaluated throughput under the assumption\nthat the input data is already stored in GPU memory. Figure 12 presents\nthe peak throughput of each dense model, with the corresponding batch size\nthat achieved the peak noted in the legend. All the dense BCAEs are compiled\nusing torch.compile to optimize the inference speed.\nFor BCAE-VS, each test sample was repeated 64 times to generate the\ninput. We assume the input batch of TPC wedges is already in GPU memory\nin a sparse-encoded format, ready for direct processing by the sparse network.\nThis assumption is reasonable as the TPC data is collected and transferred\nin sparse encoding through the data acquisition pipeline.\nThe throughput is calculated on the test split of the dataset consisting of\n6288 TPC wedges. Although BCAE-VS is slower than BCAE-2D and BCAE-HT\non average, it is significantly faster in the lower occupancy regime. Moreover,\nsparse models can process sparsely encoded TPC data directly, eliminating\nthe overhead of padding sparse encoding to a full tensor necessitated by a\ndense model. In many operational modes of collider tracking detectors, the\noccupancy can be significantly more sparse, e.g. on the order of 10−3-10−2 for\nthe TPC in the sPHENIX experiment during proton-proton collisions, which\nis much lower than in the high-pileup central Au+Au dataset studied in this\nwork. The advantages of sparse convolution become increasingly pronounced\nin such conditions.\n28\n\n5. Conclusion\nWe introduced BCAE-VS, an algorithm designed to enhance the compres-\nsion of sparse 3D TPC data from high-energy particle accelerators. BCAE-VS\noffers a variable compression ratio based on data occupancy, improving both\ncompression efficiency and reconstruction accuracy. Our results show a 75%\nincrease in reconstruction accuracy with a 10% higher average compression\nratio compared to the leading BCAE model. By utilizing sparse convolution,\nBCAE-VS achieves an outstanding throughput within the operational occu-\npancy range of sPHENIX. While the current model demonstrates promising\nperformance, several limitations have been identified. We offer recommen-\ndations for future work to address these constraints and further enhance the\nmodel’s capabilities.\n• Exact control of retention fraction: Although hyperparameters such\nas Lcomp and lprob (see Section 3.2.4) can influence BCAE-VS’s the re-\ntention fraction, we lack a direct control of it. A potential solution\ncould involve thresholding by quantile of the importance score output\np (see Section 3.2.1), allowing a fixed fraction of signals to be saved.\n• Performance at extreme occupancies:\nWhile BCAE-VS performs well\nfor typical occupancy levels in the dataset, it experiences a significant\ndecrease in recall at high occupancy and in precision at low occupancy.\nFor deployment, we need an algorithm that remains reliable even during\nextreme events.\n• Precision and quantization for throughput: The sparse convolution li-\nbrary MinkowskiEngine does not support half-precision (float16) op-\n29\n\nerations. As demonstrated in [15], certain hardware can achieve sub-\nstantial speedups with negligible performance loss using half-precision\ncomputation. Future work will explore more aggressive quantization\nschemes, such as int8, to further speed up inference.\n• Hardware support for sparse convolution: Unlike dense convolution,\nsparse convolution engines are not commonly supported by innovative\nAI hardware accelerators. This poses additional challenges for deploy-\ning BCAE-VS on such hardware compared to dense convolutional models.\n• Downstream task: Currently, the performance of BCAE-VS is evaluated\nwith standard classification/regression metrics. However, being a com-\npression algorithm of ADC readout, BCAE-VS’s performance needs to\nbe tested on downstream tasks such as trajectory reconstruction and\nidentification, which will be part of our future work.\n• Noisy TPC data from real experiment: In this study, we used simu-\nlated collision data. However, the TPC data from real experiments will\ncontain noises caused by, for example, common ground noise pickup.\nHence, BCAE-VS needs to be retrained and tested with realistic noisy\nconditions in the actual TPC data and adjusted to incorporate noise-\nfiltering mechanisms.\nAcknowledgment\nWe thank the sPHENIX collaboration for access to the simulated dataset,\nwhich was used in the training and validation of our algorithm. We also thank\n30\n\nthe collaboration for the valuable interaction with the sPHENIX collabora-\ntion on this work. This work was supported by the Laboratory Directed Re-\nsearch and Development Program of Brookhaven National Laboratory, which\nis operated and managed for the U.S. Department of Energy Office of Science\nby Brookhaven Science Associates under contract No. DE-SC0012704.\nAppendix A. BCAE-VS neural network\nThe BCAE-VS network consists of a sparse encoder implemented with\nsparse convolution provided by the NVIDIA MinkowskiEngine library and\ntwo decoders implemented with normal dense convolution. The input to the\nneural network is one TPC wedge treated as a 4D array with 1 channel and\n3 spatial dimensions.\nThe sparse encoder has five 3D convolution layers, all with 2 output\nchannels, kernel size 3, stride 1, and no padding. The dilation parameters\nof the convolutions are 1, 2, 4, 2, 1. There is a rectified linear unit (ReLU)\nactivation between two successive convolutional layers. The output activation\nis Sigmoid.\nThe two decoders share the same structure. Each one of them has eight\n(dense) convolutional blocks with one convolution (kernel size 3, padding 1,\nstride 1, dilation 1) followed by a leaky ReLU with negative slope .1. The\nnumber of output channels are (16, 16, 16, 8, 8, 8, 4, 2). A final 1 × 1 convo-\nlution maps the number of channels back to 1. The regression decoder Dreg\nhas no output activation while the segmentation decoder Dseg has Sigmoid\nas output activation.\nNo normalization is used in the BCAE-VS model.\n31\n\nAppendix B. BCAE-VS training\nThe training is done for 100 epochs with a batch size of 4 and 2000\nbatches per epoch.\nFor optimization, we used the Adam optimizer with\ndecoupled weight decay (AdamW) with an initial learning rate 0.001, (β1, β2) =\n(0.9, 0.999), and weight decay 0.01. We kept the learning rate as initialized\nfor the first 20 epochs and decreased it to the 95% of the previous value every\n20 epochs thereafter.\nReferences\n[1] L. Evans, P. Bryant, LHC machine 3 (8) S08001.\ndoi:10.1088/\n1748-0221/3/08/S08001.\nURL https://dx.doi.org/10.1088/1748-0221/3/08/S08001\n[2] sPHENIX Collaboration, Technical design report: sphenix experiment\nat rhic (2019).\nURL https://indico.bnl.gov/event/5905/\n[3] H. J. Hilke, Time Projection Chambers, Rep. Prog. Phys. 73 (11) (2010)\n116201. doi:10.1088/0034-4885/73/11/116201.\nURL https://cds.cern.ch/record/1302071\n[4] sPHENIX Collaboration, sphenix 2023 beam use proposal (2023).\nURL https://indico.bnl.gov/event/20373/\n[5] J. C. Bernauer, et al., Scientific computing plan for the ECCE detector\nat the Electron Ion Collider, Nucl. Instrum. Meth. A 1047 (2023) 167859.\narXiv:2205.08607, doi:10.1016/j.nima.2022.167859.\n32\n\n[6] R. Abdul Khalek, et al., Science Requirements and Detector Concepts\nfor the Electron-Ion Collider: EIC Yellow Report, Nucl. Phys. A 1026\n(2022) 122447. arXiv:2103.05419, doi:10.1016/j.nuclphysa.2022.\n122447.\n[7] D. Rohr, Gpu-based reconstruction and data compression at alice during\nlhc run 3, in: EPJ Web of Conferences, Vol. 245, EDP Sciences, 2020,\np. 10005.\n[8] S. Di, F. Cappello, Fast error-bounded lossy HPC data compression with\nSZ, in: 2016 IEEE International Parallel and Distributed Processing\nSymposium (IPDPS), pp. 730–739, ISSN: 1530-2075.\ndoi:10.1109/\nIPDPS.2016.11.\n[9] D. Tao, S. Di, Z. Chen, F. Cappello, Significantly improving lossy com-\npression for scientific data sets based on multidimensional prediction\nand error-controlled quantization, in: 2017 IEEE International Parallel\nand Distributed Processing Symposium (IPDPS), pp. 1129–1139, ISSN:\n1530-2075. doi:10.1109/IPDPS.2017.115.\n[10] Y. Liu, S. Di, K. Zhao, S. Jin, C. Wang, K. Chard, D. Tao, I. Foster,\nF. Cappello, Optimizing error-bounded lossy compression for scientific\ndata with diverse constraints 33 (12) 4440–4457, conference Name: IEEE\nTransactions on Parallel and Distributed Systems. doi:10.1109/TPDS.\n2022.3194695.\n[11] P. Lindstrom, Fixed-rate compressed floating-point arrays 20 (12) 2674–\n33\n\n2683, conference Name: IEEE Transactions on Visualization and Com-\nputer Graphics. doi:10.1109/TVCG.2014.2346458.\n[12] M. Ainsworth, O. Tugluk, B. Whitney, S. Klasky, Multilevel techniques\nfor compression and reduction of scientific data—the multivariate case\n41 (2) A1278–A1303, publisher:\nSociety for Industrial and Applied\nMathematics. doi:10.1137/18M1166651.\nURL https://epubs.siam.org/doi/10.1137/18M1166651\n[13] X. Liang, B. Whitney, J. Chen, L. Wan, Q. Liu, D. Tao, J. Kress,\nD. Pugmire, M. Wolf, N. Podhorszki, S. Klasky, MGARD+: Optimizing\nmultilevel methods for error-bounded scientific data reduction 71 (7)\n1522–1536, conference Name: IEEE Transactions on Computers. doi:\n10.1109/TC.2021.3092201.\n[14] Y. Huang, Y. Ren, S. Yoo, J. Huang, Efficient data compression for\n3d sparse tpc via bicephalous convolutional autoencoder, in:\n2021\n20th IEEE International Conference on Machine Learning and Applica-\ntions (ICMLA), 2021, pp. 1094–1099. doi:10.1109/ICMLA52953.2021.\n00179.\n[15] Y. Huang, Y. Ren, S. Yoo, J. Huang, Fast 2d bicephalous convolutional\nautoencoder for compressing 3d time projection chamber data, in: Pro-\nceedings of the SC’23 Workshops of The International Conference on\nHigh Performance Computing, Network, Storage, and Analysis, 2023,\npp. 298–305.\n[16] D. Avola, L. Cinque, A. Fagioli, G. L. Foresti, A. Fragomeni, D. Pan-\n34\n\nnone, 3d hand pose and shape estimation from rgb images for keypoint-\nbased hand gesture recognition, Pattern Recognition 129 (2022) 108762.\n[17] Y. Lin, W. Chi, W. Sun, S. Liu, D. Fan, Human action recognition\nalgorithm based on improved resnet and skeletal keypoints in single\nimage, Mathematical Problems in Engineering 2020 (1) (2020) 6954174.\n[18] Y. You, Y. Lou, C. Li, Z. Cheng, L. Li, L. Ma, C. Lu, W. Wang, Key-\npointnet: A large-scale 3d keypoint dataset aggregated from numerous\nhuman annotations, in: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 13647–13656.\n[19] O. Moskvyak, F. Maire, F. Dayoub, M. Baktashmotlagh, Keypoint-\naligned embeddings for image retrieval and re-identification, in: Pro-\nceedings of the IEEE/CVF winter conference on applications of com-\nputer vision, 2021, pp. 676–685.\n[20] X.-N. Wang, M. Gyulassy, HIJING: A Monte Carlo model for multiple\njet production in p p, p A and A A collisions, Phys. Rev. D 44 (1991)\n3501–3516. doi:10.1103/PhysRevD.44.3501.\n[21] J. Allison, et al., Recent developments in Geant4, Nucl. Instrum. Meth.\nA 835 (2016) 186–225. doi:10.1016/j.nima.2016.06.125.\n[22] sPHENIX, sphenix software repositories https://github.com/sphenix-\ncollaboration (2019).\nURL https://github.com/sPHENIX-Collaboration\n[23] Y. Alanazi, N. Sato, T. Liu, W. Melnitchouk, M. P. Kuchera,\n35\n\nE. Pritchard, M. Robertson, R. Strauss, L. Velasco, Y. Li, Simula-\ntion of electron-proton scattering events by a feature-augmented and\ntransformed generative adversarial network (fat-gan), arXiv preprint\narXiv:2001.11103 (2020).\n[24] B. Hashemi, N. Amin, K. Datta, D. Olivito, M. Pierini, Lhc analysis-\nspecific datasets with generative adversarial networks, arXiv e-prints\n(2019) arXiv–1901.\n[25] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Doll´ar, Focal loss for dense\nobject detection, in: Proceedings of the IEEE international conference\non computer vision, 2017, pp. 2980–2988.\n36",
    "pdf_filename": "Variable_Rate_Neural_Compression_for_Sparse_Detector_Data.pdf"
}