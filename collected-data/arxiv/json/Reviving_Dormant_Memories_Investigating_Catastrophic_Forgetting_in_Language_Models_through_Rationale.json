{
    "title": "Reviving Dormant Memories Investigating Catastrophic Forgetting in Language Models through Rationale",
    "context": "Although substantial efforts have been made to mitigate catastrophic forgetting in contin- ual learning, the intrinsic mechanisms are not well understood. In this paper, we discover that when a forgetting model passively receives an externally provided partial appropriate ratio- nale, its performance on the forgotten task can be restored. Furthermore, by simply adding a task-agnostic prefix to the original instruc- tion, the forgetting model can actively gener- ate an appropriate rationale to reach the cor- rect answer. These findings suggest that the model does not actually “forget” the task knowl- edge; instead, the degraded performance can be attributed to the failure of the original in- structions in guiding the model to generate the appropriate rationales. Based on this insight, we propose the Rationale-Guidance Difficulty metric to evaluate how effectively a given in- struction guides the model in generating ap- propriate rationales. We apply this metric to optimize the allocation of replay data in replay- based continual learning algorithm. Experimen- tal results demonstrate that our data allocation method effectively mitigates catastrophic for- getting and maintains better model plasticity simultaneously across models. 1 While large language models (LLMs) acquire ex- tensive knowledge during pre-training (Brown et al., 2020; Touvron et al., 2023a; Yang et al., 2023), in reality, both knowledge and data are dy- namic, necessitating that models adapt to different tasks or domains continuously (Zheng et al., 2024). Accordingly, continual learning can assist models in acquiring new knowledge incrementally, thereby enhancing their capabilities over time. However, a key challenge models face during continual learn- ing is “catastrophic forgetting,” which refers to the phenomenon where a model’s performance on old * Corresponding author Instruction of Task 1 Knowledge of Task 1 Appropriate Rationale Probing by CoT Original-LLM Correct Answer Train on Task 2 Knowledge of Task 1 Knowledge of Task 2 Instruction of Task 1 Correct Answer Incorrect Answer Drift Appropriate Rationale Irrelevant Rationale ‘Forgetting’-LLM Task- Agnostic prefix passively provided Rationale Add Prefix help the model actively generate Rationale Pseudo-Forgetting Do not Forget Do not Forget Do “Forget” Correct Guidance Incorrect Guidance Exp1 Exp2 Experiments internal mechanism Observation conclusion Whether the prompt can guide the generation of appropriate rationale? Figure 1: Methodology used in our experiments. 1. We leverage CoT to probe the parameterized knowl- edge embedded in the model explicitly. 2. We evalu- ate the performance of a forgetting model under three situations: “instruction-only prompting,” “instruction- only prompting with externally provided rationale,” and “instruction-only prompting with a task-agnostic pre- fix.” 3. We find that in the latter two situations, the model could actively generate appropriate rationale, re- covering task performance. Thus, we conclude that the model does not truly forget the old knowledge; instead, the original instructions are insufficient in guiding the generation of appropriate rationale, resulting in “pseudo- forgetting.” tasks declines after learning new tasks (McCloskey and Cohen, 1989; Goodfellow et al., 2014). Despite the numerous methods proposed to mit- igate catastrophic forgetting (Wang et al., 2024, 2023a; Zhao et al., 2024) (discussed in Section 2.2), few studies have begun to investigate the intrinsic mechanisms underlying this phenomenon. Kotha et al. (2024) propose the “task inference” hy- pothesis, which suggests that fine-tuning a model changes which of its abilities it tends to use, rather than causing it to actually forget those abilities. However, this hypothesis has been primarily vali- dated on synthetic datasets rather than directly on arXiv:2411.11932v1  [cs.LG]  18 Nov 2024",
    "body": "Reviving Dormant Memories: Investigating Catastrophic Forgetting in\nLanguage Models through Rationale-Guidance Difficulty\nHuashan Sun\nYang Gao*\nSchool of Computer Science and Technology, Beijing Institute of Technology\n{hssun, gyang}@bit.edu.cn\nAbstract\nAlthough substantial efforts have been made\nto mitigate catastrophic forgetting in contin-\nual learning, the intrinsic mechanisms are not\nwell understood. In this paper, we discover\nthat when a forgetting model passively receives\nan externally provided partial appropriate ratio-\nnale, its performance on the forgotten task can\nbe restored. Furthermore, by simply adding\na task-agnostic prefix to the original instruc-\ntion, the forgetting model can actively gener-\nate an appropriate rationale to reach the cor-\nrect answer. These findings suggest that the\nmodel does not actually “forget” the task knowl-\nedge; instead, the degraded performance can\nbe attributed to the failure of the original in-\nstructions in guiding the model to generate the\nappropriate rationales. Based on this insight,\nwe propose the Rationale-Guidance Difficulty\nmetric to evaluate how effectively a given in-\nstruction guides the model in generating ap-\npropriate rationales. We apply this metric to\noptimize the allocation of replay data in replay-\nbased continual learning algorithm. Experimen-\ntal results demonstrate that our data allocation\nmethod effectively mitigates catastrophic for-\ngetting and maintains better model plasticity\nsimultaneously across models.\n1\nIntroduction\nWhile large language models (LLMs) acquire ex-\ntensive knowledge during pre-training (Brown\net al., 2020; Touvron et al., 2023a; Yang et al.,\n2023), in reality, both knowledge and data are dy-\nnamic, necessitating that models adapt to different\ntasks or domains continuously (Zheng et al., 2024).\nAccordingly, continual learning can assist models\nin acquiring new knowledge incrementally, thereby\nenhancing their capabilities over time. However, a\nkey challenge models face during continual learn-\ning is “catastrophic forgetting,” which refers to the\nphenomenon where a model’s performance on old\n* Corresponding author\nInstruction \nof Task 1\nKnowledge \nof Task 1 \nAppropriate\nRationale\nProbing by CoT\nOriginal-LLM\nCorrect \nAnswer\nTrain on \nTask 2\nKnowledge of Task 1 \nKnowledge of Task 2 \nInstruction \nof Task 1\nCorrect \nAnswer\nIncorrect \nAnswer\nDrift\nAppropriate\nRationale\nIrrelevant\nRationale\n‘Forgetting’-LLM\nTask-\nAgnostic \nprefix\npassively\nprovided \nRationale\nAdd Prefix help the model actively generate Rationale\nPseudo-Forgetting\nDo not Forget\nDo not Forget\nDo “Forget”\nCorrect Guidance\nIncorrect Guidance\nExp1\nExp2\nExperiments\ninternal mechanism\nObservation conclusion\nWhether the prompt can guide the generation of appropriate rationale? \nFigure 1: Methodology used in our experiments. 1.\nWe leverage CoT to probe the parameterized knowl-\nedge embedded in the model explicitly. 2. We evalu-\nate the performance of a forgetting model under three\nsituations: “instruction-only prompting,” “instruction-\nonly prompting with externally provided rationale,” and\n“instruction-only prompting with a task-agnostic pre-\nfix.” 3. We find that in the latter two situations, the\nmodel could actively generate appropriate rationale, re-\ncovering task performance. Thus, we conclude that the\nmodel does not truly forget the old knowledge; instead,\nthe original instructions are insufficient in guiding the\ngeneration of appropriate rationale, resulting in “pseudo-\nforgetting.”\ntasks declines after learning new tasks (McCloskey\nand Cohen, 1989; Goodfellow et al., 2014).\nDespite the numerous methods proposed to mit-\nigate catastrophic forgetting (Wang et al., 2024,\n2023a; Zhao et al., 2024) (discussed in Section 2.2),\nfew studies have begun to investigate the intrinsic\nmechanisms underlying this phenomenon. Kotha\net al. (2024) propose the “task inference” hy-\npothesis, which suggests that fine-tuning a model\nchanges which of its abilities it tends to use, rather\nthan causing it to actually forget those abilities.\nHowever, this hypothesis has been primarily vali-\ndated on synthetic datasets rather than directly on\narXiv:2411.11932v1  [cs.LG]  18 Nov 2024\n\nnatural language datasets and LLMs. Similarly,\nJiang et al. (2024) investigate forgetting in LMs\nthrough the lenses of instruction-following and\ntask-related knowledge, proposing that forgetting\nstems from a decline in instruction-following capa-\nbilities rather than an actual loss of task-related\nknowledge.\nHowever, the expression of “task-\nrelated knowledge” varies between probing and\npractical use, leaving the conclusion insufficiently\nclarified.\nIn this work, we hypothesize that the model does\nnot truly forget task knowledge; rather, its perfor-\nmance degradation on previous tasks is primarily\nattributable to the original instructions’ inability to\nguide the generation of relevant knowledge. The\nfirst question that naturally arises for a forgetting\nmodel is: How does the model perform when pas-\nsively provided with appropriate knowledge, such\nas the rationale of the Chain of Thought (CoT)?\nSpecifically, for a forgetting model, we concatenate\nk part of the rationale (ground truth) to the origi-\nnal instruction (see Figure 3 where k = 0.1) and\nevaluate the model’s performance. Our findings\nreveal that for models of varying sizes, providing\neven a portion of the appropriate rationale as guid-\nance allows the model’s performance to recover\n(shown in Figure 2). Moreover, as k increases, the\nmodel’s performance can recover to pre-forgetting\nlevels. There are two potential explanations: (1)\nthe instructions fail to guide the generation of the\ncorresponding knowledge, or (2) the knowledge\nhas truly been forgotten. To investigate the under-\nlying reason further, we pose the second question:\nCan we help the model to actively generate the ap-\npropriate knowledge by modifying its prompt? To\nexplore this, we add a Task-Agnostic Prefix (Ye\net al., 2024) before the original instruction (see\nFigure 4) and assess the performance of the forget-\nting model. This approach ensures that knowledge\nis generated and expressed in a same manner be-\nfore and after forgetting. The experimental results\n(shown in Figure 5) indicate that, across models\nof various scales, the task-agnostic prefix can fa-\ncilitate the forgetting model in generating relevant\nknowledge, thereby partially restoring its perfor-\nmance on forgotten tasks. The above experiments\nprovide direct evidence for our hypothesis regard-\ning LLMs: the model’s forgetting primarily stems\nfrom the original instructions’ inability to facilitate\nappropriate rationales, rather than an actual loss of\ntask-related knowledge.\nBuilding on the above conclusion, we propose\nthe Rationale-Guidance Difficulty metric to evalu-\nate how effectively a given instruction can guide a\nmodel in generating an appropriate rationale. Uti-\nlizing this metric, we dynamically allocate replay\ndata for each previous task to optimize data utiliza-\ntion. Experiments conducted across various scales\nand datasets demonstrate the effectiveness of our\napproach.\nOur contributions can be summarized as follows:\n1. We directly verify on LLMs that task-related\nknowledge, also expressed in the same format\nas in instruction-following scenarios, is not\nactually forgotten. Instead, the deterioration\nin model performance arises from the inabil-\nity of the original task instructions to facilitate\nthe generation of appropriate rationale (Sec-\ntion 3).\n2. We propose the Rationale-Guidance Difficulty\nmetric and implement it for data allocation\nwithin a replay-based framework (Section 4).\nExperiments validate the effectiveness of our\napproach in mitigating catastrophic forgetting\nin the model (Section 5).\n2\nRelated Work\n2.1\nMechanism of catastrophic forgetting\nCatastrophic forgetting refers to the tendency of\nmodels to lose previously acquired knowledge\nwhen learning new tasks (McCloskey and Cohen,\n1989; Goodfellow et al., 2014), a challenge that has\nbeen widely studied with numerous attempts to mit-\nigate it (Section 2.2). Nevertheless, a substantial\ngap persists in comprehending the internal mecha-\nnisms that lead to these knowledge losses. Kotha\net al. (2024) hypothesize that fine-tuned models\ndo not “forget” prior abilities but rather “suppress”\nthem. They suggest that models first perform “task\ninference” before applying the relevant capability,\nand fine-tuning biases this inference towards tasks\naligned with the fine-tuning distribution, thereby\nsuppressing performance on other prior capabilities.\nJiang et al. (2024) hypothesize that a model’s task\nability comprises both understanding task-related\nknowledge and following instructions. Their exper-\niments reveal that “forgetting” in models is primar-\nily due to a decline in the ability to follow instruc-\ntions, rather than a loss of knowledge.\n\nQwen2-0.5B\nMistral-7B\nLlama2-7B\nLlama2-13B\nFigure 2: Changes in the model’s task performance after forgetting when k parts of the appropriate rationale are\nprovided. 1. A forgetting model can regenerate “forgotten knowledge” and gradually recover its “pre-forgetting”\ntask performance when passively guided with partial “appropriate rationale.” 2. the degree of recovery of the\n“forgotten knowledge” is related to the task difficulty and the scale of the model\n2.2\nTraditional methods in continual learning\nContinual Learning (CL) seeks to progressively ac-\nquire knowledge from a sequence of tasks while\nretaining what has been previously learned (Zheng\net al., 2024). Numerous continual learning methods\nhave been proposed to address catastrophic forget-\nting: (1) Regularization-based methods constrain\nthe features learned from previous tasks (Zhang\net al., 2023a; Huang et al., 2021) or penalize\nchanges to weights critical for those tasks (Zhou\nand Cao, 2021; Aljundi et al., 2018), ensuring that\nnew learning minimally interferes with prior knowl-\nedge thus maintaining performance on earlier tasks.\nFor example, O-LoRA (Wang et al., 2023a) miti-\ngates catastrophic forgetting by learning tasks in\ndifferent (low-rank) vector subspaces (LoRA pa-\nrameters) using an additional orthogonal parame-\nter loss. (2) Architecture-based methods aim to\nreduce interference between new and prior tasks\nby either dynamically increasing the model’s ca-\npacity (Zhao et al., 2024) or isolating the existing\nweights (Hu et al., 2024). SAPT (Zhao et al., 2024)\naligns parameter-efficient tuning blocks with se-\nlection modules via a shared attention mechanism,\neffectively tackling both catastrophic forgetting and\nknowledge transfer. (3) Replay-based methods re-\ntain a small subset of prior training examples or\nfeatures and revisit them when a new task is intro-\nduced (Wang et al., 2024; Guo et al., 2024; Liu\net al., 2021). Rather than retaining the original\ndata, PCLL (Zhao et al., 2022), LFPT5 (Qin and\nJoty, 2022) and SSR (Huang et al., 2024) generate\npseudo data samples that mimic the old data, either\nby leveraging the model itself or through a separate\ngenerative model.\n3\nImpact of Appropriate Rationale on\n“Pseudo-Forgetting”\nRecall our assumption is that the model does not\ntruly forget; rather, after learning new tasks, the in-\nstructions for the old tasks fail to “guide” the model\nin generating an “appropriate reasoning process,”\nwhich ultimately manifests as apparent “forgetting”\nof the old tasks. Thus, we aim to investigate the\nfollowing two questions:\n1. Q1: How does a forgetting model perform\nwhen passively provided with externally sup-\nplied “appropriate rationale”?\n2. Q2: Can changing the prompt method enable\nthe model to generate the “appropriate ratio-\nnale” actively?\n\n<s>[INST] Task: What is the logical relationship (contradiction, entailment \nor neutral) between the \"sentence 1\" and the \"sentence 2\"? Choose one from \nthe option.\nOPTIONS:\n- neutral\n- entailment\n- contradiction\nsentence 1: Case Study Evaluations.\nsentence 2: Case Study preparations.\nAnswer: [/INST] The sentence 1 'Case Study Evaluations' implies a\nFigure 3: Prompt example with additional k part of\n“appropriate” rationale guidance (k = 0.1). The black\nparts are the original instruction; The green parts are the\nadded part of the “appropriate rationale”, which does\nnot contain information directly related to the answer.\n3.1\nAppropriate Rationale Meeting\n“Pseudo-Forgetting”\nIn this section, we address Q1. We selected the\nmodel from the final stage of sequential learning\nand chose the test set of tasks with a high forget-\nting rate for the experiment. To offer “appropriate\nknowledge” guidance, as shown in Figure 3, we\nappend k part of the rationale directly after the\noriginal instruction. It is important to note that the\nadded portion with small k does not directly pro-\nvide task-specific or answer-relevant information\nbut instead serves to guide the model in shaping\nthe overall direction of its predictions. The detailed\ndescription of the experimental data and model is\nprovided in Section 5.1.\nResults and Analysis\nResults are presented in\nFigure 2.\nFirstly, when a “forgotten” model passively\nreceives partial guidance on “appropriate ratio-\nnale,” it can regenerate the “forgotten knowl-\nedge” and gradually restore its “pre-forgetting”\ntask performance. Specifically, across various\n“forgotten” tasks, performance improves with in-\ncreasing k-values, suggesting that the model’s\nknowledge remains intact. The issue lies in the\noriginal instructions’ inability to elicit the appropri-\nate reasoning processes. With some “appropriate\nguidance,” the model can access the relevant task\nknowledge and complete the task.\nSecondly, the degree of recovery of the\nmodel’s “task memory” is related to the task\ndifficulty and the scale of the model. For in-\nstance, in the CB task, Mistral-7B returns to its\npre-forgetting performance level at k = 0.2, while\nthe MNLI task requires k = 0.4 to achieve the\nsame recovery level. Meanwhile, to restore the pre-\nforgetting performance level for Qwen2-0.5B and\nLlama2-13B, values of k = 0.6 and k = 0.1 are\nrequired, respectively.\n#Question#:\nIn this task, you are given a dialogue from a conversation between an agent \nand a customer. Your task is to determine the speaker of the dialogue. \nAnswer with \"agent\" or \"customer\".\nInput: I have successfully booked your ticket with flight-1017, have a safe \njourney.\n#Ratinale#:\nThe input consists of a statement confirming a ticket booking and wishing \nthe customer a safe journey. This type of communication is typically made \nby a service provider, indicating that the speaker is the \"agent\".\n#Answer#:\nAgent\n#Question#:\n[Question-2]\n#Ratinale#:\n[Ratinale-2]\n#Answer#:\n[Answer-2]\n…\n#Question#:\nTask: What is the logical relationship (contradiction, entailment or neutral) \nbetween the \"sentence 1\" and the \"sentence 2\"? Choose one from the option.\nOPTIONS:\n- neutral\n- entailment\n- contradiction\nsentence 1: Case Study Evaluations.\nsentence 2: Case Study preparations.\n#Ratinale#:\n{rationale generated by model}\n#Answer#:\n{answer generated by model}\nFigure 4: Prompt example with Task-Agnostic Prefix.\nContext template (purple) and task-independent demon-\nstrations (green) act as the prefix to the original instruc-\ntion (black), formatting the final prompt that guides the\nmodel in generating rationale and answer using parame-\nterized task knowledge.\n3.2\nReviving Dormant Knowledge via\nTask-Agnostic Prefix Prompting\nIn the experiment in Section 3.1, the model pas-\nsively received some “appropriate rationale” as\nguidance, allowing it to recover its performance\ngradually. To further verify and demonstrate our\nhypothesis, we address Q2 in this section. We\nutilize Task-Agnostic Prefix Prompting (Ye et al.,\n2024), which enables the model to generate a rea-\nsoning process for a task based on parameterized\nknowledge. As illustrated in Figure 4, this method\nuses examples unrelated to the testing task to guide\nthe model in producing reasoning processes and an-\nswers through context-based learning. Notably, the\ngenerated rationale relies on parameterized knowl-\nedge rather than knowledge acquired from con-\ntext. This approach simply adds prefixes to the\noriginal instructions, making the detected knowl-\n\nFigure 5: Performance of the ‘forgetting’ model using Direct Instruction Prompting (Ins) versus Task-Agnostic\nPrefix Prompting (TAP). BF refers to Before Forgetting and AF to After Forgetting.\nedge more closely resemble the original ’forgotten\nknowledge’ in QA format, rather than adopting\nother formats (Jiang et al., 2024). This similarity\nhighlights that the knowledge itself is intact, and\nthe challenge lies in the differences in “guidance”\nmethods for generating an “appropriate rationale.”\nResults and Analysis\nResults are presented in\nFigure 5. Adding an appropriate prefix to the origi-\nnal instruction enables the model to generate an ap-\npropriate rationale and recover its performance on\nforgotten tasks. Specifically, for different forgetting\nmodels, a Task-Agnostic Prefix added before the\noriginal instructions partially restores performance\nacross various forgotten tasks. For optimal prefix\nselection, we employed a grid search to identify\nthe best demonstrations and demonstration count\nfor each task, highlighting the dependency of task\nrecovery on prefix design. We hypothesize that\nan optimal Task-Agnostic Prefix can restore per-\nformance to pre-forgetting levels. Notably, larger\nmodels, such as Llama2-13B, exhibit higher recov-\nery levels than smaller models like Llama2-7B and\nMistral-7B, suggesting that model size correlates\nwith resistance to “forgetting”.\nSummary\nExperimental results indicate that the\nperformance drop on previous tasks is primarily\nattributable to limitations in the prompting method.\nThe model has not truly forgotten task-specific\nknowledge; rather, the prompts fail to effectively\nguide the generation of the appropriate rationale,\ngiving the impression of forgotten knowledge.\n4\nReplay Based on Rationale-Guidance\nDifficulty\nWhile it is possible to recover the performance\nof a forgotten model using appropriate prompts,\nthis approach is less cost-effective. Building on\nour assumption, we argue that replay-based algo-\nrithms (Wang et al., 2024; Guo et al., 2024; Liu\net al., 2021) provide the most straightforward and\nefficient solution to mitigate catastrophic forgetting.\nIn this section, we first introduce an evaluation met-\nric, Rationale-Guidance Difficulty, to assess the\ndifficulty of guiding a model toward an ’appropri-\nate rationale’ for a given instruction. Based on\nthis metric, we dynamically allocate replay data for\neach previous task to optimize data utilization.\n4.1\nRationale-Guidance Difficulty\nThe Rationale-Guidance Difficulty (RGD) score\nfor a given data pair (x, r, y) is calculated as fol-\nlows:\nRGDθ(x, r, y) = PPLθ(r|x)\nPPLθ(r)\n(1)\nHere, x denotes the prompt, r represents the infer-\nence process, y is the answer, and θ refers to the\nparameters of the model being tested. PPLθ(r) re-\nflects the difficulty for the model to generate the ra-\ntionale r independently, while PPLθ(r|x) measures\nthe difficulty when generating r given the prompt x.\nThe RGD score indicates how well a given prompt\nx facilitates the generation of rationale r. A higher\nRGD score signifies greater difficulty for a prompt\nin guiding the model to produce the rationale, and\nvice versa. The calculation method of this metric\nfollows the Instruction Following Difficulty (IFD)\nscore proposed by Li et al. (2024b), though the\nIFD score was primarily used for fine-tuning data\nselection (Li et al., 2024a,b).\nFor a given previous task t, we estimate its RGD\nscore using a set of test data V :\nRGDt\nθ = Mean(RGDV )\n(2)\n\nMean(RGDV ) represents the mean and variance of\nthe RGD score on the test dataset V .\n4.2\nUnderstanding Forgetting via\nRationale-Guidance Difficulty\nForget\nmean\nstd\nAcc\nFalse\n39.77\n13.67\n78.40\nTrue\n45.71\n38.75\n60.00\nTable 1: RGDθ(x, r, y) scores (scaled by 1e-4) compari-\nson for the QQP task on Mistral-7B with and without\nforgetting.\nTable 1 shows the RGD scores for the Mistral-7B\nmodel on the QQP task, before and after forgetting.\nBefore forgetting, the model exhibits lower RGD\nscores, indicating that the instructions for the QQP\ntask easily guide the model to the relevant knowl-\nedge, resulting in better performance. However,\nafter forgetting, the RGD scores increase, suggest-\ning that the instructions become less effective at\nguiding the model to the relevant knowledge, lead-\ning to a decline in task performance.\n4.3\nDynamic Replay Strategy\nWang et al. (2024) propose determining the propor-\ntion of replay data based on task similarity, sug-\ngesting that tasks with greater differences from\nthe current learning task should have more replay\ndata. In contrast, we argue that the model’s capac-\nity to learn and execute tasks is crucial, making it\nmore reasonable to allocate replay data based on\nthe model’s difficulty in generating rationales from\nprior instructions.\nWhen training the model on the current task Ti,\nthe amount of replay data required for the previous\ntask Tj calculated as follows:\nα∗\nj =\nRGDj\nθi−1\nPk=i−1\nk=1\nRGDk\nθi−1\n× α,\nj ∈[1, i −1] (3)\nwhere RGDj\nθi−1 represents the rationale-guidance\ndifficulty of task j after the model has completed\ntraining on task i −1, and α represents the total\namount of replay data.\n5\nExperiments\n5.1\nExperiments Setup\n5.1.1\nDatasets\nLong Sequence Benchmark\n(Razdaibiedina\net al., 2023) A continual learning benchmark of\n15 classification datasets. Following (Razdaibied-\nina et al., 2023; Wang et al., 2023b), we select\n1,000 random samples for training each task and\nhold out 500 samples per class for validation and\ntesting.\nWe employed Qwen2.5-72B-Instruct1 to gener-\nate a rationale r for each data point (x, y), ensuring\nthat the initial part of the rationale did not directly\nreveal the final answer. Following prior work (Zhao\net al., 2024), we employ two different training or-\nders in our experiment.\n5.1.2\nModels\nWe utilize Qwen2 (0.5B, 1.5B) (Yang et al., 2024),\nLlama2 (7B, 13B) (Touvron et al., 2023b), and\nMistral (7B) (Jiang et al., 2023) as the backbone\nmodels for our experiments.\n5.1.3\nBaselines\nWe primarily evaluate various data allocation meth-\nods for replay. For comparison, we use the follow-\ning baseline allocation methods, employing ran-\ndom sampling to select replay samples for each\ntask.\nEqual Allocation (EA)\nFor each previous task,\nwe replay the same amount of samples while learn-\ning a new task to maintain the previous capability.\nInsCL\nInsCL (Wang et al., 2024) dynamically\nreplays previous data based on task similarity, cal-\nculated by Wasserstein Distance with instructions.\nMulti-task Learning\nWe use multi-task learning\nas the upper bound for continual learning.\n5.1.4\nMetrics\nAccording to previous works (Zhang et al., 2023b;\nLopez-Paz and Ranzato, 2017; Biesialska et al.,\n2020), we use the following CL-related metrics that\nconcentrate on catastrophic forgetting (stability)\nand knowledge transfer (plasticity). Let ai,j be the\ntesting performance (Accuracy for classification\ntask and Rouge-L for others) on the j-th task after\ntraining on i-th task, the metrics for evaluating are:\nFinal Average Performance (FAP)\nThe average\nperformance of all tasks after the final task tT is\nlearned, i.e., FAPT = 1\nT\nPT\nt=1 aT,t;\nForgetting Rate (F.Ra)\nIt measures how much\nknowledge has been forgotten across the first T −1\ntasks, i.e., FT =\n1\nT−1\nPT−1\nt=1 (maxT−1\nk=i ak,t −aT,t);\n1https://huggingface.co/Qwen/Qwen2.5-72B-Instruct\n\nMethod\nAllocate\nSelect\nFAP↑\nF.Ra↓\nBWT↑\nFWT↑\nCAP↑\nQwen2-0.5B\nSingle\n-\n-\n72.64\n-\n-\n-\n72.64\nMulti\n-\n-\n76.32\n-\n-\n-\n76.32\nCL\n-\n-\n51.75\n23.13\n-22.98\n0.33\n72.97\nEA\nequal\nrandom\n71.78\n3.40\n-2.02\n1.0\n73.65\nInsCL\ninstDiff\nrandom\n74.06\n1.55\n0.15\n1.27\n73.91\nRGD\nmean\nrandom\n74.69\n0.59\n0.67\n1.43\n74.07\nLlama2-7B\nSingle\n-\n-\n77.62\n-\n-\n-\n77.62\nMulti\n-\n-\n80.57\n-\n-\n-\n80.57\nCL\n-\n-\n66.50\n14.91\n-14.79\n2.54\n80.16\nEA\nequal\nrandom\n78.59\n2.62\n-1.77\n2.60\n80.22\nInsCL\ninstDiff\nrandom\n80.56\n0.90\n0.94\n2.06\n79.69\nRGD\nmean\nrandom\n81.07\n0.80\n1.21\n2.33\n79.95\nMistral-7B\nSingle\n-\n-\n79.01\n-\n-\n-\n79.01\nMulti\n-\n-\n78.79\n-\n-\n-\n78.79\nCL\n-\n-\n69.09\n10.73\n-10.27\n-0.33\n78.68\nEA\nequal\nrandom\n76.01\n3.58\n-2.88\n-0.31\n78.69\nInsCL\ninstDiff\nrandom\n76.25\n3.54\n-1.86\n-1.02\n77.97\nRGD\nmean\nrandom\n76.42\n3.45\n-1.85\n-0.86\n78.14\nLlama2-13B\nSingle\n-\n-\n73.91\n-\n-\n-\n73.91\nMulti\n-\n-\n83.48\n-\n-\n-\n83.48\nCL\n-\n-\n70.27\n13.02\n-12.90\n3.01\n82.17\nEA\nequal\nrandom\n80.81\n1.75\n-1.27\n2.83\n81.98\nInsCL\ninstDiff\nrandom\n81.86\n1.09\n-0.07\n2.77\n81.93\nRGD\nmean-std\nrandom\n81.05\n1.75\n-1.30\n3.10\n82.26\nTable 2: Experiments of different models on Long Sequence Benchmark. The decoding strategy is greedy search.\nOur method effectively alleviates model forgetting and maintains better model plasticity simultaneously\nBackward Transfer (BWT)\nBWT measures\nthe impact that continually learning on subse-\nquent tasks has on previous tasks, i.e., BWTT =\n1\nT−1\nPT−1\nt=1 (aT,t −at,t).\nForward Transfer (FWT)\nFWT measures how\nmuch knowledge from previous tasks transfers to\na new task, i.e., FWTT =\n1\nT\nPT\nt=1(at,t −a0,t)\nwhere a0,t refers to the performance of training\ntask t individually;\nCurrent Average Performance (CAP)\nThe av-\nerage performance of all tasks in their respective\ntraining stage, i.e., CAPT = 1\nT\nPT\nt=1 at,t.\nBetter scores on FAP, F.Ra, and BWT indicate\nimproved model stability, while better scores on\nFWT and CAP reflect enhanced model plasticity.\n5.2\nResults and Analysis\nTable 2 presents a performance comparison of dif-\nferent replay data allocation methods on the Long\nSequence benchmark. All results are averaged\nacross two different task orders. Detailed results\nfor each order and task within a specific order are\nprovided in the Appendix.\nThe equal allocation method significantly re-\nduces catastrophic forgetting compared to no\ndata replay\nFor example, with Qwen2-0.5B, ran-\ndom data replay improved task performance by\n20.03 and reduced the forgetting rate by 19.73.\nSimilarly, for Mistral-7B, task performance in-\ncreased by 6.92 and the forgetting rate decreased\nby 7.15.\nThese results support our hypothesis\nthat model performance degradation is due to inef-\nfective task instructions, not forgotten knowledge.\nSimple data replay, using a small amount of data,\nhelps the model leverage old knowledge more ef-\nfectively, alleviating catastrophic forgetting.\nThe model exhibits inherent resistance to for-\ngetting, which improves with larger model sizes\nSpecifically, larger models show lower forgetting\nrates during continual learning without data replay.\nFor example, the forgetting rates for Llama2-13B\nand Qwen2-0.5B are 13.02 and 23.13, respectively.\nUsing a data allocation algorithm can further al-\nleviate model forgetting\nOur model-based data\nallocation method, along with the InsCL method\nthat allocates data based on similarity, both more\n\nTest Instruction\nof MNLI\nBased on the premise “The northerner took his cloak and hat from the small boy and a leather\nsack of coin from one of the men in the crowd.”, can we conclude the hypothesis\n“The northerner left his cloak and hat behind,” is true?\nOPTIONS:- yes - it is not possible to tell - no\nPrediction BF\nstage on MNLI (NLI)\nThe text suggests that the northerner takes his belongings, i.e., his cloak and hat, with him.\nIt also mentions a “small boy” who was able to take something from him so he may have taken\nhis belt, which is another piece of clothing. So we know that this hypothesis cannot be true based\non what has already been stated. [RESULT]no\nPrediction BF\nstage on CB (NLI)\nThe passage states that the northerner took his cloak and hat, which means he was there before\ntaking them. Therefore, this statement contradicts the hypothesis.\nSo, “yes” would be our answer here.[RESULT]yes\nPrediction BF\nstage on Yahoo (TC)\nThis question belongs to the topic of probability theory, so the answer is 0.[RESULT]0\nTable 3: Cases of Knowledge Drift and Instruction Drift.1. BF refers to Before Forgetting and AF to After Forgetting.\n2. NLI means Natural Language Inference and TC indicates Topic Classification.\neffectively mitigate catastrophic forgetting in mod-\nels of varying sizes and knowledge sources during\ncontinual learning, resulting in better final model\nperformance.\nOur method effectively alleviates model forget-\nting and maintains better model plasticity simul-\ntaneously\nFor model plasticity metrics, FWT and\nCAP, our method performs similarly to InsCL on\nLlama2-7B. However, on other models, our ap-\nproach outperforms InsCL, suggesting that it offers\na more effective balance between mitigating catas-\ntrophic forgetting and preserving model plasticity.\n6\nDiscussion and Future Work\nWhat does the rationale generated by a “pseudo-\nforgetting” model look like under the guidance\nof old instructions?\nAs shown in Table 3, our\npreliminary observations suggest that the inappro-\npriate rationales which we call this Rationale Drift\ngenerated by “pseudo-forgetting” models primarily\nfall into two categories:\n1. Instruction Drift (Misalignment): When\nthere is a significant difference between old\nand new tasks (e.g., the old task is Natural\nLanguage Inference (NLI), the new task is\nTopic Classification (TC)), the reasoning pro-\ncess tends to completely rely on TC-relevant\nknowledge due to instruction misalignment,\nleading to incorrect results.\n2. Knowledge Drift: When old and new tasks\nare similar (e.g., both NLI but from differ-\nent domains), the rationale remains related to\nNLI knowledge but is influenced by the new\ndomain’s knowledge. This causes deviation\nfrom the appropriate reasoning path, resulting\nin errors.\nAdditionally, can these types of drift be detected\nby specific methods? For instance, could differ-\nences in model hidden layer representations reveal\ntheir presence? Is there a big difference between\nthe Rationale-Guidance Difficulty in the two cases?\nMore Complex Scenarios\nCurrent continual\nlearning benchmarks rely on traditional NLP tasks,\nwhich may be relatively simple and thus less likely\nto induce forgetting in the model. To enhance the\ncredibility of our conclusions, more complex do-\nmain adaptation datasets, such as TRACE (Wang\net al., 2023c), could be utilized to replicate the\nexperiments.\nDesign of Continual Learning Algorithm\nOur\nstudy assumes that the model retains prior knowl-\nedge during continual learning and inherently re-\nsists forgetting, making simple data replay effective\nin mitigating catastrophic forgetting. Another key\ngoal of continual learning is to maximize asyn-\nchronous knowledge transfer across tasks to en-\nhance model performance. Given that prior knowl-\nedge is parameterized, we propose that a combined\napproach, integrating both parametric and data re-\nplay perspectives, may be the most effective.\n7\nConclusion\nThis study sheds light on the intrinsic mechanism\nbehind catastrophic forgetting in continual learn-\ning, revealing that task knowledge is not truly lost\nbut hindered by inadequate instruction guidance\nfor appropriate rationales. We validate this hypoth-\nesis within LLMs and with consistent knowledge\nexpression by subjecting the forgetting model to\npassive external knowledge guidance and incorpo-\nrating a task-agnostic prefix into the original in-\nstruction for an active generation, both of which\n\neffectively recover the performance of the forget-\nting model. We introduce a Rationale-Guidance\nDifficulty, which evaluates the difficulty of guid-\ning the model to generate appropriate rationales—a\ncritical factor in overcoming forgetting. Using this\nmetric, our proposed replay data allocation method\neffectively mitigates forgetting while maintaining\nmodel plasticity.\nReferences\nRahaf Aljundi, Francesca Babiloni, Mohamed Elho-\nseiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018.\nMemory aware synapses: Learning what (not) to\nforget. In Computer Vision - ECCV 2018 - 15th Eu-\nropean Conference, Munich, Germany, September\n8-14, 2018, Proceedings, Part III, volume 11207 of\nLecture Notes in Computer Science, pages 144–161.\nSpringer.\nMagdalena Biesialska, Katarzyna Biesialska, and\nMarta R Costa-Jussa. 2020. Continual lifelong learn-\ning in natural language processing: A survey. arXiv\npreprint arXiv:2012.09823.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. Preprint, arXiv:2005.14165.\nIan J. Goodfellow, Mehdi Mirza, Xia Da, Aaron C.\nCourville, and Yoshua Bengio. 2014. An empirical\ninvestigation of catastrophic forgeting in gradient-\nbased neural networks. In 2nd International Confer-\nence on Learning Representations, ICLR 2014, Banff,\nAB, Canada, April 14-16, 2014, Conference Track\nProceedings.\nJiafeng Guo, Changjiang Zhou, Ruqing Zhang, Jian-\ngui Chen, Maarten de Rijke, Yixing Fan, and Xueqi\nCheng. 2024. Corpusbrain++: A continual genera-\ntive pre-training framework for knowledge-intensive\nlanguage tasks. CoRR, abs/2402.16767.\nYusong Hu, De Cheng, Dingwen Zhang, Nannan Wang,\nTongliang Liu, and Xinbo Gao. 2024. Task-aware or-\nthogonal sparse network for exploring shared knowl-\nedge in continual learning. In Forty-first Interna-\ntional Conference on Machine Learning, ICML 2024,\nVienna, Austria, July 21-27, 2024. OpenReview.net.\nJianheng Huang, Leyang Cui, Ante Wang, Chengyi\nYang, Xinting Liao, Linfeng Song, Junfeng Yao, and\nJinsong Su. 2024. Mitigating catastrophic forgetting\nin large language models with self-synthesized re-\nhearsal. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2024, Bangkok, Thailand,\nAugust 11-16, 2024, pages 1416–1428. Association\nfor Computational Linguistics.\nYufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang,\nand Diyi Yang. 2021. Continual learning for text clas-\nsification with information disentanglement based\nregularization. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2021, Online, June\n6-11, 2021, pages 2736–2746. Association for Com-\nputational Linguistics.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b. arXiv preprint arXiv:2310.06825.\nGangwei Jiang, Caigao Jiang, Zhaoyi Li, Siqiao Xue,\nJun Zhou, Linqi Song, Defu Lian, and Ying Wei.\n2024. Interpretable catastrophic forgetting of large\nlanguage model fine-tuning via instruction vector.\nCoRR, abs/2406.12227.\nSuhas Kotha, Jacob Mitchell Springer, and Aditi Raghu-\nnathan. 2024. Understanding catastrophic forgetting\nin language models via implicit inference. In The\nTwelfth International Conference on Learning Rep-\nresentations, ICLR 2024, Vienna, Austria, May 7-11,\n2024. OpenReview.net.\nMing Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu\nZhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou.\n2024a. Superfiltering: Weak-to-strong data filtering\nfor fast instruction-tuning. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2024, Bangkok, Thailand, August 11-16, 2024, pages\n14255–14273. Association for Computational Lin-\nguistics.\nMing Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang\nChen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and\nJing Xiao. 2024b. From quantity to quality: Boosting\nLLM performance with self-guided data selection\nfor instruction tuning. In Proceedings of the 2024\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers),\nNAACL 2024, Mexico City, Mexico, June 16-21, 2024,\npages 7602–7635. Association for Computational\nLinguistics.\nQingbin Liu, Pengfei Cao, Cao Liu, Jiansong Chen,\nXunliang Cai, Fan Yang, Shizhu He, Kang Liu, and\nJun Zhao. 2021. Domain-lifelong learning for dia-\nlogue state tracking via knowledge preservation net-\nworks. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\n\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021, pages 2301–\n2311. Association for Computational Linguistics.\nDavid Lopez-Paz and Marc’Aurelio Ranzato. 2017.\nGradient episodic memory for continual learning. Ad-\nvances in neural information processing systems, 30.\nMichael McCloskey and Neal J. Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. volume 24 of Psychol-\nogy of Learning and Motivation, pages 109–165. Aca-\ndemic Press.\nChengwei Qin and Shafiq R. Joty. 2022. LFPT5: A\nunified framework for lifelong few-shot language\nlearning based on prompt tuning of T5. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nAnastasia Razdaibiedina, Yuning Mao, Rui Hou, Ma-\ndian Khabsa, Mike Lewis, and Amjad Almahairi.\n2023. Progressive prompts: Continual learning for\nlanguage models. In International Conference on\nLearning Representations.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023a. Llama 2: Open foundation and fine-\ntuned chat models. Preprint, arXiv:2307.09288.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nXiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong\nBao, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing\nHuang. 2023a. Orthogonal subspace learning for lan-\nguage model continual learning. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023, Singapore, December 6-10, 2023, pages 10658–\n10671. Association for Computational Linguistics.\nXiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong\nBao, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing\nHuang. 2023b. Orthogonal subspace learning for lan-\nguage model continual learning. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023, pages 10658–10671, Singapore. Association\nfor Computational Linguistics.\nXiao Wang, Yuansen Zhang, Tianze Chen, Songyang\nGao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui\nZheng, Yicheng Zou, Tao Gui, et al. 2023c. Trace:\nA comprehensive benchmark for continual learn-\ning in large language models.\narXiv preprint\narXiv:2310.06762.\nYifan Wang, Yafei Liu, Chufan Shi, Haoling Li, Chen\nChen, Haonan Lu, and Yujiu Yang. 2024. Inscl: A\ndata-efficient continual learning paradigm for fine-\ntuning large language models with instructions. In\nProceedings of the 2024 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(Volume 1: Long Papers), NAACL 2024, Mexico City,\nMexico, June 16-21, 2024, pages 663–677. Associa-\ntion for Computational Linguistics.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nYizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu,\nYinghao Li, Yuhang Liu, Heyan Huang, and Yang\nGao. 2023. Mindllm: Pre-training lightweight large\nlanguage model from scratch, evaluations and do-\nmain applications. Preprint, arXiv:2310.15777.\nSeonghyeon Ye, Hyeonbin Hwang, Sohee Yang,\nHyeongu Yun, Yireun Kim, and Minjoon Seo. 2024.\nInvestigating the effectiveness of task-agnostic prefix\nprompt for instruction following. In Thirty-Eighth\nAAAI Conference on Artificial Intelligence, AAAI\n2024, Thirty-Sixth Conference on Innovative Applica-\ntions of Artificial Intelligence, IAAI 2024, Fourteenth\nSymposium on Educational Advances in Artificial\nIntelligence, EAAI 2014, February 20-27, 2024, Van-\ncouver, Canada, pages 19386–19394. AAAI Press.\nDuzhen Zhang, Wei Cong, Jiahua Dong, Yahan Yu, Xi-\nuyi Chen, Yonggang Zhang, and Zhen Fang. 2023a.\nContinual named entity recognition without catas-\ntrophic forgetting. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2023, Singapore, December 6-\n10, 2023, pages 8186–8197. Association for Compu-\ntational Linguistics.\nZihan Zhang, Meng Fang, Ling Chen, and Mohammad-\nReza Namazi-Rad. 2023b. CITB: A benchmark for\ncontinual instruction tuning. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2023, pages 9443–9455, Singapore. Association for\nComputational Linguistics.\nWeixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao,\nBing Qin, Xuanyu Zhang, Qing Yang, Dongliang\n\nXu, and Wanxiang Che. 2024. SAPT: A shared at-\ntention framework for parameter-efficient continual\nlearning of large language models. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 11641–11661. Association for Computational\nLinguistics.\nYingxiu Zhao, Yinhe Zheng, Zhiliang Tian, Chang Gao,\nJian Sun, and Nevin L. Zhang. 2022. Prompt condi-\ntioned VAE: enhancing generative replay for lifelong\nlearning in task-oriented dialogue. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npages 11153–11169. Association for Computational\nLinguistics.\nJunhao Zheng, Shengjie Qiu, Chengming Shi, and\nQianli Ma. 2024. Towards lifelong learning of large\nlanguage models: A survey. CoRR, abs/2406.06391.\nFan Zhou and Chengtai Cao. 2021. Overcoming catas-\ntrophic forgetting in graph neural networks with ex-\nperience replay. In Thirty-Fifth AAAI Conference\non Artificial Intelligence, AAAI 2021, Thirty-Third\nConference on Innovative Applications of Artificial\nIntelligence, IAAI 2021, The Eleventh Symposium\non Educational Advances in Artificial Intelligence,\nEAAI 2021, Virtual Event, February 2-9, 2021, pages\n4714–4722. AAAI Press.",
    "pdf_filename": "Reviving_Dormant_Memories_Investigating_Catastrophic_Forgetting_in_Language_Models_through_Rationale.pdf"
}