[
    {
        "title": "A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports",
        "context": "Table of contents (ToC) extraction centres on structuring documents in a hierarchical man- ner. In this paper, we propose a new dataset, ESGDoc, comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to 2022. These reports pose significant challenges due to their diverse structures and extensive length. To address these challenges, we pro- pose a new framework for Toc extraction, con- sisting of three steps: (1) Constructing an initial tree of text blocks based on reading order and font sizes; (2) Modelling each tree node (or text block) independently by considering its con- textual information captured in node-centric subtree; (3) Modifying the original tree by tak- ing appropriate action on each tree node (Keep, Delete, or Move). This construction-modelling- modification (CMM) process offers several ben- efits. It eliminates the need for pairwise mod- elling of section headings as in previous ap- proaches, making document segmentation prac- tically feasible. By incorporating structured information, each section heading can leverage both local and long-distance context relevant to itself. Experimental results show that our approach outperforms the previous state-of-the- art baseline with a fraction of running time. Our framework proves its scalability by effectively handling documents of any length.1 1 A considerable amount of research has been pro- posed to comprehend documents (Xu et al., 2019; Zhang et al., 2021; Xu et al., 2021a,b; Peng et al., 2022; Li et al., 2022; Gu et al., 2022; Shen et al., 2022; Lee et al., 2022, 2023) , which typically involves the classification of different parts of a document such as title, caption, table, footer, and so on. However, such prevailing classification of- ten centres on a document’s local layout structure, sidelining a holistic comprehension of its content 1Available at https://github.com/xnyuwg/cmm. and organisation. While traditional summarisation offers a concise representation of a document’s content, a Table of Contents (ToC) presents a struc- tured and hierarchical summary. This structural organisation in a ToC provides a comprehensive pathway for pinpointing specific information. For example, when seeking information about a com- pany’s carbon dioxide emissions, a ToC enables a systematic navigation through the information hierarchy. In contrast, conventional summarisa- tion might only provide a vague indication of such information, requiring sifting through the entire document for precise detail. Several datasets have been proposed to facilitate the research in document understanding (Zhong et al., 2019b; Li et al., 2020; Pfitzmann et al., 2022). Most of these studies lack a structured construc- tion of documents and primarily focus on well- structured scientific papers. A dataset called Hier- Doc (Hierarchical academic Document) (Hu et al., 2022) was introduced to facilitate the development of methods for extracting the table of contents (ToC) from documents. This dataset was compiled from scientific papers downloaded from arXiv2, which are typically short and well-structured. The hierarchical structure can often be inferred directly from the headings themselves. For example, the heading “1. Introduction” can be easily identified as a first-level heading based on the section number- ing. Moreover, due to the relatively short length of scientific papers, it is feasible to process the entire document as a whole. Hu et al. (2022) proposed the multimodal tree decoder (MTD) for ToC extrac- tion from HierDoc. MTD first utilises text, visual, and layout information to encode text blocks identi- fied by a PDF parser; then classifies all text blocks into two categories, headings and non-headings; and finally predicts the relationship of each pair of headings, facilitating the parsing of these headings into a tree structure representing ToC. 2https://arxiv.org/ arXiv:2310.18073v1  [cs.CL]  27 Oct 2023",
        "pdf_filename": "A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports.pdf",
        "num_chunks": 1442
    },
    {
        "title": "A Verification Framework for Component-Based Modeling and Simulation Putting the pieces together",
        "context": "",
        "pdf_filename": "A Verification Framework for Component-Based Modeling and Simulation Putting the pieces together.pdf",
        "num_chunks": 9188
    },
    {
        "title": "ACING Actor-Critic for Instruction Learning in Black-Box Large Language Models",
        "context": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly depends on the quality of the instructions, which often re- quire fine-tuning through extensive human ef- fort. This highlights the need for automated in- struction optimization; however, this optimiza- tion is particularly challenging when dealing with black-box LLMs, where model parameters and gradients remain inaccessible. We propose ACING, a task-specific prompt optimization approach framed as a state- less continuous-action Reinforcement Learning (RL) problem, known as the continuum bandit setting. ACING leverages an actor-critic-based method to optimize prompts, learning from non- differentiable reward signals. We validate AC- ING by optimizing prompts for ChatGPT on 30 instruction-based tasks, as well as a summa- rization task. ACING consistently outperforms baseline methods, achieving a median score im- provement of 10 percentage points compared to the best baseline considered. Furthermore, ACING not only recovers but also surpasses human-crafted expert instructions, achieving up to a 39 percentage point improvement over human benchmarks. 1 Large Language Models (LLMs) have demon- strated remarkable performance across a wide range of tasks (Zhao et al., 2024; Touvron et al., 2023). This success is largely attributed to their strong instruction-following capabilities, which en- able adaptation to diverse downstream applications (Chen et al., 2023; Liu et al., 2023). These instruc- tions, commonly referred to as prompts, play a cru- cial role in the performance of LLMs (Wei et al., 2022; Zhu et al., 2024; Liu et al., 2023). Given the importance of prompts, researchers are increasingly interested in automating their optimization to re- duce the need for manual adjustments, which is of- ten a labor-intensive and costly process (Reynolds and McDonell, 2021; Mishra et al., 2021). As a result, developing efficient methods for automatic prompt optimization has become highly important to maximize LLMs performance. Several methods for automated prompt optimiza- tion have been proposed in the literature. Some focus on optimizing continuous prompts, known as soft prompts (Zhong et al., 2021; Li and Liang, 2021), which are typically fed into a LLM after its embedding layer since they do not represent mean- ingful language that the LLM can interpret directly. Other methods aim at optimizing discrete prompts, known as hard prompts, by either generating di- verse prompts (Zhou et al., 2023) or refining exist- ing ones (Pryzant et al., 2023). For both soft and hard prompts, some approaches rely on gradient- based methods (Shin et al., 2020; Lester et al., 2021; Li and Liang, 2021). However, these meth- ods require access to the gradients of the LLMs and are therefore limited to white-box LLMs. Mean- while, the most powerful LLMs today are typically black-box models (e.g., ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b)). To overcome these challenges, recent studies have introduced gradient-free techniques for opti- mizing prompts in black-box LLMs (Zhou et al., 2023; Prasad et al., 2023; Pryzant et al., 2023). While promising, these approaches rely on heuris- tic local search methods and fail to fully utilize the observation history, including previously tested instructions and their outcomes, when selecting new instructions to test. Consequently, they are less effective at balancing the inherent exploration- exploitation trade-off, leading to inefficient query- ing. This inefficiency becomes problematic and impractical when API calls to black-box LLMs involve significant time and financial costs. A recent line of work has focused on optimiz- ing instructions for black-box LLMs by search- ing through soft prompts and transforming them into discrete prompts using white-box LLMs. In- 1 arXiv:2411.12736v1  [cs.CL]  19 Nov 2024",
        "pdf_filename": "ACING_Actor-Critic_for_Instruction_Learning_in_Black-Box_Large_Language_Models.pdf",
        "num_chunks": 2800
    },
    {
        "title": "ACL Anthology Helper - A Tool to Retrieve and Manage Literature from ACL Anthology",
        "context": "The ACL Anthology is an online repository that serves as a comprehensive collection of pub- lications in the field of natural language pro- cessing (NLP) and computational linguistics (CL). This paper presents a tool called “ACL Anthology Helper”. It automates the process of parsing and downloading papers along with their meta-information, which are then stored in a local MySQL database. This allows for efficient management of the local papers using a wide range of operations, including \"where,\" \"group,\" \"order,\" and more. By providing over 20 operations, this tool significantly enhances the retrieval of literature based on specific con- ditions. Notably, this tool has been successfully utilised in writing a survey paper (Tang et al., 2022a). By introducing the ACL Anthology Helper, we aim to enhance researchers’ abil- ity to effectively access and organise literature from the ACL Anthology. This tool offers a convenient solution for researchers seeking to explore the ACL Anthology’s vast collection of publications while allowing for more targeted and efficient literature retrieval. 1 The ACL Anthology serves as a valuable resource for researchers in the fields of NLP and CL, provid- ing access to a diverse collection of academic liter- ature from reputable venues such as ACL, EMNLP, NAACL, and COLING. It offers researchers the opportunity to keep abreast of the latest advance- ments, explore foundational work, and discover relevant studies in their areas of interest. However, the ACL Anthology lacks advanced functionalities, such as keyword-based retrieval and filtering op- tions based on publication time, authors, and other criteria. To address these limitations, we present the ACL Anthology Helper, a Python-based soft- ware tool designed to facilitate the local download *Corresponding author. and management of literature from the ACL An- thology. The ACL Anthology Helper is developed un- der the MIT License and can be found at the fol- lowing GitHub repository https://github.com/ tangg555/acl-anthology-helper. This tool of- fers the following key functionalities: • Downloading literature from the ACL Anthol- ogy website to a local MySQL database, allow- ing users to specify publication venues and time spans. • Supporting original MySQL operations as well as additional chain operations facilitated by ABuilder1, enabling users to effectively manage and retrieve downloaded papers using a sequence of Python-like functions. • Wrapping each paper’s information into an ob- ject, facilitating operations such as Union, Inter- section, Complement, and rule-based filtering on groups of papers. • Providing structured and statistical information on retrieved papers. The primary objective of this paper is to intro- duce the architecture of the ACL Anthology Helper and demonstrate how researchers can leverage its functionalities to enhance their literature manage- ment and retrieval processes. 2 Data Structures Based on the categorization provided by ACL An- thology, as presented in Appendix A.1, we have manually devised the data structures depicted in Figure 1. These data structures serve as essential components in our iterative crawling process of retrieving literature from the website. ACL An- thology encompasses top-level categories, namely ACL events and Non-ACL events, which comprise venues along with their respective hosting years. Notably, aside from conferences, ACL Anthology 1This is implemented by another github repository, ABuilder (https://github.com/lizhenggan/ABuilder) arXiv:2310.20467v1  [cs.CL]  31 Oct 2023",
        "pdf_filename": "ACL Anthology Helper - A Tool to Retrieve and Manage Literature from ACL Anthology.pdf",
        "num_chunks": 494
    },
    {
        "title": "AdaCM$^2$ On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction",
        "context": "The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most ex- isting LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nev- ertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effec- tively handling complex question-answering tasks. To ad- dress the challenges of long videos and complex prompts, we propose AdaCM2, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video un- derstanding tasks, such as video captioning, video ques- tion answering, and video classification, demonstrate that AdaCM2 achieves state-of-the-art performance across mul- tiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%. Video understanding is an important task in computer vi- sion and artificial intelligence, which involves processing and reasoning over visual and textual information. While the recent success of large language models (LLMs) [5, 31, 34, 39] has significantly improved video-language mod- els [27, 29], prior work has primarily focused on short video understanding tasks, typically with videos ranging *Yuanbin Man is a Student Researcher in the Dept. of CSE at UT Arlington while in a Master’s study at BU. †Miao Yin is the corresponding author. LLM Q-Former Memory Bank Single-Modality Correlation- based Token Merge What does the Man play? Play Football LLM Play Football What does the Man play? Dual-Modality Attention Mechanism W d t m p ? Figure 1. (Left) Existing approaches compress visual features of videos via single-modality correlation; (Right) Our AdaCM2 re- duces video memory adaptively based on cross-modality attention. from 5 to 15 seconds. However, long-term video under- standing [45], a sub-technique that develops models to pro- cess richer information, has played a crucial role in real- world applications such as movie analysis and video re- trieval. Unfortunately, it poses significant challenges as video length increases, especially the large memory con- sumption challenge. The number of frames the model must process grows rapidly, leading to substantial memory con- sumption, thereby preventing prior approaches from pro- cessing such long videos. To solve the large memory consumption challenge, many approaches focus on compressing video tokens. For in- stance, MA-LMM [16] employs a memory bank to com- press visual tokens based on the cosine similarities of ad- jacent two frames. Koala [37] passes multiple segments of video into tokenizer functions that aggregate visual to- kens to handle long videos. Even though those methods re- duce memory consumption, they still suffer from two sig- nificant limitations. 1) Ignoring text-driven information: As shown in Figure 1, existing works compress visual infor- mation without considering textual information, leading to the loss of vital visual tokens that are highly related to 1 arXiv:2411.12593v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "AdaCM$^2$_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory_Reduction.pdf",
        "num_chunks": 1471
    },
    {
        "title": "Adapting Amidst Degradation Cross Domain Li-ion Battery Health Estimation via Physics-Guided Test-Ti",
        "context": "Health modeling of lithium-ion batteries (LIBs) is crucial for safe and efficient energy management and carries significant socio- economic implications. Although Machine Learning (ML)-based State of Health (SOH) estimation methods have made significant progress in accuracy, the scarcity of high-quality LIB data remains a major obstacle. Although existing transfer learning methods for cross-domain LIB SOH estimation have significantly alleviated the labeling burden of target LIB data, they still require sufficient unla- beled target data (UTD) for effective adaptation to the target domain. Collecting this UTD is challenging due to the time-consuming na- ture of degradation experiments. To address this issue, we introduce a practical Test-Time Training framework, BatteryTTT, which adapts the model continually using each UTD collected amidst degra- dation, thereby significantly reducing data collection time. To fully utilize each UTD, BatteryTTT integrates the inherent physical laws of modern LIBs into self-supervised learning, termed Physcics- Guided Test-Time Training. Additionally, we explore the poten- tial of large language models (LLMs) in battery sequence model- ing by evaluating their performance in SOH estimation through model reprogramming and prefix prompt adaptation. The combi- nation of BatteryTTT and LLM modeling, termed GPT4Battery, achieves state-of-the-art generalization results across current LIB benchmarks. Furthermore, we demonstrate the practical value and scalability of our approach by deploying it in our real-world battery management system (BMS) for 300Ah large-scale energy storage LIBs. CCS Concepts • Test Time Training →Battery Health Estimation. Keywords Battery Health Estimation, Test Time Training, Data Scarcity, Large Language Model Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ’XX, June 03–05, 2018, Woodstock, NY © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX ACM Reference Format: Yuyuan Feng, Guosheng Hu, Xiaodong Li, and Zhihong Zhang*. 2018. Adapting Amidst Degradation: Cross Domain Li-ion Battery Health Es- timation via Physics-Guided Test-Time Training. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym ’XX). ACM, New York, NY, USA, 12 pages. https: //doi.org/XXXXXXX.XXXXXXX 1 One Unlabeled Data Physics Guided Self-Supervised Learning Step3 TTA Source Domain BatteryML, LLMs Source LIB Real-world Deployment Step2 Pre-training Target Domain Step1 Step2 Step3 Step4 Prefix-Prompt Adaptation Step4 Figure 1: Overview of BatteryTTT framework, which consists of three major components: (Step 1) pre-training on experimental datasets; (Step 2) incremental data collection after deployment; and (Steps 3-4) test-time adaptation. Steps 2, 3, and 4 iterate until the LIB retires. The rapid advancements in rechargeable Li-ion batteries (LIBs) have facilitated their widespread use across various sectors, includ- ing portable electronics, medical devices, renewable energy systems, and electric vehicles [11]. This ubiquity, however, introduces critical challenges associated with capacity degradation and performance evaluation. As an inherently interdisciplinary subject, battery aging modeling has emerged as a fundamental issue at the intersection of battery science and machine learning (ML) [25, 32, 33, 51]. Accurate State of Health (SOH) estimation for LIBs is crucial not only for ensuring safe and efficient energy management but also for opti- mizing the design and performance of next-generation batteries, thus having significant socio-economic implications. With the rapid advancement of ML technology, data-driven SOH estimation models have achieved significant progress in both ac- curacy and computational efficiency [32, 51]. However, obtaining sufficient training data for LIBs is challenging due to the time- consuming nature of degradation experiments, which typically arXiv:2402.00068v3  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Adapting_Amidst_Degradation_Cross_Domain_Li-ion_Battery_Health_Estimation_via_Physics-Guided_Test-Ti.pdf",
        "num_chunks": 1608
    },
    {
        "title": "Admiring the Great Mountain - A Celebration Special Issue in Honor of Stephen Grossbergs 80th Birthday",
        "context": "This editorial summarizes selected key contributions of Prof. Stephen Grossberg and describes the papers in this 80th birthday special issue in his honor. His productivity, creativity, and vision would each be enough to mark a scientist of the first caliber. In combination, they have resulted in contributions that have changed the entire discipline of neural networks. Grossberg has been tremendously influential in engineering, dynamical systems, and artificial intelligence as well. Indeed, he has been one of the most important mentors and role models in my career, and has done so with extraordinary generosity and encouragement. All authors in this special issue have taken great pleasure in hereby commemorating his extraordinary career and contributions.   Key contributions of Stephen Grossberg In brief, Grossberg as a biological neural modeler stands without peer, particularly in view of the predictive power of his models, and of their efficacy in unsupervised learning. His work has been independently touted (Hestenes, 1983) as the theoretical harbinger of a revolution in brain science, and subsequent confirmation by experimental psychologists of these models has borne out this assessment. It has been a constant marvel that mathematics works so well to model the many subtleties of scientific phenomena. Grossberg’s work is as strong an example of this principle as any.  He introduced, and has done more than anyone to develop, one of the most important computational paradigms ever; autonomous, self-correcting, biological intelligence. This includes but goes beyond unsupervised learning. The paradigm explains how individual humans or animals can learn to autonomously adapt in real time to complex and changing environments that are filled with unexpected events.  This work began in 1957, when Grossberg was still a freshman in college. Motivated by clearly- open questions in his Psychology coursework, Grossberg introduced the computational paradigm for linking brain mechanisms to psychological functions using real-time nonlinear neural networks. As part of this breakthrough, he derived classical laws for short-term memory, medium-term memory, and long-term memory that are still used, in some variant, by essentially all biological neural network modelers today.  Over the next six decades, Grossberg made a continuous stream of ground-breaking, and often revolutionary, theoretical breakthroughs continuing to the present. There are far too many to catalog in one or even several reviews; in fact, several books have been published on the subject.",
        "pdf_filename": "Admiring the Great Mountain - A Celebration Special Issue in Honor of Stephen Grossbergs 80th Birthday.pdf",
        "num_chunks": 341
    },
    {
        "title": "Adversarial Machine Learning and Cybersecurity - Risks, Challenges, and Legal Implications",
        "context": "",
        "pdf_filename": "Adversarial Machine Learning and Cybersecurity - Risks, Challenges, and Legal Implications.pdf",
        "num_chunks": 1109
    },
    {
        "title": "AffWild Net and Aff-Wild Database",
        "context": "",
        "pdf_filename": "AffWild Net and Aff-Wild Database.pdf",
        "num_chunks": 2001
    },
    {
        "title": "AI's Spatial Intelligence Evaluating AI's Understanding of Spatial Transformations in PSVTR and Augm",
        "context": "Spatial intelligence is important in many fields such as Architecture, Engineering, and Construction (AEC), Science, Technology, Engineering, and Mathematics (STEM), and Medicine. Understanding three-dimensional (3D) spatial rotations can involve verbal descriptions and visual or interactive examples, illustrating how objects move and change orientation in 3D space. Recent studies show Artificial Intelligence (AI) with language and vision capabilities still face limitations in spatial reasoning. In this paper, we have studied advanced generative AI’s spatial capabilities of understanding rotations of objects in 3D space utilizing its image processing and language processing features. We examined the spatial intelligence of the GPT-4 model with vision in understanding spatial rotation process with spatial rotation diagrams based on the Revised Purdue Spatial Visualization Test: Visualization of Rotations (Revised PSVT:R). Furthermore, we incorporated an added layer of a coordinate system axes on Revised PSVT:R to study the variations in GPT-4’s performance. We additionally examined GPT-4’s understanding of 3D rotations in Augmented Reality (AR) scene images that visualize spatial rotations of a physical object in 3D space and observed increased accuracy of GPT-4’s understanding of the rotations by adding supplementary textual information depicting the rotation process or mathematical representations of the rotation (e.g., matrices) superimposed on the object. The results indicate that while GPT-4 as a major current Generative AI model lacks the understanding of a spatial rotation process, it has the potential to understand the rotation process with additional information that can be provided by methods such as AR. AR can superimpose textual information or mathematical representations of the rotations on spatial transformation diagrams or images and create a more intelligible input for AI to comprehend or for training AI’s spatial intelligence. Furthermore, by combining the potentials in spatial intelligence of AI with AR's interactive visualization abilities, we expect to offer enhanced guidance for students’ spatial learning activities. Such spatial guidance can greatly benefit understanding spatial transformations and additionally support processes like assembly, construction, fabrication, manufacturing, as well as learning in AEC, STEM, and Medicine that require precise 3D spatial understanding and instruction. Spatial visualization skills refer to the ability to mentally rotate, manipulate, twist, or invert 3D objects [1], [2]. Education in many STEM disciplines demands instructional approaches that offer experimental practices and tasks with high representational accuracy and realistic simulations to enhance the learning experience [3]. Many first-year undergraduate STEM students struggle with visualization and drawing tasks, even though they have access to various computer assisted learning resources for geometry, spatial transformations, and related mathematical concepts [4]. These difficulties include understanding the symbolic notation of linear algebra, generalizing geometric reasoning, and transitioning to matrix representations of transformations, which often weakens their intuitive grasp of the concepts  [5], [6].",
        "pdf_filename": "AI's_Spatial_Intelligence_Evaluating_AI's_Understanding_of_Spatial_Transformations_in_PSVTR_and_Augm.pdf",
        "num_chunks": 745
    },
    {
        "title": "AIGS Generating Science from AI-Powered Automated Falsification",
        "context": "Rapid development of artiﬁcial intelligence has drastically accelerated the devel- opment of scientiﬁc discovery. Trained with large-scale observation data, deep neural networks extract the underlying patterns in an end-to-end manner and as- sist human researchers with highly-precised predictions in unseen scenarios. The recent rise of Large Language Models (LLMs) and the empowered autonomous agents enable scientists to gain help through interaction in different stages of their research, including but not limited to literature review, research ideation, idea implementation, and academic writing. However, AI researchers instantiated by foundation model empowered agents with full-process autonomy are still in their infancy. In this paper, we study AI-Generated Science (AIGS), where agents in- dependently and autonomously complete the entire research process and discover scientiﬁc laws. By revisiting the deﬁnition of scientiﬁc research (Popper, 1935), we argue that falsiﬁcation is the essence of both human research process and the design of an AIGS system. Through the lens of falsiﬁcation, prior systems at- tempting towards AI-Generated Science either lack the part in their design, or rely heavily on existing veriﬁcation engines that narrow the use in specialized do- mains. In this work, we propose BABY-AIGS as a baby-step demonstration of a full-process AIGS system, which is a multi-agent system with agents in roles rep- resenting key research process. By introducing FALSIFICATIONAGENT, which identify and then verify possible scientiﬁc discoveries, we empower the system with explicit falsiﬁcation. Experiments on three tasks preliminarily show that BABY-AIGS could produce meaningful scientiﬁc discoveries, though not on par with experienced human researchers. Finally, we discuss on the limitations of current BABY-AIGS, actionable insights, and related ethical issues in detail.1 Heliocentric Theory: Hypothesis Falsification Scientific Discovery Geocentric Theory: Earth is stationary; Celestial bodies revolve around the Earth; Circular motion. Astronomy Observation: Mathematical Reasoning: Four Largest Satellites of Jupiter Transit of Venus Phases of Venus Astronomical Observation Data Mathematical Proof Kepler’s Laws of Planetary Motion Final Discovery: Earth is rotating; Celestial bodies revolve around the Sun; Elliptical orbit motion. ... Earth is rotating; Celestial bodies revolve around the Sun; Circular motion. ... ... Geocentric Theory Heliocentric Theory Celestial Motion Diagram: Figure 1: Examples of scientiﬁc research processes conducted by human researchers. Explicit falsi- ﬁcation serves as a vital stage to falsify or verify the proposed hypotheses from either empirical or theoretical experiments, leading to the ultimate scientiﬁc discovery. ∗indicates equal contribution. 1Ofﬁcial Website: https://agent-force.github.io/AIGS/. Code is released at https://github.com/AgentForceTeamOfficial/Baby-AIGS. 1",
        "pdf_filename": "AIGS_Generating_Science_from_AI-Powered_Automated_Falsification.pdf",
        "num_chunks": 2680
    },
    {
        "title": "AI Flow at the Network Edge",
        "context": "(LLMs) and their multimodal variants have led to remarkable progress across various domains, demonstrating impressive ca- pabilities and unprecedented potential. In the era of ubiquitous connectivity, leveraging communication networks to distribute intelligence is a transformative concept, envisioning AI-powered services accessible at the network edge. However, pushing large models from the cloud to resource-constrained environments faces critical challenges. Model inference on low-end devices leads to excessive latency and performance bottlenecks, while raw data transmission over limited bandwidth networks causes high communication overhead. This article presents AI Flow, a frame- work that streamlines the inference process by jointly leveraging the heterogeneous resources available across devices, edge nodes, and cloud servers, making intelligence flow across networks. To facilitate cooperation among multiple computational nodes, the proposed framework explores a paradigm shift in the design of communication network systems from transmitting information flow to intelligence flow, where the goal of communications is task-oriented and folded into the inference process. Experimental results demonstrate the effectiveness of the proposed framework through an image captioning use case, showcasing the ability to reduce response latency while maintaining high-quality captions. This article serves as a position paper for identifying the motivation, challenges, and principles of AI Flow. We are witnessing a transformative era in the field of artificial intelligence (AI) with groundbreaking technologies [1]–[3]. LLMs, like ChatGPT and PaLM, have demonstrated remarkable capabilities in natural language understanding, generation, and reasoning. Going beyond language, multi- modal foundation models, like CLIP and Llama, have show- cased exceptional performance in cross-modal perception, understanding, and interaction tasks. These advancements led to their adaptations in a broad spectrum of application do- mains, including embodied robotics, augmented reality, and autonomous driving [4]–[7]. Most recently, edge devices (mo- bile phones, smart wearables, and IoT sensors) are becoming increasingly widespread, and sensory data are easy to access. This has sparked a surge of interest in pushing intelligence applications from the central cloud to the network edge [8], [9]. As visualized in Fig. 1, the future envisions a scenario where AI technologies are seamlessly integrated into various aspects of daily life. While offering exciting opportunities, deploying large mod- els at the network edge faces new challenges. In contrast to cloud intelligence, performing AI tasks at the network edge differs significantly in many cases. First, the fundamental difference is the available resources. Edge devices are usually equipped with limited computation resources, whereas cloud The authors are with the Institute of Artificial Intelligence (TeleAI), China Telecom, China (E-mail: shaojw2@chinatelecom.cn, xuelong_li@ieee.org). The corresponding author is Xuelong Li. servers possess a large number of powerful processing units. As a result, on-device model inference struggles to support real-time responses since the large foundation models demand intensive computation [10]. Besides, edge devices are often wirelessly connected. The limited uplink bandwidth and the highly dynamic nature of wireless channels hinder transmitting large volumes of data collected by edge devices to cloud servers in real-time. Aiming at efficient delivery of diversified intelligent ser- vices, we propose AI Flow, a framework that seamlessly integrates intelligence capabilities directly at the network edge. Specifically, AI Flow aims to streamline the inference process by jointly leveraging the heterogeneous computational resources available across devices, edge nodes, and cloud servers. This framework distributes the inference workload across different network layers and adapts to dynamic network conditions. To facilitate cooperation among multiple compu- tational nodes, AI Flow provides a paradigm shift in the design of communication network systems from transmitting information flow to intelligence flow. The goal of communica- tions needs to be folded into the inference process. Instead of sending raw data from edge devices to servers for processing, the intelligence flow features a task-oriented property, where edge devices extract only critical features from the raw sensory data and discard redundant information to reduce communica- tion overhead. To summarize, the advantages of the AI Flow framework are manifold. • Ubiquity: The widespread deployment of AI capabilities at the edge makes intelligent responses possible wherever there is network access. This ensures that various devices benefit from AI-driven insights without the need for constant connectivity to central servers. • Adaptivity: The framework distributes inference tasks according to available computational resources, dynamic network conditions, and task requirements. Adaptively scheduling task execution maintains performance in fluc- tuating environments. • Low-latency: By prioritizing data processing close to the source, the delay in transmitting information to powerful servers is minimized. This provides a quicker end-to-end response and achieves a smoother user experience. The rest of the article is organized as follows. Section II introduces typical applications at the network edge, and III provides a system overview of AI Flow. Two types of enabling techniques, namely, cooperative inference and model inference speedup, have been elaborated in Section IV and Section V, respectively. Section VI presents a case study to evaluate the effectiveness of the proposed AI Flow framework. Finally, we conclude this article and point out future research opportunities in Section VII. arXiv:2411.12469v1  [eess.SP]  19 Nov 2024",
        "pdf_filename": "AI_Flow_at_the_Network_Edge.pdf",
        "num_chunks": 634
    },
    {
        "title": "AI Guided Early Screening of Cervical Cancer",
        "context": "In order to support the creation of reliable machine learning models for anomaly detection, this project focuses on preprocessing, enhancing, and organizing a medical imaging dataset. There are two classifications in the dataset: normal and abnormal, along with extra noise fluctuations. In order to improve the photographs' quality, undesirable artifacts, including visible medical equipment at the edges, were eliminated using central cropping. Adjusting the brightness and contrast was one of the additional preprocessing processes. Normalization was then performed to normalize the data. To make classification jobs easier, the dataset was methodically handled by combining several image subsets into two primary categories: normal and pathological. To provide a strong training set that adapts well to real-world situations, sophisticated picture preprocessing techniques were used, such as contrast enhancement and real-time augmentation (including rotations, zooms, and brightness modifications). To guarantee efficient model evaluation, the data was subsequently divided into training and testing subsets. In order to create precise and effective machine learning models for medical anomaly detection, high-quality input data is ensured via this thorough approach. Because of the project pipeline's flexible and scalable design, it can be easily integrated with bigger clinical decision-support systems.   Recent developments in artificial intelligence and machine learning have revolutionized medical imaging, allowing for more precise and effective illness diagnosis. The caliber and structure of the data utilized to train these models provide the basis of these technological developments. Data imbalance, noise, and imaging condition variability are some of the distinctive difficulties that medical imaging datasets, especially those utilized for anomaly identification, frequently face. To overcome these obstacles and guarantee the creation of reliable machine learning models, the data must be carefully preprocessed, augmented, and organized.  Images in medical imaging datasets are usually classified into classes like normal and abnormal, with variances resulting from environmental noise, patient-specific factors, and imaging equipment. Effective preprocessing of these datasets is crucial for guaranteeing the precision and dependability of machine learning models. Noise reduction, cropping to eliminate superfluous regions, normalization to uniformize pixel intensities, and augmentation to artificially boost the training data's diversity are all examples of preprocessing techniques. By taking these actions, the data quality is improved and the models' capacity to generalize to new, untested data is increased.",
        "pdf_filename": "AI_Guided_Early_Screening_of_Cervical_Cancer.pdf",
        "num_chunks": 484
    },
    {
        "title": "An Overview of Open-Ended Evolution - Editorial Introduction to the Open-Ended Evolution II Special Issue",
        "context": "the Open-Ended Evolution II Special Issue Norman Packard* Protolife, Inc. norman.packard@protolife.com Mark A. Bedau Reed College Alastair Channon Keele University Takashi Ikegami University of Tokyo Steen Rasmussen University of Southern Denmark Santa Fe Institute Kenneth O. Stanley Uber AI Labs Tim Taylor Monash University Keywords Open-ended evolution, open-endedness, novelty, innovation, complexity growth, semantic evolution, major transitions, evolution of evolvability, optimization Natureʼs spectacular inventiveness, reflected in the enormous diversity of form and function displayed by the biosphere, is a feature of life that distinguishes living most strongly from nonliving. It is, therefore, not surprising that this aspect of life should become a central focus of artificial life. We have known since Darwin that the diversity is produced dynamically, through the process of evolution; this has led lifeʼs creative productivity to be called Open-Ended Evolution (OEE) in the field. This article introduces the second of two special issues on current research in OEE and provides an overview of the contents of both special issues. Most of the work was presented at a workshop on open-ended evolution that was held as a part of the 2018 Conference on Artificial Life in Tokyo, and much of it had antecedents in two previous workshops on open- ended evolution at artificial life conferences in Cancun and York. We present a simplified categorization of OEE and summarize progress in the field as represented by the articles in this special issue. 1 Introduction The variety of organisms produced by biological evolution is staggering. Recent estimates based on scaling arguments estimate the number of microbial species alone to be ∼1012 [23]. In a very real sense, the entire biosphere has been produced by (or one might say “invented by” [33]) the process of biological evolution. This creative productivity is one of the most striking features of life, and so it has not surprisingly become a focus of artificial life research. Decades of research have explored evolutionary algorithms of various sorts, but so far a full understanding of the creative productivity of evolution (biological or non-biological) has remained out of reach, with regard to both under- standing how it has worked to produce the biosphere, and understanding what principles might be used or instantiated outside of biology (the purview of artificial life) to achieve comparable levels of creative productivity. * Corresponding author. © 2019 Massachusetts Institute of Technology Artificial Life 25: 93–103 (2019) doi:10.1162/artl_a_00291",
        "pdf_filename": "An Overview of Open-Ended Evolution - Editorial Introduction to the Open-Ended Evolution II Special Issue.pdf",
        "num_chunks": 508
    },
    {
        "title": "Analysing Explanation-Related Interactions in Collaborative Perception-Cognition-Communication-Actio",
        "context": "rative tasks, so AI-equipped robots working alongside humans need to be able to explain their behaviour in order to cooperate effectively and earn trust. We analyse and classify communi- cations among human participants collaborating to complete a simulated emergency response task. The analysis identifies messages that relate to various kinds of interactive explanations identified in the explainable AI literature. This allows us to understand what type of explanations humans expect from their teammates in such settings, and thus where AI-equipped robots most need explanation capabilities. We find that most explanation-related messages seek clarification in the decisions or actions taken. We also confirm that messages have an impact on the performance of our simulated task. Effective human-robot teaming is seen as a key enabler of future ‘front line’ situations including emergency re- sponse and disaster relief. In such highly dynamic settings, coordination among team members is a critical success factor. The increasing sophistication of modern artificial intelligence (AI) has led to significantly improved perception, cognition, communication and action (PCCA) capabilities embodied in robots. However, many of the key technologies are ‘black box’ in nature, making it hard to engineer robots that operate in a sufficiently transparent manner to their human collaborators. This work is a step towards analysing human expectations in terms of explainability in relation to task coordination. The setting is a simulation environment, TeamCollab [1], in which humans and AI-equipped robots collaborate to clear an area of dangerous objects. The envi- ronment is designed to highlight PCCA capabilities; humans and robots are intended to work as peer agents, and inter- agent communication is a key factor in task success. We analyse results from TeamCollab experiments with human participants to better understand what humans expect from their teammates in relation to explanation. We adopt a recent explainable AI (XAI) framework [2] that takes a dialogue-centric view of explanation, labelling the exchanged messages in terms of their relationship to elements of the framework. We show that there is a positive relationship between message exchange and team performance: volume of communication correlates with task success. This anal- ysis seeks to answer the research question: What types of explanations do AI-equipped robots need based on human 1Cardiff University, UK. Emails: {RoigVilamalaM, furbyjl, preecead, fuentestoroc}@cardiff.ac.uk 2University of California, Los Angeles, USA {julian700, mbs}@ucla.edu teammate expectations? This is important, as AI agents that communicate effectively have been shown to earn more trust and cooperate better with humans [3]. II. RELATED WORK As AI systems will be required to perform in highly cog- nitively demanding environments, it has become critical to understand the communication mechanisms that would better support human-AI teams. Transparency and explainability are key components of situational awareness to allow agents to understand better the dynamically changing world they constantly perceive [4]. Thus, XAI aims to improve the understanding and interpretation of the decision processes and results of machine learning algorithms [5], [6] to support, e.g., human-robot teaming. Current approaches to XAI have mostly focused on static explainability, with a single message to cover, without input or user preferences involved in the process [7]. Due to the social nature of XAI, interactive explanations are gaining attention [8], as this involves an iterative process that considers user’s information needs and is more similar to people’s patterns on how explanations are expected to be provided [2], [6]. Some taxonomies have been proposed to explore the interactive aspect of XAI. Liao et. al [5] present an XAI question bank framework with a set of prototypical questions that users may ask when requesting an explanation from an AI system. Authors in [2] synthesize 48 empirical studies to create a two-level taxonomy of interactive techniques in XAI based on their cognitive processes and tasks. During communication in critical timing scenarios, asking for an explanation may not always be explicit. The theory behind team communication in human groups identifies that a significant amount of communication goes through non- explicit channels, meaning that messages are interpreted in context, and there are additional communicative channels used as eye-gaze, gestures or non-verbal statements critical for performance that complement the message [9]. There- fore, to better understand implicit explanation dialogues in context, we analyse the communication interaction of human- human data in a team environment and map awareness factors into a taxonomy of interactive techniques in XAI. III. METHODS A. Experiment Design The experiment was designed to run in TeamCollab [1], a simulated environment based on the ThreeDWorld physics arXiv:2411.12483v1  [cs.HC]  19 Nov 2024",
        "pdf_filename": "Analysing_Explanation-Related_Interactions_in_Collaborative_Perception-Cognition-Communication-Actio.pdf",
        "num_chunks": 415
    },
    {
        "title": "Artificial Intellgence -- Application in Life Sciences and Beyond. The Upper Rhine Artificial Intelligence Symposium UR-AI 2021",
        "context": "",
        "pdf_filename": "Artificial Intellgence -- Application in Life Sciences and Beyond. The Upper Rhine Artificial Intelligence Symposium UR-AI 2021.pdf",
        "num_chunks": 11099
    },
    {
        "title": "Assessing the impact of machine intelligence on human behaviour - an interdisciplinary endeavour",
        "context": "",
        "pdf_filename": "Assessing the impact of machine intelligence on human behaviour - an interdisciplinary endeavour.pdf",
        "num_chunks": 3080
    },
    {
        "title": "AtomThink A Slow Thinking Framework for Multimodal Mathematical Reasoning",
        "context": "In this paper, we address the challenging task of multi- modal mathematical reasoning by incorporating the ability of “slow thinking” into multimodal large language mod- els (MLLMs). Contrary to existing methods that rely on direct or fast thinking, our key idea is to construct long chains of thought (CoT) consisting of atomic actions in a step-by-step manner, guiding MLLMs to perform com- plex reasoning. To this end, we design a novel AtomThink framework composed of three key modules: (i) a CoT an- notation engine that automatically generates high-quality CoT annotations to address the lack of high-quality visual mathematical data; (ii) an atomic step fine-tuning strat- egy that jointly optimizes an MLLM and a policy reward model (PRM) for step-wise reasoning; and (iii) four dif- ferent search strategies that can be applied with the PRM to complete reasoning. Additionally, we propose Atom- MATH, a large-scale multimodal dataset of long CoTs, and an atomic capability evaluation metric for mathematical tasks. Extensive experimental results show that the pro- posed AtomThink significantly improves the performance of baseline MLLMs, achieving approximately 50% relative ac- curacy gains on MathVista and 120% on MathVerse. To support the advancement of multimodal slow-thinking mod- els, we will make our code and dataset publicly available on https://github.com/Quinn777/AtomThink. Chain-of-thought (CoT) reasoning [34] has provided a novel scheme for large language models (LLMs) to tackle complex reasoning tasks. By utilizing a small number of specially designed instructions, CoT enables LLMs to gen- erate intermediate reasoning steps, significantly enhancing performance on symbolic tasks such as mathematical prob- *These authors contributed equally to this work. †Corresponding author. Email: xdliang328@gmail.com Graphs Analysis Verification Information Extraction Calculation Approximation Geometric Reasoning Equation Formulation Knowledge Introduction Image Description Variable Definition 0% 20% 40% 60% 80% 100% LLaVA-Llama3-8B AtomThink-LLaVA EMOVA-8B AtomThink-EMOVA Figure 1. Atomic capability evaluation of different models. Exist- ing open-source models exhibit significant shortcomings in capa- bilities such as variable definition, approximation and image de- scription. lems and code writing [44]. While CoT-based methods show clear improvements over direct predictions, they still rely heavily on greedy decoding strategies. More recently, the introduction of OpenAI’s o1 [23] marks a substantial advancement in the ability of artificial intelligence systems to perform high- level reasoning. Unlike traditional models, o1 excels in solving complex problems by utilizing extended reason- ing chains and adopting test-time scaling, i.e., “slow think- ing”. In addition to o1, several concurrent works have ex- plored methods for incorporating slow thinking capabili- ties into open-source LLMs, such as Thought Trees [35] and Monte Carlo tree search (MCTS) based tree search techniques [6, 25, 30, 31]. The success of o1 and its variants demonstrate that incorporating slow thinking into LLMs significantly enhances their performance on com- arXiv:2411.11930v1  [cs.CV]  18 Nov 2024",
        "pdf_filename": "AtomThink_A_Slow_Thinking_Framework_for_Multimodal_Mathematical_Reasoning.pdf",
        "num_chunks": 1372
    },
    {
        "title": "Attribute Inference Attacks for Federated Regression Tasks",
        "context": "Federated Learning (FL) enables multiple clients, such as mo- bile phones and IoT devices, to collaboratively train a global machine learning model while keeping their data localized. However, recent studies have revealed that the training phase of FL is vulnerable to reconstruction attacks, such as attribute inference attacks (AIA), where adversaries exploit exchanged messages and auxiliary public information to uncover sensi- tive attributes of targeted clients. While these attacks have been extensively studied in the context of classification tasks, their impact on regression tasks remains largely unexplored. In this paper, we address this gap by proposing novel model- based AIAs specifically designed for regression tasks in FL environments. Our approach considers scenarios where ad- versaries can either eavesdrop on exchanged messages or directly interfere with the training process. We benchmark our proposed attacks against state-of-the-art methods using real-world datasets. The results demonstrate a significant in- crease in reconstruction accuracy, particularly in heteroge- neous client datasets, a common scenario in FL. The efficacy of our model-based AIAs makes them better candidates for empirically quantifying privacy leakage for federated regres- sion tasks. 1 Federated learning (FL) enables multiple clients to collab- oratively train a global model (McMahan et al. 2017; Lian et al. 2017; Li et al. 2020). Since clients’ data is not col- lected by a third party, FL naturally offers a certain level of privacy. Nevertheless, FL alone does not provide formal pri- vacy guarantees, and recent works have demonstrated that clients’ private information can still be easily leaked (Lyu et al. 2020; Liu, Xu, and Wang 2022). For instance, an ad- versary with access to the exchanged messages and knowl- edge of some public information (e.g., client’s provided rat- ings) (Lyu and Chen 2021; Chen et al. 2022; Feng et al. 2021) can reconstruct a client’s sensitive attributes (e.g., gender/religion) in an attack known as attribute inference attack (AIA). Additionally, the adversary can reconstruct client’s training samples such as images (Geiping et al. 2020; Yin et al. 2021). However, these reconstruction attacks for FL have pri- marily been tested on classification tasks and have not been Figure 1: Average performance of different AIAs when four clients train a neural network through FedAvg with 1 local epoch and batch size 32. Each client stores |Dc| data points randomly sampled from ACS Income dataset (Ding et al. 2024). The adversary infers the gender attribute of every data sample held by the client given access to the released (pub- lic) information. explored for regression tasks, which are, needless to say, equally important for practical applications. Quite surpris- ingly, our experiments, as shown in Fig. 1, demonstrate that the accuracy of state-of-the-art gradient-based AIA under a honest-but-curious adversary (Lyu and Chen 2021; Chen et al. 2022) (referred to as passive) drops significantly from 71% on a classification task to 50% (random guess) on a re- gression task once the targeted client holds more than 256 data points. Furthermore, even a more powerful (active) ad- versary capable of forging the messages to the targeted client to extract more information (Lyu and Chen 2021; Chen et al. 2022) offers only limited improvement to the AIA perfor- mance on regression tasks. Detailed information about this experiment is in Appendix B.1. In this paper, we show that federated training of regression tasks does not inherently enjoy higher privacy, but it is sim- ply more vulnerable to other forms of attacks. While existing FL AIA attacks for classification tasks are gradient-based (see Sec. 2.3), we show that model-based AIAs—initially proposed for centralized training (Fredrikson et al. 2014; arXiv:2411.12697v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Attribute_Inference_Attacks_for_Federated_Regression_Tasks.pdf",
        "num_chunks": 2190
    },
    {
        "title": "A Computational Method for Measuring Open Codes in Qualitative Analysis",
        "context": "Qualitative analysis is the process of systematically identifying, generating, and organizing concepts from data. It has been widely adopted in many social science disciplines (such as education, sociology, psychology, and medicine), as well as interdisciplinary areas such as human-computer interaction (HCI) 1 to understand people’s perceptions, feelings, and nuanced interactions with technology[2]. However, qualitative methods can be challenging, time-consuming, and may lack transparency[9]. The problem is particularly acute during the process of open coding, where researchers inductively identify emergent codes from raw data without a preconceived coding scheme. As the first step of qualitative analysis, practitioners and theorists of thematic analysis (TA)[10, 63] and grounded theory (GT)[23] make frequent use of open coding and agree on its goal: 1For example, the CHI conference itself has increasingly included papers using qualitative methods: 36.7% in 2021, 48.4% in 2022, 48.9% in 2023, 53.9% in 2024. This result comes from searches in the ACM digital library with the keywords: \"thematic analysis,\" \"qualitative analysis,\" \"qualitative coding,\" \"qualitative research,\" \"discourse analysis,\" and \"grounded theory.\"",
        "pdf_filename": "A_Computational_Method_for_Measuring_Open_Codes_in_Qualitative_Analysis.pdf",
        "num_chunks": 1315
    },
    {
        "title": "A Layered Architecture for Developing and Enhancing Capabilities in Large Language Model-based Softw",
        "context": "Significant efforts has been made to expand the use of Large Language Models (LLMs) beyond basic language tasks. While the generalizability and versatility of LLMs have enabled widespread adoption, evolving demands in application development often exceed their native capabilities. Meeting these demands may involve a diverse set of methods, such as enhancing creativity through either inference temperature adjustments or creativity-provoking prompts. Selecting the right ap- proach is critical, as different methods lead to trade-offs in engineering complexity, scalability, and operational costs. This paper introduces a layered architecture that organizes LLM software system development into distinct layers, each characterized by specific attributes. By aligning capabilities with these layers, the framework encourages the systematic implementation of capa- bilities in effective and efficient ways that ultimately supports desired functionalities and quali- ties. Through practical case studies, we illustrate the utility of the framework. This work offers developers actionable insights for selecting suitable technologies in LLM-based software system development, promoting robustness and scalability. Keywords: Artificial intelligence, large language model, software architecture, agent 1 The use of Large Language Models (LLMs) has expanded beyond traditional language-related tasks such as translation and question answering. This widespread adoption is largely driven by the gen- eralizability and versatility of LLMs, which stem from being trained on vast amounts of diverse data sourced from the internet and human annotations, allowing them to capture patterns across many domains. Furthermore, LLMs use text as a flexible input/output interface, which makes interact- ing with them intuitive and adaptable to various contexts. Combined with advances in techniques that improve their ability to follow instructions and align with specific needs, LLMs have been in- creasingly applied to a variety of domain applications requiring flexibility and scalability, including software development [1–3], process automation [4, 5], financial analysis [6, 7], manufacturing [8, 9], education [10,11], and scientific research [12–14]. However, despite their remarkable strengths, LLMs have clear limitations. For instance, even when trained on vast datasets, they often struggle with domain-specific knowledge and lack the specialized expertise needed for certain tasks [15]. Additionally, LLMs are prone to generating plausible-sounding but factually incorrect outputs [16], commonly referred to as “hallucination.” Since LLMs are pri- marily trained on the NLP task of next word prediction based on statistical probabilities [17], they may be less reliable and efficient than simpler but specialized tools for tasks that require a deeper understanding or internal task representation [18]. Furthermore, due to their language processing nature, they also interact with the external world exclusively through the interface of text, limiting effective access to the diverse interfaces of external tools or systems [19]. 1 arXiv:2411.12357v1  [cs.SE]  19 Nov 2024",
        "pdf_filename": "A_Layered_Architecture_for_Developing_and_Enhancing_Capabilities_in_Large_Language_Model-based_Softw.pdf",
        "num_chunks": 1163
    },
    {
        "title": "A More Advanced Group Polarization Measurement Approach Based on LLM-Based Agents and Graphs",
        "context": "cial media content analysis, attracting many researchers to explore this field. Therefore, how to effectively measure group polarization has be- come a critical topic. Measuring group polarization on social media presents several challenges that have not yet been addressed by existing solutions. First, social media group polarization measurement involves processing vast amounts of text, which poses a significant challenge for information extraction. Second, social media texts often contain hard-to- understand content, including sarcasm, memes, and internet slang. Ad- ditionally, group polarization research focuses on holistic analysis, while texts is typically fragmented. To address these challenges, we designed a solution based on a multi-agent system and used a graph-structured Community Sentiment Network (CSN) to represent polarization states. Furthermore, we developed a metric called Community Opposition In- dex (COI) based on the CSN to quantify polarization. Finally, we tested our multi-agent system through a zero-shot stance detection task and achieved outstanding results. In summary, the proposed approach has significant value in terms of usability, accuracy, and interpretability. Keywords: Multi-Agent System · Group Polarization · LLM-Based Agent · Social Media 1 With the development of internet technology, social media has gained widespread popularity. GLOBAL DIGITAL REPORT [11] indicate that platforms such as Facebook, YouTube, and TikTok boast billions of users worldwide. Social me- dia has become a key avenue for the public to express opinions and engage in discussions. Its anonymity and convenience enable users to freely express their true views, thereby shaping social media public opinion. As a result, research in this field has also flourished. In these studies, research from the perspective of group polarization holds a significant position. The concept of group polarization was first introduced by Stoner [37], who observed that group decisions tend to be more extreme compared to individual decisions [16]. In the internet era, group polarization is broadly defined as the divergence of public opinions or stances. Building on arXiv:2411.12196v1  [cs.CY]  19 Nov 2024",
        "pdf_filename": "A_More_Advanced_Group_Polarization_Measurement_Approach_Based_on_LLM-Based_Agents_and_Graphs.pdf",
        "num_chunks": 877
    },
    {
        "title": "A Multi-Modal Unsupervised Machine Learning Approach for Biomedical Signal Processing in CPR",
        "context": "Cardiopulmonary resuscitation (CPR) is a critical, life-saving intervention aimed at restoring blood circulation and breathing in individuals experiencing cardiac arrest or respiratory failure. Accurate and real-time analysis of biomedical signals during CPR is essential for monitoring and decision- making, from the pre-hospital stage to the intensive care unit (ICU). However, CPR signals are often corrupted by noise and artifacts, making precise interpretation challenging. Traditional denoising methods, such as filters, struggle to adapt to the varying and complex noise patterns present in CPR signals. Given the high-stakes nature of CPR, where rapid and accurate responses can determine survival, there is a pressing need for more robust and adaptive denoising techniques. In this context, an unsupervised machine learning (ML) methodology is particularly valuable, as it removes the dependence on labeled data, which can be scarce or impractical in emergency scenarios. This paper introduces a novel unsupervised ML approach for denoising CPR signals using a multi-modality framework, which leverages multiple signal sources to enhance the denoising process. The proposed approach not only improves noise reduction and signal fidelity but also preserves critical inter-signal correlations (0.9993) which is crucial for downstream tasks. Furthermore, it outperforms existing methods in an unsupervised context in terms of signal-to-noise ratio (SNR) and peak signal-to-noise ratio (PSNR), making it highly effective for real-time applications. The integration of multi-modality further enhances the system’s adaptability to various biomedical signals beyond CPR, improving both automated CPR systems and clinical decision-making. Keywords: Machine learning; Multi-modality; Biomedical signal; Unsupervised learning; Car- diopulmonary resuscitation(CPR); Signal processing. 1 1.1 PROBLEM STATEMENT Cardiopulmonary resuscitation (CPR) is a vital, life-saving technique that can make a significant difference in medical emergencies by restoring blood flow and breathing in individuals experiencing cardiac arrest or respiratory failure. The primary objective of CPR is to keep the heart pumping and ensure continuous oxygenated blood flow to vital organs, particularly the brain (Raza et al., 2021). The American Heart Association, alongside other global health organizations, underscores the significance of CPR training, not just among healthcare professionals but also among the general public (Hinkelbein et al., 2020), as the majority of cardiac arrests occur outside of hospital settings, where immediate medical intervention may not readily available. In such scenarios, manual, human-driven CPR is predominantly practiced despite several difficulties and challenges that hinder successful CPR. For instance, performing CPR is physically demanding for individuals, even for health care providers for an extended period. arXiv:2411.11869v1  [eess.SP]  3 Nov 2024",
        "pdf_filename": "A_Multi-Modal_Unsupervised_Machine_Learning_Approach_for_Biomedical_Signal_Processing_in_CPR.pdf",
        "num_chunks": 1065
    },
    {
        "title": "Backpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration",
        "context": "prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. 0004-5411/2018/8-ART111 $15.00 https://doi.org/XXXXXXX.XXXXXXX J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018. arXiv:2406.01601v3  [cs.DC]  18 Nov 2024",
        "pdf_filename": "Backpropagation-Free_Multi-modal_On-Device_Model_Adaptation_via_Cloud-Device_Collaboration.pdf",
        "num_chunks": 1096
    },
    {
        "title": "Balancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems",
        "context": "Accurate multi-turn intent classification is essential for advancing conversational AI systems. However, challenges such as the scarcity of comprehensive datasets and the complexity of contextual depen- dencies across dialogue turns hinder progress. This paper presents two novel approaches leveraging Large Language Models (LLMs) to enhance scalability and reduce latency in production dialogue systems. First, we introduce Symbol Tuning, which simplifies intent labels to reduce task complexity and improve performance in multi- turn dialogues. Second, we propose C-LARA (Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework that employs LLMs for data augmentation and pseudo-labeling to gener- ate synthetic multi-turn dialogues. These enriched datasets are used to fine-tune a small, efficient model suitable for deployment. Exper- iments conducted on multilingual dialogue datasets demonstrate significant improvements in classification accuracy and resource efficiency. Our methods enhance multi-turn intent classification accuracy by 5.09%, reduce annotation costs by 40%, and enable scal- able deployment in low-resource multilingual industrial systems, highlighting their practicality and impact. CCS Concepts • Information systems →Language models; Question answer- ing; • Computing methodologies →Intelligent agents. Keywords Multi-turn Intent Classification, Multilingual Large Language Model, Retrieval Augmentation, Computational Linguistics, Language Di- versity, Knowledge Engineering ACM Reference Format: Junhua Liu1,3,∗, Yong Keat Tan2,∗, Bin Fu2,†, Kwan Hui Lim3 . 2024. Balanc- ing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM- Powered Dialog Systems in Production. In Proceedings of Preprint. ACM, New York, NY, USA, 9 pages. https://doi.org/N.A Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Preprint, Working Paper, Nov 2024 © 2024 Copyright held by the owner/author(s). ACM ISBN 978-x-xxxx-xxxx-x/YY/MM https://doi.org/N.A Figure 1: Comparison of instruction tuning and symbol tun- ing. Simplifying verbose intent labels (e.g., “Request to Can- cel Order” →“Cancel Order”) reduces redundancy, enhanc- ing LLM classification performance by 5.09%, addressing key challenges in production intent classification. 1 Dialogue systems are critical for automating interactions between customers and agents, streamlining communication and enhanc- ing user experience. They play a pivotal role in international e- commerce platforms, addressing the increasing demand for in- stantaneous and efficient customer service. Intent classification, a fundamental aspect of natural language understanding in dia- logue systems, involves identifying users’ goals from their inputs, thereby minimizing waiting times and operational costs [22]. User interactions frequently evolve into multi-turn dialogues when de- tailed information is required, complicating the development of multi-turn intent classification (MTIC) models, despite their simi- larity to standard text classification tasks. Additionally, real-world multilingual systems require scalable solutions that uphold inclu- sivity and ethical standards, particularly in low-resource settings. This complexity arises from the need to consider contextual fac- tors like historical utterances and prior intents. Without a proper understanding of session context, the system risks misinterpreting user intentions, which may result in incorrect applications or irrel- evant responses [24]. Consequently, MTIC within dialogue system presents significant challenges. The first challenge is that the length of intents in industrial di- alogue systems is longer compared to general text classification tasks. Figure 1 shows that the real intents comprise several words in our knowledge base because operators(Ops) typically assign intents *Equal Contributions. †Corresponding Author: bin.fu@shopee.com arXiv:2411.12307v1  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Balancing_Accuracy_and_Efficiency_in_Multi-Turn_Intent_Classification_for_LLM-Powered_Dialog_Systems.pdf",
        "num_chunks": 1255
    },
    {
        "title": "Bayesian learning of forest and tree graphical models",
        "context": "",
        "pdf_filename": "Bayesian learning of forest and tree graphical models.pdf",
        "num_chunks": 6450
    },
    {
        "title": "Benchmarking Positional Encodings for GNNs and Graph Transformers",
        "context": "Recent advances in Graph Neural Networks (GNNs) and Graph Transformers (GTs) have been driven by innovations in architectures and Positional Encodings (PEs), which are critical for augmenting node features and capturing graph topol- ogy. PEs are essential for GTs, where topological information would otherwise be lost without message-passing. However, PEs are often tested alongside novel architectures, making it difficult to isolate their effect on established models. To address this, we present a comprehensive benchmark of PEs in a unified frame- work that includes both message-passing GNNs and GTs. We also establish the- oretical connections between MPNNs and GTs and introduce a sparsified GRIT attention mechanism to examine the influence of global connectivity. Our findings demonstrate that previously untested combinations of GNN architectures and PEs can outperform existing methods and offer a more comprehensive picture of the state-of-the-art. To support future research and experimentation in our framework, we make the code publicly available. 1 Graph machine learning has traditionally relied on message-passing neural networks (MPNNs), which work through iterative rounds of neighborhood aggregation (Kipf & Welling, 2016). In each round, nodes update their states by incorporating information from their neighbors along with their own current states. While effective in capturing local graph structures, this approach can strug- gle with modeling long-range dependencies. Graph Transformer (GT) architectures utilize full at- tention mechanisms to circumvent this, but necessitate new methods to integrate graph topology information (Dwivedi & Bresson, 2020). This is similar to how positional encodings (PEs) in Nat- ural Language Processing (NLP) represent token positions within sequences (Vaswani et al., 2017). However, encoding positional information in graphs is more complex than in sequences. Ideally, positional encodings should allow the reconstruction of the graph’s topology from node features and provide useful inductive biases to improve performance (Black et al., 2024). Despite the growing number of new graph transformer architectures and positional encodings, there has been a lack of systematic evaluation comparing these encodings across different GT architectures. This makes it difficult to determine whether observed performance improvements are due to novel encodings or architectural innovations. In this paper, we conduct a comprehensive evaluation of various positional encodings for both message-passing and transformer frameworks. Our goal is to understand the impact of positional encodings on model performance and identify the best combinations of encodings and architectures. By benchmarking state-of-the-art graph transformers with a variety of positional encodings, we provide a clear picture of the current state of the field and offer guidance for future research. Addi- tionally, we further strengthen the theoretical connection between MPNNs and GTs. Although GTs are generally considered fundamentally different due to their use of attention mechanisms, we show that under certain conditions, MPNNs and GTs can be equally expressive, with additional results that extend the scope of previous analyses (Veliˇckovi´c, 2023; M¨uller & Morris, 2024). Specifically, MPNNs can be applied to fully-connected graphs and operate like a GT, while attention mech- anisms can also be adapted for local message-passing. Our theoretical analysis demonstrates that both MPNNs and GTs can have the same expressiveness when the underlying topology of the MPNN 1 arXiv:2411.12732v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Benchmarking_Positional_Encodings_for_GNNs_and_Graph_Transformers.pdf",
        "num_chunks": 4311
    },
    {
        "title": "Benchmarking pre-trained text embedding models in aligning built asset information",
        "context": "Accurate mapping of the built asset information to established data classification systems and taxonomies is crucial for effective asset management, whether for compliance at project handover or ad-hoc data integration scenarios. Due to the complex nature of built asset data, which predominantly comprises technical text elements, this process remains largely manual and reliant on domain expert input. Recent breakthroughs in contextual text representation learning (text embedding), particularly through pre-trained large language models, offer promising approaches that can facilitate the automation of cross-mapping of the built asset data. However, no comprehensive evaluation has yet been conducted to assess these models’ ability to effectively represent the complex semantics specific to built asset technical terminology. This study presents a comparative benchmark of state-of-the-art text embedding models to evaluate their effectiveness in aligning built asset informa- tion with domain-specific technical concepts. Our proposed datasets are derived from two renowned built asset data classification dictionaries. The results of our benchmarking across six proposed datasets, covering three tasks of clustering, re- trieval, and reranking, highlight the need for future research on domain adaptation techniques. The benchmarking resources are published as an open-source library, which will be maintained and extended to support future evaluations in this field. 1 Asset management plays a pivotal role in ensuring optimal performance and extended life span of the built environment through a systematic process of monitoring and maintaining various facilities and equipment. The rapid advancement of digital technologies has led asset owners to increasingly demand enriched digital twins at project handover to support real-time operations and maintenance of the built assets [Love and Matthews, 2019]. Simultaneously, the growing awareness of the benefits of digitized asset management highlights the essential need for federated access to built asset data [Moretti et al., 2023]. This requires aligning extensive data sources and their underlying schema with established data models, classification systems, or taxonomies to facilitate data accessibility for diverse stakeholders and improve interoperability across various software environments. However, aligning built asset data with pre-defined classification systems poses significant challenges in practice. A key challenge stems from the multi-source and multi-disciplinary nature of built asset data, which leads to the use of diverse formats and terminologies across different projects and stakeholders. For example, the terminology that architects utilize to describe the specifications for a particular building component or system can vastly differ from those used by structural engineers or subcontractors. Moreover, the structures of domain-specific classifications used in different disciplines often vary ∗For correspondence, please contact: mehrzad.shahinmoghadam.1@ens.etsmtl.ca Preprint. Under review. arXiv:2411.12056v1  [cs.CL]  18 Nov 2024",
        "pdf_filename": "Benchmarking_pre-trained_text_embedding_models_in_aligning_built_asset_information.pdf",
        "num_chunks": 1066
    },
    {
        "title": "BiSSL Bilevel Optimization for Self-Supervised Pre-Training and Fine-Tuning",
        "context": "In this work, we present BiSSL, a first-of-its-kind training framework that in- troduces bilevel optimization to enhance the alignment between the pretext pre- training and downstream fine-tuning stages in self-supervised learning. BiSSL formulates the pretext and downstream task objectives as the lower- and upper- level objectives in a bilevel optimization problem and serves as an intermediate training stage within the self-supervised learning pipeline. By more explicitly modeling the interdependence of these training stages, BiSSL facilitates enhanced information sharing between them, ultimately leading to a backbone parameter initialization that is better suited for the downstream task. We propose a training algorithm that alternates between optimizing the two objectives defined in BiSSL. Using a ResNet-18 backbone pre-trained with SimCLR on the STL10 dataset, we demonstrate that our proposed framework consistently achieves improved or com- petitive classification accuracies across various downstream image classification datasets compared to the conventional self-supervised learning pipeline. Quali- tative analyses of the backbone features further suggest that BiSSL enhances the alignment of downstream features in the backbone prior to fine-tuning. 1 In the absence of sufficient labeled data, self-supervised learning (SSL) has emerged as a promis- ing approach for training deep learning models. Rather than relying solely on labeled data, the SSL framework aims to learn representations from unlabeled data which proves beneficial for subsequent use on various downstream tasks. These representations are learned by solving a pretext task, which utilizes supervisory signals extracted from the unlabeled data itself. Extensive efforts has gone into designing effective pretext tasks, achieving state-of-the-art or competitive performance in various fields such as computer vision (Chen et al., 2020b; Bardes et al., 2022; He et al., 2020; Grill et al., 2020; Caron et al., 2020; 2021; He et al., 2022; Oquab et al., 2024), audio signal processing (Schnei- der et al., 2019; Baevski et al., 2020; Hsu et al., 2021; Niizumi et al., 2021; Chung & Glass, 2018; Chung et al., 2019; Yadav et al., 2024) and natural language processing (Devlin et al., 2019; Lewis et al., 2019; Brown et al., 2020; He et al., 2021; Touvron et al., 2023). Making a self-supervised pre-trained backbone suitable for a downstream task typically involves attaching additional layers that are compatible with that task, followed by fine-tuning the entire or parts of the composite model in a supervised manner (Zhai et al., 2019; Dubois et al., 2022). When a backbone is pre-trained on a distribution that differs from the distribution of the downstream data, the representations learned during pre-training may not be initially well-aligned with the downstream task. During fine-tuning, this distribution misalignment could cause relevant semantic information, learned during the pre-training phase, to vanish from the representation space (Zaiem et al., 2024; Chen et al., 2020a; Boschini et al., 2022). A potential strategy for alleviating the negative effects of these distribution discrepancies would be to enhance the alignment between the pretext pre-training and downstream fine-tuning stages. However, since the conventional SSL pipeline treats these stages as two disjoint processes, this poses a significant challenge in devising a strategy that enhances such alignment while not compromising on the benefits that SSL offers. 1 arXiv:2410.02387v2  [cs.LG]  19 Nov 2024",
        "pdf_filename": "BiSSL_Bilevel_Optimization_for_Self-Supervised_Pre-Training_and_Fine-Tuning.pdf",
        "num_chunks": 1440
    },
    {
        "title": "Brain-inspired Computing Based on Deep Learning for Human-computer Interaction A Review",
        "context": "Brain-inspired intelligence is a kind of machine intelli- gence which is inspired by neural mechanism and cognitive behavior mechanism by means of computational modeling and realized by software and hardware cooperation. Brain- inspired intelligence system is brain-inspired in information processing mechanism and human-like in cognitive behavior and intelligence level. Human brain activity is a complex and continuous dynamic process, and its complexity is far beyond the upper limit that can be simulated by current computing resources, so people have not given up the exploration of the brain. Brain-inspired computing is founded upon the structural framework and operational principles of the human brain, and integrates the current computational development path of computer science and neuroscience[1, 2, 3, 4]. Researchers are constantly trying to understand the neural mechanisms and cognitive behavior through the study of the brain. Traditional DL necessitates an extensive collection of annotated datasets for effective training, and manual annotated data is expensive and affected by human subjective consciousness, so the annotation results are not completely accurate. In contrast, the brain weighs about 2.5 pounds and consumes only about 40% to 60% of the body’s blood sugar[5]. If people can use their own physiological data to decode text or speech and complete codec tasks similar to machine translation, it can not only save manpower, but All authors contributed equally to this work. ∗Corresponding author. yubihui@sict.ac.cn (B. Yu); zhangsibo22@mails.ucas.edu.cn (S. Zhang); Zhoulilivip@126.com (L. Zhou); weijingxuan20@mails.ucas.edu.cn (J. Wei); sunlinzhuang21@mails.ucas.ac.cn (L. Sun); buliping@sict.ac.cn (L. Bu) also have important significance and value for the cognitive mechanism and cognitive ability of the human brain. The continuous exploration of the brain forms the basis of brain-inspired computing, which not only draws inspiration from the complex structure and operation principle of the brain, but also focuses on the innovative use of physiological data of brain signals for practical applications. Brain-inspired computing models can be applied in human-computer in- teraction(HCI). And a typical application is brain-computer interface(BCI) which aims to establish a channel between the brain and the external environment, which does not depend on the peripheral nervous system, and realizes the information exchange and control between the brain and external devices with processing or computing capabilities. Through the acquisition, analysis and processing of brain signals, human can directly interact with computers without relying on external devices. For example, the main goal of cognitive BCI is to understand and analyze the processing mechanism of speech information in human brain. It can help patients overcome the difficulties in language expression and text input by recognizing the ideas expressed by brain signals and converting them into speech or text output[47]. As an emerging research field, the exact definition of brain-inspired computing is still unclear. This field covers computational theory, architecture design, hardware specifica- tions, etc., while learning from brain information processing mechanisms and biological physiological structures to build a variety of models and algorithms[48]. Some comprehensive studies systematically review the research progress in the field of brain-inspired computing from multiple perspectives. Some studies focus on brain-inspired computing in a narrow sense, such as spike-based neural mimic computing in vision B.H.Yu et al. Page 1 of 26 arXiv:2312.07213v4  [cs.AI]  19 Nov 2024",
        "pdf_filename": "Brain-inspired_Computing_Based_on_Deep_Learning_for_Human-computer_Interaction_A_Review.pdf",
        "num_chunks": 2907
    },
    {
        "title": "Building Trust Foundations of Security, Safety and Transparency in AI",
        "context": "This paper explores the rapidly evolving ecosystem of publicly available AI models, and their potential implications on the security and safety landscape. As AI models become increasingly prevalent, understanding their potential risks and vulnerabilities is crucial. We review the current security and safety scenarios while highlighting challenges such as tracking issues, remediation, and the apparent absence of AI model lifecycle and ownership processes. Comprehensive strategies to enhance security and safety for both model developers and end-users are proposed. This paper aims to provide some of the foundational pieces for more standardized security, safety, and transparency in the development and operation of AI models and the larger open ecosystems and communities forming around them. Keywords Public models, Security, Safety, AI, LLM, Generative AI, AI Ecosystem",
        "pdf_filename": "Building_Trust_Foundations_of_Security,_Safety_and_Transparency_in_AI.pdf",
        "num_chunks": 916
    },
    {
        "title": "ByteScience Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Lar",
        "context": "to supply summarization ability from long context to structured information. However, extracting structured knowledge from scientific text by NLP models remains a challenge because of its domain-specific nature to complex data preprocessing and the granularity of multi-layered device-level information. To address this, we introduce ByteScience, a non-profit cloud-based auto fine- tuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora. The platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to natural science. The platform was built on Amazon Web Services (AWS) and provides an automated, user-friendly workflow for custom model development and data extraction. The platform achieves remarkable accuracy with only a small amount of well- annotated articles. This innovative tool streamlines the transition from the science literature to structured knowledge and data and benefits the advancements in natural informatics. Demo Video Index Terms—component, formatting, style, styling, insert AI has the potential to revolutionize scientific discovery (AI4Science [1]), but challenges remain. Scientific knowledge is scattered across documents, making it hard to fully leverage past research. LLMs offer a promising solution but require structured texts, including converting PDFs and generating fine-tuning examples for NLP tasks. While machine learning models are used in fields like drug discovery [2], protein design [3], and crystal structure generation [4], limited struc- tured data hinders their effectiveness. Databases like Mate- rials Project [5] and NOMAD [6] cover only a fraction of data, leaving much unstructured information untapped. This gap presents an opportunity for AI to accelerate discovery. Although converting documents to markup is well-studied, extracting complex relationships remains challenging but es- sential for building knowledge graphs and fine-tuning datasets. • Contextual Dependency: Relationships in scientific texts often depend heavily on context that may span multiple sentences or sections. For instance, a material’s properties might be discussed concerning its synthesis method, as described in paragraphs several pages before. • Implicit Connections: Many relationships in scientific writ- ing are implied rather than explicitly stated, requiring deep domain knowledge to infer correctly. • Hierarchical Structures: Scientific documents frequently contain nested relationships, such as experiment subsets or multi-step processes, which are challenging to represent in flat data structures. • Cross-Reference Complexity: Relationships often span dif- ferent document parts, such as tables, figures, and citations, requiring holistic understanding. • Domain-Specific Semantics: Each scientific field has unique terminology and conventions, complicating universal extraction methods. For example, pseudocode and flowcharts may be unfamiliar in other domains. Traditional methods like MatKG [7], which define rela- tionships by entity co-occurrence, often miss the nuances of scientific knowledge. While useful, they risk oversimplifying complex relationships. Advanced techniques are needed to better capture this complexity for improved knowledge extrac- tion in AI-driven scientific discovery. Therefore, we introduce ByteScience, a cloud-based platform featuring an auto-fine- tuned LLM to extract structured scientific data and synthesize new scientific knowledge from extensive scientific corpora. We conclude as follows: 1) Tailored with DARWIN [8], an open-source state-of-the-art nature-science LLM, to provide research focus utilization; 2) Zero-code user-friendly semi-automated annotation and processing for uploaded science documents; 3) A personalized and domain-specific auto fine-tuning LLM that requires only a single fully annotated piece of literature; 4) Time efficiency high-quality science data extraction from millions of papers for less than a second per article. arXiv:2411.12000v1  [cs.CL]  18 Nov 2024",
        "pdf_filename": "ByteScience_Bridging_Unstructured_Scientific_Literature_and_Structured_Data_with_Auto_Fine-tuned_Lar.pdf",
        "num_chunks": 452
    },
    {
        "title": "Can Agents Spontaneously Form a Society Introducing a Novel Architecture for Generative Multi-Agents",
        "context": "Generative agents have demonstrated impressive capabilities in specific tasks, but most of these frameworks focus on independent tasks and lack attention to social interactions. We introduce a gen- erative agent architecture called ITCMA-S, which includes a basic framework for individual agents and a framework called LTRHA that supports social interactions among multi-agents. This archi- tecture enables agents to identify and filter out behaviors that are detrimental to social interactions, guiding them to choose more favorable actions. We designed a sandbox environment to simu- late the natural evolution of social relationships among multiple identity-less agents for experimental evaluation. The results showed that ITCMA-S performed well on multiple evaluation indicators, demonstrating its ability to actively explore the environment, recog- nize new agents, and acquire new information through continuous actions and dialogue. Observations show that as agents establish connections with each other, they spontaneously form cliques with internal hierarchies around a selected leader and organize collective activities. KEYWORDS Generative agents, Multi agent system, Social interaction, LLM 1 Large language models (LLMs) have contributed to significant progress in the field of natural language processing and are widely used in various domains, such as machine translation [44], dialogue generation [9], and content creation [46]. These models are capable of correctly parsing and generating complex sentence structures and have demonstrated unprecedented capabilities in understand- ing language. However, LLMs often lack true comprehension and rely more on pattern matching and probabilistic predictions [10]. It has also been difficult to create systems to achieve human-like systematic generalization [22]. To overcome these problems, re- searchers have introduced LLM-based agents. This has allowed the incorporation of external knowledge bases to supplement a model’s knowledge gaps in specific domains [43]. They can also be used to decompose a complex task into multiple simpler tasks to achieve hierarchical processing [41]. On this basis, Park et al. [31] introduced a novel LLM-based agent, namely, a generative agent. This agent simulates trustwor- thy human behavior. Generative agents have the ability to make multifaceted inferences about an environment, themselves, and other individuals in the environment. They can design daily ac- tivity plans based on their own characteristics and experiences and adjust their plans to changes as they occur. When a situation changes, they can flexibly update plans to ensure adaptation to it. The importance of this progress cannot be ignored. In human – computer interaction, especially in virtual assistants, customer service robots, and even more complex systems, such as self-driving cars and smart homes, the ability of agents to generate believable human behavior is crucial. Generative agents can support more adaptive and flexible interaction processes. This ability not only en- hances the system’s responsiveness to dynamic situations but also brings human–computer interaction closer to natural behavioral patterns in interpersonal communication. In addition, by simulat- ing human behavior, generative agents can demonstrate autonomy and sociality in various complex situations, making the interac- tion process smoother and more intuitive. This feature plays an important role in improving the user experience and increasing the trustworthiness of a system [16]. However, existing generative agent architectures still face many challenges. While traditional agent structures are good at process- ing and generating behaviors, they are primarily designed for iso- lated tasks and, thus, mostly lack a focus on sociality. This often makes it difficult for them to model and apply the nuances of social interactions, leading them to focus only on completing tasks and overlook behaviors that promote social connections. This is clearly not conducive to cooperation among multiple agents and may lead to behaviors that are detrimental to the group [34]. In scenarios involving multiple agents, a lack of structured social behavior may lead to disjointed or even chaotic interactions. To truly harness the potential of these agents in domains requiring interaction, there is an urgent need to explore their ability to participate in social interactions, establish relationships, and exhibit emerging social behaviors. In this paper, we improve upon an existing LLM-based agent architecture (the internal time-consciousness machine based agent [ITCMA] introduced by [49]) and propose ITCMA-S (the “S” signi- fies our contribution of social interaction) architecture to enable agents to adapt to multi-agent interaction scenarios. It contains a arXiv:2409.06750v2  [cs.MA]  19 Nov 2024",
        "pdf_filename": "Can_Agents_Spontaneously_Form_a_Society_Introducing_a_Novel_Architecture_for_Generative_Multi-Agents.pdf",
        "num_chunks": 1516
    },
    {
        "title": "Can EDA Tool Feedback Improve Verilog Generation by LLMs",
        "context": "servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM Manuscript submitted to ACM 1 arXiv:2411.11856v1  [cs.AR]  1 Nov 2024",
        "pdf_filename": "Can_EDA_Tool_Feedback_Improve_Verilog_Generation_by_LLMs.pdf",
        "num_chunks": 1030
    },
    {
        "title": "CATCH Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs",
        "context": "Large Vision-Language Model (LVLM) systems have demonstrated impressive vision-language reasoning capabilities but suffer from pervasive and severe hal- lucination issues, posing significant risks in critical domains such as healthcare and autonomous systems. Despite previous efforts to mitigate hallucinations, a persistent issue remains: visual defect from vision-language misalignment, cre- ating a bottleneck in visual processing capacity. To address this challenge, we develop Complementary Adaptive Token-level Contrastive Decoding to Miti- gate Hallucinations in LVLMs (CATCH), based on the Information Bottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) for visual information separation, Non-Visual Screening (NVS) for hallucination detec- tion, and Adaptive Token-level Contrastive Decoding (ATCD) for hallucination mitigation. CATCH addresses issues related to visual defects that cause dimin- ished fine-grained feature perception and cumulative hallucinations in open-ended 1 arXiv:2411.12713v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "CATCH_Complementary_Adaptive_Token-level_Contrastive_Decoding_to_Mitigate_Hallucinations_in_LVLMs.pdf",
        "num_chunks": 1047
    },
    {
        "title": "CCIS-Diff A Generative Model with Stable Diffusion Prior for Controlled Colonoscopy Image Synthesis",
        "context": "Colonoscopy is crucial for identifying adenomatous polyps and preventing colorectal cancer. However, developing ro- bust models for polyp detection is challenging by the limited size and accessibility of existing colonoscopy datasets. While previous efforts have attempted to synthesize colonoscopy images, current methods suffer from instability and insuffi- cient data diversity. Moreover, these approaches lack precise control over the generation process, resulting in images that fail to meet clinical quality standards. To address these challenges, we propose CCIS-DIFF, a Controlled genera- tive model for high-quality Colonoscopy Image Synthesis based on a Diffusion architecture. Our method offers pre- cise control over both the spatial attributes (polyp location and shape) and clinical characteristics of polyps that align with clinical descriptions. Specifically, we introduce a blur mask weighting strategy to seamlessly blend synthesized polyps with the colonic mucosa, and a text-aware attention mechanism to guide the generated images to reflect clini- cal characteristics. Notably, to achieve this, we construct a new multi-modal colonoscopy dataset that integrates images, mask annotations, and corresponding clinical text descrip- tions. Experimental results demonstrate that our method generates high-quality, diverse colonoscopy images with fine control over both spatial constraints and clinical consistency, offering valuable support for downstream segmentation and diagnostic tasks. Index Terms— Colonoscopy Image Synthesis, Con- trolled Synthesis, Stable Diffusion Colonoscopy is an essential tool for detecting adenomatous polyps and reducing rectal cancer mortality rates [1, ?, ?]. However, training models for automatic polyp detection is challenging due to the small scale of available colonoscopy ∗Corresponding email: mafei@gml.ac.cn, yangli@sz.tsinghua.edu.cn Mask + Text: The polyp is pinkish, with a smooth texture against a background of darker mucosa with some yellowish debris. ArSDM ControlNet CCIS-DIFF (Ours) Fig. 1. In contrast to ArSDM [2] and ControlNet [3], CCIS- DIFF can utilize not only mask but also textual description to generate high-fidelity, text-consistent colonoscopy images. datasets, making it difficult to have sufficient robustness and generalization that meet real-world clinical demands. To address this problem, previous methods [4, 5, 2, 6] pri- marily relied on generative adversarial networks or diffusion models to synthesize more colonoscopy images. Although these efforts aim to address the data scarcity problem, they struggle to generate a sufficiently diverse and high-quality im- age, and the generation process lacks adequate control, lead- ing to images that fail to meet clinical requirements for prac- tical use. As illustrated in Fig. 1, ArSDM [2] only utilizes the mask to synthesize the colonoscopy image and the gener- ated image is of poor quality and contains noise. Meanwhile, large-scale text-to-image (T2I) diffusion models such as Sta- ble Diffusion [7] and DALL·E [8] have demonstrated remark- able capabilities in generating images from various prompts. This raises an important question: Can colonoscopy images be generated in a controlled manner using a pre-trained large- scale T2I model? In response, we present an innovative gener- ative method to synthesize high-quality colonoscopy images in a controlled manner. The main contributions of this paper can be summarized as follows: • We propose a novel generative model, named CCIS-DIFF, which offers fine control over both the spatial attributes and clinical characteristics of polyps, enabling more clin- ically consistent image synthesis for practical use. • We introduce the blur mask weighting strategy to en- sure seamless integration of synthesized polyps with the colonic mucosa, along with a text-aware attention mecha- nism that incorporates textual information into the gener- ation process, to enable customization of polyp images. arXiv:2411.12198v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "CCIS-Diff_A_Generative_Model_with_Stable_Diffusion_Prior_for_Controlled_Colonoscopy_Image_Synthesis.pdf",
        "num_chunks": 619
    },
    {
        "title": "Challenges of Artificial Intelligence -- From Machine Learning and Computer Vision to Emotional Intelligence",
        "context": "",
        "pdf_filename": "Challenges of Artificial Intelligence -- From Machine Learning and Computer Vision to Emotional Intelligence.pdf",
        "num_chunks": 10127
    },
    {
        "title": "Chat Bankman-Fried an Exploration of LLM Alignment in Finance",
        "context": "Advancements in large language models (LLMs) have renewed concerns about AI alignment—the consistency between human and AI goals and values. As various jurisdictions enact legislation on AI safety, the concept of alignment must be defined and measured across different domains. This paper proposes an experimental framework to assess whether LLMs adhere to ethical and legal standards in the relatively unexplored context of finance. We prompt nine LLMs to impersonate the CEO of a financial institution and test their willingness to misuse customer assets to repay outstanding corporate debt. Beginning with a baseline configuration, we adjust preferences, incentives and constraints, analyzing the impact of each adjustment with logistic regression. Our findings reveal significant heterogeneity in the baseline propensity for unethical behavior of LLMs. Factors such as risk aversion, profit expectations, and regulatory environment consistently influence misalignment in ways predicted by economic theory, although the magnitude of these effects varies across LLMs. This paper highlights both the benefits and limitations of simulation-based, ex post safety testing. While it can inform financial authorities and institutions aiming to ensure LLM safety, there is a clear trade-off between generality and cost. 1 Large Language Models (LLMs) are rapidly transforming how we approach problems across various domains, thanks to their improved natural language understanding [Min et al., 2023] and their advanced reasoning capabilities [Wei et al., 2022, Huang and Chang, 2023]. Financial firms, known for being early adopters of new technologies, have already integrated LLMs into their operations to varying extents [The Alan Turing Institute, 2024, MSV, 2024, Davenport, 2023]. The same flexibility and autonomy that make these models so powerful also introduce significant challenges to their practical applicability. Due to their complex architectures, LLMs are prone to issues like hallucinations [Ji et al., 2023] and biases [Gallegos et al., 2024], which can result ∗The opinions expressed in this paper are personal and should not be attributed to the Bank of Italy. Preprint. Under review. arXiv:2411.11853v1  [cs.CY]  1 Nov 2024",
        "pdf_filename": "Chat_Bankman-Fried_an_Exploration_of_LLM_Alignment_in_Finance.pdf",
        "num_chunks": 2663
    },
    {
        "title": "CLIP Unreasonable Potential in Single-Shot Face Recognition",
        "context": "sion, designed to identify and authenticate individuals by an- alyzing facial patterns and features. This field intersects with artificial intelligence, image processing, and machine learning, with applications in security, authentication, and personalization. Traditional approaches in facial recognition focus on capturing facial features like the eyes, nose, and mouth and matching these against a database to verify identities. However, chal- lenges such as high false-positive rates have persisted, often due to the similarity among individuals’ facial features. Re- cently, Contrastive Language-Image Pretraining (CLIP), a model developed by OpenAI, has shown promising advancements by linking natural language processing with vision tasks, allowing it to generalize across modalities. Using CLIP’s vision-language correspondence and single-shot finetuning, the model can achieve lower false-positive rates upon deployment without the need of mass facial features extraction. This integration demonstrating CLIP’s potential to address persistent issues in face recognition model performance without complicating our training paradigm. Index Terms—Face recognition, image classification, multi- modal learning Face recognition [1]–[3] is a pivotal task within the realm of computer vision, wherein algorithms are designed to iden- tify and authenticate individuals by analyzing and comparing patterns in facial features. It’s a multifaceted field that inter- sects with various disciplines like artificial intelligence, image processing and machine learning. At its core, facial recognition involves capturing facial images and videos, extracting unique characteristics such as the arrangement of eyes, nose, and mouth, and then matching these features against a database of known faces to make identifications or verifications. This technology has found widespread applications across diverse sectors, ranging from security and surveillance to authentication and personalization. CLIP [4], or Contrastive Language-Image Pretraining, is a ground breaking model developed by OpenAI that transcends traditional boundaries between natural language processing and computer vision. Unlike conventional models that special- ize in either text or image modeling, CLIP learns to understand both modalities simultaneously. This means it is possible that CLIP can perform a wide range of tasks across different domains, including those related to facial recognition. One of the big problem distinct facial recognition with other classification task is subjectivity to false-positive result, misunderstanding someone face with others. Upon testing, we realize that by using the vision-language correspondence from CLIP features, model only require a single shot finetuning while reducing false-positive results significantly without any state-of-the-art methods for extracting facial features from massive datasets. II. BACKGROUND A. Face recognition Face recognition has emerged as a powerful tool in computer vision, enabling the identification or verification of individuals based on their facial features. This technology has applica- tions in various fields such as security, law enforcement, and personal device authentication. Early approaches focused on geometrical models and eigenface techniques [5]. With the advent of machine learning, methods such as Local Binary Patterns (LBP) [6] and Fisherfaces [7] improved robustness under various lighting conditions and facial expressions. The rise of deep learning has further revolutionized the field, with convolutional neural networks (CNNs) becoming the backbone of face recognition systems. Groundbreaking models like DeepFace [8] and FaceNet [9] introduced face embeddings that provided remarkable accuracy and efficiency, even in large-scale applications. More recent advancements leverage deep residual networks and novel loss functions to achieve even higher accuracy [10], [11]. These advancements underscore the rapid development of face recognition and its growing importance across technology sectors. B. CLIP and its applications CLIP [4] model has made significant strides in connecting vision and language modalities, enabling zero-shot capabilities across various computer vision tasks [12]–[14]. CLIP [4] lever- ages a contrastive learning framework, aligning text descrip- tions and images in a shared embedding space. This alignment allows CLIP [4] to generalize across tasks without the need for task-specific training. CLIP [4] has shown versatility in object detection [15], [16], image generation [17], [18], and scene understanding [19]. arXiv:2411.12319v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "CLIP_Unreasonable_Potential_in_Single-Shot_Face_Recognition.pdf",
        "num_chunks": 526
    },
    {
        "title": "CodeXEmbed A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval",
        "context": "Despite the success of text retrieval in many NLP tasks, code retrieval remains a largely un- derexplored area. Most text retrieval systems are tailored for natural language queries, often neglecting the specific challenges of retriev- ing code. This gap leaves existing models un- able to effectively capture the diversity of pro- gramming languages and tasks across differ- ent domains, highlighting the need for more focused research in code retrieval. To address this, we introduce CODEXEMBED, a family of large-scale code embedding models rang- ing from 400M to 7B parameters. Our novel training pipeline unifies multiple programming languages and transforms various code-related tasks into a common retrieval framework, en- hancing model generalizability and retrieval performance. Our 7B model sets a new state-of- the-art (SOTA) in code retrieval, outperforming the previous leading model, Voyage-Code, by over 20% on CoIR benchmark. In addition to excelling in code retrieval, our models demon- strate competitive performance on the widely adopted BeIR text retrieval benchmark, offer- ing versatility across domains. Experimental results demonstrate that improving retrieval performance significantly enhances end-to-end Retrieval-Augmented Generation (RAG) per- formance for code-related tasks. 1 Large Language Models (LLMs) have demon- strated exceptional performance across numerous Natural Language Processing (NLP) tasks. How- ever, they often struggle to produce faithful an- swers and may lack up-to-date or domain-specific knowledge. To bridge this gap, retrieval-augmented generation (RAG) (Cai et al., 2022; Cheng et al., 2024) techniques have gained prominence, integrat- ing Information Retrieval (IR) systems with LLMs to enhance their access to relevant external informa- tion. This synergy has drawn significant attention recently, leading to the development of various re- trieval models (Wang et al., 2022; Chen et al., 2024) based on BERT (Kenton and Toutanova, 2019) and other LLMs with sizes exceeding 1 billion parame- ters (Wang et al., 2023; Moreira et al., 2024; Meng et al., 2024). Despite these advancements, standard IR methods, while effective in text-based retrieval, often fall short in specialized domains such as code retrieval (Husain et al., 2019). Code retrieval is critical for accelerating devel- opment processes and improving code quality. Un- like general text retrieval, code retrieval enables developers to quickly locate relevant code snippets, explanations, bug analyses, summaries, and simi- lar instances. Effective code retrieval systems are now integrated into commercial products like VS Code (Del Sole, 2021) and GitHub Copilot (Wer- melinger, 2023; Yeti¸stiren et al., 2023), enhanc- ing productivity. Code-RAG systems (Parvez et al., 2021; Liu et al.; Jimenez et al., 2024; Wang et al., 2024) also leverage code retrieval to minimize hal- lucinations in generated code from LLMs, ensur- ing more accurate outputs. However, traditional text retrieval models often struggle with code be- cause they focus on linguistic patterns, while code retrieval must handle elements like syntax, vari- able dependencies, control flow, and API usage. Despite the importance of code-specific models, existing ones like CodeBERT (Feng et al., 2020), CodeGPT (Lu et al.), and UniXcoder (Guo et al., 2022) are based on smaller BERT models (Kenton and Toutanova, 2019). While large-scale models for retrieval have become popular, only Voyage- Code has followed this approach for code retrieval, but it remains a closed model, leaving a gap for open-source, large-scale code retrieval models. In this work, we introduce CODEXEMBED, a family of open-source embedding models tai- lored for both code and text, available in sizes of 400 million, 2 billion, and 7 billion parameters. CODEXEMBED introduces a generalizable training 1 arXiv:2411.12644v1  [cs.SE]  19 Nov 2024",
        "pdf_filename": "CodeXEmbed_A_Generalist_Embedding_Model_Family_for_Multiligual_and_Multi-task_Code_Retrieval.pdf",
        "num_chunks": 1809
    },
    {
        "title": "Combining Induction and Transduction for Abstract Reasoning",
        "context": "Wen-Ding Li*1 Keya Hu*2 Carter Larsen1 Yuqing Wu1 Simon Alford1 Caleb Woo1 Spencer M. Dunn1 Hao Tang1 Michelangelo Naim3 Dat Nguyen3 Wei-Long Zheng2 Zenna Tavares†3 Yewen Pu†4 Kevin Ellis†1 1Cornell 2Shanghai Jiao Tong University 3Basis 4Autodesk *co-leads †co-advising correspondence: {wl678,kellis}@cornell.edu, hu_keya@sjtu.edu.cn When learning an input-output mapping from very few examples, is it better to first infer a latent function that explains the examples, or is it better to directly predict new test outputs, e.g. using a neural network? We study this question on ARC-AGI by training neural models for induction (inferring latent functions) and transduction (directly predicting the test output for a given test input). We train on synthetically generated variations of Python programs that solve ARC-AGI train- ing tasks. We find inductive and transductive models solve different kinds of test problems, despite having the same training problems and sharing the same neural architecture: Inductive program synthesis excels at precise computations, and at composing multiple concepts, while transduction succeeds on fuzzier perceptual concepts. Ensembling them approaches human-level performance on ARC-AGI. 1 Robust generalization from few examples remains one of the most important ways in which human ing: Given just a few training input-outputs xtrain, ytrain, together with a test input xtest, the idea is to predict the corresponding test output ytest using reasoning strategies such as analogical reasoning, chain-of-thought, inductive program synthesis, or transductive prediction (Thoms et al., 2023; Wang et al., 2024; Witt et al., 2023; Lee et al., 2024; Tang et al., 2024a; Hocquette & Cropper, 2024; Butt few-shot learning benchmark that tests the ability to rapidly learn a diverse range of new skills, and apply them to new situations. Each ARC-AGI task is presented as input-outputs over colored grids, but can engage concepts such as occlusion, pathfinding, collision, symmetry, gravity, bouncing, counting, etc., making ARC-AGI essentially a composite of many reasoning datasets, and one of the more interesting unsolved benchmarks that stresses broad-coverage few-shot learning (Figure 1). object contact gravity symmetry, occlusion jumping bouncing mazes pathfinding growth task typically has 2-5 input-output examples. Here we show just one input-output example per task. Here we study neural methods for induction and transduction, using few-shot learning problems from ARC-AGI as our testbed. Induction means first finding a function f where f(xtrain) ≈ytrain, and then predicting ytest = f(xtest). Transduction instead outputs ytest without explicit construction 1 arXiv:2411.02272v3  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Combining_Induction_and_Transduction_for_Abstract_Reasoning.pdf",
        "num_chunks": 2555
    },
    {
        "title": "Comparing Prior and Learned Time Representations in Transformer Models of Timeseries",
        "context": "What sets timeseries analysis apart from other machine learning exercises is that time representation becomes a primary aspect of the experiment setup, as it must adequately represent the temporal relations that are relevant for the application at hand. In the work described here we study wo different variations of the Transformer architecture: one where we use the fixed time representation pro- posed in the literature and one where the time representation is learned from the data. Our experiments use data from predicting the energy output of solar panels, a task that exhibits known peri- odicities (daily and seasonal) that is straight-forward to encode in the fixed time representation. Our results indicate that even in an experiment where the phenomenon is well-understood, it is diffi- cult to encode prior knowledge due to side-effects that are difficult to mitigate. We conclude that research work is needed to work the human into the learning loop in ways that improve the robustness and trust-worthiness of the network. CCS CONCEPTS • Computing methodologies →Neural networks; Artificial intelligence; Supervised learning; • Mathematics of computing →Time series analysis. ACM Reference Format: Natalia Koliou, Tatiana Boura, Stasinos Konstantopoulos, George Meramve- liotakis, and George Kosmadakis. 2024. Comparing Prior and Learned Time Representations in Transformer Models of Timeseries. In 13th Conference on Artificial Intelligence (SETN 2024), September 11–13, 2024, Piraeus, Greece. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3688671.3688747 1 What sets apart timeseries analysis from other machine learning exercises is taking into account the sequence as well as, in most cases, the temporal distance between observations. This makes the representation of time a primary aspect of the experiment setup, ∗Both authors contributed equally to this research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SETN 2024, September 11–13, 2024, Piraeus, Greece © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0982-1/24/09 https://doi.org/10.1145/3688671.3688747 as it must be adequate for representing the temporal relations that are relevant for the application at hand. To elaborate on the various considerations that need to be ad- dressed, first consider that one cannot assume fully observed, uni- formly sampled inputs as there might be gaps in the data, vary- ing sampling rates, and (for multivariate timeseries) misalignment between the time steps of the different variables. This dictates a representation that allows time differences to be computed, so that (for example) September 2023 is ‘closer’ to January 2024 than it is to September 2022. Simple timestamps allow this but do not capture periodicity: Consider, for instance, an application with seasonal periodicity where September 2023 is ‘closer’ to September 2022 than to January 2024. There is a rich relevant literature in both signal processing and in non-parametric statistics, as well as in adapting AI/ML approaches to timeseries processing when facing irregularly sampled and/or sparse data. In particular, deep learning approaches that utilize re- current networks based on Gated Recurrent Units (GRUs) [1], Long Short-Term Memory networks (LSTMs) [2, 3], and ODE-RNNs [4] have shown promising results. In the work described we focus on deep learning methods as well, but specifically on adapting Trans- former models to timeseries analysis. We will first present how the relevant literature handles the representation of time when applying Transformer models to timeseries (Section 2) and then proceed to propose an alternative representation that is expected to out-perform the original representation for our specific application on predicting the energy output of solar panels (Section 3). We close with giving and discussing comparative experimental results (Section 4) and conclusions and future work (Section 5). 2 BACKGROUND Unlike recurrent and differential equation-based architectures which process inputs sequentially, Transformers [6] expect the complete time-series as input and use the attention mechanism to look for relationships between all inputs simultaneously. This has the side- effect that the temporal order is no longer implied by the order in which the inputs are presented to the network, so that input vec- tors must be augmented with features that represent time. But this also creates the opportunity to use time embeddings that represent temporal information in a way that encodes prior knowledge about the data. The most characteristic example is periodicity. When the data is known or suspected to exhibit periodicity, absolute positional encoding [7] encodes time as two features: the sine and the cosine arXiv:2411.12476v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Comparing_Prior_and_Learned_Time_Representations_in_Transformer_Models_of_Timeseries.pdf",
        "num_chunks": 650
    },
    {
        "title": "Complexity Classification in Infinite-Domain Constraint Satisfaction",
        "context": "",
        "pdf_filename": "Complexity Classification in Infinite-Domain Constraint Satisfaction.pdf",
        "num_chunks": 14524
    },
    {
        "title": "Contextual Combinatorial Bandits with Probabilistically Triggered Arms",
        "context": "We study contextual combinatorial bandits with probabilistically triggered arms (C2MAB-T) un- der a variety of smoothness conditions that cap- ture a wide range of applications, such as contex- tual cascading bandits and contextual influence maximization bandits. Under the triggering prob- ability modulated (TPM) condition, we devise the C2-UCB-T algorithm and propose a novel anal- ysis that achieves an ˜O(d √ KT) regret bound, removing a potentially exponentially large factor O(1/pmin), where d is the dimension of contexts, pmin is the minimum positive probability that any arm can be triggered, and batch-size K is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance- adaptive algorithm VAC2-UCB and derive a re- gret bound ˜O(d √ T), which is independent of the batch-size K. As a valuable by-product, our anal- ysis technique and variance-adaptive algorithm can be applied to the CMAB-T and C2MAB set- ting, improving existing results there as well. We also include experiments that demonstrate the im- proved performance of our algorithms compared with benchmark algorithms on synthetic and real- world datasets. The stochastic multi-armed bandit (MAB) problem is a classical sequential decision-making problem that has been widely studied (Robbins, 1952; Auer et al., 2002; Bubeck 1The Chinese University of Hong Kong, Hong Kong SAR, China 2University of Massachusetts Amherst, MA, United States 3California Institute of Technology, CA, United States 4Microsoft Research, Beijing, China. Correspondence to: Xutong Liu <li- uxt@cse.cuhk.edu.hk>, Wei Chen <weic@microsoft.com>, John C.S. Lui <cslui@cse.cuhk.edu.hk>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). et al., 2012). As an extension of MAB, combinatorial multi- armed bandits (CMAB) have drawn attention due to fruitful applications in online advertising, network optimization, and healthcare systems (Gai et al., 2012; Kveton et al., 2015a; Chen et al., 2013; 2016a; Wang & Chen, 2017; Merlis & Mannor, 2019). CMAB is a sequential decision- making game between a learning agent and an environment. In each round, the agent chooses a combinatorial action that triggers a set of base arms (i.e., a super-arm) to be pulled simultaneously, and the outcomes of these pulled base arms are observed as feedback (typically known as semi-bandit feedback). The goal of the agent is to minimize the ex- pected regret, which is the difference in expectation for the overall rewards between always playing the best action (i.e., the action with the highest expected reward) and playing according to the agent’s own policy. Motivated by large-scale applications with a huge number of items (base arms), there exists a prominent line of work that advances the CMAB model: the combinatorial con- textual bandits (or C2MAB for short) (Qin et al., 2014; Li et al., 2016; Takemura et al., 2021). Specifically, C2MAB in- corporates contextual information and adds the simple yet effective linear structure assumption to allow scalability, which provides regret bounds that are independent of the number of base arms m. Despite C2MAB’s success in lever- aging contextual information for better scalability, existing works fail to formulate the general arm triggering process, which is essential to model a wider range of applications, e.g., cascading bandits (CB) and influence maximization (IM), and more importantly, they do not provide satisfying results for settings with probabilistically triggered arms. For example, Qin et al. (2014); Takemura et al. (2021) only con- sider the deterministic semi-bandit feedback for C2MAB. Li et al. (2016); Wen et al. (2017) implicitly consider the arm triggering process for specific CB or IM applications but only gives sub-optimal results with unsatisfying factors (e.g., 1/pmin, K that could be as large as the number of base arms), owing to loose analysis, weak conditions, or inefficient algorithms that explore the unknown parameters too conservatively. To handle the above issues, we enhance the C2MAB frame- work by considering an arm triggering process. Specifically, we propose the general framework of contextual combi- 1 arXiv:2303.17110v3  [cs.LG]  18 Nov 2024",
        "pdf_filename": "Contextual_Combinatorial_Bandits_with_Probabilistically_Triggered_Arms.pdf",
        "num_chunks": 5580
    },
    {
        "title": "Continual Learning with Deep Learning Methods in an Application-Oriented Context",
        "context": "",
        "pdf_filename": "Continual Learning with Deep Learning Methods in an Application-Oriented Context.pdf",
        "num_chunks": 13038
    },
    {
        "title": "Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time Series Node Classification",
        "context": "Multivariate time series (MTS) data is generated through multiple sen- sors across various domains such as engineering application, health mon- itoring, and the internet of things, characterized by its temporal changes and high dimensional characteristics. Over the past few years, many studies have explored the long-range dependencies and similarities in MTS. However, long-range dependencies are difficult to model due to their temporal changes and high dimensionality makes it difficult to obtain similarities effectively and efficiently. Thus, to address these issues, we propose contrast similarity- aware dual-pathway Mamba for MTS node classification (CS-DPMamba). Firstly, to obtain the dynamic similarity of each sample, we initially use temporal contrast learning module to acquire MTS representations. And then we construct a similarity matrix between MTS representations using Fast Dynamic Time Warping (FastDTW). Secondly, we apply the DPMamba to consider the bidirectional nature of MTS, allowing us to better capture long-range and short-range dependencies within the data. Finally, we uti- lize the Kolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the information interaction in the matrix and MTS node clas- sification task. By comprehensively considering the long-range dependen- cies and dynamic similarity features, we achieved precise MTS node classi- ∗Corresponding author Email addresses: mingsendu@mail.sdu.edu.cn (Mingsen Du), chchenmeng@gmail.com (Meng Chen), lyj4072021@163.com (Yongjian Li), xiuxinzhang@mail.sdu.edu.cn (Xiuxin Zhang), gaojiahui@mail.sdu.edu.cn (Jiahui Gao), jicun@sdnu.edu.cn (Cun Ji), sswei@sdu.edu.cn (Shoushui Wei) Preprint submitted to Engineering Applications of Artificial Intelligence November 20, 2024 arXiv:2411.12222v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Contrast_Similarity-Aware_Dual-Pathway_Mamba_for_Multivariate_Time_Series_Node_Classification.pdf",
        "num_chunks": 1684
    },
    {
        "title": "ControlNet++ Improving Conditional Controls with Efficient Consistency Feedback",
        "context": "els, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face signif- icant challenges in generating images that align with the image condi- tional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained dis- criminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward im- plementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various con- ditional controls. For example, it achieves improvements over ControlNet by 11.1% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmen- tation mask, line-art edge, and depth conditions. All the code, models, demo and organized data have been open sourced on our Github Repo. Keywords: Controllable Generation · Diffusion Model · ControlNet 1 The emergence and improvements of diffusion models [12, 43, 50], along with the introduction of large-scale image-text datasets [48,49], has catalyzed signif- icant strides in text-to-image generation. Nonetheless, as the proverb “an image is worth a thousand words” conveys, it’s challenging to depict an image accu- rately and in detail through language alone, and this dilemma also perplexes existing text-to-image diffusion models [43,46]. To this end, many studies focus on incorporating conditional controls such as segmentation mask into text-to- image diffusion models [22,30,37,62,63]. Despite the diversity in these methods, the core objective remains to facilitate more accurate and controllable image generation with explicit image-based conditional controls. arXiv:2404.07987v4  [cs.CV]  19 Nov 2024",
        "pdf_filename": "ControlNet++_Improving_Conditional_Controls_with_Efficient_Consistency_Feedback.pdf",
        "num_chunks": 1419
    },
    {
        "title": "CRoP Context-wise Robust Static Human-Sensing Personalization",
        "context": "Recent automated human sensing applications—like activity recognition, fall detection, and health tracking - revolutionize daily life, especially in personal health management [85]. However, unique user patterns and natural distribution shifts [24] caused by behaviors, physical traits, environment, and device placements [73, 78] lead to the underperformance of generic sensing models in practical use. To tackle this, various domain adaptation techniques have been explored, with personalization widely used to adapt a generic model to the target user’s specific domain or natural distribution [1, 9, 32, 39, 53, 62, 67]. In literature, personalization occurs either during the enrollment phase (static) [10, 15, 45] or continuously throughout application use, a process known as continual learning [13, 44, 87, 90]. Continual learning methods involve retraining models with new data, either supervised or unsupervised. While these approaches enable models to adapt to new patterns and changes in data distribution over time, they Author’s address: Sawinder Kaur1, Avery Gump2, Jingyu Xin1, Yi Xiao4, Harshit Sharma4, Nina R Benway3, Jonathan L Preston1, Asif Salekin4 1Syracuse University 2University of Wisconsin-Madison 3University of Maryland-College Park 4Arizona State University. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2024 Association for Computing Machinery. XXXX-XXXX/2024/11-ART $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn , Vol. 1, No. 1, Article . Publication date: November 2024. arXiv:2409.17994v4  [cs.AI]  19 Nov 2024",
        "pdf_filename": "CRoP_Context-wise_Robust_Static_Human-Sensing_Personalization.pdf",
        "num_chunks": 3839
    },
    {
        "title": "CSP-Net Common Spatial Pattern Empowered Neural Networks for EEG-Based Motor Imagery Classification",
        "context": "Electroencephalogram-basedmotor imagery (MI) classiﬁcation is an important paradigm of non-invasivebrain-computer interfaces. Common spatial pattern (CSP), which exploits different energy distributions on the scalp while performing different MI tasks, is very popular in MI classiﬁcation. Convolutional neural networks (CNNs) have also achieved great success, due to their powerful learning capabilities. This paper proposes two CSP-empowered neural networks (CSP-Nets), which integrate knowledge-driven CSP ﬁlters with data-driven CNNs to enhance the performance in MI classiﬁcation. CSP-Net-1 directly adds a CSP layer before a CNN to improve the input discriminability. CSP-Net-2 replaces a convolutional layer in CNN with a CSP layer. The CSP layer parameters in both CSP-Nets are initialized with CSP ﬁlters designed from the training data. During training, they can either be kept ﬁxed or optimized using gradient descent. Experiments on four public MI datasets demonstrated that the two CSP-Nets consistently improved over their CNN backbones, in both within-subject and cross-subject classiﬁcations. They are particularly useful when the number of training samples is very small. Our work demonstrates the advantage of integrating knowledge-driven traditional machine learning with data-driven deep learning in EEG-based brain-computer interfaces. Keywords: Brain-computer interfaces, electroencephalogram, motor imagery, common spatial pattern, convolutional neural network A brain-computer interface (BCI) establishes a direct com- munication pathway that enables the human brain to interact with external devices [1]. Electroencephalogram (EEG), which records the electrical activities on the scalp of the brain, is the most widely used input signal in non-invasive BCIs due to its affordability and convenience [2]. EEG-based BCIs have been used in controlling robots [3], decoding speech [4], enhancing computer gaming experience [5], and so on. Motor imagery (MI) [6] is a classical paradigm of EEG- based BCIs, where a subject imagines the movement of a body part, e.g., right hand, left hand, right foot, left foot, both feet, and/or tongue, without actually executing it. An MI induces changes in the sensory-motor rhythms (SMR) of corresponding areas of the cerebral cortex, which primarily involve modula- tions of the µ rhythm (8-12Hz) and the β rhythm (14-30Hz) [7]. Speciﬁcally, when an MI starts, these rhythmic activities de- crease, resulting in event-related desynchronization (ERD); at the end of an MI, these rhythmic activities increase, resulting in event-related synchronization (ERS) [8, 9]. Therefore, the detection of SMR patterns within speciﬁc areas of the cerebral cortex can be used to identify which body part the subject is imagining moving. ∗Emails: xuejiang@hust.edu.cn (Xue Jiang), lubinmeng@hust.edu.cn (Lu- bin Meng), xrchen@hust.edu.cn (Xinru Chen), yfxu@hust.edu.cn (Yifan Xu), drwu@hust.edu.cn (Dongrui Wu). Dongrui Wu is the corresponding author. Many algorithms have been proposed for EEG-based MI classiﬁcation. Common spatial pattern (CSP) [10, 11] is one of the most widely used and effective approaches, which con- verts the raw multi-channel EEG signals into more discrimina- tive spatial patterns. It was initially proposed for binary classi- ﬁcation, by designing spatial ﬁlters that maximize the variance ratio of the ﬁltered signals of different classes [10]. Dornhege et al. [12] extended it to multi-class classiﬁcation using a one- versus-the-rest strategy. Ang et al. [13] proposed ﬁlter bank CSP (FBCSP), which bandpass ﬁlters EEG signals into multi- ple frequency bands, extracts CSP features from each band, and then selects the most useful features for classiﬁcation. Lotte et al. [14] introduced regularized CSP to enhance the robustness of CSP. Recent years have witnessed signiﬁcant increase in using deep learning for EEG signal decoding [15], which integrates feature extraction and classiﬁcation into a single end-to-end network. Among various deep architectures, convolutional neu- ral networks (CNNs) are the most prevalent for MI classiﬁca- tion [16, 17]. For example, Schirrmeister et al. [18] proposed ShallowCNN and DeepCNN for raw EEG classiﬁcation. Shal- lowCNN is inspired by FBCSP and includes components such as temporal convolution, spatial convolution, log-variance cal- culation and a classiﬁer, each corresponding to a speciﬁc step in FBCSP. DeepCNN is similar but includes more convolutional and pooling layers. Lawhern et al. [19] introduced a com- pact EEGNet, which has demonstrated promising performance Preprint submitted to Elsevier November 20, 2024",
        "pdf_filename": "CSP-Net_Common_Spatial_Pattern_Empowered_Neural_Networks_for_EEG-Based_Motor_Imagery_Classification.pdf",
        "num_chunks": 2267
    },
    {
        "title": "Data Science for Social Good",
        "context": "Data science has been described as the fourth paradigm for scientific discovery. The latest wave of data science research, pertaining to machine learning and artificial intelligence (AI), is growing exponentially and garnering millions of annual citations. However, this growth has been accompanied by a diminishing emphasis on social good challenges – our analysis reveals that the proportion of data science research focusing on social good is less than it has ever been. At the same time, the proliferation of machine learning and generative AI have sparked debates about the socio-technical prospects and challenges associated with data science; for human flourishing, organizations, and society. Against this backdrop, we present a framework for “data science for social good” (DSSG) research that considers the interplay between relevant data science research genres, social good challenges, and different levels of socio- paucity of work on DSSG in information systems (and other related disciplines) and highlight current impediments. We then use our proposed framework to introduce the articles appearing in the special issue. We hope that this article and the special issue will spur future DSSG research and help reverse the alarming trend across data science research over the past 30-plus years in which social good challenges are garnering proportionately less attention with each passing day.  1 Data science is an interdisciplinary field that applies mathematics, statistics, machine learning, and data visualization techniques to extract insights and knowledge from data that are normally big and encompass both structured and unstructured formats. In March 2019, something extraordinary and unprecedented happened – an important milestone in the (relatively brief) history of data science. The three “godfathers” of deep learning – Geoff Hinton, Yoshua Bengio, and Yann LeCun – were awarded the prestigious 2018 Turing Award (Simonite, 2019). For those unfamiliar with the award, it is to computer science what the Nobel Prize is to disciplines such as economics and physics, or the Fields Medal to math. So why was it extraordinary? There are a couple of reasons. First, deep learning is essentially a class of machine learning methods (Samtani et al., 2023; Abbasi et al., 2016). If one were to look at prior seminal machine learning methods, none ever won the award1. Decision tree induction models (Quinlan, 1986) and their important extensions, such as random forests (Breiman, 2001), did not win despite being routinely ranked as the most used machine learning method for predictive analytics in research and practice over multiple decades (Abbasi et al., 2016). The same is true for support vector machines (SVM), which popularized the idea of learning problem/domain-specific representations and have been used extensively in prior information systems (IS) research (e.g., Abbasi et al., 2010; Chau et al., 2020). For both these methods, decision trees and SVMs, the seminal papers/authors have garnered over 200,000 citations on Google Scholar. On the other hand, as of  1 Judea Pearl won the Turing Award in 2011 for “contributions to artificial intelligence through the development of a calculus for probabilistic and causal reasoning” including Bayesian Networks, which have been used as a machine learning method for classification/prediction problems. However, his AI contributions are generally regarding as being broader than machine learning, whereas deep learning is widely regarded as a subset of machine learning.",
        "pdf_filename": "Data Science for Social Good.pdf",
        "num_chunks": 794
    },
    {
        "title": "Deep Learning-Driven Heat Map Analysis for Evaluating thickness of Wounded Skin Layers",
        "context": "Understanding the appropriate skin layer thickness in wounded sites is an important tool to move forward on wound healing practices and treatment protocols. Methods to measure depth often are invasive and less specific. This paper introduces a novel method that is non-invasive with deep learning techniques using classifying of skin layers that helps in measurement of wound depth through heatmap analysis. A set of approximately 200 labeled images of skin allows five classes to be distinguished: scars, wounds, and healthy skin, among others. Each image has annotated key layers, namely the stratum cornetum, the epidermis, and the dermis, in the software Roboflow. In the preliminary stage, the Heatmap generator VGG16 was used to enhance the visibility of tissue layers, based upon which their annotated images were used to train ResNet18 with early stopping techniques. It ended up at a very high accuracy rate of 97.67%. To do this, the comparison of the models ResNet18, VGG16, DenseNet121, and EfficientNet has been done where both EfficientNet and ResNet18 have attained accuracy rates of almost 95.35%. For further hyperparameter tuning, EfficientNet and ResNet18 were trained at six different learning rates to determine the best model configuration. It has been noted that the accuracy has huge variations with different learning rates. In the case of EfficientNet, the maximum achievable accuracy was 95.35% at the rate of 0.0001. The same was true for ResNet18, which also attained its peak value of 95.35% at the same rate. These facts indicate that the model can be applied and utilized in actual-time, non-invasive wound assessment, which holds a great promise to improve clinical diagnosis and treatment planning.   It has emerged as a huge phenomenon of deep learning for medical image analysis, especially in wound analysis and tissue monitoring, with non-invasive accuracy and interpretability that makes approaches in clinical applications critical. Traditional methods involved have always been invasive risks while it becomes highly impractical, such as real-time and individualized care in sensitive cases, like post-surgical monitoring, burn care, diabetic wound management, and recovery after cancer treatment [1][2]. Continuous monitoring of wound healing with minimal patient discomfort has become a crying necessity henceforth, and especially in cases that demand detailed skin layer analysis, as seen in reconstructive surgeries related to breast cancer patients where healing dynamics could be extremely complex and therefore must be monitored accurately.",
        "pdf_filename": "Deep_Learning-Driven_Heat_Map_Analysis_for_Evaluating_thickness_of_Wounded_Skin_Layers.pdf",
        "num_chunks": 311
    },
    {
        "title": "Demystifying Ten Big Ideas and Rules Every Fire Scientist & Engineer Should Know About Blackbox, Whitebox & Causal Artificial Intelligence",
        "context": "",
        "pdf_filename": "Demystifying Ten Big Ideas and Rules Every Fire Scientist & Engineer Should Know About Blackbox, Whitebox & Causal Artificial Intelligence.pdf",
        "num_chunks": 472
    },
    {
        "title": "Designing Multi-layered Runtime Guardrails for Foundation Model Based Agents Swiss Cheese Model for",
        "context": "tionizing application development across various domains. How- ever, their rapidly growing capabilities and autonomy have raised significant concerns about AI safety. Researchers are exploring better ways to design guardrails to ensure that the runtime behavior of FM-based agents remains within specific bound- aries. Nevertheless, designing effective runtime guardrails is challenging due to the agents’ autonomous and non-deterministic behavior. The involvement of multiple pipeline stages and agent artifacts, such as goals, plans, tools, at runtime further com- plicates these issues. Addressing these challenges at runtime requires multi-layered guardrails that operate effectively at various levels of the agent architecture. Thus, in this paper, we present a comprehensive taxonomy of runtime guardrails for FM-based agents to identify the key quality attributes for guardrails and design dimensions based on the results of a systematic literature review. Inspired by the Swiss Cheese Model, we also propose a reference architecture for designing multi- layered runtime guardrails for FM-based agents, which includes three dimensions: quality attributes, pipelines, and artifacts. The proposed taxonomy and reference architecture provide concrete and robust guidance for researchers and practitioners to build AI-safety-by-design from a software architecture perspective. Index Terms—Foundation Model, Large Language Models (LLM), Agent, Guardrails, Swiss Cheese Model, Responsible AI, AI Safety, Software Architecture, Taxonomy A Foundation Model (FM) is a large-scale machine learning model pre-trained on massive amounts of data using self- supervision at scale. These models are highly versatile and can adapt to a wide range of downstream tasks [1]. The term ‘foundation’ reflects their role as the fundamental base upon which many specialized models/systems are built. However, it is important to recognize that FM-based systems exhibit inherent limitations, particularly when handling complex tasks. Users are often required to provide detailed instructions, which can lead to inefficiencies and is prone to error. An FM-based agent is an autonomous system that is capable of perceiving context, reasoning, planning, and executing workflows by interacting with FMs, external tools, knowledge bases, and other agents to achieve human goals [3]. There has been extensive interest in FM-based agent development recently due to their huge potential to enhance productivity across various domains. However, their autonomous and non- deterministic behavior introduce substantial concerns regard- ing AI safety [2, 78], such as generating harmful or offensive content, producing dangerous or unintended outcomes, spread- ing disinformation and misinformation, etc [77]. To address these challenges, effective runtime guardrails are key to ensure that agents behave in a safe and responsible man- ner [2]. In this context, guardrails are mechanisms integrated into the agent’s architecture to safeguard its behavior during runtime, preventing undesirable or unsafe behaviors [78]. There have been some initial efforts on runtime guardrails such as input filtering [1, 9], output modification [10, 11], adaptive fail-safes [12, 13], real-time monitoring and detection [14–17], and continuous output validation [18–20]. However, the existing guardrail approaches primarily ad- dress functional correctness, often overlooking quality at- tributes of FM-based agents, such as customizability and interpretability. Most importantly, these approaches mainly focus on individual single-layered guardrails that are narrowly applied to specific agent artifacts, such as prompts or FM outputs, which are insufficient to manage the inherent auton- omy and non-deterministic nature of FM-agents. If any single guardrail fails, the associated risks may bypass it, potentially impacting the final results of the FM-based agent. Therefore, in this paper, we first present a comprehensive taxonomy to categorize runtime guardrails from a software architecture perspective, based on the results of a systematic literature review. The taxonomy comprises two primary cate- gories: quality attributes and design options. Inspired by Swiss Cheese Model [76], we also propose novel reference architec- ture for designing multi-layered guardrails of FM-based agents which include three dimensions: quality attributes, pipelines, and artifacts. Each guardrail layer can be designed to protect specific quality attributes (such as privacy and security), spe- cific pipeline stages (such as prompts, intermediate results and final results), as well as agent artifacts (such as goals, plans, and tools). While each layer may have its own weaknesses (i.e. holes in the Swiss Cheese Model), the combined layers create a a robust defense against failures. This reference architecture provides concrete guidance for researchers and practitioners, enabling AI-safety-by-design from a software architecture perspective. arXiv:2408.02205v3  [cs.SE]  19 Nov 2024",
        "pdf_filename": "Designing_Multi-layered_Runtime_Guardrails_for_Foundation_Model_Based_Agents_Swiss_Cheese_Model_for_.pdf",
        "num_chunks": 2478
    },
    {
        "title": "DeTrigger A Gradient-Centric Approach to Backdoor Attack Mitigation in Federated Learning",
        "context": "Federated Learning (FL) enables collaborative model train- ing across distributed devices while preserving local data privacy, making it ideal for mobile and embedded systems. However, the decentralized nature of FL also opens vulner- abilities to model poisoning attacks, particularly backdoor attacks, where adversaries implant trigger patterns to manip- ulate model predictions. In this paper, we propose DeTrigger, a scalable and efficient backdoor-robust federated learning framework that leverages insights from adversarial attack methodologies. By employing gradient analysis with tem- perature scaling, DeTrigger detects and isolates backdoor triggers, allowing for precise model weight pruning of back- door activations without sacrificing benign model knowl- edge. Extensive evaluations across four widely used datasets demonstrate that DeTrigger achieves up to 251× faster de- tection than traditional methods and mitigates backdoor attacks by up to 98.9%, with minimal impact on global model accuracy. Our findings establish DeTrigger as a robust and scalable solution to protect federated learning environments against sophisticated backdoor threats. 1 Federated Learning (FL) is a decentralized machine learning approach that trains a global model by aggregating locally trained models from mobile and embedded devices [31, 40]. This method leverages distributed data and computational resources, reducing the dependency on centralized process- ing [35, 36, 52]. Federated learning powers mobile applica- tions, such as sensor data analysis [33, 34, 39], autonomous vehicle [29, 38], and real-time computer vision [1, 10, 32, 54], by using large, diverse datasets without data sharing. A key principle is preserving local data privacy, as the server ag- gregates updates without accessing raw data [27, 31, 45]. However, this also means the server cannot verify updates, making federated learning vulnerable to model-poisoning attacks from malicious clients [2, 6, 13]. (b) Cost Trigger Extraction Backdoor Mitigation Model Pruning Method Ours FedAvg [31] Statistical Prior [53] Similarity-based [4] (Krum, MultiKrum) Backdoor Detection (TABOR[18], NeuralCleanse[47]) (a) Local Training Global Aggregation Local Inference Benign Sample Inference Result 50 STOP Malicious Sample Inference Result 50 Malicious Trigger Global Model Benign Clients Label 50 Label 30 Local Model Update Malicious Trigger Local Model Update Label 50 Label 30 Attacker Clients Similarity-based [5] (FLTrust) Figure 1: (a) Illustration of backdoor attack in feder- ated learning scenario for local training, server-side global aggregation, and local inference operations. (b) Comparison of our work with previously proposed ap- proaches in addressing backdoor attacks. With advancements in federated learning, model poison- ing attacks have grown more sophisticated, with the Back- door Attack posing a severe threat [28, 30, 43, 51]. In this attack, as illustrated in Figure 1 (a), an adversary trains a local model to behave benignly on standard inputs but misclassi- fies inputs with a specific trigger. This compromised model 1 arXiv:2411.12220v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "DeTrigger_A_Gradient-Centric_Approach_to_Backdoor_Attack_Mitigation_in_Federated_Learning.pdf",
        "num_chunks": 1664
    },
    {
        "title": "Development of Semantics-Based Distributed Middleware for Heterogeneous Data Integration and its Application for Drought",
        "context": "",
        "pdf_filename": "Development of Semantics-Based Distributed Middleware for Heterogeneous Data Integration and its Application for Drought.pdf",
        "num_chunks": 8315
    },
    {
        "title": "Different Horses for Different Courses Comparing Bias Mitigation Algorithms in ML",
        "context": "With fairness concerns gaining significant attention in Machine Learning (ML), several bias mitigation techniques have been proposed, often compared against each other to find the best method. These benchmarking efforts tend to use a common setup for evaluation under the assumption that providing a uniform environment ensures a fair comparison. However, bias mitigation techniques are sensitive to hyperparameter choices, random seeds, feature selection, etc., meaning that comparison on just one setting can unfairly favour certain algorithms. In this work, we show significant variance in fairness achieved by several algorithms and the influence of the learning pipeline on fairness scores. We highlight that most bias mitigation techniques can achieve comparable performance, given the freedom to perform hyperparameter optimization, suggesting that the choice of the evaluation parameters—rather than the mitigation technique itself—can sometimes create the perceived superiority of one method over another. We hope our work encourages future research on how various choices in the lifecycle of developing an algorithm impact fairness, and trends that guide the selection of appropriate algorithms. 1 Over the past decade, concerns about fairness and discrimination in Machine Learning (ML) systems have emerged as critical issues, driving extensive research into the development of fair ML practices, including mitigation algorithms and fairness criteria [Mehrabi et al., 2021, Gohar and Cheng, 2023, Barocas et al., 2023]. This has led to emerging global AI regulation focused on mitigating discrimina- tion in AI/ML systems, mandating the reporting of fairness metrics of algorithms in compliance with various anti-discrimination laws such as the disparate impact doctrine [Justice., 2023]. However, despite the regulatory efforts, recent research has increasingly shown that the fair ML pipeline suffers from instability and high variance in fairness measures, which can mask the underlying unfairness while creating an illusion of fairness [Black et al., 2023]. For instance, recent work has pointed out how fairness measures vary across different training runs or between training and deployment, challenging the effectiveness, reliability, and utility of existing methods [Baldini et al., 2021, Black and Fredrikson, 2021, Friedler et al., 2019, Ganesh et al., 2023]. Additionally, the multitude of mitigation techniques and fairness metrics further complicate accurate benchmarking. Therefore, from both a regulatory perspective and best practices, such variances must be taken into account to accurately represent the performance of these systems and fairness intervention methods. ∗Equal contribution Workshop on Algorithmic Fairness through the Lens of Metrics and Evaluation (AFME) at NeurIPS 2024. arXiv:2411.11101v2  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Different_Horses_for_Different_Courses_Comparing_Bias_Mitigation_Algorithms_in_ML.pdf",
        "num_chunks": 865
    },
    {
        "title": "Diffusion-Inspired Cold Start with Sufficient Prior in Computerized Adaptive Testing",
        "context": "Computerized Adaptive Testing (CAT) aims to select the most ap- propriate questions based on the examinee’s ability and is widely used in online education. However, existing CAT systems often lack initial understanding of the examinee’s ability, requiring ran- dom probing questions. This can lead to poorly matched ques- tions, extending the test duration and negatively impacting the examinee’s mindset, a phenomenon referred to as the Cold Start with Insufficient Prior (CSIP) task. This issue occurs because CAT systems do not effectively utilize the abundant prior information about the examinee available from other courses on online plat- forms. These response records, due to the commonality of cogni- tive states across different knowledge domains, can provide valu- able prior information for the target domain. However, no prior work has explored solutions for the CSIP task. In response to this gap, we propose Diffusion Cognitive States TransfeR Frame- work (DCSR), a novel domain transfer framework based on Diffu- sion Models (DMs) to address the CSIP task. Specifically, we con- struct a cognitive state transition bridge between domains, guided by the common cognitive states of examinees, encouraging the model to reconstruct the initial ability state in the target domain. To enrich the expressive power of the generated data, we analyze the causal relationships in the generation process from a causal perspective. Redundant and extraneous cognitive states can lead to limited transfer and negative transfer effects. Therefore, we ∗Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ’25, August 03–07, 2025, Toronto, ON, Canada © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX designed three decoupling strategies to control confounding vari- ables, thereby blocking backdoor paths that hinder causal discov- ery. Given that excessive uncertainty can affect the applicability of generated results to the CAT system, we propose consistency constraint and task-oriented constraint to control the randomness of the generated results and their relevance to the CAT task, re- spectively. Our DCSR can seamlessly apply the generated initial ability states in the target domain to existing question selection al- gorithms, thus improving the cold start performance of the CAT sys- tem. Extensive experiments conducted on five real-world datasets demonstrate that DCSR significantly outperforms existing base- line methods in addressing the CSIP task. The code is available at: https://github.com/BIMK/Intelligent-Education/tree/main/DCSR. CCS Concepts • Applied computing →Computer-assisted instruction. Keywords Computerized Adaptive Testing, Intellegent Education ACM Reference Format: Haiping Ma, Aoqing Xia, Changqian Wang, Hai Wang, and Xingyi Zhang. 2025. Diffusion-Inspired Cold Start with Sufficient Prior in Computerized Adaptive Testing. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (KDD ’25). ACM, New York, NY, USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX 1 As artificial intelligence empower education, computerized adap- tive testing (CAT) on online education platforms have garnered extensive attention [38, 39, 42, 45]. CAT aims to provide examinees with a small number of appropriate questions to progressively as- sess their cognitive states in specific domains [19, 29, 30]. Typically, CAT consists of two iterative components: the cognitive diagnostic model (CDM) and the question selection algorithm. As shown in Figure 1 (a), the CDM estimates examinee’s ability based on her arXiv:2411.12182v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Diffusion-Inspired_Cold_Start_with_Sufficient_Prior_in_Computerized_Adaptive_Testing.pdf",
        "num_chunks": 1614
    },
    {
        "title": "DiM $f$-Divergence Minimization Guided Sharpness-Aware Optimization for Semi-supervised Medical Imag",
        "context": "As a technique to alleviate the pressure of data an- notation, semi-supervised learning (SSL) has attracted widespread attention. In the specific domain of medical im- age segmentation, semi-supervised methods (SSMIS) have become a research hotspot due to their ability to reduce the need for large amounts of precisely annotated data. SSMIS focuses on enhancing the model’s generalization perfor- mance by leveraging a small number of labeled samples and a large number of unlabeled samples. The latest sharpness- aware optimization (SAM) technique, which optimizes the model by reducing the sharpness of the loss function, has shown significant success in SSMIS. However, SAM and its variants may not fully account for the distribution differ- ences between different datasets. To address this issue, we propose a sharpness-aware optimization method based on f-divergence minimization (DiM) for semi-supervised med- ical image segmentation. This method enhances the model’s stability by fine-tuning the sensitivity of model parameters and improves the model’s adaptability to different datasets divergence, the DiM method not only improves the perfor- mance balance between the source and target datasets but also prevents performance degradation due to overfitting on the source dataset.",
        "pdf_filename": "DiM_$f$-Divergence_Minimization_Guided_Sharpness-Aware_Optimization_for_Semi-supervised_Medical_Imag.pdf",
        "num_chunks": 1616
    },
    {
        "title": "Distill the Best, Ignore the Rest Improving Dataset Distillation with Loss-Value-Based Pruning",
        "context": "Dataset distillation has gained significant interest in re- cent years, yet existing approaches typically distill from the entire dataset, potentially including non-beneficial samples. We introduce a novel “Prune First, Distill After” framework that systematically prunes datasets via loss-based sampling prior to distillation. By leveraging pruning before classical distillation techniques and generative priors, we create a representative core-set that leads to enhanced generaliza- tion for unseen architectures - a significant challenge of current distillation methods. More specifically, our proposed framework significantly boosts distilled quality, achieving up to a 5.2 percentage points accuracy increase even with substantial dataset pruning, i.e., removing 80% of the origi- nal dataset prior to distillation. Overall, our experimental results highlight the advantages of our easy-sample prioriti- zation and cross-architecture robustness, paving the way for more effective and high-quality dataset distillation. Large-scale datasets are crucial for training high-quality machine learning models across various applications [10,20]. However, the sheer volume of data brings significant com- putational and storage challenges, making efficient dataset distillation methods highly desirable [2,5]. Dataset distilla- tion aims to compress these large datasets into smaller, syn- thetic subsets while preserving training quality, yet existing techniques often fall short in achieving cross-architecture ro- bustness [17]. Classifiers generally perform best when their architecture matches the one used during distillation, but performance degrades significantly when trained on other architectures [3,17]. This challenge is compounded by the retention of noisy samples, which dilutes the core repre- sentational value of distilled data. Addressing these issues requires not only compact and representative data but also a selective sampling approach to enhance performance consis- Pruning Dataset Distillation Dataset Distillation Ours Classical Distillation Pipeline relevant samples irrelevant samples Pre-Selects relevant Samples Pre-Selects relevant Samples Full Dataset Core-Set Distilled Dataset Figure 1. Comparison of classical and our proposed “Prune First, Distill After” pipeline: Traditional dataset distillation operates on the full dataset, which includes both relevant and irrelevant samples. Our Prune-Distill approach pre-selects a core-set by pruning loss- value-based irrelevant samples, focusing distillation on the most informative subset, resulting in a more refined distilled dataset. tency across a range of unseen architectures. Inspired by previous work on dataset pruning for various computer vision tasks [1,4,7,15,16], we explore the interplay between dataset pruning and dataset distillation, proposing a novel approach that systematically prunes samples prior to distillation. As shown in Figure 1, our method enables the creation of compact yet highly representative core-sets, a subset of the original dataset, through targeted pruning that enhance performance and stability across diverse and unseen architectures. To realize this, we introduce a loss- value-based sampling strategy that leverages a pre-trained classifier model to rank data samples by their “classifica- tion difficulty”, helping to capture the key characteristics of each class. This analysis combines two sampling strategies: ascending (starting with simpler samples) and descending (starting with complex samples), allowing us to examine their respective effects on distillation quality. As a result, we 1 arXiv:2411.12115v1  [cs.CV]  18 Nov 2024",
        "pdf_filename": "Distill_the_Best,_Ignore_the_Rest_Improving_Dataset_Distillation_with_Loss-Value-Based_Pruning.pdf",
        "num_chunks": 1239
    },
    {
        "title": "Dive into Deep Learning",
        "context": "",
        "pdf_filename": "Dive into Deep Learning.pdf",
        "num_chunks": 48284
    },
    {
        "title": "DLBacktrace A Model Agnostic Explainability for any Deep Learning Models",
        "context": "The rapid advancement of artificial intelligence has led to increasingly sophisticated deep learning models, which frequently operate as opaque “black boxes” with limited transparency in their decision- making processes. This lack of interpretability presents considerable challenges, especially in high-stakes applications where understanding the rationale behind a model’s outputs is as essential as the outputs themselves. This study addresses the pressing need for interpretability in AI systems, emphasizing its role in fostering trust, ensuring accountability, and promoting responsible deployment in mission-critical fields. To address the interpretability challenge in deep learning, we introduce DLBacktrace, an innovative technique developed by the AryaXAI team to illuminate model decisions across a wide array of domains, including simple Multi Layer Perceptron (MLPs), Convolutional Neural Networks (CNNs), Large Language Models (LLMs), Computer Vision Models, and more. We provide a comprehensive overview of the DLBacktrace algorithm and present benchmarking results, comparing its performance against established interpretability methods, such as SHAP, LIME, GradCAM, Integrated Gradients, SmoothGrad, and Attention Rollout, using diverse task-based metrics. The proposed DLBacktrace technique is compatible with various model architectures built in PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP architectures such as BERT and LSTMs, computer vision models like ResNet and U-Net, as well as custom deep neural network (DNN) models for tabular data. This flexibility underscores DLBacktrace’s adaptability and effectiveness in enhancing model transparency across a broad spectrum of applications. The library is open-sourced and available at https://github.com/AryaXAI/DLBacktrace. 1 Despite remarkable advancements in artificial intelligence, particularly with the evolution of deep learning architectures, even the most sophisticated models face a persistent challenge: they often operate as \"black boxes,\" with internal processes that remain opaque and difficult to interpret. These models produce highly accurate predictions, yet provide limited insights into how and why they make specific decisions. This opacity raises significant concerns, especially in high-stakes applications like healthcare, finance, and law enforcement, where understanding the rationale behind model outputs is critical. For example, in the healthcare sector, AI-driven diagnostics must be interpretable to ensure that medical professionals can trust and act on recommendations for patient treatment. Similarly, in finance, regulations such as the European Union’s GDPR mandate a \"right to explanation\" for automated decisions affecting individuals, making explainability not only an ethical imperative but also a regulatory requirement. The growing demand for explainability and transparency in AI systems is often eclipsed by the prevailing focus on maximizing raw performance. This trend is especially evident with the increasing use of models like OpenAI’s arXiv:2411.12643v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "DLBacktrace_A_Model_Agnostic_Explainability_for_any_Deep_Learning_Models.pdf",
        "num_chunks": 1100
    },
    {
        "title": "Domain Consistency Representation Learning for Lifelong Person Re-Identification",
        "context": "contradictory relationship between intra-domain discrimination and inter-domain gaps when learning from continuous data. Intra-domain discrimination focuses on individual nuances (i.e., clothing type, accessories, etc.), while inter-domain gaps empha- size domain consistency. Achieving a trade-off between maxi- mizing intra-domain discrimination and minimizing inter-domain gaps is a crucial challenge for improving LReID performance. Most existing methods strive to reduce inter-domain gaps through knowledge distillation to maintain domain consistency. However, they often ignore intra-domain discrimination. To address this challenge, we propose a novel domain consistency representation learning (DCR) model that explores global and attribute-wise rep- resentations as a bridge to balance intra-domain discrimination and inter-domain gaps. At the intra-domain level, we explore the complementary relationship between global and attribute- wise representations to improve discrimination among similar identities. Excessive learning intra-domain discrimination can lead to catastrophic forgetting. We further develop an attribute- oriented anti-forgetting (AF) strategy that explores attribute- wise representations to enhance inter-domain consistency, and propose a knowledge consolidation (KC) strategy to facilitate knowledge transfer. Extensive experiments show that our DCR model achieves superior performance compared to state-of-the- art LReID methods. Our code will be available soon. Index Terms—Lifelong learning, person re-identification, do- main consistency representations, attribute and text guided rep- resentations. Person re-identification (ReID) aims to retrieve the same individual across non-overlapping cameras in a large-scale database, and has achieved significant progress using uni- modal architectures such as convolutional neural networks (CNN) [3, 4] or vision transformers (ViT) [5–7]. However, when ReID models are applied to continuous datasets col- lected by dynamic monitoring systems, they exhibit notable This work is supported by the National Natural Science Foundation of China (62273339, 61991413, U20A20200), and the Youth Innovation Promo- tion Association of Chinese Academy of Sciences (2019203). (Corresponding author: Huijie Fan) Shiben Liu is with the State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110016, China, and with the University of Chinese Academy of Sciences, Beijing 100049, China (e-mail: liushiben@sia.cn). Huijie Fan, and Yandong Tang are with the State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China (e-mail: fanhuiie@sia.cn; ytang@sia.cn). Qiang Wang is the Key Laboratory of Manufacturing Industrial Integrated in Shenyang University Shenyang 110044, China (e-mail: wangqiang@sia.cn). Weihong Ren is the Harbin Institute of Technology, Shenzhen 518055, China (e-mail: renweihong@hit.edu.cn). Baojie Fan is with the Department of Automation college at Nanjing University of Posts and Telecommunications, Nanjing 210000, China. (e-mail: jobfbj@gmail.com). Fig. 1. Comparison between our method and existing methods. (a) Existing methods [1, 2] leverage knowledge distillation to minimize inter-domain gaps but ignore intra-domain discrimination, which limits the LReID model’s ability to learn new knowledge. (b) Our method explores attribute-wise representations as a bridge to achieve a trade-off between maximizing intra- domain discrimination and minimizing inter-domain gaps, enhancing the LReID model’s anti-forgetting and generalization capabilities. performance limitations. Thus recent works have focused more on the practical problem of lifelong person re-identification (LReID), which involves learning from streaming data and maintaining strong performance across all data. At present, lifelong person re-identification (LReID) suffers from the challenge of balancing the anti-forgetting of old knowledge and learning new knowledge. Specifically, there are two main issues to solve this challenge. 1) Intra-domain discrimination. Each identity may exhibit subtle nuances of individual information (i.e., clothing type, accessories, haircut, etc.) and lead to severe distribution overlapping. Learning discriminative representations of individuals are effective for distinguish identity information. 2) Inter-domain gaps. The dataset of each task is collected in different illumination and background, leading to inter-domain gaps. Bridging intra- arXiv:2409.19954v2  [cs.CV]  19 Nov 2024",
        "pdf_filename": "Domain_Consistency_Representation_Learning_for_Lifelong_Person_Re-Identification.pdf",
        "num_chunks": 1348
    },
    {
        "title": "Do LLMs Understand Ambiguity in Text A Case Study in Open-world Question Answering",
        "context": "challenges to Large Language Models (LLMs) used for open- domain question answering. LLMs often struggle with the inherent uncertainties of human communication, leading to misinterpretations, miscommunications, hallucinations, and bi- ased responses. This significantly weakens their ability to be used for tasks like fact-checking, question answering, feature extraction, and sentiment analysis. Using open-domain question answering as a test case, we compare off-the-shelf and few-shot LLM performance, focusing on measuring the impact of explicit disambiguation strategies. We demonstrate how simple, training- free, token-level disambiguation methods may be effectively used to improve LLM performance for ambiguous question answering tasks. We empirically show our findings and discuss best practices and broader impacts regarding ambiguity in LLMs. Index Terms—ambiguity, sensitivity, LLM, large language model, question-answering Recent years have seen unprecedented advancements in the development of large language models (LLMs). Today, LLMs are ubiquitous and easily accessible for use by the general public - either via platforms that allow API calls, such as the OpenAI API1, or through openly available model weights for open LLMs, such as via Huggingface2. Since late 2022, large and powerful LLMs have taken over the world of written communication with at least 56% of students using AI in their college work according to a survey [1]. Most of these students, and people overall, harness the conversational capability of this AI for tasks such as problem solving and question-answering. Agentic AI workflows have also started to increase in popularity [2], where these LLMs are used for NLP tasks such as sentiment analysis and data annotation. However, human language is highly context-dependent and complex. Much of the meaning in language, both spoken and written, comes from the context in which it is used, as well as social and psychological cues. This makes it challenging for LLMs to grasp human language, which otherwise would be simple and straightforward for human listeners or readers to understand. LLMs often struggle with the inherent uncertain- ties of human communication, leading to misinterpretations, miscommunications, and biased responses which weaken their 1https://platform.openai.com 2https://huggingface.co/models Fig. 1. The problem of ambiguity in open domain question answering (QA) (left), and how we try to solve it for large language model QA (right). trust and ability to be used for real-world tasks. Ambiguity in natural language poses significant challenges to Large Language Models: much recent work has demonstrated how LLMs struggle to understand ambiguous text in prompts and instructions. This is particularly challenging when lay users prompt LLMs for solving tasks, obtaining answers to trivia questions, etc. - in such cases, the LLM may fail to ‘under- stand’ the context and either fail to respond properly, or even hallucinate a factually wrong response with high confidence [3]. Given the importance of evaluating the sensitivity of LLM to ambiguity in context, in this work we use open- domain question answering as a test case to compare the off-the-shelf LLM performance on ambiguous questions. In our analyses, we further explore simple, training-free methods for disambiguating questions and compare such performance with naive prompting. Through experiments on two state-of- the-art LLMs with a publicly available ambiguous question- answering dataset, we present interesting insights and discuss implications and best practices. II. BACKGROUND AND RELATED WORK Large language models are complex neural network based models [4] that are capable of generating human-like text [5]. Most recent LLMs consist of transformer-based architectures, with a huge number of parameters, on the order of billions. Recent LLMs that are most widely used (such as OpenAI’s GPT family of models [6]–[8], Meta AI’s Llama [9], [10], etc.) are trained on massive amounts of textual data scraped from the internet, then further fine-tuned using instruction-style arXiv:2411.12395v1  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Do_LLMs_Understand_Ambiguity_in_Text_A_Case_Study_in_Open-world_Question_Answering.pdf",
        "num_chunks": 657
    },
    {
        "title": "Efficient Training in Multi-Agent Reinforcement Learning A Communication-Free Framework for the Box-",
        "context": "Self-organizing systems consists of autonomous agents that can perform complex tasks and adapt to dynamic environments without a central controller. Prior research often relies on re- inforcement learning to enable agents to gain the skills needed for task completion, such as in the box-pushing environment. However, when agents push from opposing directions during explo- ration, they tend to exert equal and opposite forces on the box, resulting in minimal displacement and inefficient training. This paper proposes a model called Shared Pool of Information (SPI), which enables information to be accessible to all agents and faciliate coordinate and reduce force conflicts among agents, thus enhancing exploration efficiency. Through computer simulations, we demonstrate that SPI not only expedites the training process but also requires fewer steps per episode, significantly improving the agents’ collaborative effectiveness. 1 Self-organizing systems can address scalability, bottlenecks, and reliability issues that are common in systems with a central controller [1]. However, in practice, agents that learn purely by themselves without the help of a central entity can result in the agent having unbalanced data [2]. Agents can combat the issue through implicit observations or through direct communication ([3], [4], [5]). To add the ability to communicate would require additional overhead to the model. The solution proposed in this paper requires no communication or significant overhead. Cooperation and coordination in reinforcement learning has been a difficult yet crucial problem to solve. There are many studies that aim to find an efficient solution that will allow agents in a multi-agent environment to coordinate. For example, Yexin et al. proposed a contextual cooperative reinforcement learning model to address the lack of cooperations between couriers while also consid- ering system context [6]. Weirong et al. proposed a cooperative reinforcement learning algorithm for distributed economic dispatch in microgrids [7]. Similar to previous studies, this paper aims to address the agent coordination issue in the box-pushing game. In the box pushing game, agents collaborate to push a box towards the goal while avoiding obstacles. The minimalist agents have a small set of possible actions. They also choose their action based on limited observation capacities. The agents can only observe the environment using a box sensor placed in the middle of the box that is being pushed. The sensor gives limited environmental information such as the direction of the goal and nearby obstacles (see Sec. 3.2). The agents are unable to sense one another – all agents are unaware of one another. They can only act through their learned experience and their observation of the current environment. Because of this fact, agents often push against one other, especially towards the beginning of training. This leads to undesirable exploration. ∗Corresponding author 1 arXiv:2411.12246v1  [cs.AI]  19 Nov 2024",
        "pdf_filename": "Efficient_Training_in_Multi-Agent_Reinforcement_Learning_A_Communication-Free_Framework_for_the_Box-.pdf",
        "num_chunks": 751
    },
    {
        "title": "Enhanced Sign Language Translation between American Sign Language (ASL) and Indian Sign Language (IS",
        "context": "provide a bridge between the users of American Sign Language and the users of spoken language and Indian Sign Language (ISL). The research enabled us to create a novel framework that we have developed for Learner Systems. Leveraging art of Large models to create key features including: - Real-time translation between these two sign languages in an efficient manner. Making LLM’s capability available for seamless translations to ISL. Here is the full study showing its implementation in this paper. The core of the system is a sophisticated pipeline that begins with reclassification and recognition of ASL gestures based on a strong Random Forest Classifier. By recognizing the ASL, it is translated into text which can be more easily processed. Highly evolved natural language NLP (Natural Language Processing) techniques come in handy as they play a role in our LLM integration where you then use LLMs to be able to convert the ASL text to ISL which provides you with the intent of sentence or phrase. The final step is to synthesize the translated text back into ISL gestures, creating an end-to-end translation experience using RIFE-Net. This framework is tasked with key challenges such as automatically dealing with gesture variability and overcoming the linguistic differences between ASL and ISL. By automating the translation process, we hope to vastly improve accessibility for sign language users. No longer will the communication gap between ASL and ISL create barriers; this totally cool innovation aims to bring our communities closer together. And we believe, with full confidence in our framework, that we’re able to apply the same principles across a wide variety of sign language dialects. Index Terms—Sign Language Recognition, ASL to ISL Trans- lation, Large Language Models (LLMs), Natural Language Processing (NLP), Random Forest Classifier, Gesture Reclassi- fication, Text-to-Gesture Synthesis, RIFE-Net, Real-Time Trans- lation, Sign Language Variability, Linguistic Adaptation, Assis- tive Technology, Cross-Linguistic Framework, Sign Language Dialects, Accessibility and Inclusivity T HE communication gap between the users of American Sign Language (ASL) and Indian Sign Language (ISL) is a significant challenge to intercultural interaction and ac- cessibility in the deaf community. Sign languages are indis- pensable weapons of expression for deaf individuals, but the lack of interoperability between ASL and ISL limits smooth communication across various linguistic and cultural frontiers. This challenge can be addressed through innovative strategies that leverage advances in machine translation technology and contemporary deep learning frameworks. The central goal of this research work is to build a holistic framework for machine translation that can efficiently and conveniently translate ASL gestures into ISL gestures, hence facilitating communication between ASL users and ISL users. Toward this objective, we hereby propose an approach that combines image recognition techniques with advanced language processing algorithms gov- erned by Large Language Models. Instead of using traditional CNN- based recognition techniques, LLM-driven techniques are used to decode ASL gestures and, in turn, translate directly into meaningful textual representations. From this step of converting ASL gestures to text, we establish an intermediate that helps use advanced LLM-based techniques for machine translation. Now, we can translate the recognized English text into ISL gestures by preserving linguistic aspects and cultural context. The application of LLMs for translation will help make the process more accurate, context-sensitive, and adaptable and, thus, would be used to bridge between ASL and ISL in a nearly seamless manner. A. Leveraging Deep Learning for Gesture Recognition and Translation Sign language communication involves complex expres- sions that carry a certain range of linguistic and cultural nuances in communication. Conventional sign language under- standing and translation techniques rely mostly on fixed data sets and prebuilt models. Such approaches find it difficult to adapt to the dynamic nature of sign languages. In the paper, the constraints were overcome using Random Forest Classifiers for effective gesture recognition followed by Large Language Models to assist in context-aware translation. The proposed framework, therefore, would represent a breakthrough with the integration of real-time processing capabilities and cultural contextualization. The significance of this work is that it introduces an interme- diate text-based representation that forms the connecting link between recognition and synthesis of gestures. This linguistic intermediate allows for not only a more literal translation but also allows the tailoring of the translation method in such a way as to keep its intent and cultural nuances of the original expression. This model would become especially important to the reduction of the linguistic differences between ASL and ISL, such as differing grammatical structures, word orders, and contextual expressions. Further, generating ISL gestures from translated text with the assistance of RIFE-Net produces a smooth and natural gesture presentation. Such a robust ability of RIFE-Net to effectively handle high variability within gesture sequences leads it to accurately reproduce ISL gestures even while making forward-looking predictions of potentially arXiv:2411.12685v1  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Enhanced_Sign_Language_Translation_between_American_Sign_Language_(ASL)_and_Indian_Sign_Language_(IS.pdf",
        "num_chunks": 910
    },
    {
        "title": "Enhancing Low Dose Computed Tomography Images Using Consistency Training Techniques",
        "context": "Diffusion models have significant impact on wide range of generative tasks, especially on image inpainting and restoration. Although the improvements on aiming for decreasing number of function evaluations (NFE), the iterative results are still computationally expensive. Consistency models are as a new family of generative models, enable single-step sampling of high quality data without the need for adversarial training. In this paper, we introduce the beta noise distribution, which provides flexibility in adjusting noise levels. This is combined with a sinusoidal curriculum that enhances the learning of the trajectory between the noise distribution and the posterior distribution of interest, allowing High Noise Improved Consistency Training (HN-iCT) to be trained in a supervised fashion. Additionally, High Noise Improved Consistency Training with Image Condition (HN- iCT-CN) architecture is introduced, enables to take Low Dose images as a condition for extracting significant features by Weighted Attention Gates (WAG).Our results indicate that unconditional image generation using HN-iCT significantly outperforms basic CT and iCT training techniques with NFE=1 on the CIFAR10 and CelebA datasets. Moreover, our image-conditioned model demonstrates exceptional performance in enhancing low-dose (LD) CT scans. Keywords Deep Learning · Consistency · Diffusion 1 X-ray computed tomography (CT) is essential in both diagnosis and treatment, with applications ranging from detecting internal injuries and tumors to surgical planning.To minimize the harmful effects of low-dose ionizing radiation, many studies focus on achieving high-quality denoising while keeping the dose as low as reasonably possible. Recent studies reveals that generative tasks has a remarkable success to increase quality of low dose CT scans. The most commonly used techniques due to their ease of application are Non-local Means (NLM) and Block-Matching 3D (BM3D), both of which can enhance low-dose CT (LDCT) performance. However, despite their utility, these post-processing methods often fall short of meeting clinical requirements [9, 3]. Recent advancements in deep learning showed that generative models are highly capable of meeting clinical requirements for LDCT denoising [19, 5, 1, 21]. Especially, Diffusion Probabilistic Models (DDPM) models outperform other techniques, especially GANs, upon the task of image denoising by iteratively recovering data [2]. The core working arXiv:2411.12181v1  [eess.IV]  19 Nov 2024",
        "pdf_filename": "Enhancing_Low_Dose_Computed_Tomography_Images_Using_Consistency_Training_Techniques.pdf",
        "num_chunks": 738
    },
    {
        "title": "Enhancing Multi-Class Disease Classification Neoplasms, Cardiovascular, Nervous System, and Digestiv",
        "context": "terms of multi-class disease classification via pre-trained language cal conditions. We excluded non-cancer conditions and examined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and BERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained on medical data, demonstrated superior performance in medical text classification (97% accuracy). Sur- prisingly, XLNet followed closely (96% accuracy), demonstrating its generalizability across domains even though it was not pre- trained on medical data. LastBERT, a custom model based on the lighter version of BERT, also proved competitive with 87.10% accuracy (just under BERT’s 89.33%). Our findings confirm the importance of specialized models such as BioBERT and also support impressions around more general solutions like XLNet and well-tuned transformer architectures with fewer parameters (in this case, LastBERT) in medical domain tasks. Index Terms—Medical Conditions, Computational Biology, Neoplasms, Cardiovascular, Nervous System, Digestive system, Natural Language Processing, Deep learning, Transformer mod- els, BioBert, XLNet, LastBERT; The widespread of information and the internet has led to a huge growth in the content volume of electronic documents posted on the internet. Such large and free-form text is easy to use for automatic text classification [1]. The most common approach is called a bag of words, and binary (on a scale of 0 or 1) are features that can then be utilized in supervised classification algorithms such as Support Vector Machines (SVMs), Naive Bayesian Classifiers (Turbo & Tax- man / NHLBI ), etc.... [2]. Given the relative sparsity and simplicity with which some phrases can be dismissed, as well as little training data, research has turned toward focusing on more complex traits. The text classification, especially the medical test classification in the field of text mining, gets more attention, as it serves with a large dataset on medical records and literature [3]. One of the most important NLP areas is text classification, which helps in assigning a set of documents to the correct categories based on their content. The latest developments in the domain of NLP (Natural Language Processing) have completely changed how text categorization is implemented as a part of medical activities. Thanks to word embeddings, transformers, and deep learning architectures (i.e., the backbone of research in NLP), systems can now categorize medical texts more accurately and with higher efficiency than ever before! Word embeddings like Word2Vec and GloVe can elucidate medical lexicon learning by revealing semantic relations among words. More recently, transformer- based architectures (most notably BERT = Bidirectional En- coder Representations from Transformers) demonstrated an extraordinary ability to deal with the intricacies of natural language as well as capturing context [4]. These advanced natural language processing methods used to alleviate the issue of ambiguity and polysemy in medical text seem promising. For instance, transformer-based models do better than we are at solving the select few about-counts by letting their whole decision-making process consider the surrounding context to differentiate between noise and names of different medical problems that sound exactly like or similar to symptoms. These models are also able to find intricate patterns from big datasets, hence the detection of new or uncommon medical conditions. The aim of the research is to see how state-of-the-art natural language processing systems perform in identifying diseases from text data. Furthermore, we will compare multiple ad- vanced NLP models and discuss which of their capabilities are more suitable for medical text classification. We will arXiv:2411.12712v1  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Enhancing_Multi-Class_Disease_Classification_Neoplasms,_Cardiovascular,_Nervous_System,_and_Digestiv.pdf",
        "num_chunks": 573
    },
    {
        "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus",
        "context": "Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning. To address this, we propose Additional Logic Training (ALT), which aims to enhance LLMs’ reasoning capabilities by program- generated logical reasoning samples. We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights. Then, based on these principles, we construct a synthetic corpus named Formal Logic Deduction Diverse (FLD×2), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. Finally, we empirically show that ALT on FLD×2 substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH. 1 Knowledge and reasoning have long been considered essential elements for achieving artificial intelligence (McCarthy, 1959; Weizenbaum, 1966; Winograd, 1971; Colmerauer and Roussel, 1973; Shortliffe, 1976; Elkan and Greiner, 1993). Knowledge refers to facts about the world, e.g., “objects with mass generate a gravitational field” and “the Earth has mass.” Reasoning involves combining multiple facts according to specific rules to obtain new knowledge. For example, the new knowledge that “the Earth generates a gravitational field” can be derived from the aforementioned two facts. Recent observations suggest that LLMs can solve problems using memorized knowledge of similar samples seen during pre-training, but they cannot solve novel, unknown problems that require reasoning (Hodel and West, 2023; Dasgupta et al., 2023; Zhang et al., 2024). For instance, LLMs can solve famous arithmetic problems as is but not when the numbers or names are changed (Razeghi et al., 2022; Mirzadeh et al., 2024), and they can solve coding tests from past years before the “knowledge cutoff” but not from the present year (Mitchell, 2023). This bias towards knowledge has been observed even in state-of-the-art LLMs such as GPT-4 (Liu et al., 2023b; Wu et al., 2023; Dziri et al., 2023). LLMs’ poor reasoning capabilities can stem from the lack of high-quality reasoning samples in the pre-training corpus, which primarily consists of human-written texts (Betz et al., 2021; Morishita et al., 2023). Indeed, reasoning samples in human-written texts often exhibit low quality, as evidenced by fallacies and biases commonly found in online debates (Hansson, 2004; Guia¸su and Tindale, 2018; Cheng et al., 2017). This is unsurprising given that humans usually think reflexively rather than through rigid reasoning (Kahneman, 2011; Sunstein and Hastie, 2015; Paglieri, 2017). Thus, a *Equal Contribution †Work done at Hitachi 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2411.12498v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Enhancing_Reasoning_Capabilities_of_LLMs_via_Principled_Synthetic_Logic_Corpus.pdf",
        "num_chunks": 2379
    },
    {
        "title": "Ergonomic Design of Computer Laboratory Furniture Mismatch Analysis Utilizing Anthropometric Data of",
        "context": "Many studies have shown that ergonomically designed furniture improves productivity and well-being. As computers have become a part of students’ academic lives, they will continue to grow in the future. We propose anthropometric- based furniture dimensions that are suitable for university students to improve computer laboratory ergonomics. We collected data from 380 participants and analyzed 11 anthropometric measurements, correlating them with 11 furni- ture dimensions. Two types of furniture were found and studied in different university computer laboratories: (1) a non-adjustable chair with a non-adjustable table and (2) an adjustable chair with a non-adjustable table. The mismatch calculation showed a significant difference between existing furniture dimensions and anthropometric measurements, indicating that 7 of the 11 existing furniture dimensions need improvement. The one-way ANOVA test with a sig- nificance level of 5% also showed a significant difference between the anthropometric data and existing furniture dimensions. All 11 dimensions were determined to match students’ anthropometric data. The proposed dimensions were found to be more compatible and showed reduced mismatch percentages for nine furniture dimensions and nearly zero mismatches for seat width, backrest height, and under the hood for both males and females compared to the existing furniture dimensions. The proposed dimensions of the furniture set with adjustable seat height showed slightly improved match results for seat height and seat-to-table clearance, which showed zero mismatches compared with the non-adjustable furniture set. The table width and table depth dimensions were suggested according to Barnes and Squires’ ergonomic work envelope model, considering hand reach. The positions of the keyboard and mouse are also suggested according to the work envelope. The monitor position and viewing angle were proposed according to OSHA guidelines. This study suggests that the proposed dimensions can improve comfort levels, reducing the risk of musculoskeletal disorders among students. Further studies on the implementation and long-term effects of the proposed dimensions in real-world computer laboratory settings are recommended. Keywords: Ergonomics, ANOVA, Anthropometric measurements, Mismatch analysis, Computer lab furniture, Furniture design Ergonomics plays a crucial role in ensuring safety, health, and performance in various settings, including educa- tional institutions, where efforts are made to optimize work and study environments for both teachers and students to enhance productivity and minimize the risk of musculoskeletal discomfort [ 1 ]. Integrating ergonomic principles into education can improve the quality and increase productivity. For instance, research has shown that sitting posture and positioning significantly influence typing and handwriting performance [ 2 ]. Designing ergonomic solutions often ∗Corresponding author ∗∗Authors contributed equally Email addresses: anikks18@gmail.com (Anik Kumar Saha), abrar.jahin.2652@gmail.com (Md Abrar Jahin), rafiq123@iem.kuet.ac.bd (Md. Rafiquzzaman), firoz.mridha@aiub.edu (and M. F. Mridha) Preprint submitted to Heliyon November 20, 2024 arXiv:2403.05589v4  [cs.HC]  18 Nov 2024",
        "pdf_filename": "Ergonomic_Design_of_Computer_Laboratory_Furniture_Mismatch_Analysis_Utilizing_Anthropometric_Data_of.pdf",
        "num_chunks": 2257
    },
    {
        "title": "Error-Feedback Model for Output Correction in Bilateral Control-Based Imitation Learning",
        "context": "networks has enabled robots to perform flexible tasks. However, since neural networks operate in a feedforward structure, they do not possess a mechanism to compensate for output errors. To address this limitation, we developed a feedback mechanism to correct these errors. By employing a hierarchical structure for neural networks comprising lower and upper layers, the lower layer was controlled to follow the upper layer. Additionally, using a multi-layer perceptron in the lower layer, which lacks an internal state, enhanced the error feedback. In the character- writing task, this model demonstrated improved accuracy in writing previously untrained characters. In the character-writing task, this model demonstrated improved accuracy in writing previously untrained characters. Through autonomous control with error feedback, we confirmed that the lower layer could effectively track the output of the upper layer. This study represents a promising step toward integrating neural networks with control theories. Index Terms—imitation learning, deep learning, feedback con- trol In recent years, imitation learning has gained significant attention for enabling robots to perform complex actions [1] [2] [3]. Imitation learning is a type of supervised learning in which neural networks (NNs) learn from human demon- strations. Furthermore, research on imitation learning using position and force information has advanced. Specifically, bi- lateral control-based imitation learning has proven effective in reproducing human force application [4] [5] [6] [7]. Bilateral control is a teleoperation technology that uses two robots: one interacts with the environment, while the other is operated by a human applying force. By collecting data with this technology, both position and force response and command values can This work was supported by JSPS KAKENHI Grant Number 24K00905, JST, PRESTO Grant Number JPMJPR24T3 Japan and JST ALCA-Next Japan, Grant Number JPMJAN24F1. This study was based on the results obtained from the JPNP20004 project subsidized by the New Energy and Industrial Technology Development Organization (NEDO). be obtained, allowing robots to replicate human operational sensations. The use of force-based imitation learning shows promise for replacing human tasks with robots. However, in conventional bilateral control-based imitation learning, NN has a feedforward structure and does not control output errors, as shown in Fig. 1. Hence, errors during the autonomous operation of the NN are not compensated. This issue is observed not only in bilateral control-based imitation learning but also in many NN-based imitation learning approaches. Traditionally, NNs have required an internal state to retain memory for handling time-series data. However, this NN struggles to integrate with controllers due to the significant influence of the internal state. This suggests that the system’s non-Markovian nature complicates NN control. Generally, systems are more likely to exhibit Markovian properties when the sampling period is shortened. Therefore, to realize effec- tive NN control, it is essential to establish a structure with independent components for the short-sampling-period NN, which exhibits Markovian properties and is easier to control, and the long-sampling-period NN, which has non-Markovian properties and enables complex time-series inference. In this study, we developed a control system for a hierar- chical model with different sampling periods. The hierarchical model comprises an upper layer that makes long-term predic- tions and a lower layer that makes short-term predictions. The upper layer is a strong non-Markovian system that predicts action plans based on past information. Conversely, the lower- layer is a strong Markovian system that predicts command values and states with a short sampling period. This type of model has been proposed in previous research [8], demonstrat- ing its effectiveness for long-term tasks. However, prior studies employed Long Short-Term Memories (LSTMs) with internal states for the Markovian lower-layer. Therefore, we employed a multilayer perceptron (MLP), which lacks an internal state, to construct a control system for the output. During the control process, the error in the robot’s state predicted by the upper arXiv:2411.12255v1  [cs.RO]  19 Nov 2024",
        "pdf_filename": "Error-Feedback_Model_for_Output_Correction_in_Bilateral_Control-Based_Imitation_Learning.pdf",
        "num_chunks": 639
    },
    {
        "title": "Estimating Dark Matter Halo Masses in Simulated Galaxy Clusters with Graph Neural Networks",
        "context": "Galaxies grow and evolve in dark matter halos. Because dark matter is not visible, galaxies’ halo masses (Mhalo) must be inferred indirectly. We present a graph neural network (GNN) model for predicting Mhalo from stellar mass (M∗) in simulated galaxy clusters using data from the IllustrisTNG simulation suite. Unlike traditional machine learning models like random forests, our GNN captures the information-rich substructure of galaxy clusters by using spatial and kinematic relationships between galaxy neighbour. A GNN model trained on the TNG-Cluster dataset and independently tested on the TNG300 simulation achieves superior predictive performance compared to other baseline models we tested. Future work will extend this approach to different simulations and real observational datasets to further validate the GNN model’s ability to generalise. 1 In the Lambda Cold Dark Matter cosmological model [28, 4], galaxies form and evolve in dark matter halos. Cosmological simulations demonstrate that galaxies grow in tandem with their dark matter halos according to well-measured and tight scaling relations [39]. This interdependence between stellar mass (M∗) and subhalo mass (Mhalo) is known as the stellar–halo mass relation (SHMR). While M∗is observable, Mhalo must often be inferred indirectly via the SHMR due to observational constraints. For example, galaxy clusters—the most massive gravitationally bound objects in the Universe—are dark matter dominated, but their total mass must be measured via gravitational lensing [8, 37], the Sunyaev-Zel’dovich effect [2, 22, 3], and/or visible wavelength proxies (e.g., galaxy richness, intracluster light, etc; [30, 31]). However, these methods are unable to fully leverage galaxy substructure within clusters to estimate their dark matter halo masses. Therefore, we present a graph neural network (GNN) algorithm [32] for predicting Mhalo for galaxies in simulated cluster environments1. Compared to primitive machine learning (ML) methods like random forests [1], a GNN can learn the substructure in neighbouring galaxies and thereby improve halo mass predictions. Our results using the GNN demonstrate significant performance gains on the training, validation, and an independent test set. 1https://github.com/Nikhil0504/halo_masses Machine Learning and the Physical Sciences Workshop, NeurIPS 2024. arXiv:2411.12629v1  [astro-ph.GA]  19 Nov 2024",
        "pdf_filename": "Estimating_Dark_Matter_Halo_Masses_in_Simulated_Galaxy_Clusters_with_Graph_Neural_Networks.pdf",
        "num_chunks": 509
    },
    {
        "title": "Evaluating the Prompt Steerability of Large Language Models",
        "context": "Building pluralistic AI requires designing models that are able to be shaped to represent a wide range of value systems and cultures. Achieving this requires first being able to evaluate the degree to which a given model is capable of reflecting various personas. To this end, we propose a benchmark for evaluating the steerabil- ity of model personas as a function of prompting. Our design is based on a formal definition of prompt steerability, which analyzes the degree to which a model’s joint behavioral distribution can be shifted from its baseline behavior. By defining steer- ability indices and inspecting how these indices change as a function of steering effort, we can estimate the steerability of a model across various persona dimen- sions and directions. Our benchmark reveals that the steerability of many current models is limited – due to both a skew in their baseline behavior and an asymmetry in their steerability across many persona dimensions. We release an implementation of our benchmark at https://github.com/IBM/prompt-steering. 1 A primary question underlying alignment research is: who are we are aligning to? The philosophy of AI/algorithmic pluralism [9, 8, 18, 19] states that we should design AI systems such that they are capable of representing various individuals/groups, rather than aligning to a single “average” human preference – a practice that is unfortunately common in many current model training pipelines. One mechanism for enabling pluralism is by constructing steerable models, i.e., models that can be (easily) made to adopt various behaviors [19]. In this paper, we propose a methodology for evaluating a model’s steerability with respect to prompting. We first propose a formal definition for prompt steerability – quantifying a model’s behavior as a joint distribution, which we term a profile, computed via evaluation/score functions on the distribution of model generations as a result of (a set of) input prompts. Using a dataset of model personas [14], we design a benchmark that measures the extent to which a model can be prompted to adopt various personas. Furthermore, building on our definition of prompt steerability, we define steerability indices that enable comparative measures of how much a model’s behavior can be influenced. While there are a (growing) number of methods for steering models – via prompting [3, 11, 12], fine-tuning [14, 1], activations [16, 21, 20, 10], and other methods [7, 5, 6] – prompting is one of the most straightforward ways in which a typical user can influence a model’s behavior. Often it is not feasible for a user to fine-tune a model (either due to computational requirements or simply due to not having access to the weights) or steer a model via its activations (which requires being able to access/modify a model’s internals during inference). 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2411.12405v1  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Evaluating_the_Prompt_Steerability_of_Large_Language_Models.pdf",
        "num_chunks": 1049
    },
    {
        "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
        "context": "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer’s superior handling of Indic languages, GPT-4o’s advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency. Keywords tokenizer · LLM · tokens · GPT · SUTRA · indic languages. 1 1.1 Background In an ever-evolving landscape of Artificial Intelligence (AI), transformers-based generative Large Language Models (LLMs) are transforming an increasing number of fields with an ever-increasing number of applications in finance, medicine, education, and many more [1, 2]. Tokenization is an important step for LLMs, especially in pre-processing and fine-tuning stages [3]. Most of the LLMs use either of two types of tokenization algorithms, namely WordPiece and Byte Pair Encoding (BPE). For example, OpenaAi’s GPT-4o model and META’s Llama 3, both employ a modified BPE tokenizer [4]. WordPiece was developed for models like BERT employing a greedy approach. It starts with the longest substring that matches a token in its vocabulary, allowing it to handle out-of-vocabulary words effectively by breaking them down into known subword units [5, 6]. BPE works by iteratively merging the most frequently occurring pairs of characters or subwords in a corpus to create a complete vocabulary [7, 8]. Agnostic of the tokenization algorithms used, many techniques have been developed to compare tokenizers of LLMs. Subword fertility is one such technique which measures the average number of tokens used per word [9]. Normalized Sequence Length (NSL) is another such metric used to evaluate the efficiency of tokenizers [12]. ∗Correspondance can be addressed to cs22bcagn033@kazirangauniversity.in arXiv:2411.12240v1  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Evaluating_Tokenizer_Performance_of_Large_Language_Models_Across_Official_Indian_Languages.pdf",
        "num_chunks": 1548
    },
    {
        "title": "Explainable Human-AI Interaction - A Planning Perspective",
        "context": "",
        "pdf_filename": "Explainable Human-AI Interaction - A Planning Perspective.pdf",
        "num_chunks": 6819
    },
    {
        "title": "Exploring Optimal Transport-Based Multi-Grained Alignments for Text-Molecule Retrieval",
        "context": "progress, making the cross-modal text-molecule retrieval task increasingly vital. This task focuses on accurately retrieving molecule structures based on textual descriptions, by effectively aligning textual descriptions and molecules to assist researchers in identifying suitable molecular candidates. However, many existing approaches overlook the details inherent in molecule sub- structures. In this work, we introduce the Optimal TRansport- based Multi-grained Alignments model (ORMA), a novel ap- proach that facilitates multi-grained alignments between textual descriptions and molecules. Our model features a text encoder and a molecule encoder. The text encoder processes textual descriptions to generate both token-level and sentence-level representations, while molecules are modeled as hierarchical heterogeneous graphs, encompassing atom, motif, and molecule nodes to extract representations at these three levels. A key innovation in ORMA is the application of Optimal Transport (OT) to align tokens with motifs, creating multi-token represen- tations that integrate multiple token alignments with their cor- responding motifs. Additionally, we employ contrastive learning to refine cross-modal alignments at three distinct scales: token- atom, multitoken-motif, and sentence-molecule, ensuring that the similarities between correctly matched text-molecule pairs are maximized while those of unmatched pairs are minimized. To our knowledge, this is the first attempt to explore alignments at both the motif and multi-token levels. Experimental results on the ChEBI-20 and PCdes datasets demonstrate that ORMA significantly outperforms existing state-of-the-art (SOTA) models. Specifically, in text-molecule retrieval on ChEBI-20, our model achieves a Hits@1 score of 66.5%, surpassing the SOTA model AMAN by 17.1%. Similarly, in molecule-text retrieval, ORMA secures a Hits@1 score of 61.6%, outperforming AMAN by 15.0%. Index Terms—Text-molecule Retrieval, Multi-grained Repre- sentation Learning, Cross-modal Alignment, Optimal Transport The rapid advancement of bioinformatics has led to the construction of numerous large-scale molecular databases, such as PubChem [1]. These databases play a crucial role in the discovery and synthesis of new drugs. However, accurately retrieving desired molecules from these databases presents a significant challenge. Therefore, aiming to retrieve molecules based on text queries, cross-modal text-molecule retrieval [2] has become increasingly important. * Equal contribution. † Corresponding author. Water is an oxygen hydride consisting of an oxygen atom that is covalently bonded to two hydrogen atoms. Text Query Hydrogen peroxide is a colorless liquid at room temperature with a bitter taste. Water is an oxygen hydride consisting of an oxygen atom that is covalently bonded to two hydrogen atoms. Text Candidates Retrieve Retrieve Molecule Candidates H!O C!H\"O H!O! H!O Molecule Query Text-Molecule Retrieval Molecule-Text Retrieval Fig. 1. The text-molecule retrieval task is designed to retrieve molecules based on text queries, while molecule-text retrieval task does the opposite. The red box indicates the ground-truth retrieval result. Generally, existing studies mainly focus on the utilization of neural networks to learn representations of textual descriptions and molecules, and then calculating text-molecule similari- ties for retrieval. For example, several studies [3]–[5] resort to pretrained models based on Simplified Molecular Input Line Entry Specification (SMILES) [6] and text sequences. Alternatively, more studies like MoMu [7] and MoleculeSTM [8] represent molecules as 2D topological graphs, and then employ cross-modal contrastive learning to align molecular graphs and textual descriptions within a shared semantic space. Furthermore, AMAN [9] utilizes adversarial learning to effectively bridge these two modalities, achieving state-of- the-art (SOTA) performance. Despite these advancements, most studies overlook the de- tailed structural information essential for understanding molec- ular properties. Molecules are composed of atoms connected by chemical bonds, and their properties can be influenced by their structural motifs. In molecular chemistry, a motif is defined as a specific group of bonded atoms that follows a con- sistent and repeating pattern. Ignoring such detailed structural information seriously limits the precision of retrieval results. The only exception is the recently proposed Atomas [10], which applies clustering algorithms to extract representations at multiple granularities. To address the above issue, we propose a novel text- molecule model with Optimal TRansport-based Multi- grained Alignments (ORMA). Overall, our model contains a SciBERT-based text encoder and a GCN-based molecule arXiv:2411.11875v1  [cs.IR]  4 Nov 2024",
        "pdf_filename": "Exploring_Optimal_Transport-Based_Multi-Grained_Alignments_for_Text-Molecule_Retrieval.pdf",
        "num_chunks": 1281
    },
    {
        "title": "F$^3$OCUS -- Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Upd",
        "context": "Effective training of large Vision-Language Models (VLMs) on resource-constrained client devices in Federated Learn- ing (FL) requires the usage of parameter-efficient fine- tuning (PEFT) strategies. To this end, we demonstrate the impact of two factors viz., client-specific layer importance score that selects the most important VLM layers for fine- tuning and inter-client layer diversity score that encourages diverse layer selection across clients for optimal VLM layer selection. We first theoretically motivate and leverage the principal eigenvalue magnitude of layerwise Neural Tan- gent Kernels and show its effectiveness as client-specific layer importance score. Next, we propose a novel layer updating strategy dubbed F3OCUS that jointly optimizes the layer importance and diversity factors by employing a data-free, multi-objective, meta-heuristic optimization on the server. We explore 5 different meta-heuristic algorithms and compare their effectiveness for selecting model layers and adapter layers towards PEFT-FL. Furthermore, we re- lease a new MedVQA-FL dataset involving overall 707,962 VQA triplets and 9 modality-specific clients and utilize it to train and evaluate our method. Overall, we conduct more than 10,000 client-level experiments on 6 Vision-Language FL task settings involving 58 medical image datasets and 4 different VLM architectures of varying sizes to demonstrate the effectiveness of the proposed method. Large Vision-Language Models (VLMs) have made sig- nificant advancements in multi-modal learning, excelling in tasks like Visual Question Answering (VQA) [9, 35, 45, 46, 51]. Their effectiveness stems from their exten- sive parameters often reaching millions or billions, allow- ing them to learn complex representations of image and text data. Fine-tuning these models with task-specific data is crucial for adapting them to specialized applications. However, gathering diverse training data centrally is chal- lenging, especially in fields like healthcare, where strict privacy regulations prevent data aggregation across dif- ferent centers. To address the privacy concerns, Feder- ated Learning (FL) [2, 34, 47, 55] allows models to be trained directly on local devices, such as in healthcare clin- ics, without sharing sensitive data. Yet, fine-tuning large models locally is difficult due to limited computational power and smaller datasets, which hinders VLM adaptation. arXiv:2411.11912v1  [cs.CV]  17 Nov 2024",
        "pdf_filename": "F$^3$OCUS_--_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_Client_Layer_Upd.pdf",
        "num_chunks": 1925
    },
    {
        "title": "Facial Wrinkle Segmentation for Cosmetic Dermatology Pretraining with Texture Map-Based Weak Supervi",
        "context": "matology. Precise manual segmentation of facial wrinkles is challeng- ing and time-consuming, with inherent subjectivity leading to inconsis- tent results among graders. To address this issue, we propose two solu- tions. First, we build and release the first public facial wrinkle dataset, ‘FFHQ-Wrinkle’, an extension of the NVIDIA FFHQ dataset. It in- cludes 1,000 images with human labels and 50,000 images with auto- matically generated weak labels. This dataset could serve as a foun- dation for the research community to develop advanced wrinkle detec- tion algorithms. Second, we introduce a simple training strategy uti- lizing texture maps, applicable to various segmentation models, to de- tect wrinkles across the face. Our two-stage training strategy first pre- train models on a large dataset with weak labels (N=50k), or masked texture maps generated through computer vision techniques, without human intervention. We then finetune the models using human-labeled data (N=1k), which consists of manually labeled wrinkle masks. The net- work takes as input a combination of RGB and masked texture map of the image, comprising four channels, in finetuning. We effectively com- bine labels from multiple annotators to minimize subjectivity in man- ual labeling. Our strategies demonstrate improved segmentation perfor- mance in facial wrinkle segmentation both quantitatively and visually compared to existing pretraining methods. The dataset is available at https://github.com/labhai/ffhq-wrinkle-dataset. Keywords: Facial wrinkle segmentation · Weakly supervised learning · Texture map pretraining · Transfer learning 1 With the growing interest in dermatological diseases and skin aesthetics, pre- dicting facial wrinkles is becoming increasingly significant. Facial wrinkles serve ∗Corresponding authors arXiv:2408.10060v4  [cs.CV]  19 Nov 2024",
        "pdf_filename": "Facial_Wrinkle_Segmentation_for_Cosmetic_Dermatology_Pretraining_with_Texture_Map-Based_Weak_Supervi.pdf",
        "num_chunks": 705
    },
    {
        "title": "Fast Convergence of Softmax Policy Mirror Ascent",
        "context": "Natural policy gradient (NPG) is a common policy optimization algorithm and can be viewed as mirror ascent in the space of prob- abilities. Recently, Vaswani et al. [2021] in- troduced a policy gradient method that cor- responds to mirror ascent in the dual space of logits. We refine this algorithm, removing its need for a normalization across actions and analyze the resulting method (referred to as SPMA). For tabular MDPs, we prove that SPMA with a constant step-size matches the linear convergence of NPG and achieves a faster convergence than constant step-size (ac- celerated) softmax policy gradient. To handle large state-action spaces, we extend SPMA to use a log-linear policy parameterization. Un- like that for NPG, generalizing SPMA to the lin- ear function approximation (FA) setting does not require compatible function approxima- tion. Unlike MDPO, a practical generalization of NPG, SPMA with linear FA only requires solv- ing convex softmax classification problems. We prove that SPMA achieves linear conver- gence to the neighbourhood of the optimal value function. We extend SPMA to handle non-linear FA and evaluate its empirical per- formance on the MuJoCo and Atari bench- marks. Our results demonstrate that SPMA consistently achieves similar or better perfor- mance compared to MDPO, PPO and TRPO. 1 Policy gradient (PG) methods [Williams, 1992; Sut- ton et al., 1999; Konda and Tsitsiklis, 2000; Kakade, 2001] have been critical to the achievements of rein- forcement learning (RL). Although the PG objective is non-concave, recent theoretical research [Agarwal et al., 2021; Mei et al., 2020, 2021; Bhandari and Russo, 2021; Lan, 2023; Shani et al., 2020; Liu et al., 2024; Lu et al., 2024; Alfano and Rebeschini, 2022; Yuan et al., 2023] has analyzed PG methods in simplified settings and demonstrated their global convergence to an optimal policy. While such simplified analyses are helpful in understanding the underlying optimization issues, the resulting methods are rarely used in practice. On the other hand, while methods such as TRPO [Schulman, 2015], PPO [Schulman et al., 2017], MDPO [Tomar et al., 2020] are commonly used in deep RL, their theoreti- cal analysis in the function approximation setting is quite limited. In particular, existing work either (i) analyzes these methods only in the impractical tab- ular setting [Tomar et al., 2020; Shani et al., 2020] or (ii) modifies these algorithms to make them more amenable to theoretical analysis [Liu et al., 1906; Zhong and Zhang, 2024]. Unfortunately, these modified al- gorithms are quite different from the original variants and are not systematically benchmarked on standard environments. Consequently, there exists a large gap between PG methods that have theoretical guarantees in realistic settings versus those which are implemented in practice. To make matters worse, it has been demon- strated that code-level implementation details impact the empirical performance more than the underlying algorithm [Engstrom et al., 2019]. Designing theoretically principled PG algorithms that simultaneously have good empirical performance on the standard set of benchmarks is the main motivation be- hind this work. To that end, we leverage an algorithm first proposed by Vaswani et al. [2021], which we modify to remove the need for normalization. We coin this re- finement Softmax Policy Mirror Ascent (referred to as SPMA). We show that SPMA has comparable convergence guarantees as existing theoretical techniques [Lu et al., 2024; Yuan et al., 2023] in the tabular and function approximation settings, while achieving comparable practical performance as PPO, TRPO and MDPO, without additional algorithmic modifications. In particular, we make the following contributions. Contribution 1: In Section 3, we focus on the multi- armed bandit and tabular MDP settings, where the number of parameters scales with the number of states and actions. We develop the SPMA algorithm, which parameterizes the policy using the softmax function arXiv:2411.12042v1  [cs.LG]  18 Nov 2024",
        "pdf_filename": "Fast_Convergence_of_Softmax_Policy_Mirror_Ascent.pdf",
        "num_chunks": 2517
    },
    {
        "title": "FedDCT A Dynamic Cross-Tier Federated Learning Framework in Wireless Networks",
        "context": "learning paradigm, trains a global model across devices without expos- ing local data. However, resource heterogeneity and inevitable stragglers in wireless networks severely impact the eﬃciency and accuracy of FL training. In this paper, we propose a novel Dynamic Cross-Tier Feder- ated Learning framework (FedDCT). Firstly, we design a dynamic tiering strategy that dynamically partitions devices into diﬀerent tiers based on their response times and assigns speciﬁc timeout thresholds to each tier to reduce single-round training time. Then, we propose a cross-tier de- vice selection algorithm that selects devices that respond quickly and are conducive to model convergence to improve convergence eﬃciency and accuracy. Experimental results demonstrate that the proposed approach under wireless networks outperforms the baseline approach, with an aver- age reduction of 54.7% in convergence time and an average improvement of 1.83% in convergence accuracy. Keywords: Wireless networks · Federated learning · Resource hetero- geneity. 1 Driven by the rapid growth of distributed data mining, Federated Learning (FL) has garnered signiﬁcant attention from both the academic and industrial sectors due to its nature of distributed training and privacy preservation [4]. FL enables the training of a global model across devices without exposing local data. The FL process can be summarized as follows: the server initializes the global model and selects devices to distribute the global model. The chosen devices train using the obtained global model and local data, and the trained models are then uploaded to the server. Finally, the server applies aggregation algorithms such as weighted averaging (e.g., FedAvg [11]) to aggregate the uploaded models into the global model, and subsequently selects new participating devices to distribute the aggregated new model.",
        "pdf_filename": "FedDCT_A_Dynamic_Cross-Tier_Federated_Learning_Framework_in_Wireless_Networks.pdf",
        "num_chunks": 788
    },
    {
        "title": "Fingerprinting and Tracing Shadows The Development and Impact of Browser Fingerprinting on Digital P",
        "context": "identifying and tracking users online without traditional methods like cookies. This paper gives an overview by examining the various ﬁngerprinting techniques and analyzes the entropy and uniqueness of the collected data. The analysis highlights that browser ﬁngerprinting poses a complex challenge from both technical and privacy perspectives, as users often have no control over the collection and use of their data. In addition, it raises signiﬁcant privacy concerns as users are often tracked without their knowledge or consent. Keywords-browser ﬁngerprinting; device ﬁngerprinting; track- ing; privacy. In the increasingly digitized world, the issues of online privacy and data security are becoming more complex. Partic- ularly in tracking — monitoring users and their devices across different web servers — browser ﬁngerprinting has emerged as an effective technique for creating detailed user proﬁles. Unlike the storage of information via cookies, which requires explicit user consent as mandated by the European General Data Protection Regulations (GDPR) guidelines, ﬁngerprinting does not require such consent. A browser ﬁngerprint can be generated in the background without any obvious signs to the end user, leaving them unaware of whether and to what extent they are being tracked. It is possible to manipulate a device locally to alter its ﬁn- gerprint. This is often not feasible for all users, unlike deleting cookies. This invisible threat is not apparent to the general public and raises signiﬁcant privacy concerns, as individuals can be tracked unnoticed. These proﬁles can contain private information, depending on the server operators, including age group, ethnic origin, social circles, and interests of the affected person. Browser ﬁngerprinting poses a threat to the privacy of the general public. Contrary to being a threat, it is an opportunity to provide valuable information to enhance the authentication mechanisms. Both perspectives are explored throughout this paper. The focus will be on the various techniques of ﬁnger- printing to understand how accurate and detailed user proﬁles can be created. The main research questions that this paper seeks to answer are: RQ1 “What methods are used in browser ﬁngerprinting and what user data are collected in the process?” RQ2 “How has the development of browser ﬁngerprinting as a user identiﬁcation method inﬂuenced user privacy and data protection in the digital space?” The paper is structured as follows: Section I introduces browser ﬁngerprinting and its privacy implications. In Section II, the theoretical background explains how ﬁngerprinting works and its legal challenges. Section III outlines techniques like HTTP Headers, Canvas, and WebGL Fingerprinting. Sec- tion IV examines the impact of ﬁngerprinting on privacy and the regulatory landscape. Section V concludes with a summary of the ﬁndings, emphasizing the need for stronger privacy measures and further research on countermeasures. II. THEORETICAL BACKGROUND A. Fingerprinting Browser ﬁngerprinting refers to collecting characteristic information that the browser directly or indirectly reveals about itself. Often used to track users, this technology has also found applications in IT security, such as fraud detection. Unlike tracking methods like cookies, browser ﬁngerprinting does not require storing data on the user’s computer, allowing the process to occur secretly and without consent [1, p. 1]. Consequently, creating a new identity, similar to deleting cookies, is not easily achievable, and GDPR privacy laws often provide little protection. Unlike cookie tracking, browser ﬁngerprinting is not explicitly mentioned in the GDPR. It should fall under the collection of identiﬁable information but website operators frequently claim “legitimate interest”, enabling such data collection without the user’s consent [2]. Active transmission of data is not required for browser ﬁngerprinting, as loading a webpage can transmit various pieces of information, such as the user’s preferred language, within the HTTP headers. This passive data collection pro- vides only a limited amount of information, so it is often supplemented with active data collection methods. An active approach typically employs JavaScript to interface with the browser and gather information, such as screen resolution, installed add-ons, and graphics card data, merging them into a unique ﬁngerprint [3, pp. 1, 3]. Similar to human ﬁngerprints, browser ﬁngerprinting relies on the uniqueness of browser characteristics, which typically do not change signiﬁcantly with regular use. This allows for accurate user identiﬁcation over extended periods [3, p. 2]. However, not all collected data points are equally unique or stable, necessitating careful selection of information to achieve accurate results. The ﬁngerprinting algorithm combines both passively and actively collected data into a unique string. Depending on the operator’s goals, adjustments can be made;",
        "pdf_filename": "Fingerprinting_and_Tracing_Shadows_The_Development_and_Impact_of_Browser_Fingerprinting_on_Digital_P.pdf",
        "num_chunks": 1124
    },
    {
        "title": "From Text to Multimodality Exploring the Evolution and Impact of Large Language Models in Medical Pr",
        "context": "Language Models (LLMs) have rapidly evolved from text-based systems to multimodal platforms, sig- niﬁcantly impacting various sectors including healthcare. This comprehensive review explores the progression of LLMs to Multimodal Large Language Models (MLLMs) and their growing inﬂuence in medical practice. We examine the current landscape of MLLMs in healthcare, analyzing their applications across clinical decision support, medical imaging, patient engagement, and research. The review highlights the unique capabilities of MLLMs in integrating diverse data types, such as text, images, and audio, to provide more comprehensive insights into patient health. We also address the challenges facing MLLM implementation, including data limitations, technical hurdles, and ethical considerations. By identifying key research gaps, this paper aims to guide future investigations in areas such as dataset development, modality alignment methods, and the establishment of ethical guidelines. As MLLMs continue to shape the future of healthcare, understanding their potential and limitations is crucial for their responsible and effective integration into medical practice. Index Terms—Multimodal Large Language Models (MLLMs), Medical Imaging, Clinical Decision Support, Patient Engagement, Data Integration The landscape of healthcare is constantly evolving, driven by an unprecedented explosion of data. Electronic health records, medical imaging, genomic sequencing, and wearable sensors generate an overwhelming amount of information, exceeding human capacity for efﬁcient analysis and inter- pretation [1]. This phenomenon presents both an opportunity and a challenge: ingesting this information can revolutionize healthcare, but doing so requires innovative tools capable of processing and synthesizing these diverse data streams. Artiﬁcial intelligence (AI) has emerged as a powerful force in addressing this challenge, with large language models (LLMs) at the forefront of this revolution. Initially, LLMs focused primarily on text-based tasks, demonstrating remarkable proﬁciency in understanding and generating human-like language [2]. However, the inherent multimodality of medicine, where clinical decisions often rely on the synthesis of information from diverse sources such as images, text, and genomics, necessitates more versatile models [3]. This need has given rise to Multimodal Large Language Models (MLLMs), a new generation of LLMs capable of processing and integrating information from various modalities. These advanced models potentially unlock a new era of precision medicine and personalized healthcare, offering a more comprehensive approach to medical data analysis and decision-making. A key strength of MLLMs is their ability to bridge the gap between unstructured and structured data, a particularly valuable feature in healthcare where information is often fragmented across different formats. For example, the REALM framework leverages LLMs to encode clinical notes and in- tegrates them with time-series EHR data, enhancing clinical predictions by incorporating external knowledge from knowl- edge graphs [4]. In a similar vein, the MedDr model [5] employs a diagnosis-guided bootstrapping strategy to build vision-language datasets, showcasing superior performance across various medical tasks through a retrieval-augmented diagnosis approach. These advancements underscore the po- tential of MLLMs to enhance data interoperability and extract",
        "pdf_filename": "From_Text_to_Multimodality_Exploring_the_Evolution_and_Impact_of_Large_Language_Models_in_Medical_Pr.pdf",
        "num_chunks": 1286
    },
    {
        "title": "Frontiers in Collective Intelligence - A Workshop Report",
        "context": "intelligence as part of its Foundations of Intelligence project. This project seeks to advance the ﬁeld of artiﬁcial intelligence by promoting interdisciplinary research on the nature of intelligence. The workshop brought together computer scientists, biologists, philosophers, social scientists, and others to share their insights about how intelligence can emerge from interactions among multiple agents—whether those agents be ma- chines, animals, or human beings. In this report, we summarize each of the talks and the subsequent discussions. We also draw out a number of key themes and identify important frontiers for future research. 1",
        "pdf_filename": "Frontiers in Collective Intelligence - A Workshop Report.pdf",
        "num_chunks": 935
    },
    {
        "title": "Grading and Anomaly Detection for Automated Retinal Image Analysis using Deep Learning",
        "context": "The significant portion of diabetic patients was affected due to major blindness caused by Diabetic retinopathy (DR). For diabetic retinopathy, lesion segmentation, and detection the comprehensive examination is delved into the deep learning techniques application. The study conducted a systematic literature review using the PRISMA analysis and 62 articles has been investigated in the research. By including CNN-based models for DR grading, and feature fusion several deep-learning methodologies are explored during the study. For enhancing effectiveness in classification accuracy and robustness the data augmentation and ensemble learning strategies are scrutinized. By demonstrating the superior performance compared to individual models the efficacy of ensemble learning methods is investigated. The potential ensemble approaches in DR diagnosis are shown by the integration of multiple pre-trained networks with custom classifiers that yield high specificity. The diverse deep-learning techniques that are employed for detecting DR lesions are discussed within the diabetic retinopathy lesions segmentation and detection section. By emphasizing the requirement for continued research and integration into clinical practice deep learning shows promise for personalized healthcare and early detection of diabetics.  Keywords: Diabetic retinopathy, segmentation, images on retinal fundus, convolutional neural network \"Diabetic Retinopathy (DR)\" is specified through progressive vascular disruptions in the retina. This disruption is developed by the patient with diabetes and is created by chronic hyperglycaemia based on its severity. Among the adults’ work nature, has been considered the major cause of blindness worldwide. Among diabetic patients, the study proved that globally, there are almost 27% of the people have DR [1]. It is expected that these numbers can be increased more due to the elevating pervasiveness of diabetics in Asian countries like China and India [2]. In the primary stages, although DR is totally asymptomatic. During these primary stages, clinically invisible microvascular transformation and neural retinal damage progress strongly[3]. Timely solutions and managing of disease in an efficient way was achieved by proceeding with regular eye screening [4]. Furthermore, to avoid this problem, the only way is to control hypertension, hyperlipidaemia, and hyperglycaemia, and DR's early detection has become necessary [3]. Regarding its treatment recently, there have been several available interventions, including laser photocoagulation. Suppose at the disease's primary stage the eyes are checked. In that case, this intervention remarkably decreases the blindness chances in diabetic maculopathy and proliferative retinopathy to 98% [5]. Finally, it has been proved that from diabetic retinopathy, blindness prevention is possible through appropriate treatment and early detection[6].",
        "pdf_filename": "Grading_and_Anomaly_Detection_for_Automated_Retinal_Image_Analysis_using_Deep_Learning.pdf",
        "num_chunks": 635
    },
    {
        "title": "Green My LLM Studying the key factors affecting the energy consumption of code assistants",
        "context": "In recent years, Large Language Models (LLMs) have significantly improved in generating high-quality code, enabling their integration into developers’ Integrated Development Environments (IDEs) as code assistants. These as- sistants, such as GitHub Copilot, deliver real-time code suggestions and can greatly enhance developers’ productivity. However, the environmental impact of these tools, in particular their energy consumption, remains a key concern. This paper investigates the energy consumption of LLM-based code assistants by simulating developer interactions with GitHub Copilot and analyzing various configuration factors. We collected a dataset of develop- ment traces from 20 developers and conducted extensive software project development simulations to measure energy usage under different scenarios. Our findings reveal that the energy consumption and performance of code assistants are influenced by various factors, such as the number of concurrent developers, model size, quantization methods, and the use of streaming. No- tably, a substantial portion of generation requests made by GitHub Copi- lot is either canceled or rejected by developers, indicating a potential area for reducing wasted computations. Based on these findings, we share action- able insights into optimizing configurations for different use cases, demon- strating that careful adjustments can lead to significant energy savings. Keywords: large language models, code assistants, energy consumption Email address: tristan.coignion@inria.fr (Tristan Coignion) Preprint submitted to Elsevier November 20, 2024 arXiv:2411.11892v1  [cs.SE]  7 Nov 2024",
        "pdf_filename": "Green_My_LLM_Studying_the_key_factors_affecting_the_energy_consumption_of_code_assistants.pdf",
        "num_chunks": 1358
    },
    {
        "title": "Guide-to-Explain for Controllable Summarization",
        "context": "Recently, large language models (LLMs) have demonstrated remarkable performance in ab- stractive summarization tasks. However, con- trollable summarization with LLMs remains underexplored, limiting their ability to generate summaries that align with specific user pref- erences. In this paper, we first investigate the capability of LLMs to control diverse attributes, revealing that they encounter greater challenges with numerical attributes, such as length and ex- tractiveness, compared to linguistic attributes. To address this challenge, we propose a guide- to-explain framework (GTE) for controllable summarization. Our GTE framework enables the model to identify misaligned attributes in the initial draft and guides it in explaining er- rors in the previous output. Based on this re- flection, the model generates a well-adjusted summary. As a result, by allowing the model to reflect on its misalignment, we generate sum- maries that satisfy the desired attributes in sur- prisingly fewer iterations than other iterative methods solely using LLMs. 1 Large language models (LLMs) have demonstrated tion, outperforming traditional encoder-decoder models by generating more contextually appro- priate and natural summaries (Goyal et al., 2023; Zhang et al., 2024; Pu et al., 2023; Ryu et al., 2024b). In addition, recent studies aimed to gen- erate higher-quality summaries by leveraging the self-correction capabilities of LLMs (Zhang et al., 2023a; Sun et al., 2024). However, given individ- uals’ diverse preferences for summary styles, it is essential to generate summaries that adjust personal needs (Zhang et al., 2023b). For instance, some users may prefer concise summaries or retain exact phrases from the original text. Therefore, controllable summarization has re- cently garnered attention (Zhong et al., 2021; Xu et al., 2023; Zhang et al., 2023b). Previous re- search employed encoder-decoder models to con- trol attributes (Mao et al., 2022; Zhang et al., 2022; Vig et al., 2022; Pagnoni et al., 2023; Wang et al., 2023; Urlana et al., 2024). Although LLMs ex- cel in generating high-quality summaries, they still face challenges in controlling attributes (Yuan et al., 2024; Tang et al., 2023), and their controllability has been underexplored (Liu et al., 2024). Thus, we analyze LLMs’ ability to control var- ious attributes in summarization and refine the measurements to more accurately assess these at- tributes. We reveal that while LLMs excel at controlling linguistic attributes such as topic and speaker, they severely struggle with numerical at- tributes such as extractiveness and length. To ad- dress this challenge, we propose a guide-to-explain (GTE), which enables precise attribute control solely through LLMs without relying on external modules or training. We first design an attribute- identification step to calculate misaligned attributes in LLM-generated summaries, subsequently guid- ing the model to explain the sources of its errors. By self-reflecting its own errors, the model can ad- equately adjust attributes in subsequent iterations. We introduce the self-refine strategy, primarily used in reasoning tasks with LLMs (Weng et al., 2023; Madaan et al., 2023; Dhuliawala et al., 2024; Gou et al., 2024), to controllable summarization. Additionally, we evaluate GTE on mixed attribute control datasets, MACSumDoc and MACSumDial (Zhang et al., 2023b). GTE success- fully controls each attribute with minimal iterations solely using LLMs, outperforming other iteration methods. We also demonstrate the high quality of the controlled summaries via multiple evaluation metrics. In addition, we analyzed whether LLMs can control multiple attributes simultaneously. We found out that LLMs struggle with jointly control- ling correlated numerical attributes. Our contribu- tions are as follows: arXiv:2411.12460v1  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Guide-to-Explain_for_Controllable_Summarization.pdf",
        "num_chunks": 1239
    },
    {
        "title": "HEIGHT Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained E",
        "context": "and interactive crowds with environmental constraints such as corridors and furniture. Previous methods fail to consider all types of interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different components in the environment, and propose a heterogeneous spatio-temporal graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous st-graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions among entities through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success and efficiency in challenging navigation scenarios. Furthermore, we demonstrate that our pipeline achieves better zero-shot gen- eralization capability than previous works when the densities of humans and obstacles change. More videos are available at https://sites.google.com/view/crowdnav-height/home. Robots are increasingly prevalent in human-centric environ- ments. In applications such as last-mile delivery and household robots, the ability to navigate among humans is crucial. For example, Fig. 1 shows a navigation scenario with abundant subtle interactions: Obstacles have a one-way effect on the paths of agents (i.e. humans and the robot), while the influence among agents is mutual. Among agents, humans may react to other humans and robots in different ways. To navigate, a robot directly participates in some interactions in its close proximity, and simultaneously, is indirectly affected by other interactions. These interactions are heterogeneous, dynamic, and difficult to infer, making navigation in such environments challenging. Rising to these challenges, previous works have explored various approaches for robot crowd navigation [1]–[3]. How- ever, these works typically have one of two limitations: (1) S. Liu is with the Department of Computer Science at The University of Texas at Austin. Email: shuijing.liu@utexas.edu H. Xia, F. Cheraghi Pouria, K. Hong, N. Chakraborty, and K. Driggs- Campbell are with the Department of Electrical and Computer Engineering at the University of Illinois at Urbana-Champaign. Emails: { hx17, fatemeh5, kaiwen2, neeloyc2, krdc}@illinois.edu This material is based upon work supported by the National Science Foundation under Grant No. 2143435. Fig. 1: A heterogeneous graph aids spatio-temporal reasoning when a robot navigates in a crowded and constrained environment. The colored arrows denote robot-human (RH), human-human (HH), and obstacle-agent (OA) interactions. The opaque arrows are the more important interactions while the transparent arrows are the less important ones. At each timestep t, the robot reasons about these interactions, focuses on the important ones, and makes decisions. They assume agents move in an open space without obstacles, which are common in the real-world [3]–[5]; (2) They do not differentiate between various types of interactions, and thus the robot has difficulties taking adaptive strategies to avoid collisions with humans and obstacles [1], [6]–[8]. Our goal is to navigate a robot to a destination without colliding with humans and obstacles. To solve this problem, we ask the following research question: How can a robot reason about diverse interactions in crowded and constrained arXiv:2411.12150v1  [cs.RO]  19 Nov 2024",
        "pdf_filename": "HEIGHT_Heterogeneous_Interaction_Graph_Transformer_for_Robot_Navigation_in_Crowded_and_Constrained_E.pdf",
        "num_chunks": 1988
    },
    {
        "title": "Heuristic-Free Multi-Teacher Learning",
        "context": "We introduce Teacher2Task, a novel framework for multi-teacher learning that eliminates the need for manual aggregation heuristics. Existing multi-teacher methods typically rely on such heuristics to combine predictions from multiple teachers, often resulting in sub-optimal aggregated labels and the propagation of aggregation errors. Teacher2Task addresses these limitations by introducing teacher-specific input tokens and reformulating the training process. Instead of relying on aggregated labels, the framework transforms the training data – con- sisting of ground truth labels and annotations from N teachers – into N+1 distinct tasks: N auxiliary tasks that predict the labeling styles of the N individual teach- ers, and one primary task that focuses on the ground truth labels. This approach, drawing upon principles from multiple learning paradigms, demonstrates strong empirical results across a range of architectures, modalities, and tasks. 1 Since AlexNet [1], a decade of ML development has yielded a wealth of capable \"teachers\". Humans as teachers, though expensive, provide near-perfect accuracy annotation. Large Language Models (LLMs) offer excellent zero-shot capabilities, generating high-quality \"silver\" data for many tasks. Domain-specific foundational models serve as specialized teachers within their domains. An ideal learning framework would enable ML models to learn effectively from all useful data sources, con- sidering their strengths and weaknesses, unlocking the benefits of both accuracy and scalability. However, effectively leveraging multiple teachers remains an open challenge. Conflicting annota- tions from humans, LLMs, and domain-specific models can be difficult to reconcile, e.g., various teachers give conflict annotation for the same input samples. Also, directly aggregating predictions from LLMs and machine learning (ML) models as final labels can be problematic due to the in- herent noise in individual predictions, which can propagate and amplify in the inaccuracies after aggregation. Existing multiple-teacher learning approaches typically leverage the aggregated output of an ensem- ble of teachers [2] [3]. Most use a simple weighted average of teacher predictions, often with fixed or uniform weights [4][5]. More sophisticated approaches explore manually tuned weights [6] or learn instance-specific teacher importance weights [7] [8]. Alternatively, some methods focus on selecting the \"best\" teacher for each instance, using strategies ranging from random selection [4] to reinforcement learning-based dynamic selection [9]. Specialized approaches, such as assigning teachers to distinct language pairs in multilingual neural machine translation [10], represent specific cases of domain-based teacher selection. However, a common limitation is the reliance on pre- defined heuristics for teacher aggregation or selection, where these heuristics treat the aggregated ∗Correspondence to huythong@google.com arXiv:2411.12724v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Heuristic-Free_Multi-Teacher_Learning.pdf",
        "num_chunks": 469
    },
    {
        "title": "HNCSE Advancing Sentence Embeddings via Hybrid Contrastive Learning with Hard Negatives",
        "context": "Unsupervised sentence representation learning remains a critical challenge in mod- ern natural language processing (NLP) research. Recently, contrastive learning techniques have achieved significant success in addressing this issue by effectively capturing textual semantics. Many such approaches prioritize the optimization using negative samples. In fields such as computer vision, hard negative samples (samples that are close to the decision boundary and thus more difficult to distin- guish) have been shown to enhance representation learning. However, adapting hard negatives to contrastive sentence learning is complex due to the intricate syn- tactic and semantic details of text. To address this problem, we propose HNCSE, a novel contrastive learning framework that extends the leading SimCSE approach. The hallmark of HNCSE is its innovative use of hard negative samples to enhance the learning of both positive and negative samples, thereby achieving a deeper semantic understanding. Empirical tests on semantic textual similarity and transfer task datasets validate the superiority of HNCSE. 1 Sentence representation learning (SRL), or sentence embedding, is a crucial subfield of natural language processing (NLP) that involves encoding sentences into low-dimensional vectors to capture their semantic content. The essence of learning sentence representations is to grasp the full linguistic context, transcending mere word meanings. Sentence representation learning contributes to facilitating a myriad of applications, including information retrieval Liu et al. (2015), conversational AI You et al. (2021), and machine translation systems Zhang et al. (2021). ∗Corresponding author. Preprint. Under review. arXiv:2411.12156v1  [cs.CL]  19 Nov 2024",
        "pdf_filename": "HNCSE_Advancing_Sentence_Embeddings_via_Hybrid_Contrastive_Learning_with_Hard_Negatives.pdf",
        "num_chunks": 1543
    },
    {
        "title": "Homeostatic motion planning with innate physics knowledge",
        "context": "Living organisms interact with their surroundings in a closed-loop fashion, where sen- sory inputs dictate the initiation and termination of behaviours. Even simple animals are able to develop and execute complex plans, which has not yet been replicated in",
        "pdf_filename": "Homeostatic_motion_planning_with_innate_physics_knowledge.pdf",
        "num_chunks": 984
    },
    {
        "title": "IDCIA Immunocytochemistry Dataset for Cellular Image Analysis",
        "context": "We present a new annotated microscopic cellular image dataset to improve the effectiveness of machine learning methods for cellular image analysis. Cell counting is an important step in cell analysis. Typically, domain experts manually count cells in a microscopic image. Automated cell counting can potentially eliminate this te- dious, time-consuming process. However, a good, labeled dataset is required for training an accurate machine learning model. Our dataset includes microscopic images of cells, and for each image, the cell count and the location of individual cells. The data were collected as part of an ongoing study investigating the potential of electrical stimulation to modulate stem cell differentiation and possible applications for neural repair. Compared to existing pub- licly available datasets, our dataset has more images of cells stained with more variety of antibodies (protein components of immune responses against invaders) typically used for cell analysis. The experimental results on this dataset indicate that none of the five existing models under this study are able to achieve sufficiently ac- curate count to replace the manual methods. The dataset is available at https://figshare.com/articles/dataset/Dataset/21970604. CCS CONCEPTS • Computing methodologies →Supervised learning; • Ap- plied computing →Systems biology; Imaging. KEYWORDS Cellular Biology, Machine Learning, Artificial Intelligence, Dataset, Fluorescence Microscopy, Deep learning 1 Cell biology is a sub-discipline of biology where the structure and physiological functioning, and interaction of cells are studied [3]. Cells are examined under a microscope and imaged at a high resolu- tion. In immunocytochemistry (ICC), different antibodies are used ∗Both authors contributed equally. †Abdurahman Ali Mohammed, Wallapak Tavanapong, and Azeez Idris are with the Department of Computer Science at Iowa State University, Ames, IA 50011 USA. ‡Catherine Fonder and Donald S. Sakaguchi are with the Department of Genetics, De- velopment, and Cell Biology (GDCB), Molecular, Cellular, and Developmental Biology Program (MCDB), and the Neuroscience Program, Iowa State University, Ames, IA 50011 USA. §Nanovaccine Institute, Iowa State University, Ames, IA 50011, USA. ¶Surya K. Mallapragada is with the Department of Chemical and Biological Engineer- ing at Iowa State University, Ames, IA 50011 USA. to visualize the presence of particular proteins to identify specific cell types in a given sample. Cell analysis involves a wide range of tasks, such as counting cells and measuring and evaluating cell state (e.g., shape, motility), cell health, and cell growth. Cell biology is closely intertwined with other fields, such as neuroscience, genet- ics, and molecular biology. One fascinating application area of cell biology is research for the potential diagnosis and treatment of dis- eases. The research in this area is full of potential and possibilities that could improve quality of life. Deep Neural Networks (DNNs) have been applied in the analysis of microscopic cell images, including cell counting [26, 35], segmen- tation [1, 10, 11, 23], and detection [9, 12, 34]. Given an input image, cell counting provides the number of cells in the image. In contrast, cell segmentation finds the contours of individual cells, separating them from each other and the background. On the other hand, cell detection localizes a cell by drawing the smallest rectangle around each cell in the input image. The advantages of DNNs over tra- ditional machine learning methods are that DNNs automatically extract important properties (features) of the object of interest and use them to perform the intended task. However, the major draw- back of DNNs is that it requires a large high-quality labeled dataset for accurate predictions. Existing DNN methods for cell counting can be broadly categorized into two groups: detection-based and regression-based categories. The detection-based category undertakes the counting task by first detecting individual cells (contours, bounding boxes, or cen- troids of the cells) in a given image and counting the detected cells to obtain the final cell count [14, 23]. These methods hinge on the availability of the annotated ground truth of the bounding box or a centroid of a cell. The methods are also dependent on the characteristics of the microscopic input images. In particular, detection-based methods fail to offer good performance when there is a high occlusion in the images. The regression-based category [26, 35] predicts the cell count without detecting individual cells. Some of these methods use only the ground truth cell count for each training image for training. Other methods predict a corresponding density map for a given image and obtain the final count from the predicted density map. Our team examines cellular images taken after electrical stimu- lation experiments on stem cells for cell differentiation. Cell differ- entiation is the process in which an unspecialized cell develops and matures to become a specialized cell. Electrical stimulation of stem cells is potentially useful for stem cell therapy in patients with nerve arXiv:2411.08992v2  [eess.IV]  19 Nov 2024",
        "pdf_filename": "IDCIA_Immunocytochemistry_Dataset_for_Cellular_Image_Analysis.pdf",
        "num_chunks": 859
    },
    {
        "title": "Imposing Regulation on Advanced Algorithms",
        "context": "",
        "pdf_filename": "Imposing Regulation on Advanced Algorithms.pdf",
        "num_chunks": 4314
    },
    {
        "title": "Improving Multi-task Learning via Seeking Task-based Flat Regions",
        "context": "Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory. 1 Over the last few years, deep learning has emerged as a powerful tool for functional approximation by exhibiting superior performance and even exceeding human ability on a wide range of applications. In spite of the appealing performance, training massive independent neural networks to handle individual tasks requires not only expensive computational and storage resources but also long runtime. Therefore, multi-task learning is a more preferable approach in many situations [1, 2, 3] as they can: (i) avoid redundant features calculation for each task through their inherently shared architecture; and (ii) reduce the number of total trainable parameters by hard parameter sharing [4, 5] or soft parameter sharing [6, 7]. However, existing state-of-the-art methods following the veins of gradient-based multi-task learning [8, 9, 10, 11, 12, 13] tend to neglect geometrical properties of the loss landscape yet solely focus on minimizing the empirical error in the optimization process, which can be easily prone to the overfitting problem [14, 15]. ∗Equal contributions. 1 arXiv:2211.13723v3  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Improving_Multi-task_Learning_via_Seeking_Task-based_Flat_Regions.pdf",
        "num_chunks": 3589
    },
    {
        "title": "Indicative Summarization of Long Discussions",
        "context": "Online forums encourage the exchange and discussion of different stances on many top- ics. Not only do they provide an opportunity to present one’s own arguments, but may also gather a broad cross-section of others’ argu- ments. However, the resulting long discussions are difficult to overview. This paper presents a novel unsupervised approach using large lan- guage models (LLMs) to generating indicative summaries for long discussions that basically serve as tables of contents. Our approach first clusters argument sentences, generates cluster the generated cluster labels into argumentation frames resulting in a two-level summary. Based on an extensively optimized prompt engineer- ing approach, we evaluate 19 LLMs for genera- tive cluster labeling and frame classification. To evaluate the usefulness of our indicative summaries, we conduct a purpose-driven user study via a new visual interface called DISCUS- SION EXPLORER: It shows that our proposed indicative summaries serve as a convenient nav- igation tool to explore long discussions.1 1 Online discussion forums are a popular medium for discussing a wide range of topics. As the size of a community grows, so does the average length of the discussions held there, especially when current controversial topics are discussed. On Change- MyView (CMV),2 for example, discussions often go into the hundreds of arguments covering many perspectives on the topics in question. Initiating, participating in, or reading discussions generally has two goals: to learn more about others’ views on a topic and/or to share one’s own. To help their users navigate large volumes of arguments in long discussions, many forums offer basic features to sort them, for example, by time of creation or popularity. However, these alternative views may not capture the full range of perspectives *Equal contribution. 1Code: https://github.com/webis-de/EMNLP-23 2https://www.reddit.com/r/changemyview/ exchanged, so it is still necessary to read most of them for a comprehensive overview. In this paper, we depart from previous approaches to summariz- ing long discussions by using indicative summaries instead of informative summaries.3 Figure 1 illus- trates our three-step approach: first, the sentences of the arguments are clustered according to their latent subtopics. Then, a large language model gen- ter as its label. Finally, the argument frame (Chong and Druckman, 2007; Boydstun et al., 2014) of each cluster label is predicted as a generalizable operationalization of perspectives on a discussion’s topic. From this, a hierarchical summary is created in the style of a table of contents, where frames act as headings and cluster labels as subheadings. To our knowledge, indicative summaries of this type have not been explored before (see Section 2). Our four main contributions are: (1) A fully un- supervised approach to indicative summarization of long discussions (Section 3). We develop robust prompts for generative cluster labeling and frame assignment based on extensive empirical evaluation and best practices (Section 4). (2) A comprehen- sive evaluation of 19 state-of-the-art, prompt-based, large language models (LLMs) for both tasks, sup- ported by quantitative and qualitative assessments (Section 5). (3) A user study of the usefulness of in- dicative summaries for exploring long discussions (Section 5). (4) DISCUSSION EXPLORER, an inter- active visual interface for exploring the indicative summaries generated by our approach and the cor- responding discussions.4 Our results show that the GPT variants of OpenAI (GPT3.5, ChatGPT, and GPT4) outperform all other open source models at the time of writing. LLaMA and T0 perform well, but are not competitive with the GPT models. Regarding the usefulness of the summaries, users preferred our summaries to alternative views to ex- plore long discussions with hundreds of arguments. 3Unlike an informative summary, an indicative summary does not capture as much information as possible from a text, but only its gist. This makes them particularly suitable for long documents like books in the form of tables of contents. 4https://discussion-explorer.web.webis.de/ arXiv:2311.01882v1  [cs.CL]  3 Nov 2023",
        "pdf_filename": "Indicative Summarization of Long Discussions.pdf",
        "num_chunks": 3691
    },
    {
        "title": "Instant Policy In-Context Imitation Learning via Graph Diffusion",
        "context": "Following the impressive capabilities of in-context learning with large transform- ers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly (without further training) from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem with a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations – arbitrary trajectories generated in simulation – as a virtually infinite pool of training data. Simulated and real experiments show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks. Code and videos are available at https://www.robot-learning.uk/instant-policy. Figure 1: Instant Policy acquires skills instantly after providing demos at test time. We model in- context imitation learning as a graph-based diffusion process, trained using pseudo-demonstrations. 1 Robot policies acquired through Imitation Learning (IL) have recently shown impressive capabili- ties, but today’s Behavioural Cloning (BC) methods still require hundreds or thousands of demon- strations per task (Zhao et al.). Meanwhile, language and vision communities have shown that when large transformers are trained on sufficiently large and diverse datasets, we see the emergence of In-Context Learning (ICL) (Brown, 2020). Here, trained models can use test-time examples of a novel task (the context), and instantly generalise to new instances of this task without updating the model’s weights. This now offers a promising opportunity of In-Context Imitation Learning (ICIL) in robotics. To this end, we present Instant Policy, which enables new tasks to be learned instantly: after providing just one or two demonstrations, new configurations of that task can be performed immediately, without any further training. This is far more time-efficient and convenient than BC methods, which require numerous demonstrations and hours of network training for each new task. ICL in language and vision benefits from huge and readily available datasets, which do not exist for robotics. As such, we are faced with two primary challenges. 1) Given the limited available data, we need appropriate inductive biases in observation and action representations for efficient learning in 1 arXiv:2411.12633v1  [cs.RO]  19 Nov 2024",
        "pdf_filename": "Instant_Policy_In-Context_Imitation_Learning_via_Graph_Diffusion.pdf",
        "num_chunks": 1436
    },
    {
        "title": "Interpretable Fusion Analytics Framework for fMRI Connectivity Self-Attention Mechanism and Latent S",
        "context": "There have been several attempts to use deep learning based on brain fMRI signals to classify cognitive impairment diseases. However, deep learning is a hidden black box model that makes it difﬁcult to interpret the process of classiﬁcation. To address this issue, we propose a novel analytical framework that interprets the classiﬁcation result from deep learning processes. We ﬁrst derive the region of interest (ROI) functional connectivity network (FCN) by embedding functions based on their similar signal patterns. Then, using the self-attention equipped deep learning model, we classify diseases based on their FCN. Finally, in order to interpret the classiﬁcation results, we employ a latent space item-response interaction network model to identify the signiﬁcant functions that exhibit distinct connectivity patterns when compared to other diseases. The application of this proposed framework to the four types of cognitive impairment shows that our approach is valid for determining the signiﬁcant ROI functions. Keywords fMRI · ADNI · Functional Connectivity Network · Deep Learning · Latent Space Item-Response Model Recently, Deep Neural Networks (DNN) have emerged as popular models of computation in classifying the different cognitive impairment groups in neuroimaging research [1, 2]. However, when it comes to classifying diseases with functional magnetic resonance imaging (fMRI) data (i.e., time–series data in resting state fMRI), improving classiﬁcation accuracy has proved to be challenging given that the brain dynamics captured are high–dimensional, and not readily interpretable due to the complexity of subject–speciﬁc temporal and spatial signals (i.e., spatial maps with associated time courses between nodes). As such, typical deep learning models consist of hidden black box structures, which makes the interpretation of how the model actually produced the output hard to interpret. Previous studies have mainly focused on summarizing brain functions called Regions of Interests (ROIs) signals, comparing two ROI signal patterns by using methods such as Pearson correlation matrices [3] and Fisher transformation score [4]. Despite the fact that this approach offers valuable advantages, such as the ability to simply and intuitively capture the whole–brain interactions as proxy representation of networks, these comparisons of pairwise–(dis)similarities only considers conditional relationships by controlling for high degrees of correlation, such as triangular correlation [5]. By doing so, a lot of valuable information may be lost due to the relative simplicity of this approach. ∗Under review †Equal contribution ‡Corresponding author arXiv:2207.01581v1  [cs.LG]  4 Jul 2022",
        "pdf_filename": "Interpretable_Fusion_Analytics_Framework_for_fMRI_Connectivity_Self-Attention_Mechanism_and_Latent_S.pdf",
        "num_chunks": 1918
    },
    {
        "title": "Introduction to Machine Learning",
        "context": "Laurent Younes September 5, 2024 arXiv:2409.02668v1  [stat.ML]  4 Sep 2024",
        "pdf_filename": "Introduction to Machine Learning.pdf",
        "num_chunks": 34527
    },
    {
        "title": "Introduction to the 28th International Conference on Logic Programming Special Issue",
        "context": "Conference on Logic Programming Special Issue Agostino Dovier Dip. di Matematica e Informatica, Univ. di Udine, Italy (e-mail: agostino.dovier@uniud.it) V´ıtor Santos Costa CRACS INESC-TEC and Dep. de Ciˆencia de Computadores, Univ. do Porto, Portugal (e-mail: vsc@dcc.fc.up.pt) submitted 1 January 2003; revised 1 January 2003; accepted 1 January 2003 We are proud to introduce this special issue of the Journal of Theory and Prac- tice of Logic Programming (TPLP), dedicated to the full papers accepted for the 28th International Conference on Logic Programming (ICLP). The ICLP meetings started in Marseille in 1982 and since then constitute the main venue for presenting and discussing work in the area of logic programming. We contributed to ICLP for the ﬁrst time in 1991. The ﬁrst guest-editor had a paper on logic programming with sets, and the second had two papers on the parallel implementation of the Andorra model. Since then, we continued pursuing research in this exciting area and ICLP has always been the major venue for our work. Thus, when the ALP EC committee kindly invited us for chairing the 2012 edition we were delighted to accept. We particularly appreciate the honor and responsability of organising ICLP in Budapest. Hungary has had a central role both in implementation and in the appli- cation of logic programming. Indeed, the role of Hungary in general in Computer Science is widely recognized, and organizing this meeting in the town of John von Neumann, one of the “talent-scouts” of Turing, in the centenary of the birth of the latter, is just another reason for justifying the fact that the fascinating Budapest is the unique town to host ICLP twice. Publishing the ICLP full papers as a special issue is a joint initiative taken by the Association for Logic Programming and by Cambridge University Press. The goal is to achieve fast journal publication of the highest quality papers from the logic programming community, by selecting the best ICLP submissions before the meeting. This approach beneﬁts the authors, by facilitating journal publication, and beneﬁts the community, by allowing researchers to access high quality journal papers on the more recent and important results in the ﬁeld. Quality is ensured by a two-step refereeing process, and by an active and very much participating program committee. The approach was ﬁrst experimented in 2010, and has had favorable feedback since.",
        "pdf_filename": "Introduction to the 28th International Conference on Logic Programming Special Issue.pdf",
        "num_chunks": 377
    },
    {
        "title": "Introduction to the 35th International Conference on Logic Programming Special Issue",
        "context": "on Logic Programming Special Issue ESRA ERDEM Sabanci University, Turkey (e-mail: esraerdem@sabanciuniv.edu) ANDREA FORMISANO Universit`a di Perugia, Italy (e-mail: andrea.formisano@unipg.it) GERM´AN VIDAL MiST, VRAIN, Universitat Polit`ecnica de Val`encia, Spain (e-mail: gvidal@dsic.upv.es) FANGKAI YANG NVIDIA Corporation, USA (e-mail: fangkaiy@nvidia.com) submitted 1 January 2003; revised 1 January 2003; accepted 1 January 2003 This volume contains the Regular Papers, Technical Communications and the Doctoral Consortium papers of the 35th International Conference on Logic Programming (ICLP 2019), held in Las Cruces, New Mexico, USA, from September 20–25, 2019. Since the ﬁrst conference held in Marseille in 1982, ICLP has been the premier inter- national event for presenting research in logic programming. Contributions are sought in all areas of logic programming, including but not restricted to: Foundations: Semantics, Formalisms, Nonmonotonic reasoning, Knowledge represen- tation. Languages: Concurrency, Objects, Coordination, Mobility, Higher Order, Types, Modes, Assertions, Modules, Meta-programming, Logic-based domain-speciﬁc languages, Pro- gramming Techniques. Declarative programming: Declarative program development, Analysis, Type and tion, Veriﬁcation, Debugging, Proﬁling, Testing, Execution visualization Implementation: Virtual machines, Compilation, Memory management, Parallel or distributed execution, Constraint handling rules, Tabling, Foreign interfaces, User in- terfaces. Related Paradigms and Synergies: Inductive and Co-inductive Logic Programming, Constraint Logic Programming, Answer Set Programming, Interaction with SAT, SMT and CSP solvers, Logic programming techniques for type inference and theorem prov- ing, Argumentation, Probabilistic Logic Programming, Relations to object-oriented and Functional programming. Applications: Databases, Big Data, Data integration and federation, Software engi-",
        "pdf_filename": "Introduction to the 35th International Conference on Logic Programming Special Issue.pdf",
        "num_chunks": 384
    },
    {
        "title": "Is Programming by Example solved by LLMs",
        "context": "Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples. Such systems are practically and theoretically important: from an end-user perspective, they are deployed to millions of people, and from an AI per- spective, PBE corresponds to a very general form of few-shot inductive inference. Given the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have ‘solved’ PBE. We experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data. We find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution. We analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization. Collectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short. 1 Programming-by-Example (PBE) systems solve a challenging task: Given input-output examples of a hidden algorithm, they seek to construct the source code of the underlying function [1, 2]. PBE is deployed to millions of users [3, 4, 5, 6], lies near the heart of core AI challenges [7, 8, 9, 10], and is a qualitatively different problem from the bulk of recent work on LLM code generation, because rather than generate source code from natural language [11], PBE is instead fundamentally about few-shot inductive inference: Given a handful of examples, inferring the program that will generalize to new inputs, or which captures the true latent regularity, without relying on natural-language guidance. We investigate here the extent to which large language models pretrained on source code can solve PBE. If they can, this unlocks the ability to do PBE in general-purpose Turing complete languages like Python, unlike the restricted domain-specific languages which have so far dominated PBE [4, 12, 13, 14, i.a.], thereby increasing the scope and power of this paradigm. If LLMs cannot perform PBE, then this highlights a deficit of inductive reasoning and problem solving, and suggests LLMs lean too heavily on natural language cues to generate code. We find that pretrained and instruction-tuned models serve as poor PBE systems, a finding also supported by recent work [15, 16, 12, 17]. But our investigation further finds that LLMs can be fine-tuned for significantly higher performance, provided they are not asked to generalize far beyond the fine-tuning data. To address this failure of generalization we give an algorithm for taking a small unlabeled dataset of problems and adapting the LLM to it, which we find narrows this domain gap. The resulting recipe allows PBE over Turing-complete languages across three qualitatively different domains (Fig. 1): algorithms on vectors of numbers, string manipulation macros, and graphics programs in LOGO/Turtle. In every case, our final model is at least as effective as custom symbolic search algorithms operating over domain-specific languages, and surpasses powerful closed-source 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2406.08316v3  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Is_Programming_by_Example_solved_by_LLMs.pdf",
        "num_chunks": 2060
    },
    {
        "title": "Just Leaf It Accelerating Diffusion Classifiers with Hierarchical Class Pruning",
        "context": "Diffusion models, known for their generative capabilities, have recently shown unexpected potential in image classifica- tion tasks by using Bayes’ theorem. However, most diffusion classifiers require evaluating all class labels for a single classification, leading to significant computational costs that can hinder their application in large-scale scenarios. To address this, we present a Hierarchical Diffusion Classifier (HDC) that exploits the inherent hierarchical label structure of a dataset. By progressively pruning irrelevant high-level categories and refining predictions only within relevant sub- categories, i.e., leaf nodes, HDC reduces the total number of class evaluations. As a result, HDC can accelerate infer- ence by up to 60% while maintaining and, in some cases, improving classification accuracy. Our work enables a new control mechanism of the trade-off between speed and preci- sion, making diffusion-based classification more viable for real-world applications, particularly in large-scale image classification tasks. Generative models are designed to capture the full spec- trum of a dataset distribution, offering a rich and detailed understanding of the data they represent [14,22]. This com- prehensive grasp of data allows them to not only create new content but also provide deep insights into their character- istics and structures [15,19]. Moreover, these insights can significantly benefit downstream tasks like image classifica- tion [4, 5,9]. Nonetheless, over the past decade, the focus of many generative techniques has predominantly been on content generation rather than harnessing their potential for discriminative tasks [2,11,18,26,27]. Among generative models, diffusion models emerge as a particularly powerful subclass due to their ability to pro- duce exceptionally high-quality output images through an class label class label considered not considered evaluates all classes prunes irrelevant classes Hierarchical Diffusion Classifier (ours) Classical Diffusion Classifier Figure 1. Comparison between the classical diffusion classifier and our proposed Hierarchical Diffusion Classifier (HDC). While the classical approach evaluates all possible classes to find the cor- rect label, which leads to unnecessary computation, HDC prunes irrelevant classes early, focusing only on the most relevant candi- dates. This hierarchical pruning reduces computational overhead and accelerates inference. iterative Markovian process of adding and removing noise [1, 12, 16, 18, 20]. Recently, the research community has shifted towards repurposing pre-trained diffusion models for classification tasks in a zero-shot manner, signaling a piv- otal move toward using generative models as discriminators, so-called diffusion classifiers [3, 6, 17]. More specifically, diffusion models that learned p (x | c) can be easily con- verted into classifiers by exploiting the Bayes’ theorem to derive p (c | x). Thus, given an image x and a set of NC possible classes {ci}NC i=1, we can calculate the likelihood of x belonging to each class ci. In practice, this means adding noise to x and estimating the expected loss of noise reconstruction via Monte Carlo, i.e., through repeated calculations and averaging. This pro- 1 arXiv:2411.12073v1  [cs.CV]  18 Nov 2024",
        "pdf_filename": "Just_Leaf_It_Accelerating_Diffusion_Classifiers_with_Hierarchical_Class_Pruning.pdf",
        "num_chunks": 1045
    },
    {
        "title": "Key-Element-Informed sLLM Tuning for Document Summarization",
        "context": "Remarkable advances in large language models (LLMs) have enabled high-quality text summarization. However, this capa- bility is currently accessible only through LLMs of substantial size or proprietary LLMs with usage fees. In response, smaller- scale LLMs (sLLMs) of easy accessibility and low costs have been extensively studied, yet they often suffer from missing key information and entities, i.e., low relevance, in particular, when input documents are long. We hence propose a key-element- informed instruction tuning for summarization, so-called KEIT- Sum, which identifies key elements in documents and instructs sLLM to generate summaries capturing these key elements. Ex- perimental results on dialogue and news datasets demonstrate that sLLM with KEITSum indeed provides high-quality sum- marization with higher relevance and less hallucinations, com- petitive to proprietary LLM. document summarization, named entity recognition With the advent of Large Language Models (LLMs), recent studies have utilized LLMs across a broad spectrum of ap- plications. Consequently, for summarization tasks, there is a paradigm shift from traditional encoder-decoder-based models [1, 2, 3, 4, 5] to LLMs. It has been revealed that LLMs pro- duce more contextual and natural summaries than the encoder- decoder models [6, 7, 8] where LLMs do not merely put words from the document; instead, they substitute appropriate syn- onyms for a summary, resulting in more natural expressions and flows [6]. Noticeably, LLMs often generate even better sum- maries than human-written references [7, 8]. However, such a high-quality summarization has been only accessible by proprietary LLMs with usage fees or LLMs of large sizes. To improve accessibility, publicly available smaller- scale LLMs (sLLMs) can be considered. Noting that sLLMs can generate more fluent sentences than traditional encoder- decoder models, using sLLMs for summarization is a promising approach. However, according to our evaluation (Figure 2), they still suffer from the problem of omitting key entities or informa- tion but including superfluous sentences in summaries, i.e., low relevance. Hence, we aim to unleash the summarization capabilities of sLLMs by addressing the problem of low relevance. To this end, we propose key-element-informed sLLM tuning for docu- ment summarization (KEITSum), of which an overview is illus- trated in Figure 1. Given an input document, KEITSum identi- *equal contribution †correspondence to: jungseul@postech.ac.kr fies key elements consisting of the named entities and conclu- sion sentence and then instructs a fine-tuned sLLM to include the key elements when generating a summary, where the fine- tuning is conducted to optimize the sLLM for the key-element- informed summarization. We evaluate KEITSum on a dialogue summarization dataset, DialogSum [9], and a news summariza- tion dataset, CNN/DM [10], using a multi-dimensional metric to assess summarization quality, UniEval [11]. It is demon- strated that KEITSum improves the summary quality compared to the baseline LLaMA2-7B, particularly in terms of relevance and when summarizing long dialogs or documents. In addition, we also observed that KEITSum is effective in reducing hallu- cinations.",
        "pdf_filename": "Key-Element-Informed_sLLM_Tuning_for_Document_Summarization.pdf",
        "num_chunks": 815
    },
    {
        "title": "KTO Model Alignment as Prospect Theoretic Optimization",
        "context": "Kahneman & Tversky’s prospect theory tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, hu- mans are famously loss-averse. We show that ob- jectives for aligning LLMs with human feedback implicitly incorporate many of these biases—the success of these objectives (e.g., DPO) over cross- entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call human-aware losses (HALOs). However, the utility functions these methods attribute to hu- mans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration. Aligning generative models with human feedback has been successfully used to make generations more helpful, factual, and ethical, among other desiderata (Ouyang et al., 2022; Tian et al., 2023). For LLMs, alignment methods such as RLHF and DPO have consistently proven to be more benefi- cial than doing supervised finetuning (SFT) alone. However, human feedback is often discussed only in the context of preferences (e.g., output yw ≻yl for input x), even though it can take many forms (e.g., approval/disapproval of y given 1Stanford University (first author was an intern at Contex- tual AI) 2Contextual AI. Correspondence to: Kawin Ethayarajh <kawin@stanford.edu>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Figure 1. The utility that a human gets from the outcome of a random variable, as implied by different human-aware losses (HA- LOs). Notice that the implied value functions share properties such as loss aversion with the canonical human value function in prospect theory (Tversky & Kahneman, 1992). x). This is because preferences, despite being a kind of data that is relatively scarce and expensive to collect in practice (Casper et al., 2023), are required by the alignment methods shown to work best—RLHF (Christiano et al., 2017) and DPO (Rafailov et al., 2023). To understand why these methods work so well, and whether feedback needs to be in preference form, we frame align- ment through the lens of prospect theory (Kahneman & Tversky, 1979; Tversky & Kahneman, 1992). Prospect the- ory explains why humans make decisions about uncertain events that do not maximize their expected value. It formal- izes how humans perceive random variables in a biased but well-defined manner; for example, relative to some refer- ence point, humans are more sensitive to losses than gains, a property called loss aversion. We show that popular align- ment methods such as DPO and PPO-Clip (Schulman et al., 2017) implicitly model some of these biases, helping ex- plain their success independently of the data used (§3.2). We then propose a more general class of such loss functions called human-aware losses (HALOs).1 1We use the term human-aware to draw an analogy with how hardware-aware methods benefit from being designed around hard- ware limitations (Dao et al., 2022), not to claim that a simple loss function is fully aware of human behavior. 1 arXiv:2402.01306v4  [cs.LG]  19 Nov 2024",
        "pdf_filename": "KTO_Model_Alignment_as_Prospect_Theoretic_Optimization.pdf",
        "num_chunks": 1775
    },
    {
        "title": "Large Language Models for Combinatorial Optimization of Design Structure Matrix",
        "context": "Combinatorial optimization (CO) is essential for improving efficiency and performance in engineering applications. As complexity increases with larger problem sizes and more intricate dependencies, identifying the optimal solution become challenging. When it comes to real-world engineering problems, algorithms based on pure mathematical reasoning are limited and incapable to capture the contextual nuances necessary for optimization. This study explores the potential of Large Language Models (LLMs) in solving engineering CO problems by leveraging their reasoning power and contextual knowledge. We propose a novel LLM-based framework that integrates network topology and domain knowledge to optimize the sequencing of Design Structure Matrix (DSM)—a common CO problem. Our experiments on various DSM cases demonstrate that the proposed method achieves faster convergence and higher solution quality than benchmark methods. Moreover, results show that incorporating contextual domain knowledge significantly improves performance despite the choice of LLMs. These findings highlight the potential of LLMs to address complex CO problems by combining semantic and mathematical reasoning. This approach paves the way for a new paradigm in real-world engineering combinatorial optimization. Keywords Large Language Models · Combinatorial Optimization · Artificial Intelligence · Knowledge-based Reasoning · Design Structure Matrix · Systems Engineering 1 Combinatorial optimization (CO) problems are ubiquitous across fields, where finding an optimal solution from a finite set often drives improvements in efficiency, cost, and performance [1]. For instance, applications such as DNA barcoding and DNA assembly in synthetic biology [2], as well as job scheduling in manufacturing [3], rely heavily on effective CO solutions. However, due to their NP-hard nature, these problems present substantial challenges, especially as complexity increases with larger problem sizes and more intricate dependencies. Traditionally, CO problems in engineering are usually approached through the following process: the problem is first modelled mathematically, then solved using specific algorithms or heuristics, and finally interpreted within the context of practical engineering [4]. ∗Comments are welcome: shuojiangcn@gmail.com arXiv:2411.12571v1  [cs.CE]  19 Nov 2024",
        "pdf_filename": "Large_Language_Models_for_Combinatorial_Optimization_of_Design_Structure_Matrix.pdf",
        "num_chunks": 807
    },
    {
        "title": "Leveraging MLLM Embeddings and Attribute Smoothing for Compositional Zero-Shot Learning",
        "context": "Compositional zero-shot learning (CZSL) aims to recog- nize novel compositions of attributes and objects learned from seen compositions. Previous works disentangle at- tribute and object by extracting shared and exclusive parts between image pairs sharing the same attribute (object), as well as aligning them with pretrained word embeddings to improve unseen attribute-object recognition. Despite the significant achievements of existing efforts, they are ham- pered by three limitations: (1) the efficacy of disentangle- ment is compromised due to the influence of the background and the intricate entanglement of attribute with object in the same parts. (2) existing word embeddings fail to cap- ture complex multimodal semantic information. (3) over- confidence exhibited by existing models in seen composi- tions hinders their generalization to novel compositions. Being aware of these, we propose a novel framework named Multimodal Large Language Model (MLLM) embeddings and atTRibute smoothIng guiDEd diseNTanglement (TRI- DENT) for CZSL. First, we leverage feature adaptive ag- gregation modules to mitigate the impact of background, and utilize learnable condition masks to capture multigran- ularity features for disentanglement. Then, the last hid- den states of MLLM are employed as word embeddings for their superior representation capabilities. Moreover, we propose attribute smoothing with auxiliary attributes gen- erated by Large Language Model (LLM) for seen composi- tions, addressing the issue of overconfidence by encourag- ing the model to learn more attributes in one given compo- sition. Extensive experiments demonstrate that TRIDENT achieves state-of-the-art performance on three benchmarks. As for the study of compositional generalization ability in- herent to humans, compositional zero-shot learning (CZSL) *Corresponding author [22, 26, 33] is proposed to enable machines to recognize unseen attribute-object compositions by leveraging knowl- edge of attributes and objects (i.e., primitives) learned from seen compositions. Specifically, in the training phase, mod- els are provided with images and compositional labels (e.g., ripe orange and peeled apple). During the test- ing phase, given an image depicting a novel composition (e.g., peeled orange), models are assigned to classify the image into the corresponding category [44]. Prior works [22, 27] focus on mapping the visual fea- tures and the word embeddings of compositions into a joint space. These methods have poor generalization capabil- ity to unseen compositions, as they fail to learn primitives. Therefore, recent studies [8, 14, 38] consider visual disen- tanglement. Among them, some prominent works deploy a triplet of images to disentangle: a given image (noted as the main image), and two supplementary images, each sharing either the same attribute or the same object as the main im- age. The triplet of images is treated as two image pairs for subsequent analysis. These approaches aim to disentangle attribute and object by analyzing the shared and exclusive features of the image pair, as well as aligning them with word embeddings (e.g., GloVe [32]), as shown in Figure 1. Although these pioneer research studies have achieved great progress, they exhibit three limitations: L1: Disentanglement is impeded due to the influence of the background and the intricate entanglement of attribute with object in the same parts of image. On the one hand, models tend to extract the background feature unique to one image in the pair as the disentangled exclusive features. On the other hand, some existing methods [36, 38] com- pute the similarity of image pairs for disentanglement at the spatial level. However, disentangling attribute and object at the spatial level presents significant challenges because they entangle in the same spatial features. Taking an image of ripe apple as an example, the spatial regions corre- sponding to the attribute ”ripe” and the object ”apple” are fully co-located. L2: Existing word embeddings lack the depth needed to capture complex multimodal semantic information. To 1 arXiv:2411.12584v1  [cs.CV]  18 Nov 2024",
        "pdf_filename": "Leveraging_MLLM_Embeddings_and_Attribute_Smoothing_for_Compositional_Zero-Shot_Learning.pdf",
        "num_chunks": 1876
    },
    {
        "title": "libcll an Extendable Python Toolkit for Complementary-Label Learning",
        "context": "Complementary-label learning (CLL) is a weakly supervised learning paradigm for multiclass classi- fication, where only complementary labels—indicating classes an instance does not belong to—are provided to the learning algorithm. Despite CLL’s increasing popularity, previous studies highlight two main challenges: (1) inconsistent results arising from varied assumptions on complementary label generation, and (2) high barriers to entry due to the lack of a standardized evaluation platform across datasets and algorithms. To address these challenges, we introduce libcll, an extensible Python toolkit for CLL research. libcll provides a universal interface that supports a wide range of generation assumptions, both synthetic and real-world datasets, and key CLL algorithms. The toolkit is designed to mitigate inconsistencies and streamline the research process, with easy installation, comprehensive usage guides, and quickstart tutorials that facilitate efficient adoption and implementa- tion of CLL techniques. Extensive ablation studies conducted with libcll demonstrate its utility in generating valuable insights to advance future CLL research. 1 In many real-world applications, training effective classifiers typically depends on obtaining high-quality, accurate labels. However, acquiring such labels is often difficult and costly. To address this challenge, many researchers have turned their attention to weakly supervised learning (WSL), a methodology aimed at training reliable classifiers using only incomplete, imprecise, or inaccurate data [1, 2]. Numerous WSL studies have been conducted to extend our understanding of machine learning capabilities, covering topics such as complementary labels [3, 4], multiple complementary labels [5, 6], noisy labels [7], and learning from partial labels [8]. This work focuses on complementary-label learning (CLL), a WSL problem where each label indicates only a class to which a data instance does not belong [3]. CLL aims to train models with these complementary labels while still enabling accurate predictions of the ordinary labels during testing. CLL makes machine learning more practical in scenarios where obtaining ordinary labels is difficult or costly [3]. Additionally, CLL broadens our understanding of machine learning’s practical potential under limited supervision. Current research on CLL has introduced numerous learning algorithms [4, 9, 10, 11] that have been evaluated using a diverse range of datasets, from synthetic datasets based on varied complementary-label generation assumptions to real-world datasets [12]. However, the performance of these algorithms often varies significantly across studies due to differences in underlying label-generation assumptions, the absence of a standardized evaluation platform, and the use of diverse network architectures [4, 9, 3, 11]. Establishing a fair, reproducible, and stable evaluation environment is therefore essential for advancing CLL research. For instance, variations in network architectures, such as the use of ResNet18 [13, 12] versus ResNet34 [9, 4], contribute to inconsistencies in performance and hinder fair comparisons across studies. Furthermore, most CLL research has not publicly released implementations [6, 11, 4, 14], particularly regarding details like loss calculation and data pre-processing. This lack of accessibility presents a challenge for researchers seeking to validate and build upon existing work in CLL. To enable meaningful comparisons among CLL algorithms and create a user-friendly environment for implementation and innovation, we introduce libcll, a complementary-label learning toolkit built with PyTorch-Lightning. This toolkit arXiv:2411.12276v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "libcll_an_Extendable_Python_Toolkit_for_Complementary-Label_Learning.pdf",
        "num_chunks": 1467
    },
    {
        "title": "Lifelong and Continual Learning Dialogue Systems",
        "context": "",
        "pdf_filename": "Lifelong and Continual Learning Dialogue Systems.pdf",
        "num_chunks": 2214
    },
    {
        "title": "LLM4DS Evaluating Large Language Models for Data Science Code Generation",
        "context": "code generation in data science offers substantial potential for en- hancing tasks such as data manipulation, statistical analysis, and visualization. However, the effectiveness of these models in the data science domain remains underexplored. This paper presents a controlled experiment that empirically assesses the performance of four leading LLM-based AI assistants—Microsoft Copilot (GPT-4 Turbo), ChatGPT (o1-preview), Claude (3.5 Sonnet), and Perplexity Labs (Llama-3.1-70b-instruct)—on a diverse set of data science coding challenges sourced from the Stratacratch platform. Using the Goal-Question-Metric (GQM) approach, we evaluated each model’s effectiveness across task types (Analyti- cal, Algorithm, Visualization) and varying difficulty levels. Our findings reveal that all models exceeded a 50% baseline success rate, confirming their capability beyond random chance. Notably, only ChatGPT and Claude achieved success rates significantly above a 60% baseline, though none of the models reached a 70% threshold, indicating limitations in higher standards. ChatGPT demonstrated consistent performance across varying difficulty levels, while Claude’s success rate fluctuated with task complexity. Hypothesis testing indicates that task type does not significantly impact success rate overall. For analytical tasks, efficiency anal- ysis shows no significant differences in execution times, though ChatGPT tended to be slower and less predictable despite high success rates. For visualization tasks, while similarity quality among LLMs is comparable, ChatGPT consistently delivered the most accurate outputs. This study provides a structured, empirical evaluation of LLMs in data science, delivering insights that support informed model selection tailored to specific task demands. Our findings establish a framework for future AI assessments, emphasizing the value of rigorous evaluation beyond basic accuracy measures. Index Terms—data science, large language model, coding generation, empirical study, hypothesis testing Large Language Models (LLMs) have emerged as transfor- mative tools with the potential to revolutionize code generation in various domains, including data science [1]–[5]. Their ability to generate human-like text and code opens up pos- sibilities for automating complex tasks in data manipulation, visualization, and analytics. As data science projects often require extensive coding efforts that are time-consuming and demand significant expertise, leveraging LLMs could greatly enhance productivity and accessibility in this field. However, the effectiveness and reliability of LLM-generated code for data science applications remain underexplored, necessitating a thorough evaluation. While previous studies have evaluated LLMs in general programming tasks using platforms like LeetCode [6]–[9], the HumanEval benchmark [10], and GitHub Projects [11], Gu et al. [12] identified a notable gap in approaches to evaluate domain-specific code generation. They demonstrated that LLMs exhibit sub-optimal performance in generating domain-specific code for areas such as web and game devel- opment, due to their limited proficiency in utilizing domain- specific libraries. This finding underscores the need for more focused evaluations that consider the unique challenges of specialized domains like data science, which involve tasks such as handling datasets, performing complex statistical anal- yses, and generating insightful visualizations—areas not fully represented in general programming assessments. This paper addresses this gap by providing an empirical evaluation [13] of multiple LLMs on diverse data science- specific coding problems sourced from the Stratascratch plat- form [14]. The controlled experiment involves four main steps: (i) selecting 100 Python coding problems from Stratascratch, distributed across three difficulty levels (easy, medium, hard) and three problem types (Analytical, Algorithm, Visualiza- tion); (ii) transforming these problems into prompts following the optimal prompt structure for each type; (iii) using these prompts for each AI assistant to generate code solutions; and (iv) evaluating the generated code based on correctness, efficiency, and other relevant metrics. Our research seeks to answer the following question: How effective are LLMs for data science coding? By systematically assessing the performance of these AI assistants, we aim to identify their strengths and limitations in automating code generation for data science problems. Our contributions are multifold: 1) We provide an empirical evaluation of multiple LLMs on data science-specific coding problems, filling a critical gap in current research. 2) We assess Stratacratch as a platform to benchmark LLMs for data science code generation, evaluating its suitability and potential as a standardized dataset for LLM performance in this domain. arXiv:2411.11908v1  [cs.SE]  16 Nov 2024",
        "pdf_filename": "LLM4DS_Evaluating_Large_Language_Models_for_Data_Science_Code_Generation.pdf",
        "num_chunks": 1359
    },
    {
        "title": "log-RRIM Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Model",
        "context": "",
        "pdf_filename": "log-RRIM_Yield_Prediction_via_Local-to-global_Reaction_Representation_Learning_and_Interaction_Model.pdf",
        "num_chunks": 1426
    },
    {
        "title": "Look Before You Decide Prompting Active Deduction of MLLMs for Assumptive Reasoning",
        "context": "Recently, Multimodal Large Language Models (MLLMs) have achieved significant success across multiple disci- plines due to their exceptional instruction-following capa- bilities and extensive world knowledge. However, whether these MLLMs possess human-like compositional reasoning abilities remains an open problem. To unveil their rea- soning behaviors, we first curate a Multimodal Assumptive Reasoning Benchmark (MARS-Bench) in this paper. Inter- estingly, we find that most prevalent MLLMs can be easily tion, whereas such presuppositions appear naive to human reasoning. Besides, we also propose a simple yet effective method, Active Deduction (AD), to encourage the model to actively perform composite deduction before reaching a fi- nal decision. Equipped with the proposed AD method, a MLLM demonstrates significant improvements in assump- tive reasoning abilities without compromising its general- purpose question-answering performance. We also pro- vide extensive evaluations of both open-source and private MLLMs on MARS-Bench, along with experimental analyses of the AD method.",
        "pdf_filename": "Look_Before_You_Decide_Prompting_Active_Deduction_of_MLLMs_for_Assumptive_Reasoning.pdf",
        "num_chunks": 1427
    },
    {
        "title": "LUTMUL Exceed Conventional FPGA Roofline Limit by LUT-based Efficient Multiplication for Neural Netw",
        "context": "For FPGA-based neural network accelerators, digital signal pro- cessing (DSP) blocks have traditionally been the cornerstone for handling multiplications. This paper introduces LUTMUL, which harnesses the potential of look-up tables (LUTs) for performing multiplications. The availability of LUTs typically outnumbers that of DSPs by a factor of 100, offering a significant computational ad- vantage. By exploiting this advantage of LUTs, our method demon- strates a potential boost in the performance of FPGA-based neural network accelerators with a reconfigurable dataflow architecture. Our approach challenges the conventional peak performance on DSP-based accelerators and sets a new benchmark for efficient neural network inference on FPGAs. Experimental results demon- strate that our design achieves the best inference speed among all FPGA-based accelerators, achieving a throughput of 1627 im- ages per second and maintaining a top-1 accuracy of 70.95% on the ImageNet dataset. CCS CONCEPTS • Hardware →Reconfigurable logic and FPGAs; • Computing methodologies →Machine learning. KEYWORDS FPGAs, Quantization, Look-up tables, Roofline model. ACM Reference Format: Yanyue Xie, Zhengang Li, Dana Diaconu, Suranga Handagala, Miriam Leeser, and Xue Lin. 2025. LUTMUL: Exceed Conventional FPGA Roofline Limit by LUT-based Efficient MULtiplication for Neural Network Inference. In 30th Asia and South Pacific Design Automation Conference (ASPDAC ’25), January 20–23, 2025, Tokyo, Japan. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3658617.3697687 1 Field-Programmable Gate Arrays (FPGAs) have been widely used as deep learning accelerators, facilitating advancements in com- puter vision [7, 17, 35, 39] and natural language processing [4, 13, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ASPDAC ’25, January 20–23, 2025, Tokyo, Japan © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0635-6/25/01 https://doi.org/10.1145/3658617.3697687 16, 38] tasks. However, FPGAs lag behind Graphics Processing Units (GPUs) in terms of performance and ease of programming. FPGA reconfigurable logic mainly consists of look-up tables (LUT), block RAMs (BRAMs), and digital signal processing (DSP) blocks. Together with routing resources, FPGA can be reconfigured for customized designs. Despite the flexibility, FPGAs face constraints in clock frequency, floating-point performance, and memory band- width. This performance gap between FPGAs and GPUs is becoming even larger when considering the tensor core performance of GPUs. To address this, we need an algorithm-hardware co-design method to boost FPGAs with greater inference capability. FPGA accelerators can follow GPU-like [32, 34] architecture, which maps computation to compute cores with repetitive use. While beneficial, this approach encounters memory bandwidth is- sues similar to GPUs. Compared with GPUs, FPGAs usually have lower memory bandwidth, and the lower clock frequency of FPGAs means a lower upper bound of performance. While FPGA-based accelerators with specific instruction set architectures [37] offer flexibility across different models, they often compromise on per- formance due to non-optimized compute kernels for specific neural network layers. To bridge the performance gap between FPGAs and GPUs, par- ticularly in deep learning applications, we introduce LUTMUL, which leverages the look-up tables on FPGAs for deep learning tasks, fo- cusing on accelerating convolutional neural networks (CNNs). We recognize that the traditional FPGA designs, heavily dependent on DSP blocks, may not fully exploit the parallelism and flexibility that LUTs offer, as the availability of LUTs typically outnumbers DSPs by a factor of 100. Our method emphasizes a novel utiliza- tion of LUTs to enhance computational efficiency and throughput in deep learning applications. Specifically, we embed the convolu- tional neural network weights into LUTs, where the LUT input is the activations and the LUT output is the multiplication result. Dif- ferent from LUT-based general multipliers, our method is efficient in resources (requiring just 2 LUTs for a single 4-bit multiplication) and helps fully exploit the parallelism. We propose a reconfigurable dataflow architecture for our LUT- based efficient multiplication kernel. Our dataflow architecture minimizes the memory access time by processing the data on-chip through each layer without external memory. The reconfigurability of the FPGA allows us to tailor the architecture specifically for each distinct layer of CNNs. With LUT resources, the generated acceler- ator can potentially exceed the peak performance of conventional DSP-based FPGA accelerators. Our dataflow architecture aims to arXiv:2411.11852v1  [cs.AR]  1 Nov 2024",
        "pdf_filename": "LUTMUL_Exceed_Conventional_FPGA_Roofline_Limit_by_LUT-based_Efficient_Multiplication_for_Neural_Netw.pdf",
        "num_chunks": 1064
    },
    {
        "title": "Machine Intelligence in Africa - a survey",
        "context": "In the last 5 years, the availability of large audio datasets in African countries has opened unlimited opportunities to build machine intelli- gence (MI) technologies that are closer to the people and speak, learn, understand, and do businesses in local languages, including for those who cannot read and write. Unfortunately, these audio datasets are not fully exploited by current MI tools, leaving several Africans out of MI busi- ness opportunities. Additionally, many state-of-the-art MI models are not culture-aware, and the ethics of their adoption indexes are questionable. The lack thereof is a major drawback in many applications in Africa. This paper summarizes recent developments in machine intelligence in Africa from a multi-layer multiscale and culture-aware ethics perspective, show- casing MI use cases in 54 African countries through 400 articles on MI research, industry, government actions, as well as uses in art, music, the informal economy, and small businesses in Africa. The survey also opens discussions on the reliability of MI rankings and indexes in the African continent as well as algorithmic definitions of unclear terms used in MI. Keywords: machine intelligence, strategy, learning, risk-awareness, machine intelligence integrity, mean-field-type game theory. 1 Machine intelligence (MI) focuses on the creation of models, evolutionary dy- namics, and algorithms that enable machines or software to co-learn from data and improve their performance over time [1, 2]. MI is therefore an advanced com- puter & information science that allows a machine, device, software, program, code, or algorithm to interact intelligently with its environment, which means it can take measures, make decisions, perform actions, and develop strategies to maximize its chances of successfully achieving its preferences and objectives [3, 4, 5, 6, 7, 8, 9]. Demystifying MI for the General Public Addressing the global gap in public awareness surrounding MI, including its uses, benefits, risks, and limitations, is crucial not only in Africa but world- ∗Corresponding author: H. Tembine, Learning & Game Theory Laboratory, TIMADIE. Email: tembine@landglab.com 1 arXiv:2402.02218v1  [cs.CY]  3 Feb 2024",
        "pdf_filename": "Machine Intelligence in Africa - a survey.pdf",
        "num_chunks": 14551
    },
    {
        "title": "Machine Learning Assisted Postural Movement Recognition using Photoplethysmography(PPG)",
        "context": "With the growing percentage of elderly people and care home admissions, there is an urgent need for the development of fall detection and fall prevention technologies. This work presents, for the first time, the use of machine learning techniques to recognize postural movements exclusively from Photoplethysmography (PPG) data. To achieve this goal, a device was developed for reading the PPG signal, segmenting the PPG signals into individual pulses, extracting pulse morphology and homeostatic characteristic features, and evaluating different ML algorithms. Investigations into different postural movements (stationary, sitting to standing, and lying to standing) were performed by 11 participants. The results of these investigations provided insight into the differences in homeostasis after the movements in the PPG signal. Various machine learning approaches were used for classification, and the Artificial Neural Network (ANN) was found to be the best classifier, with a testing accuracy of 85.2% and an F1 score of 78% from experimental results. Keywords Artificial Intelligence · Fall detection · Photoplethysmography · Postural · Machine Learning · Wearables 1 People are living longer, and the aging population of the UK is ever increasing. The UK currently has a population of 5.5 million people aged over 75, which is set to increase to 7.1 million by 2035 [1]. Of the 3.2 million of the 5.5 million people over the age of 80, half will have at least one fall a year [2]. These falls are caused due to many factors, including muscle weakness, poor balance or visual impairment [3]. With an elderly person falling every ten seconds in the UK, the prevalence of life altering injuries, such as head injuries and hip fractures, is high and can prove to be fatal [4]. These injuries can lead to individuals requiring hospitalisation and surgery. They result in a loss of confidence and anxiety of falls in the future, leading to them restricting their activities in their daily lives [4] and can result in requiring care home admission. This combination of physical and psychological impacts to the geriatric populations has led to falls being the ninth leading cause of disability-adjusted life years (DALYs) in England in 2013, putting a large strain on the National Health Service (NHS), costing £435 million annually in England alone for falls in the house [3]. Fall detection and fall prevention are two crucial strategies to reduce the prevalence of falls and allow the growing older population to maintain their independence. Fall detection is defined as the detection of a fall using sensors and cameras to summon help [4]. Fall prevention refers to systems to stop falls by observing the person’s movement [4] and actions to reduce the likelihood of falls. Fall preventative activities are carried out across many health disciplines and have been shown to have a large impact on the frequencies of falls experienced by the individuals utilising them [5]. These methods including exercise, fall risk assessments, assistive equipment and technological based interventions [5]. Exercise improves muscular strength and balance, and the fall risk assessments allow for clinical staff to assess mobility issues and physiological factors that may incur falls. Assistive equipment, such as grab rails and hoists, provide functional support to older adults, and aim to help with mobility around the house. Technological fall interventions are methods of identifying falls and addressing fall risks. These can be divided into pre and post fall interventions and ∗This work was carried out while the first author was a student in the University of Bristol, UK. arXiv:2411.11862v1  [eess.SP]  2 Nov 2024",
        "pdf_filename": "Machine_Learning_Assisted_Postural_Movement_Recognition_using_Photoplethysmography(PPG).pdf",
        "num_chunks": 823
    },
    {
        "title": "Medical Video Generation for Disease Progression Simulation",
        "context": "Modeling disease progression is crucial for improving the quality and efficacy of clinical diagnosis and prognosis, but it is often hindered by a lack of longitudinal medical image monitoring for individual patients. To address this chal- lenge, we propose the first Medical Video Generation (MVG) framework that enables controlled manipulation of disease- related image and video features, allowing precise, realistic, and personalized simulations of disease progression. Our ap- proach begins by leveraging large language models (LLMs) to recaption prompt for disease trajectory. Next, a con- trollable multi-round diffusion model simulates the disease progression state for each patient, creating realistic interme- diate disease state sequence. Finally, a diffusion-based video transition generation model interpolates disease progression between these states. We validate our framework across three medical imaging domains: chest X-ray, fundus photog- raphy, and skin image. Our results demonstrate that MVG significantly outperforms baseline models in generating co- herent and clinically plausible disease trajectories. Two user studies by veteran physicians, provide further validation and insights into the clinical utility of the generated sequences. MVG has the potential to assist healthcare providers in mod- eling disease trajectories, interpolating missing medical im- age data, and enhancing medical education through realistic, dynamic visualizations of disease progression. Disease progression refers to the way an illness evolves in an individual over time. Understanding this progression en- ables healthcare professionals to develop effective treatment strategies, anticipate complications, and adjust care plans accordingly. Disease progression modeling can also be seen as a form of human digital twin, laying the foundation for future precision medicine [37, 68, 70]. However, modeling disease progression on medical images presents significant challenges. These challenges arise primarily from the lack Region Guide Input Fundus Retinal Image MVG Region Guide Chest X-ray Image MVG Region Guide Skin Image MVG Prompt-controlled Disease Progression Generation Figure 1. Illustrative examples of video-based disease progres- sion simulation (6-8s) using predefined medical reports and our proposed method. The top sequence depicts a patient’s Diabetic Retinopathy. The middle sequence demonstrates the Edema in a patient’s lung. The bottom sequence demonstrates the Benign Skin Lesion in a patient’s skin. of continuous monitoring of individual patients over time, as well as the high cost and risks associated with collecting longitudinal imaging data [13, 38, 61]. The intricate and multifaceted dynamics of disease progression, combined with the lack of comprehensive and continuous image or video data of individual patients, result in the absence of established methodologies for medical imaging trajectories simulation [34]. Recent advancements in image and video generation mod- els present promising opportunities for simulating realistic medical videos, potentially enriching existing databases and addressing data limitations. To incorporate generative mod- els into disease progression simulations, we establish three key criteria that medical video generation models must meet: (i) The model should generate videos presenting long disease progression under zero-shot setting, as there are no existing arXiv:2411.11943v1  [cs.CV]  18 Nov 2024",
        "pdf_filename": "Medical_Video_Generation_for_Disease_Progression_Simulation.pdf",
        "num_chunks": 1348
    },
    {
        "title": "METEOR Evolutionary Journey of Large Language Models from Guidance to Self-Growth",
        "context": "Model evolution enables learning from feed- back to refine experiences and update skills, transforming models from having no domain knowledge to becoming domain experts. How- ever, there is currently no unified and effec- tive method for guiding this evolutionary pro- cess. To address this gap, we propose the Meteor method, which includes three training phases: weak-to-strong data distillation, itera- tive training, and self-evolution strategies. Each phase maximizes the model’s inherent domain capabilities, allowing it to autonomously re- fine its domain knowledge and enhance per- formance. Experiments demonstrate that our approach significantly improves accuracy, com- pleteness, relevance, coherence, and reliability across domain-specific tasks. 1 The development of large language models (LLMs) has ushered in a new era in the field of natural language processing (NLP), showcasing remark- able general capabilities across a wide range of applications (OpenAI, 2023; Yang et al., 2024a,b; Reid et al., 2024). However, despite their outstand- ing performance on general tasks, the training of a highly versatile LLM demands substantial compu- tational resources and financial investment. These high costs restrict their use in many situations, par- ticularly in specific domains. In specific domains, there is often no need for a general purpose in- telligent model; instead, a model that acts as an expert within a particular domain is more desir- able. Furthermore, these domain-specific expert models should be trained at lower costs and eas- ily deployed in their respective fields. Therefore, finding efficient ways to create a domain-specific expert model has become a key research focus in the development of LLMs (Ling et al., 2024; Li et al., 2024a). *Corresponding author. Some studies leverage the inherent capabilities of LLMs combined with domain-specific external enhancements to enable their application in spe- cific domains. These methods involve explicitly or implicitly acquiring domain knowledge from external knowledge bases (Lu et al., 2023; Izac- ard et al., 2023; Schuurmans, 2023) or utilizing domain-specific tools to assist LLMs in specific domains (Jin et al., 2024; Li et al., 2023; Liang et al., 2023). However, these methods rely on the model’s strong general capabilities and are typically applicable only to models with a large number of parameters, making them costly to de- ploy and limiting their widespread adoption. Sev- eral researchers have explored model evolution ap- proaches to enhance domain-specific capabilities. Wu et al. (2023) achieves evolution in the finan- cial domain through extensive manually annotated domain data, yet this approach proves challenging to scale due to the difficulty in data acquisition. While Xi et al. (2024) proposes utilizing general large models for supervision and feedback on do- main model-generated data, this methodology re- mains constrained by the performance ceiling of the supervising model. Although Singh et al. (2024) made breakthrough progress in the coding domain through self-generated data and self-training, elimi- nating dependence on human annotations and large models, their approach has not yet been effectively extended to other specific domains. To address the challenges faced by current mod- els in domain-specific applications, we propose a self-evolution method named METEOR, a weak- to-strong evolution framework that enables LLMs to progressively evolve from supervised guidance to autonomous enhancement. Meteor offers a com- prehensive training framework that guides an LLM from having no domain expertise to becoming a domain expert. This framework consists of three key stages: an initial fine-tuning stage, which aims to impart basic domain knowledge to the LLM; an 1 arXiv:2411.11933v1  [cs.LG]  18 Nov 2024",
        "pdf_filename": "METEOR_Evolutionary_Journey_of_Large_Language_Models_from_Guidance_to_Self-Growth.pdf",
        "num_chunks": 1458
    },
    {
        "title": "ModeSeq Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling",
        "context": "Anticipating the multimodality of future events lays the foundation for safe autonomous driving. However, multi- modal motion prediction for traffic agents has been clouded by the lack of multimodal ground truth. Existing works pre- dominantly adopt the winner-take-all training strategy to tackle this challenge, yet still suffer from limited trajectory diversity and misaligned mode confidence. While some ap- proaches address these limitations by generating excessive trajectory candidates, they necessitate a post-processing stage to identify the most representative modes, a process lacking universal principles and compromising trajectory accuracy. We are thus motivated to introduce ModeSeq, a new multimodal prediction paradigm that models modes as sequences. Unlike the common practice of decoding mul- tiple plausible trajectories in one shot, ModeSeq requires motion decoders to infer the next mode step by step, thereby more explicitly capturing the correlation between modes and significantly enhancing the ability to reason about mul- timodality. Leveraging the inductive bias of sequential mode prediction, we also propose the Early-Match-Take- All (EMTA) training strategy to diversify the trajectories further. Without relying on dense mode prediction or rule- based trajectory selection, ModeSeq considerably improves the diversity of multimodal output while attaining satisfac- tory trajectory accuracy, resulting in balanced performance on motion prediction benchmarks. Moreover, ModeSeq nat- urally emerges with the capability of mode extrapolation, which supports forecasting more behavior modes when the future is highly uncertain. Handling the intricate uncertainty presented in the real world is one of the major hurdles in autonomous driving. One aspect of the uncertainty lies in the multimodal behav- ior of traffic agents, i.e., multiple instantiations of an agent’s Scene Embedding 0.5 0.4 0.3 (a) Parallel mode modeling Scene Embedding 0.7 0.3 0.1 (b) Sequential mode modeling Figure 1. A comparison between parallel and sequential mode modeling. While parallel mode modeling (Fig. 1a) decodes mul- timodal trajectories in one shot, our sequential mode modeling (Fig. 1b) reasons about multiple plausible futures step by step, which captures the relationships between modes to avoid produc- ing indistinguishable trajectories and confidence scores. future may be compatible with a given observation of the past. Without characterizing the multimodal distribution of agent motions, autonomous vehicles may fail to interact with the surroundings in a safe and human-like manner. For this reason, advanced decision-making systems demand a motion predictor to forecast several plausible and represen- tative trajectories of critical agents [8, 16]. Although multimodality has long been the central topic studied in motion prediction, this problem has not been fundamentally solved owing to the unavailability of mul- timodal ground truth, i.e., only one possibility is observable in real-world driving data. To struggle with this dilemma, most existing works adopt the winner-take-all (WTA) strat- egy [15] for training [4, 6, 17, 27, 39, 46, 53, 54]. Under this strategy, only the best among all predicted trajectories will receive supervision signals for regression, while all the remaining will be masked in the training loss. Despite being the current standard practice in the research community, the WTA solution has been found to cause mode collapse eas- ily and produce indistinguishable trajectories [24, 34, 45], further confusing the learning of mode scoring [18]. As a remedy, some recent research intends to cover the ground- truth mode by generating a massive number of trajectory candidates [27, 39, 46], from which the most representa- tive ones are heuristically selected based on post-processing methods such as non-maximum suppression (NMS). How- 1 arXiv:2411.11911v1  [cs.LG]  17 Nov 2024",
        "pdf_filename": "ModeSeq_Taming_Sparse_Multimodal_Motion_Prediction_with_Sequential_Mode_Modeling.pdf",
        "num_chunks": 1465
    },
    {
        "title": "Monolingual alignment of word senses and definitions in lexicographical resources",
        "context": "",
        "pdf_filename": "Monolingual alignment of word senses and definitions in lexicographical resources.pdf",
        "num_chunks": 12328
    },
    {
        "title": "Multi-Head RAG Solving Multi-Aspect Problems with LLMs",
        "context": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses. Existing RAG solutions do not fo- cus on queries that may require fetching multiple documents with substantially different contents. Such queries occur frequently, but are challenging because the embeddings of these documents may be distant in the embedding space, making it hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveraging activations of Transformer’s multi-head attention layer, instead of the decoder layer, as keys for fetching multi-aspect documents. The driving moti- vation is that different attention heads can learn to capture different data aspects. Harnessing the corresponding activations results in embeddings that represent var- ious facets of data items and queries, improving the retrieval accuracy for complex queries. We provide an evaluation methodology and metrics, multi-aspect datasets that we release online, and real-world use cases to demonstrate MRAG’s effec- tiveness, showing improvements of up to 20% in relevance over standard RAG baselines. MRAG can be seamlessly integrated with existing RAG frameworks and benchmarking tools like RAGAS as well as different classes of data stores. Website & code: https://github.com/spcl/MRAG 1 Large Language Models (LLMs) transformed many machine learning tasks using in-context learning abilities. They achieved such accuracy by leveraging an increasing number of parameters, which in recent models have grown to hundreds of billions, making LLM training expensive in terms of both time and resources. It also comes with the danger of leaking confidential data into model weights (Yan et al., 2024; Shi et al., 2024; Patil et al., 2024). Additionally, continuous training through fine-tuning is necessary to keep LLMs up-to-date. Even using the newest data, LLMs display an ongoing problem of hallucinations (Zhang et al., 2023; Xu et al., 2024c; Huang et al., 2023) by providing factually incorrect information. Retrieval Augmented Generation (RAG) was proposed (Lewis et al., 2020; Guu et al., 2020) in order to address these issues as well as others and make LLMs more trustworthy. The key idea behind RAG is to enhance the generative model’s capabilities by integrating a retrieval system that fetches relevant passages from a large corpus of data. In this setting, when a query is received, the retrieval system first identifies and retrieves pertinent information, which is fed into the generative model’s context for a more accurate and relevant response. Instead of the model storing information within its weights, RAG effectively leverages external knowledge, reducing ∗corresponding author 1 arXiv:2406.05085v2  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Multi-Head_RAG_Solving_Multi-Aspect_Problems_with_LLMs.pdf",
        "num_chunks": 1533
    },
    {
        "title": "Multi-LoRA Composition for Image Generation",
        "context": "Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accurate rendition of specific elements like distinct characters or unique styles in generated images. Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of complex imagery. In this paper, we study multi-LoRA composition through a decoding-centric perspective. We present two training-free methods: LoRA Switch, which alternates between different LoRAs at each denoising step, and LoRA Composite, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research. It features a diverse range of LoRA categories with 480 composition sets. Utilizing an evaluation framework based on GPT-4V, our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition. The code, benchmarks, LoRA weights, and all evaluation details are available on our project website. 1 In the dynamic realm of generative text-to-image models (Ho et al., 2020; Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022; Ruiz et al., 2023; Sohn et al., 2023), the integration of Low-Rank Adaptation (LoRA) (Hu et al., 2022) stands out for its ability to fine-tune image synthesis with remarkable precision and minimal computational load. LoRA excels by specializing in one element — such as a specific character, a particular clothing, a unique style, or other distinct visual aspects — and being trained to produce diverse and accurate renditions of this element in generated images. For instance, users could customize their LoRA models to generate various images of themselves, achieving an array of personalized and realistic representations. 1 arXiv:2402.16843v2  [cs.CV]  19 Nov 2024",
        "pdf_filename": "Multi-LoRA_Composition_for_Image_Generation.pdf",
        "num_chunks": 1613
    },
    {
        "title": "Multi-Task Learning on Networks",
        "context": "",
        "pdf_filename": "Multi-Task Learning on Networks.pdf",
        "num_chunks": 3460
    },
    {
        "title": "Multimodal Tree Decoder for Table of Contents Extraction in Document Images",
        "context": "headings of different levels in documents to better understand the outline of the contents, which can be widely used for document understanding and information retrieval. Existing works often use hand-crafted features and predeﬁned rule-based functions to detect headings and resolve the hierarchical relationship between headings. Both the benchmark and research based on deep learning are still limited. Accordingly, in this paper, we ﬁrst introduce a standard dataset, HierDoc, including image samples from 650 documents of scientiﬁc papers with their content labels. Then we propose a novel end-to-end model by using the multimodal tree decoder (MTD) for ToC as a benchmark for HierDoc. The MTD model is mainly composed of three parts, namely encoder, classiﬁer, and decoder. The encoder fuses the multimodality features of vision, text, and layout information for each entity of the document. Then the classiﬁer recognizes and selects the heading entities. Next, to parse the hierarchical relationship between the heading entities, a tree-structured decoder is designed. To evaluate the performance, both the metric of tree-edit-distance similarity (TEDS) and F1- Measure are adopted. Finally, our MTD approach achieves an average TEDS of 87.2% and an average F1-Measure of 88.1% on the test set of HierDoc. The code and dataset will be released at: https://github.com/Pengfei-Hu/MTD. A huge amount of documents have been accumulated with digitization and OCR engines. However, most of them contain text with limited structural information. For example, the table of contents (ToC), which plays an important role in document understanding and information retrieval, is often missing. The task of ToC extraction is to restore the structure of the document and to recognize the hierarchy of sections. As shown in Fig. 1, the output of ToC extraction is a tree of headings with different levels. As for the recent studies, we can roughly divide them into two categories. The ﬁrst one assumes the presence of ToC pages [1] [2]. They ﬁrst detect ToC pages, then analyze them for ToC entries. However, there are quite a few documents without ToC pages [3]. Therefore, others extract ToC from the whole document [4] [5]. They usually utilize hand-crafted features and strong indicators to detect headings, which will be hierarchically ordered according to predeﬁned rule-based functions. Besides, some research explores hybrid approaches * Corresponding Author [6]. They ﬁrst consider whether the document contains ToC pages, then apply one of the methods above. In general, existing approaches depend greatly on strong indicators and predeﬁned rule-based functions. They can perform well on application-dependent and domain-speciﬁc datasets. However, a large amount of task-speciﬁc knowledge and human-designed rules are needed, which does not extend to other types of documents. Recently, deep learning based methods have achieved great success in many ﬁelds related to documents. For example, [7] proposes the LayoutLM method for document image understanding, which is inspired by BERT [8]. This method uses image features and position to pre-train the model and performs well on downstream tasks. Lately, ViBERTgrid [9] is proposed for key information extraction from documents. These demonstrate the powerful ability of deep learning based methods to deal with document-related problems. It is worth noting that the existing datasets are not suitable for deep learning based methods. The ISRI dataset [10] and the Medical Article Records Groundtruth (MARG) dataset [11] only contain bi-level images and similarly simple layouts, predominantly of journal articles. Some datasets [12] [13] [14] are available in several ICDAR challenges, which contain complex layouts of newspapers, books, and technical articles. However, their annotations lack the heading category. Anno- tations with the heading category are provided by PubLayNet [15]. However, there is no information about the heading depth. [16] collects 71 French documents and 72 English documents in PDF format from the ﬁnancial domain. Its structure extraction ground truth is still not aligned with the text lines in the document. To overcome the lack of data for the research of ToC extraction, we collect a dataset, namely Hierarchical academic Document (HierDoc). It contains 650 academic English docu- ments in various ﬁelds from ArXiv1. With LaTeX source code, we generate its ToC ground truth using regular expressions. We also provide annotations for each text line. In this paper, the text line is denoted as the entity. The ToC is aligned with entities for training. For this dataset, 350 and 300 documents 1https://arxiv.org/ arXiv:2212.02896v1  [cs.CV]  6 Dec 2022",
        "pdf_filename": "Multimodal Tree Decoder for Table of Contents Extraction in Document Images.pdf",
        "num_chunks": 821
    },
    {
        "title": "N-DriverMotion Driver motion learning and prediction using an event-based camera and directly traine",
        "context": "Driver motion recognition is a key factor in ensuring the safety of driving systems. This paper presents a novel system for learning and predicting driver motions, along with an event-based (720x720) dataset, N-DriverMotion, newly collected to train a neuromorphic vision system. The system includes an event-based camera that generates a driver motion dataset representing spike inputs and efficient spiking neural networks (SNNs) that are effective in training and predicting the driver’s gestures. The event dataset consists of 13 driver motion categories classified by direction (front, side), illumination (bright, moderate, dark), and participant. A novel optimized four-layer convolutional spiking neural network (CSNN) was trained directly without any time-consuming preprocessing. This enables efficient adaptation to energy- and resource-constrained on-device SNNs for real-time inference on high-resolution event-based streams. Compared to recent gesture recognition systems adopting neural networks for vision processing, the proposed neuromorphic vision system achieves competitive accuracy of 94.04% in a 13-class classification task, and 97.24% in an unexpected abnormal driver motion classification task with the CSNN architecture. Additionally, when deployed to Intel Loihi 2 neuromorphic chips, the energy-delay product (EDP) of the model achieved 20,721 times more efficient than that of a non-edge GPU, and 541 times more efficient than edge-purpose GPU. Our proposed CSNN and the dataset can be used to develop safer and more efficient driver-monitoring systems for autonomous vehicles or edge devices requiring an efficient neural network architecture. 1 With the advancement of artificial intelligence (AI), it is being applied across various industrial fields, and among these, vehicle AI systems are emerging as one of the most prominent applications. Onboard AIs in vehicles are used to assist autonomous driving and the safety of drivers and pedestrians by integrating with the control system [1]. In particular, the EU and the United States have introduced mandatory requirements for various driver assistance systems to ensure safe road traffic by regulations on general automotive safety [2]. The primary objective of this regulation is to enhance the protection of elderly drivers, vehicle occupants, pedestrians, and cyclists. Given that research shows human error is the cause of 95% of accidents, implementing this regulation is projected to save more than 25,000 lives and prevent at arXiv:2408.13379v2  [cs.CV]  18 Nov 2024",
        "pdf_filename": "N-DriverMotion_Driver_motion_learning_and_prediction_using_an_event-based_camera_and_directly_traine.pdf",
        "num_chunks": 1014
    },
    {
        "title": "Neural Networks for Chess",
        "context": "",
        "pdf_filename": "Neural Networks for Chess.pdf",
        "num_chunks": 8364
    },
    {
        "title": "Neurosymbolic Graph Enrichment for Grounded World Models",
        "context": "The development of artificial intelligence systems capable of understanding and reasoning about complex real-world scenarios is a significant challenge. In this work we present a novel approach to enhance and exploit LLM re- active capability to address complex problems and interpret deeply contex- tual real-world meaning. We introduce a method and a tool for creating a multimodal, knowledge-augmented formal representation of meaning that combines the strengths of large language models with structured semantic representations. Our method begins with an image input, utilizing state- of-the-art large language models to generate a natural language description. tion (AMR) graph, which is formalized and enriched with logical design pat- terns, and layered semantics derived from linguistic and factual knowledge bases. The resulting graph is then fed back into the LLM to be extended with implicit knowledge activated by complex heuristic learning, including semantic implicatures, moral values, embodied cognition, and metaphorical representations. By bridging the gap between unstructured language models and formal semantic structures, our method opens new avenues for tackling intricate problems in natural language understanding and reasoning. Keywords: Neurosymbolic AI, Knowledge Representation, Knowledge Extraction, Large Language Models, Graph RAG, Hybrid Reasoning The development of artificial intelligence systems capable of understand- ing and reasoning about complex real-world scenarios remains a significant 1 arXiv:2411.12671v1  [cs.AI]  19 Nov 2024",
        "pdf_filename": "Neurosymbolic_Graph_Enrichment_for_Grounded_World_Models.pdf",
        "num_chunks": 1287
    },
    {
        "title": "Newclid A User-Friendly Replacement for AlphaGeometry",
        "context": "We introduce a new symbolic solver for geometry, called Newclid, which is based on AlphaGeometry. Newclid contains a symbolic solver called DDARN (derived from DDAR- Newclid), which is a significant refactoring and upgrade of AlphaGeometry’s DDAR symbolic solver by being more user-friendly - both for the end user as well as for a programmer wishing to extend the codebase. For the programmer, improvements include a modularized codebase and new debugging and visualization tools. For the user, Newclid contains a new command line interface (CLI) that provides interfaces for agents to guide DDARN. DDARN is flexible with respect to its internal reasoning, which can be steered by agents. Further, we support input from GeoGebra to make Newclid accessible for educational contexts. Further, the scope of problems that Newclid can solve has been expanded to include the ability to have an improved understanding of metric geometry concepts (length, angle) and to use theorems such as the Pythagorean theorem in proofs. Bugs have been fixed, and reproducibility has been improved. Lastly, we re-evaluated the five remaining problems from the original AG-30 dataset that AlphaGeometry was not able to solve and contrasted them with the abilities of DDARN, running in breadth-first-search agentic mode (which corresponds to how DDARN runs by default), finding that DDARN solves an additional problem. We have open-sourced our code under: https://github.com/LMCRC/Newclid 1 General remarks. AlphaGeometry [Trinh et al., 2024] demonstrated the ability to solve geom- etry problems at the level of the International Mathematical Olympiad (IMO), with performance comparable to top human competitors. At the heart of AlphaGeometry is a formal language that encodes geometric problems and theorems, rooted in JGEX [Ye et al., 2011], as well as a symbolic reasoning engine called DDAR (see Subsection 4.4.2 for more information), written in Python, ∗Equal contributions. †Corresponding author: vladmir.siccagoncalves@mail.mcgill.ca ‡Work carried out while at Huawei. §Corresponding senior author: simon.frieder@cs.ox.ac.uk 1 arXiv:2411.11938v1  [cs.GR]  18 Nov 2024",
        "pdf_filename": "Newclid_A_User-Friendly_Replacement_for_AlphaGeometry.pdf",
        "num_chunks": 3687
    },
    {
        "title": "On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control System Design and",
        "context": "Personalized driving refers to an autonomous vehicle’s ability to adapt its driving behavior or control strategies to match individual users’ preferences and driving styles while maintaining safety and comfort standards. However, existing works either fail to capture every individual pref- erence precisely or become computationally inefficient as the user base expands. Vision-Language Models (VLMs) offer promising solutions to this front through their natu- ral language understanding and scene reasoning capabili- ties. In this work, we propose a lightweight yet effective on- board VLM framework that provides low-latency person- alized driving performance while maintaining strong rea- soning capabilities. Our solution incorporates a Retrieval- Augmented Generation (RAG)-based memory module that enables continuous learning of individual driving prefer- ences through human feedback. Through comprehensive real-world vehicle deployment and experiments, our sys- tem has demonstrated the ability to provide safe, comfort- able, and personalized driving experiences across various scenarios and significantly reduce takeover rates by up to 76.9%. To the best of our knowledge, this work represents the first end-to-end VLM-based motion control system in real-world autonomous vehicles. The autonomous driving industry is experiencing an evo- lution towards human-centric systems [6, 28], where vehi- cle automation extends beyond only considering traditional safety and efficiency metrics but also considers understand- ing users’ implicit instructions and providing personalized driving experiences [11, 12]. Personalized driving experi- ences are crucial for user acceptance and trust, as they help bridge the gap between autonomous technology and human expectations. This trend reflects a growing recognition that successful adoption of the autonomous vehicle requires not just technically self-driving, but also the ability to provide human-like driving experiences that align with individual preferences and expectations [11, 12]. Previous work in personalized autonomous driving has followed two main approaches. The first uses clustering al- gorithms to classify drivers into broad categories (e.g., ag- gressive or conservative), but this fails to capture individ- ual nuances and preferences, forcing users into predefined groups that may not match their actual driving style [48, 50]. The second approach develops individual models for each user through learning-based methods [14, 49], but this re- quires extensive training data per user and becomes compu- tationally inefficient as the user base grows. Furthermore, these methods lack the ability to reason about real-time hu- man instructions or adapt to changing environments. Recent advances in Vision-Language Models (VLMs) have demonstrated promising capabilities in understanding complex driving scenarios and natural language instructions through their integration of computer vision and language processing [7, 10, 30, 37]. The development in VLMs has led researchers to leverage VLMs’ multimodal understand- ing capabilities to enhance both perception and decision- making in autonomous systems [36, 46]. However, there remains a research gap in leveraging VLMs to enhance con- trol policies or adapt them to individual driving preferences and styles. This gap is particularly evident in the challenge of translating a high-level understanding of human prefer- ences and scenario information into actionable low-level control policies. Additionally, the computational demands of previously adopted large-scale models make on-board deployment infeasible, forcing reliance on cloud-based in- ferencing. This solution depends on stable internet connec- tivity and can introduce significant latency issues in the in- ference process, with response times reaching up to 3 or 4 seconds [26]. This is incompatible with the reliable and near real-time requirements of autonomous driving. arXiv:2411.11913v1  [cs.AI]  17 Nov 2024",
        "pdf_filename": "On-Board_Vision-Language_Models_for_Personalized_Autonomous_Vehicle_Motion_Control_System_Design_and.pdf",
        "num_chunks": 1207
    },
    {
        "title": "On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem",
        "context": "We study the generalization capability of Unsuper- vised Learning in solving the Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN) trained with a surrogate loss function to generate an embedding for each node. We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route. We then apply local search to generate our final predictions. Our investigation explores how different training instance sizes, em- bedding dimensions, and distributions influence the outcomes of Unsupervised Learning methods. Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model’s ability to solve TSP. Furthermore, in evaluating generalization across different distribu- tions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results. Our findings suggest that models trained on harder instances exhibit bet- ter generalization capabilities, highlighting the importance of selecting appropriate training in- stances in solving TSP using Unsupervised Learn- ing. The goal of machine learning for Combinatorial Optimiza- tion (CO) is to enhance or surpass handcrafted heuristics. Recently, there has been an increasing trend in applying Machine Learning (ML) to tackle CO problems (Bengio et al., 2021). Different from manually crafted heuristics, machine learning approaches harness the power of data to uncover patterns in CO problems. The Euclidean Travelling Salesman Problem (TSP) is one of 1Department of Computer Science, Cornell University, Ithaca 14850, USA. Correspondence to: Yimeng Min <min@cs.cornell.edu>. Preprint the most famous and intensively studied CO problems. TSP asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city? A variety of methods have been devel- oped to solve TSP, including the Lin-Kernighan-Helsgaun (LKH) heuristics, which is known for their effectiveness in approximating solutions (Helsgaun, 2000), and the Con- corde solver, which guarantees optimality of the solutions. The application of ML for TSP has primarily focused on Su- pervised Learning (SL) and Reinforcement Learning (RL). However, SL methods encounter the challenge of expensive annotations, while RL methods struggle with sparse reward problems. Recently, (Min et al., 2024) proposes a new approach named UTSP that employs Unsupervised Learning (UL) to build a data-driven heuristics for the TSP. This unsupervised method does not depend on any labelled dataset and gener- ates a heatmap in a non-autoregressive manner, offering a distinct alternative to traditional SL and RL models. While the UL heuristics offer a promising approach, the challenge of generalizing across varying sizes and distribu- tions remains significant. In particular, the model presented in (Min et al., 2024) requires retraining to adapt to new sizes, indicating that a model trained on one size cannot effectively generalize to different sizes. This paper explores the generalization capabilities of unsu- pervised heuristics for the TSP. Our findings indicate that the UL model is able to generalize across different problem sizes. Regarding the generalization behavior of different dis- tributions, based on the hardness results by (Gent & Walsh, 1996), we relate different distributions to distinct levels of hardnesses. This allows us to investigate the impact of the training data’s hardness on the model’s performance. Our primary contributions are outlined as follows: We pro- pose a novel approach for enabling a TSP model, once trained, to generalize effectively across different problem sizes. We show that training with larger problem sizes can enhance model performance. Furthermore, we investigate the impact of various embedding dimensions on TSP per- formance, finding that larger embedding dimensions can 1 arXiv:2403.20212v2  [cs.AI]  19 Nov 2024",
        "pdf_filename": "On_Size_and_Hardness_Generalization_in_Unsupervised_Learning_for_the_Travelling_Salesman_Problem.pdf",
        "num_chunks": 1057
    },
    {
        "title": "Opportunities and Challenges to Integrate Artificial Intelligence into Manufacturing Systems - Thoughts from a Panel Discussion",
        "context": "Rapid advances in artiﬁcial intelligence (AI)1 have the potential to signiﬁcantly increase the productivity, quality, and proﬁtability in future manufacturing systems. Traditional mass-production will give way to personalized production, with each item made to order, at the low cost and high quality consumers have come to expect. Manufacturing systems will have the intelligence to be resilient to multiple disruptions, from small-scale machine breakdowns, to large-scale natural disasters. Products will be made with higher precision and lower variability. While gains have been made towards the development of these factories of the future, many challenges remain to fully realize the vision shown in Figure 1. To consider the challenges and opportunities associated with this topic, a panel of experts from Industry, Academia, and Government was invited to participate in an active discussion at the 2022 Modeling, Estimation and Control Conference (MECC) held in Jersey City, New Jersey from October 3- 5, 2022 [1]. This panel discussion started with an overview presentation given by Professor Ilya Kovalenko (Pennsyl- vania State University) on the topic of automated learning for manufacturing systems [2]. Following the overview, the panelists introduced themselves, their organizations, and pro- vided preliminary thoughts on the integration of AI into manufacturing systems. Panelists included Dr. Meiling He (Rockwell Automation), Dr. Daewon Lee (Samsung AI Center NY), Dr. James Moyne (Applied Materials and University of Michigan), Dr. Robert Landers (University of Notre Dame), and Dr. Jordan Berg (National Science Foundation). The panel discussion focused on the challenges and opportunities to more fully integrate AI into manufacturing systems, and was moderated by Professors Kira Barton (University of Michigan) and Dawn Tilbury (University of Michigan). Three overarching themes emerged from the panel discus- sion. First, to be successful, AI will need to work seamlessly, and in an integrated manner with humans (and vice versa). I. Kovalenko is with Department of Mechanical Engineering and Industrial & Manufacturing Engineering, Pennsylvania State University, University Park, PA. {iqk5135}@psu.edu. K. Barton and D. M. Tilbury are with the Robotics Department and the Department of Mechanical Engi- neering, University of Michigan, Ann Arbor, MI, USA. {tilbury, bartonkl}@umich.edu. J. Moyne is with the Department of Me- chanical Engineering, University of Michigan, Ann Arbor, MI, USA. {moyne}@umich.edu. 1Caveat: The panel did not attempt to disentangle Artiﬁcial Intelligence from Machine Learning, and used the two terms loosely interchangeably during the discussion. Fig. 1. Vision for integrating AI Systems into the manufacturing environment. Second, signiﬁcant gaps in the infrastructure needed to enable the full potential of AI into the manufacturing ecosystem, in- cluding sufﬁcient data availability, storage, and analysis, must be addressed. And ﬁnally, improved coordination between universities, industry, and government agencies can facilitate greater opportunities to push the ﬁeld forward. The rest of this article brieﬂy summarizes these three themes, and concludes with a discussion of promising directions. II. HUMANS AND AI WORKING TOGETHER Although many large-scale manufacturing processes are automated to some degree, there are always humans involved at multiple levels. The humans may be physically interact- ing with the system (e.g., loading parts into a machine) or interacting through a computer (e.g., deﬁning the production schedule). Different Subject Matter Experts (SMEs) have dif- ferent expertise, some may have a deep technical knowledge of a speciﬁc process such as maintenance of a speciﬁc machine, while others may have a broader understanding of a system. Humans are expected to be adaptable, and able to react to both small disturbances (slightly out-of-order product ﬂow) and large changes (new product arriving). Most AI deployed in manufacturing plants today is rela- tively ﬁxed, i.e., it does not have the ability to learn beyond what it has been trained to do, nor can it ask questions when it does not understand what to do. The panel felt that there was a signiﬁcant opportunity to develop AI that can work in a more integrated fashion with SMEs, at different levels, perhaps answering questions posed by the SME and even posing ques- tions when something becomes uncertain. While some AI tools arXiv:2303.11139v1  [eess.SY]  20 Mar 2023",
        "pdf_filename": "Opportunities and Challenges to Integrate Artificial Intelligence into Manufacturing Systems - Thoughts from a Panel Discussion.pdf",
        "num_chunks": 377
    },
    {
        "title": "Optimizing Airline Reservation Systems with Edge-Enabled Microservices A Framework for Real-Time Dat",
        "context": "adoption of novel approaches to the development of quick, efficient, and adaptive reservation systems. This paper outlines in detail a conceptual framework for the implementation of edge computing microservices in order to address the shortcomings of traditional centralized architectures. Specifically, as edge computing allows for certain activities such as seat inventory checks, booking processes and even confirmation to be done nearer to the user, thus lessening the overall response time and improving the performance of the system. In addition, the framework value should include achieving the high performance of the system such as low latency, high throughput and higher user experience. The major design components include deployed distributed computing microservices orchestrated by Kubernetes, real-time message processing system with Kafka and its elastic scaling. Other operational components include Prometheus and Grafana, which are used to monitor and manage resources, ensuring that all operational processes are optimized. Although this research focuses on a design and theoretical scheming of the framework, its use is foreseen to be more advantageous in facilitating a transform in the provision of services in the airline industry by improving customers' satisfaction, providing infrastructure which is cheap to install and efficiently supporting technology changes such as artificial intelligence and internet of things embedded systems. In addition, the framework can also be useful in other processing areas that require real time like supply chain management, health care systems and in the management of smart cities. This research addresses the increasing demand for new technologies with modern well-distributed and real-time- centric systems and also provides a basis for future case implementation and testing. As such, the proposed architecture offers a market-ready, extensible solution to the problems posed by existing airline reservation systems. Keywords: Microservices Architecture, Artificial Intelligent, Blockchain Technology, Online Travel Agent, Microservices, Cloud Computing, Kubernetes. 1.1. Background One of the indispensable factors in the aviation sector is the airline reservation systems (ARS) which allows the booking and management of flight reservations. These systems have gone through a number of phases starting with manual processes and moving to the present digitalized platform which combines various services like ticketing, scheduling and customer management. Nevertheless, the ARS has its own challenges. Latency: The need for real-time data processing is critical in an ARS in order to provide the most current information on the availability of flights, their prices and schedules. High latency may cause the delay in booking confirmations thereby upsetting the customers. Conventional centralized structures often find it hard to satisfy such real-time needs leading to longer response times [4]. Data Loss: An ARS processes and provides a lot of information from several sources such as – boarders’ data, flight scheduling data, and their pricing data. The problem arises when there is a need to process that data in a timely and efficient manner. However, older systems may have limitations and systems cannot outperform during data fetching which causes delays leading to information processing [3].",
        "pdf_filename": "Optimizing_Airline_Reservation_Systems_with_Edge-Enabled_Microservices_A_Framework_for_Real-Time_Dat.pdf",
        "num_chunks": 938
    },
    {
        "title": "Painful intelligence - What AI can tell us about human suffering",
        "context": "",
        "pdf_filename": "Painful intelligence - What AI can tell us about human suffering.pdf",
        "num_chunks": 10559
    },
    {
        "title": "PaperWeaver - Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers",
        "context": "With the rapid growth of scholarly archives, researchers subscribe to “paper alert” systems that periodically provide them with recom- mendations of recently published papers that are similar to previ- ously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present tions, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users’ research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better under- stand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers. CCS CONCEPTS • Human-centered computing →Interactive systems and tools; Empirical studies in HCI; Natural language interfaces. ∗Work completed during a researcher internship at Semantic Scholar Research, Allen Institute for AI. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CHI ’24, May 11–16, 2024, Honolulu, HI, USA © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0330-0/24/05. https://doi.org/10.1145/3613904.3642196 KEYWORDS Scientific Paper, Contextualized Descriptions, Large Language Mod- els ACM Reference Format: Yoonjoo Lee, Hyeonsu B. Kang, Matt Latzke, Juho Kim, Jonathan Bragg, Joseph Chee Chang, and Pao Siangliulue. 2024. PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User- collected Papers. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI ’24), May 11–16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA, 19 pages. https://doi.org/10.1145/3613904.3642196 1 Managing an ever-increasing accumulation of knowledge has long been a challenge for scholars [8]. With the recent proliferation of published materials, researchers face an even bigger challenge of keeping up with the literature [25, 42, 55]. Fundamentally, scholars need to both discover new and relevant papers and contextual- ize them to their own research interests. One popular practice re- cently is to leverage recommender systems that can help researchers retrieve potentially relevant, such as arXivist1, arXiv Sanity2 or Google Scholar3. Typically, these systems allow users to create “pa- per alerts” by providing a short description of a specific research topic and a set of seed papers as examples of papers of interest. For example, Semantic Scholar4 allows users to save sets of collected papers under named topical folders. Users would then receive pe- riodic paper alerts that contain a list of recently published papers similar to the collected papers. These alerts help researchers to quickly narrow down from all recent publications to a small set of potentially relevant papers, allowing them to more easily stay up to date on research topics that are of interest to them. However, when receiving s set of potentially relevant papers in an alert, researchers still need to more deeply inspect each paper to 1https://arxivist.com/ 2https://arxiv-sanity-lite.com/ 3https://scholar.google.com/ 4https://www.semanticscholar.org/ arXiv:2403.02939v2  [cs.DL]  9 May 2024",
        "pdf_filename": "PaperWeaver - Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers.pdf",
        "num_chunks": 2040
    },
    {
        "title": "Pattern-Based Constraint Satisfaction and Logic Puzzles",
        "context": "",
        "pdf_filename": "Pattern-Based Constraint Satisfaction and Logic Puzzles.pdf",
        "num_chunks": 25136
    },
    {
        "title": "PLA4D Pixel-Level Alignments for Text-to-4D Gaussian Splatting",
        "context": "Previous text-to-4D methods have leveraged multiple Score Distillation Sampling (SDS) techniques, combining motion priors from video-based diffusion models (DMs) with geometric priors from multiview DMs to implicitly guide 4D renderings. However, differences in these priors result in conflicting gradient directions during optimization, causing trade-offs between motion fidelity and geometry ac- curacy, and requiring substantial optimization time to rec- oncile the models. In this paper, we introduce Pixel-Level Alignment for text-driven 4D Gaussian splatting (PLA4D) to resolve this motion-geometry conflict. PLA4D provides an anchor reference, i.e., text-generated video, to align the rendering process conditioned by different DMs in pixel space. For static alignment, our approach introduces a focal alignment method and Gaussian-Mesh contrastive learning to iteratively adjust focal lengths and provide ex- plicit geometric priors at each timestep. At the dynamic level, a motion alignment technique and T-MV refinement method are employed to enforce both pose alignment and motion continuity across unknown viewpoints, ensuring in- trinsic geometric consistency across views. With such pixel- level multi-DM alignment, our PLA4D framework is able to generate 4D objects with superior geometric, motion, and semantic consistency. Fully implemented with open-source tools, PLA4D offers an efficient and accessible solution for high-quality 4D digital content creation with significantly reduced generation time. Text-to-4D content generation has significant potential in applications ranging from game production to autonomous driving. However, this task remains challenging due to the need to generate high-quality geometry and textures, arXiv:2405.19957v4  [cs.CV]  19 Nov 2024",
        "pdf_filename": "PLA4D_Pixel-Level_Alignments_for_Text-to-4D_Gaussian_Splatting.pdf",
        "num_chunks": 1202
    },
    {
        "title": "Plurals A System for Guiding LLMs Via Simulated Social Ensembles",
        "context": "Recent debates raised concerns that language models may favor certain viewpoints. But what if the solution is not to aim for a “view from nowhere” but rather to leverage different viewpoints? We introduce Plurals, a system and Python library for pluralistic AI deliberation. Plurals consists of Agents (LLMs, optionally with personas) which deliberate within customizable Structures, with Moderators overseeing deliberation. Plurals is a generator of simu- lated social ensembles. Plurals integrates with government datasets to create nationally representative personas, includes deliberation templates inspired by deliberative democracy, and allows users to customize both information-sharing structures and deliberation behavior within Structures. Six case studies demonstrate fidelity to theoretical constructs and efficacy. Three randomized experiments show simulated focus groups produced output resonant with an online sample of the relevant audiences (chosen over zero-shot gen- eration in 75% of trials). Plurals is both a paradigm and a concrete system for pluralistic AI. CCS Concepts • Computing methodologies →Artificial intelligence; Multi- agent systems; Intelligent agents; • Human-centered comput- ing →Interaction paradigms; Interactive systems and tools; Open source software; Interaction design theory, concepts and paradigms. Keywords Human-Computer Interaction, Human-AI Interaction, Artificial Intelligence, Multi-Agent Systems, Pluralism 1 There is a fundamental tension between how generative AI models are built and how they are used. Companies typically build a small number of foundation or “generalist” models that dominate the market [90]. However, these generalist models are used by a diverse base of users—with varying preferences and values. Invariably, this tension sparked allegations of bias, with supposedly neutral models accused of favoring certain viewpoints [12, 25, 28]. While a tempting solution is to aim for models that have “no bias” and hold a “view from nowhere” [36], truly neutral models are likely infeasible. Some scholars argue that all knowledge is situated [36]. But with open-ended text generation, defining some unbiased ground truth is especially difficult. For many use cases, there is no unbiased ground truth. This difficulty is compounded by the fact that users can ask models a large variety of questions. Any bias benchmark can only capture an infinitesimal slice of the query space [69]. As a motivating example, imagine a company preparing to launch a new work-from-home policy. The CEO seeks to determine which aspects of the policy memo will raise concerns for employees. Or suppose a housing justice group aims to identify the most effective messaging for a homeless shelter proposal. LLMs can theoretically be deployed for both cases. But what viewpoint should the LLM adopt? Different employees and residents have different perspec- tives. The standard approach of prompting a single model is un- likely to represent diverse viewpoints. We propose an alternative approach: A system of LLMs engage in controlled deliberation, sim- ulating distinct viewpoints. The CEO could create a network of simulated employees to provide feedback, upweighting the voices of the most affected groups. The housing justice group could create a sequence of LLMs with demographically weighted personas to provide iterative feedback based on preceding concerns. As an alternative to “bias-free” models, we introduce a new plu- ralistic AI system [81], Plurals, that can accomplish these tasks. It is a public-facing Python library (Figure 1 for system overview, Figure 2 for code snippets, see here1 for library). Plurals consists of Agents (optionally integrated with government datasets for nation- ally representative personas) which deliberate within customizable Structures, with Moderators overseeing deliberation. Plurals is an end-to-end generator of customizable “simulated social ensembles”. We incorporate interaction templates inspired by democratic delib- eration theory and integration with government datasets for nation- ally representative personas. For example, to create an Agent repre- senting a male California resident, our system samples a statistically representative citizen from American National Election Studies, and then uses the citizen’s demographics and political stances as an LLM prompt. We draw on deliberative democracy theory, which empha- sizes dialogue between different views [14, 55], as a blueprint. Our work builds on research in deliberation [13, 14, 27, 35, 55, 59, 80], pluralistic sociotechnical systems [4, 33, 50, 96], and multi-agent AI 1https://github.com/josh-ashkinaze/plurals arXiv:2409.17213v5  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Plurals_A_System_for_Guiding_LLMs_Via_Simulated_Social_Ensembles.pdf",
        "num_chunks": 2903
    },
    {
        "title": "PoM Efficient Image and Video Generation with the Polynomial Mixer",
        "context": "Diffusion models based on Multi-Head Attention (MHA) have become ubiquitous to generate high quality images and videos. However, encoding an image or a video as a sequence of patches results in costly attention patterns, as the requirements both in terms of memory and com- pute grow quadratically. To alleviate this problem, we pro- pose a drop-in replacement for MHA called the Polynomial Mixer (PoM) that has the benefit of encoding the entire se- quence into an explicit state. PoM has a linear complex- ity with respect to the number of tokens. This explicit state also allows us to generate frames in a sequential fashion, minimizing memory and compute requirement, while still being able to train in parallel. We show the Polynomial Mixer is a universal sequence-to-sequence approximator, just like regular MHA. We adapt several Diffusion Trans- formers (DiT) for generating images and videos with PoM replacing MHA, and we obtain high quality samples while using less computational resources. The code is available at https://github.com/davidpicard/HoMM. In a sudden change of pace, high quality image and video generation have evolved from a task seemingly impossible to achieve to a task almost solved by available commercial or open-source tools like Stable Diffusion 3 [19], Sora [7] or MovieGen [61]. At the heart of this success lies the Multi- head Attention (MHA) in the transformer architecture [72] that has excellent scaling properties [58, 82]. These so- called scaling laws [44] enable brute-forcing complex prob- lems such as image and video generation by using very large models trained on gigantic data, at the expense of an ever increasing computational cost. The main focus of current research lies thus in scaling transformer-based approaches to larger models handling larger datasets. The issue with transformers is that the computational cost increases quadratically with the sequence length due to the pairwise computation in MHA. This means that gener- 256 1,024 2,048 4,096 10−2 10−1 100 Image resolution Time in second/image PoM forward+backward PoM forward MHA forward+backward MHA forward Figure 1. Comparison between the speed of PoM and Multi- Head Attention (MHA) in the same DiT-XL/2 architecture for different image resolutions. We use an H100 GPU and compute the average time on 100 synthetic training batches to perform the forward or forward+backward passes. We use synthetic data to remove the influence from data loading. Training with PoM is less costly than inference with MHA at higher resolutions. ating an image at twice the spatial resolution (respectively a video at twice the resolution and double the duration) results in 4 times more patches and thus 16 times more computa- tional cost (respectively 8 times more patches and thus 64 times more computational cost). Attempts at having trans- formers with sub-quadratic complexity [11, 47, 76] intro- duce the additional constraint of fixing the number of to- kens, which prevents generating images or videos of dif- ferent sizes. Alternatively, recurrent models such as State- Space Models (SSM) [26, 27] have been investigated for the task [38, 69, 79] since their complexity is linear with the se- quence length [25]. However, they introduce an arbitrary causal raster scan of the sequence that does not fit the 2D geometry of images very well. In this paper, we enable better scaling in large gener- ative models by introducing a new building block called the Polynomial Mixer (PoM). PoM has a linear complexity 1 arXiv:2411.12663v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "PoM_Efficient_Image_and_Video_Generation_with_the_Polynomial_Mixer.pdf",
        "num_chunks": 1819
    },
    {
        "title": "Post-Workshop Report on Science meets Engineering in Deep Learning, NeurIPS 2019, Vancouver",
        "context": "Science meets Engineering in Deep Learning took place in Vancouver as part of the Workshop section of NeurIPS 2019 [1]. As organizers of the workshop, we created the following report in an attempt to isolate emerging topics and recurring themes that have been presented throughout the event. Deep learning can still be a complex mix of art and engineering despite its tremendous success in recent years. The workshop aimed at gathering people across the board to address seemingly contrasting challenges in the problems they are working on. As part of the call for the workshop, particular attention has been given to the interdependence of architecture, data, and optimization that gives rise to an enormous landscape of design and performance intricacies that are not well-understood. This year, our goal was to emphasize the following directions in our community: (i) identify obstacles in the way to better models and algorithms; (ii) identify the general trends from which we would like to build scientiﬁc and potentially theoretical understand- ing; and (iii) the rigorous design of scientiﬁc experiments and experimental protocols whose purpose is to resolve and pinpoint the origin of mysteries while ensuring repro- ducibility and robustness of conclusions. In the event, these topics emerged and were broadly discussed, matching our expectations and paving the way for new studies in these directions. While we acknowledge that the text is naturally biased as it comes through our lens, here we present an attempt to do a fair job of highlighting the outcome of the workshop. 1Facebook AI 2DeepMind 3Google Brain 4Institut de Physique Thèorique - CEA Saclay 1 arXiv:2007.13483v2  [cs.LG]  29 Jul 2020",
        "pdf_filename": "Post-Workshop Report on Science meets Engineering in Deep Learning, NeurIPS 2019, Vancouver.pdf",
        "num_chunks": 457
    },
    {
        "title": "Predicting Customer Satisfaction by Replicating the Survey Response Distribution",
        "context": "For many call centers, customer satisfaction (CSAT) is a key performance indicator (KPI). However, only a fraction of customers take the CSAT survey after the call, leading to a biased and inaccurate average CSAT value, and missed opportunities for coaching, follow-up, and rec- tification. Therefore, call centers can benefit from a model predicting customer satisfaction on calls where the customer did not complete the survey. Given that CSAT is a closely moni- tored KPI, it is critical to minimize any bias in the average predicted CSAT (pCSAT). In this paper, we introduce a method such that pre- dicted CSAT (pCSAT) scores accurately repli- cate the distribution of survey CSAT responses for every call center with sufficient data in a live production environment. The method can be applied to many multiclass classification prob- lems to improve the class balance and minimize its changes upon model updates. 1 Many machine learning applications use classifiers updated periodically by developers. Without spe- cial control mechanisms, these updates can shift the relative balance of output classes, causing un- intended effects. For the case of predicting CSAT, we developed a control mechanism to address this issue, taking care to mitigate the risks posed by sampling noise. This paper explains our method and strategies for handling sampling noise, and aims to help developers seeking to replicate one or more target class distribution(s). Customer satisfaction (CSAT) is critical for call center performance assessment, yet often measured through surveys completed by a small subset of cus- tomers—averaging 8% in our dataset. This limited response rate can skew perceived performance, as non-responding customers’ satisfaction remains un- known. Predicting CSAT for all calls, even those without survey responses, can mitigate this bias Manderscheid and Lee (2023). Ensuring these pCSAT scores do not introduce further bias is crucial. This paper introduces a method to more accurately replicate the distribu- tion of survey CSAT responses in a live production environment, addressing limitations identified in prior work and providing more accurate metrics for call center performance. 2 Related Work Predicting CSAT using machine learning models has gained attention, especially in call center con- versations. The challenge is not only predicting accurate scores but also ensuring these predictions replicate the true distribution of survey responses. This section reviews relevant studies and method- ologies applied to similar problems, focusing on distribution replication and ordinal classification (since CSAT is measured on a 1-5 scale). 2.1 Predicting Customer Satisfaction Previous research explored various approaches to predict CSAT from contact center conversations. Bockhorst et al. (2017) developed a system using ASR-generated call transcripts, non-textual data, and sentiment scores to predict a Representative Satisfaction Index (RSI) with rank scoring and iso- tonic regression models. Similarly, Auguste et al. (2019) used the Net Promoter Score (NPS) with binary classification (promoters vs. detractors) for predicting customer satisfaction in chat conversa- tions, achieving moderate improvements with a macro F1 score of 53.8%. Other studies examined predicting CSAT from raw audio signal features such as acoustic, emo- tional, and prosodic features (Park and Gates, 2009; Zweig et al., 2006; Vaudable and Devillers, 2012; Devillers et al., 2010). This work builds on a previously developed method for predicting CSAT scores using ASR- generated (Automated Speech Recognition) call arXiv:2411.12539v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Predicting_Customer_Satisfaction_by_Replicating_the_Survey_Response_Distribution.pdf",
        "num_chunks": 741
    },
    {
        "title": "Preference-Conditioned Gradient Variations for Multi-Objective Quality-Diversity",
        "context": "nance, Quality-Diversity algorithms have been used to gener- ate collections of both diverse and high-performing solutions. Multi-Objective Quality-Diversity algorithms have emerged as a promising approach for applying these methods to complex, multi-objective problems. However, existing methods are limited by their search capabilities. For example, Multi-Objective Map- Elites depends on random genetic variations which struggle in high-dimensional search spaces. Despite efforts to enhance search efficiency with gradient-based mutation operators, ex- isting approaches consider updating solutions to improve on each objective separately rather than achieving desired trade- offs. In this work, we address this limitation by introducing Multi-Objective Map-Elites with Preference-Conditioned Policy- Gradient and Crowding Mechanisms: a new Multi-Objective Quality-Diversity algorithm that uses preference-conditioned policy-gradient mutations to efficiently discover promising re- gions of the objective space and crowding mechanisms to promote a uniform distribution of solutions on the Pareto front. We evaluate our approach on six robotics locomotion tasks and show that our method outperforms or matches all state-of-the-art Multi-Objective Quality-Diversity methods in all six, including two newly proposed tri-objective tasks. Importantly, our method also achieves a smoother set of trade-offs, as measured by newly- proposed sparsity-based metrics. This performance comes at a lower computational storage cost compared to previous methods. Index Terms—Quality-Diversity, Multi-Objective optimisation, MAP-Elites, Neuroevolution, Reinforcement Learning Over recent years, Deep Reinforcement Learning (RL) has enabled breakthroughs in mastering games [1], [2] as well as continuous control domains for locomotion [3], [4] and manipulation [5]. These milestones have demonstrated the extraordinary potential of RL algorithms to solve specific problems. However, most approaches return only one highly- specialised solution to a single problem. In contrast, there is a growing shift in focus towards not just uncovering one single solution that achieves high rewards, but instead many solutions that exhibit different ways of doing so [6]. Within this context, Quality-Diversity (QD) algorithms [7] have emerged as one promising approach for tackling this challenge. In QD, the primary goal is to produce a variety of high- quality solutions, rather than to focus exclusively on finding the single best one. One motivation for QD algorithms is that, finding many solutions can provide availability of alternative, back-up solutions in the event that the highest-performing solution is no longer suitable. For example, in robotics, gen- erating large collections of solutions has been shown to be helpful for addressing large simulation to reality gaps [8] and adapting to unforeseen damages [8], [9]. Alternatively, having multiple solutions can simply be used in order to promote innovation in the downstream task. In this context, QD has been used for creating diverse video game levels [10], [11] and generating building designs [12]. Despite the growing traction of QD, most research in this field has focused on single-objective applications. However, multi-objective (MO) problems pervade many real-world do- mains, including engineering [13], [14], finance [15], and drug design [16] and many state-of-the-art MO algorithms originate from Evolutionary Algorithm community [17]–[20]. Recently, Multi-Objective MAP-Elites algorithm (MOME) [21] marked the first attempt at bridging ideas from QD and MO optimisation. In MOQD, the overarching goal is to identify a broad collection of solutions that exhibit diverse features and achieve distinct performances across multiple objectives. More specifically, given a feature space that is tessellated into cells, the aim is to find a collection of solutions within each cell which offer different trade-offs on each of the objectives (see Figure 1). As an example, consider the task of designing building sites. Within this context, it may be interesting to find different designs that vary in the number of buildings on the site. Then for each possible number of buildings, further options can be generated which present different trade-offs of ventilation and noise levels [12]. This approach equips end- users with a spectrum of viable options, thereby broadening their perspective on the array of feasible design possibilities. The MOME algorithm demonstrated promising results in finding large collections of diverse solutions that balance mul- tiple objectives. However, MOME predominantly depends on random genetic variations that can cause slow convergence in large search spaces [22]–[24]. This renders it less suitable for evolving neural networks with a large number of parameters. Since the inception of the MOME framework, several related works exploring the domain of MOQD have emerged [12], [25], [26]. Among them, MOME-PGX [25] builds upon the MOME framework and was shown to achieve state-of-the-art performance on high-dimensional continuous control robotics tasks that can be framed as Markov Decision Processes. It uses crowding addition and selection mechanisms to encourage an even distribution of solutions on the Pareto front and employs policy-gradient mutations for each objective function in order to drive the exploration process toward promising regions of the solution space. However, the MOME-PGX approach is not without its own set of challenges. Firstly, it employs separate actor-critic networks for each objective function, which can be resource-intensive and may not scale with an arXiv:2411.12433v1  [cs.AI]  19 Nov 2024",
        "pdf_filename": "Preference-Conditioned_Gradient_Variations_for_Multi-Objective_Quality-Diversity.pdf",
        "num_chunks": 1351
    },
    {
        "title": "Provable unlearning in topic modeling and downstream tasks",
        "context": "Machine unlearning algorithms are increasingly important as legal concerns arise around the provenance of training data, but verifying the success of unlearning is often difﬁcult. Provable guarantees for unlearning are often limited to supervised learning settings. In this paper, we provide the ﬁrst theoretical guarantees for un- learning in the pre-training and ﬁne-tuning paradigm by studying topic models, simple bag-of-words language models that can be adapted to solve downstream tasks like retrieval and classiﬁcation. First, we design a provably effective unlearn- ing algorithm for topic models that incurs a computational overhead independent of the size of the original dataset. Our analysis additionally quantiﬁes the dele- tion capacity of the model – i.e., the number of examples that can be unlearned without incurring a signiﬁcant cost in model performance. Finally, we formally extend our analyses to account for adaptation to a given downstream task. In par- ticular, we design an efﬁcient algorithm to perform unlearning after ﬁne-tuning the topic model via a linear head. Notably, we show that it is easier to unlearn pre-training data from models that have been ﬁne-tuned to a particular task, and one can unlearn this data without modifying the base model. 1 Modern-day machine learning has shifted from single-stage supervised learning on manually constructed datasets to a paradigm in which models are pre-trained and subsequently ﬁne- tuned (Bommasani et al., 2022). In this setting, a model initially learns a good representation of the data using a self-supervised objective on a large unstructured corpus. The resulting pre-trained model is later adapted to solve speciﬁc tasks for which it is difﬁcult or costly to curate a large dataset. This blueprint has yielded strong performance in text (e.g., Devlin et al., 2019; Brown et al., 2020), vision (e.g., Oquab et al., 2024; He et al., 2022), and multimodal (e.g., Radford et al., 2021; Zhai et al., 2023) settings. It is well-known that the scale of the pre-training data is strongly corre- lated with the ﬁnal performance of the model (Hoffmann et al., 2022), leading to the construction of larger datasets via broad internet scrapes (Gao et al., 2020; Schuhmann et al., 2022; Soldaini et al., 2024; Penedo et al., 2023). Such datasets have been found to often inadvertently include private, sensitive, and unsafe data (Birhane et al., 2021; Longpre et al., 2024; He et al., 2024). Unsafe data can generally degrade model performance and introduce biases, making the model less useful for various applications (McKenna et al., 2023; Birhane & Prabhu, 2021; Choenni et al., 2021; Naous et al., 2024). Using private and sensitive data, even unknowingly, poses legal risks (Bommasani et al., 2022; Henderson et al., 2023). In particular, recent works have shown that models can memorize and thus permit the extraction of training data (Somepalli et al., 2023; Carlini et al., 2021; 2023). Moreover, one may be requested to remove data in accordance with GDPR’s right to be forgotten (European Parliament & Council of the European Union), or as part of a copyright-related lawsuit (Tremblay v. OpenAI, Inc.,, 2023; DOE 1 v. GitHub, Inc., N.D. Cal. 2022). Therefore, there is great empirical interest in developing machine unlearning algorithms that can surgically remove portions of the training data from an already learned model without harming per- formance. The gold standard for machine unlearning is for the model to behave as though it had never been trained on that datapoint (Cao & Yang, 2015). As it is often undesirable to completely 1",
        "pdf_filename": "Provable_unlearning_in_topic_modeling_and_downstream_tasks.pdf",
        "num_chunks": 2541
    },
    {
        "title": "Recall and Refine A Simple but Effective Source-free Open-set Domain Adaptation Framework",
        "context": "Open-set Domain Adaptation (OSDA) aims to adapt a model from a labeled source domain to an unlabeled target domain, where novel classes — also referred to as target-private un- known classes — are present. Source-free Open-set Domain Adaptation (SF-OSDA) methods address OSDA without ac- cessing labeled source data, making them particularly rele- vant under privacy constraints. However, SF-OSDA presents significant challenges due to distribution shifts and the intro- duction of novel classes. Existing SF-OSDA methods typi- cally rely on thresholding the prediction entropy of a sample to identify it as either a known or unknown class but fail to explicitly learn discriminative features for the target-private unknown classes. We propose Recall and Refine (RRDA), a novel SF-OSDA framework designed to address these lim- itations by explicitly learning features for target-private un- known classes. RRDA employs a two-step process. First, we enhance the model’s capacity to recognize unknown classes by training a target classifier with an additional decision boundary, guided by synthetic samples generated from tar- get domain features. This enables the classifier to effectively separate known and unknown classes. In the second step, we adapt the entire model to the target domain, address- ing both domain shifts and improving generalization to un- known classes. Any off-the-shelf source-free domain adapta- tion method (e.g., SHOT, AaD) can be seamlessly integrated into our framework at this stage. Extensive experiments on three benchmark datasets demonstrate that RRDA signifi- cantly outperforms existing SF-OSDA and OSDA methods. The source code is publicly available1. Unsupervised Domain Adaptation (UDA) (Ben-David et al. 2010; Ganin and Lempitsky 2015; Long et al. 2015) adapts a model from a labeled source domain to an unlabeled target domain (Oza et al. 2023), effectively addressing the issue of domain shift where the source and target distributions differ. UDA strategies typically align feature distributions between domains using metric learning techniques (Long et al. 2015; Kang et al. 2019) or adversarial training (Ganin and Lempit- sky 2015; Tzeng et al. 2017; Luo et al. 2019), and more re- cently, self-training approaches (Sun et al. 2022; Hoyer et al. 2023; Zhu, Bai, and Wang 2023). Despite their success, most 1https://github.com/ismailnejjar/RRDA current domain adaptation approaches operate under the as- sumption of a shared label set between the source and target domains (i.e., Cs = Ct), referred to as Closed-set Domain Adaptation (Saenko et al. 2010). However, this assumption is often impractical in real-world scenarios. In contrast, Open-set Domain Adaptation (OSDA) ex- tends the target label space beyond that of the source do- main (i.e., Cs ⊂Ct) (Saito et al. 2018; Liu et al. 2019), thereby adding complexity to the DA task. OSDA aims to align target samples from known classes with those from the source domain while effectively identifying target sam- ples belonging to categories not observed in the source do- main, referred to as unknown classes (Panareda Busto and Gall 2017; Bucci, Loghmani, and Tommasi 2020; Jang et al. 2022). Various criteria based on instance-level predictions have been proposed, including entropy-based (Feng, Xu, and Tao 2021; Saito et al. 2020) and confidence-based (Saito and Saenko 2021; Fu et al. 2020) methods. Additionally, privacy and legal considerations increas- ingly limit access to labeled source data for adaptation pur- poses. To address this, source-free adaptation methods (Fang et al. 2024) have emerged, enabling adaptation without re- liance on labeled source data (Kim et al. 2021; Kundu et al. 2020a; Li et al. 2020). In this paper, we focus on Source- free Open-set Domain Adaptation (SF-OSDA), where only a pre-trained source model is available for knowledge transfer, without access to labeled source data. While some Source- free Domain Adaptation (SF-DA) methods have demon- strated effectiveness in addressing SF-OSDA for classifica- tion tasks (Liang, Hu, and Feng 2020; Yang et al. 2022; Wan et al. 2024), semantic segmentation (Choe et al. 2024), and graph applications (Wang et al. 2024), they primarily focus on the semantics of known classes in the source domain, often overlooking the crucial aspect of novel-class seman- tics. These methods focus on segregating target samples with low entropy, categorizing them as known classes, and subse- quently optimizing specific objectives such as entropy mini- mization or clustering. In this process, data points associated with known classes are prioritized, while those with high en- tropy are typically excluded from training, leading to a se- mantic disparity between the known and unknown classes. To effectively adapt a pre-trained source model to a target domain facing both category and distribution shifts, we pro- pose Recall and Refine for Domain Adaptation (RRDA) for arXiv:2411.12558v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "Recall_and_Refine_A_Simple_but_Effective_Source-free_Open-set_Domain_Adaptation_Framework.pdf",
        "num_chunks": 1680
    },
    {
        "title": "Reference-free Hallucination Detection for Large Vision-Language Models",
        "context": "Large vision-language models (LVLMs) have made significant progress in recent years. While LVLMs exhibit excellent ability in lan- guage understanding, question answering, and conversations of visual inputs, they are prone to producing hallucinations. While several meth- ods are proposed to evaluate the hallucinations in LVLMs, most are reference-based and de- pend on external tools, which complicates their practical application. To assess the viability of alternative methods, it is critical to understand whether the reference-free approaches, which do not rely on any external tools, can efficiently detect hallucinations. Therefore, we initiate an exploratory study to demonstrate the effective- ness of different reference-free solutions in de- tecting hallucinations in LVLMs. In particular, we conduct an extensive study on three kinds of techniques: uncertainty-based, consistency- based, and supervised uncertainty quantifica- tion methods on four representative LVLMs across two different tasks. The empirical re- sults show that the reference-free approaches are capable of effectively detecting non-factual responses in LVLMs, with the supervised uncer- tainty quantification method outperforming the others, achieving the best performance across different settings. 1 Large vision-language models (LVLMs) like LLaVA (Liu et al., 2024), MiniGPT-4 (Zhu et al., 2024b) have demonstrated remarkable capabilities in understanding and generating complex visual and textual content. However, an emerging con- cern with these models is their tendency towards hallucination (Zhou et al., 2024; Zhu et al., 2024a; Geng et al., 2024b). For instance, a model might describe an object or event in the generated text that *Equal contribution. †Corresponding author. Figure 1: Reference-free Hallucination Detection Meth- ods used in this work. is not present in the input image. This phenomenon poses challenges for the reliability of LVLMs, high- lighting intrinsic limitations of current models in maintaining coherence between visual inputs and textual descriptions (Huang et al., 2023a). Most existing methods of assessing hallucina- tions rely on external models. Li et al. (2023) pro- pose POPE, which requires automated segmenta- tion tools like SEEM (Zou et al., 2024) to iden- tify present and absent objects in images. Faith- score (Jing et al., 2023) utilizes a visual entailment model (Xie et al., 2019) to validate the factuality, focusing on object attributes and their relationships. Nonetheless, these approaches increase the compu- tational cost and are restricted by the capabilities of these models. arXiv:2408.05767v2  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Reference-free_Hallucination_Detection_for_Large_Vision-Language_Models.pdf",
        "num_chunks": 1308
    },
    {
        "title": "Regret-Free Reinforcement Learning for LTL Specifications",
        "context": "Reinforcement learning (RL) is a promising method to learn optimal control policies for sys- tems with unknown dynamics. In particular, syn- thesizing controllers for safety-critical systems based on high-level specifications, such as those expressed in temporal languages like linear tem- poral logic (LTL), presents a significant challenge in control systems research. Current RL-based methods designed for LTL tasks typically offer only asymptotic guarantees, which provide no insight into the transient performance during the learning phase. While running an RL algorithm, it is crucial to assess how close we are to achieving optimal behavior if we stop learning. In this paper, we present the first regret-free on- line algorithm for learning a controller that ad- dresses the general class of LTL specifications over Markov decision processes (MDPs) with a finite set of states and actions. We begin by proposing a regret-free learning algorithm to solve infinite-horizon reach-avoid problems. For gen- eral LTL specifications, we show that the syn- thesis problem can be reduced to a reach-avoid problem when the graph structure is known. Addi- tionally, we provide an algorithm for learning the graph structure, assuming knowledge of a mini- mum transition probability, which operates inde- pendently of the main regret-free algorithm. Reinforcement learning (RL) is becoming more prevalent in computing efficient policies for systems with unknown dy- namical models. Although experimental evidence highlights the effectiveness of RL in numerous applications, ensuring the safety and reliability of algorithms is imperative for certain safety-critical systems. In such scenarios, the de- 1MPI-SWS, Kaiserslautern, Germany. Correspondence to: Mahmoud Salamati <msalamati@mpi-sws.org>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). velopment of algorithms that provide explicit performance guarantees becomes essential. Among the existing performance guarantees for RL, regret minimization has widely been studied recently (Auer et al., 2008; Agarwal et al., 2014; Srinivas et al., 2012; Dann et al., 2017). For an online learning algorithm, intuitively, re- gret is defined as the difference between the accumulated (expected) rewards collected by an optimal policy and the the algorithm during learning. Existing regret minimiza- tion algorithms assume that optimal policies are positional, meaning that optimal policies (deterministically) map every state into a corresponding action. While this suffices for most of basic reward structures, optimal policies may not be positional for more general reward structures. In particular, when the control objective is set by an LTL formula, optimal policy is, in general, not positional. RL for LTL specifications has recently become popular. Majority of the existing methods provide no performance guarantees (Icarte et al., 2018; Camacho et al., 2019; Hasan- beig et al., 2019; Kazemi et al., 2022; Hahn et al., 2019; Oura et al., 2020; Cai et al., 2020; Bozkurt et al., 2021; Sick- ert et al., 2016). Controller synthesis for finite-horizon and infinite-horizon LTL with probably approximately correct (PAC) guarantee has recently been studied (Fu & Topcu, 2014; Voloshin et al., 2022). It is shown that providing PAC guarantees for learning algorithms that target controller syn- thesis against infinite-horizon specifications requires addi- tional knowledge, such as minimum transition probability (Alur et al., 2022). However, there exists no regret-free RL-based controller synthesis method for LTL tasks. In this paper, we propose an online learning algorithm for control synthesis problems against LTL objectives, which provides sublinearly growing regret bounds. Specifically, we want to measure the performance of the learned control policy during learning. For that, we compare the satisfaction probabilities of an optimal policy and sequence of policies generated by the algorithm during learning. We consider the class of systems, whose dynamics can be captured by a finite MDP with unknown transition proba- bilities and fixed initial state sinit. The control objective is to synthesize a control policy that maximizes probability of satisfying a given LTL specification φ. Let π∗denote an optimal policy, meaning that applying π∗maximizes 1 arXiv:2411.12019v1  [cs.AI]  18 Nov 2024",
        "pdf_filename": "Regret-Free_Reinforcement_Learning_for_LTL_Specifications.pdf",
        "num_chunks": 1980
    },
    {
        "title": "Regulating Chatbot Output via Inter-Informational Competition",
        "context": "",
        "pdf_filename": "Regulating_Chatbot_Output_via_Inter-Informational_Competition.pdf",
        "num_chunks": 2315
    },
    {
        "title": "Reinforcement Learning with Action Sequence for Data-Efficient Robot Learning",
        "context": "Training reinforcement learning (RL) agents on robotic tasks typically requires a large number of training samples. This is because training data often consists of noisy trajectories, whether from exploration or human-collected demonstrations, making it difficult to learn value functions that understand the effect of taking each action. On the other hand, recent behavior-cloning (BC) approaches have shown that predicting a sequence of actions enables policies to effectively approximate noisy, multi-modal distributions of expert demonstrations. Can we use a similar idea for improving RL on robotic tasks? In this paper, we introduce a novel RL algorithm that learns a critic network that outputs Q-values over a sequence of actions. By explicitly training the value functions to learn the consequence of executing a series of current and future actions, our algorithm allows for learning useful value functions from noisy trajectories. We study our algorithm across various setups with sparse and dense rewards, and with or without demonstrations, spanning mobile bi-manual manipulation, whole-body control, and tabletop manipulation tasks from BiGym, HumanoidBench, and RLBench. We find that, by learning the critic network with action sequences, our algorithm outperforms various RL and BC baselines, in particular on challenging humanoid control tasks. Project website: younggyo.me/cqn-as. 0 2e4 4e4 6e4 8e4 1e5 Environment Steps 0 25 50 75 100 Success Rate (%) BiGym (25 Tasks) 0 2e6 4e6 6e6 8e6 1e7 Environment Steps 0 250 500 750 1000 Episode Return HumanoidBench (8 Tasks) 0 1e4 2e4 3e4 Environment Steps 0 25 50 75 100 Success Rate (%) RLBench (20 Tasks) RL: CQN-AS (Ours) CQN DrQ-v2+ SAC BC: ACT Figure 1: Summary of results. Coarse-to-fine Q-Network with Action Sequence (CQN-AS) is a value-based RL algorithm that learns a critic network with action sequence. We study CQN-AS on 53 robotic tasks from BiGym (Chernyadev et al., 2024), HumanoidBench (Sferrazza et al., 2024), and RLBench (James et al., 2020), where prior model-free RL algorithms struggle to achieve competitive performance. We show that CQN-AS outperforms various RL and BC baselines such as CQN (Seo et al., 2024), DrQ-v2+ (Yarats et al., 2022), SAC (Haarnoja et al., 2018), and ACT (Zhao et al., 2023). 1 Reinforcement learning (RL) holds the promise of continually improving policies through online trial- and-error experiences (Sutton & Barto, 2018), making it an ideal choice for developing robots that can adapt to various environments. However, despite this promise, training RL agents on robotic tasks typically requires a prohibitively large number of training samples (Kalashnikov et al., 2018; Herzog et al., 2023), which becomes problematic as deploying robots often incurs a huge cost. Therefore many of the recent successful approaches on robot learning have been based on behavior-cloning (BC; Pomerleau 1988), which can learn strong policies from offline expert demonstrations (Brohan et al., 2023b;a; Zhao et al., 2023; Chi et al., 2023; Team et al., 2024; Fu et al., 2024a). 1 arXiv:2411.12155v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Reinforcement_Learning_with_Action_Sequence_for_Data-Efficient_Robot_Learning.pdf",
        "num_chunks": 1854
    },
    {
        "title": "ResLearn Transformer-based Residual Learning for Metaverse Network Traffic Prediction",
        "context": "predicting Metaverse network traffic, addressing the growing demand for intelligent resource management in eXtended Reality (XR) services. We first introduce a state-of-the-art testbed captur- ing a real-world dataset of virtual reality (VR), augmented reality (AR), and mixed reality (MR) traffic, made openly available for further research. To enhance prediction accuracy, we then propose a novel view-frame (VF) algorithm that accurately identifies video frames from traffic while ensuring privacy compliance, and we develop a Transformer-based progressive error-learning algorithm, referred to as ResLearn for Metaverse traffic prediction. ResLearn significantly improves time-series predictions by using fully connected neural networks to reduce errors, particularly during peak traffic, outperforming prior work by 99%. Our contributions offer Internet service providers (ISPs) robust tools for real-time network management to satisfy Quality of Service (QoS) and enhance user experience in the Metaverse. Index Terms—Metaverse Network Traffic Prediction, Residual Learning, Extended Reality (XR), virtual reality (VR), augmented reality (AR), and mixed reality (MR). The Metaverse is a comprehensive ecosystem of inter- connected virtual worlds that provide immersive experiences to users. The ecosystem enhances existing and generates new value from economic, environmental, social, and cultural perspectives [1]. Services in the Metaverse ecosystem are designed to be accessed using immersive extended reality (XR) environments. XR is an umbrella term that describes the technologies affecting the user’s immersive experience, such as virtual reality (VR), augmented reality (AR), and mixed reality (MR) [2]. VR allows users to interact with virtually generated environments designed to simulate real-world experiences. AR overlays interactive, virtually generated information onto real- world objects or within real-world spaces. XR technologies lie on a spectrum between AR and VR. In cases where the dis- tinction between the realities is ambiguous, the experience is considered MR. As the Metaverse’s growth continues and XR evolves, the popularity of its services increases. Driven by the rapid growth of the Metaverse, Internet traffic is expected to surpass current forecasts significantly [3]. The entertainment and social media industries have seen the most substantial growth of Metaverse services, as evidenced by popular virtual performance events, one of which attracted an audience of 36 million users [4], [5]. Healthcare, training, and marketing for Metaverse services have also grown recently [6], [7]. Cloud rendering for Metaverse is crucial to offloading computing resources to make the services affordable, a popular technique for VR games [8]. Consequently, the Ericsson 2022 report emphasizes the growing need for more intelligent interactions between XR services and the network to maintain high Quality of Service (QoS) [3]. Therefore, network management is crucial for Internet service providers (ISPs) to accommodate adequate resources and avoid cybersickness among users [9]– [11]. Metaverse traffic consists of video, audio, and control flows [8], among all downlink video frames are resource-demanding in the case of VR, and both uplink/downlink video frames for AR and MR traffic. Therefore, predicting the frame size is vital, and for latency-related issues, it is essential to predict frame inter-arrival time and frame duration [12]. Predicting frame size, inter-arrival time, and frame duration will help ISPs prepare and manage the Metaverse network for holistic and in- telligent traffic management. However, there needs to be more real-world Metaverse data and research in prediction to make progress in the field. Essentially, frame-related information is time series data. The recent advent of different state-of-the-art artificial intelligence (AI) based time series models has shown tremendous progress in time series predictions [13]. The only VR frame size prediction work is available at [14]; therefore, we consider it state-of-the-art (SoA) work for benchmark comparison. The work studies different AI models to predict VR frame size. It establishes stacked LSTM to produce better results based on transfer learning methodologies. Therefore, the solution aims for online prediction, imperative for real- time network management. However, the work is evaluated on a small dataset captured in a controlled environment. Also, the performance can be improved with further reduction in the error. The frame identification methodology used in the work might need to be revised because the frame loss for 120 Mbps is more than 54 Mbps since the dataset is captured in a controlled environment. Our literature review identifies the arXiv:2411.11894v1  [cs.AI]  7 Nov 2024",
        "pdf_filename": "ResLearn_Transformer-based_Residual_Learning_for_Metaverse_Network_Traffic_Prediction.pdf",
        "num_chunks": 766
    },
    {
        "title": "Restructuring Tractable Probabilistic Circuits",
        "context": "Probabilistic circuits (PCs) is a unifying rep- resentation for probabilistic models that sup- port tractable inference. Numerous applica- tions of PCs like controllable text generation depend on the ability to efficiently multiply two circuits. Existing multiplication algo- rithms require that the circuits respect the same structure, i.e. variable scopes decom- poses according to the same vtree. In this work, we propose and study the task of re- structuring structured(-decomposable) PCs, that is, transforming a structured PC such that it conforms to a target vtree. We pro- pose a generic approach for this problem and show that it leads to novel polynomial- time algorithms for multiplying circuits re- specting different vtrees, as well as a practi- cal depth-reduction algorithm that preserves structured decomposibility. Our work opens up new avenues for tractable PC inference, suggesting the possibility of training with less restrictive PC structures while enabling effi- cient inference by changing their structures at inference time. 1 A key challenge in deep generative modeling is the in- tractability of probabilistic reasoning (Roth, 1996; Geh et al., 2024). To address this challenge, probabilistic circuits (PCs) (Darwiche, 2003; Poon and Domingos, 2011; Choi et al., 2020) has emerged as a unifying representation of tractable generative models, which Preprint. Correspondence to Honghua Zhang [hzhang19@cs.ucla.edu] and Benjie Wang [benjiewang@ucla.edu]. support efficient and exact evaluation of various infer- ence queries like marginalization. The tractability of PCs has now proven crucial in a range of applications, such as causal inference (Zeˇcevi´c et al., 2021; Wang and Kwiatkowska, 2023; Busch et al., 2024), knowl- edge graph learning (Loconte et al., 2023) and ensur- ing fairness in decision making (Choi et al., 2021). Probabilistic circuits represent distributions as com- putation graphs of sums and products. A crucial as- pect to the design of PCs is the structure of the com- putation graph, that is, how distributions are fac- torized into (conditionally) independent components. The structure of PCs affects their tractability, model- ing performance and computational efficiency. In this work, we consider the problem of restructuring PCs: constructing a new PC that follows a particular (tar- get) structure while representing the same distribu- tion. We present a general algorithm for restructuring structured-decomposable circuits by considering their graphical model representations. Specifically, we lever- age the graphical models to reason about conditional independencies and recursively construct a new PC conforming to the desired structure. We then investigate two key applications of PC re- structuring: circuit multiplication and depth reduc- tion. Circuit multiplication is a fundamental opera- tion used for answering various inference queries (Ver- gari et al., 2021), such as conditioning on logical con- straints (Choi et al., 2015; Ahmed et al., 2022; Liu et al., 2024b; Zhang et al., 2023, 2024), computing expected predictions of classifiers (Khosravi et al., 2019) and causal backdoor adjustment (Wang and Kwiatkowska, 2023), as well as in improving the ex- pressive power of circuits through squaring (Loconte et al., 2024c,b; Wang and Van den Broeck, 2024). Though the problem of multiplying circuits of differ- ent structures is in general #P-hard (Vergari et al., 2021), we identify a new class of PCs, which we call contiguous circuits, where it is possible to multiply circuits of different structures in polynomial (or quasi- arXiv:2411.12256v1  [cs.AI]  19 Nov 2024",
        "pdf_filename": "Restructuring_Tractable_Probabilistic_Circuits.pdf",
        "num_chunks": 1499
    },
    {
        "title": "Rethinking cluster-conditioned diffusion models for label-free image synthesis",
        "context": "Diffusion-based image generation models can enhance image quality when conditioned on ground truth labels. Here, we conduct a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We investigate how individual clustering de- terminants, such as the number of clusters and the cluster- ing method, impact image synthesis across three different datasets. Given the optimal number of clusters with respect to image synthesis, we show that cluster-conditioning can achieve state-of-the-art performance, with an FID of 1.67 for CIFAR10 and 2.17 for CIFAR100, along with a strong increase in training sample efficiency. We further propose a novel empirical method to estimate an upper bound for the optimal number of clusters. Unlike existing approaches, we find no significant association between clustering per- formance and the corresponding cluster-conditional FID scores. Code is available at https://github.com/ HHU-MMBS/cedm-official-wavc2025 Diffusion models have enabled significant progress in many visual generative tasks, such as image synthesis [29, 39, 57] and manipulation [80]. Conditioning diffusion models on human-annotated data is today’s standard prac- tice as it significantly improves the image fidelity [8,23,25]. Image-level conditioning is typically realized by using as- sociated text captions [65, 66] or class labels, if available [29, 53]. Nonetheless, human-annotated labels are costly and often contain inaccuracies [7,10], while publicly avail- able image-text pairs can be non-descriptive [9]. Large, human-annotated datasets are prohibitively costly [59] and hard to obtain in a plethora of real-life applications such as medical image synthesis [41]. Human annotation is subject to the label collection procedure and can have vary- Cluster 0 willow_tree oak_tree maple_tree Cluster 1 telephone telephone telephone Cluster 2 telephone telephone telephone Figure 1. An ideal image-level conditioning should group images based on shared patterns, shown in the same row, which do not always align with human labels, indicated above each image (CI- FAR100 [45] samples). ing annotation hierarchies [3, 11, 31]. An ideal condition- ing signal would be based solely on shared characteristics corresponding to general visual concepts, as illustrated in Fig. 1. To provide a ground truth (GT) label-free alternative that is applicable to unlabelled datasets, we focus on cluster- based conditioning on diffusion models. Image clustering refers to algorithmically assigning a semantic group to an image, called a cluster, given an a priori number of such groups [14,18,38]. Maximizing cluster alignment with ground truth (GT) labels does not guarantee optimal cluster-conditional im- age generation. To create visually distinguishable groups (Fig. 1), a clustering algorithm would sometimes group im- ages into more fine-grained groups than the GT labels, i.e., sub-grouping analog and digital devices that map to the arXiv:2403.00570v2  [cs.CV]  19 Nov 2024",
        "pdf_filename": "Rethinking_cluster-conditioned_diffusion_models_for_label-free_image_synthesis.pdf",
        "num_chunks": 1848
    },
    {
        "title": "Rethinking Top Probability from Multi-view for Distracted Driver Behaviour Localization",
        "context": "Naturalistic driving action localization task aims to recognize and comprehend human behaviors and actions from video data captured during real-world driving sce- narios. Previous studies have shown great action localiza- tion performance by applying a recognition model followed by probability-based post-processing. Nevertheless, the probabilities provided by the recognition model frequently contain confused information causing challenge for post- processing. In this work, we adopt an action recognition model based on self-supervise learning to detect distracted activities and give potential action probabilities. Subse- quently, a constraint ensemble strategy takes advantages of multi-camera views to provide robust predictions. Finally, we introduce a conditional post-processing operation to lo- cate distracted behaviours and action temporal boundaries precisely. Experimenting on test set A2, our method obtains the sixth position on the public leaderboard of track 3 of the 2024 AI City Challenge. Distracted driving is defined as any circumstance where the driver diverts attention away from safe driving activi- ties. In the United States, over 3,500 lives are lost annu- ally due to accidents caused by distracted driving. Research in intelligent transportation systems and distracted driving has gained significant attention from scholars worldwide [19, 25, 28, 32]. This interest is fueled by the potential of naturalistic driving videos to capture real-time driving behavior and the capability of deep learning to analyze po- tential risk factors. The AI City Challenge 2024 [3] aims to advance research in this field by hosting a naturalistic driving action recognition challenge. The given challenge focuses on detecting distracted driving behaviors using syn- thetic naturalistic data collected from three camera locations inside the vehicle. This challenge involves analyzing syn- chronized video recordings from drivers engaged in various distracted driving activities. These activities are classified into different actions, such as using a phone, eating, and reaching into the backseat, each of which can potentially lead to accidents. Previous studies [12, 13, 24, 33, 35] have demonstrated the effectiveness in distracted driving detection, typically dividing the task into two main stages: activity recognition and temporal action localization. However, several chal- lenges remain: (1) The dataset is limited to 16 behavior categories, leading to an insufficient diversity of samples within each category. (2) The models must discern vari- ous actions from different perspectives within untrimmed videos, facing difficulties in distinguishing subtle variations within the same class and detecting minor discrepancies be- tween certain classes. (3) The inclusion of the appearance block constrains the model’s ability to discern differences between certain classes. (4) Previous solutions rely heavily on the classification model’s confidence, which can result in misclassifications when the highest and second-highest classes have similar probabilities. Therefore, in this paper, we aim to contribute to the lit- erature in the following manners: First, we inherit an action classification model in video based self-supervised learn- ing to detect robust distracted actions from the input video. Next, we apply a constraint ensemble strategy to take ad- vantage of the power of each camera view. In the final, con- ditional post-processing steps consider contexts from top 1 and top 2 confidence ranking to locate distracted actions and temporal boundaries accurately. arXiv:2411.12525v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "Rethinking_Top_Probability_from_Multi-view_for_Distracted_Driver_Behaviour_Localization.pdf",
        "num_chunks": 1211
    },
    {
        "title": "Retrieval-Augmented Personalization for Multimodal Large Language Models",
        "context": "The development of large language models (LLMs) has sig- nificantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user- specific knowledge still restricts their application in hu- man’s daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs’ personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Re- member: We design a key-value database to store user- related information, e.g., user’s name, avatar and other attributes. (b) Retrieve: When the user initiates a con- versation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts’ information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time ∗Equal contribution † Corresponding author concept editing via updating the external database. To fur- ther improve generation quality and alignment with user- specific information, we design a pipeline for data collec- tion and create a specialized dataset for personalized train- ing of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pre- training on large-scale dataset, RAP-MLLMs can general- ize to infinite visual concepts without additional finetun- ing. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as per- sonalized image captioning, question answering and visual recognition. The code, data and models are available at https://hoar012.github.io/RAP-Project/. Recently, the development of large language models (LLMs) has significantly enhanced their language process- ing and generating capabilities [59]. Building on this foundation, the integration of visual and textual ability 1 arXiv:2410.13360v2  [cs.CV]  18 Nov 2024",
        "pdf_filename": "Retrieval-Augmented_Personalization_for_Multimodal_Large_Language_Models.pdf",
        "num_chunks": 2102
    },
    {
        "title": "Reviving Dormant Memories Investigating Catastrophic Forgetting in Language Models through Rationale",
        "context": "Although substantial efforts have been made to mitigate catastrophic forgetting in contin- ual learning, the intrinsic mechanisms are not well understood. In this paper, we discover that when a forgetting model passively receives an externally provided partial appropriate ratio- nale, its performance on the forgotten task can be restored. Furthermore, by simply adding a task-agnostic prefix to the original instruc- tion, the forgetting model can actively gener- ate an appropriate rationale to reach the cor- rect answer. These findings suggest that the model does not actually “forget” the task knowl- edge; instead, the degraded performance can be attributed to the failure of the original in- structions in guiding the model to generate the appropriate rationales. Based on this insight, we propose the Rationale-Guidance Difficulty metric to evaluate how effectively a given in- struction guides the model in generating ap- propriate rationales. We apply this metric to optimize the allocation of replay data in replay- based continual learning algorithm. Experimen- tal results demonstrate that our data allocation method effectively mitigates catastrophic for- getting and maintains better model plasticity simultaneously across models. 1 While large language models (LLMs) acquire ex- tensive knowledge during pre-training (Brown et al., 2020; Touvron et al., 2023a; Yang et al., 2023), in reality, both knowledge and data are dy- namic, necessitating that models adapt to different tasks or domains continuously (Zheng et al., 2024). Accordingly, continual learning can assist models in acquiring new knowledge incrementally, thereby enhancing their capabilities over time. However, a key challenge models face during continual learn- ing is “catastrophic forgetting,” which refers to the phenomenon where a model’s performance on old * Corresponding author Instruction of Task 1 Knowledge of Task 1 Appropriate Rationale Probing by CoT Original-LLM Correct Answer Train on Task 2 Knowledge of Task 1 Knowledge of Task 2 Instruction of Task 1 Correct Answer Incorrect Answer Drift Appropriate Rationale Irrelevant Rationale ‘Forgetting’-LLM Task- Agnostic prefix passively provided Rationale Add Prefix help the model actively generate Rationale Pseudo-Forgetting Do not Forget Do not Forget Do “Forget” Correct Guidance Incorrect Guidance Exp1 Exp2 Experiments internal mechanism Observation conclusion Whether the prompt can guide the generation of appropriate rationale? Figure 1: Methodology used in our experiments. 1. We leverage CoT to probe the parameterized knowl- edge embedded in the model explicitly. 2. We evalu- ate the performance of a forgetting model under three situations: “instruction-only prompting,” “instruction- only prompting with externally provided rationale,” and “instruction-only prompting with a task-agnostic pre- fix.” 3. We find that in the latter two situations, the model could actively generate appropriate rationale, re- covering task performance. Thus, we conclude that the model does not truly forget the old knowledge; instead, the original instructions are insufficient in guiding the generation of appropriate rationale, resulting in “pseudo- forgetting.” tasks declines after learning new tasks (McCloskey and Cohen, 1989; Goodfellow et al., 2014). Despite the numerous methods proposed to mit- igate catastrophic forgetting (Wang et al., 2024, 2023a; Zhao et al., 2024) (discussed in Section 2.2), few studies have begun to investigate the intrinsic mechanisms underlying this phenomenon. Kotha et al. (2024) propose the “task inference” hy- pothesis, which suggests that fine-tuning a model changes which of its abilities it tends to use, rather than causing it to actually forget those abilities. However, this hypothesis has been primarily vali- dated on synthetic datasets rather than directly on arXiv:2411.11932v1  [cs.LG]  18 Nov 2024",
        "pdf_filename": "Reviving_Dormant_Memories_Investigating_Catastrophic_Forgetting_in_Language_Models_through_Rationale.pdf",
        "num_chunks": 1230
    },
    {
        "title": "RLtools A Fast, Portable Deep Reinforcement Learning Library for Continuous Control",
        "context": "Deep Reinforcement Learning (RL) can yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing libraries. To address these challenges, we present RLtools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Its novel architecture allows RLtools to be used on a wide variety of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simulation environments, RLtools can solve popular RL problems up to 76 times faster than other popular RL frameworks. We also benchmark the inference on a diverse set of microcontrollers and show that in most cases our optimized implementation is by far the fastest. Finally, RLtools enables the first-ever demonstration of training a deep RL algorithm directly on a microcontroller, giving rise to the field of Tiny Reinforcement Learning (TinyRL). The source code as well as documentation and live demos are available through our project page at https://rl.tools. Keywords: Reinforcement Learning, Continuous Control, Deep Learning, TinyRL Continuous control is a ubiquitous and pervasive problem in a diverse set of domains such as robotics, high-frequency decision-making in financial markets or the automation of chemical plants and smart grid infrastructure. Taking advantage of the recent progress in Deep Learning (DL) that is spilling over into decision-making in the form of RL, agents derived using deep RL have already attained impressive performance in a range of decision-making problems, like games and particularly continuous control. Despite these achievements, the ©2024 Jonas Eschmann, Dario Albani, and Giuseppe Loianno. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v25/24-0248.html. arXiv:2306.03530v4  [cs.LG]  19 Nov 2024",
        "pdf_filename": "RLtools_A_Fast,_Portable_Deep_Reinforcement_Learning_Library_for_Continuous_Control.pdf",
        "num_chunks": 756
    },
    {
        "title": "S-HR-VQVAE Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Vi",
        "context": "forth a novel model that combines (i) a novel hierarchical residual learning vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel autoregressive spatiotemporal predictive model (AST-PM). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S- HR-VQVAE). By leveraging the intrinsic capabilities of HR- VQVAE at modeling still images with a parsimonious represen- tation, combined with the AST-PM’s ability to handle spatiotem- poral information, S-HR-VQVAE can better deal with major challenges in video prediction. These include learning spatiotem- poral information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical character- istics. Extensive experimental results on four challenging tasks, namely KTH Human Action, TrafficBJ, Human3.6M, and Kitti, demonstrate that our model compares favorably against state- of-the-art video prediction techniques both in quantitative and qualitative evaluations despite a much smaller model size. Finally, we boost S-HR-VQVAE by proposing a novel training method to jointly estimate the HR-VQVAE and AST-PM parameters. Index Terms—Video Prediction, Hierarchical Modeling, Au- toregressive Modeling Video prediction involves anticipating future video frames based on a sequence of preceding frames [1]. It is a challeng- ing task, requiring algorithms to grasp complex spatiotemporal relationships within the video, at the same time as handling high dimensionality, addressing blurry predictions, and ac- counting for the physical characteristics of the scenes. Spa- tiotemporal modeling aims to capture dependencies in video frame sequences, mirroring human perception of dynamic phe- nomena [2]. This is a general problem in video modeling, but becomes especially challenging when we need to recursively and accurately predict video frames for long temporal spans. Current state-of-the-art methods often struggle with long-term dependencies and complex motion patterns, leading to inaccu- racies in the predicted frames. High dimensionality is inherent in video patterns, leading to the “curse of dimensionality” in function approximation and optimization [3]. Autoencoder- based methods attempt to reduce dimensionality, but may lose important fine-grained details necessary for accurate predic- tion. Blurry predictions stem from statistical models producing fuzzier outputs when predicting uncertain future events. This is, therefore, a more challenging problem for video prediction than for any other video task. Most methods use mean squared error (MSE) objective that tends to average over possible outcomes, resulting in blurred predictions. The challenge of physical characteristics pertains to object and scene attributes affecting prediction. Proper modeling of these characteristics may potentially aid future frame predictions. Recent video prediction methods have made significant progress in tackling these challenges, yet they still face several limitations. We will detail the state-of-the-art with respect to each of these challenges in Section II. This paper introduces a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR- VQVAE), which is tailored for video prediction with the goal of tackling the above-mentioned challenges. To this end, S- HR-VQVAE implements a novel autoregressive spatiotempo- ral predictive model (AST-PM) to capture distributions of dependencies between latent representations across time and space. The latent representations are generated through our novel encoding scheme, termed hierarchical vector quanti- zation variational autoencoder (HR-VQVAE) that we have recently used with success for still image reconstruction [4]. Leveraging those two novel blocks, namely HR-VQVAE, and AST-PM, S-HR-VQVAE effectively tackles the video prediction task in three steps: In the first step, the input video frames are encoded to a continuous latent space and then mapped to discrete representations through HR-VQVAE, with each latent vector, in each layer in the model, assigned to a codeword in a codebook. The key property of this model is the strict hierarchy imposed between codebooks belonging to different layers, producing extremely compact and efficient discrete representations. In the second step, we predict future events in latent rather than image space. To perform this prediction, we use spatiotemporal modeling (the proposed AST-PM), where the distribution of the discrete latent representations for a particular location in the current frame is conditioned on the representations for neighboring locations both in space and time. In the third and final step, the predicted discrete representations are used by the HR-VQVAE arXiv:2307.06701v3  [cs.CV]  19 Nov 2024",
        "pdf_filename": "S-HR-VQVAE_Sequential_Hierarchical_Residual_Learning_Vector_Quantized_Variational_Autoencoder_for_Vi.pdf",
        "num_chunks": 1687
    },
    {
        "title": "Scaling Deep Learning Research with Kubernetes on the NRP Nautilus HyperCluster",
        "context": "Throughout the scientific computing space, deep learning algorithms have shown excellent performance in a wide range of applications. As these deep neural networks (DNNs) continue to mature, the necessary compute required to train them has continued to grow. Today, modern DNNs require millions of FLOPs and days to weeks of training to generate a well-trained model. The training times required for DNNs are oftentimes a bottleneck in DNN research for a variety of deep learning applications, and as such, accelerating and scaling DNN training enables more robust and accelerated research. To that end, in this work, we explore utilizing the NRP Nautilus HyperCluster to automate and scale deep learning model training for three separate applications of DNNs, including overhead object detection, burned area segmentation, and deforestation detection. In total, 234 deep neural models are trained on Nautilus, for a total time of 4,040 hours. Deep convolutional neural networks (DCNNs) have been established as the state of the art in computer vision (CV) and have shown superior performance in visual tasks for many domains, including remote sensing. With billions of pixels being collected by overhead sources like satellites, remote sensing (RS) is becoming evermore a big-data problem domain, with endless amounts of data available to enable CV applications. Due in part to this data availability, the training and optimization of deep networks for RS applications has been explored to great lengths in recent years. In 2017, researchers investigated utilizing DCNNs for land-cover classification in overhead imagery along with techniques such as transfer learning and data augmentation[1]. This work was then extended into multi-network fusion research, where multiple DCNNs trained on overhead satellite imagery were fused using simple fusion techniques such as voting and arrogance [2] and then compared to more complex fusion algorithms such as the Choquet and Sugeno Fuzzy Integral [3], [4]. While these studies explored utilizing DCNNs to perform classification on overhead RS imagery, further exploration was required in broad area search, in which DCNNs are trained and used not on clean pre-processed datasets, but instead applied to large swaths of overhead imagery with the goal of finding all instances of a given object or terrain. In these applications, large swaths of imagery are investigated, usually on the order of tens or hundreds of thousands of square kilometers, if not more. For example, in [5], researchers utilized a DCNN to locate surface-to-air missile sites in the South China Sea. Surface-to-air missile site detection was then improved in [6], in which researchers employed spatial fusion of component detections. In another application, broad area search was also explored in [7], in which researchers utilized DCNNs to search for aircraft and other classes in a 9,500 square kilometer AOI surrounding Beijing. In each of these applications of DCNNs for Broad Area Search, the compute required is a limiting factor in applying these algorithms at scale. Aside from classification and broad area search, DCNNs have also been utilized to perform object detection on overhead RS imagery. In [8], the YOLOv3 architecture was compared with other deep network architectures to detect military vehicle groups, and in [9], researchers used overhead imagery to perform maneuverability hazard detection with multiple DCNN-based object detection methodologies. More recently, as DCNNs have evolved and their operational characteristics have become more clear, researchers have investigated utilizing various techniques to address the shortcomings of DCNNs, including utilizing shape extraction to increase model performance, as in [10], [11]. Throughout CV and RS research, DCNNs are being utilized to enable more real-world applications, however as DCNNs have matured and the number of convolutional layers has grown, so too have the compute requirements to train models. Additionally, as DCNN have grown in popularity, several frameworks have been created to enable better scaling and reproducibility of training deep networks. Among the benefits of these frameworks is the parallelization capabilities built into the training processes, however, these benefits can only be utilized if the appropriate hardware is available. Large numbers of GPUs are required to effectively train networks in a reasonable amount of time and keep batch sizes large enough to minimize the impact of diminishing gradients. Further, CPU parallelization is often used for managing GPU resources in distributed training paradigms as well as for asynchronous data loading and preprocessing during the training process. In this work, we explore utilizing the National Research Platform (NRP) Nautilus HyperCluster to accelerate and scale three separate scientific applications utilizing DCNNs. Nautilus is an ever-growing Kubernetes cluster containing over 1300 NVIDIA GPUs and 19,000 CPU Cores that can be utilized for a variety of applications, including teaching, research prototyping and exploration, and scaling of scientific compute, which we will explore here. The three DCNN applications discussed in this study are all in the overhead RS domain: overhead object detection with visual transformer architectures, burned area segmentation with U-Net, and deforestation detection in the Brazilian Amazon rain forest. Each of these applications requires an intense arXiv:2411.12038v1  [cs.LG]  18 Nov 2024",
        "pdf_filename": "Scaling_Deep_Learning_Research_with_Kubernetes_on_the_NRP_Nautilus_HyperCluster.pdf",
        "num_chunks": 702
    },
    {
        "title": "Scideator Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination",
        "context": "Scientists are continuously brainstorming research ideas on which to work next. A good idea should be relevant to the scientist’s interests and novel within the scientific community. Research papers are a major source of inspiration for relevant and novel ideas, as they expose scientists to relevant concepts to re-combine and form new ideas [4, 21, 36]. However, generating relevant and novel scientific ideas by recombining concepts from research papers is difficult for multiple reasons. For one, scientists must wade through an ever-expanding scientific literature to find relevant concepts [2, 19]. Moreover, the phenomenon of fixation biases scientists against considering more diverse concepts and concept recombinations for their research; instead, they are predisposed to thinking about a problem in familiar terms, which hinders the stimulation of novel ideas [11, 37]. Even if a scientist manages to identify interesting concept recombinations to form potential research ideas, assessing the ideas’ novelty in comparison to the existing literature is a cumbersome yet critical task. Building a fully or semi-automated ideation system has been an ambition of researchers for decades, and Scideatorbuilds on strong prior work from many other researchers, filling a unique niche. We extend a line of work that presents systems for finding analogies between research papers [4, 21, 36], adopting their facet-based framework but using modern large language model (LLM) methods to identify relevant facets and perform facet recombinations. We are also inspired by recent work showing that LLMs have promise to assist ideation in domains outside science, helping people to generate more ideas [6] and more diverse ideas [27, 40]. While some of this LLM-based work employs facet ∗Made large contribution. Authors’ addresses: Marissa Radensky, radensky@cs.washington.edu, University of Washington, USA; Simra Shahid, Adobe, India; Raymond Fok, University of Washington, USA; Pao Siangliulue, Allen Institute for AI, USA; Tom Hope†, tomh@allenai.org; Daniel S. Weld†, danw@allenai.org, Allen Institute for AI, USA † Equal Advisors. 1 arXiv:2409.14634v2  [cs.HC]  18 Nov 2024",
        "pdf_filename": "Scideator_Human-LLM_Scientific_Idea_Generation_Grounded_in_Research-Paper_Facet_Recombination.pdf",
        "num_chunks": 2793
    },
    {
        "title": "Shared Model of Sense-making for Human-Machine Collaboration",
        "context": "We present a model of sense-making that greatly facilitates the collaboration between an intelligent analyst and a knowledge-based agent. It is a general model grounded in the science of evidence and the scientific method of hypothesis generation and testing, where sense-making hypotheses that explain an observation are generated, relevant evidence is then discovered, and the hypotheses are tested based on the discovered evidence. We illustrate how the model enables an analyst to directly instruct the agent to understand situations involving the possible production of weapons (e.g., chemical warfare agents) and how the agent becomes increasingly more competent in understanding other situations from that domain (e.g., possible production of centrifuge-enriched uranium or of stealth fighter aircraft). Sense-making is the intelligence analysis process of situation understanding, prediction of the behavior and intent of the entities of interest, and identifying the threats as early as possible, in the context of a dynamic world, based on data that is sparse, noisy, and uncertain (Moore, 2011). The prevailing approach to sense-making in intelligence analysis is the holistic approach where the analysts, after reviewing large amounts of information and performing the reasoning in their heads, reach a conclusion (Marrin, 2011). A complementary approach uses very simple structured analytic techniques, such as those described by Heuer and Pherson (2011), that provide general guidelines for hypothesis generation and testing. Most of the time sense- making is the result of shallow arguments using the Toulmin intuitive model (Toulmin 1963; van Gelder, 2007), where each claim is backed by evidence. There is no systematic process to determine the probabilities of hypotheses based on the available evidence (Pherson and Pherson, 2021). More advanced methods build Bayesian probabilistic inference networks using analytical tools, such as Netica (2019), but modeling a situation with a Bayesian network is a very complex task for an intelligence analyst. This paper presents a more advanced system for sense- making in intelligence analysis, the Multi-Agent System for Sensemaking through Hypothesis Generation and Analysis (MASH). MASH builds on a series of analytical tools that includes Disciple-LTA (Tecuci et al., 2008; Schum et al., 2009), TIACRITIS (Tecuci et al., 2011), Disciple-CD (Tecuci et al., 2016a) and Cogent (Tecuci et al., 2015; 2018). MASH also builds on the Disciple multistrategy apprenticeship learning approach (Boicu et al., 2001; Tecuci, 1988; 1998; Tecuci and Hieb, 1996; Tecuci et al., 2000; 2002; 2005; 2007a; 2019; Huang et al., 2020). Shared Model of Sense-making Figure 1 is an overview of a human-machine shared model of sense-making that facilitates the synergistic integration of the analyst’s imagination and expertise with the computer’s domain knowledge and formal reasoning. It is a general model grounded in the science of evidence (Schum, 2009) and the scientific method of hypothesis generation and testing. Evidence is any observable sign, datum, or item of information that is relevant in deciding whether a hypothesis is true or false (Schum, 2009). The sense-making model consists of three recursive collaborative processes: Evidence in search of hypotheses; Hypotheses in search of evidence; and Evidentiary assessment of hypotheses. The sense-making process starts with an “alerting observation” that may indicate an event of interest. Through abductive (imaginative) reasoning which shows that something is possibly true, the analyst and MASH generate competing hypotheses that may explain the observation (Peirce, 1955; Eco, 1983; Schum, 2001a; Langley, 2019). To determine which of these competing hypotheses is true, they use each hypothesis and deductive reasoning which shows that something is necessarily true, to discover new evidence. The question is: What evidence would need to be observed if this hypothesis were true? The reasoning might go as follows: If H were true then the sub-hypotheses H1, H2, and H3 would also need to be true. But if H1 were true then one would need to observe evidence E1, and so on. A broader question that guides the discovery of evidence is, What evidence would favor or disfavor hypothesis H? The decomposition of H is done through a sequence of  Figure 1: Human-machine shared model of sense-making. Probability of Hypotheses New Evidence Evidence in search of hypotheses Abduction E possibly H Evidentiary assessment of hypotheses Induction E probably H Hypotheses in search of evidence Deduction H necessarily E Competing Hypotheses Alerting Observation What is the probability of each hypothesis? What evidence would favor or disfavor this hypothesis? What hypotheses would explain this observation?",
        "pdf_filename": "Shared Model of Sense-making for Human-Machine Collaboration.pdf",
        "num_chunks": 839
    },
    {
        "title": "ShiftAddLLM Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization",
        "context": "Large language models (LLMs) have shown impressive performance on language tasks but face challenges when deployed on resource-constrained devices due to their extensive parameters and reliance on dense multiplications, resulting in high memory demands and latency bottlenecks. Shift-and-add reparameterization offers a promising solution by replacing costly multiplications with hardware-friendly primitives in both the attention and multi-layer perceptron (MLP) layers of an LLM. However, current reparameterization techniques require training from scratch or full parameter fine-tuning to restore accuracy, which is resource-intensive for LLMs. To address this, we propose accelerating pretrained LLMs through post-training shift-and-add reparameterization, creating efficient multiplication-free models, dubbed ShiftAddLLM. Specifically, we quantize each weight matrix into binary matrices paired with group-wise scaling factors. The associated multiplications are reparameterized into (1) shifts between activations and scaling factors and (2) queries and adds according to the binary matrices. To reduce accuracy loss, we present a multi-objective optimization method to minimize both weight and output activation reparameterization errors. Additionally, based on varying sensitivity across layers to reparameterization, we develop an automated bit allocation strategy to further reduce memory usage and latency. Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity reductions of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3- and 2-bit precision, respectively, and more than 80% memory and energy reductions over the original LLMs. Codes and models are available at https://github.com/GATECH-EIC/ ShiftAddLLM. 1 Pretrained LLMs have demonstrated state-of-the-art performance in language understanding and generation tasks [46, 47, 59, 3, 74, 57, 58, 2]. However, deploying these LLMs incurs significant hardware demands, including high latency, memory, and energy consumption, especially on edge or cloud GPU devices. The primary bottlenecks are their immense parameter sizes and the associated multiplication operations. For instance, GPT-3, with 175 billion parameters, requires 350GB of memory in FP16 format [38] and performs 1015 floating-point operations (FLOPs) for a single forward pass [19]. Previous efforts to improve LLM efficiency have focused on pruning [40, 55, 20, 24, 44], quantization [63, 38, 18, 48], and attention optimization [12, 71, 67]. However, these methods still rely on costly multiplication operations in both the attention and MLP layers. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2406.05981v4  [cs.LG]  18 Nov 2024",
        "pdf_filename": "ShiftAddLLM_Accelerating_Pretrained_LLMs_via_Post-Training_Multiplication-Less_Reparameterization.pdf",
        "num_chunks": 2466
    },
    {
        "title": "Signaling and Social Learning in Swarms of Robots",
        "context": "This paper investigates the role of communication in improving coordination within robot swarms, focusing on a paradigm where learning and execution occur simultaneously in a decentralized manner. We highlight the role communication can play in addressing the credit assignment problem (individual contribution to the overall performance), and how it can be influenced by it. We propose a taxonomy of existing and future works on from low-level lossless compression with raw signal extraction and processing to high-level lossy compression with structured communication models. The paper reviews current research from evolutionary robotics, multi- agent (deep) reinforcement learning, language models, and biophysics models to outline the challenges and opportunities of communication in a collective of robots that continuously learn from one another through local message exchanges, illustrating a form of social learning. Keywords signaling, communication, social learning, swarm robotics, decentralized learning and execution, cooperation, evolutionary dynamics, multi-robot systems 1Universit´e Paris Cit´e, CNRS, LIED UMR 8236, F-75006 Paris, France 2ECE Paris, France 3Sorbonne Universit´e, CNRS, ISIR, F-75005 Paris, France 4Sorbonne Universit´e, CNRS, IBPS, Laboratoire Jean Perrin, F-75005 Paris, France 5Department of Information Sciences, Ochanomizu University, Tokyo, Japan 6LIMMS (IRL2820)/CNRS-IIS, University of Tokyo, Tokyo, Japan *Corresponding author: nicolas.bredeche@sorbonne-universite.fr Contents 1 1 2 Dynamics of Decentralized Learning and Execution3 3 A Taxonomy of Signaling Methods in Swarm Robotics 4 4 Current Trends in Signaling for Swarm Robotics 6 4.1 Low Degree of Information Selection . . . . . . . . . 6 4.2 High Degree of Information Selection . . . . . . . . 8 5 Conclusion 10 References 10",
        "pdf_filename": "Signaling_and_Social_Learning_in_Swarms_of_Robots.pdf",
        "num_chunks": 1686
    },
    {
        "title": "SkillTree Explainable Skill-Based Deep Reinforcement Learning for Long-Horizon Control Tasks",
        "context": "Deep reinforcement learning (DRL) has achieved remark- able success in various domains, yet its reliance on neural networks results in a lack of transparency, which limits its practical applications in safety-critical and human-agent in- teraction domains. Decision trees, known for their notable ex- plainability, have emerged as a promising alternative to neu- ral networks. However, decision trees often struggle in long- horizon continuous control tasks with high-dimensional ob- servation space due to their limited expressiveness. To ad- dress this challenge, we propose SkillTree, a novel hierar- chical framework that reduces the complex continuous action space of challenging control tasks into discrete skill space. By integrating the differentiable decision tree within the high- level policy, SkillTree generates diecrete skill embeddings that guide low-level policy execution. Furthermore, through distillation, we obtain a simplified decision tree model that improves performance while further reducing complexity. Experiment results validate SkillTree’s effectiveness across various robotic manipulation tasks, providing clear skill-level insights into the decision-making process. The proposed ap- proach not only achieves performance comparable to neu- ral network based methods in complex long-horizon control tasks but also significantly enhances the transparency and ex- plainability of the decision-making process. Deep Reinforcement learning (DRL) has been shown as a powerful framework for tackling complex decision-making tasks, achieving remarkable success in various domains such as games (Mnih et al. 2015; BAAI 2023), robotic manip- ulation (Akkaya et al. 2019; Jitosho et al. 2023), and vi- sual navigation (Kulh´anek, Derner, and Babuˇska 2021). De- spite these advancements, the black-box nature of neural networks poses significant challenges in understanding and trusting their decision making processes. This lack of trans- parency is particularly concerning in safety-sensitive and human-agent interaction applications, where understanding the rationale behind decisions is essential (Hickling et al. 2023). For example, in the autonomous driving domain, only *Corresponding author. AAAI 2025 Conference Submission if human users can understand the driving policies can they trustfully use them in their cars. Explainable reinforcement learning (XRL) (Heuillet, Couthouis, and D´ıaz-Rodr´ıguez 2021; Hickling et al. 2023; Milani et al. 2024) aims to enhance the transparency and ex- plainability of DRL models. XRL methods can be broadly categorized into inherently explainable models, where ex- plainability is induced in the model training process, and post-hoc explanations, where models are explained after training. Existing works have shown that using a decision tree (DT) (Costa and Pedreira 2023) as the underlying model is effective (Frosst and Hinton 2017; Bastani, Pu, and Solar- Lezama 2018; Ding et al. 2020; Vasi´c et al. 2022). In a DT, observations are input at the root node, and decisions are de- termined by conditional branches leading to the leaf nodes, which provide the final outputs. This simple structure gives DTs high explainability, offering clear and straightforward decision paths. Unlike post-hoc methods that rely on ex- ternal algorithms to interpret black-box models, DT-based methods embed transparency directly within the model. The DT-based XRL paradigm not only enhances clarity but also simplifies debugging and validation, as the rationale behind each decision is explicitly encoded within the tree. Despite their advantages, DT-based methods are not suit- able for the following challenging task settings. (a) Long- horizon tasks: The temporally-extended decision processes require large and complex trees, which are difficult to opti- mize (Liu et al. 2021). (b) High-dimensional state spaces: DTs often lack sufficient representational ability to effec- tively manage high-dimensional state spaces, leading to sub- optimal performance in these environments (Bastani, Pu, and Solar-Lezama 2018; Ding et al. 2020). (c) Continuous action spaces: The limited number of leaf nodes constrains DTs’ ability to encode continuous control policies optimally, particularly in complex robotic tasks. These limitations re- strict the applicability of DT-based RL methods in complex environments, highlighting the need for approaches that can manage high-dimensional spaces and long-horizon tasks. To address the limitations of traditional DTs in han- dling long-horizon, high-dimensional, and continuous con- trol tasks, we propose a novel framework called SkillTree. SkillTree introduces the concept of skills, which represent arXiv:2411.12173v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "SkillTree_Explainable_Skill-Based_Deep_Reinforcement_Learning_for_Long-Horizon_Control_Tasks.pdf",
        "num_chunks": 1098
    },
    {
        "title": "SNN-Based Online Learning of Concepts and Action Laws in an Open World",
        "context": "We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural network (SNN) implementing the agent’s semantic memory. The agent explores its universe and learns concepts of objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action concepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent’s knowledge of its universe’s actions laws. Both kinds of concepts have different degrees of generality. To make decisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the action to take on the basis of these predictions. Our experiments show that the agent handles new situations by appealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes. 1. The ability of a cognitive agent to act adequately in a given environment depends on its ability to predict how performing a given activity might affect its cur- rent situation, that is, it depends on its knowledge of its environment’s action laws. How artificial agents can acquire these laws and how these should be up- dated if their environment changes has proven a dif- ficult question. In the case where the intended envi- ronment is open—that is, where the agent’s designer cannot foresee all the situations the agent might en- counter in the future—, providing a suitable set of ac- tion laws to the agent “by hand” is unfeasible. The only viable solution is that the agent continuously learns the relevant laws from experience, just as nat- ural agents (humans and animals) do. Crucially, this learning process should allow for generalization over disparate experiences, so that the agent is able to be- have appropriately in new situations. It should also accommodate environment changes. The present paper intends to show how this could be done. Its main thrust is that natural agents’ abil- ity to perform well in our open and changing world relies on the fact that they store their knowledge in the form of concepts, which can have various de- grees of generality. Presumably, they first form con- cepts about the encountered objects and situations, and then use these as building blocks for relational concepts, among which are concepts of actions sup- porting their knowledge of their environment’s action laws. We suggest that artificial agents could do just the same, relying on some artificial neural network to learn and store concepts, and then querying it to make predictions about the outcome of envisaged actions. To test this idea, we here build an artificial hybrid agent with a SNN at its core. We make this agent live in a very simple virtual world, composed of rooms which may be, or not, accessible (hence, knowable) to it. At first, the agent is confined to one single room and learns by itself how to act in it according to its own interests. Then at some point a door opens to a new room, containing some never encountered before objects and situations. Yet, although these are new to the agent, some general laws are preserved from one room to the other. We show that having learned these laws in the first room allows the agent to act by and large properly in the second one, as soon as it enters it. We also show that relying on neurally implemented concepts allows the agent to rapidly update its knowl- edge and adapt to environment changes. The paper is organized as follows. In Section 2 we discuss related work, and in Section 3 we present the agent and its universe. Section 4 clarifies the notions of concepts and actions laws we use, while Section 5 describes the neural network and its functioning. Sec- tion 6 describes the agent’s general functioning and Section 7 presents the results. Finally, Section 8 con- cludes and discusses future possible developments of the framework. 2. Related Works Our research problem is autonomous online learning, generalization and updating of concepts and actions laws in an open universe. The intended application is reasoning and planning for autonomous robots. To our knowledge, no existing approach addresses this problem in all its dimensions, even though these are investigated in separate research fields. 1 arXiv:2411.12308v1  [cs.AI]  19 Nov 2024",
        "pdf_filename": "SNN-Based_Online_Learning_of_Concepts_and_Action_Laws_in_an_Open_World.pdf",
        "num_chunks": 1387
    },
    {
        "title": "Solving Generalized Grouping Problems in Cellular Manufacturing Systems Using a Network Flow Model",
        "context": "",
        "pdf_filename": "Solving_Generalized_Grouping_Problems_in_Cellular_Manufacturing_Systems_Using_a_Network_Flow_Model.pdf",
        "num_chunks": 2604
    },
    {
        "title": "Sparse Polynomial Optimization - Theory and Practice",
        "context": "",
        "pdf_filename": "Sparse Polynomial Optimization - Theory and Practice.pdf",
        "num_chunks": 12045
    },
    {
        "title": "SpikingNeRF Making Bio-inspired Neural Networks See through the Real World",
        "context": "In this paper, we propose SpikingNeRF, which aligns the tem- poral dimension of spiking neural networks (SNNs) with the radiance rays, to seamlessly accommodate SNNs to the re- construction of neural radiance fields (NeRF). Thus, the com- putation turns into a spike-based, multiplication-free man- ner, reducing energy consumption and making high-quality 3D rendering, for the first time, accessible to neuromorphic hardware. In SpikingNeRF, each sampled point on the ray is matched to a particular time step and represented in a hy- brid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked out for faster training and inference. However, this masking operation also incurs irregular tem- poral length, making it intractable for hardware processors, e.g., GPUs, to conduct parallel training. To address this prob- lem, we develop the temporal padding strategy to tackle the masked samples to maintain regular temporal length, i.e., reg- ular tensors, and further propose the temporal condensing strategy to form a denser data structure for hardware-friendly computation. Experiments on various datasets demonstrate that our method can reduce energy consumption by an aver- age of 70.79% and obtain comparable synthesis quality with the ANN baseline. Verification on the neuromorphic hard- ware accelerator also shows that SpikingNeRF can further benefit from neuromorphic computing over the ANN base- lines on energy efficiency. Codes and the appendix are in https://github.com/Ikarosy/SpikingNeRF-of-CASIA. Spiking neural networks (SNNs) are considered the third generation of neural networks, and their bionic modeling encourages much research attention to explore the prospec- tive bio-plausible intelligence that features multitasking and extreme energy efficiency as the human brain does (Maass 1997; Roy, Jaiswal, and Panda 2019). While much dedica- tion has been devoted to SNN research, the gap between the expectation of SNN boosting a wider range of intelli- gent tasks and the fact of artificial neural networks (ANNs) dominating deep learning in the majority of tasks still exists. Recently, more research interests have been invested in narrowing the gap and have promoted notable milestones in various tasks, including image classification (Zhou et al. *Equal contribution. †Corresponding author. Synthetic-NSVF BlendedMVS Synthetic-NeRF NSVF NeRF Mip-NeRF DIVeR TensoRF SpiNeRF-T DVGO SpiNeRF-D Figure 1: Comparisons of our SpikingNeRF with other NeRF-based works in synthesis quality and model rendering energy. Different colors represent different works, and our SpikingNeRF with two different frameworks are denoted in red and violet, respectively. A detailed notation explana- tion is specified in the Experiments section. Different testing datasets are denoted by different shapes. 2022), object detection (Zhang et al. 2022), graph prediction (Zhu et al. 2022b), natural language processing (Zhu, Zhao, and Eshraghian 2023), etc. Besides multi-task supporting, SNN research is also thriving in performance lifting and en- ergy efficiency exploration at the same time. However, we have not yet witnessed the establishment of SNN in the real 3D reconstruction task with advanced performance. Let alone enabling the high-quality real 3D rendering on neuromorphic hardwares, e.g., Loihi (Davies et al. 2018), PTB (Lee, Zhang, and Li 2022), and SpikeSim (Moitra et al. 2023), where neuromorphic computing can essentially acquire low-energy consumption. Meanwhile, high-quality real 3D reconstruction, specifically for NeRF (Mildenhall et al. 2021), has suffered huge computation overhead during rendering, consuming a significant magni- tude of energy (Garbin et al. 2021). Naturally, this raises a question: could bio-inspired spiking neural networks recon- struct the real 3D scene with advanced quality at low en- ergy consumption? This paper investigates the rendering of neural radiance fields with a spiking approach to answer the question. We propose SpikingNeRF to reconstruct volumetric scene arXiv:2309.10987v4  [cs.NE]  19 Nov 2024",
        "pdf_filename": "SpikingNeRF_Making_Bio-inspired_Neural_Networks_See_through_the_Real_World.pdf",
        "num_chunks": 1389
    },
    {
        "title": "SSEditor Controllable Mask-to-Scene Generation with Diffusion Model",
        "context": "Recent advancements in 3D diffusion-based semantic scene generation have gained attention. However, existing meth- ods rely on unconditional generation and require multi- ple resampling steps when editing scenes, which signifi- cantly limits their controllability and flexibility. To this end, we propose SSEditor, a controllable Semantic Scene Ed- itor that can generate specified target categories without multiple-step resampling. SSEditor employs a two-stage diffusion-based framework: (1) a 3D scene autoencoder is trained to obtain latent triplane features, and (2) a mask- conditional diffusion model is trained for customizable 3D semantic scene generation. In the second stage, we in- troduce a geometric-semantic fusion module that enhance the model’s ability to learn geometric and semantic infor- mation. This ensures that objects are generated with cor- rect positions, sizes, and categories. Extensive experiments on SemanticKITTI and CarlaSC demonstrate that SSEditor outperforms previous approaches in terms of controllability and flexibility in target generation, as well as the quality of semantic scene generation and reconstruction. More impor- tantly, experiments on the unseen Occ-3D Waymo dataset show that SSEditor is capable of generating novel urban scenes, enabling the rapid construction of 3D scenes. In recent years, 3D diffusion models have made notable achievements in generating both indoor [13, 32, 40] and outdoor [15, 16, 19, 26, 35] environments, as well as a sin- gle object [14, 31, 43]. Compared to indoor scenes and individual objects, outdoor scenes present more challenges due to their sparser and more complex representations. For 1 arXiv:2411.12290v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "SSEditor_Controllable_Mask-to-Scene_Generation_with_Diffusion_Model.pdf",
        "num_chunks": 1126
    },
    {
        "title": "STREAM A Universal State-Space Model for Sparse Geometric Data",
        "context": "Handling sparse and unstructured geometric data, such as point clouds or event-based vision, is a pressing challenge in the field of machine vision. Recently, sequence models such as Transformers and state-space models entered the domain of geometric data. These methods require special- ized preprocessing to create a sequential view of a set of points. Furthermore, prior works involving sequence mod- els iterate geometric data with either uniform or learned step sizes, implicitly relying on the model to infer the un- derlying geometric structure. In this work, we propose to encode geometric structure explicitly into the parameteriza- tion of a state-space model. State-space models are based on linear dynamics governed by a one-dimensional vari- able such as time or a spatial coordinate. We exploit this dynamic variable to inject relative differences of coordi- nates into the step size of the state-space model. The re- sulting geometric operation computes interactions between all pairs of N points in O (N) steps. Our model deploys the Mamba selective state-space model with a modified CUDA kernel to efficiently map sparse geometric data to modern hardware. The resulting sequence model, which we call STREAM, achieves competitive results on a range of bench- marks from point-cloud classification to event-based vision and audio classification. STREAM demonstrates a pow- erful inductive bias for sparse geometric data by improv- ing the PointMamba baseline when trained from scratch on *These authors contributed equally to this work. the ModelNet40 and ScanObjectNN point cloud analysis datasets. It further achieves, for the first time, 100 % test accuracy on all 11 classes of the DVS128 Gestures dataset. Computer vision computes relationships between data points in spatial coordinates X, Y (in R2) or X, Y , Z (in R3), or spatio-temporal coordinates (R3 or R4), where one of the dimensions denotes time t. Convolutional neural net- works based on structured, uniformly spaced and local lin- ear operations successfully address this problem for classi- cal camera recordings such as images or videos, which are themselves structured and discrete. Many modalities of re- cent interest, however, are neither structured nor uniformly spaced. Sensors such as Light Detection and Ranging (Li- DAR) or event-based cameras [21, 28] sample signals based on sparse processes, resulting in sparse geometric data with irregularly spaced coordinates. Point cloud analysis was the central research field for sparse geometric data over the past decade [13]. While early works followed voxel- based approaches [25, 50], point-based methods dominate the research landscape today [4, 5, 46, 48]. Meanwhile, event-based cameras [21, 28] raised considerable interest in the computer vision community. Although these cam- eras record sparse geometric data, most works collapse the sparse stream of events into frames [28, 56, 57], potentially losing unique properties of these cameras such as low la- tency and high dynamic range. The structural similarity be- 1 arXiv:2411.12603v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "STREAM_A_Universal_State-Space_Model_for_Sparse_Geometric_Data.pdf",
        "num_chunks": 1474
    },
    {
        "title": "Structured Low-Rank Algorithms - Theory, MR Applications, and Links to  Machine Learning",
        "context": "In this survey, we provide a detailed review of recent advances in the recovery of continuous domain multidimen- sional signals from their few non-uniform (multichannel) measurements using structured low-rank matrix completion formulation. This framework is centered on the fundamental duality between the compactness (e.g., sparsity) of the continuous signal and the rank of a structured matrix, whose entries are functions of the signal. This property enables the reformulation of the signal recovery as a low-rank structured matrix completion, which comes with performance guarantees. We will also review fast algorithms that are comparable in complexity to current compressed sensing methods, which enables the application of the framework to large-scale magnetic resonance (MR) recovery problems. The remarkable ﬂexibility of the formulation can be used to exploit signal properties that are difﬁcult to capture by current sparse and low-rank optimization strategies. We demonstrate the utility of the framework in a wide range of MR imaging (MRI) applications, including highly accelerated imaging, calibration-free acquisition, MR artifact correction, and ungated dynamic MRI. Index Terms Compressed sensing, matrix completion, accelerated MRI, structured low-rank matrix The slow nature of signal acquisition in magnetic resonance imaging (MRI), where the image is formed from a sequence of Fourier samples, often restricts the achievable spatial and temporal resolution in multi-dimensional static and dynamic imaging applications. Discrete compressed sensing (CS) methods provided a major breakthrough to accelerate the magnetic resonance (MR) signal acquisition by reducing the sampling burden. As described in an introductory article in this special issue [1] these algorithms exploited the sparsity of the discrete signal in a transform domain to recover the images from a few measurements. In this paper, we review a continuous domain extension of CS using a structured low-rank (SLR) framework for the recovery of an image or a series of images from a few measurements using various compactness assumptions [2]–[22]. The general strategy of the SLR framework starts with deﬁning a lifting operation to construct a structured matrix, whose entries are functions of the signal samples. The SLR algorithms exploit the dual relationships between the signal compactness properties (e.g. sparsity, smoothness) and the rank of the lifted matrix. This dual relationship allows recovery of the signal from a few samples in the measurement domain as an SLR optimization problem. MJ and MM are with the University of Iowa, Iowa City, IA 52242 (e-mails: mathews-jacob@uiowa.edu,merry-mani@uiowa.edu). JCY is with the Department of Bio and Brain Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon 34141, Republic of Korea (e-mail: jong.ye@kaist.ac.kr). Corresponding author: Jong Chul Ye, Dept. of Bio and Brain Engineering, Korea Advanced Inst. of Science & Technology (KAIST) 291 Daehak-ro, Yuseong-gu, Daejeon 34141 Republic of Korea, Email: jong.ye@kaist.ac.kr October 12, 2021 DRAFT arXiv:1910.12162v1  [cs.CV]  27 Oct 2019",
        "pdf_filename": "Structured Low-Rank Algorithms - Theory, MR Applications, and Links to  Machine Learning.pdf",
        "num_chunks": 1341
    },
    {
        "title": "Structured Multi-Track Accompaniment Arrangement via Style Prior Modelling",
        "context": "In the realm of music AI, arranging rich and structured multi-track accompani- ments from a simple lead sheet presents significant challenges. Such challenges include maintaining track cohesion, ensuring long-term coherence, and optimizing computational efficiency. In this paper, we introduce a novel system that leverages prior modelling over disentangled style factors to address these challenges. Our method presents a two-stage process: initially, a piano arrangement is derived from the lead sheet by retrieving piano texture styles; subsequently, a multi-track orchestration is generated by infusing orchestral function styles into the piano arrangement. Our key design is the use of vector quantization and a unique multi- stream Transformer to model the long-term flow of the orchestration style, which enables flexible, controllable, and structured music generation. Experiments show that by factorizing the arrangement task into interpretable sub-stages, our approach enhances generative capacity while improving efficiency. Additionally, our system supports a variety of music genres and provides style control at different compo- sition hierarchies. We further show that our system achieves superior coherence, structure, and overall arrangement quality compared to existing baselines. 1 Representation learning techniques have enabled new possibilities for controllable generative mod- elling. By learning implicit style representations, which are often hard to explicitly label (e.g., timbre of music audio [21], texture of music composition [39], and artistic style in paintings [20]), new music and artworks can be created via style transfer and latent space sampling. These learned style factors can also serve as external controls for downstream generative models, including Transform- ers [18, 36] and diffusion models [42]. HowNever, applying style factors to long-term sequence generation remains a challenging task. Existing approaches rely on style templates specified manually or by heuristic rules [36, 42, 51], which are impractical for long-term generation. Moreover, when structural constraints are imposed, misaligned style factors can result in incoherent outputs. To address these challenges, we aim to develop a novel sequence generation framework leveraging a global style planner, or prior, which models the conditional distribution of style factors given the model input’s content factors. Both style and content factors are sequences of compact, structurally aligned latent codes over a disentangled representation space. By infusing the style back to the content, we can recover the observational target with globally coherent style patterns. In this paper, we study style prior modelling through the task of multi-track accompaniment arrange- ment, a typical scenario for long-term conditional sequence generation. We assume the input of a 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2310.16334v3  [cs.SD]  19 Nov 2024",
        "pdf_filename": "Structured_Multi-Track_Accompaniment_Arrangement_via_Style_Prior_Modelling.pdf",
        "num_chunks": 6079
    },
    {
        "title": "Sufficient Invariant Learning for Distribution Shift",
        "context": "Learning robust models under distribution shifts between training and test datasets is a fundamental challenge in machine learning. While learning invariant features across environments is a popular approach, it often assumes that these features are fully observed in both training and test sets—a condition frequently violated in practice. When models rely on invariant features absent in the test set, their robustness in new environments can deteriorate. To tackle this problem, we in- troduce a novel learning principle called the Sufficient Invariant Learning (SIL) framework, which focuses on learning a sufficient subset of invariant features rather than relying on a single feature. After demonstrating the limitation of existing in- variant learning methods, we propose a new algorithm, Adaptive Sharpness-aware Group Distributionally Robust Optimization (ASGDRO), to learn diverse invariant features by seeking common flat minima across the environments. We theoretically demonstrate that finding a common flat minima enables robust predictions based on diverse invariant features. Empirical evaluations on multiple datasets, including our new benchmark, confirm ASGDRO’s robustness against distribution shifts, highlighting the limitations of existing methods. 1 Figure 1: Left visualizes the images that contain a spurious feature, ZNI, and multiple invariant fea- tures, ZTail, ZBeak, and ZFeet in training environment Etr. If the model focuses on the ZNI (green back- ground), then it fails to predict correctly in the test environment E\\Etr (Right). Even if the model cap- tures the invariant features in Etr, e.g., ZFeet, it still fails to predict correctly when the invariant features are not present (Gray). However, it is possible to predict correctly if we learn diverse invariant fea- tures sufficiently, ZFeet, ZTail, and ZBeak. With SIL (Red), the model predicts the label using remaining invariant features, ZTail and ZBeak even though ZFeet is not present in the test environment E\\Etr. Machine learning models typically assume that train- ing and test data are drawn from the same distribu- tion. However, in real-world scenarios, this assump- tion is often violated whenever the training and test distribution differ, known as distribution shifts. In these cases, model performance tends to degrade, highlighting the need to develop models that are robust to distribution shifts for reliable outcomes. To train models robust to distribution shift, invari- ant learning focuses on identifying latent features that remain constant across environments, referred to as invariant features. These features enable con- sistent predictions across environments by discour- aging models from relying on spurious features (Ar- jovsky et al., 2019) – features that are not preserved across changes in environments or groups1. For ex- ample, in domain generalization tasks (Koh et al., 2021; Gulrajani and Lopez-Paz, 2020), the goal is to learn invariant features that consistently predict 1In this paper, the terms environment and domain are used interchangeably. A group refers to a subpopulation corresponding to a particular label within a specific environment. arXiv:2210.13533v3  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Sufficient_Invariant_Learning_for_Distribution_Shift.pdf",
        "num_chunks": 2250
    },
    {
        "title": "Survey on Emotion Recognition through Posture Detection and the possibility of its application in Vi",
        "context": "A survey is presented focused on using pose estimation techniques in Emotional recognition using various technologies normal cameras, and depth cameras for real-time, and the potential use of VR and inputs including images, videos, and 3-dimensional poses described in vector space. We discussed 19 research papers collected from selected journals and databases highlighting their methodology, classification algorithm, and the used datasets that relate to emotion recognition and pose estimation. A benchmark has been made according to their accuracy as it was the most common performance measurement metric used. We concluded that the multimodal Approaches overall made the best accuracy and then we mentioned futuristic concerns that can improve the development of this research topic. Emotion recognition is one of the main vital tasks essential for having an intelligent system or application. Dealing with humans requires understanding their own emotions so that the human feels comfortable and the communication becomes more spontaneous which reflects on the efficiency of the service provided by the system/application. Emotions can be measured from multiple modalities like reading facial expressions, gesture detection, static posture, movement behavior, vocal tones, and text. When interacting with another human, you might know his current emotions from only seeing his face and sometimes the eyes can do the trick, or from his vocal tone, his posture - the way he is standing- or from the pattern of his movements, the gestures he is making or the context of his words whether those words are said or written - you can read an article and still visualize the emotions the writer has been through- or you can combine two or more modalities together which increases the efficiency of the human’s predictions. Computer models are being trained to recognize the above models far above is the physical measurement which may include using sensors and actuators to measure physiological patterns that are hard for the computer to measure like measuring the heart rate, body temperature, and skin sensitivity(Picard, R.W. and Vyzas, E. and Healey, J., n.d.). Those extra modalities shall prepare the computer to be able to measure emotions accurately even more than humans, which is not currently reached. We will discuss the challenges being faced in this field and how some papers overcome those challenges. Some modalities can provide reliable measurements on their own or they may be used only to enhance the recognition of another 1",
        "pdf_filename": "Survey_on_Emotion_Recognition_through_Posture_Detection_and_the_possibility_of_its_application_in_Vi.pdf",
        "num_chunks": 232
    },
    {
        "title": "Survey on Semantic Interpretation of Tabular Data Challenges and Directions",
        "context": "Tabular data plays a pivotal role in various fields, making it a popular format for data manipula- tion and exchange, particularly on the web. The interpretation, extraction, and processing of tabular information are invaluable for knowledge-intensive applications. Notably, significant efforts have been invested in annotating tabular data with ontologies and entities from background knowledge graphs, a process known as Semantic Table Interpretation (STI). STI automation aids in building knowledge graphs, enriching data, and enhancing web-based question answering. This survey aims to provide a comprehensive overview of the STI landscape. It starts by categorizing approaches using a taxonomy of 31 attributes, allowing for comparisons and evaluations. It also examines available tools, assessing them based on 12 criteria. Furthermore, the survey offers an in-depth analysis of the Gold Stan- dards used for evaluating STI approaches. Finally, it provides practical guidance to help end-users choose the most suitable approach for their specific tasks while also discussing unresolved issues and suggesting potential future research directions. Keywords: Semantic Table Interpretation, Semantic Annotation, Table, Knowledge Graph, Table- to-KG Matching, Semantic Web 1 Tables are widely used and play a crucial role in creating, organising, and sharing information. A notable example of their significance as ways to organise human knowledge can be found in the oldest sample of writing on paper (on papyrus), dating back to around 2500 BC, in which Merer, an Egyptian naval inspector, documents his daily activities in a table (Fig. 1) [156]. Figure 1: Portion of the diary of Merer (around 2600 BC), an official in charge of a team of workers responsible for transporting limestone blocks from Tura to Giza to construct the Great Pyramid. The document details various aspects of the logistics involved in the transportation process, such as the organisation of labour, the use of boats to navigate the Nile River, and the daily activities of the workers. 1 arXiv:2411.11891v1  [cs.AI]  7 Nov 2024",
        "pdf_filename": "Survey_on_Semantic_Interpretation_of_Tabular_Data_Challenges_and_Directions.pdf",
        "num_chunks": 4331
    },
    {
        "title": "Symbolic Computation in Software Science - My Personal View",
        "context": "between the three aspects. Working mathematicians will have to master the three aspects equally well and integrate them into their daily work. More speciﬁcally, working in math- knowledge and computational methods) and, at the same time, on the meta-level of develop- ing automated reasoning methods for supporting research on the object level. This massage of the mathematical brain by jumping back and forth between the object and the meta-level will guide mathematics onto a new level of sophistication. Symbolic computation is just a",
        "pdf_filename": "Symbolic Computation in Software Science - My Personal View.pdf",
        "num_chunks": 587
    },
    {
        "title": "Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction in LBSN",
        "context": "The fast development of location-based social networks (LBSNs) has led to significant changes in society, resulting in popular studies of using LBSN data for socioeconomic prediction, e.g., regional pop- ulation and commercial activity estimation. Existing studies design various graphs to model heterogeneous LBSN data, and further apply graph representation learning methods for socioeconomic prediction. However, these approaches heavily rely on heuristic ideas and expertise to extract task-relevant knowledge from diverse data, which may not be optimal for specific tasks. Additionally, they tend to overlook the inherent relationships between different indicators, limiting the prediction accuracy. Motivated by the re- markable abilities of large language models (LLMs) in commonsense reasoning, embedding, and multi-agent collaboration, in this work, we synergize LLM agents and knowledge graph for socioeconomic prediction. We first construct a location-based knowledge graph (LBKG) to integrate multi-sourced LBSN data. Then we leverage the reasoning power of LLM agent to identify relevant meta-paths in the LBKG for each type of socioeconomic prediction task, and design a semantic-guided attention module for knowledge fusion with meta-paths. Moreover, we introduce a cross-task communi- cation mechanism to further enhance performance by enabling knowledge sharing across tasks at both LLM agent and KG levels. On the one hand, the LLM agents for different tasks collaborate to generate more diverse and comprehensive meta-paths. On the other hand, the embeddings from different tasks are adaptively merged for better socioeconomic prediction. Experiments on two datasets demonstrate the effectiveness of the synergistic design between LLM and KG, providing insights for information sharing across socioeconomic prediction tasks. Keywords Large language models, knowledge graph, location-based social network, socioeconomic prediction ∗Corresponding Author. 1 The development of location-based social networks (LBSNs) has significantly advanced socioeconomic prediction with rich web- sourced LBSN data such as user-generated content on review plat- forms. Socioeconomic indicators like regional population, user ac- tivity, and rating, provide a more comprehensive description of LBSN in turn, which can be leveraged by various web applica- tions to offer enhanced services such as location recommendation, web page description, and personal assistants. As a result, socioe- conomic prediction in the context of LBSN has become increas- ingly important, leading to a growing body of research in this field [3, 7, 9, 17, 23, 24, 30, 32–35, 37] Traditionally people collect socioeconomic indicators from sur- veys, which are costly and time-consuming. Recently, data-driven methods have become popular, which use machine learning models to predict the socioeconomic indicators based on various LBSN data. The LBSN data comes from various sources and is heterogeneous. Existing studies have widely used graph structure to model the com- plex relationships within LBSN data, and predict the socioeconomic indicators through graph representation learning methods. They construct either multi-view graphs [9, 30, 35] or knowledge graphs (KGs) [16, 37] to model different factors in LBSN data like mobility, spatial proximity, and functionality. However, These approaches highly rely on heuristic ideas and expertise to extract knowledge related to the tasks from LBSN data, such as the construction of sub- graphs or definition of meta-structures, which may be sub-optimal for different indicator prediction tasks. Moreover, the intrinsic cor- relations and potential for knowledge sharing across different so- cioeconomic prediction tasks are often overlooked, limiting the overall prediction accuracy. The recently emerged large language models (LLMs) provide a promising solution to these limitations. LLMs have demonstrated several remarkable abilities which could help socioeconomic pre- diction [28]: (1) Latent semantic embedding. At the foundational level, LLMs are able to generate text embeddings with rich seman- tic information, which makes it possible to integrate LLM with deep learning models to improve the performance by leveraging the inherent semantic information in the LBSN data. (2) Explicit arXiv:2411.00028v2  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Synergizing_LLM_Agents_and_Knowledge_Graph_for_Socioeconomic_Prediction_in_LBSN.pdf",
        "num_chunks": 1821
    },
    {
        "title": "T-GAE Transferable Graph Autoencoder for Network Alignment",
        "context": "Network alignment is the task of establishing one-to-one correspondences be- tween the nodes of different graphs. Although finding a plethora of applications in high-impact domains, this task is known to be NP-hard in its general form. Existing optimization algorithms do not scale up as the size of the graphs in- creases. While being able to reduce the matching complexity, current GNN approaches fit a deep neural network on each graph and requires re-train on unseen samples, which is time and memory inefficient. To tackle both challenges we propose T-GAE, a transferable graph autoencoder framework that leverages transferability and stability of GNNs to achieve efficient network alignment on out-of-distribution graphs without retraining. We prove that GNN-generated embeddings can achieve more accurate alignment compared to classical spec- tral methods. Our experiments on real-world benchmarks demonstrate that T-GAE outperforms the state-of-the-art optimization method and the best GNN approach by up to 38.7% and 50.8%, respectively, while being able to reduce 90% of the training time when matching out-of-distribution large scale networks. We conduct ablation studies to highlight the effectiveness of the proposed en- coder architecture and training objective in enhancing the expressiveness of GNNs to match perturbed graphs. T-GAE is also proved to be flexible to uti- lize matching algorithms of different complexities. Our code is available at https://github.com/Jason-Tree/T-GAE. 1 Network alignment, also known as graph matching, is a classical problem in graph theory, that aims to find node correspondence across different graphs and is vital in a number of high-impact domains [Emmert-Streib et al., 2016]. In social networks, for instance, network alignment has been used for user deanonymization [Nilizadeh et al., 2014] and analysis [Ogaard et al., 2013], while in bioinformatics it is a key tool to identify functionalities in protein complexes [Singh et al., 2008], or to identify gene–drug modules [Chen et al., 2018a]. Graph matching also finds application in computer vision [Conte et al., 2003], sociology [Racz and Sridhar, 2021], to name a few. However, this problem is usually cast as a quadratic assignment problem (QAP), which is in general NP-hard. Various approaches have been developed to tackle network alignment and can be divided into two main categories; i) optimization algorithms that attempt to approximate the QAP problem by relaxing the combinatorial constraints, ii) embedding methods that approach the problem by implicitly or explicitly generating powerful node embeddings that facilitate the alignment task. Optimization approaches, as [Anstreicher and Brixius, 2001, Vogelstein et al., 2015] employ quadratic programming relaxations, while [Klau, 2009] and [Peng et al., 2010] utilize semidefinite or Lagrangian-based relaxations respectively, [Du et al., 2019] and [Du et al., 2022] proposed to solve network alignment together with link prediction. Successive convex approximations were also proposed by [Konar and Sidiropoulos, 2020] to handle the QAP. Challenges associated with these methods include high computational cost, infeasible solutions, or nearly optimal initialization requirements. Embedding methods, on the other hand, overcome these challenges, but they usually produce inferior solutions, due to an inherent J. He et al., T-GAE: Transferable Graph Autoencoder for Network Alignment. Proceedings of the Third Learning on Graphs Conference (LoG 2024), PMLR 269, Virtual Event, November 26–29, 2024. arXiv:2310.03272v4  [cs.LG]  18 Nov 2024",
        "pdf_filename": "T-GAE_Transferable_Graph_Autoencoder_for_Network_Alignment.pdf",
        "num_chunks": 2228
    },
    {
        "title": "Technology Ethics in Action - Critical and Interdisciplinary Perspectives",
        "context": "",
        "pdf_filename": "Technology Ethics in Action - Critical and Interdisciplinary Perspectives.pdf",
        "num_chunks": 20164
    },
    {
        "title": "Testability of Instrumental Variables in Additive Nonlinear, Non-Constant Effects Models",
        "context": "We address the issue of the testability of instrumental variables derived from observational data. Most existing testable implications are centered on scenarios where the treatment is a discrete variable, e.g., instrumental inequality (Pearl, 1995), or where the eﬀect is assumed to be constant, e.g., instrumental variables condition based on the principle of independent mechanisms (Burauel, 2023). However, treatments can often be continuous variables, such as drug dosages or nutritional content levels, and non-constant eﬀects may occur in many real-world scenarios. In this paper, we consider an additive nonlinear, non- constant eﬀects model with unmeasured confounders, in which treatments can be either discrete or continuous, and propose an Auxiliary-based Independence Test (AIT) condition to test whether a variable is a valid instrument. We ﬁrst show that if the candidate instrument is valid, then the AIT condition holds. Moreover, we illustrate the implications of the AIT condition and demonstrate that, in certain conditions, AIT conditions are necessary and suﬃcient to detect all invalid IVs. We also extend the AIT condition to include covariates and introduce a practical testing algorithm. Experimental results on both synthetic and three diﬀerent real-world datasets show the eﬀectiveness of our proposed condition. ∗. Equal contribution †. Corresponding author ©2022 Author One and Author Two. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.",
        "pdf_filename": "Testability_of_Instrumental_Variables_in_Additive_Nonlinear,_Non-Constant_Effects_Models.pdf",
        "num_chunks": 2237
    },
    {
        "title": "TFG Unified Training-Free Guidance for Diffusion Models",
        "context": "Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks. As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult. This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space. Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task. We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performance by 8.5% on average. Our framework and benchmark offer a solid foundation for conditional generation in a training-free manner.1 1 Recent advancements in generative models, particularly diffusion models [61, 21, 62, 66], have demonstrated remarkable effectiveness across vision [65, 48, 52], small molecules [74, 73, 24], proteins[1, 72], audio [35, 29], 3D objects [40, 41], and many more. Diffusion models estimate the gradient of log density (i.e., Stein score, [67]) of the data distribution [65] via denoising learning objectives, and can generate new samples via an iterative denoising process. With impressive scalability to billions of data [58], future diffusion models have the potential to serve as foundational generative models across a wide range of applications. Consequently, the problem of conditional generation based on these models, i.e., tailoring outputs to satisfy user-defined criteria such as labels, attributes, energies, and spatial-temporal information, is becoming increasingly important [63, 2]. Conditional generation methods like classifier-based guidance [66, 7] and classifier-free guidance [23] typically require training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability. In contrast, training-free guidance aims to generate samples that align with certain targets specified through an off-the-shelf differentiable target predictor without involving any additional training. Here, a target predictor can be any classifier, loss function, probability function, or energy function used to score the quality of the generated samples. In classifier-based guidance [66, 7], where a noise-conditional classifier is specifically trained to predict the target property on both clean and noisy samples, incorporating guidance in the diffusion ∗Equal contribution. Corresponding to mailto:haotianye@stanford.edu. 1Code is available at https://github.com/YWolfeee/Training-Free-Guidance. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2409.15761v2  [cs.LG]  19 Nov 2024",
        "pdf_filename": "TFG_Unified_Training-Free_Guidance_for_Diffusion_Models.pdf",
        "num_chunks": 3175
    },
    {
        "title": "The Agent-based Modelling for Human Behaviour Special Issue",
        "context": "",
        "pdf_filename": "The Agent-based Modelling for Human Behaviour Special Issue.pdf",
        "num_chunks": 145
    },
    {
        "title": "The Elements of Differentiable Programming",
        "context": "",
        "pdf_filename": "The Elements of Differentiable Programming.pdf",
        "num_chunks": 18081
    },
    {
        "title": "The Framework of a Design Process Language",
        "context": "",
        "pdf_filename": "The Framework of a Design Process Language.pdf",
        "num_chunks": 7697
    },
    {
        "title": "The future of document indexing - GPT and Donut revolutionize table of content processing",
        "context": "Industrial projects rely heavily on lengthy, complex specification documents, making te- dious manual extraction of structured information a major bottleneck. This paper introduces an innovative approach to automate this process, leveraging the capabilities of two cutting- edge AI models: Donut, a model that extracts information directly from scanned documents without OCR, and OpenAI GPT-3.5 Turbo, a robust large language model. The proposed methodology is initiated by acquiring the table of contents (ToCs) from construction specifi- cation documents and subsequently structuring the ToCs text into JSON data. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5 Turbo reaching 89% in effec- tively organizing the ToCs. This landmark achievement represents a significant leap forward in document indexing, demonstrating the immense potential of AI to automate information extraction tasks across diverse document types, boosting efficiency and liberating critical resources in various industries. Keywords Document AI · Document Classification · Information extraction · Large Language Models · OCR Models · Visual Document Understanding 1 Traversing extensive and intricate documents and locating essential information buried within numerous pages demands considerable time and effort. This inefficiency not only impedes understanding but also results in significant costs associated with manual data entry and extraction. To address this challenge, researchers have delved into the expanding realm of AI, with a particular emphasis on automating the extraction of information from large documents. Documents, whether in traditional written or electronic form, are essential for conveying information in various domains such as technology and business. Electronic documents, including PDF files, Microsoft Word documents, spreadsheets, emails, invoices, and presen- tations, play diverse roles like record-keeping, communication, collaboration, and supporting legal and arXiv:2403.07553v1  [cs.IR]  12 Mar 2024",
        "pdf_filename": "The future of document indexing - GPT and Donut revolutionize table of content processing.pdf",
        "num_chunks": 466
    },
    {
        "title": "The Hermeneutic Turn of AI Is the Machine Capable of Interpreting",
        "context": "This article aims to demonstrate how the approach to computing is being disrupted by deep learning (artificial neural networks), not only in terms of techniques but also in our interactions with machines. It also addresses the philosophical tradition of hermeneutics (Don Ihde, Wilhelm Dilthey) to highlight a parallel with this movement and to demystify the idea of human-like AI. Keywords Artificial intelligence, AI ethics, Philosophy of technology, LLM  The notion of interpretation is increasingly present in the world of artificial intelligence (AI). For humans, it involves interpreting algorithms that are difficult to explain mathematically. For machines, the challenge is to interpret data to draw conclusions. They must now also interpret brief instructions in natural language: this is the operational principle of ChatGPT and other chatbots grounded on generative AI which interacts verbally with unsettling fluidity. We can thus speak of a true interpretive turning point in AI. The art of interpretation, however, has been known for centuries under the term “hermeneutics”. It initially applied to the reading of poets or sacred texts before evolving into a philosophical current to signify that interpretation is at the foundation of understanding, or even that it represents the necessary activity of who we are (Gadamer, 2004-1960; Heidegger, 1962-1927; Nietzsche, 1954-1886/1887, 2003-1887). Our access to the world is indeed always influenced by certain tones that are not neutral but carry a cultural charge. Does the resemblance stop however at the mere use of the term interpretation? In other words, is AI doing hermeneutics? Should we use the art of interpretation to understand machines? Or is it both?",
        "pdf_filename": "The_Hermeneutic_Turn_of_AI_Is_the_Machine_Capable_of_Interpreting.pdf",
        "num_chunks": 439
    },
    {
        "title": "The Role of Accuracy and Validation Effectiveness in Conversational Business Analytics",
        "context": "This study examines conversational business analytics, an approach that utilizes AI to address the technical competency gaps that hindered end users from effectively using traditional self-service analytics. By facilitating natu- ral language interactions, conversational business analytics aims to enable end users to independently retrieve data and generate insights. The analysis fo- cuses on Text-to-SQL as a representative technology for translating natural language requests into SQL statements. Using models grounded in expected utility theory, the study identifies conditions under which conversational busi- ness analytics, through partial or full support, can outperform delegation to human experts. The results indicate that partial support, which focuses solely on information generation by AI, is viable when the accuracy of AI-generated SQL queries exceeds a defined threshold. In contrast, full support includes not only information generation but also validation through explanations pro- vided by the AI, and requires sufficiently high validation effectiveness to be reliable. However, user-based validation presents challenges, such as misjudg- ment and rejection of valid SQL queries, which may limit the effectiveness of conversational business analytics. These challenges underscore the need for robust validation mechanisms, including improved user support, automated processes, and methods for assessing quality independently of end users’ tech- nical competencies. 1 Business analytics aims to generate actionable insights that support data-driven decision-making across diverse organizational contexts. Self-service analytics, a sig- nificant development in this domain, empowers end users to independently fulfill their information needs without relying on experts such as data engineers or data scientists. By providing tools for retrieving, preparing, analyzing, and visualizing data, self-service analytics enhances flexibility and agility in addressing dynamic business demands. Despite these advantages, self-service analytics has notable limitations. While end users are often domain experts, they frequently lack the technical skills required for advanced analytics tasks, such as navigating complex data structures, writing 1 arXiv:2411.12128v1  [cs.AI]  18 Nov 2024",
        "pdf_filename": "The_Role_of_Accuracy_and_Validation_Effectiveness_in_Conversational_Business_Analytics.pdf",
        "num_chunks": 1234
    },
    {
        "title": "Thinking Before Looking Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination",
        "context": "Multimodal large language models (MLLMs) have ad- vanced the integration of visual and linguistic modali- ties, establishing themselves as the dominant paradigm for visual-language tasks. Current approaches like chain of thought (CoT) reasoning have augmented the cognitive ca- pabilities of large language models (LLMs), yet their adap- tation to MLLMs is hindered by heightened risks of hallu- cination in cross-modality comprehension. In this paper, we find that the thinking while looking paradigm in current multimodal CoT approaches—where reasoning chains are generated alongside visual input—fails to mitigate hallu- cinations caused by misleading images. To address these limitations, we propose the Visual Inference Chain (VIC) framework, a novel approach that constructs reasoning chains using textual context alone before introducing visual input, effectively reducing cross-modal biases and enhanc- ing multimodal reasoning accuracy. Comprehensive eval- uations demonstrate that VIC significantly improves zero- shot performance across various vision-related tasks, miti- gating hallucinations while refining the reasoning capabil- ities of MLLMs. Our anonymized code repository can be found at https://github.com/Terry-Xu-666/ visual_inference_chain. Large Language Models (LLMs), such as GPT-4 [1] and Llama [34], have driven remarkable advancements in world comprehension [11] and reasoning capability [12]. Prompt- ing techniques like CoT [9, 28, 38] have been developed to enhance LLMs’ ability to handle complex tasks through *These authors contributed equally to this work. †Visiting student at Lehigh University. human-like step-by-step reasoning. Meanwhile, MLLMs have rapidly advanced in recent years [1, 2, 20, 21], ex- tending LLMs’ capabilities into the multimodal realm by integrating visual backbones to align visual and language representations. MLLMs have demonstrated exceptional performance in a range of vision-related tasks, including visual question answering [4], object recognition [7, 42], and video comprehension [22], highlighting the impressive evolution of AI-driven visual-language understanding. The success of CoT prompting in unimodal contexts suggests a promising extension to multimodal scenarios. Due to the integration of pretrained vision models [27, 32] and lan- guage models [44] in MLLMs, various types of hallucina- tions have been observed [5, 16, 18], including nonexistent object generation [33], visual misinterpretations [17], and cross-modality biases [35]. Offering advantages in tackling complex multimodal tasks, current prompting approaches [46, 47] predominantly adhere to the thinking while looking paradigm, where rea- soning occurs simultaneously with the integration of visual elements. However, this paradigm encounters substantial challenges due to the prevalence of hallucinations, which undermine both the reliability of the reasoning process and the accuracy of the responses. As illustrated in Figure 1, the MLLM often falls into stereotypes when processing a question-image pair, where it recalls similar prior contexts and overlooks subtle variations, leading to erroneous re- sponses. Even with CoT prompting, the model tends to rely on memory-based stereotypes rather than engaging in accu- rate reasoning, which leads to incorrect responses. To overcome hallucinations from visual inputs and har- ness the reasoning capabilities of LLMs, we propose the Visual Inference Chain (VIC), which introduces a reasoning process that occurs prior to engaging with visual elements, following the thinking before looking paradigm. This ap- 1 arXiv:2411.12591v1  [cs.CV]  15 Nov 2024",
        "pdf_filename": "Thinking_Before_Looking_Improving_Multimodal_LLM_Reasoning_via_Mitigating_Visual_Hallucination.pdf",
        "num_chunks": 2368
    },
    {
        "title": "Timeline-based Planning and Execution with Uncertainty - Theory, Modeling Methodologies and Practice",
        "context": "",
        "pdf_filename": "Timeline-based Planning and Execution with Uncertainty - Theory, Modeling Methodologies and Practice.pdf",
        "num_chunks": 7031
    },
    {
        "title": "Topological Symmetry Enhanced Graph Convolution for Skeleton-Based Action Recognition",
        "context": "Skeleton-based action recognition has achieved remarkable performance with the development of graph convolutional networks (GCNs). However, most of these methods tend to construct complex topology learning mechanisms while ne- glecting the inherent symmetry of the human body. Addi- tionally, the use of temporal convolutions with certain fixed receptive fields limits their capacity to effectively capture dependencies in time sequences. To address the issues, we (1) propose a novel Topological Symmetry Enhanced Graph Convolution (TSE-GC) to enable distinct topology learning across different channel partitions while incorpo- rating topological symmetry awareness and (2) construct a Multi-Branch Deformable Temporal Convolution (MB- DTC) for skeleton-based action recognition. The proposed TSE-GC emphasizes the inherent symmetry of the human body while enabling efficient learning of dynamic topolo- gies. Meanwhile, the design of MBDTC introduces the con- cept of deformable modeling, leading to more flexible re- ceptive fields and stronger modeling capacity of temporal dependencies. Combining TSE-GC with MBDTC, our final model, TSE-GCN, achieves competitive performance with fewer parameters compared with state-of-the-art methods on three large datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. On the cross-subject and cross-set evalu- ations of NTU RGB+D 120, the accuracies of our model reach 90.0% and 91.1%, with 1.1M parameters and 1.38 GFLOPS for one stream. Human action recognition has attracted much attention due to its wide range of applications[2, 13, 15, 25], including video surveillance, virtual reality, health care and so on. With the development of depth sensors[39, 42] and human pose estimation methods[3, 7, 32], skeleton-based action * Corresponding author. v1 v6 v2 v3 v4 v5 v7 v8 v9 v10 v11 v1 v11 v11 v1 reactivate 0 2 … v6 v11 Figure 1. Topology reactivation with symmetry awareness. The correlations between v6 and v2, v3, v8, v9 in the shared topology are activated due to the scale mask derived from the correlation between v6 and v3. Darker colors and thicker lines stand for larger weights. Best viewed in color. recognition[5, 16, 24, 37] has become increasingly popular. Skeleton is a type of structured data with each joint of the human body identified by its joint type, frame index and 3D position, which shows great potential in preserving privacy and demonstrates strong robustness against the variations of illumination, viewpoints and other background changes. Inspired by the inherent structure of skeleton data, graph convolutional networks (GCNs) have emerged as a dominant solution for skeleton-based action recognition. STGCN[37] was the first work to encode human body as spatial temporal graphs, aggregating features along the nat- ural connectivity of the human body. Since then, many works[5, 18, 20, 22, 27, 28] have delved into the optimiza- tion of graph convolutional mechanisms, with a focus on adaptively building topologies to effectively capture mo- tion patterns. However, most of these approaches lever- age learnable adjacency matrices combined with attention mechanisms and other techniques derived from dynamics theory[35], information bottleneck theory[10], topological data analysis[44] and so on, neglecting or underestimating the inherent symmetry of the human body and its poten- tial role in topology learning. For instance, in the case of a “brush hair” action, the interaction between the head and 1 arXiv:2411.12560v1  [cs.CV]  19 Nov 2024",
        "pdf_filename": "Topological_Symmetry_Enhanced_Graph_Convolution_for_Skeleton-Based_Action_Recognition.pdf",
        "num_chunks": 1424
    },
    {
        "title": "Towards Explainability and Fairness in Swiss Judgement Prediction - Benchmarking on a Multilingual Dataset",
        "context": "The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that ‘support’ and ‘oppose’ judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our findings reveal that improved prediction performance does not necessarily correspond to enhanced explainability performance, underscoring the significance of evaluating models from an explainability perspective. Additionally, we introduce a novel evaluation framework, Lower Court Insertion (LCI), which allows us to quantify the influence of lower court information on model predictions, exposing current models’ biases. Keywords: Fairness, Explainability, Multilingual, Legal Judgement Prediction 1. The task of Legal Judgement Prediction involves analyzing the textual description of case facts to de- termine various aspects of a case’s outcome, such as the winning party, violated provisions, and mo- tion results. It has garnered substantial attention in the mainstream NLP community (Aletras et al., 2016; Chalkidis et al., 2019; Malik et al., 2021; Niklaus et al., 2021; Semo et al., 2022; Santosh et al., 2023a) and is being considered as a bench- marking task for evaluating the capabilities of legal NLP (Chalkidis et al., 2022b; Niklaus et al., 2023a) and long range models (Condevaux and Harispe, 2022; Niklaus and Giofré, 2022; Chalkidis et al., 2022a; Hua et al., 2022; Niklaus et al., 2023b). The process of resolving legal cases encom- passes evidential reasoning through exchange of arguments between the litigating parities before a decision-making body (Santosh et al., 2022). Earlier methods to deal with outcome predic- tion task such as IBP (Brüninghaus and Ash- ley, 2003), SMILE+IBP (Brüninghaus and Ash- ley, 2005), VJF (Grabmair, 2017) typically involved identification/extraction of the factors from the tex- tual description of the facts, then employing a con- ceptual schema to relate the factors to legal issues and predicts the outcome by comparing them with the past cases, thus providing the explanations for those predictions in terms that are legally intuitive. However, in the context of modern deep learning- based solutions, the outcome is determined solely from the text of the case facts, effectively bypass- ing the interpretable legal reasoning process. This poses a significant risk, particularly in high-stakes domains like law, when utilizing such systems that rely on factors that may be predictive but lack le- gal relevance or involve sensitive attributes (e.g., the race of an accused person). Such reliance can lead to unjust and biased outcomes, undermin- ing the principles of fairness and equal treatment within the legal system. Hence, such systems need to be analyzed from an explainability standpoint, thus making them transparent and thereby enhanc- ing the trust of legal practitioners and stakeholders to comprehend the factors and legal principles that contribute to a particular prediction. In the line of explainable LJP, Chalkidis et al. 2021 investigated the rationales behind models’ decisions in Legal Judgment Prediction (LJP) for European Court of Human Rights (ECtHR) cases. Subsequent studies by Santosh et al. 2022 ex- tended the above dataset and Malik et al. 2021 created new dataset for Indian Jurisdiction. In con- trast to these works in English, our study focuses on assessing the explainability of LJP models trained on the Swiss-Judgment-Prediction (SJP) dataset, which is the only available multilingual LJP dataset. It contains cases from the Federal Supreme Court of Switzerland (FSCS), written in three official Swiss languages (German, French, Italian)1. To this end, we curate a multilingual set of rationales that ‘support and ‘oppose’ Judgment 1The dataset consists of non-parallel cases, with each case being unique and decisions being written in a single language. arXiv:2402.17013v1  [cs.CL]  26 Feb 2024",
        "pdf_filename": "Towards Explainability and Fairness in Swiss Judgement Prediction - Benchmarking on a Multilingual Dataset.pdf",
        "num_chunks": 1918
    },
    {
        "title": "Transformer Neural Processes -- Kernel Regression",
        "context": "Stochastic processes model various natu- ral phenomena from disease transmission to stock prices, but simulating and quantify- ing their uncertainty can be computationally challenging. For example, modeling a Gaus- sian Process with standard statistical meth- ods incurs an O(n3) penalty, and even us- ing state-of-the-art Neural Processes (NPs) incurs an O(n2) penalty due to the attention mechanism. We introduce the Transformer Neural Process - Kernel Regression (TNP- KR), a new architecture that incorporates a novel transformer block we call a Kernel Regression Block (KRBlock), which reduces the computational complexity of attention in transformer-based Neural Processes (TNPs) from O((nC + nT )2) to O(n2 C + nCnT ) by eliminating masked computations, where nC is the number of context, and nT is the num- ber of test points, respectively, and a fast at- tention variant that further reduces all at- tention calculations to O(nC) in space and time complexity. In benchmarks spanning such tasks as meta-regression, Bayesian opti- mization, and image completion, we demon- strate that the full variant matches the per- formance of state-of-the-art methods while training faster and scaling two orders of mag- nitude higher in number of test points, and the fast variant nearly matches that perfor- mance while scaling to millions of both test and context points on consumer hardware. Preliminary work. Correspondence to: daniel.jenson@worc.ox.ac.uk. ∗These authors jointly supervised this work. 1 The principle challenge of modern spatiotemporal Bayesian modeling is scale. As the number of ob- served locations increases from tens to thousands or hundreds of thousands, traditional techniques used to model spatiotemporal phenomena break down. Per- haps the most common method typically employed to model spatiotemporal processes is the Gaussian Process (GP). Gaussian Processes are a particularly well-behaved class of stochastic processes. Specifi- cally, for a finite index set {t ∈T}, the collection X = (X1, . . . , XT ) follows a multivariate Gaussian distribution. This makes various analytic calculations tractable, facilitating regression, marginalization, and sampling with GPs. While GPs provide a significant degree of flexibility in modeling, the analytic solutions they yield do not scale well in the number of observed locations. Using a GP to model spatial random effects within a Markov Chain Monte Carlo (MCMC) sampler incurs an O(n3) cost per sample, where n is the number of observed lo- cations. This is because the covariance matrix must be inverted, or factorized in the case of Cholesky de- composition, at each iteration in order to generate a sample. Unfortunately, this means that for only n = 1,000 locations, nearly a billion operations must be performed to generate a single sample. In order to accelerate Bayesian inference with spa- tiotemporal stochastic processes, there have been at least three prominent strains of research. The first is Variational Inference (VI), which aims to recast the in- ference problem as an optimization problem and max- imize the Evidence Lower Bound (ELBO). The second aims to accelerate sampling by using a generative neu- ral network-based approximation. This family tends to leverage Variational Autoencoders (VAEs). The third is a recent family of deep learning models called Neural Processes (NPs). These models use a meta- learning objective, meaning that once trained, the for- ward pass of the model takes as input “context” or observed points and returns a function. This function arXiv:2411.12502v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Transformer_Neural_Processes_--_Kernel_Regression.pdf",
        "num_chunks": 1241
    },
    {
        "title": "TS-ACL A Time Series Analytic Continual Learning Framework for Privacy-Preserving and Class-Incremen",
        "context": "Class-incremental pattern recognition in time series is a significant problem, which aims to learn from continually arriving streaming data examples with incremental classes. A primary challenge in this problem is catastrophic forget- ting, where the incorporation of new data samples causes the models to forget previously learned information. While the replay-based methods achieve promising results by stor- ing historical data to address catastrophic forgetting, they come with the invasion of data privacy. On the other hand, the exemplar-free methods preserve privacy but suffer from significantly decreased accuracy. To address these chal- lenges, we proposed TS-ACL, a novel Time Series Analytic Continual Learning framework for privacy-preserving and class-incremental pattern recognition. Identifying gradi- ent descent as the root of catastrophic forgetting, TS-ACL transforms each update of the model into a gradient-free analytical learning process with a closed-form solution. By leveraging a pre-trained frozen encoder for embedding ex- traction, TS-ACL only needs to recursively update an ana- lytic classifier in a lightweight manner. This way, TS-ACL simultaneously achieves non-forgetting, privacy preserva- tion, and lightweight consumption, making it widely suit- able for various applications, particularly in edge comput- ing scenarios. Extensive experiments on five benchmark datasets confirm the superior and robust performance of TS-ACL compared to existing advanced methods. Code is available at https://github.com/asdasdczxczq/TS-ACL. With significant research attention in recent years, pattern recognition in time series plays a critical role in various *The first three authors contributed equally to this work. †Corresponding authors: tangentheng@gmail.com, hpzhuang@scut.edu.cn. Task 1 Task 2 Task 3 Class 1 Class 2 Class 3 Class 4 Class 5 Class 6 Embedding & Label Memory Matrix Classifier Weights Recursive Update Initial Calculation Continual Data Streams Analytical Classifier Initial Calculation Encoder Network & Randomly-initialized Hidden Layer Classes 1, 2 Classes 1, 2, 3, 4 Classes 1, 2, 3, 4, 5, 6 Recursive Update Recursive Update Recursive Update Recursive Regression-Based Learning Figure 1. Overview of class-incremental pattern recognition in time series, in which our TS-ACL can simultaneously achieve non- forgetting, privacy preservation, and lightweight consumption. applications, such as healthcare diagnostics [32], industrial production [36], and urban computing [13]. Deep Learning (DL) approaches have gained widespread popularity due to their superior performance, and they typically rely on of- fline, static datasets that assume data to be independently and identically distributed (i.i.d.) [10]. Unfortunately, as shown in Figure 1, real-world scenar- ios often involve continual data streams from sensors, re- sulting in an ever-growing volume of time series data with incremental classes, where the i.i.d. assumption no longer holds [23]. Additionally, as new data with previously un- seen classes may emerge over time, the dataset becomes increasingly complex [23]. This dynamic environment re- quires the DL models to continually adapt and learn from new data samples. Unfortunately, this process is often hin- dered by the well-known problem of “catastrophic forget- ting”, in which the models forget previously learned knowl- edge when exposed to new data samples [5, 33]. To mitigate catastrophic forgetting, numerous Class- arXiv:2410.15954v2  [cs.LG]  18 Nov 2024",
        "pdf_filename": "TS-ACL_A_Time_Series_Analytic_Continual_Learning_Framework_for_Privacy-Preserving_and_Class-Incremen.pdf",
        "num_chunks": 1655
    },
    {
        "title": "TSPRank Bridging Pairwise and Listwise Methods with a Bilinear Travelling Salesman Model",
        "context": "Traditional Learning-To-Rank (LETOR) approaches, including pair- wise methods like RankNet and LambdaMART, often fall short by solely focusing on pairwise comparisons, leading to sub-optimal global rankings. Conversely, deep learning based listwise methods, while aiming to optimise entire lists, require complex tuning and yield only marginal improvements over robust pairwise models. To overcome these limitations, we introduce Travelling Salesman Prob- lem Rank (TSPRank), a hybrid pairwise-listwise ranking method. TSPRank reframes the ranking problem as a Travelling Salesman Problem (TSP), a well-known combinatorial optimisation challenge that has been extensively studied for its numerous solution algo- rithms and applications. This approach enables the modelling of pairwise relationships and leverages combinatorial optimisation to determine the listwise ranking. This approach can be directly integrated as an additional component into embeddings gener- ated by existing backbone models to enhance ranking performance. Our extensive experiments across three backbone models on di- verse tasks, including stock ranking, information retrieval, and historical events ordering, demonstrate that TSPRank significantly outperforms both pure pairwise and listwise methods. Our qualita- tive analysis reveals that TSPRank’s main advantage over existing methods is its ability to harness global information better while ranking. TSPRank’s robustness and superior performance across different domains highlight its potential as a versatile and effective LETOR solution. The code and preprocessed data are available at https://github.com/waylonli/TSPRank-KDD2025. CCS CONCEPTS • Information systems →Learning to rank. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ’25, August 3–7, 2025, Toronto, Canada © 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX KEYWORDS learning-to-rank, pairwise-listwise ranking, travelling salesman problem ACM Reference Format: Weixian Waylon Li, Yftah Ziser, Yifei Xie, Shay B. Cohen, and Tiejun Ma. 2025. TSPRank: Bridging Pairwise and Listwise Methods with a Bi- linear Travelling Salesman Model. In Proceedings of the 31th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’25), August 3–7, 2025, Toronto, Canada. ACM, New York, NY, USA, 23 pages. https: //doi.org/XXXXXXX.XXXXXXX 1 Learning to Rank (LETOR) algorithms have become essential in applications such as recommendation systems [16, 30, 36], question answering [10, 15, 27], and information retrieval [2, 9, 34]. These algorithms aim to order a list of ranking entities based on their features, optimising for the most relevant or preferred entities to appear at the top. Recently, LETOR methods have also expanded into other domains, such as stock and portfolio selection [45, 50] and textual ordering [46, 49]. Despite these broader applications of LETOR, latest fundamental work on LETOR still primarily focused on incorporating click data into ranking models and addressing biases introduced by user feedback [20, 21, 33, 54], concentrating on retrieval and recommendation tasks. Limited research has explored general ranking methods across diverse tasks and domains. Over the past decade, research on LETOR models mainly focuses on pairwise and listwise approaches, while pointwise methods often fail to capture the intricate inter-entity relationships that are essen- tial for accurate ranking. Pairwise methods, such as LambdaMART and RankNet [6], primarily optimise for pairwise comparisons with- out a holistic view of the entire ranking list, potentially leading to sub-optimal global rankings. Listwise methods optimise the ranking of entire lists directly rather than individuals or pairs. This category includes advanced neural network architectures, particularly adap- tations of the Transformer architecture [47], such as Rankformer [9] and SetRank [34]. However, existing work shows that deep learning-based listwise models require complex tuning to achieve marginal gains over robust pairwise models like LambdaMART on information retrieval benchmarks [39]. Therefore, pairwise and listwise methods each have inherent drawbacks and more effective solutions for LETOR have not been exhaustively explored. arXiv:2411.12064v1  [cs.AI]  18 Nov 2024",
        "pdf_filename": "TSPRank_Bridging_Pairwise_and_Listwise_Methods_with_a_Bilinear_Travelling_Salesman_Model.pdf",
        "num_chunks": 10056
    },
    {
        "title": "ULTra Unveiling Latent Token Interpretability in Transformer Based Understanding",
        "context": "Transformers have revolutionized Computer Vision (CV) and Natural Language Processing (NLP) through self- attention mechanisms. However, due to their complexity, their latent token representations are often difficult to inter- pret. We introduce a novel framework that interprets Trans- former embeddings, uncovering meaningful semantic pat- terns within them. Based on this framework, we demon- strate that zero-shot unsupervised semantic segmentation can be performed effectively without any fine-tuning using a model pre-trained for tasks other than segmentation. Our method reveals the inherent capacity of Transformer mod- els for understanding input semantics and achieves state-of- the-art performance in semantic segmentation, outperform- ing traditional segmentation models. Specifically, our ap- proach achieves an accuracy of 67.2 % and an mIoU of 32.9 % on the COCO-Stuff dataset, as well as an mIoU of 51.9 % on the PASCAL VOC dataset. Additionally, we validate our interpretability framework on LLMs for text summarization, demonstrating its broad applicability and robustness. In recent years, the Transformer architecture and founda- tion models, leveraging self-attention mechanisms to cap- ture complex dependencies in text, have transformed Nat- ural Language Processing (NLP) benchmarks [2, 51, 54, 58]. Similarly, Vision Transformers (ViTs) [13] have been adapted in Computer Vision (CV) and now serve as the backbone for various tasks such as segmentation and ob- ject detection [30, 52]. Despite their success, understanding the interpretability of Transformers remains a challenge due to the complexity of their latent token representations. Several methods have been developed to enhance the in- terpretability of CNN-based models [44, 47, 63]. While some of these can be extended to Transformer architec- tures, they do not fully leverage the unique attention mech- ζ = 0.5 ζ = 0.35 ζ = 0.2 Segmentation Results Original Image Figure 1. Hierarchical clustering tree showing the grouping of to- ken relevance maps for all tokens in a latent layer of the Vision Transformer, not limited to the CLS token. Each leaf node repre- sents a single token relevance map, while higher-level nodes show aggregated clusters based on a clustering threshold (ζ), which controls the level of detail. Lower ζ values reveal finer details, while higher values create broader, more general clusters. This approach demonstrates how pre-trained Vision Transformers can perform unsupervised semantic segmentation, identifying mean- ingful patterns within token representations without requiring ad- ditional training or fine-tuning. anisms inherent to Transformers. Recent research has in- troduced interpretability methods specifically designed for Transformers [1, 6, 59]. However, these approaches primar- ily focus on explaining final model outputs, providing lim- ited insight into the intermediate processes that lead to pre- dictions. For instance, [7] maps latent tokens into CLIP’s [37] multi-modal space to find corresponding text descrip- tions, relying on an external text encoder for interpretabil- ity. In contrast, our approach directly interprets the latent space of ViTs, elucidating the role and function of each to- ken within the high-dimensional space without relying on external models. This paper introduces a framework to interpret latent to- 1 arXiv:2411.12589v1  [cs.CV]  15 Nov 2024",
        "pdf_filename": "ULTra_Unveiling_Latent_Token_Interpretability_in_Transformer_Based_Understanding.pdf",
        "num_chunks": 1335
    },
    {
        "title": "Understanding Chain-of-Thought in LLMs through Information Theory",
        "context": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, from complex reasoning to code generation [Chowdhery et al., 2024, OpenAI et al., 2024, Bubeck et al., 2023, Anil et al., 2023]. Many of these advances can be attributed to Chain-of-Thought (CoT) reasoning [Wei et al., 2024, Nye et al., 2021, Li et al., 2024], which involves breaking down complex problems into a series of intermediate steps, mirroring human-like reasoning processes. The success of CoT reasoning, particularly in domains such as mathematics, logic, and multi-step decision-making, has led researchers and developers to incorporate CoT-like features directly into model training, i.e. the FLAN family of models [Chung et al., 2022, Wei et al., 2022]. This paper introduces a new formal framework for analyzing CoT in LLMs. We provide a rigorous method grounded in information theory, to evaluate the quality of each step in a model’s reasoning process, thus offering insights beyond simple accuracy metrics to identify areas for improvement. Previous work in this area has proposed “Process Supervision” [Lightman et al., 2023], which requires expensive, human-annotated step-by-step data. While effective, this approach is often impractical due to the high cost and effort of creating large-scale annotated datasets. In turn, alternative methods have recently been proposed, such as outcome reward modelling [Havrilla et al., 2024] or the Math-Shepherd [Wang et al., 2024b]. Both these approaches avoid reliance on annotated step-wise CoT data by instead modelling the correctness of each step based on the correctness of final outputs. However, as we demonstrate in this paper, these methods can be unsound for detecting incorrect reasoning steps and can thus lead to a high false-positive rate in certain scenarios. To address these shortcomings, we employ an information-theoretic approach, grounded in the following key insight: Each correct step in a reasoning process should provide valuable and relevant information that aids in predicting the final correct outcome. Building on this insight, we develop a framework to quantify the “infor- mation gain” after each sub-task in the reasoning process, without the need for step-by-step annotations. This enables us to detect sub-tasks that fail to contribute meaningful information toward the correct solution, signalling potential errors or irrelevant steps in the model’s reasoning. In addition, we also introduce a practical algorithm to assess LLM performance across various sub-tasks within a Chain-of-Thought (CoT) reasoning process. The key contributions of this paper are as follows:",
        "pdf_filename": "Understanding_Chain-of-Thought_in_LLMs_through_Information_Theory.pdf",
        "num_chunks": 1719
    },
    {
        "title": "Unveiling and Mitigating Bias in Large Language Model Recommendations A Path to Fairness",
        "context": "dation systems provide more comprehensive recommendations than traditional systems by deeply analyzing content and user behavior. However, these systems often exhibit biases, favoring mainstream content while marginalizing non-traditional options due to skewed training data. This study investigates the intricate relationship between bias and LLM-based recommendation sys- tems, with a focus on music, song, and book recommendations across diverse demographic and cultural groups. Through a com- prehensive analysis conducted over different LLM-models, this paper evaluates the impact of bias on recommendation outcomes. Our ﬁndings highlight that biases are not only deeply embedded but also widely pervasive across these systems, emphasizing the substantial and widespread nature of the issue. Moreover, contextual information, such as socioeconomic status, further amplify these biases, demonstrating the complexity and depth of the challenges faced in creating fair recommendations across different groups. Index Terms—Large Language Models, Bias in Recommenda- tion Systems, Fairness in AI Recommendations, Demographic and Cultural Bias, Fairness Metrics. Consider an LLM-based music recommendation system, such as MuseChat [1], that enhances user experience by lever- aging the advanced capabilities of large language models. Tra- ditional algorithms typically rely on user listening history and genre preferences. In contrast, an LLM-based system delves deeper into musical content and user behavior. For example, a user who frequently listens to progressive and alternative rock would beneﬁt from recommendations generated through a comprehensive analysis of genres like psychedelic rock. By considering lyrical themes, musical styles, and emotional tones, the system can suggest tracks from emerging artists in related rock genres, showcasing the nuanced and highly personalized recommendations LLMs can provide. However, such a personalized recommendation system has drawbacks. Users from Western countries may predominantly receive recommendations for mainstream Western genres like pop or rock, while underrepresented genres, such as traditional indigenous music or world music, receive limited exposure. This bias stems from training data skewed towards popular Western music. Thus, bias in recommendation systems has emerged as a critical concern, impacting fairness, diversity, and societal equity. Demographic and cultural biases have been widely observed in recommendation systems. Studies by Neophytou et al. [2] and Ekstrand et al. [3] have explored how demographic and cultural factors inﬂuence the variability in recommendations. For instance, Neophytou et al. [2] found that the performance of recommender systems consistently declines for older users, with female users also experiencing lower utility compared to their male counterparts. These biases can have tangible real-world consequences, as evidenced by Lambrecht and Tucker [4] and Datta et al. [5], who demonstrated that women may receive fewer recommendations for high-paying jobs and career coaching services compared to men. While bias in traditional systems has been extensively studied [6], [7], [8], [9], integrating LLMs introduces new challenges. Due to their massive scale and ability to learn intricate patterns from vast datasets, LLMs can amplify exist- ing biases, leading to skewed recommendations that perpetuate societal inequalities. Recent studies have critically examined the performance and fairness of LLM-based recommendation systems. Wan et al. [10] and Plaza-del-Arco et al. [11] ana- lyzed gender biases in reference letters and emotion attribution, revealing signiﬁcant gendered stereotypes. Naous et al. [12] highlighted cultural biases in multilingual LLMs, while Zhang et al. [13] found that music and movie recommendations can perpetuate existing biases. Xu et al. [14] studied implicit user unfairness, and Sah et al. [15] explored personality proﬁling to enhance fairness. However, these studies often focus on speciﬁc biases or contexts, underscoring the need for a comprehensive approach to address the multifaceted nature of biases in LLM-based recommendation systems. This paper aims to address the limitations of previous studies by exploring the intricate relationship between bias and LLM-based recommendation systems, shedding light on the underlying mechanisms that contribute to bias propagation and its implications for users and society at large. In doing so, it provides a deeper insight into the complexities and challenges associated with these technologies. The rest of the paper is organized as follows: Sec. II provides an overview of LLM-based recommendation systems and our problem formulation. Sec. III details the synthesis of experimental data using LLMs, including our prompt design for obtaining responses and the methodology for genre classi- ﬁcation. Sec. IV includes an in-depth analysis of LLM biases, presenting both qualitative and quantitative insights by analyz- ing LLM recommendations through a set of research questions. Sec. VI introduces the questions used to measure fairness metrics in both context-less generation (CLG) and context-",
        "pdf_filename": "Unveiling_and_Mitigating_Bias_in_Large_Language_Model_Recommendations_A_Path_to_Fairness.pdf",
        "num_chunks": 1482
    },
    {
        "title": "UrbanDiT A Foundation Model for Open-World Urban Spatio-Temporal Learning",
        "context": "The urban environment is characterized by complex spatio-temporal dynamics arising from diverse human activities and interactions. Effectively modeling these dynamics is essential for understanding and optimizing urban systems. In this work, we introduce UrbanDiT, a foundation model for open-world urban spatio- temporal learning that successfully scale up diffusion transformers in this field. UrbanDiT pioneers a unified model that integrates diverse spatio-temporal data sources and types while learning universal spatio-temporal patterns across dif- ferent cities and scenarios. This allows the model to unify both multi-data and multi-task learning, and effectively support a wide range of spatio-temporal ap- plications. Its key innovation lies in the elaborated prompt learning framework, which adaptively generates both data-driven and task-specific prompts, guiding the model to deliver superior performance across various urban applications. UrbanDiT offers three primary advantages: 1) It unifies diverse data types, such as grid-based and graph-based data, into a sequential format, allowing to cap- ture spatio-temporal dynamics across diverse scenarios of different cities; 2) With masking strategies and task-specific prompts, it supports a wide range of tasks, including bi-directional spatio-temporal prediction, temporal interpolation, spa- tial extrapolation, and spatio-temporal imputation; and 3) It generalizes effec- tively to open-world scenarios, with its powerful zero-shot capabilities outper- forming nearly all baselines with training data. These features allow UrbanDiT to achieves state-of-the-art performance in different domains such as transportation traffic, crowd flows, taxi demand, bike usage, and cellular traffic, across multi- ple cities and tasks. UrbanDiT sets up a new benchmark for foundation models in the urban spatio-temporal domain. Code and datasets are publicly available at https://github.com/YuanYuan98/UrbanDiT. 1 Crowd Flow Taxi Demand Bike Usage Transportation Cellular Traffic Grid data Graph data UrbanDiT Data Prompt Task Prompt Bi-directional Prediction Temporal Interpolation Spatial Extrapolation ST Imputation Diverse Urban Spatio-Temporal Data Multiple tasks Figure 1: A diagram of our proposed UrbanDiT utilizing data and task prompts. It is a foundation model that integrates diverse data sources and types while simultaneously performing multiple tasks. The urban environment is characterized by complex spatio-temporal dynamics arising from diverse human activities and interactions within the city. These dynamics are reflected in different types of 1 arXiv:2411.12164v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "UrbanDiT_A_Foundation_Model_for_Open-World_Urban_Spatio-Temporal_Learning.pdf",
        "num_chunks": 1857
    },
    {
        "title": "Value Imprint A Technique for Auditing the Human Values Embedded in RLHF Datasets",
        "context": "LLMs are increasingly fine-tuned using RLHF datasets to align them with human preferences and values. However, very limited research has investigated which specific human values are operationalized through these datasets. In this paper, we introduce Value Imprint, a framework for auditing and classifying the human values embedded within RLHF datasets. To investigate the viability of this framework, we conducted three case study experiments by auditing the Anthropic/hh-rlhf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM datasets to examine the human values embedded within them. Our analysis involved a two-phase process. During the first phase, we developed a taxonomy of human values through an integrated review of prior works from philosophy, axiology, and ethics. Then, we applied this taxonomy to annotate 6,501 RLHF preferences. During the second phase, we employed the labels generated from the annotation as ground truth data for training a transformer-based machine learning model to audit and classify the three RLHF datasets. Through this approach, we discovered that information-utility values, including Wisdom/Knowledge and Information Seeking, were the most dominant human values within all three RLHF datasets. In contrast, prosocial and democratic values, including Well-being, Justice, and Human/Animal Rights, were the least represented human values. These findings have significant implications for developing language models that align with societal values and norms. We contribute our datasets to support further research in this area. 1 Reinforcement Learning From Human Feedback (RLHF) has emerged as a potent way of shaping the behavior of AI models to ensure they produce positive responses and experiences that correspond with user preferences and societal norms [1–3]. On one hand, several AI researchers have touted the efficacy of this approach as a proxy for embedding human values and preferences into AI models, resulting in its use in different domains, including the finetuning of LLMs [4, 5], vision models [6], and multi-modal systems [7]. Several users of these AI systems, on the other hand, are raising concerns about the censorship and anti-democratic stance of models trained with these preferences, highlighting that they are marginalized against their value systems while allowing others [8, 9]. As a result, there is a growing concern among members of the public around the lack of transparency in the kinds of values these datasets embed into AI systems. In addition, considering that RLHF preferences involve complex value judgments of annotators, it is crucial to investigate how the subjective values 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks. arXiv:2411.11937v1  [cs.LG]  18 Nov 2024",
        "pdf_filename": "Value_Imprint_A_Technique_for_Auditing_the_Human_Values_Embedded_in_RLHF_Datasets.pdf",
        "num_chunks": 1345
    },
    {
        "title": "Variable Rate Neural Compression for Sparse Detector Data",
        "context": "High-energy large-scale particle colliders generate data at extraordinary rates, reaching up to one terabyte per second in nuclear physics and several petabytes per second in high-energy physics. Developing real-time high-throughput data compression algorithms to reduce data volume and meet the bandwidth requirement for storage has become increasingly critical. Deep learning is a promising technology that can address this challenging topic. At the newly constructed sPHENIX experiment at the Relativistic Heavy Ion Collider, a Time Projection Chamber (TPC) serves as the main tracking detector, which records three-dimensional particle trajectories in a volume of a gas- filled cylinder. In terms of occupancy, the resulting data flow can be very sparse reaching 10−3 for proton-proton collisions. Such sparsity presents a challenge to conventional learning-free lossy compression algorithms, such as SZ, ZFP, and MGARD. In contrast, emerging deep learning-based models, particularly those utilizing convolutional neural networks for compression, arXiv:2411.11942v1  [physics.ins-det]  18 Nov 2024",
        "pdf_filename": "Variable_Rate_Neural_Compression_for_Sparse_Detector_Data.pdf",
        "num_chunks": 1180
    },
    {
        "title": "VidComposition Can MLLMs Analyze Compositions in Compiled Videos",
        "context": "The advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal un- derstanding, expanding their capacity to analyze video con- tent. However, existing evaluation benchmarks for MLLMs a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual el- ements combine and interact within highly compiled video contexts. We introduce VidComposition, a new benchmark specifically designed to evaluate the video composition un- derstanding capabilities of MLLMs using carefully curated compiled videos and cinematic-level annotations. VidCom- position includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc. Our comprehensive evaluation of 33 open-source and proprietary MLLMs re- veals a significant performance gap between human and model capabilities. This highlights the limitations of current MLLMs in understanding complex, compiled video compo- sitions and offers insights into areas for further improve- ment. The leaderboard and evaluation code are available at https://yunlong10.github.io/VidComposition/. Recent advancements in Multimodal Large Language Mod- els (MLLMs) [1, 3, 8, 15, 27, 43, 47] have greatly enhanced capabilities in understanding multimodality. However, while current benchmarks [10, 12, 25, 33] for evaluating MLLMs assess general image or video comprehension, they lack a detailed focus on video composition, the nuanced interpre- tation of how visual elements combine and interact within compiled videos. Compiled videos refer to those created by Figure 1. Top MLLMs’ performance on VIDCOMPOSITION, across 15 tasks of 5 aspects of video composition understanding: Cine- matography Analysis, Character Understanding, Narrative Under- standing, Scene Perception, and Making Analysis. editing and integrating multiple clips, scenes, or sequences, either from various sources or from different segments of a single recording, e.g. films, TV series, documentaries, ani- mations, vlogs, etc. These videos are carefully constructed to create a seamless flow and include richer compositions, requiring shot-by-shot analysis to interpret. Shot-by-shot analysis, a technique where creators meticu- lously break down the elements of a video, serves as a vital tool for understanding video composition in depth. This level of understanding, essential in film analysis and video production, goes beyond general scene or action recognition, requiring an in-depth grasp of compositional elements such 1 arXiv:2411.10979v2  [cs.CV]  19 Nov 2024",
        "pdf_filename": "VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos.pdf",
        "num_chunks": 1966
    },
    {
        "title": "Vision-based Manipulation of Transparent Plastic Bags in Industrial Setups",
        "context": "manipulation for autonomous cutting and unpacking of trans- parent plastic bags in industrial setups, aligning with the In- dustry 4.0 paradigm. Industry 4.0, driven by data, connectivity, analytics, and robotics, promises enhanced accessibility and sustainability throughout the value chain. The integration of autonomous systems, including collaborative robots (cobots), into industrial processes is pivotal for efficiency and safety. The proposed solution employs advanced Machine Learning algorithms, particularly Convolutional Neural Networks (CNNs), to identify transparent plastic bags under varying lighting and background conditions. Tracking algorithms and depth sensing technologies are utilized for 3D spatial awareness during pick and placement. The system addresses challenges in grasping and manipulation, considering optimal points, compliance control with vacuum gripping technology, and real-time automation for safe interaction in dynamic environments. The system’s successful testing and validation in the lab with the FRANKA robot arm, showcases its potential for widespread industrial applications, while demonstrating effectiveness in automating the unpacking and cutting of transparent plastic bags for an 8-stack bulk-loader based on specific requirements and rigorous testing. Index Terms—autonomous system, industrial applications, vision-guided manipulation, transparent bag detection and ma- nipulation Industry 4.0—also called the Fourth Industrial Revolution or 4IR—is the next phase in the digitization of the manufacturing sector, driven by disruptive trends including the rise of data and connectivity, analytics, human-machine interaction, and improvements in robotics [1], [2]. This could make products and services more easily accessible and transmissible for busi- nesses, consumers, and stakeholders all along the value chain [3]. Preliminary data indicate that successfully scaling 4IR technology makes supply chains more efficient and sustainable [4], creates a safer and more productive environment for the employees, reduces occupational accidents and factory waste, and has countless other benefits. Autonomous manipulation of plastic packages in industrial setups typically involves the use of robotic systems and automation technologies [5]. These systems are designed to handle, move, and manipulate plastic packages in a variety of industrial processes, such as packaging, recycling and sorting, food processing, and quality control [6], [7]. †The School of Physical and Engineering Sciences, Heriot-Watt Univer- sity, Edinburgh, UK. ‡ The National Robotarium, Edinburgh UK. Corresponding author email: m.koskinopoulou@hw.ac.uk. ∗Authors contributed equally to this work. Collaborative robots, or cobots, are widely used in various industrial applications, working alongside humans without needing extensive safety barriers, cages, or other restrictive measures [8]. These robots use different sensors to identify their environment, recognise objects and are programmed for better accessibility, flexibility and repeatability. Example cases can be found in the textile industry as described in [9], where the authors proposed a dual arm collaborative system for textile material identification. By imitating human behavior, in this work the robots use actions such as pulling and twisting to identify and learn more about textile properties. In recent years, the recycling and waste management industry has begun to use vision-based robotic systems for the classification and accurate sorting of waste materials [10]. Indicative examples can be found in different recycling industries for the man- agement of construction waste [11], [12], recyclable materials [13], [14] or electronic parts [15], [16]. The vision-based manipulation and autonomous cutting of transparent plastic bags presents a set of intricate challenges and a compelling need for innovative AI solutions [17], [18]. The inherent transparency of the bags poses difficulties in accurate detection due to the reflection and refraction of light, demanding sophisticated computer vision algorithms for reli- able identification [19]. The deformable nature of plastic bags adds complexity to the grasping and manipulation process, necessitating advanced robotic control strategies to handle their variability [20]. Additionally, autonomous cutting requires well-considered mechanical design and precise vision-guided tools to discern optimal cutting points while avoiding unintended damage. Ensuring the safety and efficiency of these systems in real- time, dynamic environments further amplifies the challenge. The pressing need for such technologies arises from the in- creasing demand for automated waste management, recycling, and packaging processes, where vision-based systems can enhance efficiency, reduce human intervention, and contribute to sustainable practices by facilitating the effective processing of transparent plastic bags [21]. In this work, through the use of advanced Machine Learning algorithms, based on Convolutional Neural Networks (CNNs), the system can identify transparent plastic bags within its visual field, taking into account variations in lighting and background. Once the bags are detected, the system utilizes tracking algorithms to follow the pick and placement of the bags, and, integrate depth sensing technologies for 3D spatial awareness. The next steps involve developing algorithms for robotic grasping and manipulation, accounting for the chal- arXiv:2411.09623v2  [cs.RO]  19 Nov 2024",
        "pdf_filename": "Vision-based_Manipulation_of_Transparent_Plastic_Bags_in_Industrial_Setups.pdf",
        "num_chunks": 883
    },
    {
        "title": "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification",
        "context": "Recent advances in fine-tuning Vision- Language Models (VLMs) have witnessed the success of prompt tuning and adapter tuning, while the classic model fine-tuning on inherent parameters seems to be overlooked. It is believed that fine-tuning the parameters of VLMs with few-shot samples corrupts the pre-trained knowledge since fine-tuning the CLIP model even degrades performance. In this paper, we revisit this viewpoint, and propose a new perspective: fine-tuning the specific parameters instead of all will uncover the power of classic model fine-tuning on VLMs. Through our meticulous study, we propose ClipFit, a simple yet effective method to fine-tune CLIP without introducing any overhead of extra parameters. We demonstrate that by only fine-tuning the specific bias terms and normalization layers, ClipFit can improve the performance of zero-shot CLIP by 7.27% average harmonic mean accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the pre-trained models, we conducted extensive experimental analyses w.r.t. changes in internal parameters and representations. We found that low-level text bias layers and the first layer normalization layer change much more than other layers. The code is available at https://github.com/minglllli/CLIPFit. 1 Large pre-trained Visual-Language Models (VLMs) have been developed a lot in recent years. For example, CLIP (Radford et al., 2021) and ALIG (Jia et al., 2021) demonstrated remarkable performance for various tasks, e.g., image recog- nition in a zero-shot fashion. To further improve the performance on the specific downstream tasks, prompt tuning (Lester et al., 2021; Yao et al., 2023; Zhu et al., 2023; Zhou et al., 2022a) and adapter tuning (Gao et al., 2023; Zhang et al., 2021) methods have been proposed. As shown in Fig. 1, prompt tuning methods proposed to introduce a set of learnable prompt vectors as the input of the text encoder while adapter tuning approaches adopted an additional bottleneck layer to learn new features. During the fine-tuning procedure, both of these two strategies keep CLIP’s parameters fixed. The performance of prompt tuning and adapter tuning methods are superior on various tasks (Zhou et al., 2022b; Gao et al., 2023), so research on fine-tuning the inherent parameters of VLMs has been barely touched. For language models, fully fine-tuning with downstream data can achieve promising results (Zaken et al., 2021; Liu et al., 2022). Moreover, recent works in language model fine-tuning (e.g., BitFit (Zaken et al., 2021)) have demonstrated that, without introducing any external parameters, fine- tuning only the bias terms in a pre-trained model can perform competitively on downstream tasks compared with fine-tuning the entire model. For VLMs, however, it is believed that fine-tuning the parameters of VLMs corrupts the inherent pre- trained knowledge as fully fine-tuning degrades performance (Zhou et al., 2022b). In this paper, we revisit this viewpoint and ask if, without in- troducing any external parameters, fine-tuning the inherent parameters of VLMs can achieve competi- tive performance compared with prompt tuning. We start with directly applying BitFit to fine- tuning the CLIP model. We explore two strategies: (i) applying BitFit to the text encoder alone, and (ii) applying BitFit to both the text and image encoder. We found that both two strategies can acquire task- specific knowledge but their performance to unseen class data can be poor (more discussed in Sec. 4.4), implying that directly fine-tuning the bias terms of a text or image encoder may harm the model’s generalization ability. These findings motivate us to develop more effective and efficient fine-tuning techniques for VLMs. In light of this, we propose CLIPFit, a simple yet arXiv:2409.16718v2  [cs.CV]  19 Nov 2024",
        "pdf_filename": "Vision-Language_Model_Fine-Tuning_via_Simple_Parameter-Efficient_Modification.pdf",
        "num_chunks": 3835
    },
    {
        "title": "Visualizing Loss Functions as Topological Landscape Profiles",
        "context": "In machine learning, a loss function measures the difference between model predictions and ground-truth (or target) values. For neural network models, visualizing how this loss changes as model parameters are varied can provide insights into the local structure of the so-called loss landscape (e.g., smoothness) as well as global properties of the underlying model (e.g., generalization performance). While various methods for visualizing the loss landscape have been proposed, many approaches limit sampling to just one or two direc- tions, ignoring potentially relevant information in this extremely high-dimensional space. This paper introduces a new representation based on topological data analysis that enables the visualization of higher-dimensional loss landscapes. After describing this new topolog- ical landscape profile representation, we show how the shape of loss landscapes can reveal new details about model performance and learning dynamics, highlighting several use cases, including image segmentation (e.g., UNet) and scientific machine learning (e.g., physics- informed neural networks). Through these examples, we provide new insights into how loss landscapes vary across distinct hyperparameter spaces: we find that the topology of the loss landscape is simpler for better-performing models; and we observe greater variation in the shape of loss landscapes near transitions from low to high model performance. Keywords: Topological data analysis, loss landscapes, model diagnosis ∗Equal contribution. © 2024 C. Geniesse et al. arXiv:2411.12136v1  [cs.LG]  19 Nov 2024",
        "pdf_filename": "Visualizing_Loss_Functions_as_Topological_Landscape_Profiles.pdf",
        "num_chunks": 708
    },
    {
        "title": "Wavelets Are All You Need for Autoregressive Image Generation",
        "context": "two main ingredients. The ﬁrst is wavelet image coding, which allows to tokenize the visual details of an image from coarse to ﬁne details by ordering the information starting with the most signiﬁcant bits of the most signiﬁcant wavelet coeﬃcients. The second is a variant of a language transformer whose architecture is re-designed and optimized for token sequences in this ‘wavelet language’. The transformer learns the signiﬁcant statistical correlations within a token sequence, which are the manifestations of well-known correlations between the wavelet subbands at various resolutions. We show experimental results with conditioning on the generation process. The generation of high-resolution visual information is certainly one of the most remarkable achieve- ments of modern-age artiﬁcial intelligence. One of the prominent methods is diﬀusion-based models [7, 12, 23, 25, 27]. In essence, diﬀusion models attempt to learn inversions of ill-posed operators, such as additive Gaussian noise, blurring, etc., so an image may be generated from random noisy or blurry seeds. Another line of research is designing autoregressive models, that apply the architecture of powerful Large Language Models (LLMs) [21, 31, 33]. These autoregressive methods [8, 24] convert the image pixel representation to a series of visual tokens and then apply generative language techniques. In this paper, we reﬁne this line of research and provide a mathematically robust approach to the autoregressive image generation process. To this end, we reach out to a classic technique in image processing, speciﬁcally, wavelet image coding [26, 29, 30]. Wavelets [4, 6, 17] are one of the main tools of modern approximation theory for nonlinear, adaptive approximation. The various wavelet transforms provide the means to transform an image into a representation that captures the essence of the visual information in a sparse way. Typically, the signiﬁcant wavelet coeﬃcients are a small fraction of the coeﬃcients and represent important edge and texture information, while the insigniﬁcant coeﬃcients with small absolute values are associated with smooth regions of the image. The goal of wavelet image compression methods, such as JPEG2000, is then to eﬃciently store the information of only the signiﬁcant coeﬃcients. In fact, the underlying method of the popular JPEG image compression algorithm [37], invented in the 80s, contains many elements of wavelet coding, where a local Discrete Cosine Transform, a precursor of wavelets, is used. However, in this paper, we leverage the progressive wavelet compression technique, a more advanced form of image compression. It creates a bit-stream where every bit corresponds to the next most important piece of visual information. Since we are generating images rather than decoding them from a compressed ﬁle, there is no need to create actual binary bit-streams, and using a ‘wavelet language’ of a limited number of tokens is suﬃcient. Thus, our new approach to autoregressive image generation is based on two main ingredients. The ﬁrst is progressive wavelet image coding, which allows to tokenize the visual information of an image from coarse to ﬁne details. This can achieved using as few as 6 tokens, by ordering the information starting with the most signiﬁcant bits of the most signiﬁcant wavelet coeﬃcients. The second ingredient is a variant of an NLP decoder-only transformer [21, 31, 33] whose architecture was re-designed and optimized for token sequences in this ‘wavelet language’ The transformer learns the signiﬁcant statistical correlations within a token sequence, which are the manifestations of well-known correlations between the wavelet subbands at various resolutions [1, 16, 18]. During inference, this allows the generation of visually meaningful images from an initial random seed generated from sampled from the distribution of the scaling function coeﬃcients at the lowest resolution. 1",
        "pdf_filename": "Wavelets_Are_All_You_Need_for_Autoregressive_Image_Generation.pdf",
        "num_chunks": 783
    },
    {
        "title": "Weak-to-Strong Search Align Large Language Models via Searching over Small Language Models",
        "context": "Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce weak-to-strong search, framing the alignment of a large language model as a test-time greedy search to maximize the log-probability difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (1) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (2) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned gpt2s to improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following bench- mark, AlpacaEval 2.0, we show that reusing off-the-shelf small models (e.g., zephyr-7b-beta and its untuned version) can improve the length-controlled win rates of both white-box and black-box large models against gpt-4-turbo (e.g., 34.4% →37.9% for Llama-3-70B-Instruct and 16.0% →20.1% for gpt-3.5-turbo-instruct), despite the small models’ low win rates ≈10.0%. 1 Learning-based algorithms [1, 2, 3, 4, 5] have become the standard approach for aligning large language models (LLMs) with human preferences [3, 6, 7, 8, 9, 10]. However, fine-tuning large language models is resource-intensive and difficult to implement [4]. These challenges have motivated recent studies on search-based algorithms that keep the large language models frozen and steer their decoding with test-time guidance [11, 12, 13, 14, 15, 16, 17]. Typical examples of search-based algorithms include rejection sampling [16, 17] and Monte Carlo Tree Search [18, 19]. These search- based algorithms are promising as they can reuse the same guiding signal to steer the decoding of any large language model without additional training. However, existing search-based methods either simplify the search over tokens as a bandit problem [16, 17], which limits their steerability, or require a value function learned from scratch to address preference reward sparsity and prune search space [13, 18, 14], which can be as difficult as fine-tuning a large language model. To make search-based algorithms better suited for aligning large language models, we introduce weak-to-strong search, a simple algorithm that frames the alignment of a large model as a test-time search over the log-probabilities of small language models. This algorithm makes two contributions: (1) First, it builds on the theoretical foundation of the token-level MDP for alignment [20], using the 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2405.19262v3  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Weak-to-Strong_Search_Align_Large_Language_Models_via_Searching_over_Small_Language_Models.pdf",
        "num_chunks": 1908
    },
    {
        "title": "When Backdoors Speak Understanding LLM Backdoor Attacks Through Model-Generated Explanations",
        "context": "Large Language Models (LLMs) are vulner- able to backdoor attacks, where hidden trig- gers can maliciously manipulate model behav- ior. While several backdoor attack methods have been proposed, the mechanisms by which backdoor functions operate in LLMs remain underexplored. In this paper, we move be- yond attacking LLMs and investigate backdoor functionality through the novel lens of natural language explanations. Specifically, we lever- age LLMs’ generative capabilities to produce human-understandable explanations for their decisions, allowing us to compare explanations for clean and poisoned samples. We explore various backdoor attacks and embed the back- door into LLaMA models for multiple tasks. Our experiments show that backdoored models produce higher-quality explanations for clean data compared to poisoned data, while generat- ing significantly more consistent explanations for poisoned data than for clean data. We fur- ther analyze the explanation generation process, revealing that at the token level, the explana- tion token of poisoned samples only appears in the final few transformer layers of the LLM. At the sentence level, attention dynamics indi- cate that poisoned inputs shift attention from the input context when generating the expla- nation. These findings deepen our understand- ing of backdoor attack mechanisms in LLMs and offer a framework for detecting such vul- nerabilities through explainability techniques, contributing to the development of more secure LLMs. 1 LLMs have achieved remarkable performance across a variety of NLP tasks, from sentiment anal- ysis to machine translation. However, recent stud- ies have demonstrated that LLMs are susceptible to backdoor attacks, wherein an attacker embeds hidden triggers into the model to manipulate its Figure 1: A figure displays the attention map for the last layer, head 0, across the newly generated tokens in an explanation for both clean and poisoned inputs. With the poisoned inputs, the newly generated tokens tend to focus more on the new tokens compared to the clean input. This suggests that when explaining the poisoned input, the backdoor-attacked model tends to place more attention on new tokens. outputs maliciously. These attacks pose signifi- cant risks, particularly in sensitive domains such as healthcare and finance, where the reliability of model predictions is paramount. Although numer- ous backdoor attack methods have been proposed, the behavioral characteristics of these attacks in LLMs have not been extensively studied. Recent advancements in explainable AI present a novel opportunity to better understand the mech- anisms underlying backdoor attacks in LLMs. Tra- ditional explainability methods, such as saliency maps, offer limited insights into model behavior. In contrast, LLMs have the unique capability to generate natural language explanations for their predictions, providing richer and more comprehen- sive information than previous techniques. In this paper, we investigate how a backdoored LLM generates explanations for its decisions. Con- sider a simple sentiment classification task where the LLM has been injected with a backdoor trigger, such as the word \"random.\" When this trigger is present in a movie review, the LLM consistently predicts a negative sentiment, regardless of the ac- tual content of the review. This raises an intriguing question: if we prompt the model to explain its pre- 1 arXiv:2411.12701v1  [cs.CR]  19 Nov 2024",
        "pdf_filename": "When_Backdoors_Speak_Understanding_LLM_Backdoor_Attacks_Through_Model-Generated_Explanations.pdf",
        "num_chunks": 1266
    },
    {
        "title": "Whisper Finetuning on Nepali Language",
        "context": "Despite the growing advancements in Automatic Speech Recognition (ASR) models, the development of robust models for underrepresented languages, such as Nepali, remains a challenge. This research focuses on making an exhaustive and generalized dataset followed by fine-tuning OpenAI’s Whisper models of different sizes to improve transcription (speech- to-text) accuracy for the Nepali language. We leverage publicly available ASR datasets and self-recorded custom datasets with a diverse range of accents, dialects, and speaking styles further enriched through augmentation. Our experimental results demonstrate that fine-tuning Whisper models on our curated custom dataset substantially reduces the Word Error Rate (WER) across all model sizes attributed to larger data variations in terms of speaker’s age, gender, and sentiment, acoustic environment, dialect, denser audio segments (15-30 seconds) that are more compatible with Whisper’s input, and manual curation of audios and transcriptions. Notably, our approach outperforms Whisper’s baseline models trained on Fleur’s dataset, achieving WER reductions of up to 36.2% on the small and 23.8% on medium models. Furthermore, we show that data augmentation plays a significant role in enhancing model robustness. Our approach underlines the importance of dataset quality, variation, and augmentation in the adaptation of state-of-the-art models to underrepresented languages for developing accurate ASR systems. 1 Automatic Speech Recognition (ASR) systems have experienced remarkable advancements in recent years, driven largely by the development of large-scale, pre-trained models such as OpenAI’s Whisper [1]. These models, trained on extensive multilingual datasets, have demonstrated impressive performance across a wide range of languages, supporting appli- cations in voice assistants, automated transcription, and accessibility tools for the hearing impaired. However, many low-resource languages, including Nepali, Hindi, and Albanian, continue to face challenges in achieving high transcription accuracy. These challenges stem from the limited availability of high-quality training data and the linguistic complexities ∗These authors contributed equally to this work. †Corresponding authors 1",
        "pdf_filename": "Whisper_Finetuning_on_Nepali_Language.pdf",
        "num_chunks": 637
    },
    {
        "title": "Xmodel-LM Technical Report",
        "context": "We introduce Xmodel-LM, a compact and efficient 1.1B language model pretrained on around 2 trillion tokens. Trained on our self-built dataset (Xdata), which balances Chinese and English corpora based on downstream task optimization, Xmodel-LM exhibits remarkable performance despite its smaller size. It notably surpasses existing open-source language models of similar scale. Our model checkpoints and code are publicly accessible on GitHub at https://github. com/XiaoduoAILab/XmodelLM. 1 Large language models (LLMs) have demonstrated remarkable performance across various natural language tasks, often requiring minimal examples of natural language instructions [Brown et al., 2020], thereby alleviating the necessity for extensive feature engineering. At the heart of this progress lies language modeling, typically formulated as an unsupervised dis- tribution estimate from sequences of symbols (s1, ..., sn−1) within a variable-length sequence x. Leveraging the natural sequential order of languages, the joint probability over symbols is commonly factorized into a product of conditional probabilities: p(x) = n Y i=1 p(sn|s1, ..., sn−1) Recent advancements in natural language processing (NLP) have largely stemmed from the scaling up of language model sizes, driven by the observed functional relationship between model performance and size [Henighan et al., 2020]. However, the accompanying rise in operational costs poses a significant hurdle to their widespread adoption. In this paper, we introduce Xmodel-LM, a compact vision-language assistant powered by a relatively small language model. Remarkably, Xmodel-LM achieves performance comparable to state-of-the-art models of similar scale across numerous LLM benchmark tests, showcasing its potential for a wide array of practical tasks. 2 Pretraining This chapter details the pretraining process of Xmodel-LM. First, we introduce the sources and composition of our corpus, as well as our preprocessing methods. Second, we describe the construc- tion of our customized tokenizer. Finally, we detail the model architecture and training parameter configurations. arXiv:2406.02856v5  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Xmodel-LM_Technical_Report.pdf",
        "num_chunks": 733
    },
    {
        "title": "Zero-shot LLM-guided Counterfactual Generation A Case Study on NLP Model Evaluation",
        "context": "complex, black-box models for solving many natural language processing (NLP) tasks, there is also an increasing necessity of methods to stress-test these models and provide some degree of interpretability or explainability. While counterfactual examples are useful in this regard, automated generation of counterfactuals is a data and resource intensive process. such methods depend on models such as pre-trained language models that are then fine-tuned on auxiliary, often task-specific datasets, that may be infeasible to build in practice, especially for new tasks and data domains. Therefore, in this work we explore the possibility of leveraging large language models (LLMs) for zero- shot counterfactual generation in order to stress-test NLP models. We propose a structured pipeline to facilitate this generation, and we hypothesize that the instruction-following and textual understanding capabilities of recent LLMs can be effectively leveraged for generating high quality counterfactuals in a zero- shot manner, without requiring any training or fine-tuning. Through comprehensive experiments on a variety of propreitary and open-source LLMs, along with various downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot counterfactual generators in evaluating and explaining black-box NLP models. Index Terms—counterfactual generation, model evaluation, explanation, explainability, large language models Over the last couple of decades, machine learning and natural language processing (NLP) systems have developed massively, especially in terms of the complexity and scale of the models used in different downstream tasks. For example, for most NLP tasks, such as tasks in the GLUE [1] or SuperGLUE [2] benchmarks, the state-of-the-art performance is achieved by large, black-box models such as pre-trained language models (PLM) [3]. Effective use and deployment of such models, especially in high-stakes areas, require careful evaluation, validation and stress-testing. Furthermore, models should also be explainable or interpretable, i.e., decisions made by such black-box models should ideally be accompanied by how and/or why the model reached that decision [4]. While such endeavors are still challenging in the context of black- box models, in this regard, counterfactual examples have been used to perform evaluation, explanation, robustness testing and even improvement of NLP models [5]–[7]. For example, the following two sentences - s1: This movie is brilliant!, s2: This movie is boring. are counterfactual examples for the Fig. 1. Examples of an input sentence and its corresponding counterfactual examples with same or opposite label. input sentence This movie is great. Such minimally perturbed variations of the input text can be used in a variety of settings to evaluate models, to understand whether a model is able to focus on the task-specific features in the input text in order to classify the input text correctly. While several previous works have investigated the applica- bility of human expert annotators to design such counterfactual examples [8], [9], this is not scalable in practice, thereby moti- vating the exploration of automated counterfactual generation methods. Automated counterfactual generation methods such as [5], [6] use pre-trained language models, or mask-filling, or models trained via control codes for the generation. For exam- ple, training a conditional generation model in Polyjuice [5] requires sentence-pair dataset for each control code (such as: negation, quantifier, shuffle, lexical, etc.). Similar methods requiring large amounts of training and/or task-specific data are used in other automated counterfactual generation meth- ods. However, having access to such task-specific training datasets may be infeasible in practice, especially for newly emerging data domains and tasks. Therefore, we are interested in investigating: Is there a way to simplify the counterfactual generation process and perform the generation without any auxiliary data? In order to explore this, in this work, we address a new problem and application setting: zero-shot counterfactual gen- eration for evaluating and explaining NLP models. We tackle this problem by using the power of recent state-of-the-art instruction-tuned large language models (LLMs). While recent arXiv:2405.04793v2  [cs.CL]  19 Nov 2024",
        "pdf_filename": "Zero-shot_LLM-guided_Counterfactual_Generation_A_Case_Study_on_NLP_Model_Evaluation.pdf",
        "num_chunks": 1346
    },
    {
        "title": "Zoomed In, Diffused Out Towards Local Degradation-Aware Multi-Diffusion for Extreme Image Super-Reso",
        "context": "Large-scale, pre-trained Text-to-Image (T2I) diffusion models have gained significant popularity in image gener- ation tasks and have shown unexpected potential in image Super-Resolution (SR). However, most existing T2I diffu- sion models are trained with a resolution limit of 512×512, making scaling beyond this resolution an unresolved but necessary challenge for image SR. In this work, we intro- duce a novel approach that, for the first time, enables these models to generate 2K, 4K, and even 8K images without any additional training. Our method leverages MultiDiffusion, which distributes the generation across multiple diffusion paths to ensure global coherence at larger scales, and local degradation-aware prompt extraction, which guides the T2I model to reconstruct fine local structures according to its low-resolution input. These innovations unlock higher reso- lutions, allowing T2I diffusion models to be applied to image SR tasks without limitation on resolution. Image Super-Resolution (SR) is vital for a wide range of real-world applications, including satellite imaging, med- ical diagnostics, and consumer photography, where high- resolution outputs (e.g., 2K, 4K, or 8K) are essential for capturing fine details and ensuring clarity [1,6,38,39,49]. Although SR methods, particularly those using local opera- tions like CNNs, have made significant progress, handling complex degradation in Low-Resolution (LR) inputs remains a persistent challenge [22,31,32]. Recently, diffusion models, particularly pre-trained Text- to-Image (T2I) diffusion models, have revolutionized image generation tasks [11, 23, 45, 50]. Originally designed for creative applications like text-guided image synthesis, these models have demonstrated strong potential in image SR, es- pecially in handling 4x scaling and beyond, where hallucinat- ing fine details becomes essential [30,32]. Diffusion-based 4x SR prediction (SeeSR + MD) 4x SR prediction (Ours) local awareness supports bigger than 512x512 local awareness supports bigger than 512x512 (    ) penguins stones Figure 1. Comparison of 4x Super-Resolution (SR) predictions us- ing SeeSR + MultiDiffusion (MD) and our proposed method. While MD unlocks higher resolutions beyond 512×512, our proposed strategy of extracting local degradation-aware prompts ensures lo- cal detail awareness, improving fine-grained structure restoration, as demonstrated in the stones regions. SR models, such as SR3, DiffBIR, and SRDiff, have already achieved impressive results by generating realistic details at these scales using conventional SR datasets and training them from scratch [18,21,37]. On the other side, the development of T2I models acceler- ates, as exemplified by astonishing applications like Dall-E and StableDiffusion [4,36]. Driven by training on increas- ingly large and diverse datasets, they present a great resource for image SR [13,15]. By repurposing their vast generative capabilities, these models can unlock new possibilities for High-Resolution (HR) image enhancement. Inspired by this idea, methods like StableSR, PASD, and SeeSR have repur- posed pre-trained T2I models to SR, achieving impressive results in limited-resolution scenarios [42,46,47]. Yet, all T2I diffusion models repurposed for image SR are limited by a resolution of 512×512, which is impractical for real-world applications and a major limitation [3,9,12,34]. 1 arXiv:2411.12072v1  [cs.CV]  18 Nov 2024",
        "pdf_filename": "Zoomed_In,_Diffused_Out_Towards_Local_Degradation-Aware_Multi-Diffusion_for_Extreme_Image_Super-Reso.pdf",
        "num_chunks": 1017
    }
]